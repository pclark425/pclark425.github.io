<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1958 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1958</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1958</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-279391992</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.11261v1.pdf" target="_blank">Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Robotic manipulation faces a significant challenge in generalizing across unseen objects, environments and tasks specified by diverse language instructions. To improve generalization capabilities, recent research has incorporated large language models (LLMs) for planning and action execution. While promising, these methods often fall short in generating grounded plans in visual environments. Although efforts have been made to perform visual instructional tuning on LLMs for robotic manipulation, existing methods are typically constrained by single-view image input and struggle with precise object grounding. In this work, we introduce Gondola, a novel grounded vision-language planning model based on LLMs for generalizable robotic manipulation. Gondola takes multi-view images and history plans to produce the next action plan with interleaved texts and segmentation masks of target objects and locations. To support the training of Gondola, we construct three types of datasets using the RLBench simulator, namely robot grounded planning, multi-view referring expression and pseudo long-horizon task datasets. Gondola outperforms the state-of-the-art LLM-based method across all four generalization levels of the GemBench dataset, including novel placements, rigid objects, articulated objects and long-horizon tasks.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1958.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1958.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gondola</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gondola: Grounded Vision-Language Planning Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-view, history-aware vision-language planning model that generates interleaved textual plans and per-view segmentation masks (<seg> token) as grounded action plans for robotic manipulation; integrates a frozen InternVL ViT visual encoder, an InternVL LLM with LoRA fine-tuning, and SAM2 for mask decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gondola</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gondola tokenizes K multi-view RGB images with a frozen InternVL ViT (448×448 -> patch embeddings + 2-layer MLP), concatenates view tokens (view separation via special token), and prepends image tokens to an InternVL-4B LLM (frozen base, LoRA adapters). The LLM outputs textual plan tokens interleaved with special paired delimiters <p>...</p> for object/location references and a dedicated <seg> token. The hidden embedding for <seg> (h_seg) is projected via a 2-layer MLP to form a prompt embedding for SAM2, which decodes per-view binary segmentation masks. Predicted per-view masks are fused with aligned depth to a consolidated 3D point cloud and category-labelled (target object, target location, robot, obstacle) for downstream 3D motion planning (3D-LOTUS++ motion policy). Training jointly optimizes next-token cross-entropy and per-pixel BCE + dice losses for masks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>InternVL ViT (InternVL-300M) frozen + 2-layer adaption MLP</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>InternVL foundation model (large-scale visual-linguistic pretraining / image & video grounding data as used in InternVL / Sa2VA pipelines) — (as reported: trained on large-scale image and video grounding datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>LLM outputs special <seg> token; its hidden embedding is projected to a prompt embedding that conditions SAM2 to produce per-view segmentation masks for the referenced object/location (decoupled LLM-to-segmentation via learned prompt embedding); grounding is therefore via projected LLM token -> SAM2 mask decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level masks per view, fused into a 3D point cloud (multi-level: pixel-level -> fused 3D point-level for motion planning)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>multi-view 2D masks per view + aligned depth to create a consolidated 3D point cloud with point categories (target object, target location, robot, obstacle); no explicit learned 3D coordinate backbone, uses depth+mask fusion and point categorization for spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation / instruction-following (robotic manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>GemBench (evaluated in RLBench simulator) and real-robot demo</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation (RLBench) with a small real-robot deployment (RealSense cameras + UR5) for transfer tests</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task execution Success Rate (SR); grounded planning metrics: action accuracy, object name accuracy, mask IoU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Simulation task SR (Gondola integrated with 3D-LOTUS++ motion policy, action-chunk=5): L1 87.3% ±1.9, L2 74.8% ±1.8, L3 52.4% ±2.1, L4 19.0% ±1.0. Grounded planning (multi-view + history + full fine-tuning) shows per-keystep action/object accuracies near 99-100% on L1 and mask IoU improvements over box baselines (see ablations). Real-robot seen tasks average: 50% (5/10); unseen tasks average: 37% (3.7/10).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Box-based grounding ablation (textual box coords output + boxes -> SAM2 masks) on grounded planning evaluation: L1 Action 95.1%, Object 93.7%, Grd IoU 62.8%; L2 Action 97.4%, Obj 89.0%, Grd IoU 58.7%; L3 Action 69.3%, Obj 53.8%, Grd IoU 46.2%; L4 Action 70.0%, Obj 36.8%, Grd IoU 16.6%. By contrast, mask-based end-to-end VLM grounding achieves much higher grounding IoU (e.g. L1 Grd ~87.8% in mask variant).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>End-to-end mask generation vs box-based text boxes improved mask IoU by large margins (approx +25.0 p.p. IoU on L1 (62.8 -> ~87.8), +21.1 p.p. on L2, +16.5 p.p. on L3, +20.8 p.p. on L4) and also improved action/object name accuracies in planning evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Occlusions and limited field-of-view are identified as major perceptual bottlenecks; single-view inputs degrade grounding and planning. Multi-view inputs improve occlusion handling but increase token budget and make encoding long visual histories difficult. In real-robot deployment, lower multi-view consistency causes frequent segmentation errors and degrades cloth/appearance generalization. History-plan distribution shift causes the model to default to textual planning at the expense of grounded mask generation in long-horizon settings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Examples and analyses provided: (1) PutInCupboard: Gondola's plan correct but partial observation of cupboard causes motion planner to fail (object falls outside intended area) — symptomatic of limited scene coverage/occlusion; (2) SlideBlock: correct plan but motion planner fails on contact-rich long-horizon trajectory prediction, and incorrect history fed into Gondola can cause re-planning failure; (3) Real-world transfer: segmentation errors due to inconsistent multi-view appearance reduce success (real seen tasks average 50% vs simulation L1 87.3%). The paper provides qualitative failure examples and per-task tables; quantitative frequency: L4 SR is low (19% in sim), real unseen avg 37% success — indicating grounding + motion planning / domain shift failure rates.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Mitigation via fine-tuning on synthetic RLBench grounded planning data (15k tuples), multi-view referring-expression data (15k multi-view examples, 58k referring expressions), and constructed pseudo long-horizon tasks (concatenated sequences with LLM-written joint instructions) to reduce history-distribution shift; for real-robot transfer they fine-tune on small teleoperated demonstration dataset (20×7 demos) jointly with RLBench. Despite these measures, significant sim-to-real drop persists; paper reports lower multi-view consistency and increased segmentation errors in real-world.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>GemBench generalization: Gondola (no L2-L4 training) achieves SR L2 74.8% (novel rigid objects), L3 52.4% (novel articulated), L4 19.0% (long-horizon). Compared to 3D-LOTUS++, Gondola shows +10.3 p.p. in L2, +10.9 p.p. in L3 and +1.6 p.p. in L4.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Architecture choices: visual encoder (InternVL ViT) and base LLM (InternVL-4B) are kept frozen; LoRA adapters are trained for the LLM and SAM2 mask decoder is fine-tuned; image encoder adaptor MLP (2-layer) is trained. The paper does not present a direct ablation comparing fully frozen vs fully fine-tuned visual encoder performance.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early multimodal fusion: image patch tokens for all views are concatenated into the LLM token sequence (with view separation token). Language and visual tokens are jointly processed by the LLM; grounding is signalled via dedicated tokens (<p>...</p>, <seg>) and decoded via SAM2 (decoupled LLM->seg decoder prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training datasets: ~15k robot grounded planning tuples (100 episodes per 31 GemBench training variations), ~15k multi-view referring examples and ~58k referring expressions; trained for 10k iterations on 8 H100s (effective batch 32). The paper shows that adding multi-view referring data and pseudo long-horizon data substantially improves grounding and L4 planning performance (see Table 2), indicating improved sample efficiency vs single-dataset fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>End-to-end mask-based grounding from a VLM + segmentation decoder (LLM special <seg> token -> SAM2 prompt) yields more precise and reliable object grounding than generating numeric boxes; multi-view inputs substantially reduce occlusion-driven grounding errors; including history plans improves mid-horizon planning but causes a distribution shift that harms long-horizon grounding unless pseudo long-horizon data are used; 3D post-processing (DBSCAN filtering) gives minimal gains, indicating spatial coherence already present in predicted masks; real-world multi-view consistency and data scarcity remain major bottlenecks for grounding transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1958.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1958.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Box-based variant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Box-based grounding variant (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that generates bounding box coordinates as textual outputs from the VLM/LLM and converts boxes to masks by feeding them to SAM2, used to evaluate whether explicit numeric boxes suffice vs end-to-end mask generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Box-based grounding ablation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM directly emits numerical bounding box coordinates (per view) as textual tokens; predicted boxes are converted into segmentation masks by providing boxes to SAM2. This is compared against the mask-token-based end-to-end mask generation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Same InternVL ViT as Gondola when used for fair comparison</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Same InternVL pretraining as Gondola (large-scale visual-linguistic grounding data referenced via Sa2VA/InternVL)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Textual numeric bounding-box emission by LLM -> boxes passed as SAM2 prompts to get masks (proposal-like pipeline), i.e., explicit box coordinate generation then mask decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>region-level (bounding boxes converted to masks via SAM2); results show coarser grounding than direct mask generation.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>bounding boxes per view (2D); converted to per-view masks and fused to 3D point cloud as with Gondola.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation (same GemBench tasks used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>GemBench (grounded planning evaluation in RLBench)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>simulation (RLBench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Action accuracy, object name accuracy, mask IoU (grounding), and downstream SR when integrated with motion policy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Grounded planning (box ablation): L1 Grd IoU 62.8%, L2 Grd IoU 58.7%, L3 Grd IoU 46.2%, L4 Grd IoU 16.6% (with lower action/object accuracies compared to mask-based variant). Downstream SRs are lower relative to Gondola (mask end-to-end).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>This entity is the degraded-grounding condition; see 'performance_value' above.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Compared to box-based ablation, end-to-end mask-based VLM grounding improved mask IoU by ~+16-25 percentage points across GemBench levels and improved action/object name accuracies in grounded planning evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Box-based generation is error-prone: numeric format errors when generating multiple numeric values across multiple images, greater sensitivity to proposal/format quality, and lower mask IoU leading to downstream failures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Box variant suffers format errors during numeric generation, lower grounding IoU, and worse action/object prediction accuracy; prone to error accumulation from inaccurate boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not specifically addressed for this variant; performs worse on generalization levels, highlighting fragility of numeric-output grounding under domain shift.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Worse than mask-based variant across L1-L4 grounded planning metrics (see numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>LLM emits boxes as text; boxes are converted to SAM2 prompts (late decoupled pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Evaluated with same fine-tuning datasets as Gondola; requires careful numeric-output generation which was observed to be less reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Generating explicit numeric bounding boxes as textual outputs is fragile and less accurate than end-to-end mask generation; end-to-end mask tokens (<seg>) + SAM2 prompting from learned prompt embeddings produce more accurate, compact, and reliable grounded plans for manipulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1958.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1958.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sa2VA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sa2VA (VLM dense grounding backbone used)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense grounding VLM that integrates a strong visual-language model (InternVL) with a SAM-derived segmentation pipeline to produce dense region/mask-grounded outputs; used as the base for Gondola fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sa2VA (base VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sa2VA marries a pretrained VLM (InternVL) with SAM2 to enable dense image/video grounding by providing segmentation tokens/prompt embeddings produced by the VLM to SAM2 for mask decoding; Gondola builds upon and fine-tunes Sa2VA for multi-view robotic grounding/planning.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>InternVL ViT-based visual encoder (as in Sa2VA) integrated in the VLM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Sa2VA reported training on large-scale image and video grounding datasets (exact dataset names/sizes not specified in this paper beyond 'large-scale image and video grounding data').</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Decoupled VLM-to-SAM prompting: VLM produces a specialized segmentation token embedding that is projected to a SAM prompt embedding; SAM decodes pixel-level masks — effectively a learned token->segmentation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level segmentation masks (dense grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>per-view 2D masks, can be used to form higher-level 3D representations when fused with depth</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>general vision-language grounding (in this paper used as perception backbone for robotic manipulation planning)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used as base for Gondola which is evaluated on GemBench / RLBench</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Web-scale and large-image/video grounding datasets (as per Sa2VA original training); Gondola fine-tunes it for RLBench simulation and small real datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Dense grounding accuracy / mask IoU in VLM benchmarks (as reported in original Sa2VA work; in this paper used as backbone so per-task metrics are for Gondola)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Sa2VA is fine-tuned by Gondola on task-specific multi-view and referring datasets to adapt from general image/video grounding to robotic viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>In Gondola, Sa2VA's VLM backbone (InternVL encoder and LLM base) are kept frozen while LoRA and SAM2 mask decoder are fine-tuned to adapt to multi-view robotic data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>VLM embedding token -> projected prompt -> SAM segmentation decoding (decoupled but learned prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Leveraging Sa2VA pretraining reduces required robot-specific labeled data; Gondola fine-tunes on ~15k planning tuples + referring data to adapt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Starting from a dense grounding VLM (Sa2VA) and fine-tuning for multi-view robotic grounding enables precise mask outputs and reduces hallucination vs direct numeric generation approaches; Sa2VA-style token->SAM prompting is effective for multi-view mask generation when adapted to robotic viewpoints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1958.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1958.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAM2 (Segment Anything - version 2 segmentation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A segmentation decoder used to convert prompt embeddings into per-view binary masks; in Gondola SAM2 is used as the mask generator conditioned on a projected LLM <seg> embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SAM2 receives a learned prompt embedding (projected from the LLM <seg> hidden state) per view and decodes binary segmentation masks for that view. Gondola fine-tunes the SAM2 mask decoder while keeping SAM2 encoder frozen to better match robot-view appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>SAM2 segmentation model (encoder frozen in Gondola, mask decoder fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>SAM family models are pre-trained on huge segmentation datasets (Segment Anything data corpus) — referenced as large-scale segmentation pretraining in the paper but exact dataset names/sizes are not restated here.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Prompted segmentation: LLM-projected prompt embeddings are used as SAM2 prompts to produce masks that ground textual object references to image pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level segmentation masks per view</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D masks per camera view; merged with depth to explicit 3D point cloud</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation grounding (used within Gondola for per-view mask generation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>GemBench / RLBench integration via Gondola</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>pretrained on broad segmentation datasets; applied to RLBench synthetic images and a small real-robot dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>mask IoU (used in grounded planning evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When coupled with Gondola's <seg> prompt embeddings, SAM2 produces per-view masks yielding mask IoUs reported in Gondola's ablations (e.g., mask-based grounding: L1 Grd ≈ 87.8% vs box-based 62.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>SAM2 prompt-based masking (with learned prompt from LLM) outperforms generating numeric boxes then converting to masks; fine-tuning SAM2 decoder further improves mask IoU for robotic views.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>SAM2 performance degrades under low multi-view consistency and domain shift (real-robot), causing segmentation errors that reduce downstream SR; fine-tuning the mask decoder partially mitigates this but data scarcity remains an issue.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Segmentation errors in the real world (inconsistent multi-view appearance) are a frequent cause of failure; per-paper qualitative examples show lower mask reliability on real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Fine-tune SAM2 mask decoder on small real-robot dataset in conjunction with RLBench to improve transfer; still substantial sim-to-real gap reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Gondola keeps SAM2 encoder frozen but fine-tunes the SAM2 mask decoder; no direct comparison versus fully frozen SAM2 provided.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>LLM-projected prompt embedding -> SAM2 decoder (prompted segmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Fine-tuning the SAM2 decoder with several thousand examples (from constructed datasets) was sufficient to improve mask IoU for RLBench views; exact sample-efficiency numbers tied to Gondola's training datasets (~15k planning tuples + referring) reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Prompting SAM2 from an LLM segmentation token is an effective decoupled grounding mechanism; fine-tuning SAM2's decoder on domain data improves per-view mask quality for robotic viewpoints, but multi-view consistency and sim-to-real domain shift remain limiting factors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1958.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1958.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-LOTUS++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D-LOTUS++ (LLM-based 3D policy baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LLM-guided 3D policy for robotic manipulation that uses in-context learning and extensive engineering to enable LLM planning without direct visual input; used as a state-of-the-art baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-LOTUS++</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM-based planning approach (from prior work) employing in-context learning and engineering to generate plans for a 3D motion policy. In this paper it is used for baseline comparison and its motion planning policy is reused for some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Not directly used for visual grounding in its original variant (relies less on direct visual input and more on engineering/in-context prompting); in this paper the 3D-LOTUS++ 3D motion policy is reused for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Prior method relies on LLM planning with limited direct visual grounding (in-context learning and engineered affordances) rather than end-to-end mask-based VLM grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>3D point-affordance/voxel/skill-based representations for motion policy (original paper); not mask-centred grounding like Gondola.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D representations for motion planning; in this paper the 3D motion policy and point categorization approach from 3D-LOTUS family are reused.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>robotic manipulation evaluated on GemBench</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>GemBench (RLBench)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>simulation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task execution Success Rate (SR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baseline SRs inferred from comparisons: approximate 3D-LOTUS++ SRs (per text differences): L2 ≈ 64.5% SR, L3 ≈ 41.5% SR, L4 ≈ 17.4% SR (derived from Gondola - reported deltas: Gondola L2 74.8% is +10.3 p.p. over 3D-LOTUS++; Gondola L3 52.4% is +10.9 p.p.; Gondola L4 19.0% is +1.6 p.p.).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Gondola (mask-based multi-view grounding) improves over 3D-LOTUS++ by +10.3 p.p. SR in L2, +10.9 p.p. in L3, +1.6 p.p. in L4, indicating strong benefits from explicit multi-view mask grounding for novel-object generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>3D-LOTUS++ relies on extensive engineering and in-context learning without direct visual grounding, making it less reliable when visual grounding is needed for novel objects/environments; Gondola's visual grounding addresses these deficits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Prior LLM-only or weakly-grounded planners (like 3D-LOTUS++ variants) tend to underperform on novel-object/articulated-object generalization and long-horizon tasks compared to grounded, mask-based multi-view approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>3D-LOTUS++ uses in-context learning and engineered affordances to partially handle distribution shifts; Gondola's fine-tuning on simulated multi-view grounding datasets is a different strategy that achieves higher SR on unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Lower than Gondola by ~10 p.p. in L2/L3 SR as reported above.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Not mask-token + SAM-style; rather in-context LLM planning possibly combined with 3D policy signals (prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relies on engineering/in-context examples rather than fine-tuning on grounded multi-view datasets; Gondola's data-driven grounding approach requires specific fine-tuning but yields better generalization on L2/L3.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Comparison shows that explicit, multi-view, mask-based vision-language grounding (Gondola) substantially improves generalization to novel rigid and articulated objects over prior LLM-driven planners that lack direct mask-level visual grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos <em>(Rating: 2)</em></li>
                <li>3D-LOTUS++ <em>(Rating: 2)</em></li>
                <li>Segment anything <em>(Rating: 2)</em></li>
                <li>InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks <em>(Rating: 2)</em></li>
                <li>RLBench: The robot learning benchmark & learning environment <em>(Rating: 2)</em></li>
                <li>GemBench <em>(Rating: 2)</em></li>
                <li>SayCan <em>(Rating: 1)</em></li>
                <li>VoxPoser: Composable 3d value maps for robotic manipulation with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1958",
    "paper_id": "paper-279391992",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "Gondola",
            "name_full": "Gondola: Grounded Vision-Language Planning Model",
            "brief_description": "A multi-view, history-aware vision-language planning model that generates interleaved textual plans and per-view segmentation masks (&lt;seg&gt; token) as grounded action plans for robotic manipulation; integrates a frozen InternVL ViT visual encoder, an InternVL LLM with LoRA fine-tuning, and SAM2 for mask decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gondola",
            "model_description": "Gondola tokenizes K multi-view RGB images with a frozen InternVL ViT (448×448 -&gt; patch embeddings + 2-layer MLP), concatenates view tokens (view separation via special token), and prepends image tokens to an InternVL-4B LLM (frozen base, LoRA adapters). The LLM outputs textual plan tokens interleaved with special paired delimiters &lt;p&gt;...&lt;/p&gt; for object/location references and a dedicated &lt;seg&gt; token. The hidden embedding for &lt;seg&gt; (h_seg) is projected via a 2-layer MLP to form a prompt embedding for SAM2, which decodes per-view binary segmentation masks. Predicted per-view masks are fused with aligned depth to a consolidated 3D point cloud and category-labelled (target object, target location, robot, obstacle) for downstream 3D motion planning (3D-LOTUS++ motion policy). Training jointly optimizes next-token cross-entropy and per-pixel BCE + dice losses for masks.",
            "visual_encoder_type": "InternVL ViT (InternVL-300M) frozen + 2-layer adaption MLP",
            "visual_encoder_pretraining": "InternVL foundation model (large-scale visual-linguistic pretraining / image & video grounding data as used in InternVL / Sa2VA pipelines) — (as reported: trained on large-scale image and video grounding datasets)",
            "grounding_mechanism": "LLM outputs special &lt;seg&gt; token; its hidden embedding is projected to a prompt embedding that conditions SAM2 to produce per-view segmentation masks for the referenced object/location (decoupled LLM-to-segmentation via learned prompt embedding); grounding is therefore via projected LLM token -&gt; SAM2 mask decoding.",
            "representation_level": "pixel-level masks per view, fused into a 3D point cloud (multi-level: pixel-level -&gt; fused 3D point-level for motion planning)",
            "spatial_representation": "multi-view 2D masks per view + aligned depth to create a consolidated 3D point cloud with point categories (target object, target location, robot, obstacle); no explicit learned 3D coordinate backbone, uses depth+mask fusion and point categorization for spatial reasoning.",
            "embodied_task_type": "object manipulation / instruction-following (robotic manipulation)",
            "embodied_task_name": "GemBench (evaluated in RLBench simulator) and real-robot demo",
            "visual_domain": "photorealistic simulation (RLBench) with a small real-robot deployment (RealSense cameras + UR5) for transfer tests",
            "performance_metric": "Task execution Success Rate (SR); grounded planning metrics: action accuracy, object name accuracy, mask IoU",
            "performance_value": "Simulation task SR (Gondola integrated with 3D-LOTUS++ motion policy, action-chunk=5): L1 87.3% ±1.9, L2 74.8% ±1.8, L3 52.4% ±2.1, L4 19.0% ±1.0. Grounded planning (multi-view + history + full fine-tuning) shows per-keystep action/object accuracies near 99-100% on L1 and mask IoU improvements over box baselines (see ablations). Real-robot seen tasks average: 50% (5/10); unseen tasks average: 37% (3.7/10).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Box-based grounding ablation (textual box coords output + boxes -&gt; SAM2 masks) on grounded planning evaluation: L1 Action 95.1%, Object 93.7%, Grd IoU 62.8%; L2 Action 97.4%, Obj 89.0%, Grd IoU 58.7%; L3 Action 69.3%, Obj 53.8%, Grd IoU 46.2%; L4 Action 70.0%, Obj 36.8%, Grd IoU 16.6%. By contrast, mask-based end-to-end VLM grounding achieves much higher grounding IoU (e.g. L1 Grd ~87.8% in mask variant).",
            "grounding_improvement": "End-to-end mask generation vs box-based text boxes improved mask IoU by large margins (approx +25.0 p.p. IoU on L1 (62.8 -&gt; ~87.8), +21.1 p.p. on L2, +16.5 p.p. on L3, +20.8 p.p. on L4) and also improved action/object name accuracies in planning evaluation.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Occlusions and limited field-of-view are identified as major perceptual bottlenecks; single-view inputs degrade grounding and planning. Multi-view inputs improve occlusion handling but increase token budget and make encoding long visual histories difficult. In real-robot deployment, lower multi-view consistency causes frequent segmentation errors and degrades cloth/appearance generalization. History-plan distribution shift causes the model to default to textual planning at the expense of grounded mask generation in long-horizon settings.",
            "failure_mode_analysis": "Examples and analyses provided: (1) PutInCupboard: Gondola's plan correct but partial observation of cupboard causes motion planner to fail (object falls outside intended area) — symptomatic of limited scene coverage/occlusion; (2) SlideBlock: correct plan but motion planner fails on contact-rich long-horizon trajectory prediction, and incorrect history fed into Gondola can cause re-planning failure; (3) Real-world transfer: segmentation errors due to inconsistent multi-view appearance reduce success (real seen tasks average 50% vs simulation L1 87.3%). The paper provides qualitative failure examples and per-task tables; quantitative frequency: L4 SR is low (19% in sim), real unseen avg 37% success — indicating grounding + motion planning / domain shift failure rates.",
            "domain_shift_handling": "Mitigation via fine-tuning on synthetic RLBench grounded planning data (15k tuples), multi-view referring-expression data (15k multi-view examples, 58k referring expressions), and constructed pseudo long-horizon tasks (concatenated sequences with LLM-written joint instructions) to reduce history-distribution shift; for real-robot transfer they fine-tune on small teleoperated demonstration dataset (20×7 demos) jointly with RLBench. Despite these measures, significant sim-to-real drop persists; paper reports lower multi-view consistency and increased segmentation errors in real-world.",
            "novel_object_performance": "GemBench generalization: Gondola (no L2-L4 training) achieves SR L2 74.8% (novel rigid objects), L3 52.4% (novel articulated), L4 19.0% (long-horizon). Compared to 3D-LOTUS++, Gondola shows +10.3 p.p. in L2, +10.9 p.p. in L3 and +1.6 p.p. in L4.",
            "frozen_vs_finetuned": "Architecture choices: visual encoder (InternVL ViT) and base LLM (InternVL-4B) are kept frozen; LoRA adapters are trained for the LLM and SAM2 mask decoder is fine-tuned; image encoder adaptor MLP (2-layer) is trained. The paper does not present a direct ablation comparing fully frozen vs fully fine-tuned visual encoder performance.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Early multimodal fusion: image patch tokens for all views are concatenated into the LLM token sequence (with view separation token). Language and visual tokens are jointly processed by the LLM; grounding is signalled via dedicated tokens (&lt;p&gt;...&lt;/p&gt;, &lt;seg&gt;) and decoded via SAM2 (decoupled LLM-&gt;seg decoder prompt).",
            "sample_efficiency": "Training datasets: ~15k robot grounded planning tuples (100 episodes per 31 GemBench training variations), ~15k multi-view referring examples and ~58k referring expressions; trained for 10k iterations on 8 H100s (effective batch 32). The paper shows that adding multi-view referring data and pseudo long-horizon data substantially improves grounding and L4 planning performance (see Table 2), indicating improved sample efficiency vs single-dataset fine-tuning.",
            "key_findings_grounding": "End-to-end mask-based grounding from a VLM + segmentation decoder (LLM special &lt;seg&gt; token -&gt; SAM2 prompt) yields more precise and reliable object grounding than generating numeric boxes; multi-view inputs substantially reduce occlusion-driven grounding errors; including history plans improves mid-horizon planning but causes a distribution shift that harms long-horizon grounding unless pseudo long-horizon data are used; 3D post-processing (DBSCAN filtering) gives minimal gains, indicating spatial coherence already present in predicted masks; real-world multi-view consistency and data scarcity remain major bottlenecks for grounding transfer.",
            "uuid": "e1958.0"
        },
        {
            "name_short": "Box-based variant",
            "name_full": "Box-based grounding variant (ablation)",
            "brief_description": "An ablation that generates bounding box coordinates as textual outputs from the VLM/LLM and converts boxes to masks by feeding them to SAM2, used to evaluate whether explicit numeric boxes suffice vs end-to-end mask generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Box-based grounding ablation",
            "model_description": "LLM directly emits numerical bounding box coordinates (per view) as textual tokens; predicted boxes are converted into segmentation masks by providing boxes to SAM2. This is compared against the mask-token-based end-to-end mask generation approach.",
            "visual_encoder_type": "Same InternVL ViT as Gondola when used for fair comparison",
            "visual_encoder_pretraining": "Same InternVL pretraining as Gondola (large-scale visual-linguistic grounding data referenced via Sa2VA/InternVL)",
            "grounding_mechanism": "Textual numeric bounding-box emission by LLM -&gt; boxes passed as SAM2 prompts to get masks (proposal-like pipeline), i.e., explicit box coordinate generation then mask decoding.",
            "representation_level": "region-level (bounding boxes converted to masks via SAM2); results show coarser grounding than direct mask generation.",
            "spatial_representation": "bounding boxes per view (2D); converted to per-view masks and fused to 3D point cloud as with Gondola.",
            "embodied_task_type": "object manipulation (same GemBench tasks used for comparison)",
            "embodied_task_name": "GemBench (grounded planning evaluation in RLBench)",
            "visual_domain": "simulation (RLBench)",
            "performance_metric": "Action accuracy, object name accuracy, mask IoU (grounding), and downstream SR when integrated with motion policy",
            "performance_value": "Grounded planning (box ablation): L1 Grd IoU 62.8%, L2 Grd IoU 58.7%, L3 Grd IoU 46.2%, L4 Grd IoU 16.6% (with lower action/object accuracies compared to mask-based variant). Downstream SRs are lower relative to Gondola (mask end-to-end).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "This entity is the degraded-grounding condition; see 'performance_value' above.",
            "grounding_improvement": "Compared to box-based ablation, end-to-end mask-based VLM grounding improved mask IoU by ~+16-25 percentage points across GemBench levels and improved action/object name accuracies in grounded planning evaluation.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Box-based generation is error-prone: numeric format errors when generating multiple numeric values across multiple images, greater sensitivity to proposal/format quality, and lower mask IoU leading to downstream failures.",
            "failure_mode_analysis": "Box variant suffers format errors during numeric generation, lower grounding IoU, and worse action/object prediction accuracy; prone to error accumulation from inaccurate boxes.",
            "domain_shift_handling": "Not specifically addressed for this variant; performs worse on generalization levels, highlighting fragility of numeric-output grounding under domain shift.",
            "novel_object_performance": "Worse than mask-based variant across L1-L4 grounded planning metrics (see numbers above).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "LLM emits boxes as text; boxes are converted to SAM2 prompts (late decoupled pipeline).",
            "sample_efficiency": "Evaluated with same fine-tuning datasets as Gondola; requires careful numeric-output generation which was observed to be less reliable.",
            "key_findings_grounding": "Generating explicit numeric bounding boxes as textual outputs is fragile and less accurate than end-to-end mask generation; end-to-end mask tokens (&lt;seg&gt;) + SAM2 prompting from learned prompt embeddings produce more accurate, compact, and reliable grounded plans for manipulation.",
            "uuid": "e1958.1"
        },
        {
            "name_short": "Sa2VA",
            "name_full": "Sa2VA (VLM dense grounding backbone used)",
            "brief_description": "A dense grounding VLM that integrates a strong visual-language model (InternVL) with a SAM-derived segmentation pipeline to produce dense region/mask-grounded outputs; used as the base for Gondola fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Sa2VA (base VLM)",
            "model_description": "Sa2VA marries a pretrained VLM (InternVL) with SAM2 to enable dense image/video grounding by providing segmentation tokens/prompt embeddings produced by the VLM to SAM2 for mask decoding; Gondola builds upon and fine-tunes Sa2VA for multi-view robotic grounding/planning.",
            "visual_encoder_type": "InternVL ViT-based visual encoder (as in Sa2VA) integrated in the VLM pipeline",
            "visual_encoder_pretraining": "Sa2VA reported training on large-scale image and video grounding datasets (exact dataset names/sizes not specified in this paper beyond 'large-scale image and video grounding data').",
            "grounding_mechanism": "Decoupled VLM-to-SAM prompting: VLM produces a specialized segmentation token embedding that is projected to a SAM prompt embedding; SAM decodes pixel-level masks — effectively a learned token-&gt;segmentation pipeline.",
            "representation_level": "pixel-level segmentation masks (dense grounding)",
            "spatial_representation": "per-view 2D masks, can be used to form higher-level 3D representations when fused with depth",
            "embodied_task_type": "general vision-language grounding (in this paper used as perception backbone for robotic manipulation planning)",
            "embodied_task_name": "Used as base for Gondola which is evaluated on GemBench / RLBench",
            "visual_domain": "Web-scale and large-image/video grounding datasets (as per Sa2VA original training); Gondola fine-tunes it for RLBench simulation and small real datasets.",
            "performance_metric": "Dense grounding accuracy / mask IoU in VLM benchmarks (as reported in original Sa2VA work; in this paper used as backbone so per-task metrics are for Gondola)",
            "performance_value": "",
            "has_grounding_ablation": false,
            "performance_without_grounding": "",
            "grounding_improvement": "",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "",
            "failure_mode_analysis": "",
            "domain_shift_handling": "Sa2VA is fine-tuned by Gondola on task-specific multi-view and referring datasets to adapt from general image/video grounding to robotic viewpoints.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "In Gondola, Sa2VA's VLM backbone (InternVL encoder and LLM base) are kept frozen while LoRA and SAM2 mask decoder are fine-tuned to adapt to multi-view robotic data.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "VLM embedding token -&gt; projected prompt -&gt; SAM segmentation decoding (decoupled but learned prompting).",
            "sample_efficiency": "Leveraging Sa2VA pretraining reduces required robot-specific labeled data; Gondola fine-tunes on ~15k planning tuples + referring data to adapt.",
            "key_findings_grounding": "Starting from a dense grounding VLM (Sa2VA) and fine-tuning for multi-view robotic grounding enables precise mask outputs and reduces hallucination vs direct numeric generation approaches; Sa2VA-style token-&gt;SAM prompting is effective for multi-view mask generation when adapted to robotic viewpoints.",
            "uuid": "e1958.2"
        },
        {
            "name_short": "SAM2",
            "name_full": "SAM2 (Segment Anything - version 2 segmentation model)",
            "brief_description": "A segmentation decoder used to convert prompt embeddings into per-view binary masks; in Gondola SAM2 is used as the mask generator conditioned on a projected LLM &lt;seg&gt; embedding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SAM2",
            "model_description": "SAM2 receives a learned prompt embedding (projected from the LLM &lt;seg&gt; hidden state) per view and decodes binary segmentation masks for that view. Gondola fine-tunes the SAM2 mask decoder while keeping SAM2 encoder frozen to better match robot-view appearance.",
            "visual_encoder_type": "SAM2 segmentation model (encoder frozen in Gondola, mask decoder fine-tuned)",
            "visual_encoder_pretraining": "SAM family models are pre-trained on huge segmentation datasets (Segment Anything data corpus) — referenced as large-scale segmentation pretraining in the paper but exact dataset names/sizes are not restated here.",
            "grounding_mechanism": "Prompted segmentation: LLM-projected prompt embeddings are used as SAM2 prompts to produce masks that ground textual object references to image pixels.",
            "representation_level": "pixel-level segmentation masks per view",
            "spatial_representation": "2D masks per camera view; merged with depth to explicit 3D point cloud",
            "embodied_task_type": "object manipulation grounding (used within Gondola for per-view mask generation)",
            "embodied_task_name": "GemBench / RLBench integration via Gondola",
            "visual_domain": "pretrained on broad segmentation datasets; applied to RLBench synthetic images and a small real-robot dataset",
            "performance_metric": "mask IoU (used in grounded planning evaluation)",
            "performance_value": "When coupled with Gondola's &lt;seg&gt; prompt embeddings, SAM2 produces per-view masks yielding mask IoUs reported in Gondola's ablations (e.g., mask-based grounding: L1 Grd ≈ 87.8% vs box-based 62.8%).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "",
            "grounding_improvement": "SAM2 prompt-based masking (with learned prompt from LLM) outperforms generating numeric boxes then converting to masks; fine-tuning SAM2 decoder further improves mask IoU for robotic views.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "SAM2 performance degrades under low multi-view consistency and domain shift (real-robot), causing segmentation errors that reduce downstream SR; fine-tuning the mask decoder partially mitigates this but data scarcity remains an issue.",
            "failure_mode_analysis": "Segmentation errors in the real world (inconsistent multi-view appearance) are a frequent cause of failure; per-paper qualitative examples show lower mask reliability on real robot.",
            "domain_shift_handling": "Fine-tune SAM2 mask decoder on small real-robot dataset in conjunction with RLBench to improve transfer; still substantial sim-to-real gap reported.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Gondola keeps SAM2 encoder frozen but fine-tunes the SAM2 mask decoder; no direct comparison versus fully frozen SAM2 provided.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "LLM-projected prompt embedding -&gt; SAM2 decoder (prompted segmentation).",
            "sample_efficiency": "Fine-tuning the SAM2 decoder with several thousand examples (from constructed datasets) was sufficient to improve mask IoU for RLBench views; exact sample-efficiency numbers tied to Gondola's training datasets (~15k planning tuples + referring) reported in paper.",
            "key_findings_grounding": "Prompting SAM2 from an LLM segmentation token is an effective decoupled grounding mechanism; fine-tuning SAM2's decoder on domain data improves per-view mask quality for robotic viewpoints, but multi-view consistency and sim-to-real domain shift remain limiting factors.",
            "uuid": "e1958.3"
        },
        {
            "name_short": "3D-LOTUS++",
            "name_full": "3D-LOTUS++ (LLM-based 3D policy baseline)",
            "brief_description": "A prior LLM-guided 3D policy for robotic manipulation that uses in-context learning and extensive engineering to enable LLM planning without direct visual input; used as a state-of-the-art baseline for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "3D-LOTUS++",
            "model_description": "An LLM-based planning approach (from prior work) employing in-context learning and engineering to generate plans for a 3D motion policy. In this paper it is used for baseline comparison and its motion planning policy is reused for some experiments.",
            "visual_encoder_type": "Not directly used for visual grounding in its original variant (relies less on direct visual input and more on engineering/in-context prompting); in this paper the 3D-LOTUS++ 3D motion policy is reused for execution.",
            "visual_encoder_pretraining": "",
            "grounding_mechanism": "Prior method relies on LLM planning with limited direct visual grounding (in-context learning and engineered affordances) rather than end-to-end mask-based VLM grounding.",
            "representation_level": "3D point-affordance/voxel/skill-based representations for motion policy (original paper); not mask-centred grounding like Gondola.",
            "spatial_representation": "3D representations for motion planning; in this paper the 3D motion policy and point categorization approach from 3D-LOTUS family are reused.",
            "embodied_task_type": "robotic manipulation evaluated on GemBench",
            "embodied_task_name": "GemBench (RLBench)",
            "visual_domain": "simulation",
            "performance_metric": "Task execution Success Rate (SR)",
            "performance_value": "Reported baseline SRs inferred from comparisons: approximate 3D-LOTUS++ SRs (per text differences): L2 ≈ 64.5% SR, L3 ≈ 41.5% SR, L4 ≈ 17.4% SR (derived from Gondola - reported deltas: Gondola L2 74.8% is +10.3 p.p. over 3D-LOTUS++; Gondola L3 52.4% is +10.9 p.p.; Gondola L4 19.0% is +1.6 p.p.).",
            "has_grounding_ablation": false,
            "performance_without_grounding": "",
            "grounding_improvement": "Gondola (mask-based multi-view grounding) improves over 3D-LOTUS++ by +10.3 p.p. SR in L2, +10.9 p.p. in L3, +1.6 p.p. in L4, indicating strong benefits from explicit multi-view mask grounding for novel-object generalization.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "3D-LOTUS++ relies on extensive engineering and in-context learning without direct visual grounding, making it less reliable when visual grounding is needed for novel objects/environments; Gondola's visual grounding addresses these deficits.",
            "failure_mode_analysis": "Prior LLM-only or weakly-grounded planners (like 3D-LOTUS++ variants) tend to underperform on novel-object/articulated-object generalization and long-horizon tasks compared to grounded, mask-based multi-view approaches.",
            "domain_shift_handling": "3D-LOTUS++ uses in-context learning and engineered affordances to partially handle distribution shifts; Gondola's fine-tuning on simulated multi-view grounding datasets is a different strategy that achieves higher SR on unseen objects.",
            "novel_object_performance": "Lower than Gondola by ~10 p.p. in L2/L3 SR as reported above.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Not mask-token + SAM-style; rather in-context LLM planning possibly combined with 3D policy signals (prior work).",
            "sample_efficiency": "Relies on engineering/in-context examples rather than fine-tuning on grounded multi-view datasets; Gondola's data-driven grounding approach requires specific fine-tuning but yields better generalization on L2/L3.",
            "key_findings_grounding": "Comparison shows that explicit, multi-view, mask-based vision-language grounding (Gondola) substantially improves generalization to novel rigid and articulated objects over prior LLM-driven planners that lack direct mask-level visual grounding.",
            "uuid": "e1958.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos",
            "rating": 2
        },
        {
            "paper_title": "3D-LOTUS++",
            "rating": 2
        },
        {
            "paper_title": "Segment anything",
            "rating": 2
        },
        {
            "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
            "rating": 2
        },
        {
            "paper_title": "RLBench: The robot learning benchmark & learning environment",
            "rating": 2
        },
        {
            "paper_title": "GemBench",
            "rating": 2
        },
        {
            "paper_title": "SayCan",
            "rating": 1
        },
        {
            "paper_title": "VoxPoser: Composable 3d value maps for robotic manipulation with language models",
            "rating": 1
        }
    ],
    "cost": 0.023211,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Grounded Vision Language Planning for Generalizable Robotic Manipulation
12 Jun 2025</p>
<p>Shizhe Chen 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Ricardo Garcia 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Paul Pacaud 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Cordelia Schmid 
Inria
École normale supérieure
CNRS
PSL Research University</p>
<p>Grounded Vision Language Planning for Generalizable Robotic Manipulation
12 Jun 202523810B8B9B0DD82F3887E3A25256AA75arXiv:2506.11261v1[cs.RO]Robotic ManipulationTask PlanningVision-Language Model
Robotic manipulation faces a significant challenge in generalizing across unseen objects, environments and tasks specified by diverse language instructions.To improve generalization capabilities, recent research has incorporated large language models (LLMs) for planning and action execution.While promising, these methods often fall short in generating grounded plans in visual environments.Although efforts have been made to perform visual instructional tuning on LLMs for robotic manipulation, existing methods are typically constrained by single-view image input and struggle with precise object grounding.In this work, we introduce Gondola, a novel grounded vision-language planning model based on LLMs for generalizable robotic manipulation.Gondola takes multi-view images and history plans to produce the next action plan with interleaved texts and segmentation masks of target objects and locations.To support the training of Gondola, we construct three types of datasets using the RLBench simulator, namely robot grounded planning, multi-view referring expression and pseudo long-horizon task datasets.Gondola outperforms the state-of-the-art LLM-based method across all four generalization levels of the GemBench dataset, including novel placements, rigid objects, articulated objects and long-horizon tasks.</p>
<p>Introduction</p>
<p>Training robots to perform physical manipulation tasks following human instructions has been a long-term goal in robotics, enabling intuitive human-robot interaction in unstructured, dynamic environments such as homes and factories.Recently, end-to-end learning-based models [1,2,3], particularly Vision-Language-Action models (VLAs) [4,5,6,7,8] trained on real-world robot data, have achieved remarkable success in robotic manipulation.However, due to the scarcity and limited diversity of available robot datasets [9,10,11], these models still struggle to generalize beyond their training conditions, facing difficulties with novel objects, unfamiliar environments, and especially unseen long-horizon tasks [12,13,14].</p>
<p>To improve generalization, modular frameworks [15,16,14,17] have received increasing attention, separating high-level task planning from low-level action execution.As the planning component is less coupled to robot embodiments, it can leverage a broader range of internet-scale data for training to enhance its generalization capabilities.Inspired by the impressive zero-and few-shot reasoning and planning abilities of Large Language Models (LLMs) [18,19], researchers have begun exploring LLMs for task planning, such as decomposing a language instruction into substeps [14,20] or generating executable code [15,16].However, since LLMs lack direct grounding in the physical world, their ability to produce actionable and reliable plans remains limited.</p>
<p>Different methods have been proposed to ground LLM-generated plans in visual context.SayCan [21] trains an affordance score predictor based on visual input and candidate skills, allowing the system to rerank substeps proposed by the LLM.However, this is limited to evaluating a predefined set (a) Existing methods [24,23,17] using single-view input and producing various intermediate representations.</p>
<p>…</p>
<p>Gondola</p>
<p>Grounded Plans with Segmentation Masks</p>
<p>Screw in the rose light bulb.</p>
<p>(b) Our Gondola model with multi-view inputs generating grounded plans with segmentation masks.</p>
<p>Figure 1: Comparison of vision-language models for high-level planning in robotic manipulation.Multi-view inputs alleviate occlusions for improved 3D scene perception, while segmentation masks offer more precise and compact grounded plans.</p>
<p>of skills.ECoT [22] uses image captioning models to convert visual scenes into text descriptions, which are then fed to the LLM.Yet, captions can be less accurate and miss crucial details, leading to sub-optimal decision-making and raising the risk of error propagation.</p>
<p>More recently, a few works [23,24,17] have explored fine-tuning Vision-Language Models (VLMs) to generate visually grounded plans, using intermediate representations such as points [24], bounding boxes [23], or latent hidden states [17] as illustrated in Figure 1a.While promising, points and bounding boxes are often too coarse for precise robotic manipulation in 3D environments.Latent representations from VLMs, on the other hand, are difficult to interpret and may lack the conciseness needed for efficient execution.Furthermore, most existing methods rely on single-view images, which exacerbates planning challenges due to occlusions and limited fields of view.</p>
<p>To address these limitations, we propose Gondola -a grounded vision-language planning model to enhance generalization in robotic manipulation.As illustrated in Figure 1b, Gondola transforms language instructions and multi-view images into precisely grounded plans, consisting of interleaved actions and objects with accompanying segmentation masks for each referred object.The model builds upon a dense grounding VLM Sa2VA [25], leveraging a specialized segmentation token that enables object-specific mask generation across views.To improve planning consistency, we incorporate the textual history of previously generated plans as additional context to the model.For effectively training Gondola, we construct a robot grounded planning dataset using simulated environments from RLBench [26], supplemented with multi-view referring expression data to strengthen object grounding.To better handle long-horizon tasks, we create extended tasks by concatenating two short task sequences and using an LLM to generate instructions.We evaluate Gondola on both offline grounded planning and online task execution using the GemBench generalizable robotic manipulation benchmark [14].Comprehensive results demonstrate Gondola's superior performance, benefiting from multi-view inputs, history-aware planning, segmentation masks and diverse training data.It outperforms the state-of-the-art LLM-based method 3D-LOTUS++ [14] by absolute 10% on average.</p>
<p>To summarize, our contributions are three-fold:</p>
<p>• We propose Gondola to generate grounded vision-language plans with masks for generalizable robotic manipulation.It features multi-view image understanding and grounding.• We construct multi-view grounding and planning datasets using RLBench, and propose pseudo long-horizon task generation to improve long-term planning capabilities.• Our model sets a new state of the art on the generalization benchmark GemBench, and works reliably on a real robot.The code, models and datasets will be publicly released.</p>
<p>Related Work</p>
<p>Vision-and-language robotic manipulation.Learning robotic manipulation conditioned on vision and language has garnered significant interest [27,28,29].Due to the high dimensionality of the manipulation action space, directly applying reinforcement learning (RL) for training presents challenges [30].Therefore, most approaches employ imitation learning (IL) [31,4,32,33,34,35,36,37,38] using scripted trajectories [26] or tele-operation data [9].Visual representation plays a crucial role in policy learning.Existing works [31,4,2,32,34,39,40] rely on 2D images for action prediction, although recent work has begun to explore 3D visual representations [41,33,35,37,38,36,14,42]. Hiveformer [32] and RVT [34] utilize 2D images to predict a heatmap in 2D space, which is then combined with 3D point clouds to demermine the final 3D position.C2F-ARM [41] and PerAct [33] directly use 3D voxel representation as input, being less efficient due to encoding empty voxels.PolarNet [35] and 3D-LOTUS [14] improve efficiency by encoding only visible point clouds to predict actions, while SUGAR [36] further enhances point cloud representation through 3D pretraining.Given the superiority of current pre-trained 2D representations [43], works like Act3D [37] and 3D Diffuser Actor [38] lift pre-trained 2D features into 3D space, and then train 3D models to leverage the strengths of both.In this work, we leverage the strong generalization capabilities of pretrained 2D vision-language models (VLMs) for high-level task planning and integrate it with 3D-based motion planning policies for task execution.</p>
<p>Foundation models for robotics.Learning-based robotic policies struggle to generalize to novel scenarios [44].Inspired by generalization abilities of foundation models [43,45,19], recent research investigates ways to leverage these models for perception, reasoning and planning in robotics.Some methods [20,14] directly use LLMs to decompose high-level tasks into sub-steps.To better ground plans in visual world, SayCan [21] combines LLMs with value functions of pretrained skills given visual contexts.ViLa [46] replaces LLMs with a multimodal LLM GPT-4V [47].CaP [15] directs LLMs to generate code that invokes tools for visual perception and control, and VoxPoser [16] uses LLMs and VLMs to create 3D voxel maps indicating affordances, constraints, rotations, and velocities.These approaches rely on general-purpose pretrained models for task planning, but tend to be unstable in robotic settings and require heavy prompt engineering.To address this, a few recent methods fine-tune VLMs on robot datasets to generate grounded plans using intermediate representations such as points [24], bounding boxes [23], and latent vision-language embeddings [17].</p>
<p>In our work, we extend the VLM framework with multi-view inputs and finer grounding masks, and introduce synthetic robot data for long-horizon grounded planning.</p>
<p>Vision and language models for grounding.Early VLMs [48,49] are constrained to generating text outputs from multimodal inputs, such as image captioning and visual question answering.To enable VLMs to produce grounded outputs that align generated texts with specific image regions, existing methods can be broadly categorized into three types.The first category outputs box coordinates [50,51,52,53,54] or polygons of segmentation masks [55] as text.However, generating precise numerical outputs -especially when multiple grounding results are required -is difficult and prone to hallucination.The second category uses a proposal-based approach, where a separate module first generates candidate regions, and the VLM selects the one for each generated text [56,57].While more structured, this method is sensitive to proposal quality, suffers from error accumulation, and introduces more computation overhead.The third category decouples language and grounding by feeding the output of a VLM into a grounding model to produce boxes or masks [58,59,60,25].Among them, Sa2VA [25] achieves the state-of-the-art performance by integrating a strong VLM model InternVL [61] and a segmentation model SAM2 [62], as well as training on large-scale image and video grounding data.Our Gondola model fine-tunes Sa2VA on multi-view image grounding and planning datasets for robotic manipulation.</p>
<p>3 Gondola: Grounded Vision-Language Plan Generation</p>
<p>We formulate high-level task planning for robotic manipulation as a vision-language grounding problem, where the goal is to generate the next executable, visually-grounded action plan that accomplishes a natural language instruction in the observed environment.Formally, given a language instruction L and multi-view visual observations I = {I 1 , • • • , I K } from K cameras, the Gondola model produces a grounded vision-language plan P = (a, o, M o , l, M l ), where a represents the action name, o specifies the manipulated object description paired with corresponding segmentation masks
M o = {m 1 o , • • • , m K o }
across all views, and l denotes the target location description with associated location masks M l .Noting that either o or l may be empty if the particular action does not require an object or target location for execution.</p>
<p>Model Architecture</p>
<p>As illustrated in Figure 2, the Gondola architecture consists of three main components: an image encoder for tokenizing each view image, an LLM to process multimodal inputs and outputs, and a segmentation model for multi-view object grounding.Image encoder.We use a pretrained vision transformer (ViT) InternVL-300M [61] with an input image resolution of 448 × 448 to generate image patch embeddings, followed by a 2-layer multi-layer perceptron (MLP).The ViT is frozen, while the MLP is trained to adapt the visual features to the language space.The same image encoder is applied across all views, with view separation handled by a special token \n.Image tokens from all views are concatenated to form a single sequence.LLM.We adopt InternVL-4B [61] as our language model, keeping its base parameters frozen while adding LoRA [63] layers for fine-tuning.Building upon Sa2VA [25], we incorporate a specialized vocabulary that includes a dedicated <seg> token to signal mask generation, along with paired delimiter tokens <p> and </p> that precisely delineate object and location references requiring spatial grounding.To maintain contextual awareness across sequential steps in completing manipulation tasks, we further encode previously generated history plans H as compact text tokens to the model.The following example demonstrates input and output token formatting for the LLM:</p>
<p>User: <image>\n<image>\n<image>\n<image>\n You are a skilled assistant for robot task planning in tabletop environments.You can perform the following actions: grasp, move grasped object, rotate grasped object, push down, push forward, and release.Task: screw the light bulb from the rose holder into the lamp.You have completed the following action plans: grasp the rose light bulb.Please generate the next action plan.Gondola: Move the grasped object to <p> lamp </p><seg>.</p>
<p>Here, <image> represents placeholders for image tokens for each view, which are replaced by the actual visual embeddings.Segmentation model.We employ SAM2 [62] as the segmentation model.Given the hidden embedding h seg from the LLM that predicts the <seg> token, we project h seg with a 2-layer MLP to generate a prompt embedding.SAM2 uses this prompt to segment the corresponding object mask for each view image separately and thus generates K binary masks for each referred object or location.</p>
<p>Training Data</p>
<p>We construct three datasets to train Gondola using 31 task variations in GemBench training split [14] within the RLBench simulator [26], including robot grounded planning, multi-view referring expression, and pseudo long-horizon tasks.While this data construction approach can be extended to any task in RLBench, we restrict dataset construction to the GemBench training split for fair comparison with prior work [14] in evaluating generalization performance.Figure 3 illustrates examples from each of the three datasets.</p>
<p>Robot grounded planning</p>
<p>Multi-view referring expression</p>
<p>Please segment one of the violet jar  Robot grounded planning.In the RLBench simulator, each task is structured with semantically labeled objects and fixed procedure trajectories, enabling efficient grounded plan generation.First, we manually decompose the trajectory in each task into a sequence of plans, each step consisting of an action, object and placement location triplet.This only requires a single annotation effort per task with minimal annotation overhead.The corresponding segmentation masks for objects and locations are then automatically extracted given the annotated semantic labels.In this way, we create ground-truth plan {a t , o t , M t o , l t , M t l } for multi-view images I t at each keystep t1 in an episode per task variation.We use 100 episodes for each GemBench training task variation, where each episode contains randomized object placements (and optionally new distractor objects), resulting in approximately 15k training tuples for robot grounded planning.Multi-view referring expression.To strengthen Gondola's multi-view object grounding capabilities, we further create a multi-view referring expression dataset based on RLBench.Recognizing that the default semantic labels in RLBench contain noises and ambiguities, we implement an automatic preprocessing pipeline to standardize and refine object names in RLBench.More detail is presented in Appendix A. Similar to grounded planning generation, we automatically extract all object instances and their corresponding segmentation masks for each keystep in GemBench training split, yielding 15k multi-view image examples and 58k referring expressions.For each training example, we formulate the referring query as "Please segment one of the [object name]" with the expected output being the corresponding segmentation masks across all input view images.Pseudo long-horizon tasks.To enhance planning for long-horizon tasks, we propose to automatically generate pseudo long-horizon sequences.Specifically, we randomly concatenate pairs of different training task sequences from GemBench training split to create compositional tasks.We then use an LLM to create coherent joint instructions for the combined tasks.Despite the abrupt scene transitions between tasks, these pseudo long-horizon tasks still help the model learn to leverage history plans to track task progress, and predict the next step based on long-horizon context.</p>
<p>Training Objectives</p>
<p>Gondola is trained to jointly optimize plan generation and multi-view object grounding.For plan generation, we employ the cross-entropy loss for next token prediction:
L plan = − log p(y i |y &lt;i , I, L, H),(1)
where y i represents tokens in the generated plan including the special tokens.For multi-view object grounding, we adopt a joint loss of binary mask prediction and dice loss L grd = L bce + L dice :
L bce = − [M gt (p) • log(M pred (p)) + (1 − M gt (p)) • log(1 − M pred (p))],(2)L dice = 1 − 2 p M pred (p) • M gt (p) p M pred (p) + p M gt (p) + ϵ ,(3)
where p indexes over all pixels in the mask, M pred (p) is the predicted probability, M gt (p) is the ground-truth binary label, and ϵ is a small constant for numerical stability.</p>
<p>Experiments</p>
<p>Evaluation Datasets and Metrics</p>
<p>We evaluate Gondola on the GemBench benchmark [14] for robotic manipulation in RLBench [26] simulator.GemBench assesses models' generalization capabilities across four levels: Level 1 (L1) with new locations, Level 2 (L2) with novel rigid objects, Level 3 (L3) with new articulated objects, and Level 4 (L4) with unseen long-horizon tasks.To ensure a fair evaluation on generalization, tasks from L2 to L4 are excluded during training.The benchmark includes 31 task variations in L1, 28 in L2, 21 in L3 and 12 in L4.We conduct the following two types of evaluation:</p>
<p>• Grounded planning evaluation.This setup purely assesses models' grounded planning performance given instruction, multi-view images and ground-truth history plans.We construct a grounded planning evaluation set given the GemBench validation split.It contains 20 episodes per task variation in GemBench.For each keystep in an episode, we provide ground-truth annotations for the next action, object names and segmentation masks across all views.To evaluate the grounded planning performance, we measure the accuracy of action and object name predictions through exact text matches for each keystep.For grounding evaluation, we calculate the intersection over union (IoU) between predicted and ground-truth masks for each view.The averaged performances of each metric on all keysteps are reported for each generalization level of GemBench.</p>
<p>• Task completion evaluation.This setup integrates Gondola with low-level motion planning policies to execute the generated plans.We adopt the standard camera configuration in GemBench, using K = 4 cameras positioned at the front, left shoulder, right shoulder and wrist, each with an image resolution of 256 × 256.For evaluation, we use the GemBench test split across all four levels, conducting 20 episodes per task variation for 5 times, resulting in 20 × 5 × (31 + 28 + 21 + 12) evaluation episodes in total.Each episode is limited to a maximum of 25 steps.Task performance is measured by success rate (SR), where SR is 1 for a successful episode and 0 for failure.We report mean SR and standard deviations across the 5 runs.</p>
<p>Implementation Details</p>
<p>Training Gondola.We train the Gondola model using 8 NVIDIA H100 GPUs with training scripts built on the DeepSpeed engine [64].The image encoder and SAM2 encoder are kept frozen during training.We apply LoRA [63] with rank 128 for parameter-efficient fine-tuning of the LLM, and the SAM2 mask decoder is also fine-tuned.The model is optimized with AdamW, using a learning rate of 2 × 10 −5 for all trainable parameters.The batch size per device is set to 4, resulting in an effective batch size of 32.It takes 3 hours for training 10k iterations over the three constructed datasets.</p>
<p>Integrating Gondola with low-level policies.For fair comparison with prior work, we employ the same motion planning policy released by 3D-LOTUS++ [14].Unlike 3D-LOTUS++ [14] which performs task planning only once and then executes the plan, our approach runs the task and motion planning models iteratively in a feedback loop, enabling continuous re-planning and corrections as needed.Specifically, at each step, Gondola takes multi-view RGB images, instruction and previously executed history plans as input to produce the next plan, including the next action, the manipulated object, and/or the target location together with grounded masks on each view.Then we combine aligned depth images with these masks and unify the segmented objects across views into a consolidated 3D point cloud.Following [14], each point is assigned with one of four categories based on the segmentation results and robot proprioceptive information, namely target object, target location, robot, and obstacle.The predicted action name and the point cloud are fed into the 3D motion planning policy in [14] to generate a sequence of actions.We can either run the entire action sequence as in action chunking [3] or execute one action at a time before re-planning with Gondola.We compare the two strategies in Table 3.  1a.We use the same image encoder and LLM as Gondola (row 4) for fair comparison.To measure the mask IoU, the predicted boxes are fed into the SAM2 model to produce segmentation masks.We observe that the box-based model frequently suffers from format errors, as generating multiple numeric values for multiple images can be challenging.It performs worse across all levels in both action and object name accuracy, and shows significantly lower grounding quality in terms of mask IoU.These results highlight the advantages of end-to-end mask generation within VLMs, which provides more accurate and reliable grounding for robotic planning.</p>
<p>Multi-view inputs.The comparison between the 2nd and 3rd rows in Table 1 showcases the impact of multi-view image inputs for robot task planning.In the 2nd row, only the front-view image is provided to the model, whereas in the 3rd row, all four views are used.Multi-view images help mitigate occlusions and generally improve action, object and grounding prediction across levels, with only slight worse performance on a few metrics in L3 compared to the single-view setting.History plans.The last two rows in Table 1 compares the effect of incorporating history plans into task planning.Including history information boosts the performance on L2 and L3 by enabling more coherent and context-aware planning decisions.However, on L4, we observe a significant performance drop compared to the model without history.An in-depth analysis reveals that this decline is due to a distribution shift in history plans.As a result, the model tends to leverage its prior knowledge for generating purely textual plans rather than grounded plans.In contrast, the model without history does not suffer from this distribution shift.This issue can be addressed by training on our constructed pseudo long-horizon data, as shown in Table 2.</p>
<p>Fine-tuning datasets.Table 2 evaluates the contribution of each fine-tuning dataset.The multi-view referring expression dataset proves most effective in improving segmentation quality, leading to consistently better grounding performance across all four levels.The pseudo long-horizon task dataset is particularly beneficial for L4, as it mitigates the history plan shift issue and encourages the model to reason over extended plan histories when predicting subsequent actions.Action chunking.As shown in Table 3, when the action chunk size for running the motion planning policy is set to 1, Gondola replans at every step; when set to 5, it replans only after the motion planning policy completes the previous subplan.We observe that the impact of action chunk size varies depending on tasks.Detailed results and analysis are provided in Appendix B. In general, for tasks requiring fine-grained manipulation, frequent replanning (i.e., smaller action chunks) yields better performance.In contrast, for long-horizon tasks that benefit from consistent, highlevel planning, using a larger action chunk is more effective since the current Gondola model does not encode fine-grained history within subplans, which can limit coherent decision-making at this level.3D postprocessing.We ablate a postprocessing step that applies 3D point cloud filtering using the DBSCAN algorithm to remove outlier points in grounded masks.Results show that this step offers minimal improvement, indicating that Gondola already produces spatially coherent object masks.</p>
<p>Comparison with state of the art</p>
<p>Table 4 presents a comparison of our Gondola model with state-of-the-art methods on the GemBench test split.Gondola is combined with the same motion policy in 3D-LOTUS++ [14] with action chunking size of 5 and no 3D postprocessing.The methods in the upper section do not use LLMs for planning but rely on end-to-end policy training to predict actions directly.While these methods perform well on seen tasks in L1, they show limited generalization to unseen objects in L2 and L3 and struggle with unseen long-horizon tasks in L4.Models employing LLM-based planning demonstrate improved generalization from L2 to L4, despite reduced performance on familiar tasks in L1.In particular, compared to 3D-LOTUS++ [14], which uses extensive engineering and in-context learning to enable LLM-based planning without visual input, our Gondola model offers a straightforward approach to directly generate grounded plans for follow-up motion planning.Gondola outperforms 3D-LOTUS++ [14] by 10.3% on novel rigid objects in L2, 10.9% on novel articulated objects in L3 and 1.6% in L4 of long-horizon tasks.Detailed results per task and qualitative examples are included in Appendix B. We further deploy Gondola in a real robot with results detailed in Appendix C.</p>
<p>Conclusion</p>
<p>This paper presents Gondola, a grounded vision-language planning model to improve generalization in robotic manipulation.Gondola features with multi-view perception and the incorporation of planning history to generate fine-grained segmentation masks in the action plan.We construct three simulated datasets based on RLBench for model training, including robot grounded planning, multi-view referring expressions and pseudo long-horizon tasks datasets.Gondola demonstrates superior performance in both standalone planning and full execution on the GemBench benchmark, achieving stronger generalization abilities on novel rigid and articulated objects and long-horizon tasks.Our experiments highlight the importance of multi-view grounding, temporal reasoning, and end-to-end mask generation for effective robotic planning.</p>
<p>Limitations</p>
<p>First, data scarcity remains a major bottleneck for Gondola.Our current dataset is limited to shorthorizon tasks in controlled tabletop environments from the GemBench training split, which restricts generalization to unseen objects and more complex long-horizon tasks.Expanding the dataset with more diverse simulated and real-world data is essential.Second, visual history encoding can be improved.Multi-view inputs introduce many image tokens, making it difficult to include detailed historical context.A more efficient memory mechanism could support richer history representation without overwhelming the model.Lastly, our approach relies solely on imitation learning from successful episodes, making it challenging for the model to anticipate and correct errors.Introducing examples of failure and recovery could better equip Gondola for robust, real-world applications.</p>
<p>A Data Construction in RLBench</p>
<p>We detail the label construction from RLBench in Section 3.2.RLBench contains scripted trajectories for each task, and every object in the scene already has a name and an associated label id.We use regular expressions to filter out undesired objects and fix object names by removing undesired name parts, prefix numbers or words such as distractor or success.For the set of objects whose color changes from one task variation to the other, we prepend the color name whose RGB value is closest to the RGB value of the object color.We consider 20 different colors that appear in RLBench: red, maroon, lime, green, blue, navy, yellow, cyan, magenta, silver, gray, orange, olive, purple, teal, azure, violet, rose, black and white.We use 3100 episodes from the 31 GemBench training task variations.</p>
<p>For each keystep, we collect RGB images per camera viewpoint, the extracted list of object names in the scene, and object masks across views.</p>
<p>B Detailed Results on GemBench</p>
<p>Table 5 to 8 present the per-task results on four levels of GemBench benchmark, respectively.We observe that the impact of action chunk size varies depending on tasks.Frequent replanning (i.e., smaller action chunks) yields better performance for tasks requiring fine-grained and reactive actions such as 'SlideBlock' and 'PutInCupboard'.For example, in the 'PutInCupboard' task as shown in Figure 4, Step 2 involves moving the grasped object to the cupboard.Although the predictions of Gondola are correct, the partial observation of the cupboard makes it difficult to precisely localize its position for the motion planner, causing the object to fall outside the intended area.This issue could be mitigated by using smaller action chunks, which would allow the model to gradually get closer and acquire better views of the cupboard.While Gondola has replanning capabilities as shown in Step 5, the motion planner fails again due to the new object pose.Figure 5 shows a failure example for the 'SlideBlock' task.It is challenging for the motion planner to predict long-horizon action trajectories for this contact-rich task, though the initial plan generated from Gondola is correct.</p>
<p>In contrast, for long-horizon tasks in Level 4 that benefit from consistent, high-level planning, using a larger action chunk is more effective since the current Gondola model does not encode fine-grained history within subplans, which can limit coherent decision-making at this level.</p>
<p>Predicted plan grasp<p>crackers box</p> [SEG] Step 0      As illustrated in Figure 6, our real robot setup includes three RealSense d435 cameras attached to a table and a 6-DoF UR5 robotic arm equipped with an RG6 gripper.We collect 20 × 7 demonstrations via teleoperation for 7 variations across 5 tasks: stack cup (yellow in pink or navy in yellow), put fruit (strawberry or peach) in box, open drawer, put item in drawer and hang mug.Then, we fine-tune Gondola and the 3D-LOTUS [14] motion planning policy on a joint dataset of RLBench and the real robot demonstrations.We evaluate the fine-tuned models on the same 7 seen task variations with different objects placements and evaluate generalization capabilities on 7 new unseen task variations: put fruit (lemon and banana) in box, put food (tuna can then corn) in box and put food in plates (croissant in the yellow plate and grapes in the pink plate).For each task variation, we run models 10 times and report the success rate.</p>
<p>C.2 Real Robot Results</p>
<p>Table 9 and 10 show the performance on seen and unseen task variations, respectively.The final performance depends on both Gondola and the motion planning policy.Figure 7 illustrates a successful prediction by Gondola on a previously unseen task.However, we observe that Gondola performs worse in the real-world setting compared to simulation, primarily due to the limited amount of real robot data.As illustrated in Figure 8, the multiview consistency is significantly lower in the real world, leading to frequent segmentation errors.Increasing the availability of real-world multi-view images could help address this limitation, and we leave this direction for future work.The video in the supplementary material showcases more executions on the real robot.</p>
<p>Figure 2 :
2
Figure 2: Left: Gondola model architecture, consisting of a shared visual encoder for multi-view images, an LLM to generate action and object names along with segmentation tokens, and SAM2 to decode masks.Right: Integrating Gondola with a motion planning policy for task execution.</p>
<p>the maroon button first, then screw in the navy light bulb.You have completed the following actions: push down maroon button, grasp the navy light bulb.Ground-truth: move grasped object to <p>lamp</p><seg>Pseudo long-horizon taskTask 1 Task 2 Tasks: close the violet jar.Your are in the first step.Ground-truth: grasp <p> gray lid </p> <seg> View 1 View K Input Mask Ground-truth: <p> violet jar </p> <seg></p>
<p>Figure 3 :
3
Figure 3: Three types of datasets are constructed for model training: (1) robot grounded planning, (2) multi-view referring expressions for improved object grounding, and (3) pseudo long-horizon tasks for enhanced long-horizon planning.</p>
<p>Multi-view observationPredicted segmentation masks Point cloud and predicted actions black: obstacle green: robot blue: target object red: target location maroon dots: actions Predicted plan move grasped object to <p>cupboard</p>[SEG]</p>
<p>Figure 4 :
4
Figure 4: A failure example of the 'PutInCupboard' task using Gondola (AC=5).The instruction is 'put the crackers box in the cupboard'.The predictions of Gondola are correct, but the motion planner fails in Step 2 due to limited visual information of the cupboard from partial observations.Gondola replans in Step 5, but the motion planner fails due to the new object pose.</p>
<p>Figure 5 :
5
Figure5: A failure example of the 'SlideBlock' task using Gondola (AC=5).The instruction is 'slide the block towards the blue plane'.The prediction of Gondola at Step 0 is correct, but it is challenging for the motion planner to predict long-term actions for this contact-rich task.At Step 5, due to the wrong history plan fed into Gondola, Gondola fails to replan correctly.</p>
<p>Figure 6 :
6
Figure 6: Our setup includes three Re-alSense D435 cameras and a UR5 robotics arm equipped with a RG6 gripper.</p>
<ol>
<li>Grasp rose bulb [mask 1 ] … [mask K ] 2. Move grasped object to lamp [mask 1 ] … [mask K ] 3. Rotate grasped object
View 1View K</li>
</ol>
<p>Table 1 :
1
Performance on grounded planning evaluation.We measure the action (Act) and object (Obj) name prediction accuracy and grounding performance (Grd) on the four levels of GemBench validation split.All the models are fine-tuned on the robot grounded planning dataset.
GrdMulti-Hist-L1L2L3L4typevieworyAct Obj Grd Act Obj Grd Act Obj Grd Act Obj GrdBox✓✓95.1 93.7 62.8 97.4 89.0 58.7 69.3 53.8 46.2 70.0 36.8 16.6Mask××98.0 98.2 87.8 95.1 89.5 79.8 79.3 76.3 62.7 77.2 40.0 37.4Mask✓×100 100 88.6 98.0 91.3 81.2 85.6 75.6 61.4 89.9 50.1 46.5Mask✓✓100 100 87.9 99.0 93.3 79.2 88.6 83.9 66.4 79.4 44.9 40.0</p>
<p>Table 2 :
2
Performance on grounded planning evaluation.All the models use multi-view and history plans for mask generation, but are fine-tuned on different composition of datasets: robot grounded planning (Plan), multi-view referring expression (RefExp), and pseudo long-horizon tasks (Long).
Finetuning DataL1L2L3L4Plan RefExp Long Act Obj Grd Act Obj Grd Act Obj Grd Act Obj Grd✓××100 100 87.9 99.0 93.3 79.2 88.6 83.9 66.4 79.4 44.9 40.0✓✓×100 100 89.1 99.0 95.1 84.2 92.1 88.2 73.3 72.1 42.3 41.7✓✓✓100 100 89.5 99.7 95.3 85.2 88.5 82.2 73.8 93.9 51.2 53.84.3 Ablation Studies
Boxes vs. Masks.We compare Gondola's mask-based grounding approach with a box-based variant.The box variant (row 1) in Table1directly generates bounding boxes as textual outputs as illustrated in the middle of Figure</p>
<p>Table 4 :
4
Performance on four levels of GemBench testing split.
MethodL1L2L3L4Hiveformer [32]60.3±1.526.1±1.435.1±1.70.0±0.0PolarNet [35]77.7±0.937.1±1.438.5±1.70.1±0.2w/o LLM3D diffuser actor [38]91.9±0.843.4±2.837.0±2.20.0±0.0RVT-2 [39]89.1±0.851.0±2.336.0±2.20.0±0.03D-LOTUS [14]94.3±1.449.9±2.238.1±1.10.3±0.3w/ LLM3D-LOTUS++ [14] Gondola (Ours)68.7±0.6 87.3±1.964.5±0.9 74.8±1.841.5±1.8 52.4±2.117.4±0.4 19.0±1.0</p>
<p>Table 3 :
3
Success rate of task execution on four levels of GemBench testing split.The Gondola model is integrated with a 3D-based motion planning policy.
Act chunk3D filterL1L2L3L45× 87.3±1.9 74.8±1.8 52.4±2.1 19.0±1.0 ✓ 86.5±1.2 74.4±1.1 51.1±1.5 19.7±1.71× 90.8±1.2 78.2±1.4 49.5±0.5 14.9±2.2 ✓ 90.5±0.3 78.1±1.8 49.3±0.9 15.9±2.1</p>
<p>Table 5 :
5
Detailed performance on each task variation on GemBench Level 1. 'AC' denote action chunk size.
MethodAvg.Close Fridge+0Close Jar+15Close Jar+16CloseLap-topLid+0CloseMicro-wave+0LightBulb In+17LightBulb In+19Open Box+0Open Door+03D-LOTUS++ [14]68.79510099288755455579Gondola (AC=5)87.39699100968285816379Gondola (AC=1)90.897100100988380735573MethodOpen Drawer+0Open Drawer+2Pick&amp; Lift+0Pick&amp; Lift+2Pick&amp; Lift+7PickUp Cup+11PickUp Cup+8PickUp Cup+9Push Button+0Push Button+33D-LOTUS++ [14]6875979493918688100100Gondola (AC=5)8297979798889590100100Gondola (AC=1)84961001009796979497100MethodPush Button+4PutInCup-board+0PutInCup-board+3PutMoney InSafe+0PutMoney InSafe+1Reach&amp; Drag+14Reach&amp; Drag+18Slide Block+0Slide Block+1Stack Blocks+303D-LOTUS++ [14]10012221694621006586Gondola (AC=5)1005855897397981007476Gondola (AC=1)100817296941001001009589MethodStack Blocks+36Stack Blocks+393D-LOTUS++ [14]2028Gondola (AC=5)8181Gondola (AC=1)8484</p>
<p>Table 6 :
6
Detailed performance on each task variation on GemBench Level 2. 'AC' denote action chunk size.
MethodAvg.Close Jar+3Close Jar+4Lamp On+0LightBulb In+1LightBulb In+2Pick&amp; Lift+14Pick&amp; Lift+16Pick&amp; Lift+18Pick&amp;Lift Cylinder+03D-LOTUS++ [14]64.598962564394969591Gondola (AC=5)74.8991001818396989978Gondola (AC=1)78.299990837398969889MethodPick&amp;Lift Moon+0Pick&amp;Lift Star+0Pick&amp;Lift Toy+0PickUp Cup+10PickUp Cup+12PickUp Cup+13Push Button+13Push Button+15Push Button+17PutCube InSafe+03D-LOTUS++ [14]299471798984991009937Gondola (AC=5)919577849597999910042Gondola (AC=1)909583869698991009957MethodPutInCup-board+7PutInCup-board+8Reach&amp; Drag+5Reach&amp; Drag+7Slide Block+2Slide Block+3Stack Blocks+24Stack Blocks+27Stack Blocks+333D-LOTUS++ [14]109464275228359Gondola (AC=5)16197962610818072Gondola (AC=1)1521001002350918586
C.1 Experimental Setup</p>
<p>Table 9 :
9
Performance of 7 seen task variations with real robot.
TaskGondolaStack yellow cup in pink cup8/10Stack navy cup in yellow cup7/10Put strawberry in box4/10Put peach in box4/10Open drawer6/10Put item in drawer1/10Hang mug5/10Avg.5/10</p>
<p>Table 10 :
10
Performance of 7 unseen task variations with real robot.
TaskGondolaStack red cup in black cup7/10Stack black cup in orange cup2/10Place the yellow cup inside the red cup,then the cyan cup on top2/10Put lemon in box5/10Put banana in box6/10Put tuna can in box, then corn in box3/10Put croissant in yellow plate,then grapes in pink plate1/10Avg.3.7/10
The keystep is defined as step with significant motion change as in prior work[32,35,39,14,37], which helps avoid over-sampling similar images in training.
AcknowledgmentsThis work was partially supported by the HPC resources from GENCI-IDRIS (Grant 20XX-AD011012122 and AD011014846).It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the "France 2030" program, reference ANR-23-IACL-0008 (PR[AI]RIE-PSAI projet), the ANR project VideoPredict (ANR-21-FAI1-0002-01), and the Paris Île-de-France Région in the frame of the DIM AI4IDF.Predicted plan and masks grasp<p>black cup</p>[SEG] Multi-view observationInstruction: place the black cup onto the orange cup.
Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.071132019arXiv preprint</p>
<p>C Chi, S Feng, Y Du, Z Xu, E Cousineau, B Burchfiel, S Song, arXiv:2303.04137Diffusion policy: Visuomotor policy learning via action diffusion. 2023</p>
<p>Z Fu, T Z Zhao, C Finn, arXiv:2401.02117Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. 2024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.09246An open-source vision-language-action model. 2024arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. Q Vuong, S Levine, H R Walke, K Pertsch, A Singh, R Doshi, C Xu, J Luo, L Tan, D Shah, CoRL. 2023</p>
<p>Droid: A large-scale in-the-wild robot manipulation dataset. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, RSS 2024 Workshop: Data Generation for Robotics. </p>
<p>Q Agibot-World-Contributors, J Bu, L Cai, X Chen, Y Cui, S Ding, S Feng, X Gao, X He, X Hu, S Huang, Y Jiang, C Jiang, H Jing, J Li, C Li, Y Liu, Y Liu, J Lu, P Luo, Y Luo, Y Mu, Y Niu, J Pan, Y Pang, G Qiao, C Ren, J Ruan, Y Shan, C Shen, M Shi, M Shi, C Shi, J Sima, H Song, W Wang, D Wang, C Wei, G Xie, J Xu, C Yan, L Yang, S Yang, M Yang, J Yao, C Zeng, Q Zhang, B Zhang, C Zhao, J Zhao, J Zhao, Zhu, arXiv:2503.06669Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. 2025arXiv preprint</p>
<p>CALVIN: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, 2022IEEE RA-L</p>
<p>The colosseum: A benchmark for evaluating generalization for robotic manipulation. W Pumacay, I Singh, J Duan, R Krishna, J Thomason, D Fox, arXiv:2402.081912024arXiv preprint</p>
<p>Towards generalizable vision-language robotic manipulation: A benchmark and LLM-guided 3D policy. R Garcia, S Chen, C Schmid, ICRA. 2025</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, ICRA. 2023</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023</p>
<p>J Bjorck, F Castañeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>A I , Meta , Llama 3 model card. 2024</p>
<p>arXiv:2302.11550OpenAI. GPT-4 technical report. 2023</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, ICML. 2022</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, CoRL. 2023</p>
<p>Robotic control via embodied chain-of-thought reasoning. Z Michał, C William, P Karl, M Oier, F Chelsea, L Sergey, CORL. 2024</p>
<p>X Li, C Mata, J Park, K Kahatapitiya, Y S Jang, J Shang, K Ranasinghe, R Burgert, M Cai, Y J Lee, arXiv:2406.20095Supercharging robot learning data for vision-language policy. 2024arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, arXiv:2406.107212024arXiv preprint</p>
<p>H Yuan, X Li, T Zhang, Z Huang, S Xu, S Ji, Y Tong, L Qi, J Feng, M.-H Yang, arXiv:2501.04001Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. 2025arXiv preprint</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, 2020IEEE RA-L</p>
<p>L Shao, T Migimatsu, Q Zhang, K Yang, J Bohg, Concept2robot: Learning manipulation concepts from instructions and human demonstrations. IJRR. 2021</p>
<p>Interactive language: Talking to robots in real time. C Lynch, A Wahid, J Tompson, T Ding, J Betker, R Baruch, T Armstrong, P Florence, 2023IEEE RA-L</p>
<p>Languageconditioned imitation learning for robot manipulation tasks. S Stepputtis, J Campbell, M Phielipp, S Lee, C Baral, H Ben Amor, 2020NeurIPS</p>
<p>Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, CoRL. 2018</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, CoRL2022</p>
<p>Instruction-driven history-aware policies for robotic manipulations. P.-L Guhur, S Chen, R Garcia, M Pinel, I Tapaswi, C Laptev, Schmid, CoRL2023</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, CoRL2023</p>
<p>Rvt: Robotic view transformer for 3d object manipulation. A Goyal, J Xu, Y Guo, V Blukis, Y.-W Chao, D Fox, CoRL2023</p>
<p>Polarnet: 3d point clouds for language-guided robotic manipulation. S Chen, R Garcia, C Schmid, I Laptev, CoRL2023</p>
<p>S Chen, R Garcia, I Laptev, C Schmid, Sugar: Pre-training 3d visual representations for robotics. CVPR. 2024</p>
<p>Act3d: 3d feature field transformers for multi-task robotic manipulation. T Gervet, Z Xian, N Gkanatsios, K Fragkiadaki, CoRL2023</p>
<p>T.-W Ke, N Gkanatsios, K Fragkiadaki, arXiv:2402.108853d diffuser actor: Policy diffusion with 3d scene representations. 2024</p>
<p>Rvt2: Learning precise manipulation from few demonstrations. A Goyal, V Blukis, J Xu, Y Guo, Y.-W Chao, D Fox, RSS. 2024</p>
<p>G Tziafas, H Kasaei, arXiv:2406.18722Towards open-world grasping with large vision-language models. 2024arXiv preprint</p>
<p>Coarse-to-fine q-attention: Efficient learning for visual robotic manipulation via discretisation. S James, K Wada, T Laidlow, A J Davison, CVPR. 2022</p>
<p>Learning robotic manipulation policies from point clouds with conditional flow matching. E Chisari, N Heppert, M Argus, T Welschehold, T Brox, A Valada, arXiv:2409.073432024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, ICML. 2021</p>
<p>Scaling robot learning with semantically imagined experience. T Yu, T Xiao, A Stone, J Tompson, A Brohan, S Wang, J Singh, C Tan, D M , J Peralta, B Ichter, K Hausman, F Xia, arXiv:2302.115502023</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollár, R Girshick, arXiv:2304.02643Segment anything. 2023</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Y Hu, F Lin, T Zhang, L Yi, Y Gao, arXiv:2311.178422023</p>
<p>4v (ision) system card. G Openai, 2023preprint</p>
<p>H Liu, C Li, Q Wu, Y J Lee, Visual instruction tuning. NeurIPS. 2024</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, CVPR. 2024</p>
<p>Kosmos-2: Grounding multimodal large language models to the world. Z Peng, W Wang, L Dong, Y Hao, S Huang, S Ma, F Wei, arXiv:2306.148242023arXiv preprint</p>
<p>K Chen, Z Zhang, W Zeng, R Zhang, F Zhu, R Zhao, arXiv:2306.15195Shikra: Unleashing multimodal llm's referential dialogue magic. 2023arXiv preprint</p>
<p>J Chen, D Zhu, X Shen, X Li, Z Liu, P Zhang, R Krishnamoorthi, V Chandra, Y Xiong, M Elhoseiny, arXiv:2310.09478Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. 2023arXiv preprint</p>
<p>Ferret: Refer and ground anything anywhere at any granularity. H You, H Zhang, Z Gan, X Du, B Zhang, Z Wang, L Cao, S.-F Chang, Y Yang, arXiv:2310.077042023arXiv preprint</p>
<p>W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, arXiv:2311.03079Visual expert for pretrained language models. 2023arXiv preprint</p>
<p>Polyformer: Referring image segmentation as sequential polygon generation. J Liu, H Ding, Z Cai, Y Zhang, R K Satzoda, V Mahadevan, R Manmatha, CVPR. 2023</p>
<p>Groma: Localized visual tokenization for grounding multimodal large language models. C Ma, Y Jiang, J Wu, Z Yuan, X Qi, ECCV. 2025</p>
<p>Groundhog: Grounding large language models to holistic segmentation. Y Zhang, Z Ma, X Gao, S Shakiah, Q Gao, J Chai, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>Lisa: Reasoning segmentation via large language model. X Lai, Z Tian, Y Chen, Y Li, Y Yuan, S Liu, J Jia, CVPR. 2024</p>
<p>Llavagrounding: Grounded visual chat with large multimodal models. H Zhang, H Li, F Li, T Ren, X Zou, S Liu, S Huang, J Gao, C Li, J Yang, ECCV. Springer2025</p>
<p>Glamm: Pixel grounding large multimodal model. H Rasheed, M Maaz, S Shaji, A Shaker, S Khan, H Cholakkal, R M Anwer, E Xing, M.-H Yang, F S Khan, CVPR. 2024</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Z Chen, J Wu, W Wang, W Su, G Chen, S Xing, M Zhong, Q Zhang, X Zhu, L Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>N Ravi, V Gabeur, Y.-T Hu, R Hu, C Ryali, T Ma, H Khedr, R Rädle, C Rolland, L Gustafson, arXiv:2408.00714Segment anything in images and videos. 20242arXiv preprint</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. J Rasley, S Rajbhandari, O Ruwase, Y He, ACM SIGKDD. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1766 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1766</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1766</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-87808112171390772bbe4d0e9eade906de0bf8e6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/87808112171390772bbe4d0e9eade906de0bf8e6" target="_blank">Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch</a></p>
                <p><strong>Paper Venue:</strong> IEEE Robotics and Automation Letters</p>
                <p><strong>Paper TL;DR:</strong> This letter extends the Tactile Gym simulator to include three new optical tactile sensors (TacTip, DIGIT and DigiTac) of the two most popular types, Gelsight- style (image-shading based) and TacTip-style (marker based), and demonstrates that a single sim-to-real approach can be used with these three different sensors to achieve strong real-world performance despite the significant differences between real tactile images.</p>
                <p><strong>Paper Abstract:</strong> High-resolution optical tactile sensors are increasingly used in robotic learning environments due to their ability to capture large amounts of data directly relating to agent-environment interaction. However, there is a high barrier of entry to research in this area due to the high cost of tactile robot platforms, specialised simulation software, and sim-to-real methods that lack generality across different sensors. In this letter we extend the Tactile Gym simulator to include three new optical tactile sensors (TacTip, DIGIT and DigiTac) of the two most popular types, Gelsight-style (image-shading based) and TacTip-style (marker based). We demonstrate that a single sim-to-real approach can be used with these three different sensors to achieve strong real-world performance despite the significant differences between real tactile images. Additionally, we lower the barrier of entry to the proposed tasks by adapting them to an inexpensive 4-DoF robot arm, further enabling the dissemination of this benchmark. We validate the extended environment on three physically-interactive tasks requiring a sense of touch: object pushing, edge following and surface following. The results of our experimental validation highlight some differences between these sensors, which may help future researchers select and customize the physical characteristics of tactile sensors for different manipulations scenarios.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1766.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1766.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tactile Gym 2.0 sim-to-real platform</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tactile Gym 2.0: Sim-to-real Deep Reinforcement Learning for Comparing Low-cost High-Resolution Robot Touch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-cost desktop tactile robot platform and sim-to-real pipeline that trains deep RL policies in a physics-based tactile simulator (Tactile Gym extended) and transfers them zero-shot to a DOBOT MG400 equipped with high-resolution optical tactile sensors (TacTip, DIGIT, DigiTac) via real-to-sim image translation (GAN). Validated on object-pushing, edge-following and surface-following tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>DOBOT MG400 desktop tactile robot with TacTip / DIGIT / DigiTac end-effector sensors</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 4-DoF DOBOT MG400 desktop robot arm (x,y,z + yaw) used as an affordable tactile research platform; interchangeable high-resolution optical tactile sensors (TacTip: marker-based, DIGIT: GelSight-style, DigiTac: TacTip skin on DIGIT housing) are mounted as the end-effector for physically-interactive manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (tactile-enabled manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Tactile Gym (extended)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A custom rigid-body physics simulator (Tactile Gym) with CAD-based sensor geometry and synthetic cameras that renders tactile observations as depth images relative to the sensor CAD; simulates contact interactions between sensor skin and stimuli, and allows tuning of skin physics and camera parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Moderate-to-high fidelity for contact geometry and contact dynamics (rigid-body physics + depth-image rendering); not full continuum (FE) elastomer deformation or photorealistic shading.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact geometry and rigid-body contact dynamics, friction, stiffness and damping parameters for sensor skins, exact CAD sensor geometry and synthetic camera perspective, depth-image rendering of contact, adjustable camera and lighting parameters (calibrated to real sensors).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Continuous elastomer deformation not modeled with Finite Element (FE) methods (skin deformation approximated via rigid contact and depth rendering), GelSight-like image shading approximated (depth images rendered rather than full photorealistic shaded images), sensor optical noise and subtle material heterogeneities simplified, actuator dynamics and timing effects not emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical setup using a DOBOT MG400 4-DoF arm with mounted TacTip, DIGIT or DigiTac tactile sensors, 3D-printed stimulus objects (edges and surfaces) and manipulanda (cube, cylinder, triangular prism) tracked with ArUco markers; experiments performed zero-shot with no further policy tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Tactile-feedback control policies for object pushing (trajectory following), edge following (maintain distance while sliding along an edge), and surface following (maintain contact depth and normal orientation while sliding over curved side surfaces).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning (PPO) in simulation for policy learning; paired with real-to-sim image translation using a conditional GAN (U-Net generator, convolutional discriminator) to map real tactile images to simulated tactile-image domain.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Image translation: Structural Similarity Index (SSIM) between translated and simulated images; Task performance: mean Euclidean position/trajectory error (mm) between actual and ground-truth trajectories for pushing, edge-following and surface-following.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Object pushing: mean Euclidean trajectory error typically ~9–16 mm (varying by sensor, object and trajectory); Edge following: mean position errors ~0.6–1.8 mm (TacTip and DigiTac ~0.6–1.4 mm, DIGIT ~0.9–1.8 mm); Surface following: mean position errors ~0.5–1.2 mm (DIGIT achieved 0.5 mm on circular wall). Image translation SSIM: Edge ~0.9867–0.9956, Surface ~0.9818–0.9932 (sensor-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Domain randomization of sensor poses during data collection (pose ranges for edge and surface contacts), variation of contact dynamics parameters (friction, stiffness, damping) and camera/internal lighting/camera-parameter calibration across simulated sensors; policies trained with domain randomization to improve robustness to real-world variability.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Differences in tactile image appearance due to sensor design (marker vs shading), lens dynamics and camera perspective, elastomer stiffness and geometry, asymmetric sensor shapes (DIGIT and DigiTac), unmodeled continuous elastomer deformation (no FE), friction and adhesion during sliding, insufficient deformation for light objects (DIGIT failure cases), and subtle lighting/shading differences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of CAD-based sensor simulation with calibrated synthetic cameras; real-to-sim image translation GAN (U-Net conditioned generator) to map real sensor images into the simulated depth-image domain; domain randomization during RL training; careful image preprocessing and per-sensor hyperparameter tuning; collecting paired real datasets across sampled sensor poses covering contact variations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate modeling of contact geometry/camera perspective and matching the distribution of simulated tactile images to real images (via camera calibration and GAN translation) is critical; full FE elastomer modeling is not required for successful zero-shot transfer when using depth rendering plus image translation and domain randomization; contact dynamics (friction/stiffness) must be within the GAN training distribution to avoid failures (e.g., DIGIT requires sufficient deformation).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Real-to-sim image translation combined with depth-based tactile rendering and domain-randomized RL enables zero-shot sim-to-real transfer across fundamentally different optical tactile sensors (TacTip marker-based and DIGIT GelSight-style), validated on pushing, edge-following and surface-following tasks; performance differences were dominated by physical sensor construction (stiffness, shape) rather than sensing modality, DIGIT showed slightly lower image-translation SSIM and failure modes on light objects and concave surfaces (mitigated by increasing object weight or coating to reduce friction), and accurate camera/sensor calibration plus domain randomization and GAN translation were sufficient to bridge the sim-to-real gap without real-world policy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tactile Sim-to-Real Policy Transfer via Real-to-Sim Image Translation <em>(Rating: 2)</em></li>
                <li>Sim-to-Real Transfer for Optical Tactile Sensing <em>(Rating: 2)</em></li>
                <li>Tacto: A fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors <em>(Rating: 2)</em></li>
                <li>Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing <em>(Rating: 2)</em></li>
                <li>Generation of gelsight tactile images for sim2real learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1766",
    "paper_id": "paper-87808112171390772bbe4d0e9eade906de0bf8e6",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Tactile Gym 2.0 sim-to-real platform",
            "name_full": "Tactile Gym 2.0: Sim-to-real Deep Reinforcement Learning for Comparing Low-cost High-Resolution Robot Touch",
            "brief_description": "A low-cost desktop tactile robot platform and sim-to-real pipeline that trains deep RL policies in a physics-based tactile simulator (Tactile Gym extended) and transfers them zero-shot to a DOBOT MG400 equipped with high-resolution optical tactile sensors (TacTip, DIGIT, DigiTac) via real-to-sim image translation (GAN). Validated on object-pushing, edge-following and surface-following tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "DOBOT MG400 desktop tactile robot with TacTip / DIGIT / DigiTac end-effector sensors",
            "agent_system_description": "A 4-DoF DOBOT MG400 desktop robot arm (x,y,z + yaw) used as an affordable tactile research platform; interchangeable high-resolution optical tactile sensors (TacTip: marker-based, DIGIT: GelSight-style, DigiTac: TacTip skin on DIGIT housing) are mounted as the end-effector for physically-interactive manipulation tasks.",
            "domain": "general robotics manipulation (tactile-enabled manipulation)",
            "virtual_environment_name": "Tactile Gym (extended)",
            "virtual_environment_description": "A custom rigid-body physics simulator (Tactile Gym) with CAD-based sensor geometry and synthetic cameras that renders tactile observations as depth images relative to the sensor CAD; simulates contact interactions between sensor skin and stimuli, and allows tuning of skin physics and camera parameters.",
            "simulation_fidelity_level": "Moderate-to-high fidelity for contact geometry and contact dynamics (rigid-body physics + depth-image rendering); not full continuum (FE) elastomer deformation or photorealistic shading.",
            "fidelity_aspects_modeled": "Contact geometry and rigid-body contact dynamics, friction, stiffness and damping parameters for sensor skins, exact CAD sensor geometry and synthetic camera perspective, depth-image rendering of contact, adjustable camera and lighting parameters (calibrated to real sensors).",
            "fidelity_aspects_simplified": "Continuous elastomer deformation not modeled with Finite Element (FE) methods (skin deformation approximated via rigid contact and depth rendering), GelSight-like image shading approximated (depth images rendered rather than full photorealistic shaded images), sensor optical noise and subtle material heterogeneities simplified, actuator dynamics and timing effects not emphasized.",
            "real_environment_description": "Physical setup using a DOBOT MG400 4-DoF arm with mounted TacTip, DIGIT or DigiTac tactile sensors, 3D-printed stimulus objects (edges and surfaces) and manipulanda (cube, cylinder, triangular prism) tracked with ArUco markers; experiments performed zero-shot with no further policy tuning.",
            "task_or_skill_transferred": "Tactile-feedback control policies for object pushing (trajectory following), edge following (maintain distance while sliding along an edge), and surface following (maintain contact depth and normal orientation while sliding over curved side surfaces).",
            "training_method": "Deep reinforcement learning (PPO) in simulation for policy learning; paired with real-to-sim image translation using a conditional GAN (U-Net generator, convolutional discriminator) to map real tactile images to simulated tactile-image domain.",
            "transfer_success_metric": "Image translation: Structural Similarity Index (SSIM) between translated and simulated images; Task performance: mean Euclidean position/trajectory error (mm) between actual and ground-truth trajectories for pushing, edge-following and surface-following.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Object pushing: mean Euclidean trajectory error typically ~9–16 mm (varying by sensor, object and trajectory); Edge following: mean position errors ~0.6–1.8 mm (TacTip and DigiTac ~0.6–1.4 mm, DIGIT ~0.9–1.8 mm); Surface following: mean position errors ~0.5–1.2 mm (DIGIT achieved 0.5 mm on circular wall). Image translation SSIM: Edge ~0.9867–0.9956, Surface ~0.9818–0.9932 (sensor-dependent).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Domain randomization of sensor poses during data collection (pose ranges for edge and surface contacts), variation of contact dynamics parameters (friction, stiffness, damping) and camera/internal lighting/camera-parameter calibration across simulated sensors; policies trained with domain randomization to improve robustness to real-world variability.",
            "sim_to_real_gap_factors": "Differences in tactile image appearance due to sensor design (marker vs shading), lens dynamics and camera perspective, elastomer stiffness and geometry, asymmetric sensor shapes (DIGIT and DigiTac), unmodeled continuous elastomer deformation (no FE), friction and adhesion during sliding, insufficient deformation for light objects (DIGIT failure cases), and subtle lighting/shading differences.",
            "transfer_enabling_conditions": "Use of CAD-based sensor simulation with calibrated synthetic cameras; real-to-sim image translation GAN (U-Net conditioned generator) to map real sensor images into the simulated depth-image domain; domain randomization during RL training; careful image preprocessing and per-sensor hyperparameter tuning; collecting paired real datasets across sampled sensor poses covering contact variations.",
            "fidelity_requirements_identified": "Accurate modeling of contact geometry/camera perspective and matching the distribution of simulated tactile images to real images (via camera calibration and GAN translation) is critical; full FE elastomer modeling is not required for successful zero-shot transfer when using depth rendering plus image translation and domain randomization; contact dynamics (friction/stiffness) must be within the GAN training distribution to avoid failures (e.g., DIGIT requires sufficient deformation).",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Real-to-sim image translation combined with depth-based tactile rendering and domain-randomized RL enables zero-shot sim-to-real transfer across fundamentally different optical tactile sensors (TacTip marker-based and DIGIT GelSight-style), validated on pushing, edge-following and surface-following tasks; performance differences were dominated by physical sensor construction (stiffness, shape) rather than sensing modality, DIGIT showed slightly lower image-translation SSIM and failure modes on light objects and concave surfaces (mitigated by increasing object weight or coating to reduce friction), and accurate camera/sensor calibration plus domain randomization and GAN translation were sufficient to bridge the sim-to-real gap without real-world policy fine-tuning.",
            "uuid": "e1766.0",
            "source_info": {
                "paper_title": "Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tactile Sim-to-Real Policy Transfer via Real-to-Sim Image Translation",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-Real Transfer for Optical Tactile Sensing",
            "rating": 2
        },
        {
            "paper_title": "Tacto: A fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors",
            "rating": 2
        },
        {
            "paper_title": "Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing",
            "rating": 2
        },
        {
            "paper_title": "Generation of gelsight tactile images for sim2real learning",
            "rating": 1
        }
    ],
    "cost": 0.0097655,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Tactile Gym 2.0: Sim-to-real Deep Reinforcement Learning for Comparing Low-cost High-Resolution Robot Touch</h1>
<p>Yijiong Lin1, John Lloyd1, Alex Church1, Nathan F. Lepora1
Project Webpage: https://sites.google.com/my.bristol.ac.uk/tactilegym2</p>
<p><strong>Abstract</strong>—High-resolution optical tactile sensors are increasingly used in robotic learning environments due to their ability to capture large amounts of data directly relating to agent-environment interaction. However, there is a high barrier of entry to research in this area due to the high cost of tactile robot platforms, specialised simulation software, and sim-to-real methods that lack generality across different sensors. In this letter we extend the Tactile Gym simulator to include three new optical tactile sensors (TacTip, DIGIT and DigiTac) of the two most popular types, Gelsight-style (image-shading based) and TacTip-style (marker based). We demonstrate that a single sim-to-real approach can be used with these three different sensors to achieve strong real-world performance despite the significant differences between real tactile images. Additionally, we lower the barrier of entry to the proposed tasks by adapting them to an inexpensive 4-DoF robot arm, further enabling the dissemination of this benchmark. We validate the extended environment on three physically-interactive tasks requiring a sense of touch: object pushing, edge following and surface following. The results of our experimental validation highlight some differences between these sensors, which may help future researchers select and customize the physical characteristics of tactile sensors for different manipulations scenarios.</p>
<p><strong>Index Terms</strong>—Force and tactile sensing; Deep Learning; Dexterous Manipulation</p>
<h2>I. INTRODUCTION</h2>
<p>A plausible route to human-like robot dexterity is to combine deep learning with high-resolution tactile sensing, given the unprecedented recent advances in controlling robots with deep learning applied to robot vision [2]. Moreover, the use of deep reinforcement learning (RL) would seem to offer the potential for learning complex manipulation tasks based on a reward information, which is both a mechanism for how humans acquire new skills and has achieved impressive results in simulated environments such as computer games. However, there are major challenges preventing tactile deep RL from being realised: (1) the lack of available and accessible tactile sensing technologies limits the research capacity available to develop RL methods for touch; (2) those labs that have</p>
<p>YL was supported by the China Scholarship Council (CSC)/University of Bristol joint-funded scholarship. AC was supported by an EPSRC CASE award sponsored by Google DeepMind. NL and JL were supported by a Leadership Award from the Leverhulme Trust on 'A biomimetic forebrain for robot touch' (RL-2016-39).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of the tactile sim-to-real deep RL robotic system: (a) Three low-cost high-resolution optical tactile sensors and the raw sensor images. (b) Training real-to-sim tactile image translation. (c) Policy learning in the Tactile Gym [1] with simulated tactile images from multiple integrated sensors. (d) A desktop robot equipped with the DigiTac performing the surface-following task by translating real tactile images to simulated images for the RL policy.</p>
<p>expertise in fabricating tactile sensors tend to stay with the technology where they have expertise; (3) hence, approaches to tactile deep RL, e.g. [1], [3], [4], stay confined within those labs, which is inefficient for progressing the field and opposite</p>
<p>YL was supported by the China Scholarship Council (CSC)/University of Bristol joint-funded scholarship. AC was supported by an EPSRC CASE award sponsored by Google DeepMind. NL and JL were supported by a Leadership Award from the Leverhulme Trust on 'A biomimetic forebrain for robot touch' (RL-2016-39). 1 All authors are with the Department of Engineering Mathematics and Bristol Robotics Laboratory, University of Bristol, Bristol BS8 1UB, U.K. (email: {yijiong.lin, j.lloyd, a.church, n.lepora}@bristol.ac.uk) Code is open-sourced at: https://github.com/ac-93/tactile_gym</p>
<p>to the open culture that has benefitted AI research.
Meta AI researchers have developed and open-sourced a tactile robot learning platform called TACTO [5] and tactile processing libraries PyTouch [6] for GelSight-based tactile sensors such as the low-cost, open-sourced DIGIT tactile sensor [7]. Meanwhile, researchers in Bristol Robotics Laboratory have developed a family of high-resolution biomimetic optical tactile sensors called the TacTip [8], [9], alongside an open-sourced suite of learning environments called Tactile Gym that features highly-efficient tactile image rendering [1]. In consequence, sim-to-real policy transfer via tactile image translation enabled zero-shot performance on multiple exploration and manipulation tasks requiring tactile feedback, such as delicately tracing the edges and surfaces of complex objects and pushing objects to goal locations.</p>
<p>The goal of this present research is to bring together this progress with GelSight-type sensors with the sim-to-real deep RL methods developed with the TacTip. Whilst it was previously claimed that the approach used in [1] should be applicable to "a broad range of other high-resolution optical tactile sensors including sensors of the Gelsight type", the approach was only demonstrated with the TacTip. Here we extend the approach to the DIGIT sensor (of Gelsight type), a reduced form-factor TacTip sensor and a DIGIT-TacTip hybrid sensor referred to as the DigiTac. We demonstrate strong real-world performance across these sensors despite the significantly different tactile images produced by each.</p>
<p>The main contributions of this work are as follows:</p>
<p>1) We extend the Tactile Gym [1] from one to three lowcost, high-resolution optical tactile sensors. To the best of our knowledge, this is the first work that integrates two widelyused yet fundamentally different styles of the optical tactile sensors: Gelsight-style (DIGIT) and TacTip-style (DigiTac and TacTip), into one platform. Such an integration validates and extends earlier results for this platform while making it more accessible and applicable to the wider research community.
2) With the extended Tactile Gym environments, we successfully learn deep RL policies for three physically-interactive tasks (edge following, surface following and object pushing), and transfer them into the real world without further tuning. By comparing task performance, we identify strengths and weaknesses of the tactile sensors in these different scenarios. As far as we know, this is the first empirical comparison of optical tactile sensors in a sim-to-real context, which we intend as a benchmark to aid improvement of this technology.
3) We improve the accessibility of this research by adapting the tasks to the reduced workspace and degrees of freedom of the DOBOT MG400, an inexpensive and commercially-available 4-DoF robot arm. Additionally, we commit to openly releasing all software and hardware developments in this work to aid dissemination of this benchmark.</p>
<h2>II. RELATED WORK</h2>
<p>1) Deep Reinforcement Learning in Tactile Robotics: Deep reinforcement learning (RL) has proved successful in solving many sequential decision making problems in robotics, particularly those with high-dimensional observation spaces such
as in computer vision [2]. Some work has explored tactile RL with low-resolution tactile sensing to perform tasks such as object stabilisation [10] and learning a forward predictive model [11]. Work in [12] proposed a deep tactile modelpredictive RL framework for learning how to re-position an object using a Gelsight-style tactile sensor, and [3] used Twin Delayed DDPG (TD3) [13] to learn a general tactileguided insertion policy in the physical environment from a sequence of tactile images. A follow-up study simplified the observation space containing the raw tactile images and robot proprioceptive data for the deep RL policy by learning an extrinsic contact line model for contact localization. This policy was learned in a simulation environment [14].</p>
<p>Recently, a tactile simulation environment for deep RL, called Tactile Gym [1], has successfully applied the trained policies to some challenging tasks such as object pushing and rolling in a real physical environment. The transfer from simulation to real-world physical environment was facilitated using a novel real-to-sim tactile image translation technique, in a zero-shot manner. Tactile-based deep RL has also been successfully applied to robotic service tasks like learning to type on a braille keyboard [15] with the TacTip and learning to play the piano [4] with the DIGIT optical tactile sensor.
2) Tactile Sim-to-real Transfer: The tactile sim-to-real gap significantly hinders the application of learned policy in simulation to reality. Two research directions have been explored to close this gap: using the Finite Element (FE) method to model the sensor deformation dynamics [16]-[21], or leveraging the image rendering method to replicate the sensory data [1], [5], [22], [23]. For a more thorough review we refer to [1]. In the present work, we follow the tactile sim-toreal method described in [1] by using depth image-rendering and image translation for two popular optical tactile sensor classes: GelSight-type which is based on image shading [24] and TacTip type which is based on biomimetic marker-based transduction [9].</p>
<h2>III. Method and Experiments</h2>
<h2>A. Tactile Robot System</h2>
<p>In this paper, we use a tactile robot comprising a lowcost desktop robot arm with high-resolution tactile sensor mounted as an end effector. This is intended to be a lowercost, desktop version of the setup used by Church et al [1] for investigating sim-to-real tactile deep RL, which used a 6-axis industrial robot from Universal Robotics. In this paper, we expand the approach to compare three distinct optical tactile sensors: the TacTip, DIGIT, and DigiTac. Table I compares this robot platform to those used in related works, in terms of the features, cost and the integration of tactile sensors. The robot platform and the operation of the tactile sensors are presented in detail in an accompanying second paper, with the main focus of the present paper on the application of sim-to-real deep RL on this low-cost desktop robot. Hence, we summarize just the main aspects of the tactile robot.</p>
<p>1) Desktop robot arm: We use a Dobot MG400 4-axis arm designed for affordable automation. The base and control unit has footprint $190 \mathrm{~mm} \times 190 \mathrm{~mm}$, payload 750 g , maximum</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Comparison between the different optical tactile sensors: (a) DIGIT, (b) DigiTac, (c) TacTip. For each sensor we show: top-left, the real sensor hardware; top-right, simulated contact geometry between sensor skin and a blue edge stimulus; bottom-left, tactile image gathered by the real sensor pressing onto an edge; and bottom-right, a generated depth image matching those gathered in simulation.</p>
<p>Table I Comparison of the existing tactile sim-to-real robotic platforms for TacTip-style (T) and GelSight-style (G) sensors, respectively.</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Features</th>
<th>[1]</th>
<th>[21]</th>
<th>[23]</th>
<th>Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td>Robots</td>
<td>Type</td>
<td>UR5</td>
<td>Sawyer</td>
<td>UR5</td>
<td>MG400</td>
</tr>
<tr>
<td></td>
<td>Accuracy (mm)</td>
<td>0.1</td>
<td>0.1</td>
<td>0.1</td>
<td>0.05</td>
</tr>
<tr>
<td></td>
<td>Price ($)</td>
<td>45k</td>
<td>26k</td>
<td>45k</td>
<td>2.7k</td>
</tr>
<tr>
<td></td>
<td>Max. Reach (mm)</td>
<td>850</td>
<td>1260</td>
<td>850</td>
<td>440</td>
</tr>
<tr>
<td></td>
<td>Playload (kg)</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>0.75</td>
</tr>
<tr>
<td></td>
<td>Weight (kg)</td>
<td>18.4</td>
<td>19</td>
<td>18.4</td>
<td>8</td>
</tr>
<tr>
<td></td>
<td>DoF</td>
<td>6</td>
<td>7</td>
<td>6</td>
<td>4</td>
</tr>
<tr>
<td>Sensors (if integrated)</td>
<td>TacTip (T)</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
<td>✓</td>
</tr>
<tr>
<td></td>
<td>DIGIT (G)</td>
<td>✗</td>
<td>✗</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td></td>
<td>DigiTac (T)</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>Reach 440 mm and repeatability ±0.05 mm. As we describe later, the accuracy of tactile models trained using this arm is similar to larger industrial robot arms. The main constraint is that only the (x, y, z)-position and rotation around the z-axis of the end effector are actuated.</p>
<p>2) High-resolution optical tactile sensing: Here we consider three distinct optical tactile sensors:</p>
<p>(a) The TacTip, a soft, curved, 3D-printed tactile skin with an internal array of pins tipped with markers, which are used to amplify the surface deformation from physical contact against a stimulus. For more details, we refer to ref. [8], [9].</p>
<p>(b) The DIGIT shares the same principle of the Gelsight tactile sensor [25], but can be fabricated at low cost and is of a size suitable for integration of some robotic hands, such as on the fingertips of the Allegro hand [7].</p>
<p>(c) The DigiTac is an adapted version of the DIGIT and the TacTip, whereby the 3D-printed skin of a TacTip is customized to integrated onto the DIGIT housing, while keeping the camera and lighting system. In other words, this sensor outputs tactile images of the same dimension as the DIGIT, but with a soft biomimetic skin like other TacTip sensors.</p>
<h3>B. Tactile Sim-to-Real Deep RL Framework</h3>
<p>Following Church et al [1] we take a sim-to-real deep reinforcement learning approach to achieve the desired robot behaviour on physical hardware. This approach consists of three distinct phases:</p>
<p>1) An online learning in simulation phase, where deep RL is applied to images captured by a simulated tactile sensor for the learning of several distinct tasks (here edge following, surface following and object pushing).</p>
<p>2) A domain adaptation phase where a model is learned for the translation of images captured by a real tactile sensor to images captured by the simulated sensor.</p>
<p>3) A zero-shot sim-to-real phase where policies learned in simulation are transferred to the real hardware using the networks trained in the previous two steps. An overview of this approach is shown in Fig. 1 and more details can be found in the original reference [1].</p>
<p>To adapt the existing approach [1] to our setting, we needed to make several changes. In this work we chose to use a low-cost desktop robot arm as described above (Sec.III A). This arm has only 4 actuated axes, whereas previous work used a 6-axis industrial robotic arm (UR5, Universal Robotics). To facilitate the use of this lower degree-of-freedom arm we have adapted the tasks and data collection procedures while attempting to meet the challenge of successful performance on tasks originally developed for a more capable robot arm. In particular the previous surface following experiments made use of Roll and Pitch to accurately maintain a normal orientation to a surface. Instead, we rely on a custom 3D-printed flange so the sensor is mounted perpendicular to the end effector. In this way, we can make use of the Yaw DoF of the DOBOT MG400 when maintaining normal orientation to a surface varying in only one direction, as shown in Fig. 1(d).</p>
<p>Moreover, whilst it was previously claimed that the approach used in [1] should be applicable to "a broad range of other high-resolution optical tactile sensors, including sensors of the Gelsight type", it was only demonstrated with a hemispherical and flat TacTip. Here we validate that the approach works with the DIGIT sensor of the Gelsight-type, which has a distinct tactile sensing principle based on image shading in place of marker-based transduction. We also validate the approach with alternative forms of the TacTip sensor, including a new (more compact) version of TacTip than used in [1] and</p>
<p>the DigiTac which has a TacTip skin on a DIGIT housing. When compared with the original work these sensors introduce differences in camera perspectives, lens dynamics, marker size, density of markers and have significantly different lighting conditions. To do this, we extended the simulation to include simulated sensors that matched real hardware using the CAD files for 3D-printing those sensors.</p>
<h2>C. Deep RL with Different Tactile Dynamics in Simulation</h2>
<p>The sim-to-real deep RL framework uses the Tactile Gym [1] (see above) to simulate the contact dynamics with rigid-body physics, using tactile information rendered as depth images relative to the CAD model for the sensor. Thus, the learnt tactile-feedback policies from [1] apply only the TacTip originally considered in that study.</p>
<p>Hence, we extend the Tactile Gym [1] with two new virtual optical tactile sensors: DIGIT (Gelsight-style) and DigiTac (TacTip-style) (Fig. 2 a,b), based on their open-source CAD files [7] used for 3D-printing the DIGIT. We follow the method described in [1] to efficiently capture the depth images as tactile images by synthetic cameras embedded within those sensors. Specifically, we adjust the physics (friction, stiffness, damping, etc.) of the skin and the parameters of the camera for each sensor, to achieve realistic performance during learning.</p>
<p>The three distinct tactile observation spaces corresponding to the three type of sensor (Fig. 2) are then used as input to train deep RL polices for the tasks, following the methods detailed in [1]. We report on the training results in Sec. IV.</p>
<h2>D. Sim-to-real Transfer for Tactile Images</h2>
<p>Here we aim for zero-shot learning so the learned policy in simulation can be transferred to the real-world task without further training or tuning. Hence it is essential to have a model to bridge the gap between the simulated and real domains. Progress in Generative Adversarial Network (GAN) methods has enabled realistic image generation, which we leverage to learn an image-to-image translation GAN [26] applied to real-to-sim tactile image translation. The network takes advantage of a U-net architecture [27] for the imageconditioned generator to infer better the spatial features during training, alongside a standard convolutional network [28] with batch normalization [29] for the discriminator.</p>
<p>Since each of the three optical tactile sensors considered here have a different design and illumination, the imagepreprocessing and hyper-parameters need tuning for each sensor. The arrays of markers in the TacTip and the DigiTac reflect light more clearly, which eases the fine-tuning of their image processing compared to the DIGIT, where the image shading is more subtle. However, after tuning we find that all sensors work effectively for sim-to-real transfer, which we will cover in the results sections later.</p>
<h2>E. Sim-to-real Data Collection</h2>
<p>The three manipulation tasks considered here require distinct sim-to-real models across two distinguishing contact features: an edge for the edge-following task, and a surface</p>
<p>Table II
Sensor pose sampling ranges used during data collection. Sensor poses are expressed relative to a fixed coordinate frame attached to the training feature ( $R z=$ axial rotation around $z$-axis).</p>
<table>
<thead>
<tr>
<th>Contact Features</th>
<th>Edge</th>
<th></th>
<th></th>
<th>Surface</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sensors</td>
<td>$y(\mathrm{~mm})$</td>
<td>$z(\mathrm{~mm})$</td>
<td>$R z(\mathrm{deg})$</td>
<td>$x(\mathrm{~mm})$</td>
</tr>
<tr>
<td>Tactip</td>
<td>$[-6,6]$</td>
<td>$[2,5]$</td>
<td>$[-179,180]$</td>
<td>$[1,4]$</td>
</tr>
<tr>
<td>DigiTac</td>
<td>$[-5,5]$</td>
<td>$[2,4]$</td>
<td>$[-179,180]$</td>
<td>$[1,3]$</td>
</tr>
<tr>
<td>DIGIT</td>
<td>$[-5,5]$</td>
<td>$[2,3]$</td>
<td>$[-179,180]$</td>
<td>$[1,2]$</td>
</tr>
</tbody>
</table>
<p>for the surface-following and object-pushing tasks. We collect a training and validation dataset per contact feature (edge and surface) and per sensor, leading to twelve datasets in total.</p>
<p>Each training dataset comprised 5000 tactile images and each validation dataset 2000 tactile images, collected by using the desktop tactile robot (Sec.III A) to randomly contact data of the appropriate edge or surface feature. For the data collection, straight edge and flat surface of a 3D-printed stimulus, labeled with the relative poses between the sensor and the stimulus under contact. The movement ranges for each random contact feature are shown in Table II. These datasets take about 6 hours to collect on the physical robot and less than 1 minute to collect in simulation.</p>
<p>A further subtlety for the DIGIT and the DigiTac is that they are no longer symmetric (being broader across one axis) unlike the original TacTip. This meant we needed to customize the range of $(x, y)$-pose data collection depending on the rotation angle, which was implemented by scaling the $y$-range orthogonal to the edge by the tangent of the angle.</p>
<h2>F. Tactile Control Tasks</h2>
<p>Here we adapt three tactile control tasks proposed in [1] to the desktop tactile robot: edge-following, surface-following and object-pushing. Although we expect our platform would also be viable for the ball-rolling task, we do not implement it in this work because the DigiTac does not currently have a flat skin that is suitable for ball-rolling.</p>
<p>1) Object Pushing: This task aims for the robot to push an object through a sequence of goal positions along a trajectory on a flat surface. Three trajectories are considered: straight, curved and sinusoidal. In practise, each trajectory was divided into 10 equal-length sections with the final point on each section specified as the goal position; thus, there are ten goals for each trajectory. The 2D action space comprises the $x$ position and rotation angle of the tool center point (TCP) located at the tip of the tactile sensor. We use three distinct objects with different shapes and weights (Fig. 3c): a triangular prism, cube and cylinder varying in weight from 185 g to 363 g . These objects differ from those used in [1], which were lighter of mass ( 50 g to 80 g ), to better suit the elasticity of the DIGIT that required a heavier object for an appreciable tactile deformation. An ArUco marker is place on top of each object to track its movement for comparison with the ground truth using the tracking method described in [30].
2) Edge Following: This task aims for the robot to slide the tactile sensor along a contacted edge while maintaining a fixed distance between the edge and the TCP located at the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Tactile stimuli used in manipulation tasks: (a) square, clover leaf, and teardrop stimuli for the edge following task; (b) arch, flower, and disc for the (side) surface following task; (c) blue cylinder, yellow triangular prism, and red cube for the object pushing task (ArUco markers for ground truth tracking).</p>
<p>tip of the tactile sensor. The 2D action space comprises the <em>x</em> and <em>y</em> position of the TCP. To evaluate the robustness and maneuverability of the tactile robot, we use three stimuli that contain interesting geometrical features such as straight edges, positively/negatively curved edges and a right-angled corner (Fig. 3a). To measure tracking performance, the ground truth shapes are extracted from the CAD models of these 3D-printed objects by importing into the Blender CAD software [31] and outputting the point clouds of their boundaries.</p>
<p><em>3) Surface Following:</em> This task aims for the robot to slide the tactile sensor over a contacted surface while maintaining a fixed contact depth and orientating the TCP representing the tip of the sensor normal to that surface. The 2D action space comprises the <em>y</em>-position and rotation angle of the TCP. To evaluate the robustness and maneuverability of the tactile robot, we use the side surfaces of three objects comprising closed-loop 2D surfaces in 3D space (Fig. 3b) that contain interesting geometrical features such as locally planar, concave and convex surfaces. As with the edge-following task, the ground truth shapes and dimensions of the 3D-printed stimuli are obtained from the CAD models using Blender.</p>
<h2>IV. RESULTS</h2>
<h3>A. RL Performance in the Simulated Environments</h3>
<p>The deep RL method PPO [32] is here used to learn policies for the three simulated optical tactile sensors on the three considered tasks (pushing, edge-following, surface-following). Specifically, we use the Stable-Baselines-3 [33] implementation of PPO for training the policies.</p>
<p>The results of training in simulation are given in Fig. 4. Although there are slight differences in performance during training between the sensors, all the learned policies achieved similar accurate final performances in all tasks. We attribute these small differences to the different contact dynamics due to the different shapes of the tips for the three tactile sensors, but as we mentioned this does not affect the overall performance. In particular, we did not find it necessary to fine-tune the training hyperparameters for each tactile sensor. In addition, we apply the learned policy in each simulated task to the real robot later without any further fine-tuning.</p>
<p><strong>Table III</strong></p>
<p>Mean SSIM values for image translation GANs trained with edge and surface features (values closer to 1.0 represent better image translation).</p>
<table>
<thead>
<tr>
<th>Sensors Features</th>
<th>Tactip</th>
<th>DigiTac</th>
<th>DIGIT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Edge</td>
<td>0.9956</td>
<td>0.9953</td>
<td>0.9867</td>
</tr>
<tr>
<td>Surface</td>
<td>0.9927</td>
<td>0.9932</td>
<td>0.9818</td>
</tr>
</tbody>
</table>
<h3>B. Image Translation for Sim-to-real Transfer</h3>
<p>We use the Structural Similarity Index (SSIM) to evaluate the quality of the translated images for both the edge and surface validation datasets (Table III). The high SSIM values close to unity (perfect match) obtained with the trained model shows that the image-to-image translation produces accurate tactile images for all sensors. Therefore, the method implemented from [1] is applicable not only for TacTip family of tactile sensors but also for GelSight-style sensors such as the DIGIT. Some examples of tactile images and their translated versions for edge contacts are shown in Fig. 2.</p>
<p>As an aside, we comment that the performance of the DIGIT image translation was slightly lower than the TacTip and DigiTac. It is possible that a different architecture of neural network more suited to the GelSight-type sensors could change this result. We note that this minor difference does not affect the task overall performance, which we attribute to the deep RL policies being learnt with domain randomization.</p>
<h3>C. RL Performance in the Physical Environments</h3>
<p><em>1) Object Pushing:</em> We consider first the object pushing task, where the tactile robot must move the object along a desired trajectory through a series of goal points. Successful task performance corresponds to accurately pushing the test object (a cube, cylinder or triangular prism) along the trajectories (straight: <em>y</em> = <em>kx</em>, curved: <em>y</em> = 0.001<em>x</em>², sinuosoidal: <em>y</em> = 0.02sin(0.02<em>x</em>), where <em>k</em> ∈ [−0.3, 0.3], <em>x</em> ∈ [0, 200] mm), using feedback from the tactile sensor to maintain the object on its desired path.</p>
<p>The tactile robot successfully pushed the object along its desired trajectory, with a typical mean Euclidean distance the actual trajectory from its intended trajectory of ~10 mm.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Averaged training performance of learned policies for different sensors in (a) edge-following, (b) surface-following, (c) object-pushing tasks.</p>
<p>Table IV
Mean Euclidean distances of the actual trajectories from the ground-truth trajectories for the object pushing task. The numbers in bold denote the best result among the three sensors for the same object and trajectory. Failure cases are denoted "N/A". All the experiments are using the objects without additional weights except the final row "DIGIT $\backslash$ (weighted)" where we increase all the objects' weights by 150 g .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Trajectories</th>
<th style="text-align: center;">Straight</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Curve</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sine</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cube</td>
<td style="text-align: center;">Cylinder</td>
<td style="text-align: center;">Tri. Prism</td>
<td style="text-align: center;">Cube</td>
<td style="text-align: center;">Cylinder</td>
<td style="text-align: center;">Tri. Prism</td>
<td style="text-align: center;">Cube</td>
<td style="text-align: center;">Cylinder</td>
<td style="text-align: center;">Tri. Prism</td>
</tr>
<tr>
<td style="text-align: center;">Tactip</td>
<td style="text-align: center;">10.33 mm</td>
<td style="text-align: center;">9.21 mm</td>
<td style="text-align: center;">11.51 mm</td>
<td style="text-align: center;">12.19 mm</td>
<td style="text-align: center;">11.29 mm</td>
<td style="text-align: center;">13.20 mm</td>
<td style="text-align: center;">11.93 mm</td>
<td style="text-align: center;">11.30 mm</td>
<td style="text-align: center;">13.89 mm</td>
</tr>
<tr>
<td style="text-align: center;">DigiTac</td>
<td style="text-align: center;">11.25 mm</td>
<td style="text-align: center;">10.24 mm</td>
<td style="text-align: center;">16.09 mm</td>
<td style="text-align: center;">13.01 mm</td>
<td style="text-align: center;">11.20 mm</td>
<td style="text-align: center;">16.41 mm</td>
<td style="text-align: center;">12.32 mm</td>
<td style="text-align: center;">11.48 mm</td>
<td style="text-align: center;">15.13 mm</td>
</tr>
<tr>
<td style="text-align: center;">DIGIT</td>
<td style="text-align: center;">11.20 mm</td>
<td style="text-align: center;">10.13 mm</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">12.94 mm</td>
<td style="text-align: center;">12.00 mm</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">12.41 mm</td>
<td style="text-align: center;">11.33 mm</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">DIGIT $\backslash$ (weighted)</td>
<td style="text-align: center;">10.92 mm</td>
<td style="text-align: center;">11.00 mm</td>
<td style="text-align: center;">16.65 mm</td>
<td style="text-align: center;">12.28 mm</td>
<td style="text-align: center;">11.51 mm</td>
<td style="text-align: center;">16.58 mm</td>
<td style="text-align: center;">12.07 mm</td>
<td style="text-align: center;">11.53 mm</td>
<td style="text-align: center;">17.06 mm</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. The tactile robot executing 3 pushing policies: (a) triangular prism along a curve trajectory with the DigiTac, (b) cube along a straight line with the DIGIT, (c) cylinder along a sinusoidal trajectory with the TacTip. The plots in the most left columns are the objects actual paths recorded by the tracking system; the right four columns are snapshots taken from the camera.</p>
<p>Table V
Mean Euclidean distances of the trajectories from the ground truth for the specific object (triangle prism) pushing task using DIGIT. The number in bold denotes the best result among the weights.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Traj.</th>
<th style="text-align: center;">Straight</th>
<th style="text-align: center;">Curve</th>
<th style="text-align: center;">Sine</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Weights</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">$185 \mathrm{~g}+0 \mathrm{~g}$</td>
<td style="text-align: center;">18.05 mm</td>
<td style="text-align: center;">17.73 mm</td>
<td style="text-align: center;">17.95 mm</td>
</tr>
<tr>
<td style="text-align: center;">$185 \mathrm{~g}+50 \mathrm{~g}$</td>
<td style="text-align: center;">16.93 mm</td>
<td style="text-align: center;">17.54 mm</td>
<td style="text-align: center;">18.12 mm</td>
</tr>
<tr>
<td style="text-align: center;">$185 \mathrm{~g}+150 \mathrm{~g}$</td>
<td style="text-align: center;">16.65 mm</td>
<td style="text-align: center;">16.58 mm</td>
<td style="text-align: center;">17.06 mm</td>
</tr>
<tr>
<td style="text-align: center;">$185 \mathrm{~g}+200 \mathrm{~g}$</td>
<td style="text-align: center;">17.17 mm</td>
<td style="text-align: center;">16.92 mm</td>
<td style="text-align: center;">17.26 mm</td>
</tr>
</tbody>
</table>
<p>(Table IV), compared to an overall distance travelled of 200280 mm ( 250 steps in total for each episode) and a sensor tip size of $20-40 \mathrm{~mm}$. The successful task performance is also indicated by snapshots taken from the trajectories (Fig. 5) and videos are provided in supplementary material. Overall, the performance when successfully pushing the objects is similar to that reported in [1].</p>
<p>Examining the results more closely, the TacTip performs slightly better than the DigiTac with accuracies of $9-13 \mathrm{~mm}$ compared with $11-16 \mathrm{~mm}$, which we attribute to a stabler push due to the larger contact surface. The DIGIT has similar accuracy to the DigiTac, but failed at the pushing task with the triangular prism for all trajectories. We hypothesise that the main reason of this failure is that the triangular prism is the lightest object (185g) and the DIGIT has a relatively stiff elastomer compared with the TacTip and DigiTac, which causes the tactile image translation to fail on this object. To validate this hypothesis, we extend the experiments with the DIGIT to pushing the triangle prism with additional weights
(ranging from $50-200 \mathrm{~g}$ ) using the same deep RL and GAN models. The results (Table V) show that the performance improves with increased object weight upto 150 g after which there was no benefit. We expect this is because the additional weight helps the tactile images lie within the distribution of the GAN training for accurate real-to-sim transfer. Once in this range, the bottleneck on the performance is the RL policy instead of the GAN.
2) Edge Following: Next, we consider the edge following task, where the tactile robot must slide the sensor around the edge of various planar objects with geometrical features such as curved edges and a corner. We note that the sim-to-real image translation was trained only on a straight edge, but as we see below the method generalizes to more complex shapes.</p>
<p>In all cases, the tactile robot successfully completed the edge-following trajectory, with typical mean Euclidean position errors of $0.6-1.4 \mathrm{~mm}$ for the DigiTac and TacTip, and $0.9-1.8 \mathrm{~mm}$ for the DIGIT (Table VI). The successful task performance is shown by the trajectories superimposed on the ground truth shapes (Fig. 6) and videos are provided in supplementary material. Again these results are comparable to those reported in [1] and also for servo control under supervised learning of the pose [34].</p>
<p>Examining the results more closely, the DIGIT can achieve accurate performance (mean Euclidean position errors of 1.5 mm overall) when traversing the edge contours despite having some regions of larger error (Fig. 6 top row, coloured regions up to 5 mm error). This is because the flatter stiffer elastomer of the DIGIT sensing surface causes an increased sensitivity to small deviations in penetration distance while moving around the object, relative to the softer TacTip and DigiTac sensing surfaces. For sufficient deformation of the DIGIT sensor to give good performance, a more forceful contact needs to be applied, which also increases the frictional force. To avoid damaging the sensor, we mitigate this friction by coating the objects with wax to ease the sliding motion.
3) Surface Following: Finally, we consider the surface following task, where the tactile sensor must slide around the curved surfaces of various objects with vertical walls, which have geometrical features such as concave and convex surfaces. We note that the sim-to-real image translation was trained only on a planar surface, but as we see below the method generalizes to more complex shapes.</p>
<p>The tactile robot successfully completed the surfacefollowing trajectory, with typical mean Euclidean position errors of $0.6-1.2 \mathrm{~mm}$ for the DigiTac and TacTip, and the</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. The tactile robot executing edge-following policies on 3 distinct shapes for the (a) DIGIT, (b) DigiTac and (c) TacTip. The ground truth for each object is shown in green and errors of the traced contour from the ground truth are colour-coded (side colour bar). The blue arrow denotes the starting point and direction.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. The tactile robot executing 3 surface-following policies on 3 distinct shapes, corresponding to the (a) DIGIT, (b) DigiTac and (c) TacTip. The ground truth for each object is shown in green and errors of the traced contour from the ground truth are colour-coded (side colour bar). The blue arrow denotes the starting point and direction. The DIGIT failed to follow the arch and flower objects at points denoted by the red crosses.</p>
<p>DIGIT giving the most accurate trajectory of 0.5 mm error on the circular wall (compared to 0.6 mm for the other sensors). The successful task performance is shown by the trajectories superimposed on the ground truth shapes (Fig. 7) and videos are provided in supplementary material. Again these results are comparable to those reported in [1] and also for servo control under supervised learning of the pose [34].</p>
<p>During the experiments, we observed that the DIGIT tends to get stuck in the concave-shape surface (shown in the supplementary video). This is again because of the DIGIT's flatter, stiffer sensing surface which hinders its sliding movement over concave surfaces. To avoid breaking the sensor, we decided not to conduct this task with DIGIT on the flower and the arch.</p>
<p>Table VI</p>
<p>Mean Euclidean distances of the trajectories from the ground truth for the edge following task. The best result from the three sensors is shown in bold.</p>
<table>
<thead>
<tr>
<th>DigiTac</th>
<th>Square</th>
<th>Clover</th>
<th>Foil</th>
</tr>
</thead>
<tbody>
<tr>
<td>DigiTac</td>
<td>0.88 mm</td>
<td>1.71 mm</td>
<td>1.82 mm</td>
</tr>
<tr>
<td>TacTip</td>
<td>1.04 mm</td>
<td>0.85 mm</td>
<td>0.86 mm</td>
</tr>
<tr>
<td></td>
<td>0.63 mm</td>
<td>1.42 mm</td>
<td>0.67 mm</td>
</tr>
</tbody>
</table>
<p>Table VII</p>
<p>Mean Euclidean distances of the actual trajectories from the ground truth trajectories for the surface following task. The number in bold denotes the best result from the three sensors. Failure cases are indicated by "N/A".</p>
<table>
<thead>
<tr>
<th>Sensors</th>
<th>Arch</th>
<th>Flower</th>
<th>Circle</th>
</tr>
</thead>
<tbody>
<tr>
<td>DigiTac</td>
<td>N/A</td>
<td>N/A</td>
<td>0.47 mm</td>
</tr>
<tr>
<td>TacTip</td>
<td>0.79 mm</td>
<td>1.04 mm</td>
<td>0.58 mm</td>
</tr>
<tr>
<td></td>
<td>0.91 mm</td>
<td>1.23 mm</td>
<td>0.59 mm</td>
</tr>
</tbody>
</table>
<h2>V. DISCUSSION AND FUTURE WORK</h2>
<p>In this paper, we developed a low-cost tactile robot platform for sim-to-real deep reinforcement learning based on Tactile Gym [1]. The hardware included a desktop robot (DOBOT MG400) and three low-cost high-resolution optical tactile sensors: the TacTip, DIGIT, and DigiTac. We also integrated CAD models of the DOBOT MG400 and the considered sensors into the Tactile Gym and successfully learned policies for all sensors in several physically-interactive tasks involving different contact dynamics. To train an effective GAN model for real-to-sim image translation, we fine-tuned the image preprocessing parameters and calibrated the internal camera of each simulated tactile sensor so that the distribution of the simulated dataset was well-matched with the real dataset.</p>
<p>The performance of our low-cost tactile sim-to-real deep RL robot platform was evaluated in three real-world physically-interactive tasks: edge-following, surface-following and object-pushing. The experimental results show that the developed platform is effective for all tasks with zero-shot performance on real objects or trajectories unseen in the simulation learning for all three tactile sensors. The main differences in performance between the sensors were due to the physical construction and material properties, rather than the different sensing mechanisms. For example, the flatter, stiffer construction the DIGIT with a GelSight-type sensing surface made it unsuited for following concave surfaces, unlike the soft domed structure of the TacTip and DigiTac.</p>
<p>Such empirical studies should help other researchers select and customize the appropriate physical characteristics of tactile sensors for different manipulation scenarios. Overall, we view the generality of our low-cost platform as opening up the possibility to apply either TacTip-style or GelSight-style tactile sensors to learning general sim-to-real deep RL policies for desired complex behaviors. The tactile robot platform should also benefit sim-to-real prehensile and dexterous manipulation tasks, for example by enabling the fundamental methods to be developed in controlled scenarios before applying them to more challenging applications with dexterous robot hands.</p>
<p>Acknowledgements: We thank Mike Lambeta and Roberto Calandra for donating the DIGIT sensors. We thank Di Wu for</p>
<p>her preliminary work on the DOBOT hardware development. We thank Andrew Stinchcombe for helping with the 3Dprinting of the stimuli.</p>
<h2>REFERENCES</h2>
<p>[1] A. Church, J. Lloyd, R. Hadsell, and N. Lepora. Tactile Sim-to-Real Policy Transfer via Real-to-Sim Image Translation. In Proceedings of the 5th Conference on Robot Learning, pages 1645-1654. PMLR, October 2021.
[2] Niko Sünderhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter Fox, Jürgen Leitner, Ben Upcroft, Pieter Abbeel, Wolfram Burgard, Michael Milford, et al. The limits and potentials of deep learning for robotics. The International journal of robotics research, 37(4-5):405420, 2018.
[3] Siyuan Dong, Devesh K Jha, Diego Romeres, Sangwoon Kim, Daniel Nikovski, and Alberto Rodriguez. Tactile-rl for insertion: Generalization to objects of unknown geometry. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 6437-6443. IEEE, 2021.
[4] Huazhe Xu, Yuping Luo, Shaoxiong Wang, Trevor Darrell, and Roberto Calandra. Towards learning to play piano with dexterous hands and touch. arXiv preprint arXiv:2106.02040, 2021.
[5] Shaoxiong Wang, Mike Maroje Lambeta, Po-Wei Chou, and Roberto Calandra. Tacto: A fast, flexible, and open-source simulator for highresolution vision-based tactile sensors. IEEE Robotics and Automation Letters, 2022.
[6] Mike Lambeta, Huazhe Xu, Jingwei Xu, Po-Wei Chou, Shaoxiong Wang, Trevor Darrell, and Roberto Calandra. Pytouch: A machine learning library for touch processing. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13208-13214. IEEE, 2021.
[7] Mike Lambeta, Po-Wei Chou, Stephen Tian, Brian Yang, Benjamin Maloon, Victoria Rose Most, Dave Stroud, Raymond Santos, Ahmad Byagowi, Gregg Kammerer, et al. Digit: A novel design for a lowcost compact high-resolution tactile sensor with application to in-hand manipulation. IEEE Robotics and Automation Letters, 5(3):3838-3845, 2020.
[8] Benjamin Ward-Cherrier, Nicholas Pestell, Luke Cramphorn, Benjamin Winstone, Maria Elena Giannaccini, Jonathan Rossiter, and Nathan F Lepora. The tactip family: Soft optical tactile sensors with 3d-printed biomimetic morphologies. Soft robotics, 5(2):216-227, 2018.
[9] Nathan F. Lepora. Soft biomimetic optical tactile sensing with the tactip: A review. IEEE Sensors Journal, 21(19):21131-21143, 2021.
[10] Herke Van Hoof, Nutan Chen, Maximilian Karl, Patrick van der Smagt, and Jan Peters. Stable reinforcement learning with autoencoders for tactile and visual data. In 2016 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 3928-3934. IEEE, 2016.
[11] Filipe Veiga, Dominik Notz, Thomas Hesse, and Jan Peters. Tactile based forward modeling for contact location control. In RSS Workshop on Tactile Sensing for Manipulation, 2017.
[12] Stephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur Mudigonda, Chelsea Finn, Roberto Calandra, and Sergey Levine. Manipulation by feel: Touch-based control with deep predictive models. In 2019 International Conference on Robotics and Automation (ICRA), pages 818-824. IEEE, 2019.
[13] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pages 1587-1596. PMLR, 2018.
[14] Sangwoon Kim and Alberto Rodriguez. Active extrinsic contact sensing: Application to general peg-in-hole insertion. arXiv preprint arXiv:2110.03555, 2021.
[15] Alex Church, John Lloyd, Raia Hadsell, and Nathan F Lepora. Deep reinforcement learning for tactile robotics: Learning to type on a braille keyboard. IEEE Robotics and Automation Letters, 5(4):6145-6152, 2020.
[16] Yashraj S Narang, Balakumar Sundaralingam, Karl Van Wyk, Arsalan Mousavian, and Dieter Fox. Interpreting and predicting tactile signals for the syntouch biotac. arXiv preprint arXiv:2101.05452, 2021.
[17] Yashraj Narang, Balakumar Sundaralingam, Miles Macklin, Arsalan Mousavian, and Dieter Fox. Sim-to-real for robotic tactile sensing via physics-based simulation and learned latent projections. arXiv preprint arXiv:2103.16747, 2021.
[18] Carmelo Sferrazza, Thomas Bi, and Raffaello D'Andrea. Learning the sense of touch in simulation: a sim-to-real strategy for visionbased tactile sensing. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4389-4396. IEEE, 2020.
[19] Carmelo Sferrazza and Raffaello D'Andrea. Sim-to-real for highresolution optical tactile sensing: From images to 3d contact force distributions. arXiv preprint arXiv:2012.11295, 2020.
[20] Thomas Bi, Carmelo Sferrazza, and Raffaello D'Andrea. Zero-shot sim-to-real transfer of tactile control policies for aggressive swing-up manipulation. IEEE Robotics and Automation Letters, 2021.
[21] Zihan Ding, Nathan F. Lepora, and Edward Johns. Sim-to-Real Transfer for Optical Tactile Sensing. ICRA, pages 1639-1645, 2020.
[22] Daniel Fernandes Gomes, Paolo Paoletti, and Shan Luo. Generation of gelsight tactile images for sim2real learning. IEEE Robotics and Automation Letters, 6(2):4177-4184, 2021.
[23] Zilin Si and Wenzhen Yuan. Taxim: An example-based simulation model for gelsight tactile sensors. IEEE Robotics and Automation Letters, 2022.
[24] Alexander C Abad and Anuradha Ranasinghe. Visuotactile sensors with emphasis on gelsight sensor: A review. IEEE Sensors Journal, 20(14):7628-7638, 2020.
[25] Wenzhen Yuan, Siyuan Dong, and Edward H Adelson. Gelsight: High-resolution robot tactile sensors for estimating geometry and force. Sensors, 17(12):2762, 2017.
[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-toimage translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125-1134, 2017.
[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234-241. Springer, 2015.
[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.
[29] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. PMLR, 2015.
[30] John Lloyd and Nathan F Lepora. Goal-driven robotic pushing using tactile and proprioceptive feedback. IEEE Transactions on Robotics, 2021.
[31] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.
[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[33] Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann. Stable baselines3, 2019.
[34] Nathan F Lepora and John Lloyd. Pose-based tactile servoing: Controlled soft touch using deep learning. IEEE Robotics \&amp; Automation Magazine, 28(4):43-55, 2021.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1295 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1295</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1295</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-272880764</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.16611v1.pdf" target="_blank">Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Humanoid robots offer significant versatility for performing a wide range of tasks, yet their basic ability to walk and run, especially at high velocities, remains a challenge. This letter presents a novel method that combines deep reinforcement learning with kinodynamic priors to achieve stable locomotion control (KSLC). KSLC promotes coordinated arm movements to counteract destabilizing forces, enhancing overall stability. Compared to the baseline method, KSLC provides more accurate tracking of commanded velocities and better generalization in velocity control. In simulation tests, the KSLC-enabled humanoid robot successfully tracked a target velocity of 3.5 m/s with reduced fluctuations. Sim-to-sim validation in a high-fidelity environment further confirmed its robust performance, highlighting its potential for real-world applications.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1295.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1295.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Humanoid-Gym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Humanoid-Gym</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning simulator/environment used to train whole-body humanoid locomotion policies; used in this paper as the primary training environment for the KSLC method with massively parallel environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Humanoid-Gym</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An RL training environment for humanoid robots that supports parallelized rollouts and fast training; used here as the low-fidelity training simulator for learning locomotion policies (XBot-L) with PPO and curriculum strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (humanoid locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity (training-focused, faster/simplified physics for large-scale RL)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Paper treats it as a lower-fidelity simulator compared to MuJoCo; supports parallel instances (4096 envs used), simplified/efficient physical modeling suitable for large-scale PPO training and curriculum learning; specific omissions or simplifications are not exhaustively enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>XBot-L policy (KSLC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep reinforcement learning agent (actor-critic network trained with Proximal Policy Optimization) implementing the KSLC method (DRL augmented with kinodynamic priors and velocity-based rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn stable, high-speed humanoid locomotion policies that precisely track commanded forward velocities (velocity-tracking and gait adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Agent trained in Humanoid-Gym achieved accurate velocity tracking up to ~3.5 m/s during simulation tests and learning curves indicate improved tracking and stability versus baseline (quantitative plots shown; specific numeric loss/return curves not transcribed).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Higher-fidelity simulator (MuJoCo) for zero-shot sim-to-sim transfer; intended as an analogue to sim-to-real.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Policies trained in Humanoid-Gym with KSLC maintained consistent or superior performance in MuJoCo; the XBot-L tracked a commanded velocity of 4.0 m/s in MuJoCo without falling (zero-shot), and arm movement patterns and gait relationships matched those from Humanoid-Gym.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>The paper reports a sim-to-sim transfer from Humanoid-Gym (training, lower-fidelity) to MuJoCo (evaluation, higher-fidelity): KSLC-trained policies transferred with consistent or even improved performance in the higher-fidelity simulator, successfully tracking higher commanded velocities (4.0 m/s) without falling.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit statement of a minimum fidelity requirement is provided; authors note that the gap between simulators is analogous to sim-to-real and mitigate discrepancies using domain randomization, but they do not identify which exact simulator features are necessary vs unnecessary for successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>The paper reports that the baseline method (trained in the same Humanoid-Gym) failed to track high commanded velocities and defaulted to conservative low-velocity locomotion; no explicit sim-to-sim transfer failure of KSLC is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1295.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1295.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used high-fidelity physics simulator for articulated bodies and contact dynamics; employed here as the high-fidelity target environment for zero-shot sim-to-sim validation of policies trained in Humanoid-Gym.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A physics simulator emphasizing realistic articulated rigid-body dynamics and contact modeling; used here as the high-fidelity evaluation environment to validate transfer of locomotion policies trained in a lower-fidelity simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (humanoid locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity (more realistic physical properties and dynamics relative to the training environment)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Described in the paper as having more realistic physical properties than the training environment; used to evaluate contact dynamics, whole-body dynamics, and gait behaviors under more realistic physics; exact modelling details are not enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>XBot-L policy (KSLC) evaluated in MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The same DRL actor-critic policy (PPO-trained KSLC with kinodynamic priors) trained in Humanoid-Gym, evaluated zero-shot in MuJoCo.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Evaluate transfer robustness of learned locomotion strategies: assess whether policies trained in a lower-fidelity simulator maintain stable, high-velocity locomotion under more realistic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Evaluation environment for policies trained in Humanoid-Gym (sim-to-sim transfer); serves as an analogue for sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>KSLC policies transferred successfully: the humanoid tracked 4.0 m/s in MuJoCo without falling and exhibited gait and arm-movement patterns consistent with training, indicating robust zero-shot sim-to-sim transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>When evaluated in MuJoCo, KSLC-trained policies retained or improved locomotion performance compared to their behavior in Humanoid-Gym; this supports the claim that the training approach generalizes across the tested fidelity gap.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper does not provide a concrete analysis of the minimum fidelity required; authors instead apply domain randomization to reduce simulator-to-simulator/real-world gaps and treat MuJoCo as a higher-fidelity benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure of sim-to-sim transfer for KSLC is reported; baseline policies trained in Humanoid-Gym exhibited poor high-velocity tracking during training, but no MuJoCo-specific failure cases are documented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer. <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world. <em>(Rating: 2)</em></li>
                <li>Learning to walk in minutes using massively parallel deep reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Real-world humanoid locomotion with reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1295",
    "paper_id": "paper-272880764",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Humanoid-Gym",
            "name_full": "Humanoid-Gym",
            "brief_description": "A reinforcement-learning simulator/environment used to train whole-body humanoid locomotion policies; used in this paper as the primary training environment for the KSLC method with massively parallel environments.",
            "citation_title": "Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer.",
            "mention_or_use": "use",
            "simulator_name": "Humanoid-Gym",
            "simulator_description": "An RL training environment for humanoid robots that supports parallelized rollouts and fast training; used here as the low-fidelity training simulator for learning locomotion policies (XBot-L) with PPO and curriculum strategies.",
            "scientific_domain": "mechanics / robotics (humanoid locomotion)",
            "fidelity_level": "low-fidelity (training-focused, faster/simplified physics for large-scale RL)",
            "fidelity_characteristics": "Paper treats it as a lower-fidelity simulator compared to MuJoCo; supports parallel instances (4096 envs used), simplified/efficient physical modeling suitable for large-scale PPO training and curriculum learning; specific omissions or simplifications are not exhaustively enumerated in the paper.",
            "model_or_agent_name": "XBot-L policy (KSLC)",
            "model_description": "Deep reinforcement learning agent (actor-critic network trained with Proximal Policy Optimization) implementing the KSLC method (DRL augmented with kinodynamic priors and velocity-based rewards).",
            "reasoning_task": "Learn stable, high-speed humanoid locomotion policies that precisely track commanded forward velocities (velocity-tracking and gait adaptation).",
            "training_performance": "Agent trained in Humanoid-Gym achieved accurate velocity tracking up to ~3.5 m/s during simulation tests and learning curves indicate improved tracking and stability versus baseline (quantitative plots shown; specific numeric loss/return curves not transcribed).",
            "transfer_target": "Higher-fidelity simulator (MuJoCo) for zero-shot sim-to-sim transfer; intended as an analogue to sim-to-real.",
            "transfer_performance": "Policies trained in Humanoid-Gym with KSLC maintained consistent or superior performance in MuJoCo; the XBot-L tracked a commanded velocity of 4.0 m/s in MuJoCo without falling (zero-shot), and arm movement patterns and gait relationships matched those from Humanoid-Gym.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "The paper reports a sim-to-sim transfer from Humanoid-Gym (training, lower-fidelity) to MuJoCo (evaluation, higher-fidelity): KSLC-trained policies transferred with consistent or even improved performance in the higher-fidelity simulator, successfully tracking higher commanded velocities (4.0 m/s) without falling.",
            "minimal_fidelity_discussion": "No explicit statement of a minimum fidelity requirement is provided; authors note that the gap between simulators is analogous to sim-to-real and mitigate discrepancies using domain randomization, but they do not identify which exact simulator features are necessary vs unnecessary for successful transfer.",
            "failure_cases": "The paper reports that the baseline method (trained in the same Humanoid-Gym) failed to track high commanded velocities and defaulted to conservative low-velocity locomotion; no explicit sim-to-sim transfer failure of KSLC is reported.",
            "uuid": "e1295.0",
            "source_info": {
                "paper_title": "Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact)",
            "brief_description": "A widely-used high-fidelity physics simulator for articulated bodies and contact dynamics; employed here as the high-fidelity target environment for zero-shot sim-to-sim validation of policies trained in Humanoid-Gym.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo",
            "simulator_description": "A physics simulator emphasizing realistic articulated rigid-body dynamics and contact modeling; used here as the high-fidelity evaluation environment to validate transfer of locomotion policies trained in a lower-fidelity simulator.",
            "scientific_domain": "mechanics / robotics (humanoid locomotion)",
            "fidelity_level": "high-fidelity (more realistic physical properties and dynamics relative to the training environment)",
            "fidelity_characteristics": "Described in the paper as having more realistic physical properties than the training environment; used to evaluate contact dynamics, whole-body dynamics, and gait behaviors under more realistic physics; exact modelling details are not enumerated in the paper.",
            "model_or_agent_name": "XBot-L policy (KSLC) evaluated in MuJoCo",
            "model_description": "The same DRL actor-critic policy (PPO-trained KSLC with kinodynamic priors) trained in Humanoid-Gym, evaluated zero-shot in MuJoCo.",
            "reasoning_task": "Evaluate transfer robustness of learned locomotion strategies: assess whether policies trained in a lower-fidelity simulator maintain stable, high-velocity locomotion under more realistic dynamics.",
            "training_performance": null,
            "transfer_target": "Evaluation environment for policies trained in Humanoid-Gym (sim-to-sim transfer); serves as an analogue for sim-to-real transfer",
            "transfer_performance": "KSLC policies transferred successfully: the humanoid tracked 4.0 m/s in MuJoCo without falling and exhibited gait and arm-movement patterns consistent with training, indicating robust zero-shot sim-to-sim transfer.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "When evaluated in MuJoCo, KSLC-trained policies retained or improved locomotion performance compared to their behavior in Humanoid-Gym; this supports the claim that the training approach generalizes across the tested fidelity gap.",
            "minimal_fidelity_discussion": "The paper does not provide a concrete analysis of the minimum fidelity required; authors instead apply domain randomization to reduce simulator-to-simulator/real-world gaps and treat MuJoCo as a higher-fidelity benchmark.",
            "failure_cases": "No explicit failure of sim-to-sim transfer for KSLC is reported; baseline policies trained in Humanoid-Gym exhibited poor high-velocity tracking during training, but no MuJoCo-specific failure cases are documented.",
            "uuid": "e1295.1",
            "source_info": {
                "paper_title": "Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer.",
            "rating": 2,
            "sanitized_title": "humanoidgym_reinforcement_learning_for_humanoid_robot_with_zeroshot_sim2real_transfer"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world.",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Learning to walk in minutes using massively parallel deep reinforcement learning.",
            "rating": 1,
            "sanitized_title": "learning_to_walk_in_minutes_using_massively_parallel_deep_reinforcement_learning"
        },
        {
            "paper_title": "Real-world humanoid locomotion with reinforcement learning.",
            "rating": 1,
            "sanitized_title": "realworld_humanoid_locomotion_with_reinforcement_learning"
        },
        {
            "paper_title": "Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation.",
            "rating": 1,
            "sanitized_title": "humanoidbench_simulated_humanoid_benchmark_for_wholebody_locomotion_and_manipulation"
        }
    ],
    "cost": 0.00833725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning
25 Sep 2024</p>
<p>Xinming Zhang xm_zhang@mail.ustc.edu.cn 
School of Com-puter Science and Technology
Xinming Zhang and Guodong Guo are with
University of Science and Technology of China
HefeiChina</p>
<p>Xinming Zhang
Lerong ZhangXianghui Wang, Guodong Guo</p>
<p>Xianghui Wang xhwang@eitech.edu.cn 
Xinming Zhang
Lerong ZhangXianghui Wang, Guodong Guo</p>
<p>Lerong Zhang zhanglr7@outlook.com 
Guodong Guo gdguo@eitech.edu.cn 
School of Com-puter Science and Technology
Xinming Zhang and Guodong Guo are with
University of Science and Technology of China
HefeiChina</p>
<p>Xinming Zhang
Lerong ZhangXianghui Wang, Guodong Guo</p>
<p>Xiaoyu Shen xyshen@eitech.edu.cn 
Xinming Zhang
Lerong ZhangXianghui Wang, Guodong Guo</p>
<p>Wei Zhang 
Xinming Zhang
Lerong ZhangXianghui Wang, Guodong Guo</p>
<p>Xinming Zhang
Lerong ZhangXianghui Wang, Guodong Guo</p>
<p>Ningbo Institute of Digital Twin
Eastern Institute of Technology
NingboChina</p>
<p>Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning
25 Sep 2024FD2F41C1196AF2C76D35AA7E4BC93AE7arXiv:2409.16611v1[cs.RO]This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.Humanoid and Bipedal LocomotionHumanoid Robot SystemsMachine Learning for Robot Control
Humanoid robots offer significant versatility for performing a wide range of tasks, yet their basic ability to walk and run, especially at high velocities, remains a challenge.This letter presents a novel method that combines deep reinforcement learning with kinodynamic priors to achieve stable locomotion control (KSLC).KSLC promotes coordinated arm movements to counteract destabilizing forces, enhancing overall stability.Compared to the baseline method, KSLC provides more accurate tracking of commanded velocities and better generalization in velocity control.In simulation tests, the KSLC-enabled humanoid robot successfully tracked a target velocity of 3.5 m/s with reduced fluctuations.Sim-to-sim validation in a high-fidelity environment further confirmed its robust performance, highlighting its potential for real-world applications.</p>
<p>I. INTRODUCTION</p>
<p>I N human-centered environments, the human-like skeletal structure of humanoid robots allows for better maneuverability in complex terrains and human-designed buildings.This enables them to perform a wider range of tasks compared to wheeled robots by utilizing existing human infrastructure.The locomotion capacity of humanoid robots is crucial for performing multiple tasks in the real world.However, applying classical control methods to humanoid robot locomotion tasks requires considerable manual effort to handle specific situations.Deep Reinforcement Learning (DRL)-based methods, on the other hand, derive optimal control strategies through automatic iterative trial-and-error in simulated environments [1], resulting in greater robustness and less manual design effort.Notably, parallel training for DRL can significantly reduce the required training time, while reducing hardware costs and improving computational efficiency.Consequently, DRL has emerged as a popular methodology in the field of robotics control [2], [3], [4].</p>
<p>Despite advances in humanoid robotics, controlling locomotion remains a substantial challenge due to the complexity of their larger action space, which often leads to instability [5].While DRL-based methods have shown remarkable progress, their success has primarily been limited to low-velocity locomotion (approximately 1.0 m/s) [6], [7], and typically with fewer actuated joints.When upper-body joints are fully engaged, the robot's movements can become unpredictable, resulting in unnatural and erratic twisting behavior [8].In this letter, we challenge the conventional approach of disabling the arm joints during locomotion training [7], [8].Instead, we demonstrate that leveraging the arm joints can substantially improve the effectiveness of training for both walking and running.</p>
<p>At high speeds, humanoid robots frequently fall due to imbalanced angular momentum, primarily caused by the swinging leg in the global yaw direction [9].The primary goal of this work is to achieve precise and stable locomotion control, preventing the development of a wobbly gait or falls.</p>
<p>To achieve this, we develop a novel method called Kinodynamic-regularized Stable Locomotion Control (KSLC), a DRL-based approach for stabilizing humanoid robots.The KSLC integrates DRL with prior kinodynamic knowledge to discover the optimal control policy.Specifically, we design a reward function that minimizes angular momentum [10], encouraging the robot to move forward using its legs while generating corresponding arm movements and elbow flexion to help stabilize its body in the global yaw direction.</p>
<p>Additionally, we observed suboptimal velocity-tracking performance in the humanoid robot across a wide range of commanded velocities when fixed rewards were used.This limitation arises because fixed rewards tend to emphasize a single motion pattern.To improve the robot's velocity-tracking capability and develop a control policy with better generalization, particularly at higher speeds, we introduce velocitybased rewards.By incorporating a velocity-related term into the reward function, we aim to overcome the limitations of fixed rewards, allowing the robot to adapt its motion patterns to varying commanded velocities.</p>
<p>We conducted a series of comprehensive evaluation experiments in the Humanoid-Gym [7] using XBot-L.The results, which demonstrate accurate velocity tracking at 3.5 m/s, showed the effectiveness of the KSLC method.In additional sim-to-sim validation experiments, the KSLC maintained con-sistent or even superior performance in a simulator with more realistic physical properties, successfully tracking a wider range of commanded velocities.This indicates its potential for deployment in real-world scenarios.The contributions of this work are summarized as follows:</p>
<p>• This study develops the KSLC, a DRL-based humanoid locomotion control method enhanced with kinodynamic priors, improving locomotion stability in humanoid robots.</p>
<p>II. RELATED WORK</p>
<p>A. DRL-based Humanoid Locomotion Control</p>
<p>In recent years, there has been a significant increase in research on legged robot locomotion utilizing DRL methods.Given the complex structure and high manufacturing costs of humanoid robots, conducting trial-and-error experiments directly in the real world is both expensive and risky.Compared to quadrupedal robots, humanoid robots face additional challenges due to their higher center-of-mass (CoM) and greater number of joints, which complicates DRL-based control.</p>
<p>Radosavovic et al. [6] proposed a learning-based approach for controlling humanoid robots using a causal transformer model that predicts the robot's next action based on its history of proprioceptive observations and actions.Zhang et al. [11] tackled the complexity of humanoid locomotion by introducing an adversarial motion prior-based imitation learning framework.Cheng et al. [5] developed an approach that enhances upper-body control by closely following reference motions, while relaxing constraints on the lower body to improve robustness and adaptability.Taylor et al. [12] presented a DRL method to teach bipedal robots human-like movements directly from motion capture data, bridging the gap between simulation and reality.Their method incorporates motion retargeting during training and employs domain randomization techniques to address differences in joint configurations and physical systems.</p>
<p>However, to the best of our knowledge, existing methods have not adequately addressed the impact of upper-body movements on whole-body kinodynamics in achieving accurate and stable locomotion control in humanoid robots.Furthermore, the ability to achieve precise high-velocity tracking in humanoid robot locomotion has not yet been reported.Drawing inspiration from Raibert [10], we analyze how humans maintain stability while moving at high speeds and propose a DRL method that incorporates kinodynamic priors for humanoid robot locomotion control, utilizing only a limited number of joints.</p>
<p>B. Angular Momentum-based Humanoid Control</p>
<p>The angular momentum-based control method was introduced decades ago to enhance walking efficiency and performance, specifically by minimizing angular momentum around the CoM during the support phase [10].Although methods such as DRL and MPC have become mainstream, it is still valuable to consider real-world physical effects.Several studies have analyzed angular momentum and other physical quantities during human motion from a biomechanical perspective [13], [14], [15].Following these biomechanical studies, some methods combining MPC with angular momentum analysis have been developed.Kojio et al. [16] proposed a gait modification method that considers not only the position of the steps but also their timing and angular momentum.Specifically, this method represented the walkable area as a convex hull and calculated the robot's foot placement, step timing, and angular momentum for each step.Ding et al. [17] introduced a Nonlinear Model Predictive Control (NMPC) framework integrating multiple balance strategies, including step location adjustment, CoM height variation, and angular momentum adaptation, for maintaining stability under dynamic disturbances.However, methods that combine the angular momentum-based control method with DRL have not been explored.</p>
<p>III. PRELIMINARIES</p>
<p>A. Problem Formulation</p>
<p>The primary objective of this work is to achieve a precise control of locomotion velocity in humanoid robots, particularly in high-speed scenarios.Specifically, we aim to find an optimal control policy π * that minimizes the difference between the robot's actual velocity and the commanded velocity, as defined by (1).Here, v π represents the velocity of the humanoid robot under the control of policy π, and v cmd is the commanded velocity.This problem can be formulated as a sequential decisionmaking problem for achieving the precise velocity control.At time t, the input x t ∈ X of the agent contains the current and history information of the humanoid robot, such as periodic clock signals, proprioceptive sensor data, and commanded velocities.The action a t ∈ A refers to the target joint positions of the robot.Given x t , the humanoid robot executes the action a t based on the current control policy π.Upon receiving a new observation at time t + 1, the agent incorporates this information into the history, updates the input to x t+1 , and receives a reward r t from the reward function.
π * = argmin π ∥v π − v cmd ∥ 1(1)
We employ DRL to train a deep neural network (DNN), π θ , to approximate the optimal control policy π * , where θ represents the parameters of the model.These parameters are optimized by maximizing the expected total rewards during training.The DRL algorithm used in this study is Proximal Policy Optimization (PPO) [18], chosen for its efficiency and suitability for parallel computation.</p>
<p>B. Kinodynamics Priors</p>
<p>Humanoid robots have numerous movable joints, which, if left unrestricted, can often result in instability during motion.In kinodynamics, maintaining stability requires minimizing the overall angular momentum of the robot to prevent such instability.Rather than fixing the upper-body joints, we encourage their use to counterbalance the angular momentum generated by the legs.Specifically, we allow the shoulder pitch and elbow pitch joints of XBot-L, as shown in Fig. 2, to generate angular momentum that counteracts the leg swing.</p>
<p>IV. METHODS</p>
<p>Recent studies have demonstrated that humanoid robots can maintain stable walking at low velocities [6], [7], [11].However, the lack of consideration for the effects of angular momentum on whole-body stability during walking and other dynamic tasks often leads to unnatural movements or even falls when tracking higher velocities [8].To address these instability challenges in high-velocity locomotion, we combine DRL with kinodynamics and develop an angular momentum-based approach.This approach effectively utilizes the upper-body joints to counterbalance the angular momentum generated by the lower body at higher commanded velocities [19], [20], mitigating the negative impact seen when only lower-body joints are engaged.As a result, the overall stability of the humanoid robot is significantly enhanced, as illustrated in Fig. 3.</p>
<p>At varying commanded velocities, especially at high speeds, fixed reward functions gradually become inadequate for the demands of rapid locomotion in humanoid robots, as they typically focus on a single motion pattern, such as walking.We observed that fixed reward functions lead the robot to struggle with tracking higher velocities, often defaulting to slower walking behaviors.This failure is due to insufficient rewards or early terminations, prompting the robot to adopt overly conservative strategies at higher speeds.To address this issue, we propose velocity-related reward functions designed to improve velocity-tracking performance across a wide range of commanded speeds.</p>
<p>The total reward is a weighted sum of three categories of rewards: 1) angular momentum-based rewards, 2) velocityrelated rewards, and 3) general rewards.Specifically, the expected total reward at time t is given by:
r t = α a r a t + α v r v t + α c r c t (2)
where α a , α v , and α c are coefficients that control the weighting of each reward component in the total reward function.</p>
<p>In our work, α a and α v are set to 0.05 and 0.1, respectively, with the remaining coefficients following those used in [7].</p>
<p>A. Angular Momentum-based Reward</p>
<p>According to the kinodynamics of humanoid robot locomotion, the angular momentum generated by the swinging motion of the robot's legs is significant and cannot be ignored.</p>
<p>Moreover, this angular momentum increases with the robot's velocity, especially in the yaw direction [9].The total angular momentum is the sum of the angular momentum produced by the robot's individual links during locomotion, as described in (3),
L total = n i=1 (c i × (m i ċi ) + I i ω i )(3)
where L total represents the total angular momentum, n is the number of links, I i is the inertia tensor of the i th link's CoM, ω i denotes the angular velocity, and c i and ċi represent the position and velocity of the i th link's CoM relative to the whole-body CoM, respectively.. The substantial angular momentum generated during highvelocity locomotion presents a considerable challenge to precise control, especially in dynamic movements, where high stability is essential.To maintain postural equilibrium, we designed a reward function to minimize angular momentum for more stable and efficient locomotion.The angular momentumbased reward is defined in (4),
r a t = clip(− exp (∥A a t ∥ 2 ) , c 1 , c 2 )(4)
where ∥A a t ∥ 2 represents the L 2 norm of the angular momentum at time t.The function clip(ψ, c 1 , c 2 ) constrains the output of the function ψ to mitigate extreme values.Here, ψ signifies any function that necessitates the clipping operation, with c 1 and c 2 indicating the lower and upper bounds of the output value, respectively.</p>
<p>B. Velocity-related Reward</p>
<p>In locomotion tasks, reward functions play a critical role in shaping the behavior of humanoid robots.Previous studies often rely on fixed reward functions, which limits the humanoid robots' ability to accurately track velocities across a broad range of commanded velocities.To enhance the generalization capabilities of humanoid robots, we propose velocity-related reward functions that promote the learning of more dynamically adaptive behaviors.These functions enable precise control even over a wide range of commanded velocities.The rewards are categorized as follows: 1) base height reward, 2) feet clearance reward, and 3) joint position reward.In our approach, the coefficients α b , α f , and α p are set to 0.2, 1.0, and 1.6, respectively.
r v t = α b r b t + α f r f t + α p r p t (5)
1) Base Height: In human locomotion, the CoM height is automatically adjusted in preparation for subsequent steps [21].In contrast, humanoid robots typically maintain a fixed base height, which restricts the range of motion patterns they can learn during training.To overcome this limitation, we develop a reward that enables dynamic adjustment of the robot's base height during training.This approach significantly enhances the robot's responsiveness to changes in velocity and improves stability at higher velocities.The base height reward is calculated as follows:
r b t = exp −β b t ∥h b t − γ b t ĥb t g(v cmd t , v max t )∥ 2(6)
where h b t and ĥb t denote the current and target base heights at time t, respectively.The coefficients β b t and γ b t are set to 100 and 0.05 in our work.The function g(v cmd t , v max t ) is defined as v cmd t divided by v max t , where v cmd t is the commanded velocity and v max t is the maximum velocity of curriculum at time t.</p>
<p>2) Feet Clearance: Intuitively, maintaining appropriate feet clearance and ensuring an optimal distance between the swinging leg and the ground can help prevent tripping, thus enhancing the humanoid robot's ability to navigate challenging terrain, particularly in environments with obstacles.To facilitate this, we propose a reward function that incorporates both feet clearance and locomotion velocity.The reward function is defined as follows:
r f t = ξ(t) • h f t − ĥf t g(v cmd t , v max t )(7)
where h f t and ĥf t denote the current and target heights of feet, respectively.Following [7], ξ(t) represents a swing mask function that characterizes the swing phase of the gait cycle.</p>
<p>3) Joint Position: In high-velocity locomotion control tasks, accurate velocity tracking is influenced by stride length.By examining the relationship between joint angles and locomotion velocity of the humanoid robot, we develop a reward to improve velocity tracking by facilitating an appropriate increase in joint angles, thereby enabling longer strides.The reward is expressed as follows:
r p t = exp(β p t ∥( θp t − θ p t ) exp(− θp t )∥ 2 ) − γ p t clip(∥( θp t − θ p t ) exp(− θp t )∥ 2 , c 1 , c 2 )(8)
where θ p t and θp t denote the current and target joint positions, respectively.The term exp(− θp t ) is a velocity-related coefficient that adjusts the importance of the velocity-related term.The parameters β p t and γ p t are the tolerances for joint position tracking error, which are set to −2 and 0.2 in this work.The constants c 1 and c 2 , used to truncate excessively large values, are set to 0 and 0.5, respectively.</p>
<p>C. Curriculum Learning Strategy</p>
<p>1) Velocity Curriculum: During DRL training, the humanoid robot learns a policy by tracking various commanded velocities to develop velocity tracking capabilities.However, as the range of commanded velocities expands, the humanoid robot faces challenges in accurately tracking high-velocity commands during rapid locomotion.This difficulty arises because the humanoid robot may struggle to earn rewards if the early training is dominated by high-velocity commands, potentially leading to failure in rapid locomotion tasks [4].To address this issue, we trained the humanoid robot using lowvelocity commands initially and progressively expanded the velocity range as per the curriculum [22].</p>
<p>In addition, we employ a reward-based update rule rather than a fixed one to automatically adjust the curriculum during training, eliminating the need for manual adjustments [23], [24].The update rule f v t is defined as follows: where [v min t , v max t ] is the minimum and maximum values of the current linear velocity command in the forward direction, r trk t and r trk, max t are the linear velocity tracking reward and the maximum achievable linear velocity tracking reward at time t.If the threshold λr trk, max t is satisfied, the velocity command interval will be updated to expand the sampling range of the velocity command.In this work, λ is set to 0.8.
[v min t+1 , v max t+1 ] ← f v t ([v min t , v max t ], r trk t ), r trk t ≥ λr trk, max t [v min t , v max t ], otherwise.(9)
2) Cycle Time Curriculum: The gait cycle determines the stride frequency of the humanoid robot and is crucial for achieving high-speed locomotion.The fixed gait cycle limits the robot's ability to track higher commanded velocities, potentially leading to failure in maintaining the commanded velocities or causing the humanoid robot to fall.To address this limitation, we propose a curriculum learning strategy focused on cycle time, which encourages the humanoid robot to progressively shorten leg movement cycle times as commanded velocity increases, thereby enhancing stride frequency.The update rule f c t is shown as follows:
τ c t+1 ← f c t (τ c t , r trk t ), r trk t ≥ λr trk, max t τ c t , otherwise. (10)
where τ c t is the cycle time at time t, the update condition follows the same as (9).Our function encourages the robot to adjust and modulate the gait cycle duration in response to varying commanded velocities, ensuring the optimal stride frequency achieved.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we illustrate the training procedure conducted in the Humanoid-Gym simulator and the subsequent sim-to-sim validation in a high-fidelity simulator.We trained a policy using the KSLC method within the Humanoid-Gym simulation environment [7] on a workstation equipped with an NVIDIA 4090 GPU.To further validate the effectiveness of KSLC, we performed zero-shot sim-to-sim experiments in a high-fidelity simulator.II.</p>
<p>We trained our policy using five different random seeds.The detailed hyperparameter settings for the algorithms and environment are listed in Table I.The KSLC method significantly improves the policies' ability to accurately track commanded velocities, as illustrated in Fig. 4. We quantified the tracking performance by averaging the humanoid robot's velocity during tests at each commanded velocity.If the robot fell during testing, the average velocity was recorded as 0 m/s.The results demonstrated that the robot controlled by the KSLC method accurately followed the commanded velocities while maintaining greater stability, as evidenced by smaller fluctuations in the zero-commanded velocity directions.In contrast, the baseline method failed to track high commanded velocities, resorting instead to conservative low-velocity locomotion, as shown in Fig. 5.</p>
<p>2) Domain Randomization: To minimize the gap between simulators and the real world, addressing issues such as modeling discrepancies and unexpected disturbances, we employed domain randomization to train a more robust control policy [25].The effectiveness of the policy trained using domain randomization was validated in a high-fidelity simulator, as detailed in the following sections.</p>
<p>B. Sim-to-sim Validation</p>
<p>To further evaluate the performance of the KSLC, we conducted sim-to-sim experiments, as access to a physical humanoid robot for sim-to-real testing was unavailable.The disparity between low-fidelity and high-fidelity simulators can be considered analogous to the gap between simulation and real-world conditions.Transferring a policy learned in a lowfidelity simulator to a high-fidelity simulator is known as sim-to-sim transfer [7].As demonstrated in [7], agents that perform well in sim-to-sim transfer often keep satisfactory performance in sim-to-real transfer.Following this approach, MuJoCo was employed as the high-fidelity simulator in our sim-to-sim experiments.A video of the experiment is available at https://youtu.be/uVT8Up8vAKc and in the supplementary materials.</p>
<p>The sim-to-sim transfer results demonstrate that the policy trained with the KSLC in the Humanoid-Gym maintained consistent or even superior performance in the MuJoCo.The XBot-L successfully tracked a commanded velocity of 4.0 m/s without falling.As illustrated in Fig. 6, the arm movement patterns remained consistent with those observed in the Humanoid-Gym.Furthermore, the humanoid robot utilizing the KSLC method exhibited distinct behaviors at different commanded velocities, supporting its anticipated stability during high-speed locomotion.Fig. 6.Different gaits at 2.0 m/s, 3.5 m/s commanded velocities in the Humanoid-Gym and high-fidelity simulator.The interval between captured photos is 0.25 s, and the length of each grid is 1.0 m.Our method shows the relationship between gait and velocity in both simulation environments.The video of the experiment is available at https://youtu.be/uVT8Up8vAKc and in the supplementary material.</p>
<p>VI. CONCLUSION</p>
<p>In this work, we have proposed the KSLC, a method for achieving stable locomotion for humanoid robots by integrating DRL with kinodynamic priors, based on our analysis of the factors that enable humans to move both stably and quickly.By incorporating these priors, the KSLC method has facilitated the exploration of effective locomotion behaviors, with improved stability and enhanced velocity tracking during rapid locomotion.Extensive experimental results have demonstrated that the KSLC method enables humanoid robots to track a broader range of commanded velocities, up to 3.5 m/s, with reduced fluctuations compared to the baseline.In sim-to-sim validation, the humanoid robot enabled by the KSLC has consistently maintained its performance, exhibiting locomotion behaviors similar to those observed in the training environment.Inspired by the effectiveness of the KSLC, we plan to incorporate more human-inspired priors to further enhance the performance of humanoid robots.</p>
<p>Fig. 1 .
1
Fig. 1.(a) Training environments: Humanoid-Gym [7].(b) Actor-Critic network structure.</p>
<p>Fig. 2 .
2
Fig. 2. Humanoid robot XBot-L and Degrees-of-Freedom (DoF) configuration [7].</p>
<p>Fig. 3 .
3
Fig. 3. Angular momentum of the arms and legs under two typical locomotion status.Yellow arrows and blue arrows indicate the direction of angular momentum generated in the arms and legs, respectively.</p>
<p>Fig. 4 .
4
Fig. 4. Learning curves of the humanoid robot XBot-L trained with the KSLC and the baseline method at different commanded velocities.</p>
<p>Fig. 5 .
5
Fig.5.Performance of the humanoid robot XBot-L across various metrics when tracking different commanded velocities, using the policy trained with the KSLC and baseline method in the Humanoid-Gym[7].</p>
<p>TABLE I SUMMARY
I
OF HYPERPARAMETERS
ParameterValueNumber of Environments4096Mini-batch size61440Discount Factor0.994GAE discount factor0.9Entropy Regularization Coefficient0.001Angular momentum factor-0.05Learning rate1e-5Frame Stack of Single Observation15Frame Stack of Single Privileged Observation3Number of Single Observation59Number of Single Privileged Observation89TABLE IISUMMARY OF OBSERVATION SPACEComponentsDims Observation StateClock Input (sin(t), cos(t))2✓✓Commands ( Ṗx,y,γ )3✓✓Joint Position (θ)16✓✓Joint Velocity ( θ)16✓✓Angular Velocity ( Ṗ b αβγ )3✓✓Euler Angle (P b αβγ )3✓✓Last Actions (a t−1 )16✓✓Frictions1✓Body Mass1✓Base Linear Velocity3✓Push Force2✓Push Torques3✓Tracking Difference16✓Periodic Stance Mask2✓Feet Contact detection2✓
This work is supported by 2035 Key Research and Development Program of Ningbo City under Grant No.2024Z127.
Rl+ model-based control: Using on-demand optimal control to learn versatile legged locomotion. D Kang, J Cheng, M Zamora, F Zargarbashi, S Coros, IEEE Robotics and Automation Letters. 2023</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Conference on Robot Learning. PMLR2022</p>
<p>Anymal parkour: Learning agile navigation for quadrupedal robots. D Hoeller, N Rudin, D Sako, M Hutter, Science Robotics. 98875662024</p>
<p>Rapid locomotion via reinforcement learning. G B Margolis, G Yang, K Paigwar, T Chen, P , The International Journal of Robotics Research. 4342024</p>
<p>Expressive whole-body control for humanoid robots. X Cheng, Y Ji, J Chen, R Yang, G Yang, X Wang, Robotics: Science and Systems. 2024</p>
<p>Real-world humanoid locomotion with reinforcement learning. I Radosavovic, T Xiao, B Zhang, T Darrell, J Malik, K Sreenath, Science Robotics. 98995792024</p>
<p>Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer. X Gu, Y.-J Wang, J Chen, arXiv:2404.056952024arXiv preprint</p>
<p>Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. C Sferrazza, D.-M Huang, X Lin, Y Lee, P Abbeel, Robotics: Science and Systems. 2024</p>
<p>Upper-body control and mechanism of humanoids to compensate for angular momentum in the yaw direction based on human running. T Otani, K Hashimoto, S Miyamae, H Ueta, A Natsuhara, M Sakaguchi, Y Kawakami, H.-O Lim, A Takanishi, Applied Sciences. 81442018</p>
<p>Legged robots that balance. M H Raibert, 1986MIT press</p>
<p>Whole-body humanoid robot locomotion with human reference. Q Zhang, P Cui, D Yan, J Sun, Y Duan, A Zhang, R Xu, arXiv:2402.182942024arXiv preprint</p>
<p>Learning bipedal robot locomotion from human movement. M Taylor, S Bashkirov, J F Rico, I Toriyama, N Miyada, H Yanagisawa, K Ishizuka, 2021</p>
<p>Angular momentum regulation during human walking: biomechanics and control. M Popovic, A Hofmann, H Herr, 2004 IEEE International Conference on Robotics and Automation. 2004</p>
<p>Angular momentum during unexpected multidirectional perturbations delivered while walking. D Martelli, V Monaco, L B Luciani, S Micera, IEEE Transactions on Biomedical Engineering. 6072013</p>
<p>Angular momentum of walking at different speeds. B C Bennett, S D Russell, P Sheth, M F Abel, Human movement science. 2912010</p>
<p>Footstep modification including step time and angular momentum under disturbances on sparse footholds. Y Kojio, Y Omori, K Kojima, F Sugai, Y Kakiuchi, K Okada, M Inaba, IEEE Robotics and Automation Letters. 532020</p>
<p>Nonlinear model predictive control for robust bipedal locomotion: exploring angular momentum and com height changes. J Ding, C Zhou, S Xin, X Xiao, N G Tsagarakis, Advanced Robotics. 35182021</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017arXiv preprint</p>
<p>Whole body movement: coordination of arms and legs in walking and running. R N Hinrichs, 1990Multiple muscle systems: biomechanics and movement organization</p>
<p>Upper extremity function in running. ii: Angular momentum considerations. R N Hinrichs, Journal of Applied Biomechanics. 331987</p>
<p>Humans adjust the height of their center of mass within one step when running across camouflaged changes in ground level. M Ernst, M Götze, R Blickhan, R Müller, Journal of biomechanics. 842019</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science robotics. 54759862020</p>
<p>Teacher-student curriculum learning. T Matiisen, A Oliver, T Cohen, J Schulman, IEEE transactions on neural networks and learning systems. 201931</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017. 2017</p>            </div>
        </div>

    </div>
</body>
</html>
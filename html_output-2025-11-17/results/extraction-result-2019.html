<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2019 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2019</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2019</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-47.html">extraction-schema-47</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <p><strong>Paper ID:</strong> paper-281195374</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.05249v1.pdf" target="_blank">COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization</a></p>
                <p><strong>Paper Abstract:</strong> The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI’s problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules – surpassing concurrent datasets by several orders of magnitude – across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide base-line experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field. 2</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2019.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2019.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CompGen study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compositional Generalization Study (C1-C3) in COGITAO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of experiments that test models' ability to generalize to novel compositions of known atomic transformations (C1: atomic+composite -> unseen composite; C2: restricted composites -> unseen composite; C3: composites -> deeper composites). Evaluations report in-domain (ID) and out-of-domain (OOD) grid-accuracy and ID-OOD gaps across three baseline models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>visual reasoning (grid-based object transformations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Vanilla-ViT, Grid-ViT, LLaDA (baseline encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>No explicit curriculum training; experiments vary training data composition per setting: C1 trains on a mixture of atomic (depth=1) and composite (depth=2) tasks, C2 trains only on restricted composite tasks (depth=2), C3 trains on depth=2 composites and tests on deeper composites (depth=3). Training is end-to-end supervised with a task code (token sequence) provided in CompGen.</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td>Primitive transformations were included in training for some settings (C1 includes explicit atomic tasks; each experiment's primitive pool typically contains 3 atomic transformations such as {translate_up, rotate_90, mirror_horizontal}); training dataset size per experiment was 100,000 unique samples; primitives were not trained separately in isolated stages—no separate mastery threshold was required before composition training.</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Training: depth = 1 (atomic) and/or depth = 2 (composite); OOD tests include unseen depth-2 sequences (C1/C2) and depth-3 sequences (C3).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Each experiment uses a small pool of atomic transformations (typically 3 per experiment) whose mutual compositions produce multiple composite rules; 5 different experiment instances per setting (different transformation sets) were evaluated to ensure robustness; total training examples per experiment = 100k; the full generator supports 28 primitives but each CompGen experiment used constrained subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Aggregated in-domain (ID) grid accuracy (Table 1 averages across experiments C1-C3): Vanilla-ViT: C1 ID 28.4%, C2 ID 34.0%, C3 ID 15.1%; Grid-ViT: C1 ID 71.1%, C2 ID 69.0%, C3 ID 75.6%; LLaDA: C1 ID 54.2%, C2 ID 49.2%, C3 ID 78.9%. (These ID values quantify performance on compositions seen during training or drawn from the training distribution.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Aggregated out-of-domain (OOD) grid accuracy (unseen compositions): Vanilla-ViT: C1 OOD 15.7%, C2 OOD 15.6%, C3 OOD 1.0%; Grid-ViT: C1 OOD 18.7%, C2 OOD 18.6%, C3 OOD 5.1%; LLaDA: C1 OOD 26.2%, C2 OOD 15.2%, C3 OOD 6.4%. (These OOD values correspond to novel orders or increased depth not observed during training.)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Reported ID−OOD gaps (percentage points) averaged per setting: C1 gaps — Vanilla: 12.7 pp, Grid: 52.4 pp, LLaDA: 28.0 pp. C2 gaps — Vanilla: 18.4 pp, Grid: 50.4 pp, LLaDA: 34.0 pp. C3 gaps — Vanilla: 14.1 pp, Grid: 70.5 pp, LLaDA: 72.5 pp. (See Table 1 in paper for aggregated values.)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Strong negative scaling: going from training depth (2) to testing depth (3) produced dramatic OOD performance collapse (C3 OOD accuracies near 1–6% for many models), indicating a steep (super-linear) decrease in OOD performance with increased composition depth.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>No formal curriculum-vs-baseline controlled comparison was performed. The study contrasts training-set composition variants (C1 mixture of atomic+composite vs C2 composite-only) but does not implement staged/temporal curricula or adaptive pacing; differences between C1 and C2 are reported but not framed as curriculum efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Models achieve substantially higher ID grid-accuracy than OOD on novel compositions; large ID→OOD gaps are systematic across architectures. Grid-ViT attains high ID performance but still shows large OOD gaps (particularly severe when extrapolating depth in C3). LLaDA sometimes attains relatively better OOD on some settings (e.g., some C1/C2 cases) but still fails on depth extrapolation. Training on atomic examples in addition to composites (C1) does not eliminate large OOD failures on unseen compositions. Overall, novel ordering and increased depth of composition cause dramatic generalization failures.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - provides empirical evidence of a compositional generalization gap: despite seeing primitives and some composite examples, models fail to systematically generalize to novel compositions and deeper compositions, consistent with the Compositional Generalization Gap Theory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2019.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2019.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EnvGen study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Environmental Generalization Study (G1-G5) in COGITAO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of experiments that fix transformation sequences and vary environmental factors (number of objects, grid size, object size, object complexity, and combined difficulty) to test whether models can apply learned transformations under environment distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>visual reasoning (grid-based object transformations under environmental shifts)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Vanilla-ViT, Grid-ViT, LLaDA (baseline encoders) and ResNet reported in appendix</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>No curriculum; models trained end-to-end on fixed transformation sequences with training sampled across constrained environment parameters (e.g., training grids with 1–2 objects, or sizes 10x10–15x15) then OOD tested on more complex environment variants (e.g., more objects, larger grids, larger objects, more complex objects, or combined shifts).</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Not applicable for EnvGen experiments (transformations per experiment are fixed; composition depth is constant within each experiment), though fixed sequences were used (examples: translate_up, rotate_90, mirror_horizontal, crop_top_side, extend_contours_same_color).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>For each EnvGen experiment, a single fixed transformation was used per experiment instance (5 transformation choices across G1–G5). Training sets contained 100k samples with variability only in the environmental parameters under the 'in-domain' ranges; OOD test ranges present systematic shifts in those environment axes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Aggregated in-domain (ID) grid accuracy by setting (Table 1): G1 ID — Vanilla-ViT 98.3%, Grid-ViT 99.2%, LLaDA 99.4%; G2 ID — Vanilla 68.1%, Grid 97.0%, LLaDA 99.3%; G3 ID — Vanilla 57.1%, Grid 78.5%, LLaDA 76.9%; G4 ID — Vanilla 47.6%, Grid 75.2%, LLaDA 76.3%; G5 ID — Vanilla 70.0%, Grid 98.0%, LLaDA 71.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Aggregated out-of-domain (OOD) grid accuracy by setting (Table 1): G1 OOD — Vanilla 76.7%, Grid 90.0%, LLaDA 89.1%; G2 OOD — Vanilla 0.0% (note: formatting shows 0.0 in Table 1 for Vanilla G2 OOD), Grid 73.0%, LLaDA 64.3%; G3 OOD — Vanilla 17.5%, Grid 27.2%, LLaDA 27.2%; G4 OOD — Vanilla 21.3%, Grid 13.8%, LLaDA 35.6%; G5 OOD — Vanilla 0.0%, Grid 0.2%, LLaDA 14.0%. (See paper tables for per-experiment breakdown; some reported zeros indicate near-total failure under the tested environment shift for the given architecture/setting.)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Reported ID−OOD gaps (percentage points) averaged per setting: G1 gaps — Vanilla: 21.6 pp, Grid: 9.2 pp, LLaDA: 10.3 pp. G2 gaps — Vanilla: 68.1 pp, Grid: 24.0 pp, LLaDA: 35.0 pp. G3 gaps — Vanilla: 39.6 pp, Grid: 51.3 pp, LLaDA: 49.7 pp. G4 gaps — Vanilla: 26.3 pp, Grid: 61.4 pp, LLaDA: 40.7 pp. G5 gaps — Vanilla: 70.0 pp, Grid: 97.8 pp, LLaDA: 57.0 pp. (From Table 1 aggregated rows.)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>No curriculum experiments; no direct comparison between curricula and baseline end-to-end training in EnvGen. The paper only contrasts ID vs OOD performance under environmental shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td>Not explicitly analyzed; paper notes ResNet sometimes generalizes better in EnvGen because environmental shifts preserve local pixel statistics that convolutional inductive biases can exploit, suggesting possible reliance on spurious/local cues rather than compositional rules.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Environmental distribution shifts cause substantial performance degradation in many settings, though severity depends on model inductive biases and the nature of the shift: Grid-ViT and LLaDA generally maintain higher OOD performance on some environment shifts (e.g., G1), while Vanilla-ViT and ResNet can fail catastrophically on others (notably some G2 and G5 cases). Some shifts (e.g., increased object count or larger grids) can be handled reasonably well by models with appropriate spatial inductive biases, but combined multi-axis shifts cause the largest drops.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - demonstrates that generalization failure is not only about compositional composition depth but also about environmental systematic shifts; inductive biases (convolutional vs grid-specific transformer vs language-model style) modulate the size of the generalization gap, so the compositional gap theory needs to account for architecture-environment interactions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2019.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2019.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum learning (COGITAO mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mentioned suitability for Curriculum Learning in COGITAO (discussion/future work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper suggests that the COGITAO framework is well-suited to curriculum learning (gradually increasing task complexity) as a future research direction, but the authors did not run curriculum-learning experiments in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>visual reasoning (proposal / future experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>Suggested: gradual increase of task complexity (curriculum), e.g., primitive-first then compositions, but no implemented protocol; the paper explicitly recommends exploring staged curricula and in-context demonstration strategies in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Authors note COGITAO's controllable difficulty and abundant data make it a natural testbed for curriculum learning (e.g., gradually increasing transformation depth or environment difficulty), but they did not evaluate curricula in the present experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - authors propose curriculum learning as a potential approach to close compositional generalization gaps, but provide no empirical evidence in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks <em>(Rating: 2)</em></li>
                <li>SCAN: A Diagnostic Dataset for Compositional Generalization <em>(Rating: 2)</em></li>
                <li>COGS: A Compositional Generalization Challenge Based on Semantic Interpretation <em>(Rating: 2)</em></li>
                <li>CFQ: Compositional Freebase Questions (Measuring compound divergence) <em>(Rating: 2)</em></li>
                <li>A Benchmark for Compositional Visual Reasoning (CVR) <em>(Rating: 2)</em></li>
                <li>Good-Enough Compositional Data Augmentation <em>(Rating: 1)</em></li>
                <li>Discovering modular solutions that generalize compositionally <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2019",
    "paper_id": "paper-281195374",
    "extraction_schema_id": "extraction-schema-47",
    "extracted_data": [
        {
            "name_short": "CompGen study",
            "name_full": "Compositional Generalization Study (C1-C3) in COGITAO",
            "brief_description": "A set of experiments that test models' ability to generalize to novel compositions of known atomic transformations (C1: atomic+composite -&gt; unseen composite; C2: restricted composites -&gt; unseen composite; C3: composites -&gt; deeper composites). Evaluations report in-domain (ID) and out-of-domain (OOD) grid-accuracy and ID-OOD gaps across three baseline models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "visual reasoning (grid-based object transformations)",
            "agent_or_model_name": "Vanilla-ViT, Grid-ViT, LLaDA (baseline encoders)",
            "curriculum_structure": "No explicit curriculum training; experiments vary training data composition per setting: C1 trains on a mixture of atomic (depth=1) and composite (depth=2) tasks, C2 trains only on restricted composite tasks (depth=2), C3 trains on depth=2 composites and tests on deeper composites (depth=3). Training is end-to-end supervised with a task code (token sequence) provided in CompGen.",
            "primitive_training_details": "Primitive transformations were included in training for some settings (C1 includes explicit atomic tasks; each experiment's primitive pool typically contains 3 atomic transformations such as {translate_up, rotate_90, mirror_horizontal}); training dataset size per experiment was 100,000 unique samples; primitives were not trained separately in isolated stages—no separate mastery threshold was required before composition training.",
            "composition_depth_range": "Training: depth = 1 (atomic) and/or depth = 2 (composite); OOD tests include unseen depth-2 sequences (C1/C2) and depth-3 sequences (C3).",
            "compositional_diversity_description": "Each experiment uses a small pool of atomic transformations (typically 3 per experiment) whose mutual compositions produce multiple composite rules; 5 different experiment instances per setting (different transformation sets) were evaluated to ensure robustness; total training examples per experiment = 100k; the full generator supports 28 primitives but each CompGen experiment used constrained subsets.",
            "performance_trained_compositions": "Aggregated in-domain (ID) grid accuracy (Table 1 averages across experiments C1-C3): Vanilla-ViT: C1 ID 28.4%, C2 ID 34.0%, C3 ID 15.1%; Grid-ViT: C1 ID 71.1%, C2 ID 69.0%, C3 ID 75.6%; LLaDA: C1 ID 54.2%, C2 ID 49.2%, C3 ID 78.9%. (These ID values quantify performance on compositions seen during training or drawn from the training distribution.)",
            "performance_novel_compositions": "Aggregated out-of-domain (OOD) grid accuracy (unseen compositions): Vanilla-ViT: C1 OOD 15.7%, C2 OOD 15.6%, C3 OOD 1.0%; Grid-ViT: C1 OOD 18.7%, C2 OOD 18.6%, C3 OOD 5.1%; LLaDA: C1 OOD 26.2%, C2 OOD 15.2%, C3 OOD 6.4%. (These OOD values correspond to novel orders or increased depth not observed during training.)",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Reported ID−OOD gaps (percentage points) averaged per setting: C1 gaps — Vanilla: 12.7 pp, Grid: 52.4 pp, LLaDA: 28.0 pp. C2 gaps — Vanilla: 18.4 pp, Grid: 50.4 pp, LLaDA: 34.0 pp. C3 gaps — Vanilla: 14.1 pp, Grid: 70.5 pp, LLaDA: 72.5 pp. (See Table 1 in paper for aggregated values.)",
            "composition_depth_scaling": "Strong negative scaling: going from training depth (2) to testing depth (3) produced dramatic OOD performance collapse (C3 OOD accuracies near 1–6% for many models), indicating a steep (super-linear) decrease in OOD performance with increased composition depth.",
            "curriculum_vs_baseline_comparison": "No formal curriculum-vs-baseline controlled comparison was performed. The study contrasts training-set composition variants (C1 mixture of atomic+composite vs C2 composite-only) but does not implement staged/temporal curricula or adaptive pacing; differences between C1 and C2 are reported but not framed as curriculum efficacy.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Models achieve substantially higher ID grid-accuracy than OOD on novel compositions; large ID→OOD gaps are systematic across architectures. Grid-ViT attains high ID performance but still shows large OOD gaps (particularly severe when extrapolating depth in C3). LLaDA sometimes attains relatively better OOD on some settings (e.g., some C1/C2 cases) but still fails on depth extrapolation. Training on atomic examples in addition to composites (C1) does not eliminate large OOD failures on unseen compositions. Overall, novel ordering and increased depth of composition cause dramatic generalization failures.",
            "supports_or_challenges_theory": "supports - provides empirical evidence of a compositional generalization gap: despite seeing primitives and some composite examples, models fail to systematically generalize to novel compositions and deeper compositions, consistent with the Compositional Generalization Gap Theory.",
            "uuid": "e2019.0"
        },
        {
            "name_short": "EnvGen study",
            "name_full": "Environmental Generalization Study (G1-G5) in COGITAO",
            "brief_description": "A set of experiments that fix transformation sequences and vary environmental factors (number of objects, grid size, object size, object complexity, and combined difficulty) to test whether models can apply learned transformations under environment distribution shifts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "visual reasoning (grid-based object transformations under environmental shifts)",
            "agent_or_model_name": "Vanilla-ViT, Grid-ViT, LLaDA (baseline encoders) and ResNet reported in appendix",
            "curriculum_structure": "No curriculum; models trained end-to-end on fixed transformation sequences with training sampled across constrained environment parameters (e.g., training grids with 1–2 objects, or sizes 10x10–15x15) then OOD tested on more complex environment variants (e.g., more objects, larger grids, larger objects, more complex objects, or combined shifts).",
            "primitive_training_details": null,
            "composition_depth_range": "Not applicable for EnvGen experiments (transformations per experiment are fixed; composition depth is constant within each experiment), though fixed sequences were used (examples: translate_up, rotate_90, mirror_horizontal, crop_top_side, extend_contours_same_color).",
            "compositional_diversity_description": "For each EnvGen experiment, a single fixed transformation was used per experiment instance (5 transformation choices across G1–G5). Training sets contained 100k samples with variability only in the environmental parameters under the 'in-domain' ranges; OOD test ranges present systematic shifts in those environment axes.",
            "performance_trained_compositions": "Aggregated in-domain (ID) grid accuracy by setting (Table 1): G1 ID — Vanilla-ViT 98.3%, Grid-ViT 99.2%, LLaDA 99.4%; G2 ID — Vanilla 68.1%, Grid 97.0%, LLaDA 99.3%; G3 ID — Vanilla 57.1%, Grid 78.5%, LLaDA 76.9%; G4 ID — Vanilla 47.6%, Grid 75.2%, LLaDA 76.3%; G5 ID — Vanilla 70.0%, Grid 98.0%, LLaDA 71.0%.",
            "performance_novel_compositions": "Aggregated out-of-domain (OOD) grid accuracy by setting (Table 1): G1 OOD — Vanilla 76.7%, Grid 90.0%, LLaDA 89.1%; G2 OOD — Vanilla 0.0% (note: formatting shows 0.0 in Table 1 for Vanilla G2 OOD), Grid 73.0%, LLaDA 64.3%; G3 OOD — Vanilla 17.5%, Grid 27.2%, LLaDA 27.2%; G4 OOD — Vanilla 21.3%, Grid 13.8%, LLaDA 35.6%; G5 OOD — Vanilla 0.0%, Grid 0.2%, LLaDA 14.0%. (See paper tables for per-experiment breakdown; some reported zeros indicate near-total failure under the tested environment shift for the given architecture/setting.)",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Reported ID−OOD gaps (percentage points) averaged per setting: G1 gaps — Vanilla: 21.6 pp, Grid: 9.2 pp, LLaDA: 10.3 pp. G2 gaps — Vanilla: 68.1 pp, Grid: 24.0 pp, LLaDA: 35.0 pp. G3 gaps — Vanilla: 39.6 pp, Grid: 51.3 pp, LLaDA: 49.7 pp. G4 gaps — Vanilla: 26.3 pp, Grid: 61.4 pp, LLaDA: 40.7 pp. G5 gaps — Vanilla: 70.0 pp, Grid: 97.8 pp, LLaDA: 57.0 pp. (From Table 1 aggregated rows.)",
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": "No curriculum experiments; no direct comparison between curricula and baseline end-to-end training in EnvGen. The paper only contrasts ID vs OOD performance under environmental shifts.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": "Not explicitly analyzed; paper notes ResNet sometimes generalizes better in EnvGen because environmental shifts preserve local pixel statistics that convolutional inductive biases can exploit, suggesting possible reliance on spurious/local cues rather than compositional rules.",
            "negative_transfer_observed": null,
            "key_findings_summary": "Environmental distribution shifts cause substantial performance degradation in many settings, though severity depends on model inductive biases and the nature of the shift: Grid-ViT and LLaDA generally maintain higher OOD performance on some environment shifts (e.g., G1), while Vanilla-ViT and ResNet can fail catastrophically on others (notably some G2 and G5 cases). Some shifts (e.g., increased object count or larger grids) can be handled reasonably well by models with appropriate spatial inductive biases, but combined multi-axis shifts cause the largest drops.",
            "supports_or_challenges_theory": "nuances - demonstrates that generalization failure is not only about compositional composition depth but also about environmental systematic shifts; inductive biases (convolutional vs grid-specific transformer vs language-model style) modulate the size of the generalization gap, so the compositional gap theory needs to account for architecture-environment interactions.",
            "uuid": "e2019.1"
        },
        {
            "name_short": "Curriculum learning (COGITAO mention)",
            "name_full": "Mentioned suitability for Curriculum Learning in COGITAO (discussion/future work)",
            "brief_description": "The paper suggests that the COGITAO framework is well-suited to curriculum learning (gradually increasing task complexity) as a future research direction, but the authors did not run curriculum-learning experiments in this work.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "study_domain": "visual reasoning (proposal / future experimental design)",
            "agent_or_model_name": "N/A",
            "curriculum_structure": "Suggested: gradual increase of task complexity (curriculum), e.g., primitive-first then compositions, but no implemented protocol; the paper explicitly recommends exploring staged curricula and in-context demonstration strategies in future work.",
            "primitive_training_details": null,
            "composition_depth_range": null,
            "compositional_diversity_description": null,
            "performance_trained_compositions": null,
            "performance_novel_compositions": null,
            "generalization_gap_measured": null,
            "generalization_gap_value": null,
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": null,
            "adaptive_curriculum_used": null,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Authors note COGITAO's controllable difficulty and abundant data make it a natural testbed for curriculum learning (e.g., gradually increasing transformation depth or environment difficulty), but they did not evaluate curricula in the present experiments.",
            "supports_or_challenges_theory": "nuances - authors propose curriculum learning as a potential approach to close compositional generalization gaps, but provide no empirical evidence in this paper.",
            "uuid": "e2019.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
            "rating": 2
        },
        {
            "paper_title": "SCAN: A Diagnostic Dataset for Compositional Generalization",
            "rating": 2
        },
        {
            "paper_title": "COGS: A Compositional Generalization Challenge Based on Semantic Interpretation",
            "rating": 2
        },
        {
            "paper_title": "CFQ: Compositional Freebase Questions (Measuring compound divergence)",
            "rating": 2
        },
        {
            "paper_title": "A Benchmark for Compositional Visual Reasoning (CVR)",
            "rating": 2
        },
        {
            "paper_title": "Good-Enough Compositional Data Augmentation",
            "rating": 1
        },
        {
            "paper_title": "Discovering modular solutions that generalize compositionally",
            "rating": 2
        }
    ],
    "cost": 0.01630175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization
5 Sep 2025</p>
<p>Yassine Taoudi-Benchekroun 
Klim Troyan 
Pascal Sager 
Stefan Gerber 
Lukas Tuggener 
Benjamin Grewe </p>
<p>Institute of Neuroinformatics
University of Zurich
ETH Zurich</p>
<p>ETH Zurich</p>
<p>Centre for Artificial Intelligence
Zurich University of Applied Sciences</p>
<p>ETH Zurich</p>
<p>Institute of Neuroinformatics
University of Zurich
ETH Zurich</p>
<p>COGITAO: A Visual Reasoning Framework To Study Compositionality &amp; Generalization
5 Sep 20250E92FD7025FEED017EA59CF092378654arXiv:2509.05249v1[cs.CV]
The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models.To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains.Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments.It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties.This flexibility enables the creation of millions of unique task rules -surpassing concurrent datasets by several orders of magnitude -across a wide range of difficulties, while allowing virtually unlimited sample generation per rule.We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance.COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field. 2</p>
<p>Introduction</p>
<p>Compositionality and generalization are foundational principles of human cognition [1].From observing a few examples of given "atomic" concepts, humans can effortlessly combine these in exponentially many novel ways, and do so in radically different contexts from those in which they were learned.In the words of Lake &amp; Baroni [2]: "Once a person learns the meaning of a new verb 'dax,' he or she can immediately understand the meaning of 'dax twice' or 'sing and dax'."Machine learning systems still struggle with compositionality and generalization, which has become a major focus of research [3,4].Both principles are now often jointly studied as "compositional generalization" [5,6] under the belief that leveraging the compositional nature of the world is key to achieving broader generalization [7] To date, several benchmarks have been proposed to direct the community's efforts on the problem of compositional generalization -both in language [2,8,5] and in vision [9,10,11,12].However, in the vision domain, these benchmarks fall short in flexibility and scope compared to their language counterparts.They offer less control over compositional structure and priors, a narrower range of tasks, and a systematic conflation of visual complexity with relational structure -ultimately diverting attention away from true compositional generalization.</p>
<p>To fill this gap, we introduce the visual reasoning framework of COmpositional Generalization In Transformations And Objects: COGITAO.It is designed to generate simple, object-centric data to precisely gauge the compositional generalization abilities of visual machine learning methods.Specifically, we propose a parametrizable generator able to generate tasks which apply a sequence of atomic transformations onto random objects positioned in grids (see Figure 1).Our generator's strength stems from its capacity to freely compose, at arbitrary depth, a set of 28 atomic transformations -thus enabling the creation of millions of unique transformation sequences, with virtually infinite amount of task samples, across a wide range of difficulties.</p>
<p>In our COGITAO framework, models are expected to learn to apply the sequence of objecttransformations in order to predict output grids given input grids and transformation sequences.Furthermore, models do so in different variations of the environment (e.g.varying the number or type of objects, or the grid dimensions).Our framework becomes valuable when varying the transformation sequences and environment parametrizations between training and testing sets -which is how we propose to gauge models' capabilities to compositionally generalize.Similar to Lake and Baroni's "dax twice" and "sing and dax" analogy [2], we suggest that visual models trained to "rotate" objects once should know how to "rotate twice", as well as "rotate and translate" -provided both rotate and translate transformations were seen during training.</p>
<p>To demonstrate our generator's utility, and to encourage further research, we follow the aforementioned paradigm and provide a set of benchmark datasets built from different configurations of COGITAO.We train state-of-the-art vision models on the proposed benchmark tasks, and find that models consistently fail to generalize to novel combinations of familiar visual elements -despite high performance on training-like examples.These findings reinforce the urgency for methods that can move beyond simple pattern recognition to structured, compositional understanding.</p>
<p>Related Works</p>
<p>Our proposed dataset and methods build upon prior research in compositionality, generalization and reasoning -applied to both language (and other sequential data modalities) and vision data.While these areas are often interconnected, we find that compositional generalization benchmarks in the visual domain lag behind their language counterpart in and flexibility and scope.This is mainly due to the difficulty of procedurally generating visual data of adequate task fidelity compared to language-like data.We note that we are particularly interested in methods that can showcase the emergence of compositional generalization (as opposed to forms of memorization [13]) in an efficient and scalable way by training them with an appropriate amount of data from scratch.Hence, foundation models are considered out of the scope of this work.</p>
<p>Compositional Generalization in Language Several datasets and benchmarks have enabled targeted focus on compositionality and generalization in deep learning architectures for natural language processing (NLP).The SCAN dataset [2] is most akin to our work, but in the language domain.It consists of commands mapped to action sequences.Models must compose commands, both within and outside the training distribution, to execute them correctly.Our dataset is similar to SCAN in that it contains various experiment settings in which the difference between training and testing distribution varies (across primitives, combinations, or sequence length).Several works have shown that models can achieve good performance on SCAN with specific architectures, representation methods, or data augmentation techniques [14,15,16,17], but there is still no consensus on the best architecture for compositional tasks.Analogous to SCAN, COGS [18] is a dataset that combines compositionality and generalization.It focuses on the compositionality of language and tests models on novel combinations of syntactic structures or familiar words.In contrast, our dataset recombines transformations.Similarly, the CFQ dataset [5] builds a dataset that aims to maximize compound divergence while minimizing atomic divergence between train and test sets, which they argue is an optimal setting to study compositionality.Our approach follows this logic, as primitive elements are shared, but their combinations differ between training and test time.Other datasets reproduce these logics in similar 1D settings, such as SQL [19] or mathematical reasoning [20].</p>
<p>Compositional Generalization in Vision</p>
<p>In Vision, compositional generalization is often studied in pair with visual reasoning, which has gained significant traction in recent years.Most aligned to our work are benchmarks such as ARC [21], Raven's Progressive Matrices [22], and Procedurally Generated Matrices [11], which use handcrafted shapes and environments to simplify visual processing in light of reasoning.With similar handcrafted shapes and environments, but with a more targeted focus on compositional generalization as opposed to reasoning, we find other important datasets such as dSprites [23], CLEVR [9], Compositional Visual Reasoning [10], SVRT [12], and SVIB [24].These datasets are all built out of procedurally generated shapes with varying properties (e.g., color, size, texture) and make use of different types of rules, as well as composition of properties to evaluate models' compositional capabilities, and object-centric representations.Most aligned with our objectives is the CVR dataset [10], which focuses on compositional visual relations in an odd-one-out classification format.When CVR proposes a set of 9 primitive attributes and a total of 103 unique reference rules, we propose a set of 28 atomic transformations which can be freely composed at arbitrary depth into a much higher number of unique rules.Other datasets, such as CATER [25], contrary to the aforementioned datasets which use procedurally generated shapes, focus on compositional generalization in real-world visual domain.However, we believe these inevitably conflate compositional reasoning ability with visual complexity, preventing a focused study on compositional generalization.</p>
<p>COGITAO Generator</p>
<p>Our motivation stemmed from attempting to make progress at the original ARC challenge [21].Similarly to the authors of conceptARC [26], we observed that the data regime of the ARC-AGI was too small (low data availability), too diverse (high variance between individual tasks) to make sound scientific progress on some of the abilities crucial to the challenge and lacking from modern deep learning approaches.While some researchers adopted the path of scale, big-data regime and novel inference-time methods such as test-time-training and chain of thoughts reasoning to tackle the challenge [27,28,29], we aimed to follow a more principled approach and evaluate the capabilities of vision architectures to compose and generalize on basic sets of problems.As such, we designed a generator tailored to gauge the compositional generalization abilities of models, specifically through composing object-centric transformations.Our tasks consist of rule-based input-output pairs, where each rule is defined by applying a sequence of transformations to objects arranged on grids of variable sizes.This setup enables the creation of a wide spectrum of tasks -from simple to complex -built from controlled atomic operations.Specifically, we control generalization and compositionality along two axes:</p>
<p>• Compositional Generalization: COGITAO allows to compose multiple object transformations, at various levels of depth (i.e., number of transformations applied in a row).We provide a set of 28 primitive transformations (such as different forms of translations, rotations, and mirroring) which can be combined to an arbitrary depth, thus allowing generations of millions of different task rules, which can be combined in various ways between train and test time.• Environmental Generalization: COGITAO allows to control other parameters which allows to assess "generalization" capabilities of models -that is, whether they can apply the transformations learned in variations of the environment.For this purpose, we allow the user to modify the number of objects per grid, the complexity, size and color of shapes, and the grid size, thereby allowing generation of different versions of the environment between train and test time.</p>
<p>COGITAO Generator</p>
<p>The COGITAO generator is a Python-based generator that allows the generation of object-centric tasks.The environment is grid like, and inspired by the ARC-AGI [21] format, with pixels numbered from 0 to 9. We present an overview of our generator in figure 2.</p>
<p>Object Generation</p>
<p>We pre-generate a set of 23000 distinct objects by randomly iterating through key properties, including size (rows and columns), symmetry (horizontal, vertical, diagonal, point, or none), connectivity (adjacent edges, adjacent and diagonal edges, or none), colors (single or multi-colored), color patterns (column, row, diagonal stripes, uniform, hollow, or random), and object footprint (rectangle, disk, diamond, or ellipse).Unlike datasets such as RPM [22], PGMs [11], and CLEVR [9], which offer a fixed set of objects, our approach generates a vast number of unique objects -this provides a significant advantage for assessing generalization.Moreover, the generated objects are better-suited to object-centric learning research than CVR [10] or SVRT [12], where compositionality is paramount [30,31].</p>
<p>Transformations</p>
<p>We provide a set of 28 object-transformations, which the community can further expand.Each transformation was chosen to respect two core rules: 1) each transformation should be composable with all other transformations, and 2) each transformation should modify the object in a way that should not be reproducible by a combination of other transformations.Rule 1 is critical to ensure that our generator maximizes the number of possible tasks that can be generated and avoids degenerate cases.Rule 2 avoids redundancy in the transformations -for instance, we could have implemented an individual transformation "translate_up_right", but this would always be equivalent to the transformation sequence "translate_up" and "translate_right".Rule 2, however, doesn't imply that each transformation suite necessarily yields a unique output grid that could not have been reached with another transformation suite.For instance, for symmetric objects, applying mirror transformations could yield the same output objects as applying the rotation transformations twice -this is not the case Figure 2: Overview of the COGITAO generator.COGITAO builds and selects transformation sequences, then, given the configuration requested by the user and the selected transformations, proceeds to randomly sample objects and position them in the defined input grid.Once objects are positioned in the input grid, each transformation is sequentially applied to each object, one after another.</p>
<p>for non-symmetric objects, thereby still satisfying Rule 2. We summarize below the transformations available in the generator, and a description of their variations.To generate tasks, our generator allows to randomly sample within a specified pool of transformations, or directly specify the wanted transformation sequence we'd like to generate tasks with.Several (not all) transformations (such as the Crop family of transformations) are not reversible, and as such, the order of the transformations is important.We can define an upper bound on the number of possible tasks that can be generated with a given set of transformations.For instance, with our 28 available transformations, with a maximum transformation depth of k, the number of possible tasks to generate is N = n k transf orms = 28 k tasks.For instance, taking a max depth of k = 5</p>
<p>(while using an adequate3 number of objects and grid size) would yield a theoretical number of n tasks = 28 5 = 1.7 × 10 8 possible tasks.We note that, provided the grid size is set to be large enough, and the generation time is not constrained, the depth of composition of transformations can be arbitrarily large, thereby making the set of possible tasks to be generated in our COGITAO generator very large.</p>
<p>Generation Algorithm</p>
<p>Algorithm 1 GenerateCogitaoTask(init_params) -Below is a simplified pseudo-code for the general algorithmic logic of the COGITAO generator: Generator task ← (input_grid, output_grid, transformation_suite) 10: Return: task, transform</p>
<p>The sampleTransformations() function simply iterates through the pool of transformations at the desired depth d to create the transformation sequence.An important function of our algorithm is SetInitialGrid(), which sets up the randomly sampled objects in the grid given the init_params (e.g.number of objects, desired object properties) and the sampled transformation sequence.Indeed, each transformation is defined with a set of object constraints, which are object properties for which the transformation can be applied without ambiguity.This function thus ensures that the random sampling of objects aligns with the sampled transformation sequence to avoid unexpected errors.The SetInitialGrid() function positions objects to keep them fully within the grid and to avoid contact with other objects.Finally, the transform_and_position function applies the transformations to objects and positions them back to the grid.Importantly, this function verifies that the transformed objects are still fully within grid dimension and do not collide with other objects (although adjacent contact is allowed at the transformation stage).If contact occurs, or objects go out of bounds, the entire input-output pair is discarded and generation is restarted.</p>
<p>For a standard 20x20 grid, with 4 objects (smaller than 6x6) and a sequence of 2 transformations applied, the average input-output pair generation time is at 0.005s ± 0.002s4 .Some configurations are more difficult and therefore more time-consuming to generate.For instance, decreasing the grid size and increasing the number of objects can significantly increase the generation time.</p>
<p>Experiments</p>
<p>To illustrate the utility of the COGITAO generator, we designed a set of benchmark experiments that highlight its potential for studying compositional generalization in the visual domain.These experiments serve as examples-COGITAO is not limited to them.In the Discussion section, we suggest additional configurations and encourage the community to further explore the flexibility of COGITAO for other experimental setups.</p>
<p>Experiment details</p>
<p>We define different "experiment settings" which focus on different aspects of compositionality and systematic generalization.Specifically, we outline two "studies" -one focusing on the composition of transformations, the Compositional Generalization (CompGen) study, and the other on environment variations (with fixed transformation sequence): the Environmental Generalization (EnvGen) study.In each case, we measure the capacity of models to perform tasks that differ in some way from the tasks seen during training -either with respect to transformation composition in the CompGen study or with respect to environment parametrization in the EnvGen study.</p>
<p>For both studies and each experiment setting, we create 5 "experiments" -these are different instances of the experiment settings in which we only vary the transformation sequences.This is to ensure that the results we report are robust across different transformation combinations (e.g., translation-based transformation sequences might be easier than rotation-based ones).</p>
<p>For each experiment, we limit the number of unique training samples to 100,000, and test on two sets of 1,000 samples each; one set follows the same distribution (i.e., in-domain set, or neutral split) as the training set, and the other is "out-of-distribution" (OOD), from which we evaluate compositionality or generalization.For all experiment settings, we control data uniqueness to ensure that no instance of parametrization and input grid is repeated across training, validation, and test sets.</p>
<p>In the CompGen study, we train and test a model on multiple transformation sequences within a single experiment.This contrasts with the EnvGen study, where models are trained on only a single transformation per experiment.To handle this complexity, we must provide context that tells the model which sequence to perform.We achieve this by appending a task embedding to the input sequence, similar to the approach in CVR [10].This embedding is a sequence of tokens that specifies the transformations and the correct order.For example, the task "mirror_horizontal-rotate_90" would have an embedding like ['M', 'R'], while the inverse task "rotate_90-mirror_horizontal" would be ['R', 'M'].For OOD testing, the models may be evaluated on new or longer sequences of these transformations.However, every individual token (like 'M' and 'R') will have been seen during training.We emphasize that this task embedding is not needed for the Generalization study, as its transformation is fixed for both in-distribution and OOD tasks.</p>
<p>We provide below a clearer outline of each experiment setting for each of our two studies.We refer the reader to the Appendix B for more in-depth overview of our proposed benchmark.</p>
<p>CompGen: Compositionality Generalization Study</p>
<p>For the compositionality study, we perform experiments within the following settings:</p>
<p>• C1 -From Atomic and Composite Tasks to Unseen Composite: Models are trained on a mix of individual atomic tasks (depth = 1) and composite tasks (depth = 2).The OOD test set consists of unseen composite tasks (depth = 2) composed of the same atomic transformations in novel orders.This setting evaluates whether models can generalize to new compositions when trained on both atomic and composite examples.• C2 -From Restricted Composite Tasks to Unseen Composite: Models are trained solely on a restricted set of composite tasks (depth = 2).The OOD test set consists of unseen composite tasks (depth = 2) composed of the same atomic transformations in novel orders.This setting evaluates whether models can generalize to new compositions when trained only on other composite examples.• C3 -From Composite Tasks to Deeper Composite Tasks: Models are trained on composite tasks of a specific depth (e.g., depth = 2).The OOD test set consists of deeper composite tasks (e.g., depth = 3) composed of the same atomic transformations.This setting evaluates whether models can generalize to compositions of greater depth than those seen during training.</p>
<p>EnvGen: Environmental Generalization Study</p>
<p>For the generalization study, we perform experiments within the following settings:</p>
<p>• G1 -Generalization to More Objects: Models are trained on 15x15 grids containing 1 or 2 objects.The OOD test set consists of grids of the same size containing 3 or 4 objects.This setting evaluates whether models can generalize to a greater number of objects.• G2 -Generalization to Larger Grids: Models are trained on grids with sizes ranging from 10x10 to 15x15.The OOD test set consists of grids with larger sizes, from 16x16 to 20x20.This setting evaluates whether models can generalize to larger spatial environments while the number of objects is held constant at 2.</p>
<p>• G3 -Generalization to Larger Objects: Models are trained on grids containing objects with dimensions ranging from 1x1 to 5x5.The OOD test set consists of grids containing objects with larger dimensions, from 6x6 to 10x10.This setting evaluates whether models can generalize to objects of a significantly different scale.• G4 -Generalization to More Complex Objects: Models are trained on grids containing symmetric and single-colored objects.The OOD test set consists of grids containing asymmetric and multicolored objects.For both sets, all other parameters (2 objects, 15x15 grid size, object dimensions &lt; 6x6) are fixed.This setting evaluates whether models can generalize from simple object features to more complex ones.• G5 -Combined Generalization: Models are trained on grids with a combination of simple properties: 1-2 objects, grid sizes from 10x10-15x15, and objects that are symmetric, singlecolored, and 1x1-5x5.The OOD test set consists of grids where all properties are more complex: 3-4 objects, grid sizes from 16x16-20x20, and objects that are asymmetric, multi-colored, and 6x6-10x10.This setting evaluates robust generalization across multiple difficulty axes simultaneously.</p>
<p>Models</p>
<p>We evaluate three encoder architectures, each paired with a two-layer MLP head for grid tokens classification.All models are of comparable size (approximately 1.5 million parameters) to ensure a fair evaluation.Empirically, increasing the model size did not substantially improve generalization.</p>
<p>• Vanilla ViT: We use a standard Vision Transformer (ViT) [32] with learned absolute positional encodings [33].Vision transformers are considered state-of-the-art for vision tasks and offer a strong baseline for COGITAOtasks.• Grid ViT: To better capture the structure of grid-based reasoning tasks, we introduce Grid ViT, an adapted version of ViTARC [34] that incorporates task-specific biases: object positional encoding (OPE) [34], PEMixer modules [34], register tokens [35], and modified positional encoding schemes [36,37].These additions are designed to support spatial reasoning over structured grid inputs.• LLaDA: Besides vision and grid-specific architectures, we investigate the performance of language models.Specifically, we evaluate LLaDA [38], a diffusion-based language model yielding state-ofthe-art performance on symbolic and logical tasks.</p>
<p>The selected models encompass a diverse set of architectural paradigms based on the current stateof-the-art in both vision and sequence modeling.In addition to general-purpose architectures, we introduce Grid ViT, which incorporates a strong inductive bias tailored to grid-structured tasks.This selection is designed to address the heterogeneous computational requirements of AVR tasks and spans a wide spectrum of modeling characteristics, thus providing baselines for COGITAO.</p>
<p>Comprehensive architectural details are provided in Appendix C.</p>
<p>Training</p>
<p>All models are trained5 from scratch using supervised learning.For each experiment, we evaluate and test in-domain (ID) and out-of-domain (OOD) data.We aim to train all models as similar as possible to have a fair comparison among them.The models are trained for 10 epochs (except LLaDA for 20 as on average 50% of the tokens are masked) and we select the best model based on the performance on the validation OOD split before evaluating them on the test sets.</p>
<p>All models are trained using the AdamW optimizer [39] with a linear warm-up for 200 steps, followed by cosine learning rate annealing.We use a batch size of 64 for training and 50 for evaluation and testing.Further training details for all models are provided in Appendix D.</p>
<p>Experiment Results</p>
<p>To provide robust empirical baselines for the COGITAO benchmark and to systematically evaluate the compositional and generalization capabilities of different model architectures, we apply the baseline models to the three compositionality experiment settings (C1-C3) and the five generalization experiment settings (G1-G5) described in Section 4.1.</p>
<p>Table 1 provides an overview of the results across these settings, reporting both in-domain (ID) and out-of-domain (OOD) grid accuracy.Grid accuracy is defined as the percentage of samples for which the predicted grid structure matches the ground truth exactly (i.e., the entire grid is predicted correctly).Performance values are averaged across 5 variations of the experiment setting (i.e.different transformation sequences -see B for more details).The experimental results reveal several important trends.Grid-ViT, our proposed model, achieves the strongest in-domain performance across nearly all experimental settings, demonstrating its robustness in compositional reasoning tasks.Its performance improvements over Vanilla-ViT across both ID and OOD scenarios highlight the effectiveness of the introduced inductive biases tailored to grid-based relational structures.Notably, in certain settings, such as C3 and G1, LLaDA performs on par with or marginally better than Grid-ViT, particularly in terms of out-of-domain generalization.In fact, LLaDA achieves the highest OOD accuracy in several configurations (e.g., C1, G4, and G5), suggesting its potential for systematic generalization.Vanilla-ViT shows moderate in-domain competence in some settings (e.g., G1 and G2), but its performance degrades substantially under distribution shifts, indicating limited capacity for generalization beyond the training distribution.Overall, these findings confirm the challenging nature of the COGITAO benchmark and highlight the need for architectures that are versed in compositionality and systematic generalization.</p>
<p>Discussion</p>
<p>COGITAO offers a controlled framework for visual reasoning, specifically designed to study compositionality and generalization.By leveraging simple grid-based environments and object-centric transformations, it enables researchers to isolate core reasoning capabilities while avoiding the visual and data complexity of more naturalistic datasets.With millions of possible tasks and abundant training samples, COGITAO provides an unmatched degree of compositional control in the visual domain.</p>
<p>Unlike classification-based benchmarks such as CVR [10], RPM [22], SVRT [12], and PGM [11], our generator requires models to generate output grids.This task formulation significantly raises the difficulty and makes the benchmark more robust for evaluating a model's compositional and generalization capabilities.It aligns more closely with challenges like ARC-AGI [21] and its successors [40,26,41].However, in contrast to ARC, which loosely defines "core knowledge principles", COGITAO provides an explicit and accessible set of concepts that models must master to succeed.</p>
<p>In our experiments, we evaluate Vanilla ViT, Grid ViT, and LLaDA across the COGITAO tasks.These models were chosen to represent the state-of-the-art in vision and sequence modeling, including both general-purpose and grid-specialized architectures.While they provide a diverse and strong baseline, future work could explore additional architectures.The experiments reveal a consistent trend: while these models excel on in-domain tasks, they fail dramatically in out-of-distribution scenarios requiring compositional understanding.For instance, in our compositionality study, accuracies plummeted to as low as 5.1% (Grid ViT) and 6.4% (LLaDA) when faced with unseen transformation sequences.Similarly, generalization performance degrades significantly with increased task complexity.These results support growing evidence that current vision models rely heavily on pattern recognition rather than systematic compositional reasoning [42].COGITAO provides a controlled environment to diagnose these fundamental limitations and guide the development of more robust, generalizable architectures.</p>
<p>Achieving strong performance on the most challenging COGITAO tasks would mark a substantial leap forward in AI research.Solving it using only generator-produced training data would demonstrate that a model can isolate individual operations, apply them iteratively, and recombine them in novel contexts; abilities that are central to human intelligence [2].</p>
<p>We acknowledge, however, that COGITAO is abstract and synthetic, and lacks grounding in realworld visual data.While many tasks are conceptually straightforward for humans, transformation sequences with large depth can be cognitively demanding.Moreover, although the task space is large, some transformations do not differ drastically in structure or complexity.Nevertheless, we believe that mastering such structured, synthetic benchmarks is a necessary next step for progress in visual reasoning.</p>
<p>COGITAO offers a compelling platform for driving progress in compositional generalization, and we believe it supports several promising research directions.First, extending the framework to in-context learning, for example, by providing demonstration examples, could allow evaluation of generalization in settings known to benefit from longer contexts [28], particularly in large foundation models [43,44,45].Second, due to its controllable environment and adjustable difficulty, COGITAO is well-suited for curriculum learning, where task complexity is gradually increased to guide learning.Third, analyzing internal model representations trained on COGITAO may reveal whether and how models develop object-centric or transformation-centric abstractions.</p>
<p>A COGITAO Objects</p>
<p>Rather than generating objects on the fly, which can be more costly computationally, we favour a method where we pre-generate a large set of 23000 object, and compute a table of properties or that these objects satisfy.This allows us to efficiently sort through objects at generation time with-respectto the constraints specified by the user, as well as the object constraints that the transformation suite may impose.We share the file with these objects along with our code release, as well as the code used to generate these, so the community can expand the number of objects at will.Below, are all the properties through which we iterate, creating combination of every single parameter, up until a max object dimension of 15x15 pixels.</p>
<p>• Size: Number of rows, columns, and pixels.</p>
<p>• Symmetry: Horizontal, vertical, diagonal, point, and no symmetry.</p>
<p>• Connectivity: "4 connected" (only connected through adjacent edges), "8 connected" (only connected through adjacent or diagonal edges), or even "distance" (object can be composed of unconnected blocks).• Colors: Single colored or multi-colored.</p>
<p>• Color Pattern: Uniform (single color), column stripes, row stripes, diagonal stripes, top-bottom coloring (object split in two colors), right-left coloring (object split in two colors), or random.• Footprints: Predefined objects such as Rectangle, disk, square, diamond, or ellipse.</p>
<p>B Experiment Details</p>
<p>We designed our benchmark experiment to exemplify the use of our generator, and show how state-ofthe-art vision models still consistently fail at such elementary tasks as they require compositionality.We provide in this appendix further details into each experiment.</p>
<p>Compositional Generalization Study (CompGen) For all CompGen study, we fix the number of objects to 2, the grid size to 20x20, the object dimension to be smaller or equal to 6x6, and all objects to be fully connected (no unconnected parts -see A "Connectivity").For each CompGen experiment setting, we generate 100000 training samples, 1000 in-distribution validation samples, 1000 out-of-distribution (OOD) validation samples, 1000 test samples, and 1000 OOD test samples.The results we report are based on the two aforementioned test sets.Based on these parameters, we then design all experiment settings in such a way that the training and testing sets are built from different "transformation sequences" (i.e.sequence of transformations).In the CompGen study, the transformation sequence varies within and between data samples.We thus provide the model with information on which transformation sequence it must apply by appending a task code to the input sequence.The task code is a sequence of tokens that indicates to the model the transformations it must apply (in the correct order).To account for varying "depth" of transformation sequence, we simply pad the input sequence with "identity transformation" tokens -as such, we always append a task code of depth 4 -which is the maximum depth of transformation sequence we consider in the entirety of our experiments.We chose the transformations sequences to be from different transformation families, and to be easily composable with one another within the constrained object and grid dimensions.Below is a detailed summary of our experiment settings and experiments.</p>
<p>• C1 -From Restricted Composite Tasks and Atomic Tasks to Unseen Composite: We train on a set of composite tasks made of some atomic transformations, and test on composite tasks of the same depth, but not seen during training -C1-1: Train on the atomic transformations translate_up, rotate90, mirror_horizontal and all of their mutual compositions (depth d = 2), except the specific composition translate_up − rotate90, which we leave for OOD testing.-C1-2:</p>
<p>Train on the atomic transformations change_object_color, pad_right, fill_holes_different_color and all of their mutual compositions (depth d = 2), except the specific composition change_object_color − pad_right, which we leave for OOD testing.</p>
<p>-C1-3: Train on the atomic transformations crop_bottom_side, rotate_90, pad_top and all of their mutual compositions (depth d = 2), except the specific composition rotate_90 − crop_bottom_side, which we leave for OOD testing.-C1-4: Train on the atomic transformations extend_contours_different_color, mirror_horizontal, translate_down and all of their mutual compositions (depth d = 2), except the specific composition mirror_horizontal − extend_contours_different_color, which we leave for OOD testing.</p>
<p>-C1-5:</p>
<p>Train on the atomic transformations extend_contours_same_color, mirror_vertical, pad_left and all of their mutual compositions (depth d = 2), except the specific composition pad_left − extend_contours_same_color, which we leave for OOD testing.</p>
<p>• C2 -From Restricted Composite Tasks to Unseen Composite: We train on a set of composite tasks made of some atomic transformations, and test on composite tasks of the same depth, but not seen during training.</p>
<p>-C2-1: Train on all the mutual compositions (depth d = 2) of translate_up, rotate90, mirror_horizontal, except the specific composition translate_up − rotate90, which we leave for OOD testing.-C2-2: Train on all the mutual compositions (depth d = 2) of change_object_color, pad_right, fill_holes_different_color, except the specific composition change_object_color − pad_right, which we leave for OOD testing.</p>
<p>-C2-3: Train on all the mutual compositions (depth d = 2) of crop_bottom_side, rotate_90, pad_top, except the specific composition rotate_90 − crop_bottom_side, which we leave for OOD testing.-C2-4:</p>
<p>Train on all the mutual compositions (depth d = 2) of extend_contours_different_color, mirror_horizontal, translate_down, except the specific composition mirror_horizontal − extend_contours_different_color, which we leave for OOD testing.</p>
<p>-C2-5: Train on all the mutual compositions (depth d = 2) of extend_contours_same_color, mirror_vertical, pad_left, except the specific composition pad_left − extend_contours_same_color, which we leave for OOD testing.</p>
<p>• C3 -From Composite Tasks to Deeper Composite Tasks: We train on a set of composite tasks made of some atomic transformations, and test on composite tasks with the same transformations, but with one additional level of depth.Environment Generalization Study (EnvGen) For each EnvGen experiment setting, we generate 100000 training samples, 1000 in-distribution validation samples, 1000 out-of-distribution (OOD) validation samples, 1000 test samples, and 1000 OOD test samples.The results we report are based on the two aforementioned test sets.For each EnvGen experiment, we fix the transformation sequence (as described below depending on the experiment) and vary some parameters, such as the grid size, the number of objects, the object dimensions or the object properties.The varying parameters constitutes what changes between the in-distribution and OOD testing sets, and forms the basis of the "generalization" experiment.As opposed to the CompGen setting, where there are multiple transformation sequences per set, we do not need to provide a task code to the model, but only a single grid on which it must perform the transformation based on the grid settings.To account for varying grid sizes in some of the experiments, we simply pad the the input sequence to the max size which can be observed during the experiment.</p>
<p>• G1 -Number of Objects Difficulty: We train on grids with 1 or 2 objects, and OOD test on grids with 3 or 4 objects.We fix the grid size to 15x15.</p>
<p>-G1-1: Perform experiment with the translate_up transformation.</p>
<p>-G1-2: Perform experiment with the rotate_90 transformation.</p>
<p>-G1-3: Perform experiment with the mirror_horizontal transformation.</p>
<p>-G1-4: Perform experiment with the crop_top_side transformation.</p>
<p>-G1-5: Perform experiment with the extend_contours_same_color transformation.</p>
<p>• G2 -Grid Size Difficulty: We train on grid sizes between 10x10 and 15x15, and OOD test on grid sizes between 16x16 and 20x20.We fix the number of objects to 2.</p>
<p>-G2-1: Perform experiment with the translate_up transformation.</p>
<p>-G2-2: Perform experiment with the rotate_90 transformation.</p>
<p>-G2-3: Perform experiment with the mirror_horizontal transformation.</p>
<p>-G2-4: Perform experiment with the crop_top_side transformation.</p>
<p>-G2-5: Perform experiment with the extend_contours_same_color transformation.</p>
<p>• G3 -Object Dimension Difficulty.We train on grids with objects of size between 1x1 and 5x5, and OOD test on grids with objects of size between 6x6 and 10x10.</p>
<p>-G3-1: Perform experiment with the translate_up transformation.</p>
<p>-G3-2: Perform experiment with the rotate_90 transformation.</p>
<p>-G3-3: Perform experiment with the mirror_horizontal transformation.</p>
<p>-G3-4: Perform experiment with the crop_top_side transformation.</p>
<p>-G3-5: Perform experiment with the extend_contours_same_color transformation.</p>
<p>• G4 -Object Complexity Difficulty We train on grids with symmetric and single-colored objects, and OOD test on grids with asymmetric and multi-colored objects.We fix the number of objects to 2, the grid size to 15x15, and the object dimension to smaller than 6x6.</p>
<p>-G4-1: Perform experiment with the translate_up transformation.</p>
<p>-G4-2: Perform experiment with the rotate_90 transformation.</p>
<p>-G4-3: Perform experiment with the mirror_horizontal transformation.</p>
<p>-G4-4: Perform experiment with the crop_top_side transformation.</p>
<p>-G4-5: Perform experiment with the extend_contours_same_color transformation.</p>
<p>• G5 -All Difficulties Combined We train on grids with symmetric, single-colored objects of size between 1x1 and 5x5, with 1 or 2 objects, and grid sizes between 10x10 and 15x15.We OOD test on grids with asymmetric, multi-colored objects of size between 6x6 and 10x10, with 3 or 4 objects, and grid sizes between 16x16 and 20x20.</p>
<p>-G5-1: Perform experiment with the translate_up transformation.</p>
<p>-G5-2: Perform experiment with the rotate_90 transformation.</p>
<p>-G5-3: Perform experiment with the mirror_horizontal transformation.</p>
<p>-G5-4: Perform experiment with the crop_top_side transformation.</p>
<p>-G5-5: Perform experiment with the extend_contours_same_color transformation.</p>
<p>C Models</p>
<p>The selection of encoder networks includes both standard baselines (ResNet, Vanilla ViT) and more specialized architectures (Grid ViT, LLaDA) designed to better capture abstract and spatial reasoning, compositionality, and generalization capabilities.</p>
<p>Table 2 summarizes some of the notable architectural and modeling characteristics used for each encoder network part of the models.The models were designed to roughly have the same size of 1.2 Million of learnable parameters in order to propose a fairer performance comparison, in spite of being aware that different architectures may have different modeling requirements to optimize their performance.</p>
<p>C.1 ResNet</p>
<p>We employ an architecture based on ResNet [46] as a standard baseline notable for its strong historical performances on vision tasks, principally distinguishing itself from the other models evaluated here by its convolutional inductive bias and lack of attention mechanism.</p>
<p>In our implementation, the input grid is processed as a low-resolution image after an artificial channel dimension is created through one-hot encoding of the token categories that can possibly be predicted (i.e., num_token_categories).Then, the core of the encoder consists of a sequence of residual blocks.Crucially, all convolutional operations within these blocks are performed without any spatial downsampling (e.g., using stride 1 convolutions and no pooling layers).Despite missing on a slightly more global receptive field, this design choice is important for our tasks, as it strictly preserves the spatial dimensions of the feature maps throughout the network and thus does not require an approach of upsampling.The stacking of convolutional layers without downsampling should also allow the effective receptive field for each individual output pixel to grow, enabling the model to better capture useful context regions while retaining more precise spatial information.</p>
<p>Our specific ResNet architecture comprises:</p>
<p>• An initial convolutional layer to project the input grid channels to the model's hidden dimension: num_token_categories input channels, 32 output channels, kernel size of 1, stride of 1, padding of 0. This is followed by Batch Normalization [47] and a ReLU activation function [48].</p>
<p>• A series of 5 residual blocks: 1. 32 input channels, 64 output channels, kernel size of 3, stride of 1, padding of 1.</p>
<ol>
<li>
<p>64 input channels, 128 output channels, kernel size of 3, stride of 1, padding of 1.</p>
</li>
<li>
<p>128 input channels, 128 output channels, kernel size of 1, stride of 1, padding of 0. 4. 128 input channels, 256 output channels, kernel size of 3, stride of 1, padding of 1. 5. 256 input channels, 128 output channels, kernel size of 1, stride of 1, padding of 0.</p>
</li>
</ol>
<p>• A final convolutional layer (1x1 convolution) to map the features from the last residual block to the required embed dimension per pixel/token: 128 input channels, embed_dim output channels, kernel size of 1, stride of 1, padding of 0.</p>
<p>C.2 Vanilla ViT</p>
<p>We adapt the standard Vision Transformer (ViT) architecture, as defined in [32], to our grid-based tasks.The (padded and one-hot encoded) input grid is divided into a sequence of non-overlapping patches, which are effectively single tokens since we use a patch size and stride of 1 when linearly projecting the grid image into the embedding dimension using a 2D convolution.</p>
<p>A basic approach to incorporate spatial information is to use (randomly initialized) 1D learnable positional embeddings to add to the embedded input sequence before it is passed to the encoder network.</p>
<p>The ViT, essentially a Transformer encoder, consists of multiple layers, each containing a multi-head self-attention (MHSA) mechanism followed by a position-wise feed-forward network.We use Pre-Layer Normalization (Pre-LN) [49][50] and thus apply Layer Normalization before both the MHSA and feed-forward sub-layers.Residual connections are also used around each sub-layer, as is standard.</p>
<p>To be able to produce the output grid using an MLP head, a linear layer is applied to each output token of the sequence (from which the special extra tokens have been truncated) at the end of the encoder network in order to map the embedding dimension to that of the MLP head which will then predict the logits for all of the tokens which, once softmax applied and spatially reshaped, form the output grid.</p>
<p>Notable hyperparameters are:</p>
<p>• Input grid partitioning: Patch size of 1, stride of 1.The grid is partitioned at the pixel-level.</p>
<p>• Embedding dimension: 128.</p>
<p>• Number of encoder layers: 6.</p>
<p>• Number of attention heads in MHSA: 4.</p>
<p>• Dimensionality of the feed-forward layer: Factor of 4 times the embedding dimension, thus 512.</p>
<p>• Activation Functions: GELU [51].</p>
<p>• Output projection: A linear layer maps each embed_dim-dimensional output token (excluding the extra tokens such as the register tokens or the task embedding) to num_classes logits, where num_classes depends on the number of tokens to predict (e.g., 15 if Visual Tokens are used, 11 otherwise, as there are ten for the 0-9 symbols and one for padding).</p>
<p>C.3 Grid ViT</p>
<p>The Grid-ViT is a variant of the Vision Transformer architecture adapted to improve performance on abstract visual reasoning tasks, especially in grid-like environments, similar to the COGITAO data.It incorporates several modifications to improve spatial and abstract visual reasoning.Similar to the Vanilla ViT, the input grid is embedded as patches at the pixel level (i.e., patch size and stride of 1 when performing the 2D convolution to transform the grid into an embedded sequence).</p>
<p>The key architectural modifications from the Vanilla-ViT are:</p>
<p>• Positional Encodings:</p>
<p>-2D Absolute Positional Encoding (APE): We use 2D sinusoidal absolute positional encodings, extended from the 1D sinusoidal absolute positional encodings used in [33], which are fixed (i.e., not learned) and directly reflect the 2D nature of the input grid.They are added to the patch embeddings following the PEMixer strategy.</p>
<p>-Object Positional Encoding (OPE): OPE is used as part of the APE, as described in [34] where half of the APE dimension is allocated to the object positions while the other half is used for the x and y positions in the grid.The OPE is also typically coupled with an appropriate PEMixer strategy, such as a vector-weighted sum.We make the choice to compute the object positions after having padded (whether with Visual Tokens or with simple padding) the input grid, as opposed to before.</p>
<p>-Positional Encoding Mixer (PEMixer): The PEMixer presented in [34] defines the strategy by which we encode the absolute positional information into the input embeddings from the absolute positional embeddings.It allows different strategies such as a sum (the standard one), a weighted sum, a vector-weighted sum, etc.</p>
<p>-Relative Positional Encoding (RPE): Due to the importance of the relative positions of the grid tokens when transforming objects, an RPE scheme comes in as a natural consideration.Among possible techniques of RPE, we choose to use Rotary Position Embedding (RoPE) [37], incorporated into the self-attention mechanism in order to inject relative spatial information between the grid tokens.After initial experiments with ALiBi [52], another RPE method, extended to 2D as in [34], we found that RoPE yields comparable performance on our tasks and thus decided to use RoPE for its simplicity.</p>
<p>• Registers: We prepend (6) register tokens to the input sequence, following results from [35] and a drawn parallel to slots for object-centric learning in [30].Those registers are additional, randomly initialized and learnable tokens appended to the sequence of patch embeddings in order to possibly improve model performance by functioning as containers for less informative "background" regions of the grid.Thus, they do not correspond to any specific input informing the model and should be leveraged by the attention mechanism to improve global context aggregation and internal representations.The positional encodings are not used for those extra tokens.</p>
<p>Another modification to the Vanilla-ViT is the use of dropout.We apply dropout with a rate of 0.1 after the multi-head self-attention layer and after the feed-forward layer in each Transformer encoder block.</p>
<p>The remaining notable hyperparameters are the same as for the Vanilla-ViT.</p>
<p>C.4 LLaDA</p>
<p>We include LLaDA [38] in our set of models due to its demonstrated strength on logical and arithmetic tasks.LLaDA operates non-autoregressively and exploits bidirectional dependencies over masked target sequences, making it well-suited for structured input-output mappings.We adapt the original LLaDA setup skipping pretraining and training it from scratch via supervised learning.Each instance of input data is represented as a flattened concatenation of a task embedding, input sequence, and a partially masked target sequence.During training, a random portion of target tokens is masked (mask ratio sampled uniformly in [0, 1]), and the model is trained to reconstruct the masked tokens.At inference, the target is fully masked, and reconstruction proceeds over 32 denoising steps, each resolving a fraction of the masked tokens based on confidence.</p>
<p>In contrast to the original architecture, We use a lightweight 6-layer version with approximately 1.3M parameters.The architecture uses an embedding dimension of 128, a feedforward hidden size of 384 (MLP ratio of 4), and 4 attention heads.The model incorporates RMSNorm [53] with affine parameters and uses SiLU activation functions [54].Rotary positional embeddings (RoPE) [37] are applied independently within each transformer layer, using a shared base frequency parameter of θ = 500,000.Weights are initialized via the Mitchell method with a standard deviation of 0.02.No dropout is used throughout training.The model's vocabulary comprises 12 tokens, reflecting the tokenized representation used in COGITAO, and includes special tokens for padding and masking.</p>
<p>D Training Procedure</p>
<p>To ensure a fairer comparison across different models, we adopted a consistent training and evaluation framework whenever possible.Key aspects are detailed as follows:</p>
<p>• Training Procedure and Data: All models were trained from scratch using supervised learning.We used a dataset of 100'000 unique samples.Training proceeded for 10 epochs with a batch size of 64, meaning each model saw a total of 1'000'000 samples, This implies a sample-efficient mode of experimentation, as observed through the training learning curves hinting at a nonterminal convergence for several experiments and considering the typical quantity of samples that Transformer-based models require.</p>
<p>• Validation and Testing: Model performance was monitored during training on a validation set of 1'000 samples.After training, models were evaluated on a distinct held-out test set of 1'000 samples.We always compute the performance on ID and OOD val/test sets.</p>
<p>• Modeling Strategy:</p>
<p>-An input grid with shape [H, W] is converted to an artificial image of shape [C, H, W] by the creation of a channel dimension through one-hot encoding of the categories of tokens that can be predicted.</p>
<p>-For the experiments within the CompGen study, the transformations sequence vary within the trainin set (and w.r.t. the OOD sets).We therefore provide the model with some context in the form of a task embedding in order to inform it of what task it should perform given the input grid.For that purpose, we considered two approaches: task tokens and in-context example.</p>
<p>We decided to use the first approach, where we provide the model with a sequence of tokens representing the atomic transformations composing the task to perform.The second approach was to provide the model with an example (i.e., an input-output pair of grids) of the task to perform.However, it was not experimented with due to time and resource constraints.</p>
<p>• Early Stopping and Model Selection: No early stopping was performed as the number of epochs was restricted and in most experiments the models showed steady convergence throughout the 10 epochs.The checkpoint used for final testing was chosen based on the best (lowest) validation loss achieved on the OOD validation set.</p>
<p>• Loss Function: We employed a pixel-wise cross-entropy loss, calculated between the entire predicted token grid and the ground-truth target grid, both padded to a maximum size equal to the largest grid size within the dataset.</p>
<p>• Evaluation Metric: The primary metric for reporting performance is grid accuracy.This is the percentage of predicted grids that perfectly match the ground-truth target grids.Both grids were padded to the maximum grid size observed in the dataset before comparison.The padding is either composing of usual padding tokens or of special padding tokens named "Visual Tokens" (VTs) [34].This means that when VTs are used as part of the modeling strategy for Transformer-based models, the model has to predict correctly the whole padded grid, with its special padding tokens, in order to perform well since the metric computation does not discard the tokens/pixels outside of the boundaries denoted by the true grid size.</p>
<p>• Optimizer and Learning Rate: The AdamW optimizer [39] was used, with a learning rate following a Cosine Annealing schedule that monitors the OOD validation loss at each epoch.</p>
<p>• Weight Initialization: Model weights were initialized from a truncated normal distribution.</p>
<p>• Training Precision: Training was conducted using FP16 mixed precision.</p>
<p>• Training Duration: A training run lasted between 7 minutes and 1 hour depending on the model, hardware and specific experiment.</p>
<p>D.1 ResNet</p>
<p>The learning rate and weight decay used for ResNet are of 1e-3, same as for the Vanilla-ViT and Grid-ViT models.</p>
<p>ResNet is trained without Visual Tokens, as their purpose is mainly to mitigate the loss of the 2D spatial structure when going from a 2D grid to a flattened sequence, as it is the case for Transformer models, as well as defining more explicit grid boundaries.</p>
<p>For the Compositionality experiments, the task embedding is appended (along the spatial dimension) to the spatially flattened output of the ResNet encoder network.This means that the task embedding only enters the input transformation process at a late stage, similar to what is done in [10], compared to how the task embedding is used with the Transformer-based models working with sequences.Practically, it is the MLP head that has to leverage the provided by the task embedding in order to make better-informed predictions.</p>
<p>D.2 Vanilla-ViT &amp; Grid-ViT</p>
<p>The two ViT models use exactly the same hyperparameters.The learning rate and weight decay are of 1e-3.Before the initial learning rate is set to change with the Cosine Annealing schedule, a linear warm-up phase takes place for 200 steps.</p>
<p>A key difference between the Vanilla-ViT and the Grid-ViT is that the latter uses Visual Tokens while the former does not.</p>
<p>The input sequence to the ViT results from the one-hot encoding of the input grid for which embedding patches are created using a 2D convolution with patch size of 1 and stride of 1.</p>
<p>For the Compositionality experiments, the task embedding is appended (along the spatial dimension) to the input sequence before being encoded by the ViT.This provides the full task context at the start of the processing of the input by the model, thus increasing the information about the task to perform propagated through the model.</p>
<p>D.3 LLaDA</p>
<p>We train the LLaDA model using the same hyperparameter configuration as that employed for the vision transformer baselines, including the Vanilla ViT and Grid ViT architectures.However, we introduce two key modifications: Firstly, while retaining the AdamW optimizer [39], we reduce the learning rate to 2 × 10 −4 and set the weight decay to 0.01.Empirically, this lower learning rate leads to significantly faster convergence for LLaDA during training.Second, we extend the training schedule to 20 epochs, doubling the number of training epochs compared to the baseline setup.This adjustment compensates for the fact that LLaDA masks only approximately 50% of the target tokens on average, thereby ensuring that all models are trained to predict a comparable total number of tokens and enabling a fair evaluation.</p>
<p>E Results</p>
<p>We present a set of results for each model and each of the 40 individual experiments considered in the studies of CompGen and EnvGen.The results reported are the average of three runs with different random seeds.A few experiments showed a high sensitivity to the random seed, up to a 15% difference, while the others yielded similar results for different runs.Tables 3 4 5 6 display the in-domain (ID) and out-of-domain (OOD) test grid accuracies (i.e., the predicted grid and the ground-truth target grid, both padded to a max.size, should perfectly match) for all models considered across all the experiment settings (i.e., C1-C3 and G1-G5).While the main paper provides a summary in the form of averaged results over each setting for clarity and space constraints, the exhaustive results here enable a more granular understanding of model behavior given different transformations and compositions thereof.</p>
<p>We include here results of a ResNet-like model baseline, which is omitted from the main text table and discussion.As a standard convolutional neural network performing well on vision tasks -even when it includes a level of reasoning [10], ResNet is poorly designed for compositionality tasks, and is more sample efficient than Transformer-based architectures [10].Consequently, it was not deemed to be of focus in this paper, alth-ough it could provide few insights.Its inclusion here also serves to put into perspective the performance of models with clearly different inductive biases.In the main paper, we focus on models better designed for abstraction and generalization, with a Vanilla-ViT as starting point, followed by the Grid-ViT and LLaDA models.</p>
<p>The results clearly highlight that compositional generalization remains a major challenge, which COG-ITAO allows to explore.Across most CompGen experiments, models perform poorly in the OOD setting.Notably, some transformations stand out as yielding easier or more difficult tasks.For example, translate_up is by far the easier transformation followed for example by extend_contours_same and crop_top_side.More difficult transformations include mirror_horizontal and rotate90 which empirically, but also intuitively, are seen to be more complex.A transformation such as crop_top_side may be easier as it mainly introduces localized changes, allowing models, especially those with strong inductive biases for locality, to generalize more effectively.In contrast, transformations such as mirror_horizontal and rotate90 result in drastic spatial reconfigurations of visual features, which can severely disrupt learned representations, in particular when not explicitly trained on such variations.</p>
<p>We point the reader to Experiment C1-4 and C2-4, for which models perform much better than the other experiments of the same experiment setting.For these experiments, we included the equivariant transformation which we test for in the training set.Specifically, the OOD transformation sequence we test for in Experiment C1-4 and C2-4 is mirror_horizontal − extend_contours_different_color, we included the equivariant extend_contours_different_color − mirror_horizontal in the training set.This shows that our Vanilla-ViT, Grid-ViT and LlaDA are able to recognize the equivariance of the transformations, and use this knowledge to apply a novel transformation sequence in the OOD test set.We note that the tested OOD transformation sequence of C1-1 and C2-1 also presents equivariance (translate_up − rotate_90 and (rotate_90 − translate_up), however, we did not include the equivariant transformation in the training set unlike C1-4 and C2-4.We added this difference between experiments to highlight a given model's capacity to recognize equivariance.</p>
<p>In the EnvGen experiments, the ResNet performed competitively, and even better for settings focused on changes of grid size and number of objects.These experiments involve systematic variations of properties of the grid environment and scene (e.g., the grid size, the number of objects, their spatial distribution) which can still be captured through localized pixel/symbol statistics and spatial patterns that do not require compositional abilities.ResNet's strong performance here could be attributed to its architectural inductive biases: overlapping receptive fields, translation invariance from convolutions, and the ability to encode detailed local structures without aggressive downsampling (as per our implementation).We suspect that these properties enable it to effectively maintain useful representations when the domain shift involves more spatially coherent variations, as opposed to more abstract ones based on a compositional prior.</p>
<p>These detailed tables serve not only as a complement to the averaged results in the main paper, but also as a valuable diagnostic tool for evaluating the specific generalization challenges posed by different transformations.They reinforce the necessity of designing models and training regimes that can robustly handle a wide range of compositional and systematic shifts in visual input with respect to diverse transformations implying fundamentally different types of changes in the grid.Consequently, an exhaustive table with more experiments for all the transformations enabled by COGITAO would be a natural extension of those results.experiment-4 100.0 ± 0.0 99.9 ± 0.0 98.9 ± 0.6 77.9 ± 3.5 experiment-5 99.9 ± 0.1 99.3 ± 0.1 98.2 ± 0.3 83.7 ± 1.4</p>
<p>G2</p>
<p>experiment-1 100.0 ± 0.0 99.9 ± 0.1 98.7 ± 0.3 1.9 ± 1.9</p>
<p>experiment-2 88.9 ± 1.</p>
<p>Figure 1 :
1
Figure 1: Set of input-output pair examples from our COGITAO generator, with input grids on top rows, and corresponding output grids (after transforming input) on the bottom rows.Each input-output pair follows a different transformation sequence and grid/object parametrization.Task A: extend_object.Task B: empty_inside -double_up.Task C: mirror_horizontal -fill_holes -crop_top_side -rotate_90.</p>
<p>Figure 3 :
3
Figure 3: Random example of generated objects.The objects are generated with a variety of properties, including size, symmetry, connectivity, colors, color patterns, and footprints.Note: objects are not allowed to overlap, touch, or be inside one another in our generator environment, as reflected in the above image.</p>
<p>-C3- 1 :
1
Train on all the atomic transformations and mutual compositions (depth d = 2) of translate_up, rotate90, mirror_horizontal, and OOD test on all the compositions of depth d = 3 of these transformations.-C3-2: Train on all the atomic transformations and mutual compositions (depth d = 2) of change_object_color, pad_right, fill_holes_different_color, and OOD test on all the compositions of depth d = 3 of these transformations.. -C3-3: Train on all the atomic transformations and mutual compositions (depth d = 2) of crop_bottom_side, rotate_90, pad_top, and OOD test on all the compositions of depth d = 3 of these transformations.-C3-4: Train on all the atomic transformations and mutual compositions (depth d = 2) of extend_contours_different_color, mirror_horizontal, translate_down, and OOD test on all the compositions of depth d = 3 of these transformations.. -C3-5: Train on all the atomic transformations and mutual compositions (depth d = 2) of extend_contours_same_color, mirror_vertical, pad_left, and OOD test on all the compositions of depth d = 3 of these transformations.</p>
<p>Table 3 : 1 Table 4 : 1 experiment 1 experiment- 2 7 Table 5 :
31411275
ID and OOD test grid accuracy with SEM (Standard Error of the Mean) as error bars for the CompGen experiments part of settings C1-C3 and for the ResNet and Vanilla-ViT models.± 0.0 88.6 ± 5.2 72.3 ±10.8 experiment-5 0.2 ± 0.1 0.0 ± 0.0 21.6 ± 1.5 0.0 ± 0.0 C3 experiment-1 0.0 ± 0.0 0.0 ± 0.0 13.8 ± 1.2 0.8 ± 0.1 experiment-2 10.4 ± 3.2 3.2 ± 1.1 37.4 ± 5ID and OOD test grid accuracy with SEM (Standard Error of the Mean) as error bars for the CompGen experiments part of settings C1-C3 and for the Grid-ViT and LLaDA models.43.5 ±16.9 0.0 ± 0.0 27.7 ± 1.4 0.0 ± 0.0 experiment-2 47.2 ±18.6 0.0 ± 0.0 67.0 ± 0.4 0.0 ± 0.0 experiment-3 56.7 ± 7.7 0.0 ± 0.0 33.3 ± 2.5 0.0 ± 0.0 experiment-4 98.6 ± 0.1 95.6 ± 1.2 87.7 ± 1.6 70.1 ± 6.68.1 ±29.5 15.0 ± 7.5 98.4 ± 0.6 24.5 ± 1.1 experiment-3 51.4 ± 1.2 2.6 ± 0.4 82.7 ± 5.9 5.5 ± 0.8 experiment-4 67.7 ± 6.5 0.4 ± 0.1 51.2 ±15.0 0.3 ± 0.0 experiment-5 66.7 ±14.1 0.6 ± 0.3 84.3 ± 9.9 1.8 ± 0.ID and OOD test grid accuracy with SEM (Standard Error of the Mean) as error bars for the EnvGen experiments part of settings G1-G5 and for the ResNet and Vanilla-ViT models.100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 experiment-2 99.8 ± 0.0 96.5 ± 0.1 97.3 ± 0.2 55.4 ± 6.6 experiment-3 99.8 ± 0.1 98.5 ± 0.2 97.5 ± 0.6 77.5 ± 3.4</p>
<p>8 . 2 ± 5 . 4 experiment- 4 2 G3experiment- 1 6 experimentexperiment- 5 3 G4experiment- 1 9 . 4 ± 2 Table 6 : 8 G3experiment- 1 5 experimentexperiment- 5 7 G5
82544216531942681557
3 78.2± 7.2 57.1 ±21.5 0.2 ± 0.2 experiment-3 99.3 ± 0.3 98.2 ± 0.8 81.1 ±11.3 100.0 ± 0.0 100.0 ± 0.0 96.3 ± 1.5 0.0 ± 0.0 experiment-5 100.0 ± 0.0 100.0 ± 0.0 76.9 ± 2.3 0.2 ± 0.100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 88.6 ± 9.99.4 ± 0.0 35.3 ± 2.7 92.6 ± 1.4 23.8 ± 3.100.0± 0.0 23.7 ± 5.8 100.0 ± 0.0 90.6 ± 0.9 experiment-2 30.7 ± 1.2 0.0 ± 0.0 3.8 ± 2.9 0.0 ± 0.0 experiment-3 58.8 ± 2.9 0.1 ± 0.1 14.8 ± 1.0 0.0 ± 0.0 experiment-4 95.2 ± 0.6 64.1 ± 4.0 51.9 ± 2.8 9.0 ± 1.4 experiment-5 97.1 ± 0.9 36.9 ± 0.5 60.4 ±14.7 ID and OOD test grid accuracy with SEM (Standard Error of the Mean) as error bars for the EnvGen experiments part of settings G1-G5 and for the Grid-ViT and LLaDA models.100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 99.9 ± 0.1 experiment-2 99.7 ± 0.0 92.7 ± 1.3 99.0 ± 0.3 83.5 ± 3.6 experiment-3 99.8 ± 0.1 95.3 ± 0.4 99.1 ± 0.1 84.2 ± 2.6 experiment-4 99.2 ± 0.1 88.5 ± 2.5 99.6 ± 0.0 88.9 ± 1.4 experiment-5 97.8 ± 0.5 73.9 ± 4.9 99.8 ± 0.1 94.1 ± 1.5 G2 experiment-1 100.0 ± 0.0 90.5 ± 4.9 100.0 ± 0.0 60.6 ± 6.6 experiment-2 97.1 ± 1.8 68.5 ± 3.6 95.5 ± 1.3 45.3 ± 3.4 experiment-3 97.9 ± 2.1 90.6 ± 5.8 97.7 ± 1.8 47.8 ±16.8 experiment-4 99.5 ± 0.2 94.2 ± 1.1 99.7 ± 0.1 89.7 ± 1.2 experiment-5 95.6 ± 1.1 41.4 ± 6.2 99.6 ± 0.2 68.7 ± 7.100.0± 0.0 99.7 ± 0.3 100.0 ± 0.0 99.1 ± 0.95.7 ± 1.4 33.1 ± 2.4 99.2 ± 0.5 34.7 ± 0.8 G4 experiment-1 100.0 ± 0.0 79.4 ± 9.2 100.0 ± 0.0 99.6 ± 0.3 experiment-2 79.6 ± 5.2 0.0 ± 0.0 28.3 ±28.2 0.0 ± 0.0 experiment-3 61.8 ±30.9 0.0 ± 0.0 88.5 ± 3.6 0.9 ± 0.2 experiment-4 97.0 ± 1.0 15.2 ± 6.3 91.6 ± 2.2 24.7 ± 2.9 experiment-5 96.0 ± 0.8 0.1 ± 0.1 98.3 ± 0.7 34.7 ± 6.</p>
<p>Table 1 :
1
Performance of models on COGITAO experiments for the CompGen and EnvGen studies across different experiment settings (ES).We report the ID (in-domain) and OOD (out-of-domain) results for Grid accuracy (i.e., percentage of perfect match) and the relative drop (∆ = ID -OOD) averaged over all the experiments within the experiment setting.
Vanilla-ViTGrid-ViTLLaDAStudyESIDOOD∆IDOOD∆IDOOD∆C1 28.4 15.7 12.7 71.1 18.7 52.4 54.2 26.2 28.0CompGenC2 34.0 15.6 18.4 69.0 18.6 50.4 49.2 15.2 34.0C3 15.11.0 14.1 75.65.1 70.5 78.96.472.5G1 98.3 76.7 21.6 99.2 90.09.2 99.4 89.1 10.3EnvGenG2 68.10.0 68.1 97.0 73.0 24.0 99.3 64.3 35.0G3 57.1 17.5 39.6 78.5 27.2 51.3 76.9 27.2 49.7G4 47.6 21.3 26.3 75.2 13.8 61.4 76.3 35.6 40.7G5 70.00.0 70.0 98.00.2 97.8 71.0 14.0 57.0</p>
<p>Table 2 :
2
Overview of the encoder networks for the different architectural and modeling techniques considered.
ArchitecturalModelingEncoderAPERPEPEMixerVT RegistersResNet-----Vanilla ViTlearned-sum--Grid ViT2D-sincos w/OPE RoPE vec weighted sum ✓✓LLaDA-RoPE-✓-
APE: Absolute Positional Encoding.OPE: Object Positional Encoding.RPE: Relative Positional Encoding.VT: Visual Tokens.Registers: Register tokens.PEMixer: Positional Encoding Mixer, where "vec weighted sum" signifies a vector-weighted sum.</p>
<p>We open source the code on GitHub (https://github.com/yassinetb/COGITAO) as well as the benchmark datasets we propose in this paper (https://huggingface.co/datasets/yassinetb/COGITAO).Preprint. Under review.
We mean by "adequate" a grid configuration that would not yield too many failed generations. For instance, it is not practical to attempt positioning 10 objects within a 5x5 grid and perform e.g.
object transformations.4 These figures were obtained on an Ubuntu 22.04.5 LTS machine equipped with two AMD EPYC 7742 64-Core Processors (128 cores total) running at up to 2.25 GHz, and 256 GB of system RAM
Training of the ViT models was performed using an NVIDIA GeForce RTX 3090 GPU with 24 GB of VRAM, while training of the LLaDA model used an NVIDIA V100 GPU with 32 GB of VRAM.
AcknowledgmentsThis work is part of the "Learn to learn safely" project funded by a grant of the Hasler foundation (grant nr: 21039).We thank Joonsu Gha for valuable guidance and feedback on the experimental section of this paper.We also thank Kevin Lopez Andrade and Michael Hodel for their insightful ideas and contributions to early versions of this work.We are grateful to Rachida Assoughay for fruitful discussions that motivated this work.Finally, we thank Anas Ben Mejdoub for help with the visuals, and Chiara Louisa Hempel for help with coordination and structuring.
Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, Cognition. 281-2March 1988</p>
<p>Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. Brenden Lake, Marco Baroni, The 35th International Conference on Machine Learning (ICML). PMLRJuly 201880of Proceedings of Machine Learning Research</p>
<p>Deep neural networks and humans both benefit from compositional language structure. Lukas Galke, Yoav Ram, Limor Raviv, Nature Communications. 15110816December 2024</p>
<p>Why Generality Is Key to Human-Level Artificial Intelligence. R Tarek, Ute Besold, Schmid, Advances in Cognitive Systems. 20164</p>
<p>Measuring Compositional Generalization: A Comprehensive Method on Realistic Data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc Van Zee, Olivier Bousquet, The 8th International Conference on Learning Representations (ICLR). 2020</p>
<p>Compositional Generalization in Unsupervised Compositional Representation Learning: A Study on Disentanglement and Emergent Language. Zhenlin Xu, Marc Niethammer, Colin A Raffel, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc202235</p>
<p>Compositional Generalization from First Principles. Thaddäus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, Wieland Brendel, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc202336</p>
<p>Discovering modular solutions that generalize compositionally. Simon Schug, Seijin Kobayashi, Yassir Akram, Maciej Wolczyk, Alexandra Maria Proca, Razvan Johannes Von Oswald, Joao Pascanu, Angelika Sacramento, Steger, The 12th International Conference on Learning Representations (ICLR). 2024</p>
<p>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). July 2017</p>
<p>A Benchmark for Compositional Visual Reasoning. Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, Thomas Serre, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc202235</p>
<p>Measuring abstract reasoning in neural networks. David Barrett, Felix Hill, Adam Santoro, Ari Morcos, Timothy Lillicrap, The 35th International Conference on Machine Learning (ICML). PMLRJuly 201880of Proceedings of Machine Learning Research</p>
<p>Comparing machines and humans on a visual categorization test. François Fleuret, Ting Li, Charles Dubout, Emma K Wampler, Steven Yantis, Donald Geman, Proceedings of the National Academy of Sciences. 10843October 2011</p>
<p>Faith and Fate: Limits of Transformers on Compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, ( Xiang, ) Lorraine, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Le Ronan, Jena Bras, Soumya Hwang, Xiang Sanyal, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc202336</p>
<p>CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks. Roberto Dessì, Marco Baroni, The 57th Annual Meeting of the Association for Computational Linguistics (ACL). Florence, ItalyAssociation for Computational Linguistics2019</p>
<p>Good-Enough Compositional Data Augmentation. Jacob Andreas, The 58th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics2020</p>
<p>Solving SCAN Tasks with Data Augmentation and Input Embeddings. Michal Auersperger, Pavel Pecina, The International Conference on Recent Advances in Natural Language Processing (RANLP). INCOMA LtdSeptember 2021</p>
<p>Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks. João Loula, Marco Baroni, Brenden Lake, The 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Brussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. Najoung Kim, Tal Linzen, The 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics2020</p>
<p>Improving Text-to-SQL Evaluation Methodology. Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev, The 56th Annual Meeting of the Association for Computational Linguistics (ACL). Melbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Analysing Mathematical Reasoning Abilities of Neural Models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, The 7th International Conference on Learning Representations (ICLR). 2019</p>
<p>François Chollet, arXiv:1911.01547On the Measure of Intelligence. 2019arXiv preprint</p>
<p>Raven Progressive Matrices. John , Jean Raven, Handbook of Nonverbal Assessment. Boston, MASpringer US2003</p>
<p>MONet: Unsupervised Scene Decomposition and Representation. Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, Alexander Lerchner, arXiv:1901.1139020191arXiv preprint</p>
<p>Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models. Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin Ahn, The 27th Conference on Neural Information Processing Systems (NeurIPS): Datasets and Benchmarks Track. 2023</p>
<p>Cater: A diagnostic dataset for compositional actions and temporal reasoning. Rohit Girdhar, Deva Ramanan, arXiv:1910.047442019arXiv preprint</p>
<p>The Con-ceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, Melanie Mitchell, Transactions on Machine Learning Research (TMLR). 2023</p>
<p>Ryan Greenblatt, Getting 50% (SOTA) on ARC-AGI with GPT-4o. June 2024</p>
<p>Don't Throw the Baby Out With the Bathwater: How and Why Deep Learning for ARC. Jack Cole, Mohamed Osman, March 2025</p>
<p>François Chollet, OpenAI o3 Breakthrough High score on ARC-AGI-Pub. December 2024</p>
<p>Object-Centric Learning with Slot Attention. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc202033</p>
<p>. Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, Francesco Locatello, arXiv:2209.1486020222Bridging the Gap to Real-World Object-Centric Learning. arXiv preprint</p>
<p>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, The 9th International Conference on Learning Representations (ICLR). 2021</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects. Wenhao Li, Yudong Xu, Scott Sanner, Elias Boutros Khalil, arXiv:2410.0640520241arXiv preprint</p>
<p>Vision Transformers Need Registers. Timothée Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski, The 12th International Conference on Learning Representations (ICLR). 2024</p>
<p>Self-Attention with Relative Position Representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, The 2018 Conference of the North American Chapter. New Orleans, LouisianaAssociation for Computational Linguistics20182</p>
<p>RoFormer: Enhanced transformer with Rotary Position Embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, Neurocomputing. 568127063February 2024</p>
<p>Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li, arXiv:2502.09992Large Language Diffusion Models. 20252arXiv preprint</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, The 7th International Conference on Learning Representations (ICLR). 2019</p>
<p>Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation. Michael Hodel, arXiv:2404.0735320241arXiv preprint</p>
<p>Objectcentric Compositional Imagination for Visual Abstract Reasoning. Rim Assouel, Pau Rodriguez, Perouz Taslakian, David Vazquez, Yoshua Bengio, ICLR 2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality. 2022</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 211November 2020</p>
<p>How Do In-Context Examples Affect Compositional Generalization?. Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, Dongmei Zhang, The 61st Annual Meeting of the Association for Computational Linguistics (ACL). Toronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study. Xingxuan Zhang, Haoran Wang, Jiansheng Li, Yuan Xue, Shikai Guan, Renzhe Xu, Hao Zou, Han Yu, Peng Cui, The 13th International Conference on Learning Representations (ICLR). 2025</p>
<p>On the generalization of language models from in-context learning and finetuning: a controlled study. Andrew K Lampinen, Arslan Chaudhry, Stephanie C Y Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, James L Mcclelland, arXiv:2505.0066120252arXiv preprint</p>
<p>Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NV, USAIEEEJune 2016</p>
<p>Batch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, 2015</p>
<p>Deep learning using rectified linear units (relu). Abien Fred, Agarap , 2019</p>
<p>Learning deep transformer models for machine translation. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, Lidia S Chao, 2019</p>
<p>On layer normalization in the transformer architecture. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu, 2020</p>
<p>. Dan Hendrycks, Kevin Gimpel, 2023Gaussian error linear units (gelus</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Ofir Press, Noah A Smith, Mike Lewis, 2022</p>
<p>Root Mean Square Layer Normalization. Biao Zhang, Rico Sennrich, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc201932</p>
<p>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Stefan Elfwing, Eiji Uchibe, Kenji Doya, Neural Networks. 107November 2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-347 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-347</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-347</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-198926035</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/W19-1608.pdf" target="_blank">What a neural language model tells us about spatial relations</a></p>
                <p><strong>Paper Abstract:</strong> Understanding and generating spatial descriptions requires knowledge about what objects are related, their functional interactions, and where the objects are geometrically located. Different spatial relations have different functional and geometric bias. The wide usage of neural language models in different areas including generation of image description motivates the study of what kind of knowledge is encoded in neural language models about individual spatial relations. With the premise that the functional bias of relations is expressed in their word distributions, we construct multi-word distributional vector representations and show that these representations perform well on intrinsic semantic reasoning tasks, thus confirming our premise. A comparison of our vector representations to human semantic judgments indicates that different bias (functional or geometric) is captured in different data collection tasks which suggests that the contribution of the two meaning modalities is dynamic, related to the context of the task.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e347.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e347.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VG-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative LSTM language model trained on Visual Genome descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent (LSTM) generative language model trained on Visual Genome image-region descriptions that learns 300-dim word embeddings and 300 LSTM-unit hidden states; used as the core text-only model whose predictions (perplexities) are probed to reveal spatial semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM generative language model (trained on Visual Genome)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-layer LSTM-based language model with a 300-dimensional embedding layer and 300 LSTM units, trained with Adam for 20 epochs on ~4.54M Visual Genome region descriptions (vocabulary ≈ 4,985), batch size 1024; used as a decoder/generative LM to estimate next-word probabilities and sentence perplexities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Textual spatial-relation prediction and representation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict next words in image-region descriptions and evaluate fit (perplexity) of the LM on sentences where spatial relation tokens are swapped into other contexts; used to (i) measure selectional preferences of spatial relations and (ii) derive vector representations of relations and embeddings for semantic tests (analogy, odd-one-out, human-correlation). No embodied environment or sensory input is used during these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational / spatial (text-only prediction and representation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (functional selectional preferences encoded in text distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (Visual Genome image-region descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing via perplexity on artificially generated substituted-context sentences; intrinsic evaluation via analogy/odd-one-out; extraction of internal word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit knowledge encoded in model weights and learned word embeddings (300-D); operationalized externally by computing sentence perplexities for swapped relations (see P-vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Analogy accuracy (word/relation analogies), odd-one-out accuracy, Spearman correlation with human judgments (word-association and direct spatial-similarity tasks); perplexity used as model fit measure for contexts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitatively, LM internal embeddings (LM-embeddings) achieved the best performance on single-word analogical tasks; LM-embeddings also performed best on odd-one-out tests, while LM-based P-vectors (perplexity-derived) performed slightly better on multi-word relation analogy tasks. Exact numeric accuracies are reported in the paper's Tables 3–5.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Encodes functional/object-relational selectional preferences (which objects co-occur with which spatial relations), allowing discrimination between functionally-biased relations (e.g., above vs. over) and supporting analogical reasoning based on distributional context; internal embeddings and perplexity-based probes both capture functional associations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Lacks direct geometric grounding — geometric distinctions that require visual scene information (precise metric/location grounding) are not reliably encoded; confusions occur for geometrically ambiguous relations (e.g., left vs. right, front vs. back) when functional cues are weak; multi-word composition by simple vector-sum reduces discrimination for LM-embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against GloVe trained on the same Visual Genome corpus (GloVe VG), pre-trained GloVe on Common Crawl (GloVe CC), spatial templates (geometric templates from psycholinguistic work), and a random baseline; results: GloVe CC > GloVe VG generally, LM-embeddings often outperform GloVe on single-word tests, and P-vectors outperform GloVe on multi-word relation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit component ablation studies reported in this paper (no removal of LM components or embedding layers to measure impact).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A text-only generative LSTM encodes strong functional, object-relational selectional knowledge about spatial relations in its embeddings and prediction behavior; this text-derived knowledge partly complements (and sometimes conflates with) geometric information, but does not substitute for direct visual/geometric grounding required for precise scene-localization tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e347.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e347.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P-vectors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity-based contextual vectors (P-vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vector representations of spatial relations derived by measuring a pretrained language model's normalized perplexity when a relation token is swapped into many held-out relation contexts; capture selectional/functional preferences of multi-word spatial expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM generative language model (used to compute perplexities)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a neural architecture by itself but a method: for each spatial relation rel_i, assemble sentence collections where rel_i is placed into contexts of rel_j (S_i→j), compute LM perplexities PP_i,j, normalize per-column, and form a vector of normalized perplexities across all target contexts (57-D or similar).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Contextual discrimination and analogy of multi-word spatial relations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Construct P-vectors for each spatial relation by evaluating LM perplexity on held-out Visual Genome sentences with swapped relations; use these vectors for clustering, analogical reasoning, odd-one-out, and correlation with human judgments to probe what spatial knowledge the LM carries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational / spatial representation and reasoning (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (functional selectional preferences, multi-word relation discrimination); also captures latent geometric signals via distributional complementarity</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (Visual Genome descriptions) and statistical measurement (perplexity) over held-out contexts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing via structured context substitution and perplexity measurement (intrinsic probing and downstream intrinsic/extrinsic vector tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>External, explicit vector of normalized perplexities (low-dimensional, e.g., 57-D) representing how well each relation fits contexts of other relations — effectively a selectional-preference profile per relation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Analogy accuracy on multi-word and single-word relation analogies, odd-one-out accuracy, clustering coherence, Spearman correlation with human word-association and direct-similarity judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>P-vectors perform competitively: they slightly outperform LM-embeddings on multi-word analogical reasoning and outperform GloVe variants on multi-word tests; they rank comparably to LM-embeddings on odd-one-out tests and show a moderate positive correlation with human word-association judgments (~0.48 reported in the paper for association correlation). Exact task accuracies are provided in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong at capturing functional/object-selection preferences for multi-word spatial expressions; effective at clustering semantically similar relations and at analogical inference where selectional preferences drive similarity (e.g., above/over distinctions).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Less able to resolve purely geometric distinctions that require metric or perceptual grounding; some confusion remains for relations with low functional selectivity (e.g., left/right), and low performance on many multi-word variants when treated by simple compositional methods is observed elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to LM-internal embeddings and GloVe (VG and CC) and spatial templates: P-vectors beat GloVe on multi-word relation tasks, are close to LM-embeddings on many tasks, and show different correlation profiles with human judgments than spatial templates.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation studies removing components of the P-vector construction were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Perplexity-based probing yields compact, interpretable vectors that expose the LM's selectional/functional knowledge about spatial relations (especially for multi-word relations) and can capture latent geometric distinctions indirectly via complementary distributions, but they do not replace visual grounding for metric spatial reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e347.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e347.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Internal word embeddings extracted from the trained LSTM language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>300-dimensional word embeddings learned as part of the generative LSTMLM; used as direct vector representations for single-word spatial relations and summed for multi-word expressions to evaluate semantic and analogical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM internal word embeddings (300-D)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Word embedding layer learned jointly with LSTM language model parameters (300 dims). Multi-word relation vectors produced by summing component token embeddings (simple additive composition).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embedding-based spatial relation reasoning (analogy, odd-one-out, similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use learned embeddings to compute cosine similarities and perform analogy/odd-one-out tasks and correlations with human judgments, evaluating how much spatial/functional/object-relational knowledge the embedding weights capture.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational / spatial (text representation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (encoded implicitly in embedding geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (Visual Genome descriptions during LM training)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>direct extraction of learned embeddings (probing via analogical vector arithmetic, cosine similarity comparisons, and clustering)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicitly encoded in continuous embedding vectors (300-D) stored in model weights; compositional multi-word representations created by vector summation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Analogy and odd-one-out accuracy, Spearman correlation with word-association and direct similarity human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>LM-embeddings performed best on single-word analogical reasoning and odd-one-out tasks in the paper's evaluations; they show moderate correlation with word-association human data (~0.48) and weaker correlation with direct spatial-similarity judgments. Exact numeric accuracies are in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effectively encodes functional/associative relations between spatial prepositions and object contexts, supporting single-word analogy and odd-one-out tasks; embeddings reflect distributional biases useful for selectional preference reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Summation-based composition for multi-word relations reduces discrimination power (LM-embeddings performed worse than P-vectors on multi-word analogies), and embeddings lack direct metric/geometric grounding absent visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against P-vectors, GloVe (VG and CC), spatial templates; LM-embeddings often outperform GloVe (VG) and are competitive with GloVe CC on single-word tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No embedding-component ablations (e.g., removing embedding layer or varying dimension) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Internal LM embeddings capture functional selectional knowledge about spatial relations from text and are effective for single-word spatial reasoning tasks, but simple additive composition struggles for multi-word spatial expressions compared to perplexity-based contextual probes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e347.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e347.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Genome (VG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Genome: Connecting language and vision using crowdsourced dense image annotations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large dataset of dense image-region annotations and associated natural language region descriptions (used here as the text corpus emphasizing spatial usages of prepositions), providing the textual training and held-out data for LM training and P-vector construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual genome: Connecting language and vision using crowdsourced dense image annotations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>dataset (Visual Genome region descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corpus of ~5.4M image-region descriptions (authors used ~5.04M after preprocessing; trained/test split 90/10), chosen because region descriptions emphasize spatial uses of prepositions more than some other caption corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Corpus for training and probing text-only spatial-relation models</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used both to train the generative LSTM and to extract held-out context bins of sentences containing spatial relations for building P-vectors; chosen to maximize spatial usages of prepositions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>data source for spatial-text representation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational information present in human-written image-region descriptions (textual, functional biases)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>crowdsourced image-region text annotations</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>not applicable (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Natural language descriptions that embed selectional and co-occurrence statistics between relations and objects; used as the sole training signal for LM and P-vector construction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not applicable (dataset); used to enable downstream LM performance and probing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Provided the textual evidence from which LSTM encoded selectional/functional biases; corpus statistics: authors report training set ~4,537,836 descriptions, vocabulary ≈ 4,985 tokens after preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides dense examples where spatial prepositions are used in spatial senses, enabling learning of functional biases; richer in common spatial relations than some other caption datasets (e.g., MSCOCO lacked many occurrences of 'left of'/'right of').</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Being textual only, the dataset lacks direct metric/visual ground-truth to enable learning of precise geometric grounding solely from text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared in discussion to other corpora (MSCOCO, Flickr30k) which were judged to contain fewer explicit geometric spatial relation usages.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A corpus of image-region textual descriptions is a useful source for encoding functional spatial knowledge in language models; using such corpora, language-only models capture selectional preferences useful for spatial description generation but cannot replace perceptual grounding for geometric precision.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e347.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e347.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>functional/geometric bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional versus geometric bias of spatial relations (selectional preference versus metric grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual distinction: 'functional' bias refers to object-specific, usage-driven selectional preferences of spatial relations (what objects are related and their interactions), while 'geometric' bias refers to scene-geometry-based meanings (relative positions, orientations, metric constraints); the paper studies how language models capture these through distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conceptual framework used to interpret whether a spatial relation's distributional behavior is driven by functional (object-relation affordances, e.g., over used for covering) versus geometric (strict metric positions, e.g., above) aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Analysis of relation bias captured by text-only models</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as analytic lens: relations with strong functional bias are expected to be more predictable from object-context text distributions (lower perplexity when swapped into contexts), while geometrically-biased relations rely more on perceptual/visual geometry and are less predictable from text alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>analytical conceptual distinction informing representation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (functional) vs. spatial (geometric) — a two-part decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>linguistic theory and empirical distributional statistics extracted from Visual Genome text</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>evaluated by LM perplexity probing (P-vectors) and embedding similarity tests compared to human judgments and spatial templates</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Not a model but a labeling/interpretation applied to relations; realized empirically by differences in perplexity-based selectivity and embedding similarity profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Measured indirectly via perplexity differences, clustering behavior, analogy/odd-one-out performance, and correlation with human judgments and spatial templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Paper reports that functionally-biased relations (e.g., 'over') tend to be more selective/predictable from textual context (lower perplexity when placed in compatible contexts) while geometrically-biased relations (e.g., 'left','right') show higher confusion across contexts; quantified results appear across clustering, analogy, and correlation tests in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>The LM and derived representations reliably capture functional biases (selectional preferences) from text, enabling discrimination of relations differing primarily in function.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Geometric biases are only weakly and indirectly present in text-derived vectors and require visual grounding for reliable metric interpretation; text-only models can be ambiguous or confounded when functional cues are absent.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually to geometric spatial templates derived from human psycholinguistic experiments; the two sources (textual functional signals vs. geometric templates) are complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not applicable (conceptual analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language models trained on image-description text primarily encode functional, object-relational selectional knowledge about spatial relations; geometric knowledge can appear indirectly via complementary distributions but requires sensory/visual grounding for precise spatial reasoning in embodied tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the functional and geometric bias of spatial relations using neural language models <em>(Rating: 2)</em></li>
                <li>Learning to compose spatial relations with grounded neural language models <em>(Rating: 2)</em></li>
                <li>Acquiring common sense spatial knowledge through implicit spatial templates <em>(Rating: 2)</em></li>
                <li>Combining geometric, textual and visual features for predicting prepositions in image descriptions <em>(Rating: 2)</em></li>
                <li>A computational analysis of the apprehension of spatial relations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-347",
    "paper_id": "paper-198926035",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "VG-LSTM",
            "name_full": "Generative LSTM language model trained on Visual Genome descriptions",
            "brief_description": "A recurrent (LSTM) generative language model trained on Visual Genome image-region descriptions that learns 300-dim word embeddings and 300 LSTM-unit hidden states; used as the core text-only model whose predictions (perplexities) are probed to reveal spatial semantics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM generative language model (trained on Visual Genome)",
            "model_size": null,
            "model_description": "Single-layer LSTM-based language model with a 300-dimensional embedding layer and 300 LSTM units, trained with Adam for 20 epochs on ~4.54M Visual Genome region descriptions (vocabulary ≈ 4,985), batch size 1024; used as a decoder/generative LM to estimate next-word probabilities and sentence perplexities.",
            "task_name": "Textual spatial-relation prediction and representation",
            "task_description": "Predict next words in image-region descriptions and evaluate fit (perplexity) of the LM on sentences where spatial relation tokens are swapped into other contexts; used to (i) measure selectional preferences of spatial relations and (ii) derive vector representations of relations and embeddings for semantic tests (analogy, odd-one-out, human-correlation). No embodied environment or sensory input is used during these tasks.",
            "task_type": "object-relational / spatial (text-only prediction and representation)",
            "knowledge_type": "spatial + object-relational (functional selectional preferences encoded in text distributions)",
            "knowledge_source": "pre-training on text corpora (Visual Genome image-region descriptions)",
            "has_direct_sensory_input": false,
            "elicitation_method": "probing via perplexity on artificially generated substituted-context sentences; intrinsic evaluation via analogy/odd-one-out; extraction of internal word embeddings",
            "knowledge_representation": "Implicit knowledge encoded in model weights and learned word embeddings (300-D); operationalized externally by computing sentence perplexities for swapped relations (see P-vectors)",
            "performance_metric": "Analogy accuracy (word/relation analogies), odd-one-out accuracy, Spearman correlation with human judgments (word-association and direct spatial-similarity tasks); perplexity used as model fit measure for contexts",
            "performance_result": "Qualitatively, LM internal embeddings (LM-embeddings) achieved the best performance on single-word analogical tasks; LM-embeddings also performed best on odd-one-out tests, while LM-based P-vectors (perplexity-derived) performed slightly better on multi-word relation analogy tasks. Exact numeric accuracies are reported in the paper's Tables 3–5.",
            "success_patterns": "Encodes functional/object-relational selectional preferences (which objects co-occur with which spatial relations), allowing discrimination between functionally-biased relations (e.g., above vs. over) and supporting analogical reasoning based on distributional context; internal embeddings and perplexity-based probes both capture functional associations.",
            "failure_patterns": "Lacks direct geometric grounding — geometric distinctions that require visual scene information (precise metric/location grounding) are not reliably encoded; confusions occur for geometrically ambiguous relations (e.g., left vs. right, front vs. back) when functional cues are weak; multi-word composition by simple vector-sum reduces discrimination for LM-embeddings.",
            "baseline_comparison": "Compared against GloVe trained on the same Visual Genome corpus (GloVe VG), pre-trained GloVe on Common Crawl (GloVe CC), spatial templates (geometric templates from psycholinguistic work), and a random baseline; results: GloVe CC &gt; GloVe VG generally, LM-embeddings often outperform GloVe on single-word tests, and P-vectors outperform GloVe on multi-word relation evaluation.",
            "ablation_results": "No explicit component ablation studies reported in this paper (no removal of LM components or embedding layers to measure impact).",
            "key_findings": "A text-only generative LSTM encodes strong functional, object-relational selectional knowledge about spatial relations in its embeddings and prediction behavior; this text-derived knowledge partly complements (and sometimes conflates with) geometric information, but does not substitute for direct visual/geometric grounding required for precise scene-localization tasks.",
            "uuid": "e347.0"
        },
        {
            "name_short": "P-vectors",
            "name_full": "Perplexity-based contextual vectors (P-vectors)",
            "brief_description": "Vector representations of spatial relations derived by measuring a pretrained language model's normalized perplexity when a relation token is swapped into many held-out relation contexts; capture selectional/functional preferences of multi-word spatial expressions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM generative language model (used to compute perplexities)",
            "model_size": null,
            "model_description": "Not a neural architecture by itself but a method: for each spatial relation rel_i, assemble sentence collections where rel_i is placed into contexts of rel_j (S_i→j), compute LM perplexities PP_i,j, normalize per-column, and form a vector of normalized perplexities across all target contexts (57-D or similar).",
            "task_name": "Contextual discrimination and analogy of multi-word spatial relations",
            "task_description": "Construct P-vectors for each spatial relation by evaluating LM perplexity on held-out Visual Genome sentences with swapped relations; use these vectors for clustering, analogical reasoning, odd-one-out, and correlation with human judgments to probe what spatial knowledge the LM carries.",
            "task_type": "object-relational / spatial representation and reasoning (text-only)",
            "knowledge_type": "spatial + object-relational (functional selectional preferences, multi-word relation discrimination); also captures latent geometric signals via distributional complementarity",
            "knowledge_source": "pre-training on text corpora (Visual Genome descriptions) and statistical measurement (perplexity) over held-out contexts",
            "has_direct_sensory_input": false,
            "elicitation_method": "probing via structured context substitution and perplexity measurement (intrinsic probing and downstream intrinsic/extrinsic vector tasks)",
            "knowledge_representation": "External, explicit vector of normalized perplexities (low-dimensional, e.g., 57-D) representing how well each relation fits contexts of other relations — effectively a selectional-preference profile per relation.",
            "performance_metric": "Analogy accuracy on multi-word and single-word relation analogies, odd-one-out accuracy, clustering coherence, Spearman correlation with human word-association and direct-similarity judgments.",
            "performance_result": "P-vectors perform competitively: they slightly outperform LM-embeddings on multi-word analogical reasoning and outperform GloVe variants on multi-word tests; they rank comparably to LM-embeddings on odd-one-out tests and show a moderate positive correlation with human word-association judgments (~0.48 reported in the paper for association correlation). Exact task accuracies are provided in the paper's tables.",
            "success_patterns": "Strong at capturing functional/object-selection preferences for multi-word spatial expressions; effective at clustering semantically similar relations and at analogical inference where selectional preferences drive similarity (e.g., above/over distinctions).",
            "failure_patterns": "Less able to resolve purely geometric distinctions that require metric or perceptual grounding; some confusion remains for relations with low functional selectivity (e.g., left/right), and low performance on many multi-word variants when treated by simple compositional methods is observed elsewhere.",
            "baseline_comparison": "Compared to LM-internal embeddings and GloVe (VG and CC) and spatial templates: P-vectors beat GloVe on multi-word relation tasks, are close to LM-embeddings on many tasks, and show different correlation profiles with human judgments than spatial templates.",
            "ablation_results": "No ablation studies removing components of the P-vector construction were reported.",
            "key_findings": "Perplexity-based probing yields compact, interpretable vectors that expose the LM's selectional/functional knowledge about spatial relations (especially for multi-word relations) and can capture latent geometric distinctions indirectly via complementary distributions, but they do not replace visual grounding for metric spatial reasoning.",
            "uuid": "e347.1"
        },
        {
            "name_short": "LM-embeddings",
            "name_full": "Internal word embeddings extracted from the trained LSTM language model",
            "brief_description": "300-dimensional word embeddings learned as part of the generative LSTMLM; used as direct vector representations for single-word spatial relations and summed for multi-word expressions to evaluate semantic and analogical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM internal word embeddings (300-D)",
            "model_size": null,
            "model_description": "Word embedding layer learned jointly with LSTM language model parameters (300 dims). Multi-word relation vectors produced by summing component token embeddings (simple additive composition).",
            "task_name": "Embedding-based spatial relation reasoning (analogy, odd-one-out, similarity)",
            "task_description": "Use learned embeddings to compute cosine similarities and perform analogy/odd-one-out tasks and correlations with human judgments, evaluating how much spatial/functional/object-relational knowledge the embedding weights capture.",
            "task_type": "object-relational / spatial (text representation)",
            "knowledge_type": "spatial + object-relational (encoded implicitly in embedding geometry)",
            "knowledge_source": "pre-training on text corpora (Visual Genome descriptions during LM training)",
            "has_direct_sensory_input": false,
            "elicitation_method": "direct extraction of learned embeddings (probing via analogical vector arithmetic, cosine similarity comparisons, and clustering)",
            "knowledge_representation": "Implicitly encoded in continuous embedding vectors (300-D) stored in model weights; compositional multi-word representations created by vector summation.",
            "performance_metric": "Analogy and odd-one-out accuracy, Spearman correlation with word-association and direct similarity human judgments.",
            "performance_result": "LM-embeddings performed best on single-word analogical reasoning and odd-one-out tasks in the paper's evaluations; they show moderate correlation with word-association human data (~0.48) and weaker correlation with direct spatial-similarity judgments. Exact numeric accuracies are in the paper tables.",
            "success_patterns": "Effectively encodes functional/associative relations between spatial prepositions and object contexts, supporting single-word analogy and odd-one-out tasks; embeddings reflect distributional biases useful for selectional preference reasoning.",
            "failure_patterns": "Summation-based composition for multi-word relations reduces discrimination power (LM-embeddings performed worse than P-vectors on multi-word analogies), and embeddings lack direct metric/geometric grounding absent visual features.",
            "baseline_comparison": "Compared against P-vectors, GloVe (VG and CC), spatial templates; LM-embeddings often outperform GloVe (VG) and are competitive with GloVe CC on single-word tasks.",
            "ablation_results": "No embedding-component ablations (e.g., removing embedding layer or varying dimension) reported.",
            "key_findings": "Internal LM embeddings capture functional selectional knowledge about spatial relations from text and are effective for single-word spatial reasoning tasks, but simple additive composition struggles for multi-word spatial expressions compared to perplexity-based contextual probes.",
            "uuid": "e347.2"
        },
        {
            "name_short": "Visual Genome (VG)",
            "name_full": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations",
            "brief_description": "A large dataset of dense image-region annotations and associated natural language region descriptions (used here as the text corpus emphasizing spatial usages of prepositions), providing the textual training and held-out data for LM training and P-vector construction.",
            "citation_title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "mention_or_use": "use",
            "model_name": "dataset (Visual Genome region descriptions)",
            "model_size": null,
            "model_description": "Corpus of ~5.4M image-region descriptions (authors used ~5.04M after preprocessing; trained/test split 90/10), chosen because region descriptions emphasize spatial uses of prepositions more than some other caption corpora.",
            "task_name": "Corpus for training and probing text-only spatial-relation models",
            "task_description": "Used both to train the generative LSTM and to extract held-out context bins of sentences containing spatial relations for building P-vectors; chosen to maximize spatial usages of prepositions.",
            "task_type": "data source for spatial-text representation",
            "knowledge_type": "spatial + object-relational information present in human-written image-region descriptions (textual, functional biases)",
            "knowledge_source": "crowdsourced image-region text annotations",
            "has_direct_sensory_input": false,
            "elicitation_method": "not applicable (dataset)",
            "knowledge_representation": "Natural language descriptions that embed selectional and co-occurrence statistics between relations and objects; used as the sole training signal for LM and P-vector construction.",
            "performance_metric": "Not applicable (dataset); used to enable downstream LM performance and probing.",
            "performance_result": "Provided the textual evidence from which LSTM encoded selectional/functional biases; corpus statistics: authors report training set ~4,537,836 descriptions, vocabulary ≈ 4,985 tokens after preprocessing.",
            "success_patterns": "Provides dense examples where spatial prepositions are used in spatial senses, enabling learning of functional biases; richer in common spatial relations than some other caption datasets (e.g., MSCOCO lacked many occurrences of 'left of'/'right of').",
            "failure_patterns": "Being textual only, the dataset lacks direct metric/visual ground-truth to enable learning of precise geometric grounding solely from text.",
            "baseline_comparison": "Compared in discussion to other corpora (MSCOCO, Flickr30k) which were judged to contain fewer explicit geometric spatial relation usages.",
            "ablation_results": "Not applicable.",
            "key_findings": "A corpus of image-region textual descriptions is a useful source for encoding functional spatial knowledge in language models; using such corpora, language-only models capture selectional preferences useful for spatial description generation but cannot replace perceptual grounding for geometric precision.",
            "uuid": "e347.3"
        },
        {
            "name_short": "functional/geometric bias",
            "name_full": "Functional versus geometric bias of spatial relations (selectional preference versus metric grounding)",
            "brief_description": "A conceptual distinction: 'functional' bias refers to object-specific, usage-driven selectional preferences of spatial relations (what objects are related and their interactions), while 'geometric' bias refers to scene-geometry-based meanings (relative positions, orientations, metric constraints); the paper studies how language models capture these through distributions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": null,
            "model_description": "Conceptual framework used to interpret whether a spatial relation's distributional behavior is driven by functional (object-relation affordances, e.g., over used for covering) versus geometric (strict metric positions, e.g., above) aspects.",
            "task_name": "Analysis of relation bias captured by text-only models",
            "task_description": "Used as analytic lens: relations with strong functional bias are expected to be more predictable from object-context text distributions (lower perplexity when swapped into contexts), while geometrically-biased relations rely more on perceptual/visual geometry and are less predictable from text alone.",
            "task_type": "analytical conceptual distinction informing representation evaluation",
            "knowledge_type": "object-relational (functional) vs. spatial (geometric) — a two-part decomposition",
            "knowledge_source": "linguistic theory and empirical distributional statistics extracted from Visual Genome text",
            "has_direct_sensory_input": false,
            "elicitation_method": "evaluated by LM perplexity probing (P-vectors) and embedding similarity tests compared to human judgments and spatial templates",
            "knowledge_representation": "Not a model but a labeling/interpretation applied to relations; realized empirically by differences in perplexity-based selectivity and embedding similarity profiles.",
            "performance_metric": "Measured indirectly via perplexity differences, clustering behavior, analogy/odd-one-out performance, and correlation with human judgments and spatial templates.",
            "performance_result": "Paper reports that functionally-biased relations (e.g., 'over') tend to be more selective/predictable from textual context (lower perplexity when placed in compatible contexts) while geometrically-biased relations (e.g., 'left','right') show higher confusion across contexts; quantified results appear across clustering, analogy, and correlation tests in the paper.",
            "success_patterns": "The LM and derived representations reliably capture functional biases (selectional preferences) from text, enabling discrimination of relations differing primarily in function.",
            "failure_patterns": "Geometric biases are only weakly and indirectly present in text-derived vectors and require visual grounding for reliable metric interpretation; text-only models can be ambiguous or confounded when functional cues are absent.",
            "baseline_comparison": "Compared conceptually to geometric spatial templates derived from human psycholinguistic experiments; the two sources (textual functional signals vs. geometric templates) are complementary.",
            "ablation_results": "Not applicable (conceptual analysis).",
            "key_findings": "Language models trained on image-description text primarily encode functional, object-relational selectional knowledge about spatial relations; geometric knowledge can appear indirectly via complementary distributions but requires sensory/visual grounding for precise spatial reasoning in embodied tasks.",
            "uuid": "e347.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the functional and geometric bias of spatial relations using neural language models",
            "rating": 2,
            "sanitized_title": "exploring_the_functional_and_geometric_bias_of_spatial_relations_using_neural_language_models"
        },
        {
            "paper_title": "Learning to compose spatial relations with grounded neural language models",
            "rating": 2,
            "sanitized_title": "learning_to_compose_spatial_relations_with_grounded_neural_language_models"
        },
        {
            "paper_title": "Acquiring common sense spatial knowledge through implicit spatial templates",
            "rating": 2,
            "sanitized_title": "acquiring_common_sense_spatial_knowledge_through_implicit_spatial_templates"
        },
        {
            "paper_title": "Combining geometric, textual and visual features for predicting prepositions in image descriptions",
            "rating": 2,
            "sanitized_title": "combining_geometric_textual_and_visual_features_for_predicting_prepositions_in_image_descriptions"
        },
        {
            "paper_title": "A computational analysis of the apprehension of spatial relations",
            "rating": 1,
            "sanitized_title": "a_computational_analysis_of_the_apprehension_of_spatial_relations"
        }
    ],
    "cost": 0.015837999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What a neural language model tells us about spatial relations
Association for Computational LinguisticsCopyright Association for Computational LinguisticsJune 6, 2019. 2019</p>
<p>Mehdi Ghanimifard mehdi.ghanimifard@gu.se 
Department of Philosophy, Linguistics and Theory of Science (FLoV)
Centre for Linguistic Theory and Studies in Probability (CLASP)
University of Gothenburg
Sweden</p>
<p>Simon Dobnik simon.dobnik@gu.se 
Department of Philosophy, Linguistics and Theory of Science (FLoV)
Centre for Linguistic Theory and Studies in Probability (CLASP)
University of Gothenburg
Sweden</p>
<p>What a neural language model tells us about spatial relations</p>
<p>Proceedings of SpLU-RoboNLP
SpLU-RoboNLPMinneapolis, MNAssociation for Computational LinguisticsJune 6, 2019. 201971
Understanding and generating spatial descriptions requires knowledge about what objects are related, their functional interactions, and where the objects are geometrically located. Different spatial relations have different functional and geometric bias. The wide usage of neural language models in different areas including generation of image description motivates the study of what kind of knowledge is encoded in neural language models about individual spatial relations. With the premise that the functional bias of relations is expressed in their word distributions, we construct multi-word distributional vector representations and show that these representations perform well on intrinsic semantic reasoning tasks, thus confirming our premise. A comparison of our vector representations to human semantic judgments indicates that different bias (functional or geometric) is captured in different data collection tasks which suggests that the contribution of the two meaning modalities is dynamic, related to the context of the task.</p>
<p>Introduction</p>
<p>Spatial descriptions such as "the chair is to the left of the table" contain spatial relations "to the left of" the semantic representations of which must be grounded in visual representations in terms of geometry (Harnad, 1990). The apprehension of spatial relations in terms of scene geometry has been investigated through acceptability scores of human judges over possible locations of objects (Logan and Sadler, 1996). In addition, other research has pointed out that there is an interplay between geometry and object-specific function in the apprehension of spatial relations (Coventry et al., 2001). Therefore, spatial descriptions must be grounded in two kinds of knowledge (Landau and Jackendoff, 1993;Coventry et al., 2001;Coventry and Garrod, 2004;Landau, 2016). One kind of knowledge is referential meaning, expressed in the geometry of scenes (geometric knowledge or where objects are) while the other kind of knowledge is higher-level conceptual world knowledge about interactions between objects which is not directly grounded in perceivable situations but is learned through our experience of situations in the world (functional knowledge or what objects are related). Furthermore, Coventry et al. (2001) argue that individual relations have a particular geometric and functional bias and "under" and "over" are more functionally-biased than "below" and "above". For instance, when describing the relation between a person and an umbrella in a scene with a textual context such as "an umbrella a person", "above" is associated with stricter geometric properties compared to "over" which covers a more object-specific extra-geometric sense between the target and the landmark (i.e. covering or protecting in this case). Of course, there will be several configurations of objects that could be described either with "over" or "above" which indicates that the choice of a description is determined by the speaker, in particular what aspect of meaning they want to emphasise. Coventry et al. (2001) consider this bias for prepositions that are geometrically similar and therefore the functional knowledge is reflected in different preferences for objects that are related. However, such functional differences also exist between geometrically different relations.</p>
<p>This poses two interesting research questions for computational modelling of spatial language. The first one is how both kinds of knowledge interact with individual spatial relations and how models of spatial language can be constructed and learned within end-to-end deep learning paradigm. Ramisa et al. (2015) compare the performance of classifiers using different multi-modal features (visual, geometric and textual) to predict a spatial preposition. Schwering (2007) applies semantic similarity metrics of spatial relations on geo-graphical data retrieval. Collell et al. (2018) show that word embeddings can be used as predictive features for common sense knowledge about location of objects in 2D images. The second question is related to the extraction of functional knowledge for applications such as generation of spatial descriptions in a robot scenario. Typically, a robot will not be able to observe all object interactions as in  to learn about the interaction of objects and choose the appropriate relation. Following the intuition that the functional bias of spatial relations is reflected in a greater selectivity for their target and landmark objects, Kelleher (2013, 2014) propose that the degree of association between relations and objects in the corpus of image descriptions can be used as filters for selecting the most applicable relation for a pair of objects. They also demonstrate that entropy-based analysis of the targets and landmarks can identify the functional and geometric bias of spatial relations. They use descriptions from a corpus of image descriptions because here the prepositions in spatial relations are used mainly in the spatial sense. The same investigation of textual corpora such as BNC (Consortium et al., 2007) does not yield such results as there prepositions are used mainly in their nonspatial sense. 1 Similarly, Dobnik et al. (2018) inspect the perplexity of recurrent language models for different descriptions containing spatial relations in the Visual Genome dataset of image captions (Krishna et al., 2017) in order to investigate their bias for objects.</p>
<p>In this paper, we follow this line of work and (i) further investigate what semantics about spatial relations are captured from descriptions of images by generative recurrent neural language models, and (ii) whether such knowledge can be extracted, for example as vector representations, and evaluated in tests. The neural embeddings are opaque to interpretations per se. The benefit of using recurrent language models is that they allow us to (i) deal with spatial relations as multi-word expressions and (ii) they learn their representations within their contexts:</p>
<p>(a) a cat on a mat (b) a cat on the top o f a mat (c) a mat under a cat 1 We may call this metaphoric or highly functional usage which is completely absent of the geometric dimension.</p>
<p>In (a) and (b), the textual contexts are the same "a cat a mat" but the meaning of the spatial relations, one of which is a multi-word expression, are slightly different. In (c) the context is made different through word order.</p>
<p>The question of what knowledge (functional or geometric) should be represented in the models can be explained in information-theoretic terms. The low surprisal of a textual language model on a new text corpora is an indication that the model has encoded the same information content as the text. In the absence of the geometric knowledge during the training of the model, this means that a language model encodes the relevant functional knowledge. We will show that the degree to which each spatial description containing a spatial relation encodes functional knowledge in different contexts can be used as source for building distributional representations. We evaluate these representations intrinsically in reasoning tests and extrinsically against human performance and human judgment.</p>
<p>The contributions of this paper are:</p>
<ol>
<li>It is an investigation of the semantic knowledge about spatial relations learned from textual features in recurrent language models with intrinsic and extrinsic methods of evaluation on internal representations. 2. It proposes a method of inspecting contextual performance of generative neural language models over a wide categories of contexts.</li>
</ol>
<p>This paper is organised as follows: in Section 2 we describe how we create distributional representations with recurrent neural language models, in Section 3 we describe our computational implementations that build these representations, and in Section 4 we provide their evaluation. In Section 5 we give our final remarks.</p>
<p>Neural representations of spatial relations</p>
<p>Distributional semantic models produce vector representations which capture latent meanings hidden in association of words in documents (Church and Hanks, 1990;Turney and Pantel, 2010). The neural word embeddings were initially introduced as a component of neural language models (Bengio et al., 2003). However, subsequently neural language models such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have become used to specifically learn word embeddings from large corpora. The word embeddings trained by these models capture world-knowledge regularities expressed in language by learning from the distribution of context words which can be used for analogical reasoning 2 . Moreover, sense embeddings (Neelakantan et al., 2014) and contextual embeddings (Peters et al., 2018) have shown to provide finegrained representation which can discriminate between different word senses or contexts, for example in substituting synonym words and multiwords in sentences (McCarthy and Navigli, 2007). However, meaning is also captured by generative recurrent neural language models used to generate text rather than predict word similarity. The focus of our work is to investigate what semantics about spatial relations is captured by these models. Generative language models use the chain rule of probability for step-by-step prediction of the next word in a sequence. In these models, the probability of a sequence of words (or sometimes characters) is defined as the multiplication of conditional probabilities of each word given the previous context in a sequence:
P(w 1:T ) = T −1 ∏ t=1 P(w t+1 |w 1:t )(1)
where T is the length of the word sequence. The language model estimates the probability of a sequence in Equation (1) by optimising parameters of a neural network trained over sufficient data. The internal learned parameters includes embeddings for each word token which can be used as word level representations directly. An alternative way of extracting semantic prediction from a generative neural language model which we are going to explore in this paper is to measure the fidelity of the model's output predictions against a new ground truth sequence of words. This is expressed in the measure of Perplexity as follows:
PP(S) = ( ∏ s∈S P(w 1:t = s)) −1 |S| (2)
where S is a collection of ground truth sentences. Perplexity is a measure of the difficulty of a gen-2 For example, "a is to a * as b is to b * " can be queried with simple vector arithmetic king − man + woman ≈ queen. More specifically, with a search over vocabulary with cosine similarity:
arg max b * ∈V /{a * ,b,a} cos(b * , a * − a + b)
eration task which is based on the information theoretic concept of entropy (Bahl et al., 1983). It is based on cross-entropy which takes into account the probability of a sequence of words in ground truth sentences and the probability of a language model generating that sequence. It is often used for intrinsic evaluation of word-error rates in NLP tasks (Chen et al., 1998). However, in this paper we use perplexity as a measure of fit of a pretrained generative neural language model to a collection of sentences.</p>
<p>Our proposal is as follows. We start with the hypothesis that in spatial descriptions some spatial relations (those that we call functional) are more predictable from the associated word contexts of targets and landmarks than their grounding in the visual features. Hence, this will be reflected in a perplexity of a (text-based) generative language model trained on spatial descriptions. Descriptions with functionally-biased spatial relations will be easier to predict by this language model than geometrically-biased spatial descriptions and will therefore have lower perplexity. If two sequences of words where only the spatial relations differ (but target and landmark contexts as well as other words are the same) have similar perplexity, it means that such spatial relations have similar selectional requirements and are therefore similar in terms of functional and geometric bias. We can exploit this to create vector representations for spatial relations as follows. Using a dictionary of spatial relations, we extract collections of sentences containing a particular spatial relation from a held-out dataset not used in training of the language model. The collection of sentences with a particular spatial relation are our context templates. More specifically, for our list of spatial relations {r 1 , r 2 , ..., r k }, we replace the original relation r i with a target relation r j in its collection of sentences, e.g. we replace to the right of i with in front of j . The outcome is a collection of artificial sentences S i→ j that are identical to the humangenerated sentences except that they contain a substituted spatial relation. The perplexity of the language model on these sentences represents the association between the original spatial relation and the context in which this has been projected:
PP(S i→ j ) = PP i, j = P(rel i , c rel j ) 1 −N (3)
where c rel j is the context of rel i , and PP i, j is the perplexity of the neural language model on the sentence collection where relation rel i is artificially placed in the contexts of relation rel j . If rel i and rel j are associated with similar contexts, then we expect low perplexity for S i→ j , otherwise the perplexity will be high. Finally, the perplexity of rel i against each collection c rel j is computed and normalised within each collection (Equation 4) and the resulting vector per rel i over all contexts is represented as a unit vector (Equation 5).
m i, j = PP i, j ∑ k i =1 PP i , j (4) v i = v i ||v i || v i = (m i,1 , ..., m i,k ) T (5)
wherev i is the vector representation of the relation rel i . These vectors create a matrix. In a particular cell of some row and some column, high perplexity means that the spatial relation in that row is less swappable with the context in the column, while a low perplexity means that the spatial relation is highly swappable with that context. This provides a measure similar to mutual information (PPMI) in traditional distributional vectors (Church and Hanks, 1990).</p>
<p>In conclusion, representing multi-word spatial relations in a perplexity matrix of different contexts allows us to capture their semantics based on the predictions and the discriminatory power of the language model. If all spatial relations are equally predictable from the language model such vector representations will be identical and vector space norms will not be able to discriminate between different spatial relations. In the following sections we report on the practical details how we build the matrix (Section 3) and evaluate it on some typical semantic tasks (Section 4). The implementation and evaluation code: https: //github.com/GU-CLASP/what_nlm_srels 3 Dataset and models</p>
<p>Corpus and pre-processing</p>
<p>We use Visual Genome region description corpus (Krishna et al., 2017). This corpus contains 5.4 million descriptions of 108 thousand images, collected from different annotators who described specific regions of each image. As stated earlier, the reason why we use a dataset of image descriptions is because we want to have spatial usages of prepositions. Other image captioning datasets such as MSCOCO (Lin et al., 2014) andFlickr30k (Plummer et al., 2015) could also be used. However, our investigation has shown that since the task in these datasets in not to describe directly the relation between selected regions, common geometric spatial relations are almost missing in them: there are less then 30 examples for "left of " and "right of " in these datasets.</p>
<p>After word tokenisation with the space operator, we apply pre-processing which removes repeated descriptions per-image and also descriptions that include uncommon words with frequency less than 100 3 Then we split the sentences into 90%-10% portions. The 90% is used for training the language model (Section 3.2), and 10% is used for generating the perplexity vectors by extracting sentences with spatial relations that represent our context bins (Section 3.3). The context bins are used for generating artificial descriptions S i→ j on which the language model is evaluated for perplexity.</p>
<p>Language model and GloVe embeddings</p>
<p>We train a generative neural language model on the 90% of the extracted corpus (Section 3.1) which amounts to 4,537,836 descriptions of maximum length of 29 and 4,985 words in the vocabulary. We implement a recurrent language model with LSTM (Hochreiter and Schmidhuber, 1997) and a word embeddings layer similar to Gal and Ghahramani (2016) in Keras (Chollet et al., 2015) with TensorFlow (Abadi et al., 2015) as back-end. The Adam optimiser (Kingma and Ba, 2014) is used for fitting the parameters. The model is set up with 300 dimensions both for the embeddingand the LSTM units. It is trained for 20 epochs with a batch size of 1024.</p>
<p>In addition to the generative LSTM language model, we also train on the same corpus GloVe (VG) embeddings with 300 dimensions and a context-window of 5 words. Finally, we also use pre-trained GloVe embeddings on the Common Crawl (CC) dataset with 42B tokens 4 .  Figure 1: Generating perplexity-based vectors for each spatial relation.</p>
<p>Perplexity vectors</p>
<p>Based on the lists of spatial prepositions in (Landau, 1996) and (Herskovits, 1986), we have created a dictionary of spatial relations which include single word relations as well as all of their possible multi-word variants. This dictionary was applied on the 10% held-out dataset where we found 67 single-and multi-word spatial relation types in total. As their frequency may have fallen below 100 words due to the dataset split, we further remove all relations below this threshold which gives us 57 relations. We also create another list of relations where composite variants such as "to the left of" and "on the left of" are grouped together as "left" which contains 44 broad relations. We group the sentences by the relation they are containing to our context bins using simple pattern matching on strings. Table 1 contains some examples of our context bins. The bins are used for artificial sentence generation as explained in the previous section.</p>
<p>Relation (rel i ) Context bin (c rel i ) above scissors the pen tall building the bridge · · · below pen is scissors bench the green trees · · · next to a ball-pen the scissors car the water · · · For each of the 67 spatial relations extracted from the larger corpus, there are 57 collections of sentences (=the number of relations in the smaller corpus). Hence, there are 3, 819(= 67 × 57) possible projections S i→ j , where a relation i is placed in the context j, including the case where there is no swapping of relations when j = i. The process is shown in Figure 1. The vector of resulting perplexities in different contexts is normalised according to Equation 5 which gives us perplexity vectors (P-vectors) as shown in Figure 2. In addition to the P-vectors we also create representations learned by the word embedding layer in the generative language model that we train. For each of the 44 broad single-word spatial relations we extract a 300-dimensional embedding vector from the pre-trained recurrent language model (LM-vectors). In order to produce LM-vectors for the multi-word spatial relations, we simply sum the embeddings of the individual words. For example the embedding vector for "to the left of" is v to + v the + v le f t + v o f . The same method is also used for the GloVe embeddings.</p>
<p>Human judgments</p>
<p>In order to evaluate our word representations we compare them to three sources of human judgments. The first one are judgments about the the fit of each spatial relation over different geometric locations of a target object in relation to a landmark which can be represented as spatial templates (Logan and Sadler, 1996). The second are 88,000 word association judgments by English speakers from (De Deyne et al., 2018). In each instance participants were presented a stimulus word and were asked to provide 3 other words. The dataset contains 4 million responses on 12,000 cues. Based on the collective performance of annotators, the dataset provides association strengths between words (which contain any kind of words, not just spatial words) as a measure of their semantic relatedness. Finally, we collected a new dataset of word similarity judgments using Amazon Mechanical Turk. Here, the participants were presented with a pair of spatial relations at a time. Their task was to use a slider bar with a numerical indicator to express how similar the pair of words are. The experiment is similar to the one described in (Logan and Sadler, 1996) except that in our case participants only saw one pair of relations at a time rather than the entire list. The shared vocabulary between these three datasets covers left, right, above, over, below, under, near, next, away.</p>
<p>Evaluation</p>
<p>As stated in Section 2 the P-vectors we have built are intended to capture the discriminatory power of a generative language model to encode and discriminate different spatial relations, their functional bias. In this section we evaluate the Pvectors on several common intrinsic and extrinsic tests for vectors. If successful, this demonstrates that such knowledge has indeed been captured by the language model. We evaluate both single-and multi-word relations.</p>
<p>Clustering</p>
<p>Method Figure 2 and its complete version in Appendix C show that different spatial relations have different context fingerprints. To find similar relations in this matrix we can use K-means clustering. K-mean is a non-convex problem: different random initialisation may lead to different local minima. We apply the clustering on 67 Pvectors for multi-word spatial relations and qualitatively examine them for various sizes k. The optimal number of clusters is not so relevant here, only that for each k we get reasonable associations that follow our semantic intuitions.</p>
<p>Results As shown in Table 2, with k = 30, the clustering of perplexity vectors shows acceptable semantics of each cluster. There are clusters with synonymous terms such as (15. above, over) or  Discussion The inspection of the perplexities of two of these clusters in Figure 3 shows that the language model has learned different selectional properties of spatial relations: above and over are generally more selective of their own contexts, while to the left of and to the right of show a higher degree of confusion with a variety of the P-vector contexts. High degree of confusion in left and right is consistent with the observation in (Dobnik and Kelleher, 2013) that these relations are less dependent on the functional relation between particular objects and therefore have a higher geometric bias. On the other hand, above and over seem to be more selective of their contexts. The functional distinction between above and over is mildly visible: the shades of blue in above are slightly darker than over.</p>
<p>Analogical reasoning with relations</p>
<p>The intrinsic properties of vector representations (the degree to which they capture functional associations between relations and their objects) can be tested with their performance in analogical reasoning tasks. We compare the performance of  the P-vectors (Section 3.3), the embeddings of the language model used to create the P-vectors and GloVe embeddings (Section 3.2) in two analogical tasks which require both geometric and functional reasoning.</p>
<p>Predicting analogical words</p>
<p>Method The task is similar to the analogy test (Mikolov et al., 2013;Levy et al., 2015) where two pairs of words are compared in terms of some relation "a is to a as b is to b ". We manually grouped spatial relations that are opposite in one geometric dimension to 6 groups. These are: Group 1: left, right; Group 2: above, below; Group 3: front, back; Group 4: with, without; Group 5: in, out; and Group 6: up, down. We generate all possible permutations of these words for the analogical reasoning task which gives us 120 permutations. We expand these combinations to include multi-word variants. This dataset has 85,744 possible analogical questions such as (above :: below, to the left of :: ?). We accept all variants of a particular relation (e.g. to the right side of and to the right of ) as the correct answer.</p>
<p>Results As shown in in Table 3, on the singleword test suite, the LM-embeddings perform better than other models. On multi-word test suite the P-vectors perform slightly better. On both test suites, GloVe trained on Common Crawl performs better than GloVe trained on Visual Genome. However, its performance on multi-word relations is considerably lower. We simulated random answers as a baseline to estimate the difficulty of the task. Although the multi-word test suite has ∼ 700 times more questions than the test suite with single-word relations, it is only approximately 2times more difficult to predict the correct answer in the multi-word dataset compared to the singleword dataset.</p>
<p>Discussion</p>
<p>The perplexity of the language model on complete context phrases (Multi-words) is as good indicator of semantic relatedness as the word embeddings of the underlying language model and much better than GloVe embeddings. The good performance of the P-vectors explains the errors of the language model in generating spatial descriptions. The confusion between in front of and on the back of is similar to the confusion between to the left of and to the right of in terms of their distribution over functional contexts. Hence, a similar lack of strong functional associations allows the vectors to make inference about geometrically related word-pairs. This indicates that functional and geometric bias of words are complementary. There are two possible explanations why P-vectors perform better than LM-embeddings on multi-word vectors: (i) low-dimensions of P-vectors (57D) intensify the contribution of spatial contexts for analogical reasoning compared to high-dimensional LMembeddings (300D); (ii) summing the vectors of the LM-embeddings for multi-words reduces their discriminatory effect.</p>
<p>Odd-one-out</p>
<p>Method Based on the semantic relatedness of words, the goal of this task is to find the odd member of the three. The ground truth for this test are the following five categories of spatial relations, again primarily based on geometric criteria: Xaxis: left, right; Y-axis: above, over, under, below; Z-axis: front, back; Containment: in, out; and Proximity: near, away. Only the Y-axis contains words that are geometrically similar but functionally different, e.g. above/over. In total there are 528 possible instances with 3,456 multi-word variations. The difficulty of the task is the same for both single-and multi-word expressions as the choice is always between three words. Hence, the random baseline is 0.33.</p>
<p>Results Table 4 shows the accuracy in predicting the odd relation out of the three. We also add a comparison to fully geometric representations captured by spatial templates (Logan and Sadler, 1996). Ghanimifard and Dobnik (2017) show that spatial templates can be compared with Spearman's rank correlation coefficient ρ X,Y and therefore we also include this similarity measure. Since our groups of relations contain those that are geometric opposites in each dimension, we take the absolute value of |ρ X,Y |. Spatial templates are not able to recognise relatedness without the right distance measure, |ρ X,Y |. LM-embeddings perform better than other vectors in both tests, but  P-vectors follow closely. All models have a low performance on the multi-word test suite. When using |ρ X,Y | all vectors other than P-vectors produce better results. While we do not have an explanation for this, it is interesting to observe that |ρ X,Y | is a better measure of similarity than cosine.</p>
<p>Discussion The results demonstrate that using functional representations based on associations of words can predict considerable information about geometric distinctions between relations, e.g. distinguishing to the right of and above, and this is also true for P-vectors. As stated earlier, our explanation for this is that functional and geometric knowledge is in complementary distribution. This has positive and negative implications for joint vision and language models used in generating spatial descriptions. In the absence of geometric information, language models provide strong discriminative power in terms of functional contexts, but even if geometric latent information is expressed in them, an image captioning system still needs to ground each description in the scene geometry.</p>
<p>Similarity with human judgments</p>
<p>We compare the cosine similarity between words in LM-and P-vector spaces with similarities from (i) word association judgments (De Deyne et al., 2018), (ii) our word similarity judgments from AMT, and (iii) spatial templates (Section 3.4). We take the maximum subset of shared vocabulary between them, including on, in only shared between (i) and (ii). Since (i) is an association test, unrelated relations do not have association strengths. There are 55 total possible pairs of 11 words, while only 28 pairs are present in (i) as shown in Figure 4.</p>
<p>Method We take the average of the two way association strengths if the association exists and for (i) we assign a zero association for unrelated pairs such as left and above. Spearman's rank correlation coefficient ρ X,Y is used to compare the calculated similarities.   Table 5: Spearman's ρ between pairwise lists of similarities. WA are similarities based on word associations and WS are direct word similarities from human judgments.</p>
<p>Discussion</p>
<p>The low correlation between the two similarities from human judgments is surprising. Our explanation is that this is because of different priming to functional and geometric dimension of meaning in the data collection task. In the WA task participants are not primed with the spatial domain but they are providing general word associations, hence functional associations. On the other hand, in the WS task participants are presented with two spatial relations, e.g. left of and right of, and therefore the geometric dimension of meaning is more explicitly attended. We also notice that judgments are not always unison, the same pair may be judged as similar and dissimilar which further confirms that participants are selecting between two different dimensions of meaning. This observation is consistent with our argument that LM-vectors and P-vectors encode functional knowledge. Both representations correlate better with WA than with WS. Finally, (Logan and Sadler, 1996) demonstrate that WS judgments can be decomposed to dimensions that correlate with the dimensions of the spatial templates. We leave this investigation for our future work.</p>
<p>Conclusion and future work</p>
<p>In the preceding discussion, we have examined what semantic knowledge about spatial relations is captured in representations of a generative neural language model. In particular, we are interested if the language model is able to encode a distinction between functional and geometric bias of spatial relations and how the two dimensions of meaning interact. The idea is based on earlier work that demonstrates that this bias can be recovered from the selectivity of spatial relations for target and landmark objects. In particular, (i) we test the difference between multi-word spatial relations at two levels: the word embeddings which are a form of internal semantic representations in a language model and the perplexity-based P-vectors which are external semantic representations based on the language model performance; (ii) we project spatial relations in the contexts of other relations and we measure the fit of the language model to these contexts using perplexity (P-vectors); (iii) we use these contexts to build a distributional model of multi-word spatial relations; (iv) in the evaluation on standard semantic similarity tasks, we demonstrate that these vectors capture fine semantic distinctions between spatial relations; (v) we also demonstrate that these representations based on word-context associations latently capture geometric knowledge that allows analogical reasoning about space; this suggests that functional and geometric components of meaning are complementary: (vi) doing so we also demonstrated that generation of spatial descriptions is also dependent on textual features, even if the system has no access to the visual features of the scene. This has implications for baselines for image captioning and how we evaluate visual grounding of spatial relations.</p>
<p>Our work could be extended in several ways, including by (i) using the knowledge about the bias of spatial relations to evaluate captioning tasks with spatial word substitutions (Shekhar et al., 2017a,b); (ii) examining how functional knowledge is complemented with visual knowledge in language generation (Christie et al., 2016;Delecraz et al., 2017) (iii) using different contextual embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) for the embedding layer of the generative language model rather than our specifically-trained word embeddings; note that P-vectors are representations of collections of context based on the performance of the decoder language model while ELMo and BERT are representations of specific context based on the encoder language model; (iv) comparing language models for spatial descriptions from different pragmatic tasks. As the focus of image captioning is to best describe the image and not for example, spatially locate a particular object, the pragmatic context of image descriptions is biased towards the functional sense of spatial relations. Our analysis should be extended to different kinds of corpora, for example those for visual question answering, human-robot interaction, and navigation instructions where we expect that precise geometric locating of objects receives more focus. Therefore, we expect to find a stronger geometric bias across all descriptions and a lower performance of our representations on analogical reasoning.</p>
<p>Figure 2 :
2A matrix of perplexity vectors for 28 spatial relations and 26 contexts. For the full 67 × 57 matrix see Appendix C. The rows represent spatial relations and columns represent the normalised average perplexity of a language model when this relation is swapped in that context.</p>
<p>Figure 3 :
3The P-vectors of two clusters. (26. below, under). Some clusters have variants of multi-word antonymous such as (30. on the top of, on the bottom of ). Other clusters have a mixture of such relations, e.g. (27. right, back, left, side, and there).</p>
<p>Figure 4 :
4(i) Word association judgments and (ii) word similarity judgmentsResults</p>
<p>Table 1 :
1Examples of context bins based on extracted descriptions from Visual Genome. The images that belong to these descriptions are shown in Appendix B.</p>
<p>Table 2 :
2K-means clusters of spatial relations based on their P-vectors.above 
across 
against 
along 
alongside 
around </p>
<p>at 
back of 
behind 
below 
beneath 
beside 
between 
bottom 
by 
down 
during 
from 
here 
in 
in back of 
in between 
in front of 
in the back of 
inside 
into 
near 
next to 
off 
on 
on back of 
on bottom of 
on front of 
on side of 
on the back of 
on the bottom of 
on the front of 
on the side of 
on the top of 
on top of 
onto 
out 
outside 
over 
side 
there 
through </p>
<p>to 
to the left of 
to the right of 
together </p>
<p>top 
under 
underneath </p>
<p>up 
with 
without </p>
<p>above over </p>
<p>to the left of 
to the right of 
next to </p>
<p>Table 3 :
3The accuracies of different representations on the word analogy test.</p>
<p>Table 4 :
4The accuracies in odd-one-out tests.</p>
<p>Table 5
5shows ranked correlations of different similarity measures. Spatial templates do not correlate with (WA) word associations and (WS) word similarities. On 28 pairs there is a weak negative correlation between spatial templates and WS. The correlation of similarities of two different human judgments is positive but weak (ρ = 0.33). The similarities predicted by LM-vectors and P-vectors correlate better with WA than WS. values: * &lt; 0.01, * * &lt; 0.01, * * * &lt; 0.00155 pairs 
28 pairs 
WA 
WS 
WA 
WS 
SpTemp 
−0.02 
−0.08 
0.06 
−0.35 
LM 0.48  *  *  <em><br />
0.15 
0.59  *  *  </em><br />
0.08 
P 0.48  *  *  <em><br />
0.19 
0.40  *  </em><br />
−0.08 
p-
The pre-processing leaves 5,042,039 descriptions in the corpus with maximum 31 tokens per sentence. The relatively high threshold of 100 tokens is chosen to insure sufficient support in the 10% of held-out data for bucketing. We did not use OOV tokens because the goal of the evaluation is to capture object-specific properties about spatial relations and OOV tokens would interfere with this. 4 http://nlp.stanford.edu/data/glove.42B. 300d.zip
AcknowledgementsWe are grateful to the anonymous reviewers for their helpful comments. The research of the authors was supported by a grant from the Swedish Research Council (VR project 2014-39) to the Centre for Linguistic Theory and Studies in Probability (CLASP) at Department of Philosophy, Linguistics and Theory of Science (FLoV), University of Gothenburg.
. Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Oriol Vinyals. Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégasand Xiaoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.orgMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor- rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schus- ter, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys- tems. Software available from tensorflow.org.</p>
<p>A maximum likelihood approach to continuous speech recognition. R Lalit, Frederick Bahl, Robert L Jelinek, Mercer, IEEE Transactions on Pattern Analysis &amp; Machine Intelligence. 2Lalit R Bahl, Frederick Jelinek, and Robert L Mercer. 1983. A maximum likelihood approach to continu- ous speech recognition. IEEE Transactions on Pat- tern Analysis &amp; Machine Intelligence, 2:179-190.</p>
<p>A neural probabilistic language model. Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin, Journal of machine learning research. 3Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic lan- guage model. Journal of machine learning research, 3(Feb):1137-1155.</p>
<p>Evaluation metrics for language models. Douglas Stanley F Chen, Ronald Beeferman, Rosenfeld, DARPA Broadcast News Transcription and Understanding Workshop. CiteseerStanley F Chen, Douglas Beeferman, and Ronald Rosenfeld. 1998. Evaluation metrics for language models. In DARPA Broadcast News Transcription and Understanding Workshop, pages 275-280. Cite- seer.</p>
<p>. François Chollet, François Chollet et al. 2015. Keras. https:// github.com/keras-team/keras.</p>
<p>Resolving language and vision ambiguities together: Joint segmentation &amp; prepositional attachment resolution in captioned scenes. Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochersberger, Dhruv Batra, arXiv:1604.02125arXiv preprintGordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol, Yash Goyal, Kevin Kochersberger, and Dhruv Batra. 2016. Resolving language and vision ambiguities together: Joint segmentation &amp; prepositional attachment resolution in captioned scenes. arXiv preprint arXiv:1604.02125.</p>
<p>Word association norms, mutual information, and lexicography. Kenneth Ward Church, Patrick Hanks, Computational linguistics. 161Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicog- raphy. Computational linguistics, 16(1):22-29.</p>
<p>Acquiring common sense spatial knowledge through implicit spatial templates. Guillem Collell, Luc Van Gool, Marie-Francine Moens, Thirty-Second AAAI Conference on Artificial Intelligence. Guillem Collell, Luc Van Gool, and Marie-Francine Moens. 2018. Acquiring common sense spatial knowledge through implicit spatial templates. In Thirty-Second AAAI Conference on Artificial Intel- ligence.</p>
<p>The british national corpus, version 3 (bnc xml edition). Bnc Consortium, Distributed by Oxford University Computing Services on behalf of the BNC Consortium. BNC Consortium et al. 2007. The british national cor- pus, version 3 (bnc xml edition). Distributed by Ox- ford University Computing Services on behalf of the BNC Consortium.</p>
<p>Spatial prepositions and vague quantifiers: Implementing the functional geometric framework. Angelo Kenny R Coventry, Rohanna Cangelosi, Alison Rajapakse, Stephen Bacon, Dan Newstead, Lynn V Joyce, Richards, International Conference on Spatial Cognition. SpringerKenny R Coventry, Angelo Cangelosi, Rohanna Ra- japakse, Alison Bacon, Stephen Newstead, Dan Joyce, and Lynn V Richards. 2004. Spatial prepo- sitions and vague quantifiers: Implementing the functional geometric framework. In International Conference on Spatial Cognition, pages 98-110. Springer.</p>
<p>Saying, seeing, and acting: the psychological semantics of spatial prepositions. R Kenny, Coventry, C Simon, Garrod, Psychology PressHove, East SussexKenny R Coventry and Simon C Garrod. 2004. Saying, seeing, and acting: the psychological semantics of spatial prepositions. Psychology Press, Hove, East Sussex.</p>
<p>The interplay between geometry and function in the comprehension of over, under, above, and below. Mercè Kenny R Coventry, Lynn Prat-Sala, Richards, Journal of memory and language. 443Kenny R Coventry, Mercè Prat-Sala, and Lynn Richards. 2001. The interplay between geometry and function in the comprehension of over, under, above, and below. Journal of memory and language, 44(3):376-398.</p>
<p>The "small world of words" english word association norms for over 12,000 cue words. Danielle J Simon De Deyne, Amy Navarro, Marc Perfors, Gert Brysbaert, Storms, Behavior research methods. Simon De Deyne, Danielle J Navarro, Amy Perfors, Marc Brysbaert, and Gert Storms. 2018. The "small world of words" english word association norms for over 12,000 cue words. Behavior research methods, pages 1-20.</p>
<p>Correcting prepositional phrase attachments using multimodal corpora. Sebastien Delecraz, Alexis Nasr, Frédéric Béchet, Benoit Favre, Proceedings of the 15th International Conference on Parsing Technologies. the 15th International Conference on Parsing TechnologiesSebastien Delecraz, Alexis Nasr, Frédéric Béchet, and Benoit Favre. 2017. Correcting prepositional phrase attachments using multimodal corpora. In Proceed- ings of the 15th International Conference on Parsing Technologies, pages 72-77.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.</p>
<p>Exploring the functional and geometric bias of spatial relations using neural language models. Simon Dobnik, Mehdi Ghanimifard, John D Kelleher, Proceedings of the First International Workshop on Spatial Language Understanding. the First International Workshop on Spatial Language UnderstandingNew Orleans, Louisiana, USAAssociation for Computational LinguisticsNAACL-HLT 2018Simon Dobnik, Mehdi Ghanimifard, and John D. Kelleher. 2018. Exploring the functional and ge- ometric bias of spatial relations using neural lan- guage models. In Proceedings of the First Interna- tional Workshop on Spatial Language Understand- ing (SpLU 2018) at NAACL-HLT 2018, pages 1- 11, New Orleans, Louisiana, USA. Association for Computational Linguistics.</p>
<p>Towards an automatic identification of functional and geometric spatial prepositions. Simon Dobnik, D John, Kelleher, Proceedings of PRE-CogSsci 2013: Production of referring expressions -bridging the gap between cognitive and computational approaches to reference. PRE-CogSsci 2013: Production of referring expressions -bridging the gap between cognitive and computational approaches to referenceBerlin, GermanySimon Dobnik and John D. Kelleher. 2013. Towards an automatic identification of functional and geo- metric spatial prepositions. In Proceedings of PRE- CogSsci 2013: Production of referring expressions -bridging the gap between cognitive and computa- tional approaches to reference, pages 1-6, Berlin, Germany.</p>
<p>Exploration of functional semantics of prepositions from corpora of descriptions of visual scenes. Simon Dobnik, D John, Kelleher, Dublin City University and the Association for Computational Linguistics. Dublin, IrelandProceedings of the Third V&amp;L Net Workshop on Vision and LanguageSimon Dobnik and John D. Kelleher. 2014. Explo- ration of functional semantics of prepositions from corpora of descriptions of visual scenes. In Proceed- ings of the Third V&amp;L Net Workshop on Vision and Language, pages 33-37, Dublin, Ireland. Dublin City University and the Association for Computa- tional Linguistics.</p>
<p>A theoretically grounded application of dropout in recurrent neural networks. Yarin Gal, Zoubin Ghahramani, Advances in neural information processing systems. Yarin Gal and Zoubin Ghahramani. 2016. A theoret- ically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pages 1019-1027.</p>
<p>Learning to compose spatial relations with grounded neural language models. Mehdi Ghanimifard, Simon Dobnik, Proceedings of IWCS 2017: 12th International Conference on Computational Semantics. IWCS 2017: 12th International Conference on Computational SemanticsMontpellier, FranceAssociation for Computational LinguisticsMehdi Ghanimifard and Simon Dobnik. 2017. Learn- ing to compose spatial relations with grounded neu- ral language models. In Proceedings of IWCS 2017: 12th International Conference on Computational Semantics, pages 1-12, Montpellier, France. Asso- ciation for Computational Linguistics.</p>
<p>The symbol grounding problem. Stevan Harnad, Physica D: Nonlinear Phenomena. 421-3Stevan Harnad. 1990. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335- 346.</p>
<p>Language and spatial cognition: an interdisciplinary study of the prepositions in English. Annette Herskovits, Cambridge University PressCambridgeAnnette Herskovits. 1986. Language and spatial cog- nition: an interdisciplinary study of the preposi- tions in English. Cambridge University Press, Cam- bridge.</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 98Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International Journal of Computer Vision. 1231Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John- son, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image anno- tations. International Journal of Computer Vision, 123(1):32-73.</p>
<p>Multiple geometric representations of objects in languages and language learners. Language and space. Barbara Landau, Barbara Landau. 1996. Multiple geometric representa- tions of objects in languages and language learners. Language and space, pages 317-363.</p>
<p>Update on "what" and "where" in spatial language: A new division of labor for spatial terms. Barbara Landau, Cognitive Science. 412Barbara Landau. 2016. Update on "what" and "where" in spatial language: A new division of labor for spa- tial terms. Cognitive Science, 41(2):321-350.</p>
<p>what" and "where" in spatial language and spatial cognition. Barbara Landau, Ray Jackendoff, Behavioral and Brain Sciences. 162Barbara Landau and Ray Jackendoff. 1993. "what" and "where" in spatial language and spatial cogni- tion. Behavioral and Brain Sciences, 16(2):217- 238, 255-265.</p>
<p>Improving distributional similarity with lessons learned from word embeddings. Omer Levy, Yoav Goldberg, Ido Dagan, Transactions of the Association for Computational Linguistics. 3Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im- proving distributional similarity with lessons learned from word embeddings. Transactions of the Associ- ation for Computational Linguistics, 3:211-225.</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European confer- ence on computer vision, pages 740-755. Springer.</p>
<p>A computational analysis of the apprehension of spatial relations. G D Logan, D D Sadler, Language and Space. M. Bloom, P.and Peterson, L. Nadell, and M. GarrettMIT PressG.D. Logan and D.D. Sadler. 1996. A computational analysis of the apprehension of spatial relations. In M. Bloom, P.and Peterson, L. Nadell, and M. Gar- rett, editors, Language and Space, pages 493-529. MIT Press.</p>
<p>Semeval-2007 task 10: English lexical substitution task. Diana Mccarthy, Roberto Navigli, Proceedings of the 4th International Workshop on Semantic Evaluations. the 4th International Workshop on Semantic EvaluationsAssociation for Computational LinguisticsDiana McCarthy and Roberto Navigli. 2007. Semeval- 2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 48-53. Association for Computational Linguistics.</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111-3119.</p>
<p>Efficient nonparametric estimation of multiple embeddings per word in vector space. Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, Andrew Mccallum, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Arvind Neelakantan, Jeevan Shankar, Alexandre Pas- sos, and Andrew McCallum. 2014. Efficient non- parametric estimation of multiple embeddings per word in vector space. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1059-1069.</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP), pages 1532-1543.</p>
<p>Deep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 2227-2237.</p>
<p>Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. A Bryan, Liwei Plummer, Chris M Wang, Juan C Cervantes, Julia Caicedo, Svetlana Hockenmaier, Lazebnik, Computer Vision (ICCV), 2015 IEEE International Conference on. IEEEBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svet- lana Lazebnik. 2015. Flickr30k entities: Col- lecting region-to-phrase correspondences for richer image-to-sentence models. In Computer Vision (ICCV), 2015 IEEE International Conference on, pages 2641-2649. IEEE.</p>
<p>Combining geometric, textual and visual features for predicting prepositions in image descriptions. Arnau Ramisa, Josiah Wang, Ying Lu, Emmanuel Dellandrea, Francesc Moreno-Noguer, Robert Gaizauskas, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingArnau Ramisa, Josiah Wang, Ying Lu, Emmanuel Dellandrea, Francesc Moreno-Noguer, and Robert Gaizauskas. 2015. Combining geometric, textual and visual features for predicting prepositions in im- age descriptions. In Proceedings of the 2015 Con- ference on Empirical Methods in Natural Language Processing, pages 214-220.</p>
<p>Evaluation of a semantic similarity measure for natural language spatial relations. Angela Schwering, International Conference on Spatial Information Theory. SpringerAngela Schwering. 2007. Evaluation of a semantic similarity measure for natural language spatial rela- tions. In International Conference on Spatial Infor- mation Theory, pages 116-132. Springer.</p>
<p>Vision and language integration: moving beyond objects. Ravi Shekhar, Sandro Pezzelle, Aurélie Herbelot, Moin Nabi, Enver Sangineto, Raffaella Bernardi, IWCS 2017-12th International Conference on Computational Semantics. Short papersRavi Shekhar, Sandro Pezzelle, Aurélie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017a. Vision and language integration: moving beyond objects. In IWCS 2017-12th International Conference on Computational Semantics-Short pa- pers.</p>
<p>Foil it! find one mismatch between image and language caption. Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot, Moin Nabi, Enver Sangineto, Raffaella Bernardi, arXiv:1705.01359arXiv preprintRavi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017b. Foil it! find one mis- match between image and language caption. arXiv preprint arXiv:1705.01359.</p>
<p>From frequency to meaning: Vector space models of semantics. D Peter, Patrick Turney, Pantel, Journal of artificial intelligence research. 37Peter D Turney and Patrick Pantel. 2010. From fre- quency to meaning: Vector space models of se- mantics. Journal of artificial intelligence research, 37:141-188.</p>            </div>
        </div>

    </div>
</body>
</html>
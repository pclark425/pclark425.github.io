<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5363 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5363</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5363</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-0088c9f4d50706c7ab71efa13bcb4b42cf2058e2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0088c9f4d50706c7ab71efa13bcb4b42cf2058e2" target="_blank">PRODIGY: Enabling In-context Learning Over Graphs</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper develops PRODIGY, the first pretraining framework that enables in- context learning over graphs with a novel \emph{prompt graph} representation, which connects prompt examples and queries and proposes a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives.</p>
                <p><strong>Paper Abstract:</strong> In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \textbf{Pr}etraining \textbf{O}ver \textbf{D}iverse \textbf{I}n-Context \textbf{G}raph S\textbf{y}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \emph{prompt graph} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\% on average with in-context learning.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5363.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5363.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptGraph (text extension)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt graph representation (with extension for free-form text prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified in-context task representation for graph classification that connects contextualized per-datapoint subgraphs (data graphs) with a small task-level graph of data nodes and label nodes; the paper notes this representation can be extended to include free-form textual instructions or descriptions in node/label features to support text-style prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Prompt graph (with textual node/label features)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct per-datapoint data graphs by retrieving k-hop neighborhoods from a source graph; create a task graph with one data node per datapoint and one label node per class; connect data and label nodes with edges that encode whether an example-label edge is true/false or is a query; to support text-style prompting, include textual instructions or description strings as features of label nodes or global data-graph features (these textual fields can be encoded with a text encoder and placed into the node/label feature vectors).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks and knowledge graphs (node-level and edge-level / relation classification tasks); general graphs where datapoints are subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Unified across node/edge/graph tasks; compact because only per-datapoint k-hop neighborhoods are retrieved rather than representing the entire source graph; interpretable structure: explicit label nodes serve as an information bottleneck that aggregates support examples; supports augmentation (node dropping, node feature masking) on retrieved subgraphs; can incorporate textual information as node/label features but the paper does not implement a full graph->text serialization (it encodes text as features). Potential information loss due to sampling k-hop neighborhoods rather than full graph context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Few-shot in-context downstream classification tasks: node classification (paper category on arXiv) and relation / link prediction / relation-type classification on KGs (ConceptNet, FB15K-237, NELL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (%) on few-shot in-context classification tasks. Example results from the paper using the prompt-graph based PRODIGY pretraining (k=3 shots): arXiv 3-way: PRODIGY 73.09% vs Contrastive 65.08% and NoPretrain 33.16%; ConceptNet/FB15K-237/NELL many-way accuracies reported in Table 2 (e.g., ConceptNet 4-way: PRODIGY 53.97% vs Contrastive 44.01%). The paper reports average improvements of ~18% (vs contrastive in in-context setup) and ~32.6% (vs standard finetuning with limited data) across experimental settings. (Units: percent accuracy; the paper reports means ± std.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly to (a) NoPretrain (random init), (b) Contrastive pretraining + hard-coded nearest-neighbor adaptation, and (c) Finetune (contrastive encoder + supervised linear head). PRODIGY (prompt-graph pretraining) consistently outperforms Contrastive and NoPretrain on in-context accuracy; in many experiments PRODIGY also matches or exceeds Finetune despite not using gradient updates on downstream data. The paper also ablates two PRODIGY variants (PG-NM self-supervised neighbor-matching; PG-MT multitask supervised) and finds the combined approach best overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The prompt-graph approach does not fully serialize graphs into natural-language text; textual prompting is supported only as node/label features (i.e., requires a text encoder to convert text to features). Context is approximated by sampled k-hop neighborhoods (possible context truncation). Self-supervised neighbor-matching can overfit to co-occurrence signals and may not generalize to all downstream tasks; domain mismatch between pretraining graphs and downstream graphs can still affect performance; the proposed free-form text prompting extension is described conceptually but not implemented/evaluated in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRODIGY: Enabling In-context Learning Over Graphs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5363.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5363.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-derived node features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node feature initialization with pretrained language models (RoBERTa / MPNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper initializes node (and in KGs, edge) feature vectors by encoding textual metadata/descriptions with pretrained language models (RoBERTa for citation datasets, MPNet for knowledge graphs), producing dense textual embeddings used as node/edge features in subsequent GNN processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text-to-node embeddings via pretrained language models (RoBERTa / MPNet)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For nodes that have textual attributes (e.g., paper title/abstract, entity description), pass the text through a pretrained language model (RoBERTa for MAG/ArXiv experiments, MPNet for knowledge graph datasets) to obtain a fixed-dimensional embedding (768→projected to model input dim). These embeddings are used as initial node/edge features for the DataGraph message-passing GNN M_B.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (papers), knowledge graphs (entities/relations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encodes rich semantic/textual information from node descriptions into dense vectors; leverages pretrained LM knowledge and transfer; easy to integrate as initial features for GNNs; compact relative to raw text. Trade-offs: does not capture structural graph information by itself; quality depends on textual coverage and domain match of the pretrained LM.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used across the paper's downstream evaluation suite: node classification on arXiv and relation classification on ConceptNet, FB15K-237, NELL; these tasks evaluate the end-to-end models that consume the text-derived node features.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports downstream in-context accuracy (%) for models that use these text-derived features (examples: arXiv 3-way PRODIGY 73.09% accuracy). The contribution of the text->feature conversion is not isolated with a separate ablation (no per-component metric isolating LM-derived features vs. others is reported), so numerical effect attributable solely to the conversion is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared implicitly against random initialization (NoPretrain) as part of whole-model baselines; all evaluated models (PRODIGY variants and baselines) use the same text-derived initialization when applicable, so the paper does not present direct head-to-head comparisons isolating alternative graph->text conversion strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No ablation isolating the impact of the textual encoder is reported, so it is unclear how much of final performance stems from text features vs. pretraining objectives; reliance on textual metadata makes the approach less applicable when nodes lack descriptive text; domain mismatch between LM pretraining corpus and graph-specific text can reduce representation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRODIGY: Enabling In-context Learning Over Graphs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5363.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5363.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-GNN: Generative pre-training of graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work (cited in related work) on generative pretraining for graph neural networks; mentioned as an example of existing graph pretraining approaches that learn an encoder and typically require fine-tuning for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-gnn: Generative pre-training of graph neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GPT-GNN (generative pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Not described in detail in this paper; mentioned as a prior graph pretraining method in the literature that trains graph encoders (reference only). The current paper contrasts such encoder-focused pretraining with in-context prompt-graph pretraining that aims to enable zero-gradient adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graph datasets (prior work; not instantiated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Not discussed in this paper beyond being a pretraining approach that learns encoders and typically requires downstream fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Cited as representative of prior pretraining paradigms that require finetuning; the paper positions PRODIGY's in-context pretraining as a different goal (no finetuning required). No experimental head-to-head comparison results vs GPT-GNN are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As discussed by the authors, encoder-only pretraining approaches like GPT-GNN typically need downstream fine-tuning and do not directly provide in-context learning ability across graphs/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRODIGY: Enabling In-context Learning Over Graphs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-gnn: Generative pre-training of graph neural networks. <em>(Rating: 2)</em></li>
                <li>Strategies for pre-training graph neural networks <em>(Rating: 2)</em></li>
                <li>Graph contrastive learning with augmentations <em>(Rating: 1)</em></li>
                <li>Relational multi-task learning: Modeling relations between data and tasks. <em>(Rating: 2)</em></li>
                <li>Roberta: A robustly optimized BERT pretraining approach. <em>(Rating: 2)</em></li>
                <li>MPNet: Masked and Permuted Pre-training for Language Understanding <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5363",
    "paper_id": "paper-0088c9f4d50706c7ab71efa13bcb4b42cf2058e2",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "PromptGraph (text extension)",
            "name_full": "Prompt graph representation (with extension for free-form text prompting)",
            "brief_description": "A unified in-context task representation for graph classification that connects contextualized per-datapoint subgraphs (data graphs) with a small task-level graph of data nodes and label nodes; the paper notes this representation can be extended to include free-form textual instructions or descriptions in node/label features to support text-style prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Prompt graph (with textual node/label features)",
            "representation_description": "Construct per-datapoint data graphs by retrieving k-hop neighborhoods from a source graph; create a task graph with one data node per datapoint and one label node per class; connect data and label nodes with edges that encode whether an example-label edge is true/false or is a query; to support text-style prompting, include textual instructions or description strings as features of label nodes or global data-graph features (these textual fields can be encoded with a text encoder and placed into the node/label feature vectors).",
            "graph_type": "Citation networks and knowledge graphs (node-level and edge-level / relation classification tasks); general graphs where datapoints are subgraphs",
            "representation_properties": "Unified across node/edge/graph tasks; compact because only per-datapoint k-hop neighborhoods are retrieved rather than representing the entire source graph; interpretable structure: explicit label nodes serve as an information bottleneck that aggregates support examples; supports augmentation (node dropping, node feature masking) on retrieved subgraphs; can incorporate textual information as node/label features but the paper does not implement a full graph-&gt;text serialization (it encodes text as features). Potential information loss due to sampling k-hop neighborhoods rather than full graph context.",
            "evaluation_task": "Few-shot in-context downstream classification tasks: node classification (paper category on arXiv) and relation / link prediction / relation-type classification on KGs (ConceptNet, FB15K-237, NELL).",
            "performance_metrics": "Accuracy (%) on few-shot in-context classification tasks. Example results from the paper using the prompt-graph based PRODIGY pretraining (k=3 shots): arXiv 3-way: PRODIGY 73.09% vs Contrastive 65.08% and NoPretrain 33.16%; ConceptNet/FB15K-237/NELL many-way accuracies reported in Table 2 (e.g., ConceptNet 4-way: PRODIGY 53.97% vs Contrastive 44.01%). The paper reports average improvements of ~18% (vs contrastive in in-context setup) and ~32.6% (vs standard finetuning with limited data) across experimental settings. (Units: percent accuracy; the paper reports means ± std.)",
            "comparison_to_other_representations": "Compared directly to (a) NoPretrain (random init), (b) Contrastive pretraining + hard-coded nearest-neighbor adaptation, and (c) Finetune (contrastive encoder + supervised linear head). PRODIGY (prompt-graph pretraining) consistently outperforms Contrastive and NoPretrain on in-context accuracy; in many experiments PRODIGY also matches or exceeds Finetune despite not using gradient updates on downstream data. The paper also ablates two PRODIGY variants (PG-NM self-supervised neighbor-matching; PG-MT multitask supervised) and finds the combined approach best overall.",
            "limitations_or_challenges": "The prompt-graph approach does not fully serialize graphs into natural-language text; textual prompting is supported only as node/label features (i.e., requires a text encoder to convert text to features). Context is approximated by sampled k-hop neighborhoods (possible context truncation). Self-supervised neighbor-matching can overfit to co-occurrence signals and may not generalize to all downstream tasks; domain mismatch between pretraining graphs and downstream graphs can still affect performance; the proposed free-form text prompting extension is described conceptually but not implemented/evaluated in experiments in this paper.",
            "uuid": "e5363.0",
            "source_info": {
                "paper_title": "PRODIGY: Enabling In-context Learning Over Graphs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Text-derived node features",
            "name_full": "Node feature initialization with pretrained language models (RoBERTa / MPNet)",
            "brief_description": "The paper initializes node (and in KGs, edge) feature vectors by encoding textual metadata/descriptions with pretrained language models (RoBERTa for citation datasets, MPNet for knowledge graphs), producing dense textual embeddings used as node/edge features in subsequent GNN processing.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Text-to-node embeddings via pretrained language models (RoBERTa / MPNet)",
            "representation_description": "For nodes that have textual attributes (e.g., paper title/abstract, entity description), pass the text through a pretrained language model (RoBERTa for MAG/ArXiv experiments, MPNet for knowledge graph datasets) to obtain a fixed-dimensional embedding (768→projected to model input dim). These embeddings are used as initial node/edge features for the DataGraph message-passing GNN M_B.",
            "graph_type": "Citation networks (papers), knowledge graphs (entities/relations)",
            "representation_properties": "Encodes rich semantic/textual information from node descriptions into dense vectors; leverages pretrained LM knowledge and transfer; easy to integrate as initial features for GNNs; compact relative to raw text. Trade-offs: does not capture structural graph information by itself; quality depends on textual coverage and domain match of the pretrained LM.",
            "evaluation_task": "Used across the paper's downstream evaluation suite: node classification on arXiv and relation classification on ConceptNet, FB15K-237, NELL; these tasks evaluate the end-to-end models that consume the text-derived node features.",
            "performance_metrics": "The paper reports downstream in-context accuracy (%) for models that use these text-derived features (examples: arXiv 3-way PRODIGY 73.09% accuracy). The contribution of the text-&gt;feature conversion is not isolated with a separate ablation (no per-component metric isolating LM-derived features vs. others is reported), so numerical effect attributable solely to the conversion is not provided.",
            "comparison_to_other_representations": "Compared implicitly against random initialization (NoPretrain) as part of whole-model baselines; all evaluated models (PRODIGY variants and baselines) use the same text-derived initialization when applicable, so the paper does not present direct head-to-head comparisons isolating alternative graph-&gt;text conversion strategies.",
            "limitations_or_challenges": "No ablation isolating the impact of the textual encoder is reported, so it is unclear how much of final performance stems from text features vs. pretraining objectives; reliance on textual metadata makes the approach less applicable when nodes lack descriptive text; domain mismatch between LM pretraining corpus and graph-specific text can reduce representation quality.",
            "uuid": "e5363.1",
            "source_info": {
                "paper_title": "PRODIGY: Enabling In-context Learning Over Graphs",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-GNN",
            "name_full": "GPT-GNN: Generative pre-training of graph neural networks",
            "brief_description": "A referenced prior work (cited in related work) on generative pretraining for graph neural networks; mentioned as an example of existing graph pretraining approaches that learn an encoder and typically require fine-tuning for downstream tasks.",
            "citation_title": "Gpt-gnn: Generative pre-training of graph neural networks.",
            "mention_or_use": "mention",
            "representation_name": "GPT-GNN (generative pretraining)",
            "representation_description": "Not described in detail in this paper; mentioned as a prior graph pretraining method in the literature that trains graph encoders (reference only). The current paper contrasts such encoder-focused pretraining with in-context prompt-graph pretraining that aims to enable zero-gradient adaptation.",
            "graph_type": "General graph datasets (prior work; not instantiated in this paper)",
            "representation_properties": "Not discussed in this paper beyond being a pretraining approach that learns encoders and typically requires downstream fine-tuning.",
            "evaluation_task": null,
            "performance_metrics": null,
            "comparison_to_other_representations": "Cited as representative of prior pretraining paradigms that require finetuning; the paper positions PRODIGY's in-context pretraining as a different goal (no finetuning required). No experimental head-to-head comparison results vs GPT-GNN are reported in this paper.",
            "limitations_or_challenges": "As discussed by the authors, encoder-only pretraining approaches like GPT-GNN typically need downstream fine-tuning and do not directly provide in-context learning ability across graphs/tasks.",
            "uuid": "e5363.2",
            "source_info": {
                "paper_title": "PRODIGY: Enabling In-context Learning Over Graphs",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-gnn: Generative pre-training of graph neural networks.",
            "rating": 2
        },
        {
            "paper_title": "Strategies for pre-training graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "Graph contrastive learning with augmentations",
            "rating": 1
        },
        {
            "paper_title": "Relational multi-task learning: Modeling relations between data and tasks.",
            "rating": 2
        },
        {
            "paper_title": "Roberta: A robustly optimized BERT pretraining approach.",
            "rating": 2
        },
        {
            "paper_title": "MPNet: Masked and Permuted Pre-training for Language Understanding",
            "rating": 2
        }
    ],
    "cost": 0.013954,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PRODIGY: Enabling In-context Learning Over Graphs</h1>
<p>Qian Huang ${ }^{1 <em>}$<br>qhwang@cs.stanford.edu<br>Hongyu Ren ${ }^{1 </em>}$<br>hyren@cs.stanford.edu<br>Peng Chen ${ }^{1}$<br>pengc@stanford.edu Gregor Kržmanc ${ }^{2}$<br>gregor.krzmanc@ijs.si Daniel Zeng ${ }^{1}$<br>dzeng@cs.stanford.edu<br>Percy Liang ${ }^{1}$<br>pliang@cs.stanford.edu<br>Jure Leskovec ${ }^{1}$<br>jure@cs.stanford.edu<br>${ }^{1}$ Stanford University<br>${ }^{2}$ University of Ljubljana</p>
<h4>Abstract</h4>
<p>In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop Pretraining Over Diverse In-Context Graph Systems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel prompt graph representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by $18 \%$ on average across all setups. Moreover, it also outperforms standard finetuning with limited data by $33 \%$ on average with in-context learning.</p>
<h2>1 Introduction</h2>
<p>In-context learning is a novel and one of the most intriguing capabilities of language models [1]. It refers to the capability of a pretrained model to perform novel and diverse tasks directly at the prediction time when prompted with just a few examples, without the need to update the model weights. For example, a person may describe the new task (e.g., question answering, machine translation, or code generation) using natural language and demonstrate it to the language model with several prompt examples. The language model then directly without any model training or finetunning performs the task.
However, how to enable in-context learning for diverse graph machine learning tasks, such as identifying misinformation spreader in social networks [14] and product suggestions across online e-commerce websites [21], still remain unexplored and challenging. An in-context learner for graphs should be able to solve novel tasks on novel graphs. For example, give music product</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In-context few-shot prompting over graphs with prompt graph for edge classification in PRODIGY. (A) Given the source graph $\mathcal{G}$, we provide prompt examples $\mathcal{S}$ that consist of the input head/tail nodes and their labels, as well as the queries. (B) For each datapoint from both prompt examples and the queries, we first construct its data graph $\mathcal{G}^{0}$ by retrieving context from the source graph $\mathcal{G}$. (C) Then we create a task graph to capture the connection between each datapoint and each label, which includes a data node $v_{x}$ for each datapoint and a label node $v_{y}$ for each label in $\mathcal{Y}$. Each pair of data and label nodes are connected with edge attributes corresponding to their binary labels.
recommendations on Spotify when being trained on Amazon purchasing graph. The first challenge here is how to formulate and represent node-, edge- and graph-level tasks over graphs with a unified task representation that allows the model to solve diverse tasks without the need for retraining or parameter tuning. In other words, the key challenge is: what is an analog of natural language prompting for graph machine learning tasks? The second challenge is how to design model architecture and pretraining objectives that enable models to achieve in-context learning capability across diverse tasks and diverse graphs in the unified task representation. Existing graph pretraining methods [7, 24, 8, 13] only aim to learn a good graph encoder and require fine-tuning to adapt to different tasks, while existing meta-learning methods over graphs [19, 9, 3, 17, 25] only aim to generalize across different tasks within the same graph. On the other hand, achieving in-context learning requires tackling the more difficult setting of generalizing across the graphs and tasks without finetuning.
Here we present a general approach for solving these two challenges for classification tasks on graphs: (1) prompt graph, an in-context graph task representation, and (2) Pretraining Over Diverse In-Context Graph Systems (PRODIGY), a framework for pretraining an in-context learner over prompt graphs.
We propose prompt graph (Figure 1) to provide unified way to represent diverse node-, edge- and graphlevel machine learning tasks. Prompt graph first contextualizes the input nodes/edges on which we make prediction (including both the prompt examples and the queries), then connects them with additional label nodes, such that the prompt examples are interconnected with queries. Such a unified representation allows us to specify diverse graph machine learning tasks to the same model regardless of the graph size.
PRODIGY then designs both model architecture and pretraining objectives with the prompt graph in-context task formulation, such that the model is pretrained to solve tasks across a wide range of tasks and graphs, and can continue to do so out-of-the-box. We design a graph architecture that utilizes graph neural networks to learn node/edge representations and an attention mechanism to communicate over prompt graph. Furthermore, we propose a family of in-context pretraining objectives over prompt graph. In particular, this includes a novel self-supervised pretraining task, neighbor matching, where we classify which neighborhood a node or edge belongs to.
We use PRODIGY framework to pretrain on citation networks (MAG240M [5]) and knowledge graphs (Wiki [22]). We then show that such model (without any retraining) provides strong performance on in-context paper category classification and knowledge graph completion tasks on novel graphs it was never trained on (arXiv, ConceptNet, FB15K-237, NELL) [6, 16, 23]. Specifically, PRODIGY improves upon contrastive pretraining baselines with hard-coded adaptation for in-context setup by $18 \%$ on average across all datasets and numbers of labels to classify among. Moreover, it also outperforms standard finetuning with limited data by $32.6 \%$ on average with in-context learning. It even outperforms the state-of-the-art few-shot learning methods trained on the testing downstream graph with pure in-context learning. Finally, we further demonstrate that our methods achieve increasingly higher performance with more examples in the prompt even beyond what it was pretrained with, which shows that the model really learns to learn from context.</p>
<h1>2 In-context Learning over Graphs</h1>
<p>In this work, we specifically focus on in-context learning for node and edge classification tasks on graphs with few-shot prompting, which are the forms of the most standard and important graph machine learning tasks. In this section, we introduce the concrete classification tasks over graphs and few-shot prompting over them with our in-context task representation prompt graph.</p>
<h3>2.1 Classification Tasks over Graphs</h3>
<p>We define a graph as $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{R})$, where $\mathcal{V}, \mathcal{E}, \mathcal{R}$ represent the set of nodes, edges and relations. An edge $e=(u, r, v) \in \mathcal{E}$ consists of a subject $u \in \mathcal{V}$, a relation $r \in \mathcal{R}$ and an object $v \in \mathcal{V}$.
Given a set of classes $\mathcal{Y}$, a standard classification task is predicting the labeling $y \in \mathcal{Y}$ of each input $x \in \mathcal{X}$. A node-level classification task is similar but each input is a single node in $\mathcal{G}$, i.e., $\mathcal{X}=\mathcal{V}$, with the additional auxiliary information of the entire graph $\mathcal{G}$. For example, over a citation network consisting of authors and papers, a node-level classification task could be predicting the primary institution of each author. Similarly, an edge-level classification task is predicting the best labeling of potential edges formed by any pair of nodes, i.e., $\mathcal{X}=\mathcal{V} \times \mathcal{V}$. A common special case is that the classes are the same as the relations $\mathcal{Y}=\mathcal{R}$, such as predicting the relation between entities over knowledge graphs. More generally, the same definitions can be extended to subgraph and graph-level classification tasks, where the input data $x$ may consist of more nodes and edges, and essentially represents a subgraph of $\mathcal{G}$.
Since we are interested in tasks of different types/levels, we design a unified formulation, where the space of the input $\mathcal{X}$ consists of graphs, i.e., $x_{i} \in \mathcal{X}, x_{i}=\left(\mathcal{V}<em i="i">{i}, \mathcal{E}</em>}, \mathcal{R<em i="i">{i}\right)$. For node classification task, $\mathcal{G}</em>}$ only consists of the input node that we aim to make predictions on, i.e., $\left|\mathcal{V<em i="i">{i}\right|=1$ and $\left|\mathcal{E}</em>}\right|=0$; for edge classification task, it consists of (subject, object) pair, i.e., $\left|\mathcal{V<em i="i">{i}\right|=2$ and $\left|\mathcal{E}</em>\right|=0$.</p>
<h3>2.2 Few-shot Prompting</h3>
<p>Here we define in-context learning setup for classification tasks over graphs with few-shot prompting. For a $k$-shot prompt with a downstream $m$-way classification tasks with $|\mathcal{Y}|=m$ classes, we use a small number of input-label pairs $\mathcal{S}=\left{\left(x_{i}, y_{i}\right)\right}<em i="i">{i=1}^{m \cdot k}$ as prompt examples of the task specification, such that there are $k$ input-label pairs with label $y$ for each $y \in \mathcal{Y}$. We also give the model a set of queries $\mathcal{Q}=\left{x</em>$ that we want to predict labels for.
We emphasize an important difference of classification tasks on graphs from language and other modalities. Namely, since all input datapoints are nodes/edges/subgraphs from the larger source graph $\mathcal{G}$, this graph contains critical information and provides contexts for the inputs, e.g., the local neighborhood of the input node that we aim to predict. Hence, besides $\mathcal{S}$ and $\mathcal{Q}$, we also need to include the source graph $\mathcal{G}$ in the prompt.
Given the above information as the prompt, the pretrained model should be able to directly output the predicted labels for each datapoint in $\mathcal{Q}$ via in-context learning. Thus, how to formulate the information as a unified and efficient form of input poses a unique challenge and affects the model architecture. Below, we present our in-context task formulation prompt graph designed to do so.}\right}_{i=1}^{n</p>
<h3>2.3 Prompt Graph Representation</h3>
<p>Inspired by [2], we propose prompt graph as a unified representation of a $k$-shot prompt over graphs for an $m$-way classification task (Figure 1). A prompt graph is composed of data graphs and a task graph:
Data graph. To construct a prompt graph, we first perform contextualization of each datapoint $x_{i}=\left(\mathcal{V}<em i="i">{i}, \mathcal{E}</em>}, \mathcal{R<em i="i">{i}\right)$ in $\mathcal{S}$ and $\mathcal{Q}$ in the source graph $\mathcal{G}$ to form data graphs. The goal is to gather more information about the $x</em>}$ from the source graph $\mathcal{G}$ without having to represent the entire source graph explicitly. There are many potential designs for contextualization, from explicitly retrieving subgraphs to implicitly using embedding-based methods. Here we construct data graph $\mathcal{G<em i="i">{i}^{b}$ by sampling $k$-hop neighborhood of $\mathcal{V}</em>}$ in $\mathcal{G}$. In other words, $\mathcal{G<em i="i">{i}^{b}=\left(\mathcal{V}</em>}^{b}, \mathcal{E<em i="i">{i}^{b}, \mathcal{R}</em>}^{b}\right) \sim \bigoplus_{i=0}^{k}$ Neighbor $\left(\mathcal{V<em i="i">{i}, \mathcal{G}, i\right)$, where $\mathcal{V}</em>} \subseteq \mathcal{V<em i="i">{i}^{b} \subseteq \mathcal{V}, \mathcal{E}</em>} \subseteq \mathcal{E<em i="i">{i}^{b} \subseteq \mathcal{E}, \mathcal{R}</em>} \subseteq \mathcal{R<em i="i">{i}^{b} \subseteq \mathcal{R}$, and Neighbor is a function that returns the exact $i$-hop neighbors of each node in $\mathcal{V}</em>}$. With this data graph $\mathcal{G<em i="i">{i}^{b}$, we call the node set that corresponds to the nodes in $\mathcal{V}</em>$ before contextualization input node set, e.g., the target node to classify in node classification task and the pair of nodes in link prediction task.</p>
<p>Task graph. After contextualizing each datapoint to a data graph $\mathcal{G}^{\mathrm{B}}$, we then construct task graph $\mathcal{G}^{\mathrm{T}}$ to better capture the connection and relationship among the inputs and the labels. For each data graph $\mathcal{G}<em x__i="x_{i">{i}^{\mathrm{B}}$ from the previous stage, we have a data node $v</em>$. So overall, a task graph contains $m \cdot k+n$ data nodes ( $m \cdot k$ prompt examples and $n$ queries) and $m$ label nodes, as shown in Figure 1.}}$ that represents each input; for each label, we have a label node $v_{y_{i}</p>
<p>Now we add edges between the data nodes and the label nodes: For the query set, since we do not know the labels of each graph, we add single directional edges from all label nodes to each datapoint in the query set, i.e., each query data node $v_{x_{i}}$ will be connected to all the label nodes as shown by the yellow edges in Figure 1; For the prompt examples, we connect each data node to all the label nodes, where the edge with the true labels is marked as T while the others are marked as F , as shown by the green and red edges in Figure 1 respectively.</p>
<p>Together we propose the prompt graph that consists of both data graphs and a task graph. Prompt graph effectively captures the relationship between input data $x_{i}$ and the label $y_{i}$ through the context captured in data graph $\mathcal{G}<em x__i="x_{i">{i}^{\mathrm{B}}$ and the data node $v</em>$. Then different label nodes would represent different prediction tasks. To support more general forms of prompting, one can include additional task information and instructions in the feature of label nodes, and additional description paired with each datapoint in the global feature in data graph.}}$ and the label node $v_{y_{i}}$ in the task graph $\mathcal{G}^{\mathrm{T}}$. It is also possible to extend prompt graph to non-classification tasks and free-form text prompting. For example, for numerical regression (e.g. molecular energy prediction) and other free-form generation tasks (e.g. text generation), one can extend our task graph to contain vector values on the edges to represent $y_{i</p>
<h1>3 Pretraining to Enable In-context Learning</h1>
<p>So far given a few-shot prompt for a classification task over graphs, we have defined a prompt graph representation for it that captures relationships between the prompt examples, queries, and labels. Now we need to design a pretraining strategy that can pretrain a generalizable model capable of in-context learning. We assume access to a pretraining graph $\mathcal{G}_{\text {pretrain }}$ that is independent of the source graph $\mathcal{G}$ for the downstream task.</p>
<p>In this section, we introduce PRODIGY, a general pretraining framework over $\mathcal{G}_{\text {pretrain }}$ that is designed specifically for enabling in-context learning over downstream classification tasks without any additional finetuning steps on arbitrary graphs. Our framework PRODIGY has two main components: model architecture over prompt graph and in-context pretraining objectives.</p>
<h3>3.1 Message Passing Architecture over prompt graph</h3>
<p>Next we introduce our model architecture over the prompt graph consisting of two submodules:
Data graph Message Passing. First, we apply a message passing GNN module $M_{\mathrm{B}}$ that learns node representation $E$ for nodes in each $\mathcal{G}^{\mathrm{B}}$.</p>
<p>$$
E \in \mathcal{R}^{\left|\mathcal{V}^{\mathrm{B}}\right| \times d}=M_{\mathrm{B}}\left(\mathcal{G}^{\mathrm{B}}\right)
$$</p>
<p>where $d$ is the embedding dimension. $M_{\mathrm{B}}$ can be implemented in multiple ways, such as using Graph Convolutional Network (GCN) or Graph Attention Networks (GAT) [11, 18].</p>
<p>To read out a single embedding $G_{i}$ for each data graph, we perform another aggregation step to pool node embeddings. For node classification tasks, we take the updated node representation of the single input node that we aim to predict, i.e.:</p>
<p>$$
G_{i}=E_{\mathcal{V}_{i}}
$$</p>
<p>For link prediction tasks, we concatenate the pair of nodes, which we want to predict a link between, as well as a max pooling over all node representations following [10] with an additional linear projection layer at the end to convert the embedding size back to $d$.</p>
<p>$$
G_{i}=W^{T}\left(E_{v_{1} \in \mathcal{V}<em v__2="v_{2">{i}} | E</em>} \in \mathcal{V<em i="i">{i}} | \max \left(E</em>\right)\right)+b
$$</p>
<p>where $|$ represents concatenation, $W \in \mathcal{R}^{3 d \times d}$ is a learnable weight matrix and $b$ is the learnable bias.
Task graph Message Passing. Note in the previous step there is no communication between different datapoints in $\mathcal{S}$ and $\mathcal{Q}$. Now we would like to communicate between them via message passing over</p>
<p>the task graph $\mathcal{G}^{\mathrm{T}}$. We apply another GNN $M_{\mathrm{T}}$ on the task graph to obtain updated representation of data nodes and label nodes.</p>
<p>$$
H=M_{\mathrm{T}}\left(\mathcal{G}^{\mathrm{T}}\right)
$$</p>
<p>where H is the obtained embedding per node. The initial embedding of data node $v_{x_{i}}$ is $G_{i}$ and the embedding of label node $v_{y_{i}}$ can either be initialized with random Gaussian or additional information available about the labels. Each edge also has two binary features $e_{i j}$ that indicate 1) whether the edge comes from an example or a query, and 2) the edge type of T or F. For $M_{\mathrm{T}}$, we use an attention-based GNN, where each node performs attention to other nodes at each layer. See the architecture detail in the appendix C.</p>
<p>The goal of this step is to learn a better representation of the label nodes using the support examples and propagate label information back to the support and query graph representation for a more task-specific graph representation.</p>
<p>Prediction Read Out. Finally, we readout the classification logits $O_{i}$ by taking cosine similarity between each pair of query graph representation and label representation, as in contrastive learning:</p>
<p>$$
O_{i}=\left[\text { cosine_similarity }\left(H_{x_{i}}, H_{y}\right), \forall y \in \mathcal{Y}\right]
$$</p>
<p>Note that we could perform the two message passing steps for multiple rounds to have more communication between $x_{i}$ and learn a better representation. One key insight is that different in-context prompt examples share information through the label nodes, which can be seen as an information bottleneck.</p>
<h1>3.2 In-context Pretraining Objectives</h1>
<p>In order to pretrain the model for solving the downstream graph tasks in-context, we propose a set of in-context pretraining objectives. The goal is to pretrain the graph model using a large pretraining graph $\mathcal{G}_{\text {pretrain }}$ independent of the downstream task graph, such that the model can directly be applied on downstream tasks with in-context learning.</p>
<p>Our main design principle is that we formulate each pretraining objective in an in-context learning way. Most previous graph pretraining objectives only pretrain a shared graph encoder to perform various tasks with task-specific heads, so they require finetuning for another task-specific head over each downstream task. In contrast, we explicitly construct in-context pretraining tasks in prompt graph form and pretrain the model to solve diverse tasks in-context with the same set of weights, such that it can perform in-context learning directly over downstream tasks.</p>
<p>Below, we detail our proposed family of in-context pretraining objectives in terms of three components: 1) pretraining task generation, including few-shot prompt (i.e. Figure 1(A)) and corresponding labels, 2) converting generated few-shot prompt to prompt graph format (i.e. Figure 1(B,C)) with augmentation, and 3) pretraining loss over the generated prompt graph.</p>
<h3>3.2.1 Pretraining Task Generation</h3>
<p>We propose two methods to generate pretraining tasks from the pretraining graph $\mathcal{G}_{\text {pretrain }}$ in the form of few-shot prompts: neighbor matching and multi-task.</p>
<p>Neighbor Matching. Given the pretraining graph, we construct self-supervised in-context pretraining tasks with the goal of classifying which local neighborhood a node belongs to, where each local neighborhood is defined by the example nodes belonging to that neighborhood. Intuitively, we sample multiple subgraphs from the pretraining graph $\mathcal{G}_{\text {pretrain }}$ as the local neighborhoods, and we say a node belongs to a local neighborhood if it is in the sampled subgraph.</p>
<p>Formally, we denote $\mathrm{NM}<em _pretrain="{pretrain" _text="\text">{k, m}$ as a sampler that generates $m$-way neighbor matching tasks, where each includes a $k$-shot prompt $\left(\mathcal{G}</em>}}, \mathcal{S<em _mathrm_NM="\mathrm{NM">{\mathrm{NM}}, \mathcal{Q}</em>$ as paired with the inputs:}}\right)$ (see subsection 2.2 and Figure 1(A)) and the labels of the queries. For simplicity of the notation, we will include the labels in $\mathcal{Q}_{\mathrm{NM}</p>
<p>$$
\left(\mathcal{G}<em _mathrm_NM="\mathrm{NM">{\text {pretrain }}, \mathcal{S}</em>}}, \mathcal{Q<em k_="k," m="m">{\mathrm{NM}}\right) \sim \mathrm{NM}</em>\right)
$$}\left(\mathcal{G}_{\text {pretrain }</p>
<p>To generate these, we first sample $m$ nodes from the pretraining graph $\mathcal{G}_{\text {pretrain }}$, where each of the sampled node corresponds to one class.</p>
<p>$$
\mathcal{C}=\left{c_{i}\right}<em i="i">{i=1}^{m} \quad c</em>\right)
$$} \sim \operatorname{Uniform}\left(\mathcal{V}_{\text {pretrain }</p>
<p>For each sampled node/class $c_{i}$, we sample $k$ different nodes from its exact $l$-hop neighbors. These $k$ nodes serve as examples of label $c_{i}$. We also sample additional $\left\lceil\frac{n}{m}\right\rceil$ nodes similarly for each label $c_{i}$ to form the query set. Formally,</p>
<p>$N_{i}=\operatorname{Neighbor}\left(c_{i},\mathcal{G}<em i="i">{\text {pretrain }}, l\right)$ (8)
$\mathcal{S}</em>\right)\right}}=\left{\left(x_{j}, y_{j}=c_{i<em j="j">{j=1}^{k} \quad x</em>\right)$
$\mathcal{Q}} \sim \operatorname{Uniform}\left(N_{i<em j="j">{i}=\left{\left(x</em>\right)\right}}, y_{j}=c_{i<em j="j">{j=1}^{\lceil\frac{n}{m}\rceil} \quad x</em>\right)$
In such a way, we constructed a neighbor matching pretraining task sample in the format of a few-shot prompt $\left(\mathcal{G}} \sim \operatorname{Uniform}\left(N_{i<em _mathrm_NM="\mathrm{NM">{\text {pretrain }}, \mathcal{S}</em>}}=\bigcup \mathcal{S<em _mathrm_NM="\mathrm{NM">{i}, \mathcal{Q}</em>}}=\bigcup \mathcal{Q<em i="i">{i}\right)$.
The neighbor matching task generation process outlined above is specifically applicable when the downstream tasks are also node classification. When the downstream task is link prediction, we may adapt the above neighbor matching tasks to over edges correspondingly. Specifically, we can expand each sampled input node $x</em>$. Then, instead of classifying to which neighborhood a node in the query set belongs, now the neighbor matching task is to classify to which neighborhood an edge in the query set belongs.
Multi-task. When the pretraining graphs have node or edge-level labeling $f\left(x_{i}\right)=y_{i} \in \mathcal{Y}$ for some $x_{i} \in \mathcal{V}}$ to an edge by randomly sampling an edge that contains $x_{i<em _pretrain="{pretrain" _text="\text">{\text {pretrain }}$ or $\mathcal{E}</em>$, we can further leverage this signal to perform supervised pretraining. Similar to neighbor matching, the key is to construct such supervised pretraining tasks in the format of few-shot prompts and corresponding labels.}</p>
<p>$$
\left(\mathcal{G}<em _mathrm_MT="\mathrm{MT">{\text {pretrain }}, \mathcal{S}</em>}}, \mathcal{Q<em k_="k," m="m">{\mathrm{MT}}\right) \sim \mathrm{MT}</em>, f\right)
$$}\left(\mathcal{G}_{\text {pretrain }</p>
<p>For node classification tasks, we first sample $m$ labels from the whole label set. Then, for each label, we directly sample $k$ nodes as support examples and $\left\lceil\frac{n}{m}\right\rceil$ nodes with labels in this set as query examples.</p>
<p>$$
\begin{aligned}
\mathcal{C}=\left{c_{i}\right}<em i="i">{i=1}^{m} &amp; c</em>) \
\mathcal{S}} \sim \operatorname{Uniform}(\mathcal{Y<em j="j">{i}=\left{\left(x</em>\right)\right}}, y_{j}=c_{i<em j="j">{j=1}^{k} &amp; x</em>\right}\right) \
\mathcal{Q}} \sim \operatorname{Uniform}\left(\left{x_{i} \mid f\left(x_{i}\right)=c_{i<em j="j">{i}=\left{\left(x</em>\right)\right}}, y_{j}=c_{i<em j="j">{j=1}^{\lceil\frac{n}{m}\rceil} &amp; x</em>\right}\right)
\end{aligned}
$$} \sim \operatorname{Uniform}\left(\left{x_{i} \mid f\left(x_{i}\right)=c_{i</p>
<p>We then construct a task with the few-shot prompt as $\left(\mathcal{G}<em _mathrm_MT="\mathrm{MT">{\text {pretrain }}, \mathcal{S}</em>}}=\bigcup \mathcal{S<em _mathrm_MT="\mathrm{MT">{i}, \mathcal{Q}</em>}}=\bigcup \mathcal{Q<em 1="1">{i}\right)$. For link prediction, we directly use the edge type function as $f$, i.e. $f\left(\left(v</em>$. With this $f$, we may directly sample $m$ edge types and construct pretraining tasks in a similar way as above.
The benefit of such a supervised pretraining objective is that it could directly resemble the format of downstream tasks, compared with neighbor matching objective, which may only serve as a surrogate. However, it requires extra labels if $f$ is not part of $\mathcal{G}_{\text {pretrain }}$, e.g. node classification labels that may not exist for some pretraining graphs.}, v_{2}\right)\right)=r \Longleftrightarrow\left(v_{1}, r, v_{2}\right) \in \mathcal{E</p>
<h1>3.2.2 Prompt graph generation with augmentation</h1>
<p>After we obtained the few-shot prompts and labels for either of the two tasks (NeighborMatching and multi-task), we need to construct the prompt graph for pretraining. In addition to the standard construction process described in subsection 2.3, we add an additional augmentation step to augment
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Neighbor matching pretraining task generation. The key idea of neighbor matching is to classify whether a node/edge is in the local neighborhood of a set of sampled nodes (as labels). Given a set of sampled nodes and their two-hop neighbors as prompt examples, we aim to classify whether a query node is also a two-hop neighbor of each sampled node.</p>
<p>the data graphs as inspired by Contrastive Learning. The key insight is to corrupt data graph such that the pretrained model learns representation invariant to various corruptions.</p>
<p>Here we demonstrate how we adopt graph augmentation techniques during the construction of prompt graph from a few-shot prompt generated from $\mathcal{G}<em i="i">{\text {pretrain }}$. We first still sample the $k$-hop neighbor subgraph of each sample $\mathcal{G}</em>}$ in the prompt examples and queries: $\mathcal{G<em _mathcal_G="\mathcal{G">{i}^{0} \sim \mathcal{G}</em><em i="i">{j=1}^{k}}^{k}$ Neighbor $\left(\mathcal{G}</em>}, \mathcal{G<em i="i">{\text {pretrain }} . j\right)$. Then we adopt the following two augmentation techniques to create augmented data graph $\mathcal{G}</em>}^{\text {aug }}$, including (1) node dropping, and (2) node feature masking [24]. For node dropping, we randomly drop nodes from the $k$-hop neighbor subgraph and take the remaining graph as $\mathcal{G<em i="i">{i}^{\text {aug }}=\operatorname{DropNode}\left(\mathcal{G}</em>}^{0}\right)$. For node feature masking, we randomly mask the feature of a subset of nodes with value zero to create $\mathcal{G<em i="i">{i}^{\text {aug }}=\operatorname{MaskNode}\left(\mathcal{G}</em>$ as introduced in subsection 2.3. Combining data graphs with task graph, we obtain the prompt graph formulation with augmentation for the few-shot prompt.}^{0}\right)$. With the augmented data graphs for each datapoint in the prompt examples and the queries, we may accordingly construct the task graph $\mathcal{G}^{\mathrm{T}}$ by creating a data node $v_{x_{i}}$ for each augmented data graphs and the label node $v_{y_{i}</p>
<h1>3.2.3 Pretraining Loss</h1>
<p>Finally, we pretrain the model with the cross-entropy objectives over generated prompt graphs:</p>
<p>$$
\begin{gathered}
\left(\mathcal{G}<em _mathrm_NM="\mathrm{NM">{\text {pretrain }}, \mathcal{S}</em>}}, \mathcal{Q<em k_="k," m="m">{\mathrm{NM}}\right) \sim \mathrm{NM}</em>}\left(\mathcal{G<em _pretrain="{pretrain" _text="\text">{\text {pretrain }}\right) \
\left(\mathcal{G}</em>}}, \mathcal{S<em _mathrm_MT="\mathrm{MT">{\mathrm{MT}}, \mathcal{Q}</em>}}\right) \sim \mathrm{MT<em _pretrain="{pretrain" _text="\text">{k, m}\left(\mathcal{G}</em>, f\right) \
\mathcal{L}=\underset{x_{i} \in \mathcal{Q}}<em _mathrm_NM="\mathrm{NM">{\mathrm{NM}}}{\mathbb{E}} \mathrm{CE}\left(O</em>}, i}, y_{\mathrm{NM}, i}\right)+\underset{x_{i} \in \mathcal{Q<em _mathrm_MT="\mathrm{MT">{\mathrm{MT}}}{\mathbb{E}} \mathrm{CE}\left(O</em>\right)
\end{gathered}
$$}, i}, y_{\mathrm{MT}, i</p>
<p>where $O_{\mathrm{NM}, i}$ is the logits produced by our model over input of $\mathcal{G}<em _mathrm_NM="\mathrm{NM">{i}^{\text {aug }}$ and $\mathcal{G}^{\mathrm{T}}$ produced from $\mathcal{Q}</em>$; Similar for MT terms.}}$, as described in subsection 3.1; $y_{\mathrm{NM}, i}$ is the corresponding label of $x_{i}$ in $\mathcal{Q}_{\mathrm{NM}</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>Datasets. For pretraining, we use two datasets: MAG240M [5], a large scale citation network with 122 million nodes and 1.3 billion edges; and Wiki, a knowledge graph (KG) constructed from Wikipedia [22] with 4.8 million nodes and 5.9 million edges. After the model is pretrained we evaluate its in-context learning capability on diverse classification tasks over 4 graphs: arXiv [6], ConceptNet [16], FB15K-237 [23], NELL [23]. We use subsets of knowledge graph datasets same as in [10, 23]. For arXiv, the downstream task is an $m$-ways node classification task that predicts the paper category. For knowledge graph datasets (ConceptNet, FB15K-237, NELL), the downstream task is an $m$-ways relation type classification task that predicts the relationship connecting the two input nodes.</p>
<p>Evaluation. We pretrain our model on MAG240M and Wiki and then we evaluate the in-context learning performance on differnt downstream datasets that belong to similar domain as the pretraining dataset (e.g., pretraining on Wiki and evaluating on ConceptNet, FB15K-237, and NELL). Each of the downstream classification datasets has its original train, validation, and test splits. To simulate the situation where there are a limited amount of labeled data in the downstream task, we randomly select 10 nodes (or edges) from the training split per way as the prompt examples with known labels. Then, we construct a $k$-shot prompt for test nodes (or edges) from the test split by randomly selecting $k$ examples per way from these available examples. This allows us to test the model's ability to learn in-context relationships and perform well on classification tasks with truly limited known labels. By default we use $k=3$ shots in our experiments.</p>
<p>Methods and Baselines. We consider three versions of our proposed framework PRODIGY: 1) PG-NM, which uses neighbor matching task for pretraining; 2) PG-MT, which employs multi-task pretraining; and 3) full PRODIGY, which combines the previous two methods. To augment the data, we use DropNode and MaskNode augmentations with a probability of 0.5 per node for each method.</p>
<p>We consider three baselines for comparison: 1) NoPretrain, which uses a randomly-initialized model with the same architecture as our pretrained models; 2) Contrastive [24], which employs a standard contrastive learning method with the same augmentation as above and uses a hard-coded nearest neighbor algorithm to adapt to our in-context learning setting. Specifically, we classify the query by</p>
<p>Table 1: In-context learning accuracy (%) on arXiv paper category classification on 500 sampled test tasks with 3-shot prompts. PRODIGY was pretrained on MAG240M and is then applied in-context to arXiv, which has completely different structure and a different set of paper categories. PG-NM and PG-MT are ablations of PRODIGY.</p>
<table>
<thead>
<tr>
<th>Classes</th>
<th>NoPretrain</th>
<th>Contrastive</th>
<th>PG-NM</th>
<th>PG-MT</th>
<th>PRODIGY</th>
<th>Finetune</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>$33.16 \pm 0.30$</td>
<td>$65.08 \pm 0.34$</td>
<td>$72.50 \pm 0.35$</td>
<td>$65.64 \pm 0.33$</td>
<td>$\mathbf{7 3 . 0 9} \pm \mathbf{0 . 3 6}$</td>
<td>$65.42 \pm 5.53$</td>
</tr>
<tr>
<td>5</td>
<td>$18.33 \pm 0.21$</td>
<td>$51.63 \pm 0.29$</td>
<td>$61.21 \pm 0.28$</td>
<td>$51.97 \pm 0.27$</td>
<td>$\mathbf{6 1 . 5 2} \pm \mathbf{0 . 2 8}$</td>
<td>$53.49 \pm 4.61$</td>
</tr>
<tr>
<td>10</td>
<td>$9.19 \pm 0.11$</td>
<td>$36.78 \pm 0.19$</td>
<td>$46.12 \pm 0.19$</td>
<td>$37.23 \pm 0.20$</td>
<td>$\mathbf{4 6 . 7 4} \pm \mathbf{0 . 2 0}$</td>
<td>$30.22 \pm 3.77$</td>
</tr>
<tr>
<td>20</td>
<td>$4.72 \pm 0.06$</td>
<td>$25.18 \pm 0.11$</td>
<td>$33.71 \pm 0.12$</td>
<td>$25.91 \pm 0.12$</td>
<td>$\mathbf{3 4 . 4 1} \pm \mathbf{0 . 1 2}$</td>
<td>$17.68 \pm 1.15$</td>
</tr>
<tr>
<td>40</td>
<td>$2.62 \pm 0.02$</td>
<td>$17.02 \pm 0.07$</td>
<td>$23.69 \pm 0.06$</td>
<td>$17.19 \pm 0.08$</td>
<td>$\mathbf{2 5 . 1 3} \pm \mathbf{0 . 0 7}$</td>
<td>$8.04 \pm 3.00$</td>
</tr>
</tbody>
</table>
<p>Table 2: In-context learning accuracy (%) on ConceptNet, FB15K-237 and NELL (from top to bottom) on 500 sampled test tasks with 3-shot prompts. PRODIGY was pretrained on Wiki, which has completely different node and relation types from graphs it is then applied on in-context.</p>
<table>
<thead>
<tr>
<th>Classes</th>
<th>NoPretrain</th>
<th>Contrastive</th>
<th>PG-NM</th>
<th>PG-MT</th>
<th>PRODIGY</th>
<th>Finetune</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>$30.4 \pm 0.63$</td>
<td>$44.01 \pm 0.61$</td>
<td>$46.94 \pm 0.61$</td>
<td>$51.78 \pm 0.63$</td>
<td>$\mathbf{5 3 . 9 7} \pm \mathbf{0 . 6 3}$</td>
<td>$53.85 \pm 9.29$</td>
</tr>
<tr>
<td>5</td>
<td>$33.54 \pm 0.61$</td>
<td>$81.35 \pm 0.58$</td>
<td>$80.35 \pm 0.57$</td>
<td>$89.15 \pm 0.46$</td>
<td>$\mathbf{8 8 . 0 2} \pm \mathbf{0 . 4 8}$</td>
<td>$82.01 \pm 12.83$</td>
</tr>
<tr>
<td>10</td>
<td>$20.0 \pm 0.35$</td>
<td>$70.88 \pm 0.48$</td>
<td>$71.68 \pm 0.45$</td>
<td>$82.26 \pm 0.40$</td>
<td>$\mathbf{8 1 . 1} \pm \mathbf{0 . 3 9}$</td>
<td>$71.97 \pm 6.16$</td>
</tr>
<tr>
<td>20</td>
<td>$9.2 \pm 0.18$</td>
<td>$59.8 \pm 0.35$</td>
<td>$59.9 \pm 0.35$</td>
<td>$73.47 \pm 0.32$</td>
<td>$\mathbf{7 2 . 0 4} \pm \mathbf{0 . 3 3}$</td>
<td>$64.01 \pm 4.66$</td>
</tr>
<tr>
<td>40</td>
<td>$2.5 \pm 0.08$</td>
<td>$49.39 \pm 0.23$</td>
<td>$46.82 \pm 0.21$</td>
<td>$58.34 \pm 0.22$</td>
<td>$\mathbf{5 9 . 5 8} \pm \mathbf{0 . 2 2}$</td>
<td>$57.27 \pm 3.33$</td>
</tr>
<tr>
<td>5</td>
<td>$33.44 \pm 0.57$</td>
<td>$84.08 \pm 0.54$</td>
<td>$80.53 \pm 0.58$</td>
<td>$84.79 \pm 0.51$</td>
<td>$87.02 \pm 0.44$</td>
<td>$\mathbf{8 7 . 2 2} \pm \mathbf{1 2 . 7 5}$</td>
</tr>
<tr>
<td>10</td>
<td>$18.82 \pm 0.31$</td>
<td>$76.54 \pm 0.45$</td>
<td>$72.77 \pm 0.48$</td>
<td>$78.5 \pm 0.44$</td>
<td>$\mathbf{8 1 . 0 6} \pm \mathbf{0 . 4 1}$</td>
<td>$71.90 \pm 3.90$</td>
</tr>
<tr>
<td>20</td>
<td>$7.42 \pm 0.16$</td>
<td>$66.56 \pm 0.35$</td>
<td>$62.82 \pm 0.36$</td>
<td>$69.82 \pm 0.34$</td>
<td>$\mathbf{7 2 . 6 6} \pm \mathbf{0 . 3 2}$</td>
<td>$66.19 \pm 8.46$</td>
</tr>
<tr>
<td>40</td>
<td>$3.04 \pm 0.07$</td>
<td>$57.44 \pm 0.24$</td>
<td>$49.59 \pm 0.22$</td>
<td>$53.55 \pm 0.23$</td>
<td>$\mathbf{6 0 . 0 2} \pm \mathbf{0 . 2 2}$</td>
<td>$55.06 \pm 4.19$</td>
</tr>
</tbody>
</table>
<p>comparing its pretrained embedding against the average embedding of the example inputs of each class. 3) Finetune [7], which trains an additional linear classification head on top of the graph encoder pretrained with contrastive learning, following the standard practice.</p>
<h1>4.2 In-Context Learning Results</h1>
<p>We first evaluate the in-context learning capability for node classification and link prediction with various numbers of ways (i.e. number of classes to classify among).</p>
<p>Strong in-context learning performance. The results demonstrate that our method PRODIGY consistently outperforms all other baselines in this setting. It achieves the highest average accuracy across all ways on arXiv, with an average improvement of $28.6 \%$ and up to $48 \%$ over the best baseline of Contrastive. Over KGs, PRODIGY also outperforms contrastive learning on average by $12.2 \%$. PRODIGY also demonstrates similar-to-better performance compared to Finetune, which requires additional training on downstream tasks. On arXiv, we see an average improvement of $77.7 \%$ over all ways. This can be attributed to the diverse set of pretraining tasks incorporated in PRODIGY, which allows the model to avoid overfitting on specific tasks and learn in-context.</p>
<p>Self-supervised pretraining PG-NM bridges different tasks. In particular, we highlight that the pure self-supervised pretraining method PG-NM produces significantly higher in-context learning performance over arXiv than baselines, even though the model is pretrained on different tasks from the downstream task. This advantage can be further leveraged by pretraining on even larger-scale unlabeled datasets. On the other hand, PG-MT follows the supervised pretraining objective that directly resembles the format of downstream tasks. On KGs, this allows PG-MT to adapt better to downstream task even sometimes compared to the full PRODIGY ( marked by underlines), while PG-NM might have overfitted to the incorrect strategy of only identifying co-occurring nodes. Yet, PG-MT performs worse on arXiv potentially due to less diversity. The full PRODIGY, which ensembles the two, achieves more diversity than either single task and therefore achieves the best performance over both worlds.</p>
<p>Outperforming meta-learning method trained on test graph. Finally, we compare PG-NM in-context learning performance against state-of-the-art meta-learning method TENT [20] over the downstream test graph arXiv. We evaluate the average 3 -ways classification tasks performance over only test labels, since TENT trains on train labels from arXiv. PG-NM achieves $69.07 \%$ over the $65.13 \%$ of TENT, even though PG-NM has never been trained on any paper category classification</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: In-context learning accuracy on ConceptNet in a 4 -ways setting wrt. the number of prompt examples (shots).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: With more training steps and data on arXiv (5-ways), PRODIGY keeps improving while the baseline (Contrastive) saturates.
task during pretraining. This demonstrates the power of self-supervised pretraining over large amount of data compared to supervised meta-learning over the limited labeled data (train labels in arXiv).</p>
<h1>4.3 Ablations</h1>
<p>Aside from PG-NM and PG-MT, we also conduct ablation studies on various configurations of the selfsupervised objective PG-NM as described in 3.2. See the full results in Appendix E and Table 4. Overall, the ablation results reveal that using all of the elements together results in the highest performance. Specifically, attribute prediction (see appendix A) has the greatest impact on PG-NM's performance, as its removal results in an average $7 \%$ drop across all ways, shown in the 'No-Attr' column.</p>
<h3>4.4 Evaluation using different numbers of in-context examples</h3>
<p>We investigate our method's ability to learn from the context by analyzing its performance as the number of prompt examples changes. Figure 3 shows the result on ConceptNet. See full results on other datasets in Appendix F. As the number of prompt examples increases, the margin of our proposed PG models over the baseline increases. This supports the hypothesis that the PRODIGY models can more effectively learn the unknown task by reasoning about the common characteristics of prompt examples.</p>
<h3>4.5 Scaling with Data Size</h3>
<p>Finally, we explore how the model scales with more pretraining data. The result on arXiv in a 5 -ways setting is illustrated in Figure 4. It shows that the Contrastive baseline saturates quickly and its performance fluctuates as trained over more pretraining data. Instead, PRODIGY consistently shows an improvement in performance as more data is pretrained on, since the pretraining tasks are harder and more diverse.</p>
<h2>5 Related Work</h2>
<h3>5.1 In-context Learning of Large Language Models</h3>
<p>Pretrained large language models can make predictions for diverse downstream tasks directly by prompting with a few examples of the task or more generally any textual instructions. This ability is called in-context learning. Comparing to previous language encoder models like BERT [4], it drastically reduces the adaptation effort comparing to fine-tuning, and has demonstrated strong performance in a broad range of models and tasks. Our work extends this success similarly to graph data compared to the current pretrained graph encoders, such that a single pretrained model can be adapted to different classification tasks over different graphs without additional fine-tuning but only few-shot prompting.</p>
<h3>5.2 Pretraining on Graphs</h3>
<p>There are many existing works on pretraining over graphs [7, 24, 8, 13]. However, they all follow the general paradigm of learning a good graph encoder that can perform certain pretraining tasks, such as masked feature prediction [7] and paired graph classification [24]. To adapt to any downstream tasks, it then requires finetuning a classification head on top of the encoder with large amount of task specific data for each downstream task. In contrast, we explore pretraining methods for inducing general in-context learning ability, such that the pretrained model can be directly used for various downstream tasks with no gradient updates.</p>
<h1>5.3 Meta Learning on Graphs</h1>
<p>Another closely related line of works is meta-learning methods over graphs that aim to address standard few shot learning problems over graphs [19, 9, 3, 17, 25]. However, existing meta-learning methods are only designed and tested for generalizing across different tasks on the same graph: the methods are trained on a set of training tasks on a graph, then tested over a disjoint but similar set of test tasks over the same graph. They are shown to exhibit optimal performance only when trained on similar curated tasks [10]. Different from this, our work explicitly focuses on the in-context learning performance, i.e. model performance on graphs and tasks completely different from the pretraining without additional fine-tuning.</p>
<h2>6 Conclusion</h2>
<p>We introduce PRODIGY, the first framework that enables in-context learning on graphs. A model that is pretrained using PRODIGY can seamlessly execute a new classification task over new graphs represented by prompt graph. It markedly surpasses the performance of other baseline models with in-context learning, even those that employ finetuning, in both the node and edge classification tasks.</p>
<h2>References</h2>
<p>[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] K. Cao, J. You, and J. Leskovec. Relational multi-task learning: Modeling relations between data and tasks. In International Conference on Representation Learning (ICLR), 2022.
[3] M. Chen, W. Zhang, W. Zhang, Q. Chen, and H. Chen. Meta relational learning for few-shot link prediction in knowledge graphs. arXiv preprint arXiv:1909.01515, 2019.
[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019.
[5] W. Hu, M. Fey, H. Ren, M. Nakata, Y. Dong, and J. Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.
[6] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020.
[7] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
[8] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun. Gpt-gnn: Generative pre-training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 1857-1867, 2020.
[9] K. Huang and M. Zitnik. Graph meta learning via local subgraphs, 2020.
[10] Q. Huang, H. Ren, and J. Leskovec. Few-shot relational reasoning via connection subgraph pretraining. Advances in neural information processing systems, abs/2210.06722, 2022.
[11] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks, 2016.
[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.
[13] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pages 1150-1160, 2020.
[14] S. Sharma and R. Sharma. Identifying possible rumor spreaders on twitter: A weak supervised learning approach. In 2021 International Joint Conference on Neural Networks (IJCNN), pages $1-8,2021$.</p>
<p>[15] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. MPNet: Masked and Permuted Pre-training for Language Understanding. In Advances in Neural Information Processing Systems, volume 33, pages 16857-16867. Curran Associates, Inc., 2020.
[16] R. Speer, J. Chin, and C. Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.
[17] J. Sun, Y. Zhou, and C. Zong. One-shot relation learning for knowledge graphs via neighborhood aggregation and paths encoding. Transactions on Asian and Low-Resource Language Information Processing, 21(3):1-19, 2021.
[18] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph attention networks, 2017.
[19] S. Wang, K. Ding, C. Zhang, C. Chen, and J. Li. Task-adaptive few-shot node classification. Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022.
[20] S. Wang, K. Ding, C. Zhang, C. Chen, and J. Li. Task-adaptive few-shot node classification. Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022.
[21] S. Wu, W. Zhang, F. Sun, and B. Cui. Graph neural networks in recommender systems: A survey. ACM Computing Surveys, 55:1-37, 2020.
[22] W. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang. One-shot relational learning for knowledge graphs, 2018.
[23] W. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang. One-shot relational learning for knowledge graphs. In EMNLP, 2018.
[24] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen. Graph contrastive learning with augmentations. Advances in neural information processing systems, 33:5812-5823, 2020.
[25] C. Zhang, H. Yao, C. Huang, M. Jiang, Z. Li, and N. V. Chawla. Few-shot knowledge graph completion. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $3041-3048,2020$.</p>
<h1>A Attribute Prediction Loss</h1>
<p>For each augmented DataGraph $\mathcal{G}^{\text {aug }}$, the certain node features $F_{v}$ are masked during MaskNode augmentation. Therefore, we can reconstruct them using the learned embedding $E_{v}$ with a MLP and train with MSE reconstruction Loss.</p>
<p>$$
\mathcal{L}<em v="v">{\text {attr }}\left(\mathcal{G}^{\text {aug }}\right)=\frac{1}{\left|\mathcal{V}^{0}\right|} \sum</em>\right)\right)
$$} \operatorname{MSE}\left(F_{v}, \operatorname{MLP}\left(E_{v</p>
<h2>B Dataset Statistics</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Table 3: Dataset statistics</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: right;"># Nodes</td>
<td style="text-align: right;"># Edges</td>
<td style="text-align: right;"># Classes</td>
</tr>
<tr>
<td style="text-align: left;">MAG240M</td>
<td style="text-align: right;">122 M</td>
<td style="text-align: right;">1.3 B</td>
<td style="text-align: right;">153</td>
</tr>
<tr>
<td style="text-align: left;">Wiki</td>
<td style="text-align: right;">4.8 M</td>
<td style="text-align: right;">5.9 M</td>
<td style="text-align: right;">639</td>
</tr>
<tr>
<td style="text-align: left;">arXiv</td>
<td style="text-align: right;">169 K</td>
<td style="text-align: right;">1.2 M</td>
<td style="text-align: right;">40</td>
</tr>
<tr>
<td style="text-align: left;">ConceptNet</td>
<td style="text-align: right;">791 K</td>
<td style="text-align: right;">2.5 M</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">FB15K-237</td>
<td style="text-align: right;">15 K</td>
<td style="text-align: right;">268 K</td>
<td style="text-align: right;">200</td>
</tr>
<tr>
<td style="text-align: left;">NELL</td>
<td style="text-align: right;">69 K</td>
<td style="text-align: right;">181 K</td>
<td style="text-align: right;">291</td>
</tr>
</tbody>
</table>
<h2>C Task Graph GNN Architecture</h2>
<p>For the GNN over task graph $M_{\mathrm{T}}$, we use an attention-based GNN, where each node performs attention to other nodes at each layer:</p>
<p>$$
\begin{aligned}
\beta_{i j} &amp; =M L P\left(W_{\eta}^{T} H_{i}^{l}\left|\left|W_{k}^{T} H_{j}^{l}\right|\right| e_{i j}\right) \
\alpha_{i j} &amp; =\frac{\exp \left(\beta_{i j}\right)}{\sum_{k \in \mathcal{N}(i) \cup{i}} \exp \left(\beta_{i k}\right)} \
H_{i}^{l+1} &amp; =\operatorname{ReLU}\left(B N\left(H_{i}^{l}+W_{\alpha}^{T} \sum_{j \in \mathcal{N}(i) \cup{i}} \alpha_{i j} W_{\alpha}^{T} H_{j}^{l}\right)\right)
\end{aligned}
$$</p>
<h2>D Hyperparameters</h2>
<h2>D. 1 Model Architecture, MAG240M and arxiv</h2>
<p>We initialize the node features in citation network datasets using a pretrained language model (RoBERTa [12] base model trained on NLI and STSB). The architecture of our PromptGraph model in all of our proposed methods for citation network datasets (full PRODIGY, PG-NM, and PG-MT) and the baseline (NoPretrain), consists of two message passing layers, $M_{\mathrm{B}}$, over the DataGraph and one message passing layer, $M_{\mathrm{T}}$, over the TaskGraph. These layers are defined in Section 3.1.
For the Contrastive method, the architecture includes two message passing layers, $M_{d}$, over the DataGraph, and a contrastive learning component that is defined in Section 4.1. The mode for Finetune is the same as the Contrastive method, with the addition of a linear layer head over the output of the two $M_{d}$ layers, also described in Section 4.1.</p>
<h2>D. 2 Model Architecture, knowledge graph datasets</h2>
<p>We initialize node and edge features in knowledge graph datasets using a pretrained language model (MPNet [15]). The architecture of our PromptGraph model in all of our proposed methods for knowledge graph datasets (full PRODIGY, PG-NM, and PG-MT) and the baseline (NoPretrain), consists of two message passing layers, $M_{\mathrm{B}}$, over the DataGraph, an aggregator as described by</p>
<p>Table 4: Ablation of PG-NM on arXiv.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ways</th>
<th style="text-align: center;">PG-NM</th>
<th style="text-align: center;">$3 \rightarrow 1$ shot</th>
<th style="text-align: center;">No Attr</th>
<th style="text-align: center;">No Aug</th>
<th style="text-align: center;">No Attr, Aug</th>
<th style="text-align: center;">No Attr, Aug, $M_{\mathrm{T}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\mathbf{7 2 . 5 0} \pm \mathbf{0 . 3 5}$</td>
<td style="text-align: center;">$69.13 \pm 1.09$</td>
<td style="text-align: center;">$65.74 \pm 1.12$</td>
<td style="text-align: center;">$68.98 \pm 1.09$</td>
<td style="text-align: center;">$66.53 \pm 1.12$</td>
<td style="text-align: center;">$63.60 \pm 1.06$</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\mathbf{6 1 . 2 1} \pm \mathbf{0 . 2 9}$</td>
<td style="text-align: center;">$57.49 \pm 0.92$</td>
<td style="text-align: center;">$52.78 \pm 0.90$</td>
<td style="text-align: center;">$57.50 \pm 0.85$</td>
<td style="text-align: center;">$53.89 \pm 0.92$</td>
<td style="text-align: center;">$51.27 \pm 0.69$</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$\mathbf{4 6 . 1 2} \pm \mathbf{0 . 1 9}$</td>
<td style="text-align: center;">$42.03 \pm 0.60$</td>
<td style="text-align: center;">$37.99 \pm 0.63$</td>
<td style="text-align: center;">$42.43 \pm 0.64$</td>
<td style="text-align: center;">$38.87 \pm 0.59$</td>
<td style="text-align: center;">$37.62 \pm 0.34$</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$\mathbf{3 3 . 7 1} \pm \mathbf{0 . 1 1}$</td>
<td style="text-align: center;">$30.18 \pm 0.38$</td>
<td style="text-align: center;">$26.60 \pm 0.36$</td>
<td style="text-align: center;">$30.89 \pm 0.38$</td>
<td style="text-align: center;">$27.50 \pm 0.36$</td>
<td style="text-align: center;">$27.44 \pm 0.17$</td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$\mathbf{2 3 . 6 9} \pm \mathbf{0 . 0 7}$</td>
<td style="text-align: center;">$21.44 \pm 0.22$</td>
<td style="text-align: center;">$18.03 \pm 0.21$</td>
<td style="text-align: center;">$21.97 \pm 0.24$</td>
<td style="text-align: center;">$18.52 \pm 0.22$</td>
<td style="text-align: center;">$19.69 \pm 0.08$</td>
</tr>
</tbody>
</table>
<p>Equation 3, and two message passing layers, $M_{\mathrm{T}}$, over the TaskGraph, which only pass messages along the positive and query edges. These layers are defined in Section 3.1.</p>
<p>For the Contrastive method, the architecture includes two message passing layers, $M_{d}$, over the DataGraph, an aggregator as described by Equation 3 and a contrastive learning component that is defined in Section 4.1. The mode for Finetune is the same as the Contrastive method, with the addition of a linear layer head over the output of the two $M_{d}$ layers, also described in Section 4.1.</p>
<h1>D. 3 Training, MAG240M</h1>
<p>The following describes our pretraining setup:
The pretraining task we used consisted of 30 ways, 3 shots, and 4 queries per task. This specific task configuration was carefully selected to strike a balance between complexity and diversity in the training data, without overwhelming the GPU memory.</p>
<p>We checkpoint the model every 500 steps.
Our pretraining setup included a model with an input dimension of 768 and an embedding dimension of 256, batch size of 1 , and the AdamW optimizer with a learning rate of $1 \times 10^{-3}$ and weight decay of $1 \times$ $10^{-3}$, a pretraining task with 30 ways, 3 shots, and 4 queries per task, and checkpointing every 500 steps. This consistent configuration was applied across all the methods for fair comparison. Our full PRODIGY setup, on average, involves sampling 1 Neighbor Matching task per 1 multitask pretraining tasks.</p>
<p>For our evaluation process, we computed zero-shot transfer performance of the model on the test set, using the pretraining checkpoint at the 10,000 step of pretraining. The evaluation was conducted on 500 test tasks, with batch size of 5 , measured on the downstream task of graph classification accuracy. To maintain consistency, we kept the number of shots and queries constant at 3 for all evaluation tasks.</p>
<h2>D. 4 Training, Wiki</h2>
<p>The following describes our pretraining setup:
Our pretraining setup included a model with an input dimension of 768 and an embedding dimension of 256, the AdamW optimizer with a learning rate of $1 \times 10^{-3}$ and weight decay of $1 \times 10^{-3}$, a pretraining task with 30 ways, 3 shots, and 4 queries per task, using a batch size of 10 , and checkpointing every 500 steps. This specific task configuration was carefully selected to strike a balance between complexity and diversity in the training data, without overwhelming the GPU memory. This consistent configuration was applied across all the methods for fair comparison. Our full PRODIGY setup involves sampling one neighbor matching task per 50 multitask pretraining tasks.</p>
<p>For our evaluation process, we computed zero-shot transfer performance of the model on the test set, using the pretraining checkpoint at the 8,000 step of pretraining. The evaluation was conducted on 500 test tasks, with batch size of 1 , measured on the downstream task of graph classification accuracy. To maintain consistency, we kept the number of shots and queries constant at 3 for all evaluation tasks. We sample 1-hop neighbourhoods for ConceptNet and FB15K-237 and 2-hop neigbourhoods for NELL and Wiki.</p>
<h2>E Ablation on Table 4 for the PG-NM setting</h2>
<p>In Table 4, we ablate on various configurations of the self-supervised objective PG-NM. As described in Section 3.2, PG-NM is composed of an attribute prediction loss, dropnode and zeronode augmentations,</p>
<p>with the default setting of sampling 3 shots neighbor matching tasks. Our best setting, referred to as simply "PG-NM", is also shown in Table 1 and comprises of attribute prediction, dropnode and zeronode augmentations, with the default setting of 3 shots.</p>
<p>The ablation results reveal that using all of these elements together results in the highest performance. Specifically, attribute prediction has the greatest impact on PG-NM's performance, as its removal results in an average $7 \%$ drop across all ways, as shown in the 'No-Attr' column.</p>
<p>Removing the dropnode and zeronode augmentations results in an average $3 \%$ drop across all ways, as shown in the No Aug' column. Removing both attribute prediction and augmentations results in performance that is similar to just removing attribute prediction alone, which is also roughly a $7 \%$ drop across all ways, as shown in the 'No Attr, Aug' column. Additionally, we found that decreasing the number of shots to 1 from the default setting of 3 resulted in an average $3.5 \%$ drop across all ways, as shown.</p>
<h1>F Evaluation using different numbers of shots</h1>
<p>We show evaluation using different numbers of shots, as shown in Figures 3, 5, and 6, 7.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: In-context learning accuracy on FB15K-237 in a 20-ways setting wrt. the number of prompt examples (shots).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: In-context learning accuracy on NELL in a 20-ways setting wrt. the number of prompt examples (shots).</p>
<h2>G Scaling with Data Size</h2>
<p>We explore how the model scales with more pretraining tasks. Note that we use the number of train steps as a proxy because the model sees more pretraining tasks as the training proceeds with almost no redundancy ( $0.20 \%$ for 10k tasks). The result on arXiv in a 5 -ways setting is illustrated in Figure 4. It shows that the Contrastive baseline saturates quickly and its performance fluctuates given more</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: In-context learning accuracy on arXiv in a 5-ways setting wrt. the number of prompt examples (shots).
pretraining tasks. Instead, PRODIGY consistently shows an improvement in performance as more data is pretrained on.</p>
<h1>H Compute</h1>
<p>We use one NVIDIA A100-SXM4-80GB GPU for all our experiments. One pretrain run of 10k steps takes 3 to 4 hours.</p>
<h2>I Broader Impacts</h2>
<p>Our work aims to extend the success of in-context learning to graphs and start building toward graph foundation models. This would allow cost-effective and accurate predictions, especially in domains where labeled data is scarce and long tail such as network anomaly detection, rare disease diagnosis/treatment, supply chain disruption, and recommendations for new users. However, overreliance on prior knowledge from pretraining could also lead to increased social bias and unfair benefits to the dominate groups. To mitigate this, pretraining data should be diverse and well-balanced, and the pretrained models should be tested on downstream tasks over different groups and subdistributions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*indicates equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-529 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-529</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-529</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-89c8aad71433f7638d2e2c009e1ea20e039f832d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/89c8aad71433f7638d2e2c009e1ea20e039f832d" target="_blank">AI2-THOR: An Interactive 3D Environment for Visual AI</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks and facilitate building visually intelligent models.</p>
                <p><strong>Paper Abstract:</strong> We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e529.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e529.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALFRED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALFRED</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark/dataset for interactive instruction following where agents interpret natural language instructions to perform multi-step household tasks in AI2-THOR-like environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Alfred: A benchmark for interpreting grounded instructions for everyday tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive instruction following (ALFRED)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents receive natural language instructions describing multi-step household tasks (e.g., pick-and-place, cooking) and must execute sequences of navigation and interaction actions in photorealistic indoor scenes to complete those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / multi-step planning / household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on embodied task data (instruction-annotated trajectories) / paired vision-language data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised training / imitation learning from annotated demonstrations; used as a benchmark for models that combine vision and language</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>multimodal (vision + language) episodic representations and learned action policies; procedural steps encoded as sequences aligned to instructions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables evaluation of models that can follow multi-step natural language instructions by grounding language to actions and objects in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Paper does not report specific failure modes here; AI2-THOR only cites ALFRED as a dataset/benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a primary benchmark for embodied instruction following; demonstrates the need for models to encode procedural sequences, object affordances, and spatial grounding to succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e529.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEACh</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEACh</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset for interactive instruction following that uses human-robot dialog to specify tasks in an embodied environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TEACh: Task-driven embodied agents that chat.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive instruction following via dialog (TEACh)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents must complete tasks specified via multi-turn human-robot dialog, requiring grounding of conversational instructions to sequences of navigation and manipulation actions in indoor scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / dialog-grounded planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on dialog-annotated embodied trajectories and paired vision-language data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised/fine-tuning on dialog-instruction datasets; multi-turn grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>dialog-conditioned policies and multimodal episodic representations aligning conversational turns to action sequences and object states</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Supports research into grounding dialog to actions, indicating models must represent procedural steps and object relationships to follow multi-turn instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>No specific failure patterns given in this AI2-THOR paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an embodied language dataset highlighting the importance of procedural and object-relational grounding in instruction-following agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e529.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PIGLeT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PIGLeT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic approach for language grounding through interaction in a 3D world, connecting language instructions to executable symbolic plans in embodied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language grounding through neuro-symbolic interaction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Models map natural language instructions to symbolic representations and interact in a 3D environment to execute tasks, combining learned perception with symbolic planners/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / neuro-symbolic planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learning from interaction data + symbolic program synthesis components</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>neuro-symbolic training: perception modules learned from vision; symbolic planner executes grounded programs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic program representations combined with perceptual embeddings to ground objects and actions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Illustrates explicitly representing procedures as symbolic programs and grounding object relations to execute multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>AI2-THOR text does not enumerate failures; cited as an approach to language grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a method that explicitly represents procedural knowledge (programs) and grounds language to object-relational perception â€” an example of combining symbolic/action-sequence representations with learned perception.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e529.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Episodic Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Episodic Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based model that encodes the full history of vision and language inputs to support vision-and-language navigation and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic transformer for vision-and-language navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer architecture designed to encode episodic histories of multimodal observations (vision + language) to inform navigation/action decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent must navigate to goal locations guided by language instructions, using a representation of the episodic sequence of prior observations and instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + procedural (temporal sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>training on paired vision-language navigation data; episodic history encoding</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised/fine-tuning of transformer to map episodic multimodal history to actions</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>episodic transformer embeddings encoding history of observations and instructions (implicit in weights and attention over past states)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Demonstrates that encoding full history (episodic memory) helps integrate procedural and spatial information across time for navigation</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>No failure specifics provided in AI2-THOR; mentioned as relevant prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that sequence-modeling (transformer) of multimodal episodic history is a viable way to represent procedural/spatial knowledge for navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e529.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FILM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FILM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular method that builds a semantic map to perform exploration for instruction following in embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FILM: Following instructions in language with modular methods.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction following with semantic mapping (FILM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents construct semantic maps of the environment to guide exploration and action selection in response to language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation / instruction following / mapping</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>visual observation + learned semantic mapping modules trained on embodied data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>modular learning: perception -> semantic map -> planner</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit semantic maps (spatial representations annotated with object categories) used by downstream planners</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Shows utility of explicit spatial maps that encode object semantics to improve exploration and instruction execution.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not detailed in AI2-THOR text; cited as prior work highlighting semantic mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as an example where spatial knowledge is represented explicitly (semantic maps) and leveraged for exploration and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e529.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iSEE (probing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iSEE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing/interpretability study that analyzes what navigation agents learn about their environment by probing hidden representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What do navigation agents learn about their environment?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Probing/interpretability of navigation agents (iSEE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Uses probing techniques to query hidden representations of embodied navigation agents (ObjectNav, PointNav) to reveal encoded spatial information such as distance to target.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interpretability / navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (distance, possibly layout) + implicit procedural cues</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit in learned agent weights from training on navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing classifiers applied to hidden activations of trained agents to detect encoded properties</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit representations in network activations that can be linearly probed for spatial information</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Probing finds that agents' hidden representations encode spatial information (e.g., estimates of distance to target).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not specified in AI2-THOR; probing studies typically reveal gaps in fine-grained metric spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to illustrate that spatial knowledge (like distance to target) can be implicitly encoded in learned agent representations and recovered via probing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e529.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP embeddings (embodied)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP embeddings for Embodied AI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using CLIP's pre-trained image-language embeddings as visual encoders in embodied agents to improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simple but effective: CLIP embeddings for embodied ai.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained multimodal model that aligns image and text embeddings; used here as a frozen or fine-tuned visual encoder for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ObjectNav / visual navigation with CLIP encoders</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents navigate to semantic target categories; replacing or augmenting visual encoders with CLIP embeddings improves generalization to unseen scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (semantic category priors) + spatial (implicit via visual features)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large image-text corpora (contrastive learning)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pretrained embedding transfer; used as a visual encoder in embodied training/fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>multimodal image-text embeddings that encode semantic object-level information; implicit in learned weights</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>generalization performance (e.g., zero-shot transfer improvements reported in that work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Pretrained CLIP embeddings provide strong semantic priors that improve generalization for object-centric navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>AI2-THOR text does not detail failure modes; suggests CLIP helps but not a complete solution.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited evidence that language-supervised visual pretraining (CLIP) imparts object-relational semantic knowledge that benefits embodied navigation generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e529.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IFR-Explore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IFR-Explore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach to learn inter-object functional relationships (which objects are used together) in 3D indoor scenes to guide agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>IFR-Explore: Learning inter-object functional relationships in 3D indoor scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Learning object relationships for exploration</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learn priors about functional relationships between objects (e.g., knobs control stove burners, remote controls TVs) and use these to guide search and task completion in embodied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational learning / navigation / search</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial (relative placements/co-occurrence)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learning from 3D scene data and interactions</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>learning relational priors from egocentric or scene data and using them as guidance in policies</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>object-relation priors, likely encoded as graphs or learned embeddings linking object categories and typical spatial/functional relationships</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Shows that learned inter-object functional priors can guide agents more efficiently to complete tasks or find objects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not detailed in AI2-THOR; cited as prior work on object-relational priors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as demonstrating the utility of object-relational priors (functional relationships) to improve embodied task efficiency and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e529.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e529.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Affordance Landscapes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Affordance Landscapes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method to learn which objects afford which interactions and how, by training agents to interact in 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning affordance landscapes for interaction exploration in 3D environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Affordance learning via interaction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents interact with objects to learn affordances (what actions are possible on which objects and how), useful for downstream manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / learning with interaction</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (affordances) + procedural</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>learning through interaction (self-supervised / RL) in simulated environments</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>interaction-driven learning; exploration policies to discover affordances</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>affordance landscapes (likely function/value maps over object-action pairs) encoded in model parameters</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Learns which objects can be opened, toggled, sliced, etc., and which actions are applicable â€” useful for manipulation planning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not specified in AI2-THOR; mentioned to illustrate affordance learning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that object affordances can be learned through interaction and used to inform manipulation planning; encodes object-relational procedural knowledge about actions applicable to objects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI2-THOR: An Interactive 3D Environment for Visual AI', 'publication_date_yy_mm': '2017-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. <em>(Rating: 2)</em></li>
                <li>TEACh: Task-driven embodied agents that chat. <em>(Rating: 2)</em></li>
                <li>PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world. <em>(Rating: 2)</em></li>
                <li>Episodic transformer for vision-and-language navigation. <em>(Rating: 2)</em></li>
                <li>FILM: Following instructions in language with modular methods. <em>(Rating: 2)</em></li>
                <li>What do navigation agents learn about their environment? <em>(Rating: 2)</em></li>
                <li>Simple but effective: CLIP embeddings for embodied ai. <em>(Rating: 2)</em></li>
                <li>IFR-Explore: Learning inter-object functional relationships in 3D indoor scenes. <em>(Rating: 2)</em></li>
                <li>Learning affordance landscapes for interaction exploration in 3D environments. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-529",
    "paper_id": "paper-89c8aad71433f7638d2e2c009e1ea20e039f832d",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "ALFRED",
            "name_full": "ALFRED",
            "brief_description": "A benchmark/dataset for interactive instruction following where agents interpret natural language instructions to perform multi-step household tasks in AI2-THOR-like environments.",
            "citation_title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Interactive instruction following (ALFRED)",
            "task_description": "Agents receive natural language instructions describing multi-step household tasks (e.g., pick-and-place, cooking) and must execute sequences of navigation and interaction actions in photorealistic indoor scenes to complete those tasks.",
            "task_type": "instruction following / multi-step planning / household tasks",
            "knowledge_type": "procedural + object-relational + spatial",
            "knowledge_source": "fine-tuning on embodied task data (instruction-annotated trajectories) / paired vision-language data",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised training / imitation learning from annotated demonstrations; used as a benchmark for models that combine vision and language",
            "knowledge_representation": "multimodal (vision + language) episodic representations and learned action policies; procedural steps encoded as sequences aligned to instructions",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Enables evaluation of models that can follow multi-step natural language instructions by grounding language to actions and objects in the environment.",
            "failure_patterns": "Paper does not report specific failure modes here; AI2-THOR only cites ALFRED as a dataset/benchmark.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Mentioned as a primary benchmark for embodied instruction following; demonstrates the need for models to encode procedural sequences, object affordances, and spatial grounding to succeed.",
            "uuid": "e529.0",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "TEACh",
            "name_full": "TEACh",
            "brief_description": "A dataset for interactive instruction following that uses human-robot dialog to specify tasks in an embodied environment.",
            "citation_title": "TEACh: Task-driven embodied agents that chat.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Interactive instruction following via dialog (TEACh)",
            "task_description": "Agents must complete tasks specified via multi-turn human-robot dialog, requiring grounding of conversational instructions to sequences of navigation and manipulation actions in indoor scenes.",
            "task_type": "instruction following / dialog-grounded planning",
            "knowledge_type": "procedural + object-relational + spatial",
            "knowledge_source": "fine-tuning on dialog-annotated embodied trajectories and paired vision-language data",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised/fine-tuning on dialog-instruction datasets; multi-turn grounding",
            "knowledge_representation": "dialog-conditioned policies and multimodal episodic representations aligning conversational turns to action sequences and object states",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Supports research into grounding dialog to actions, indicating models must represent procedural steps and object relationships to follow multi-turn instructions.",
            "failure_patterns": "No specific failure patterns given in this AI2-THOR paper.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Cited as an embodied language dataset highlighting the importance of procedural and object-relational grounding in instruction-following agents.",
            "uuid": "e529.1",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "PIGLeT",
            "name_full": "PIGLeT",
            "brief_description": "A neuro-symbolic approach for language grounding through interaction in a 3D world, connecting language instructions to executable symbolic plans in embodied settings.",
            "citation_title": "PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Language grounding through neuro-symbolic interaction",
            "task_description": "Models map natural language instructions to symbolic representations and interact in a 3D environment to execute tasks, combining learned perception with symbolic planners/execution.",
            "task_type": "instruction following / neuro-symbolic planning",
            "knowledge_type": "procedural + object-relational + spatial",
            "knowledge_source": "learning from interaction data + symbolic program synthesis components",
            "has_direct_sensory_input": true,
            "elicitation_method": "neuro-symbolic training: perception modules learned from vision; symbolic planner executes grounded programs",
            "knowledge_representation": "explicit symbolic program representations combined with perceptual embeddings to ground objects and actions",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Illustrates explicitly representing procedures as symbolic programs and grounding object relations to execute multi-step tasks.",
            "failure_patterns": "AI2-THOR text does not enumerate failures; cited as an approach to language grounding.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Mentioned as a method that explicitly represents procedural knowledge (programs) and grounds language to object-relational perception â€” an example of combining symbolic/action-sequence representations with learned perception.",
            "uuid": "e529.2",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "Episodic Transformer",
            "name_full": "Episodic Transformer",
            "brief_description": "A transformer-based model that encodes the full history of vision and language inputs to support vision-and-language navigation and instruction following.",
            "citation_title": "Episodic transformer for vision-and-language navigation.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": "A transformer architecture designed to encode episodic histories of multimodal observations (vision + language) to inform navigation/action decisions.",
            "task_name": "Vision-and-language navigation",
            "task_description": "Agent must navigate to goal locations guided by language instructions, using a representation of the episodic sequence of prior observations and instructions.",
            "task_type": "navigation / instruction following",
            "knowledge_type": "spatial + procedural (temporal sequences)",
            "knowledge_source": "training on paired vision-language navigation data; episodic history encoding",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised/fine-tuning of transformer to map episodic multimodal history to actions",
            "knowledge_representation": "episodic transformer embeddings encoding history of observations and instructions (implicit in weights and attention over past states)",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Demonstrates that encoding full history (episodic memory) helps integrate procedural and spatial information across time for navigation",
            "failure_patterns": "No failure specifics provided in AI2-THOR; mentioned as relevant prior work.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Cited as evidence that sequence-modeling (transformer) of multimodal episodic history is a viable way to represent procedural/spatial knowledge for navigation tasks.",
            "uuid": "e529.3",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "FILM",
            "name_full": "FILM",
            "brief_description": "A modular method that builds a semantic map to perform exploration for instruction following in embodied tasks.",
            "citation_title": "FILM: Following instructions in language with modular methods.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Instruction following with semantic mapping (FILM)",
            "task_description": "Agents construct semantic maps of the environment to guide exploration and action selection in response to language instructions.",
            "task_type": "navigation / instruction following / mapping",
            "knowledge_type": "spatial + object-relational",
            "knowledge_source": "visual observation + learned semantic mapping modules trained on embodied data",
            "has_direct_sensory_input": true,
            "elicitation_method": "modular learning: perception -&gt; semantic map -&gt; planner",
            "knowledge_representation": "explicit semantic maps (spatial representations annotated with object categories) used by downstream planners",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Shows utility of explicit spatial maps that encode object semantics to improve exploration and instruction execution.",
            "failure_patterns": "Not detailed in AI2-THOR text; cited as prior work highlighting semantic mapping.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Used as an example where spatial knowledge is represented explicitly (semantic maps) and leveraged for exploration and instruction following.",
            "uuid": "e529.4",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "iSEE (probing)",
            "name_full": "iSEE",
            "brief_description": "A probing/interpretability study that analyzes what navigation agents learn about their environment by probing hidden representations.",
            "citation_title": "What do navigation agents learn about their environment?",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Probing/interpretability of navigation agents (iSEE)",
            "task_description": "Uses probing techniques to query hidden representations of embodied navigation agents (ObjectNav, PointNav) to reveal encoded spatial information such as distance to target.",
            "task_type": "interpretability / navigation",
            "knowledge_type": "spatial (distance, possibly layout) + implicit procedural cues",
            "knowledge_source": "implicit in learned agent weights from training on navigation tasks",
            "has_direct_sensory_input": true,
            "elicitation_method": "probing classifiers applied to hidden activations of trained agents to detect encoded properties",
            "knowledge_representation": "implicit representations in network activations that can be linearly probed for spatial information",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Probing finds that agents' hidden representations encode spatial information (e.g., estimates of distance to target).",
            "failure_patterns": "Not specified in AI2-THOR; probing studies typically reveal gaps in fine-grained metric spatial understanding.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Cited to illustrate that spatial knowledge (like distance to target) can be implicitly encoded in learned agent representations and recovered via probing.",
            "uuid": "e529.5",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "CLIP embeddings (embodied)",
            "name_full": "CLIP embeddings for Embodied AI",
            "brief_description": "Using CLIP's pre-trained image-language embeddings as visual encoders in embodied agents to improve generalization.",
            "citation_title": "Simple but effective: CLIP embeddings for embodied ai.",
            "mention_or_use": "mention",
            "model_name": "CLIP",
            "model_size": null,
            "model_description": "Pretrained multimodal model that aligns image and text embeddings; used here as a frozen or fine-tuned visual encoder for embodied agents.",
            "task_name": "ObjectNav / visual navigation with CLIP encoders",
            "task_description": "Agents navigate to semantic target categories; replacing or augmenting visual encoders with CLIP embeddings improves generalization to unseen scenes.",
            "task_type": "navigation",
            "knowledge_type": "object-relational (semantic category priors) + spatial (implicit via visual features)",
            "knowledge_source": "pre-training on large image-text corpora (contrastive learning)",
            "has_direct_sensory_input": true,
            "elicitation_method": "pretrained embedding transfer; used as a visual encoder in embodied training/fine-tuning",
            "knowledge_representation": "multimodal image-text embeddings that encode semantic object-level information; implicit in learned weights",
            "performance_metric": "generalization performance (e.g., zero-shot transfer improvements reported in that work)",
            "performance_result": null,
            "success_patterns": "Pretrained CLIP embeddings provide strong semantic priors that improve generalization for object-centric navigation tasks.",
            "failure_patterns": "AI2-THOR text does not detail failure modes; suggests CLIP helps but not a complete solution.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Cited evidence that language-supervised visual pretraining (CLIP) imparts object-relational semantic knowledge that benefits embodied navigation generalization.",
            "uuid": "e529.6",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "IFR-Explore",
            "name_full": "IFR-Explore",
            "brief_description": "Approach to learn inter-object functional relationships (which objects are used together) in 3D indoor scenes to guide agent behavior.",
            "citation_title": "IFR-Explore: Learning inter-object functional relationships in 3D indoor scenes.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Learning object relationships for exploration",
            "task_description": "Learn priors about functional relationships between objects (e.g., knobs control stove burners, remote controls TVs) and use these to guide search and task completion in embodied settings.",
            "task_type": "object-relational learning / navigation / search",
            "knowledge_type": "object-relational + spatial (relative placements/co-occurrence)",
            "knowledge_source": "learning from 3D scene data and interactions",
            "has_direct_sensory_input": true,
            "elicitation_method": "learning relational priors from egocentric or scene data and using them as guidance in policies",
            "knowledge_representation": "object-relation priors, likely encoded as graphs or learned embeddings linking object categories and typical spatial/functional relationships",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Shows that learned inter-object functional priors can guide agents more efficiently to complete tasks or find objects.",
            "failure_patterns": "Not detailed in AI2-THOR; cited as prior work on object-relational priors.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Cited as demonstrating the utility of object-relational priors (functional relationships) to improve embodied task efficiency and planning.",
            "uuid": "e529.7",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        },
        {
            "name_short": "Affordance Landscapes",
            "name_full": "Learning Affordance Landscapes",
            "brief_description": "Method to learn which objects afford which interactions and how, by training agents to interact in 3D environments.",
            "citation_title": "Learning affordance landscapes for interaction exploration in 3D environments.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_description": null,
            "task_name": "Affordance learning via interaction",
            "task_description": "Agents interact with objects to learn affordances (what actions are possible on which objects and how), useful for downstream manipulation tasks.",
            "task_type": "object manipulation / learning with interaction",
            "knowledge_type": "object-relational (affordances) + procedural",
            "knowledge_source": "learning through interaction (self-supervised / RL) in simulated environments",
            "has_direct_sensory_input": true,
            "elicitation_method": "interaction-driven learning; exploration policies to discover affordances",
            "knowledge_representation": "affordance landscapes (likely function/value maps over object-action pairs) encoded in model parameters",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Learns which objects can be opened, toggled, sliced, etc., and which actions are applicable â€” useful for manipulation planning.",
            "failure_patterns": "Not specified in AI2-THOR; mentioned to illustrate affordance learning.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Cited as evidence that object affordances can be learned through interaction and used to inform manipulation planning; encodes object-relational procedural knowledge about actions applicable to objects.",
            "uuid": "e529.8",
            "source_info": {
                "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
                "publication_date_yy_mm": "2017-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks.",
            "rating": 2
        },
        {
            "paper_title": "TEACh: Task-driven embodied agents that chat.",
            "rating": 2
        },
        {
            "paper_title": "PIGLeT: Language grounding through neuro-symbolic interaction in a 3D world.",
            "rating": 2
        },
        {
            "paper_title": "Episodic transformer for vision-and-language navigation.",
            "rating": 2
        },
        {
            "paper_title": "FILM: Following instructions in language with modular methods.",
            "rating": 2
        },
        {
            "paper_title": "What do navigation agents learn about their environment?",
            "rating": 2
        },
        {
            "paper_title": "Simple but effective: CLIP embeddings for embodied ai.",
            "rating": 2
        },
        {
            "paper_title": "IFR-Explore: Learning inter-object functional relationships in 3D indoor scenes.",
            "rating": 2
        },
        {
            "paper_title": "Learning affordance landscapes for interaction exploration in 3D environments.",
            "rating": 2
        }
    ],
    "cost": 0.01415725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AI2-THOR: An Interactive 3D Environment for Visual AI</h1>
<p>Eric Kolve ${ }^{1}$, Roozbeh Mottaghi ${ }^{1,2}$, Winson Han ${ }^{1}$, Eli VanderBilt ${ }^{1}$, Luca Weihs ${ }^{1}$, Alvaro Herrasti ${ }^{1}$, Matt Deitke ${ }^{1,2}$, Kiana Ehsani ${ }^{1}$, Daniel Gordon ${ }^{2}$, Yuke Zhu ${ }^{3}$, Aniruddha Kembhavi ${ }^{1,2}$, Abhinav Gupta ${ }^{1,4}$, Ali Farhadi ${ }^{1,2}$<br>${ }^{1}$ Allen Institute for AI, ${ }^{2}$ University of Washington, ${ }^{3}$ Stanford University, ${ }^{4}$ Carnegie Mellon University</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AI2-THOR consists of interactive 3D environments that can be used with embodied agents.</p>
<h4>Abstract</h4>
<p>We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.</p>
<h2>1 What is AI2-THOR?</h2>
<p>Humans demonstrate levels of visual understanding that go well beyond current formulations of mainstream vision tasks (e.g. object detection, scene recognition, image segmentation). A key element to visual intelligence is the ability to interact with the environment and learn from those interactions. Current state-of-the-art models in computer vision are trained by using still images or videos. This is different from how humans learn. We introduce AI2-THOR as a step towards human-like learning based on visual input.
There are several key factors that distinguish AI2-THOR from other simulated environments:</p>
<ol>
<li>Interactions. AI2-THOR supports many types of interactions, including object state changes, arm-based manipulation, and causal interactions. For example, a microwave can be opened or closed, a loaf of bread</li>
</ol>
<p>can be sliced and toasted in the toaster, and a faucet can be turned on to fill a mug with water. Figure 6 shows some examples of interactions supported in AI2-THOR.
2. Scenes. AI2-THOR provides substantially more interactive objects and scenes for training than other platforms [18, 32, 7] by using procedural generation [2]. We also provide support for many scenes designed manually by professional 3D artists, with 120 stand-alone rooms in iTHOR, 89 scenes in RoboTHOR [1], and 10 evaluation houses ArchitecTHOR [2].
3. Quality. The objects and scenes in AI2-THOR are near photo-realistic. This allows better transfer of the learned models to the real world. In contrast, ATARI games or board games such as GO, which are typically used to demonstrate the performance of AI models, are very different from the real world and lack much of the visual complexity of natural environments.
4. API. AI2-THOR provides a Python API to interact with the Unity 3D game engine that provides many different functionalities such as navigation, applying forces, object interaction, and physics modeling.</p>
<p>Real robot experiments are typically performed in lab settings or constrained scenes since deploying robots in various indoor and outdoor scenes is not scalable. This makes training models that generalize to various situations difficult. Additionally, due to mechanical constraints of robot actuators, using learning algorithms that require thousands of iterations is infeasible. Furthermore, training real robots might be costly or unsafe as they might damage the surrounding environment or the robots themselves during training. AI2-THOR provides a scalable, fast and cheap proxy for real world experiments in different types of scenarios.</p>
<p>In the following sections, we discuss more of the features included in AI2-THOR, how it compares to other simulators, and work that has been conducted in it since the initial release.</p>
<h1>2 What does AI2-THOR feature?</h1>
<p>AI2-THOR is used for a wide range of tasks in Embodied AI, robotics, and computer vision. It encompasses many different types of scenes; different types of agents, each with its own set of actions to interact with objects; support for many image modalities; and functions to provide metadata about the state of the environment.</p>
<h3>2.1 API</h3>
<p>Figure 2 shows AI2-THOR's agent-simulator loop, which shows the front-end Python API that interacts with the Unity back-end. Here, actions are called from the Python API, which are sent through a local server to Unity. Unity is a powerful real-time game engine, which stores our scenes, code pertaining to how actions should be executed, 3D objects with their properties, and shaders to render different image modalities. Unity then returns an Event, which contains images from the cameras in the scene and the environment metadata.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: AI2-THOR's agent-simulator loop, where users control an agent from the Python side that interacts with the backend Unity simulator.</p>
<h3>2.2 Scene Datasets</h3>
<p>Many scene datasets have been built as part of AI2-THOR, including iTHOR, RoboTHOR [1], ProcTHOR10K [2], and ArchitecTHOR [2]. Each of these scene datasets is interactive and can be used from the same API with any of the agents.
iTHOR is the original set of scenes used for all experiments, which includes 120 room-sized scenes, covering bedrooms, bathrooms, kitchens, and living rooms. The scenes are modeled by hand by professional 3D artists.
RoboTHOR [1] was later developed, which consists of 89 maze-styled dorm-sized apartments to study sim2real transfer. The scenes are also developed by professional 3D artists. Many of the scenes are recreated in Seattle, near</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: AI2-THOR includes many scene datasets, including iTHOR, RoboTHOR [1], ProcTHOR [2], and ArchitecTHOR [2].
the Allen Institute for AI's offices, to study the discrepancies when evaluating models in the same environments in simulation compared to reality.
ProcTHOR [2] aims to use procedural generation to massively scale up the number and diversity of training scenes to improve generalization in Embodied AI. Overfitting to the training scenes is a severe problem that is often observed when training on iTHOR and RoboTHOR scenes, and it was hypothesized that merely improving the training data could help solve this problem. ProcTHOR-10K, the initial dataset released with the paper and used for experimentation, procedurally generates 10 K diverse and semantically plausible houses for training. Using ProcTHOR for training led to remarkable generalization results, and we expect it to be used as a starting point for training most projects in AI2-THOR moving forward.
ArchitecTHOR [2] is a set of 10 evaluation houses ( 5 for validation, 5 for testing) that was developed in conjunction with ProcTHOR. With ProcTHOR being procedurally generated, a test set of houses that comes from a real-world distribution are needed to evaluate if models training on ProcTHOR merely memorize biases from the procedural generation, or if they are capable of generalizing to real-world floorplans and object placements. Similar to iTHOR and RoboTHOR, the scenes are hand-built by professional 3D artists, although ArchitecTHOR houses are much larger and styled as single story houses.</p>
<h1>2.3 Agents</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The current agents available in AI2-THOR include the ManipulaTHOR and StretchRE1 agents, which support arm manipulation, and the LoCoBot, Abstract, and Drone agents, which support navigation and abstracted interaction.</p>
<p>AI2-THOR comes equipped with many agents that support a range of embodiments, including the ManipulaTHOR [5] agent, StretchRE1 [14], LoCoBot [24], Abstract agent, and Drone [42] agent. Each of these agents is embodied with a different physical robot and has its own set of actions that it can execute in the environment.
All of the agents are able to navigate around the scenes and perform environment queries and state changes. The ManipulaTHOR and StetchRE1 agents are able to use their arm to grasp and open objects. The LoCoBot, Abstract, and Drone agents interact with objects in a more abstract way, where a high-level OPEN or PICKUP command is executed if the agent is looking at the object, the high-level action is called, and the agent within a certain distance of the object.</p>
<h1>2.4 Actions</h1>
<p>Agents in AI2-THOR support a wide range of actions, which we can break down into navigation actions, interaction actions, environment queries, and environment state changes.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Examples of actions supported in AI2-THOR, including navigation actions (e.g. movement), interactive actions (e.g. object state changes and grasping), environment queries (e.g. finding the shortest path), and environment state changes (e.g. randomizing materials).</p>
<p>Navigation Actions. Each agent comes with some ability to navigate in a given scene. Navigation actions may be discrete or continuous move (e.g. MoveAhead by 0.25 m ), rotate (e.g. RotateRight by $30^{\circ}$ ), look (e.g. LookUP by $30^{\circ}$ ), or teleport actions. Agents with an arm have more actions to control how the arm is positioned.</p>
<p>Interactive Actions. There are many types of interactions supported in AI2-THOR, including abstracted interactions, arm-based manipulation, object state changes, and causal interactions.
Abstracted interactions are often a key component of research in Embodied AI, where one may be interested in studying high-level planning rather than low-level control. Here, an agent can execute an abstracted action, such as open, pickup, push, throw, drop, or place, where as long as the agent can see the object in its frame and it is within a certain distance away from it, the action can execute successfully. Abstracted actions can be used to change an object's state, such as cooking it, breaking it, slicing it, toggling it, filling it with liquid, or using it up.
Arm-based interactions are lower-level than abstracted actions, and require interacting with objects by moving an arm to grip them. They can be used to open an object incrementally in a continuous manner (Figure 5c) or grasping an object to move it from one position to another (Figure 5d).
Causal interactions result as a consequence of interacting with another object. For instance, turning a coffee machine on, which has a mug placed in it, will fill the mug with coffee; throwing a breakable an object hard enough may cause it and the surface it is thrown at to shatter; and pushing a table over will cause objects on top of the table to fall and potentially break.</p>
<p>Environment Queries. Environment queries are used to obtain information about the state of the environment that is not provided with each EVENT because it is often unnecessary to compute at each time step for every use case. Examples include obtaining the shortest path from the agent to a given target object in the scene, querying which object appears at a pixel in the agent's current frame, or obtaining the convex hull of a given object.</p>
<p>Environment State Changes. Environment state changes involve actions that modify the environment or its properties. For example, some environment state changes include randomizing the materials in the scene (Figure 5f), randomizing the lighting in the scene, updating the rendering quality, updating the resolution of the images from the cameras, and changing the skybox in the scene.</p>
<h1>2.5 Image Modalities</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Examples of image modalities supported in AI2-THOR, including RGB, depth, semantic segmentation, instance segmentation, and normals.</p>
<p>Figure 7 shows a suite of different image modalities that can be rendered from each of the cameras in the scene, including RGB, depth, semantic segmentation, instance segmentation, and normals. Each agent comes with a camera attached to it, but more cameras can also be added, such as one to capture a top-down view of the scene. More image modalities can be added by modifying the Unity back-end (often by adding shaders).</p>
<h3>2.6 Objects</h3>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Examples of objects in AI2-THOR's object database.</p>
<p>AI2-THOR includes 3,578 interactive objects in its object database, which is rapidly growing. Each of these objects has been hand-modeled to support our set of interactive actions and state changes, such as opening, breaking, or cooking. Figure 8 shows samples of objects from 4 categories, including alarm clocks, side tables, plants, and chairs.</p>
<h3>2.7 Environment Metadata</h3>
<p>Environment metadata is returned after each action is executed. It includes information such as the pose of each agent; the pose and state of each object in the scene (e.g., whether the object is moving, if it is visible to the agent, how far open it is, if it is clean or dirty); metadata about the scene, such as its size; and if the most recent action executed successfully (e.g., the agent did not collide with an object while trying to move). Metadata is often not provided to the agent for most tasks, as it would make the tasks too simple and easily solvable with a heuristic. Instead, many tasks use metadata to build a reward function with access to "expert-level" information that is hidden from the agent, build an imitation learning expert, and construct training and evaluation datasets.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Examples of environment metadata, including the dimensions of the scene, the 3D bounding box of each object, and the reachable grid positions, which may be used to randomize the agent's starting position or to build a heuristic search agent.</p>
<h1>3 What has AI2-THOR been used for?</h1>
<p>Since the initial release of AI2-THOR in 2017, it has been used for experimentation in over 150 publications and downloaded over 500k times. Some areas of work that we found particularly interesting include:</p>
<ul>
<li>Visual Navigation. Visual navigation was the first use case of AI2-THOR [45], which trains an agent to perform ImageNav (i.e. navigating to an image where the target object is described with a picture of it). Here, the agent executes a sequence of move or rotate commands to reach the target from egocentric camera inputs at each time step. ObjectNav is another common navigation task, where the agent is tasked with navigating to a given semantic category, such as a bed. Follow-up work from [40, 3, 44] uses semantic priors about where objects typically occur to improve navigation efficiency; [37] used meta-learning to try and better adapt to unseen scenes; [22] uses a Markov network to build a map of the environment; [15] found that using CLIP as a pre-trained visual encoder helps significantly boost generalization performance; and [2] found that training on many procedurally generated scenes strongly generalizes to RoboTHOR, iTHOR, and ArchitecTHOR in a 0 -shot setting.</li>
<li>Audio-Visual Navigation. [8] proposes the task of audio-visual navigation in which the agent is tasked with navigating to find where the sound is coming from in the scene.</li>
<li>Vision-and-Language. AI2-THOR has been used extensively for embodied vision-and-language research. Noteable datasets include ALFRED [31], for interactive instruction following from natural language; TEACh [27], for interactive instruction following from human-robot dialog; and DialFRED [9] and IQA [10] for interactive question-answering. Some other interesting work includes [28], which proposes the Episodic Transformer to encode the full history of vision and language inputs with each ALFRED task; [13], which uses grammar-based methods to learn high-level abstractions through decompositions of tasks; FILM [23], which builds a semantic map to perform exploration for instruction following; and PIGLeT [41], which learns natural language grounding through interaction.</li>
<li>Human-Robot Interaction. [38] inserts a human into AI2-THOR and uses virtual reality to control its gestures in simulation. By controlling the human's gestures, it can communicate different tasks it wants the robot to achieve, such as pointing to an object to encode moving to that object.</li>
<li>Sim2Real Transfer. RoboTHOR [1] studies sim2real transfer for robotics. Here, the goal is to train in simulation because it is faster, cheaper, and more scalable, and then to deploy the trained agent in the realworld. Agents train on 75 scenes in simulation and evaluate on unseen real-world scenes that come from a similar distribution. Initial work analyzed sim2real transfer for agents trained to perform ObjectNav.</li>
<li>Multi-Agent Interaction. [12] proposes the collaborative task of having 2 agents move to lift up furniture in a scene. For example, both agents might have to navigate to find the television in the scene, and work together to lift it up. Follow-up work from [11] takes the task a step further, where the agents not only have to lift up the furniture, but also work together to move it. Both tasks require visual navigation from the agents, and for them to communicate and coordinate together. Some other notable multi-agent work includes [35], which tasks agents with playing Cache, a variant of hide-and-seek where one agent hides an object and the other agent is tasked with finding that object; [33], which uses multiple agents for interactive question answering; [20], which proposes using multiple agents to more efficiently find multiple target objects in a scene; and</li>
</ul>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: AI2-THOR has enabled research in a wide range of fields. Here, we highlight some examples of how it has been used.</p>
<p>TEACh [27], which uses a commander agent and a follower agent to mimic human-robot dialog to solve interactive tasks.</p>
<ul>
<li>Learning Object Relationships. [19] proposes an approach to learn priors about inter-object functional relationships, such as which knobs on the stove control each burner, that the light switches controls a given light, and that the remote may control a television. [26] proposes using egocentric videos to learn which objects are used together to complete certain activities. They then use the priors to help guide agents towards achieving different activities in AI2-THOR.</li>
<li>Learning Affordances. [25] train an agent to interact with the environment to learn object affordances, which encode which objects may be interacted with and how. For instance, it learns that drawers or fridges may be opened, that the stove can turn on, and that an apple may be sliced. A model with an affordance landscape would make it easier to adapt to downstream tasks, such as learning to cut a tomato with a knife.</li>
<li>Scene Synthesis. ProcTHOR [2] uses procedural generation to synthesize training houses at scale to improve the generalization abilities of embodied agents. It procedurally generated and trained on 10 K houses by first sampling floorplans and then plausibly placing objects within each of the rooms in the floorplan. Remark-</li>
</ul>
<p>ably, pre-training on ProcTHOR alone was able to achieve state-of-the-art performance for ObjectNav on RoboTHOR, iTHOR, and ArchitecTHOR, without leveraging any additional training data. LUMINOUS [43] also uses scene synthesis techniques to train embodied agents, where it focuses on placing objects in iTHOR rooms.</p>
<ul>
<li>Learning with Interaction. AI2-THOR supports a wide range of interactions that can be used to train agents, including for rearranging objects in a scene with RoomR [34], arm-based manipulation with ManipulaTHOR [5], learning about objects by interacting with them [21], and playing hide-and-seek with objects to learn visual representations [35], among many others.</li>
<li>Computer Vision. The rich annotations available in simulation make it easy to use AI2-THOR for pure computer vision tasks. Notable work includes SeGAN [6], which used a GAN to generate occluded parts of an object from images in scene; Interactron [17], which performs object detection with embodied agents that are able to move around in the environment; and [16], which performs depth estimation and action prediction to evaluate contrastive learning approaches.</li>
<li>Interpretability. iSEE [4] uses probing to discover what information is in the hidden representations of Embodied AI models. It focuses on probing ObjectNav and PointNav agents to answer interpretability questions, such as how far the agent thinks it is from the target.</li>
</ul>
<p>AI2-THOR is rapidly updating to build out features and functionality. For the latest published papers, please visit the publication tracker on our website: https://ai2thor.allenai.org/publications.</p>
<h1>4 Why use AI2-THOR?</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Simulator</th>
<th style="text-align: center;">Scale</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Interaction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Simulator</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"># of <br> Scenes</td>
<td style="text-align: center;"># of Objects</td>
<td style="text-align: center;">Object <br> States</td>
<td style="text-align: center;">Arm Manipulation</td>
<td style="text-align: center;">Multi-Agent</td>
<td style="text-align: center;">Sound</td>
<td style="text-align: center;">VR</td>
<td style="text-align: center;">Engine</td>
<td style="text-align: center;">Interactive <br> Editor</td>
</tr>
<tr>
<td style="text-align: center;">AI2-THOR</td>
<td style="text-align: center;">$\infty$ [2]</td>
<td style="text-align: center;">3578</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Unity</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">iGibson 2.0</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">1217</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">PyBullet</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Habitat 1.0</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Magnum</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Habitat 2.0</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Magnum</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">ThreeDWorld</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Unity</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">SAPIEN</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2346</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">PhysX</td>
<td style="text-align: center;">$\times$</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison table between Embodied AI simulators.</p>
<p>Following AI2-THOR's first release in 2017, a number of simulators have been developed, including iGibson 2.0 [18], Habitat 1.0 [30], Habitat 2.0 [32], ThreeDWorld [7], and SAPIEN [39]. Table 1 shows a comparison table between the simulators. AI2-THOR is significantly larger in scale than other simulators, while providing first-class support for interaction, and, by leveraging Unity, makes it easy to add new capabilities.</p>
<p>Performance Benchmark. To benchmark performance, we trained an ObjectNav agent for 1 million steps on a 2-GPU machine. Here, GPU-0 stores and performs updates to the model while GPU-1 renders a batch of parallel instances of the simulator. We obtain a training FPS ranging between 145.5-179.4 (167.7 average). For comparison, we ran the same setup with Habitat 1.0 and obtained a training FPS ranging between 119.7-264.3 (230.5 average). More details are described in Appendix B.</p>
<h2>5 Conclusion</h2>
<p>We present AI2-THOR, a large-scale interactive simulation platform for Embodied AI. It has been used for experimentation in over 150 publications, spanning a wide variety of tasks and research areas. It is highly customizable, and provides first-class support for many different types of scenes, agent embodiments, actions, and metadata. The capabilities of AI2-THOR are rapidly evolving, and we are excited to support new improvements and use cases to come. For the latest information, please visit our website: https://ai2thor.allenai.org/.</p>
<h1>References</h1>
<p>[1] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, Luca Weihs, Mark Yatskar, and Ali Farhadi. Robothor: An open simulation-to-real embodied ai platform. In CVPR, 2020. 2, 3, 6, 7
[2] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi Salvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. arXiv, 2022. 2, 3, 6, 7, 8, 12
[3] Heming Du, Xin Yu, and Liang Zheng. Learning object relation graph and tentative policy for visual navigation. In ECCV, 2020. 6
[4] Kshitij Dwivedi, Gemma Roig, Aniruddha Kembhavi, and Roozbeh Mottaghi. What do navigation agents learn about their environment? In CVPR, 2022. 7, 8
[5] Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: A framework for visual object manipulation. In CVPR, 2021. 3, 8
[6] Kiana Ehsani, Roozbeh Mottaghi, and Ali Farhadi. Segan: Segmenting and generating the invisible. In CVPR, 2018. 8
[7] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threedworld: A platform for interactive multi-modal physical simulation. In Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2020. 2, 8
[8] Chuang Gan, Yiwei Zhang, Jiajun Wu, Boqing Gong, and Joshua B Tenenbaum. Look, listen, and act: Towards audiovisual embodied navigation. In ICRA, 2020. 6, 7
[9] Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, and Gaurav S Sukhatme. Dialfred: Dialogueenabled agents for embodied instruction following. IEEE Robotics and Automation Letters, 2022. 6
[10] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2018. 6
[11] Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, and Alexander G. Schwing. A cordial sync: Going beyond marginal policies for multi-agent embodied tasks. In ECCV, 2020. 6
[12] Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander G. Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task completion. In CVPR, 2019. 6, 7
[13] Siddharth Karamcheti, Dorsa Sadigh, and Percy Liang. Learning adaptive language interfaces through decomposition. arXiv, 2020. 6
[14] Charles C Kemp, Aaron Edsinger, Henry M Clever, and Blaine Matulevich. The design of stretch: A compact, lightweight mobile manipulator for indoor human environments. In ICRA, 2022. 3
[15] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective: Clip embeddings for embodied ai. In CVPR, 2022. 6
[16] Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting contrastive selfsupervised representation learning pipelines. In ICCV, 2021. 8
[17] Klemen Kotar and Roozbeh Mottaghi. Interactron: Embodied adaptive object detection. In CVPR, 2022. 7, 8
[18] Chengshu Li, Fei Xia, Roberto MartÃ­n-MartÃ­n, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Elliott Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, Karen Liu, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, and Silvio Savarese. igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. In CoRL, 2021. 2,8
[19] Qi Li, Kaichun Mo, Yanchao Yang, Hang Zhao, and Leonidas Guibas. Ifr-explore: Learning inter-object functional relationships in 3d indoor scenes. In $I C L R, 2022.7$
[20] Xinzhu Liu, Di Guo, Huaping Liu, and Fuchun Sun. Multi-agent embodied visual semantic navigation with scene prior knowledge. IEEE Robotics and Automation Letters, 2022. 6
[21] Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, and Roozbeh Mottaghi. Learning about objects by learning to interact with them. In NeurIPS, 2020. 8
[22] Yi Lu, Yaran Chen, Dongbin Zhao, and Dong Li. Mgrl: Graph neural network based inference in a markov network with reinforcement learning for visual navigation. Neurocomputing, 2021. 6
[23] So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. Film: Following instructions in language with modular methods. In $I C L R, 2022.6$
[24] Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav Gupta. Pyrobot: An open-source robotics framework for research and benchmarking. arXiv, 2019. 3
[25] Tushar Nagarajan and Kristen Grauman. Learning affordance landscapes for interaction exploration in 3d environments. In NeurIPS, 2020. 7
[26] Tushar Nagarajan and Kristen Grauman. Shaping embodied agent behavior with activity-context priors from egocentric video. In NeurIPS, 2021. 7
[27] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In AAAI, 2022. 6, 7
[28] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navigation. In ICCV, 2021. 6</p>
<p>[29] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 11
[30] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for embodied ai research. In ICCV, 2019. 8
[31] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In CVPR, 2020. 6, 7
[32] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In NeurIPS, 2021. 2, 8
[33] Sinan Tan, Weilai Xiang, Huaping Liu, Di Guo, and Fuchun Sun. Multi-agent embodied question answering in interactive environments. In ECCV, 2020. 6
[34] Luca Weihs, Matt Deitke, Aniruddha Kembhavi, and Roozbeh Mottaghi. Visual room rearrangement. In CVPR, 2021. 7,8
[35] Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, and Ali Farhadi. Learning generalizable visual representations via interactive gameplay. In $I C L R, 2021.6,8$
[36] Luca Weihs, Jordi Salvador, Klemen Kotar, Unnat Jain, Kuo-Hao Zeng, Roozbeh Mottaghi, and Aniruddha Kembhavi. Allenact: A framework for embodied AI research. arXiv, 2020. 11
[37] Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. In CVPR, 2019. 6
[38] Qi Wu, Cheng-Ju Wu, Yixin Zhu, and Jungseock Joo. Communicative learning with natural gestures for embodied navigation agents with human-in-the-scene. In IROS, 2021. 6, 7
[39] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. Sapien: A simulated part-based interactive environment. In CVPR, 2020. 8
[40] Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, and Roozbeh Mottaghi. Visual semantic navigation using scene priors. In $I C L R, 2019.6$
[41] Rowan Zellers, Ari Holtzman, Matthew E. Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. In ACL, 2021. 6
[42] Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, and Ali Farhadi. Visual reaction: Learning to play catch with your drone. In CVPR, 2020. 3
[43] Yizhou Zhao, Kaixiang Lin, Zhiwei Jia, Qiaozi Gao, Govind Thattai, Jesse Thomason, and Gaurav S Sukhatme. Luminous: Indoor scene generation for embodied ai challenges. arXiv, 2021. 8
[44] Kaiyu Zheng, Rohan Chitnis, Yoonchang Sung, George Konidaris, and Stefanie Tellex. Towards optimal correlational object search. In ICRA, 2022. 6
[45] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017. 6, 7, 11</p>
<h1>A Contributions</h1>
<p>Eric Kolve was the lead engineer and built the API that connects Python and Unity, setup the infrastructure for maintenance and development, heavily optimized AI2-THOR to run faster, added support for headless rendering, contributed to the Unity backend, and contributed to RoboTHOR, ProcTHOR, and ManipulaTHOR.</p>
<p>Roozbeh Mottaghi managed the AI2-THOR project and its constituents and made decisions about the technical and artistic features of the framework and set priorities for the team.</p>
<p>Winson Han contributed to the Unity backend logic for features and functionality across AI2-THOR; oversaw the design and functionality of the agents; led the development of logic to support physics-based object interactions, state changes, visibility, repositioning, and the annotation pipeline; set up default object placement in scenes; contributed to the documentation; managed community feature requests and issues; and created many promotional graphics.</p>
<p>Eli VanderBilt built all of the 3D scenes for iTHOR, RoboTHOR, ArchitecTHOR; created thousands of interactive assets; modeled the agents; and designed and implemented various features, including arm-based manipulation.</p>
<p>Luca Weihs contributed to the AI2-THOR frontend and backend through the creation of new actions, tests, and processes; led the development of the AllenAct framework, a library used to train agents on AI2-THOR and Embodied AI tasks [36].</p>
<p>Alvaro Herrasti developed features and infrastructure for the Unity backend and Python API; graphics and shader work; built the WebGL infrastructure and demo integration; built the continuous action physics system for arm-based agents; led the Unity development of ProcTHOR; and contributed to RoboTHOR and ManipulaTHOR.</p>
<p>Matt Deitke led the development of ProcTHOR; built the AI2-THOR website, demo, and wrote documentation; contributed to building RoboTHOR; built infrastructure to make AI2-THOR more accessible; contributed to the Unity backend and Python API; and wrote the revised paper.</p>
<p>Kiana Ehsani led the ManipulaTHOR project and the direction of adding arm-based manipulation with the StretchRE1 and ManipulaTHOR agents.</p>
<p>Daniel Gordon developed some planning and rendering features for the early versions of AI2-THOR.
Yuke Zhu created the very first version of AI2-THOR (mentioned in [45]) with the help of EK and RM.
Aniruddha Kembhavi was involved in decision making for various features of ProcTHOR, ManipulaTHOR, ArchitecTHOR, and RoboTHOR.</p>
<p>Abhinav Gupta provided advice and guidance throughout the course of the project.
Ali Farhadi provided advice and guidance throughout the course of the project.</p>
<h1>B Performance Comparison</h1>
<p>Comparing performance between Embodied AI simulators is a surprisingly difficult question for many reasons:</p>
<ol>
<li>Different simulators support different agents, each with their own action spaces and capabilities, with little standardization across simulators. AI2-THOR supports many different types of agents, including the ManipulaTHOR, Abstract, and LoCoBot agents. The ManipulaTHOR agent is often slower to simulate than a navigation-only LoCoBot agent as it is more complex to physically model a 6 DoF arm as it interacts with objects. This is made even more complex when noting that random action sampling, the simplest policy with which to benchmark, is a poor profiling strategy as some actions are only computationally expensive in rare, but important, settings; for instance, computing arm movements is most expensive when the arm is interacting with many objects, these interactions are rare when randomly sampling but we'd expect them to dominate when using a well-trained agent.</li>
<li>Some simulators are relatively slow when run on a single process but can be easily parallelized with many processes running on a single GPU, e.g. AI2-THOR. Thus single-process simulation speeds may be highly deceptive as they do not capture the ease of scalability.</li>
<li>When training agents via reinforcement learning, there are a large number of factors that bottleneck training speed and so the value of raw simulator speed is substantially reduced. These factors include:
(a) Model forward pass when computing agent rollouts.
(b) Model backward pass when computing gradients for RL losses.
(c) Environment resets - for many simulators (e.g. AI2-THOR, Habitat, iGibson) it is orders of magnitude more expensive to change a scene than it is to take a single agent step. This can be extremely problematic when using synchronous RL algorithms as all simulators will need to wait for a single simulator when that simulator is resetting. When training this means that, in practice, important "tricks" are employed to ensure that scene changes are infrequent or synchronized, without these tricks, performance may be dramatically lower.</li>
</ol>
<p>To attempt to control for the above factors, we set up two profiling experiments, one in Habitat with HM3D [29] and one using ProcTHOR-10K, where we:</p>
<ul>
<li>Use a 2-GPU machine (GeForce RTX 2080 GPUs) where GPU-0 is reserved for the agent's actor-critic policy network and GPU-1 is reserved for simulator instances.</li>
<li>Train agents for the ObjectNav task (using the same LoCoBot agent with the same action space).</li>
<li>For both agents, use the same actor-critic policy network, the same used in the ProcTHOR paper [2].</li>
<li>Remove the "End" action so that agents always take the maximum 500 steps, this minimizes dependence on the learned policy.</li>
<li>Use a rollout length of 128 with the same set of training hyperparameters across both models.</li>
<li>Use a total of 28 parallel simulator processes, this approximately saturates GPU-1 memory. We found that Habitat instances used slightly less GPU memory than ProcTHOR instances and so we could likely increase the number instances for Habitat slightly, but we kept these equal for more direct comparison.</li>
<li>Use a scene update "trick" which forces all simulators to advance to the next scene in a synchronous fashion after every 10 rollouts (e.g. after every $10 \times 128 \times 28=35,840$ total steps across all simulators).</li>
</ul>            </div>
        </div>

    </div>
</body>
</html>
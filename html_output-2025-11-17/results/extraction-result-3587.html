<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3587 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3587</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3587</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-84bef17fc1789caef9ad47115cccca4190391e86</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/84bef17fc1789caef9ad47115cccca4190391e86" target="_blank">Deep generative model for drug design from protein target sequence</a></p>
                <p><strong>Paper Venue:</strong> Journal of Cheminformatics</p>
                <p><strong>Paper TL;DR:</strong> DeepTarget, an end-to-end DL model, was proposed to generate novel molecules solely relying on the amino acid sequence of the target protein to reduce the heavy reliance on prior knowledge.</p>
                <p><strong>Paper Abstract:</strong> Drug discovery for a protein target is a laborious and costly process. Deep learning (DL) methods have been applied to drug discovery and successfully generated novel molecular structures, and they can substantially reduce development time and costs. However, most of them rely on prior knowledge, either by drawing on the structure and properties of known molecules to generate similar candidate molecules or extracting information on the binding sites of protein pockets to obtain molecules that can bind to them. In this paper, DeepTarget, an end-to-end DL model, was proposed to generate novel molecules solely relying on the amino acid sequence of the target protein to reduce the heavy reliance on prior knowledge. DeepTarget includes three modules: Amino Acid Sequence Embedding (AASE), Structural Feature Inference (SFI), and Molecule Generation (MG). AASE generates embeddings from the amino acid sequence of the target protein. SFI inferences the potential structural features of the synthesized molecule, and MG seeks to construct the eventual molecule. The validity of the generated molecules was demonstrated by a benchmark platform of molecular generation models. The interaction between the generated molecules and the target proteins was also verified on the basis of two metrics, drug–target affinity and molecular docking. The results of the experiments indicated the efficacy of the model for direct molecule generation solely conditioned on amino acid sequence.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3587.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3587.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepTarget</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepTarget (sequence-conditioned deep generative model for ligand design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end deep generative framework that generates novel SMILES molecules conditioned only on the amino-acid sequence of a protein target, combining a Transformer-based protein embedder, a WGAN-GP latent generator, supervised contrastive learning, and a pre-trained LSTM autoencoder for SMILES decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepTarget</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-module architecture: (1) AASE — 12-layer Transformer (hidden size 512, 8 attention heads) trained by masked-token prediction to produce a 756-dim protein embedding; (2) SFI — a conditional Wasserstein GAN with gradient penalty (WGAN-GP) that maps Gaussian noise + processed protein embedding to a 512-dim latent molecular feature; (3) MG — a pre-trained frozen autoencoder (encoder: 2-layer bidirectional LSTM with 512 units per layer producing a 512-dim latent vector; decoder: 4-layer LSTM trained on ChEMBL SMILES) that decodes latent features to SMILES. Training includes adversarial losses and a supervised contrastive loss to cluster molecules by target.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional latent-space generation via WGAN-GP: sample z ~ N(0,1), combine with MLP-projected protein embedding c, generate latent molecular feature S = G(z,c); decode S to SMILES using a frozen LSTM-based autoencoder decoder. Supervised contrastive learning (SCL) is used to encourage molecules for the same target to cluster in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — de novo design of small-molecule ligands conditioned on protein amino-acid sequence (targeted ligand generation for protein targets such as DRD2 and PARP1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Docking score (Schrödinger Glide; lower better), predicted drug-target affinity (DeepPurpose model; higher better), MOSES benchmarking metrics (validity, Unique@1k, Unique@10k, novelty), molecular properties distributions (QED, SAscore, LogP, MW, NP-likeness), Tanimoto similarity to training set, zero-shot generation performance (targets removed from training).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Generated molecules showed comparable or improved in-silico binding metrics vs known ligands for case studies: DRD2 mean docking score −8.8904 kcal/mol vs real −7.7532; QED 0.6402 vs 0.6016; SAscore 3.0755 vs 2.6886. For PARP1 mean docking −9.7163 vs real −9.8276 (slightly worse), QED 0.6776 vs 0.6477, SAscore 2.8217 vs 3.1649. MOSES metrics (30k samples): Valid 0.8083, Unique@1k 0.997, Unique@10k 0.9926, Novelty 0.9989. Contrastive learning improved docking scores among top-1000 molecules. Zero-shot generation (DRD2/PARP1 removed from training) still produced many molecules with docking scores below −6 kcal/mol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to several generative baselines (AAE, VAE, CharRNN, ORGAN, LatentGAN) DeepTarget produced similar property distributions and high novelty; validity slightly lower than some baselines (but higher than other GAN-based methods). Compared to a Seq2Seq (machine-translation-like) model trained on the same data, DeepTarget produced molecules with slightly better predicted affinities and docking, and it scales better to large-batch generation (no beam-search memory bottleneck).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reported limitations include: (1) GAN-based approach is harder to train and yields slightly lower validity than some non-GAN models; (2) evaluation is computational/in silico (docking and predicted affinity) without experimental binding validation; (3) docking and affinity predictors have known limitations and may not perfectly reflect true binding; (4) the autoencoder parameters are frozen — generative capacity depends on the pre-trained autoencoder's coverage of chemical space; (5) single-sequence input ignores explicit 3D pocket information which may limit capture of some structure-specific interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep generative model for drug design from protein target sequence', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3587.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3587.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq (Transformer MT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based sequence-to-sequence (machine-translation) model for protein-specific de novo drug generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence model treating protein-to-ligand generation as a machine translation problem (produces SMILES autoregressively from protein sequence), used here as a baseline comparison and retrained by the authors following prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer neural network for protein-specific de novo drug generation as a machine translation problem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seq2Seq (Transformer MT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive sequence-to-sequence Transformer model mapping protein sequence tokens to SMILES tokens (prior work). Decoding is performed with beam search (one-per-one or ten-per-one mode as discussed); beam size affects memory and validity.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct autoregressive SMILES generation (sequence-to-sequence), beam-search decoding (beam size variable).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — protein-specific de novo ligand generation (case studies DRD2 and PARP1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, uniqueness, novelty (MOSES-like metrics on generated sets), predicted affinity (DeepPurpose), molecular docking (Schrödinger Glide) on produced molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>When retrained on the same data and evaluated, Seq2Seq produced valid molecules but with lower validity than DeepTarget; in affinity/docking comparisons for DRD2 and PARP1 DeepTarget showed slightly higher predicted affinities and better docking distributions. Exact numeric comparisons are summarized in supplementary materials (authors report DeepTarget advantaged in mean and distribution for DRD2; close on PARP1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared directly in this study: Seq2Seq had similar uniqueness/novelty but lower validity and generally produced slightly worse docking/affinity scores than DeepTarget for some targets; Seq2Seq suffers from beam-search tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reported drawbacks in this paper: beam search increases memory and can reduce validity as beam size grows (ten-per-one mode worse than one-per-one), making Seq2Seq less suitable for large-batch generation; lower validity compared to some generative approaches; performance dependent on decoding strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep generative model for drug design from protein target sequence', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3587.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3587.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CharRNN (baseline LM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level recurrent neural network (CharRNN) generative model for SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A character-level RNN trained to generate SMILES sequences used here as a baseline in MOSES benchmarking to assess validity, uniqueness, and novelty of generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CharRNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive character-level recurrent neural network (RNN/LSTM) trained on SMILES to model and sample SMILES sequences. Standard approach in de novo molecule generation literature.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation via an autoregressive character-level RNN with sampling (no latent GAN); often sampled with temperature or beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — de novo SMILES generation used as baseline for molecule generation benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics reported in this paper: Validity, Unique@1k, Unique@10k, Novelty; property distributions (QED, LogP, MW, NP-likeness, SAscore) used for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MOSES benchmark numbers reported in Table 3: Valid = 0.9842, Unique@1k = 0.998, Unique@10k = 0.9563, Novelty = 0.9953. Property distributions comparable to other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>CharRNN shows very high validity relative to GAN-based models in Table 3, slightly lower Unique@10k than some models; used as a representative language-model baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Noted implicitly: autoregressive LM sampling strategies and coverage of chemical space depend on decoding/sampling; may produce high similarity to training set if not regularized; specific weaknesses not deeply discussed in this paper beyond MOSES benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep generative model for drug design from protein target sequence', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3587.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3587.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language models (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models for molecular generation (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work showing language models (transformers/RNNs) can learn complex molecular distributions and be applied to molecule generation; this is discussed as background motivating sequence-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can learn complex molecular distributions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Language models (transformers / autoregressive LMs applied to SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General class: large pre-trained language models (transformer architectures and RNNs) trained on large SMILES corpora to model molecular distributions and generate SMILES sequences; not specifically trained or deployed in new experiments in this paper beyond being cited.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES autoregressive generation (LM sampling) or fine-tuning for conditional generation; referenced as background evidence that text-generation architectures can model molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / de novo molecular generation (general statement and literature support).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not evaluated directly in this paper, but referenced metrics in literature include MOSES metrics, property distributions, validity/novelty, and task-specific metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No results produced in this paper for these prior LMs; they are cited to justify architecture choices and to note that language-model-style generators can capture complex molecular distributions (reference provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Cited as supportive prior work demonstrating feasibility of language-model approaches; authors argue GAN-based generation has greater diversity than machine-translation approaches in their experience.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes general challenges with machine-translation-style/beam-search decoding (memory/validity tradeoffs) and implies that purely sequence-to-sequence translation approaches may be less scalable for large-batch generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep generative model for drug design from protein target sequence', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can learn complex molecular distributions <em>(Rating: 2)</em></li>
                <li>Transformer neural network for protein-specific de novo drug generation as a machine translation problem <em>(Rating: 2)</em></li>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Deep learning enables rapid identification of potent DDR1 kinase inhibitors <em>(Rating: 2)</em></li>
                <li>GENTRL: Deep learning enables rapid identification of potent DDR1 kinase inhibitors <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3587",
    "paper_id": "paper-84bef17fc1789caef9ad47115cccca4190391e86",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "DeepTarget",
            "name_full": "DeepTarget (sequence-conditioned deep generative model for ligand design)",
            "brief_description": "An end-to-end deep generative framework that generates novel SMILES molecules conditioned only on the amino-acid sequence of a protein target, combining a Transformer-based protein embedder, a WGAN-GP latent generator, supervised contrastive learning, and a pre-trained LSTM autoencoder for SMILES decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepTarget",
            "model_description": "Multi-module architecture: (1) AASE — 12-layer Transformer (hidden size 512, 8 attention heads) trained by masked-token prediction to produce a 756-dim protein embedding; (2) SFI — a conditional Wasserstein GAN with gradient penalty (WGAN-GP) that maps Gaussian noise + processed protein embedding to a 512-dim latent molecular feature; (3) MG — a pre-trained frozen autoencoder (encoder: 2-layer bidirectional LSTM with 512 units per layer producing a 512-dim latent vector; decoder: 4-layer LSTM trained on ChEMBL SMILES) that decodes latent features to SMILES. Training includes adversarial losses and a supervised contrastive loss to cluster molecules by target.",
            "generation_method": "Conditional latent-space generation via WGAN-GP: sample z ~ N(0,1), combine with MLP-projected protein embedding c, generate latent molecular feature S = G(z,c); decode S to SMILES using a frozen LSTM-based autoencoder decoder. Supervised contrastive learning (SCL) is used to encourage molecules for the same target to cluster in latent space.",
            "application_domain": "Drug discovery — de novo design of small-molecule ligands conditioned on protein amino-acid sequence (targeted ligand generation for protein targets such as DRD2 and PARP1).",
            "evaluation_metrics": "Docking score (Schrödinger Glide; lower better), predicted drug-target affinity (DeepPurpose model; higher better), MOSES benchmarking metrics (validity, Unique@1k, Unique@10k, novelty), molecular properties distributions (QED, SAscore, LogP, MW, NP-likeness), Tanimoto similarity to training set, zero-shot generation performance (targets removed from training).",
            "results_summary": "Generated molecules showed comparable or improved in-silico binding metrics vs known ligands for case studies: DRD2 mean docking score −8.8904 kcal/mol vs real −7.7532; QED 0.6402 vs 0.6016; SAscore 3.0755 vs 2.6886. For PARP1 mean docking −9.7163 vs real −9.8276 (slightly worse), QED 0.6776 vs 0.6477, SAscore 2.8217 vs 3.1649. MOSES metrics (30k samples): Valid 0.8083, Unique@1k 0.997, Unique@10k 0.9926, Novelty 0.9989. Contrastive learning improved docking scores among top-1000 molecules. Zero-shot generation (DRD2/PARP1 removed from training) still produced many molecules with docking scores below −6 kcal/mol.",
            "comparison_to_baselines": "Compared to several generative baselines (AAE, VAE, CharRNN, ORGAN, LatentGAN) DeepTarget produced similar property distributions and high novelty; validity slightly lower than some baselines (but higher than other GAN-based methods). Compared to a Seq2Seq (machine-translation-like) model trained on the same data, DeepTarget produced molecules with slightly better predicted affinities and docking, and it scales better to large-batch generation (no beam-search memory bottleneck).",
            "limitations_challenges": "Reported limitations include: (1) GAN-based approach is harder to train and yields slightly lower validity than some non-GAN models; (2) evaluation is computational/in silico (docking and predicted affinity) without experimental binding validation; (3) docking and affinity predictors have known limitations and may not perfectly reflect true binding; (4) the autoencoder parameters are frozen — generative capacity depends on the pre-trained autoencoder's coverage of chemical space; (5) single-sequence input ignores explicit 3D pocket information which may limit capture of some structure-specific interactions.",
            "uuid": "e3587.0",
            "source_info": {
                "paper_title": "Deep generative model for drug design from protein target sequence",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Seq2Seq (Transformer MT)",
            "name_full": "Transformer-based sequence-to-sequence (machine-translation) model for protein-specific de novo drug generation",
            "brief_description": "A sequence-to-sequence model treating protein-to-ligand generation as a machine translation problem (produces SMILES autoregressively from protein sequence), used here as a baseline comparison and retrained by the authors following prior work.",
            "citation_title": "Transformer neural network for protein-specific de novo drug generation as a machine translation problem",
            "mention_or_use": "use",
            "model_name": "Seq2Seq (Transformer MT)",
            "model_description": "Autoregressive sequence-to-sequence Transformer model mapping protein sequence tokens to SMILES tokens (prior work). Decoding is performed with beam search (one-per-one or ten-per-one mode as discussed); beam size affects memory and validity.",
            "generation_method": "Direct autoregressive SMILES generation (sequence-to-sequence), beam-search decoding (beam size variable).",
            "application_domain": "Drug discovery — protein-specific de novo ligand generation (case studies DRD2 and PARP1).",
            "evaluation_metrics": "Validity, uniqueness, novelty (MOSES-like metrics on generated sets), predicted affinity (DeepPurpose), molecular docking (Schrödinger Glide) on produced molecules.",
            "results_summary": "When retrained on the same data and evaluated, Seq2Seq produced valid molecules but with lower validity than DeepTarget; in affinity/docking comparisons for DRD2 and PARP1 DeepTarget showed slightly higher predicted affinities and better docking distributions. Exact numeric comparisons are summarized in supplementary materials (authors report DeepTarget advantaged in mean and distribution for DRD2; close on PARP1).",
            "comparison_to_baselines": "Compared directly in this study: Seq2Seq had similar uniqueness/novelty but lower validity and generally produced slightly worse docking/affinity scores than DeepTarget for some targets; Seq2Seq suffers from beam-search tradeoffs.",
            "limitations_challenges": "Reported drawbacks in this paper: beam search increases memory and can reduce validity as beam size grows (ten-per-one mode worse than one-per-one), making Seq2Seq less suitable for large-batch generation; lower validity compared to some generative approaches; performance dependent on decoding strategy.",
            "uuid": "e3587.1",
            "source_info": {
                "paper_title": "Deep generative model for drug design from protein target sequence",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "CharRNN (baseline LM)",
            "name_full": "Character-level recurrent neural network (CharRNN) generative model for SMILES",
            "brief_description": "A character-level RNN trained to generate SMILES sequences used here as a baseline in MOSES benchmarking to assess validity, uniqueness, and novelty of generative models.",
            "citation_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "mention_or_use": "use",
            "model_name": "CharRNN",
            "model_description": "Autoregressive character-level recurrent neural network (RNN/LSTM) trained on SMILES to model and sample SMILES sequences. Standard approach in de novo molecule generation literature.",
            "generation_method": "Direct SMILES generation via an autoregressive character-level RNN with sampling (no latent GAN); often sampled with temperature or beam search.",
            "application_domain": "Drug discovery — de novo SMILES generation used as baseline for molecule generation benchmarking.",
            "evaluation_metrics": "MOSES metrics reported in this paper: Validity, Unique@1k, Unique@10k, Novelty; property distributions (QED, LogP, MW, NP-likeness, SAscore) used for comparisons.",
            "results_summary": "MOSES benchmark numbers reported in Table 3: Valid = 0.9842, Unique@1k = 0.998, Unique@10k = 0.9563, Novelty = 0.9953. Property distributions comparable to other baselines.",
            "comparison_to_baselines": "CharRNN shows very high validity relative to GAN-based models in Table 3, slightly lower Unique@10k than some models; used as a representative language-model baseline.",
            "limitations_challenges": "Noted implicitly: autoregressive LM sampling strategies and coverage of chemical space depend on decoding/sampling; may produce high similarity to training set if not regularized; specific weaknesses not deeply discussed in this paper beyond MOSES benchmarking.",
            "uuid": "e3587.2",
            "source_info": {
                "paper_title": "Deep generative model for drug design from protein target sequence",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Language models (general mention)",
            "name_full": "Language models for molecular generation (general reference)",
            "brief_description": "The paper cites prior work showing language models (transformers/RNNs) can learn complex molecular distributions and be applied to molecule generation; this is discussed as background motivating sequence-based approaches.",
            "citation_title": "Language models can learn complex molecular distributions",
            "mention_or_use": "mention",
            "model_name": "Language models (transformers / autoregressive LMs applied to SMILES)",
            "model_description": "General class: large pre-trained language models (transformer architectures and RNNs) trained on large SMILES corpora to model molecular distributions and generate SMILES sequences; not specifically trained or deployed in new experiments in this paper beyond being cited.",
            "generation_method": "Direct SMILES autoregressive generation (LM sampling) or fine-tuning for conditional generation; referenced as background evidence that text-generation architectures can model molecules.",
            "application_domain": "Drug discovery / de novo molecular generation (general statement and literature support).",
            "evaluation_metrics": "Not evaluated directly in this paper, but referenced metrics in literature include MOSES metrics, property distributions, validity/novelty, and task-specific metrics.",
            "results_summary": "No results produced in this paper for these prior LMs; they are cited to justify architecture choices and to note that language-model-style generators can capture complex molecular distributions (reference provided).",
            "comparison_to_baselines": "Cited as supportive prior work demonstrating feasibility of language-model approaches; authors argue GAN-based generation has greater diversity than machine-translation approaches in their experience.",
            "limitations_challenges": "Paper notes general challenges with machine-translation-style/beam-search decoding (memory/validity tradeoffs) and implies that purely sequence-to-sequence translation approaches may be less scalable for large-batch generation.",
            "uuid": "e3587.3",
            "source_info": {
                "paper_title": "Deep generative model for drug design from protein target sequence",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can learn complex molecular distributions",
            "rating": 2
        },
        {
            "paper_title": "Transformer neural network for protein-specific de novo drug generation as a machine translation problem",
            "rating": 2
        },
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Deep learning enables rapid identification of potent DDR1 kinase inhibitors",
            "rating": 2
        },
        {
            "paper_title": "GENTRL: Deep learning enables rapid identification of potent DDR1 kinase inhibitors",
            "rating": 1
        }
    ],
    "cost": 0.01339075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep generative model for drug design from protein target sequence</h1>
<p>Yangyang Chen ${ }^{1 <em>}$, Zixu Wang ${ }^{1}$, Lei Wang ${ }^{3}$, Jianmin Wang ${ }^{5,6}$, Pengyong Li ${ }^{4}$, Dongsheng Cao ${ }^{3 </em>}$, Xiangxiang Zeng ${ }^{2 <em>}$, Xiucai $\mathrm{Ye}^{1 </em>}$ and Tetsuya Sakurai ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Drug discovery for a protein target is a laborious and costly process. Deep learning (DL) methods have been applied to drug discovery and successfully generated novel molecular structures, and they can substantially reduce development time and costs. However, most of them rely on prior knowledge, either by drawing on the structure and properties of known molecules to generate similar candidate molecules or extracting information on the binding sites of protein pockets to obtain molecules that can bind to them. In this paper, DeepTarget, an end-to-end DL model, was proposed to generate novel molecules solely relying on the amino acid sequence of the target protein to reduce the heavy reliance on prior knowledge. DeepTarget includes three modules: Amino Acid Sequence Embedding (AASE), Structural Feature Inference (SFI), and Molecule Generation (MG). AASE generates embeddings from the amino acid sequence of the target protein. SFI inferences the potential structural features of the synthesized molecule, and MG seeks to construct the eventual molecule. The validity of the generated molecules was demonstrated by a benchmark platform of molecular generation models. The interaction between the generated molecules and the target proteins was also verified on the basis of two metrics, drug-target affinity and molecular docking. The results of the experiments indicated the efficacy of the model for direct molecule generation solely conditioned on amino acid sequence.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Introduction</h2>
<p>The chemical space of drugs is estimated to be between $10^{23}$ and $10^{60}$ power of molecules. Despite the compound library contains millions of data, easily scalable compound libraries have covered only a tiny fraction of the synthesized, drug-like chemical space [1, 2]. Obtaining ligands (representing drugs or compounds) for proteins is a critical task in drug discovery. Given a specific protein (also called target, such as a protein representing a human or virus), two standard methods could be reemployed to discover a ligand: virtual screening and de novo design. Virtual screening [3] is carried out in two manners: (1) by studying the interaction patterns between target proteins and small molecules and (2) by comparing the structures and pharmacophores of known small molecules to select from many molecules that are reasonable for subsequent testing. De novo design towards target protein directly generates molecules by exploring the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>explicit rules of known data, in contrast to virtual screening, which is cumbersome and not straightforward.</p>
<p>Molecules with novel structures are constructed by combining existing compound fragments or genetic algorithms normally in the early stages of drug design. Recently, deep neural networks (DNNs)-based models gradually make their mark in a myriad of domains. Accordingly, methods based on deep learning (DL) [4] have a wide range of applications in the field of denovo design [5, 6] and support variable molecule format, including molecule fingerprints [7], simplified molecular input line entry system (SMILES) [8], molecule graphs [9, 10], and three-dimensional (3D) structures [11]. They try to learn the probability distribution from relevant data, explore latent features, and infer new molecules from the learned data distribution. Furthermore, the generative model combined with reinforcement learning (RL) [12] could optimize the generated molecules [13, 14]. In summary, the application of deep generative networks has made it possible to take a big step forward in drug discovery.</p>
<p>Existing generation methods (Fig. 1a) are divided into ligand-based molecule generation (LBMG) and pocketbased molecule generation (PBMG). LBMG methods generate novel molecules on the basis of the backbone structure and properties of existing ligands. For example, 3DMolGNN [15] refines the molecule (produced by a pre-trained molecule generation network) as close as possible to the known inhibitors of the same family of proteins by transfer learning. It produces small molecules that better bind to the target protein and exhibit desirable drug properties. Merk et al.[16] inputs SMILES strings and fine tunes on a smaller activate drug dataset to force the models to generate focused molecule libraries with the desired activity towards the same target. GENTRL [17] utilizes a generative model combining with combined reinforcement learning (RL), variational inference, and tensor decompositions to design inhibitors of discoidin domain receptor 1 (DDR1). It is pretrained using the ZINC database and then continued with DDR1 and common kinase inhibitors. As for PBMG methods, they are conditioned on known bind site information about proteins. For example, MolAIcal [18] generates 3D structures of molecules from 3D pockets of protein targets, and it is based on the PDBbind database and docking screening by Autodock Vina [19]. Miha et al. [20] trained a generative adversarial model to generate compound structures that are complementary to protein through the obtained structures of protein--ligand complexes. Methods, such as LiGAN and GVAE_SVAE [21, 22], encode protein pockets and choose conditional VAE to generate new molecules that could bind to protein pockets. Similarly, EGCM_cRNN [23] integrates 3D structural information of protein binding pockets into conditional RNN (cRNN) models to control the generation of drug-like molecules.</p>
<p>Although previous works have made great progress in drug generation, they suffer from different problems. LBMG methods could not jump out of the existing chemical space [17, 24], making it hard to generate molecules with very novel structures. Though those methods could generate extremely high similarity results compared with reference molecule, the result they generate is a few new atoms added to the original molecule. They could not be used to search all spaces for bioactive molecules, and they do not contribute substantially to the new drug discovery. In addition, some proteins only have few known ligands. Thus, they cannot supply sufficient data to train a model. PBMG methods require advanced knowledge of the protein, such as 3D structure or pocket information. However, determining the 3D protein structure is extremely expensive, and the 3D structures of many proteins are still uncovered. While AlphaFold [25] provides a computational method that could predict protein structures, the accuracy of the forecasts could not be guaranteed, especially for proteins that have few intra-chain or homotypic contacts. In addition, some proteins have more than one protein pockets. Literature or database research, experimental screening, or software predictions could help predict the active pocket or identify the binding site of ligand, but they are also laborious. In summary, these problems could be a hindrance to ligand design.</p>
<p>In this research, considering the complexity of the previous approach, a model (Fig. 1b) named DeepTarget was proposed for generating potent ligand for protein targets, only based on protein sequences. The method does not need to concern information about protein pockets, nor does it need to be fine-tuned in a specific range of molecular libraries. The protein sequence is simply provided to obtain the corresponding molecule. Generative adversarial networks (GANs) [26, 27] were chosen as the underlying structure of the generative model for this task, combining conditional models with contrastive learning (CL)[28] to further constrain and improve the nature of the generated results. The main contributions of the present work are as follows:</p>
<p>1) The model requires neither prior knowledge of protein binders nor preparation of a library of ligands active against the target, thus allowing molecules generation based on protein amino acid sequence only.
2) An exhaustive series of experiments was carried out on the model. The model performed strong target-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h1>b Our Work</h1>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Our work (De novo generation model from target protein sequence)</p>
<h2>c Sample of Generated Molecules</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 1 a Existing De-novo Generation: Ligand-Based Molecule Generation (LBMG) and Pocket-Based Molecule Generation (PBMG). LBMG methods generate new molecules based on the backbone structure and properties of existing ligands. PBMG methods are conditioned on known bind site information about proteins. b Our model only relies on protein sequence to directly generate molecules that interact with targets. c Sample with same binding site against target protein</p>
<p>based molecule generation capabilities under various metrics.
3) We performed a molecular docking analysis of the generated results on two representative proteins (DRD2 and Parp1) and demonstrated the generated molecules can bind with the target proteins.</p>
<h2>Methods</h2>
<h2>Datasets of details</h2>
<p>Data were collated from the ChEMBL database [29]. The original dataset contains 1,417,957 drug-protein pairs, combined with unlimited protein types and a positive-tonegative sample ratio of approximately 1:1. The following actions were applied to select records from the original data:</p>
<p>1) All activity values under each target were marked to the median based on the median value, and if the activity value was greater than the median, it was marked as 1 and vice versa.
2) Data with label of 0 were removed, and data with label of 1 were chosen.
3) SMILES was processed, and invalid SMILES molecular formulas were removed using RDKit, including removing salts and stereo-chemistry from the SMILES sequence. Sequence corresponding to inorganic molecules or those that could not be interpreted by RDKit were removed.
4) Proteins with unknown token "X" were removed.
5) Compounds with uncommon atoms were removed.
6) Characters of SMILES containing the isomer information were removed, and the strings containing the following tokens were reserved.</p>
<p>$$
\begin{aligned}
&amp; {C, c, N, n, S, s, O, o, F, \mathrm{Cl},[n H], \
&amp; \quad \mathrm{Br}, 1,2,3,4,5,6,7,8,9, #, \
&amp; \quad=,+,-,(),],[ }
\end{aligned}
$$</p>
<p>7) Sequence containing the following tags in the protein target were reserved.
${K, I, F, W, Y, H, A, T, B, G, V, E, P, L, N, Q, S, U, D, R, C, M}$
After screening was conducted, the training set consists of 551,223 drug-protein pairs, with 1,970 unique proteins and 333,399 unique SMILES sequences. Among them, a fraction of the protein targets corresponds to less than 10 drug molecules that could interact with each other, while the largest number of targets has 3091 corresponding data.</p>
<h2>Model structure</h2>
<p>As shown in Fig. 2a, DeepTarget consists of three modules: AASE, SFI, and MG. Details of the modules were presented below.</p>
<h2>AASE</h2>
<p>This module is constructed by a classical Transformer [30] architecture. It incorporates a 12-layer Transformer with a hidden size of 512 units and eight attention heads. The hyperparameters of the model are approximately the same as those in the Transformer. The model was trained by masked-token prediction (Fig. 2c). First, the protein sequence was encoded by IUPAC Tokenizer [31]. Second, a part of the token was randomly masked in the input sequence (i.e., the original token is replaced with "[MASK]"), and then the output vectors in the masked position were predicted. After the above pre-training, fine-tuning was conducted in contact prediction, sequence classification, or other missions. Lastly, a 756-dimensional feature vector, which was used to represent the features of the protein sequence, could be obtained.</p>
<h2>SFI</h2>
<p>Subsequently, a condition generator was constructed by inputting protein embedding feature as condition. The Wasserstein GANs with gradient penalty (WGAN-GP) described in [32] were adopted due to their good performance in generator realistic images/text and model stability. The generation process could be expressed as follows:</p>
<p>$$
\begin{aligned}
&amp; z=N \sim(0,1) \
&amp; c=M L P(P) \
&amp; S=G(z, c)
\end{aligned}
$$</p>
<p>Initially, the protein feature $P$ embedded in AASE was converted into a 512-dimension common space $c$ by a perception layer. Then, it was concatenated with a random generated noise $z$ that conformed to normal distribution. Finally, the combined vector was inputted into the condition generator and processed by multi-layer neural networks to obtain the final result $S$, which represents the latent feature of the expected molecule.</p>
<h2>MG</h2>
<p>The latent feature generated by SFI was inputted into a decoder architecture of an autoencoder model trained in the ChEMBL dataset. LSTM was applied as the internal</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p><strong>Fig. 2 a</strong> Three modules of DeepTarget: the Amino Acid Sequence Embedding module (AASE), the structural feature inference module (SFI) and the molecule generation module (MG). <strong>b</strong> The autoencoder model, encoder is to embed SMILES to latent feature and decoder is to access SMILES from generated results. <strong>c</strong> The classical Transformer architecture that is proposed to get the features of the protein sequence. <strong>d</strong> The discriminator in SFI, which is which works against the generator</p>
<p>module in the autoencoder because of its strong capacities of reconstruction and learn molecule distribution [33, 34]. The architecture of the autoencoder [35] is illustrated in Fig. 2b. The autoencoder is able to encode and decode the molecules by pre-training on a large-scale dataset. In the model, the autoencoder's encoder is used to encode known molecules and the GANs in the SFI module learn the encoded feature distribution so that it can generate similar molecular features. Specifically, during the encoding stage, a SMILES string was encoded as one-hot format was inputted into a two-layer bidirectional network with 512 LSTM units per layer, half of which was used in the forward direction and half in the backward direction. Afterwards, all the outputs were concatenated and inputted to a 512-dimension feed-forward layer. The 512-dimension latent vector was assumed to represent the latent feature of the SMILES. During the decoding stage, the latent representation of the molecule was inputted into a four-layer LSTM decoder model that trained under teacher forcing mechanism, generally similar to the encoder. Ultimately, the output of the last network was fed to a feed-forward layer with softmax activation. Thus, the sampling probability of each word could be obtained and the corresponding SMILES could be decoded. In MG, the latent feature for SMILES
used to determine whether molecules and protein targets could interact with each other.</p>
<p>A supervised contrastive loss was further proposed to better train generators. Traditional conditional generation models tend to focus on whether a correspondence exists between the generator and the result, and they do not consider the relationship with other conditions. A notable detail is that different protein targets correspond to molecules with different distributions in chemical space. The molecules generated from different targets are expected to better capture the characteristics of the original molecule. A resurgence of CL applied to selfsupervised representation learning has occurred, achieving state-of-the-art performance in the unsupervised training of deep image models. Some works have also extended self-supervised batch comparison methods to a fully supervised setting, allowing to make effective use of labelling information. As shown in the bottom of bottom of Fig. 2a, clusters of points belonging to the same category were clustered together in the embedding space while separating clusters of samples from different categories. Inspired by [28], different protein targets could be seen as different labeling information. The following formula was utilized to calculate the supervised contrastive loss:</p>
<p>$$
\mathcal{L}<em C="C" L="L" S="S">{G</em>
$$}}=\sum_{i=1}^{N}-\frac{1}{N_{y_{i}}}-1 \sum_{j=1}^{N} 1_{i \neq j} 1_{y_{i}=y_{j}} \log \frac{\exp \left(\Phi\left(x_{i}\right) \cdot \Phi\left(x_{j}\right) / \tau\right)}{\sum_{k=1}^{N} 1_{i \neq k} \exp \left(\Phi\left(x_{i}\right) \cdot \Phi\left(x_{k}\right) / \tau\right)</p>
<p>of the molecule replaced the encoder feature, and it was processed by a trained decoder module to obtain the expected molecule. The parameters of the autoencoder model pre-trained by[36] were directly chosen and were frozen in the MG module for reconstructing. They were trained on extremely large SMILES from the ChEMBL dataset, and good performance was achieved.</p>
<h2>Objective functions</h2>
<p>The loss function is made up of two main parts, which come from the generator G and the discriminator D .</p>
<p>During training, the generator G and discriminator D were trained alternately. In detail, the generator G was trained by minimizing the following loss:</p>
<p>$$
\mathcal{L}<em 1="1">{G</em>}}=-\frac{1}{2} E_{x \sim p_{g}}[\log (D(x))]-\frac{1}{2} \mathbb{E<em g="g">{x \sim p</em>[\log (D(x, c))]
$$}</p>
<p>where $x$ is a generated latent vector sampled from the distribution $p_{g}$. The first half of the equation is the semantic realism adversarial loss that distinguishes whether the latent feature is real or fake. The second half is the mol-ecule-target pair interaction adversarial loss, which is
where $N$ is the batch size; $x$ is the latent vector of molecules; $\Phi(\cdot)$ denotes an encoder that outputs the L2 normalized final encoder hidden layer, and $y$ is the corresponding target label for each molecule.</p>
<p>The final objective function of the generator is defined as follows:</p>
<p>$$
\mathcal{L}<em G__1="G_{1">{G}=\mathcal{L}</em>
$$}}+\lambda \mathcal{L}_{S C L</p>
<p>where $\lambda$ is a loss weight to handle the importance of adversarial loss and the supervised contrastive loss.</p>
<p>The discriminator D was trained alternately to distinguish the inputs generated by the generator G or real data. Similar to the generator, the objective function of the discriminators includes semantic realism adversarial loss and molecule-target pair interaction adversarial loss, which is defined as follows:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em P__x="P_{x" _sim="\sim" x="x">{D}= &amp; -\mathbb{E}</em>}}[D(x)]+\mathbb{E<em g="g">{x \sim P</em>[D(x)] \
&amp; -\mathbb{E}}<em x="x">{x \sim P</em>}}[D(x, c)]+\mathbb{E<em g="g">{x \sim P</em>[D(x, c)] \
&amp; +\lambda \mathbb{E}}<em g="g">{x \sim \mathcal{P}</em>
\end{aligned}
$$}}\left[\nabla_{x}(D(x)+D(x, c))_{p}-1\right]^{2</p>
<p>where P_{r} is the distribution of real data. The final part of the equation is the gradient penalty mentioned in [32].</p>
<h2>Metrics</h2>
<p>The effectiveness of our model was evaluated by the following approaches. First, the affinity and docking scores [37] were calculated to measure the binding capability of molecule and proteins. DeepPurpose [38] is a DL model-based molecular modeling and prediction toolkit that takes into an array of drug's SMILES strings, an array of target protein's amino acid sequence, and an array of label. It could either be binary 0/1 indicating interaction outcome or a real number indicating affinity value. We re-trained a model by merging our own dataset with BindingDB [39] and used the model to predict the generated data. For the Drug Encodings and Target Encodings, we selected Convolutional Neural Network on both SMILES and Targets. By utilizing the pre-trained model of DeepPurpose, the affinity scores of compounds and proteins could be obtained. Typically, the higher the score is, the stronger the affinity.</p>
<p>Molecular docking is a process in which two or more molecules recognize each other through ensemble matching and energy matching. Docking score, which could be calculated by molecular docking, has shown an important value in drug design. The glide in the Schrodinger suite [40], which is a drug discovery software platform that integrates visualization, modeling, simulations, and methodology development, was used to calculate the docking score. This score can be employed to rank the candidate poses as the sum of the electrostatic and van der Waals energies, and measure how well the generated molecules fit the binding site. For the docking score, a lower value means better.</p>
<p>Second, an assessment platform called MOSES [41], which proposes a set of metrics, was applied to evaluate the quality of generative models. The proposed metrics can detect common issues, such as overfitting, imbalance of frequent structures, and mode collapse. This method randomly chooses 30,000 generated molecules and applies this set to evaluate each of the following metrics: valid, unique, novelty, and property distribution.</p>
<p>Fraction of valid (Valid) and unique (Unique@k) molecules means validity and uniqueness of the generated SMILES strings, respectively.</p>
<p>Novelty is the fraction of the generated molecules that are not present in the training set. A high value of novelty indicates not overfitting.</p>
<p>Property distribution provides a visual representation of the properties of the resulting molecular structure.</p>
<ul>
<li>Molecular weight (MW): the sum of atomic weights in a molecule. By plotting the molecular weight histogram, whether the molecules generated are heavy or light could be determined.</li>
<li>Lipophilicity (LogP): the lipophilicity of a compound is usually estimated by the LogP of the octanol/water partition, which is closely related to solubility, permeability, metabolism, and toxicity.</li>
<li>Natural product-likeness (NP-likeness): a good source of validated substructures for the design of novel bioactive molecules.</li>
<li>Synthetic accessibility score (SAscore): a heuristic estimate of how hard (10) or how easy (1) it is to synthesize a given molecule. SA score is based on a combination of the molecule's fragments contributions.</li>
<li>Quantitative estimation of drug-likeness (QED): a [0,1] value estimating how likely a molecule is a viable candidate for a drug. QED is meant to capture the abstract notion of aesthetics in medicinal chemistry.</li>
</ul>
<h2>Results</h2>
<h2>DeepTarget framework</h2>
<p>DeepTarget consists of three submodules, namely, Amino Acid Sequence Embedding (AASE), Structural Feature Inference (SFI), and Molecule Generation (MG), corresponding to target sequence embedding module, molecule latent feature inference module, and molecule generation module, respectively. In AASE, it takes the sequence of the protein as input and embeds it to obtain features that represent the protein target. SFI generated molecule feature by applying the architectures of GANs, which exerts the idea of adversarial to force the network to generate results that interact with the protein. For generative models, the sampling distribution of chemical space generated under the same protein as conditions is often highly variable. Therefore, CL is added to the model to further optimize the chemical space and better capture the characteristic similarity of molecules under similar targets. In the end, the molecule representation is decoded to a specific molecule.</p>
<h2>Affinity prediction and molecular docking of the generated molecules to the target</h2>
<p>Whether the generated molecules could bind the target protein is the most important purpose. Two binding sites (PDB ID: 6cm4 and PDB ID: 7kk4) coming from DRD2 (Uniprot ID: P14416) [42] and PARP1(Uniprot ID: P09874) [43] were selected to calculate the docking</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p><strong>Fig. 3 a</strong> The docking score result and some samples with top score of DRD2 (left) and PARP1 (right). Green is known molecules, yellow is generated. The Lower docking score indicates higher binding affinity. <strong>b</strong> Tanimoto similarity between generated molecule and each molecule in the training dataset. <strong>c</strong> The difference of top 1000 molecules' Docking Score distribution when contrastive learning is or isn't used. The left is from DRD2 and the right is from PARP1. <strong>d</strong> The docking score result of Zero-shot generation to unseen targets. <strong>e</strong> Generated molecules with top binding affinity and the reference molecule for representative binding sites. The known inhibitors (the big image) and the generated molecules (the small four images) of DRD2 (left) and PARP1 (right)</p>
<p>score, QED, and SA for each of the generated molecules. Figure 3a presents the histogram of the docking score through Schrodinger and some generated samples with high score. Tables 1 and 2 show the mean values over all generated molecules. The two examples in the above results show some top affinity molecules could achieve docking, QED, and SA scores comparable to or even higher than those of the reference molecule. The available inhibitors of the target proteins were compared with the generated molecules to understand the results further intuitively. DRD2 and PARP1 were used as examples. For the DRD2 protein binding site, the three active site residues PHE-390, TRP-386, and Asp-114 are known to interact with various highly selective DRD2 inhibitors, which are also found to interact with generated novelty molecules, reported in literature. The left of Fig. 3e illustrates the interactions between the original active molecule and four representative generated molecules with these key active site residues. In the same manner, for the PARP1 protein binding site, the new generated molecules could interact with active site residues SER-904, TYR-907, and GLY-863 (shown in the right of Fig. 3e). These results indicated that the generated molecules could interact with the binding site, same as known inhibitors. Thus, the model could potentially learn information about the interaction of the binding site with the molecule, without informing any other information (only the protein sequence).</p>
<p>Table 1 Mean values of three metrics (docking score, QED, and SAscore) on generation quality of DRD2</p>
<table>
<thead>
<tr>
<th>Metric(Mean Value)</th>
<th>This Work</th>
<th>Real</th>
</tr>
</thead>
<tbody>
<tr>
<td>Docking Score $(\mathrm{kcal} / \mathrm{mol}, \downarrow)$</td>
<td>-8.8904</td>
<td>-7.7532</td>
</tr>
<tr>
<td>QED $(\uparrow)$</td>
<td>0.6402</td>
<td>0.6016</td>
</tr>
<tr>
<td>SAscore $(\uparrow)$</td>
<td>3.0755</td>
<td>2.6886</td>
</tr>
</tbody>
</table>
<p>$(\uparrow)$ means higher is better, and $(\downarrow)$ means lower is better</p>
<p>Table 2 Mean values of three metrics (docking score, QED, and SAscore) on generation quality of Parp1</p>
<table>
<thead>
<tr>
<th>Metric(Mean Value)</th>
<th>This Work</th>
<th>Real</th>
</tr>
</thead>
<tbody>
<tr>
<td>Docking Score $(\mathrm{kcal} / \mathrm{mol}, \downarrow)$</td>
<td>-9.7163</td>
<td>-9.8276</td>
</tr>
<tr>
<td>QED $(\uparrow)$</td>
<td>0.6776</td>
<td>0.6477</td>
</tr>
<tr>
<td>SAscore $(\uparrow)$</td>
<td>2.8217</td>
<td>3.1649</td>
</tr>
</tbody>
</table>
<p>$(\uparrow)$ means higher is better, and $(\downarrow)$ means lower is better</p>
<p>Drug-target affinity prediction is a commonly used method to conduct virtual screening. By applying DeepPurpose [38], all the affinity scores about the drug-target pairs were calculated in the present study. Afterwards, a number of generated molecules was randomly selected, and the scores were calculated. Figure 4a illustrates the distribution of real data and generated data. Clearly, the overall distribution was generally consistent. In detail, the two common binding targets DRD2 and PARP1 were still selected. The results of randomly chosen samples (Fig. 4a) show that in general, our model is able to discover molecules with similar binding affinity to targets.</p>
<h2>Augmentation of docking score by CL</h2>
<p>The top 1000 molecules of the docking scores were selected from the results of the models with and without the inclusion of individual comparative learning for comparison. Figure 3c shows the histogram of DRD2 (left) and PARP1 (right). Better docking results could be obtained with the addition of CL.</p>
<h2>The novelty of the generated molecules</h2>
<p>As mentioned above, the LBMG has the problem that hard to generate molecules with novel structures. In addition, a representative generated molecule shown in Fig. 3b from DRD2 and PARP1 was chosen to calculate the Tanimoto similarity with the training data. The similarity between this molecule and the training set was not high, both being below 0.6 (DRD2) and 0.4 (PARP1), as the similar curves indicated. It means that we can generate new molecules that could interact with their targets and also have novelty structures.</p>
<h2>Quality of generative models and property distribution of generated molecules</h2>
<p>As mentioned in MOSES [41], in Table 3, the Unique@K for the first $\mathrm{K}=1,000$ and $\mathrm{K}=10,000$ valid molecules in the generated set was computed. The proposed condi-tion-based model performed similarly to other generative models with a high percentage of novelty and unique SMILES strings. However, in the case of valid molecules, it showed a slightly inferior performance but higher than other GAN models. The reason is possibly because GANs are harder to train than other models. However, generating completely valid molecules is not our ultimate goal; few molecules that do not match the rules could be screened by a simple algorithm. Compared with works that focus only on the effective molecules, this study focused on the binding power of the generated molecules with the target protein.</p>
<p>In addition, a total of 30,000 corresponding generated molecules were randomly selected to calculate QED, LogP, SAscore, NPscore, and MW. The property distribution (Fig. 4b) of all molecules was analyzed in comparison with other models. The distribution of property values produced by the proposed method was similar to that produced by other methods.</p>
<h2>Zero-shot generation to unseen targets information</h2>
<p>Zero-shot generation (no known ligand information in training). Zero-shot ligand generation is very important for real world applications since we cannot always find active off-the-shelf ligands. In this section, we erased all DRD2 and PARP1 data from training set and retrained DeepTarget. After training, DRD2 and PARP1 were input to DeepTarget to generate molecules and 10,000 generated molecules were randomly chosen from each protein for molecule docking. As shown in Fig. 3d, DeepTarget could synthesize molecules whose docking scores were below -6 in most cases for DRD2. Potentially promising ligands could still be generated despite a slightly lower performance. In addition, we got the similar results for PARP1.</p>
<h2>Comparison with model based on machine translation mechanism</h2>
<p>In this part, we made some comparisons for DeepTarget with a protein-specific de novo drug generation methods based on sequence-to-sequence model (in the following, we will refer to this as Seq2Seq). The training set for DeepTarget were re-trained in Seq2Seq and the two proteins (DRD2 and Parp1) were also applied to take the case study comparison.</p>
<p>Because of the complexity and time-consuming of the molecular docking process, we only generated 1000</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 4 a Drug-Protein affinity distributions for randomly selected drug-protein pairs in the generated molecules and for drug-protein pairs in the original dataset (the left). Drug-Protein affinity of generating molecules and known active molecules of DRD2 and Parp1 (the two on the right). b Distributions of property values for the generated compounds. Properties include, Lipophilicity (LogP), Molecular Weight (MW), Natural Products-likeness (NP-likeness), synthetic accessibility Score (SAscore) and Quantitative Estimation of Drug-likeness (QED)</p>
<p>Table 3 Valid, unique, and novelty values of compounds generated in five generation models</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Valid</th>
<th>Unique@1 k</th>
<th>Unique@10 k</th>
<th>Novelty</th>
</tr>
</thead>
<tbody>
<tr>
<td>AAE</td>
<td>0.8282</td>
<td>0.994</td>
<td>0.9662</td>
<td>0.9972</td>
</tr>
<tr>
<td>VAE</td>
<td>0.8720</td>
<td>0.992</td>
<td>0.9564</td>
<td>0.9966</td>
</tr>
<tr>
<td>CharRNN</td>
<td>0.9842</td>
<td>0.998</td>
<td>0.9563</td>
<td>0.9953</td>
</tr>
<tr>
<td>ORGAN</td>
<td>0.6481</td>
<td>0.998</td>
<td>0.9793</td>
<td>0.9990</td>
</tr>
<tr>
<td>LatentGAN</td>
<td>0.7184</td>
<td>1.0</td>
<td>0.9994</td>
<td>0.9987</td>
</tr>
<tr>
<td>DeepTarget</td>
<td>0.8083</td>
<td>0.997</td>
<td>0.9926</td>
<td>0.9989</td>
</tr>
</tbody>
</table>
<p>molecules to calculate the valid, unique and novelty separately and randomly selected 500 to dock for each protein. In [44], it used beam search to sample relative data (one-per-one mode or ten-per-one mode). Beam search is sampled according to the magnitude of the probability, which could obtain the results with the higher combination probability (especially in one-per-one mode). However, it also has obvious drawbacks, i.e., as beam size increases, memory usage increases, the valid of generation decreases. The same conclusions are also shown in</p>
<p>[44] with ten-per-one mode having bad value than one per one mode. In Additional file 1: Table S1, compared to DeepTarget, it can be observed that Seq2Seq has lower valid than the former and got close to unique and novelty. Next, datasets that erased DRD2 and PARP1 were also trained by Seq2Seq. For each protein, 500 generated molecules were selected to take affinity score prediction and molecule docking. The distributions for the predicted affinity score are illustrated in Additional file 1: Figure S1. Meanwhile, their means and standard deviation are calculated in Additional file 1: Table S2. Clearly, the molecules generated by DeepTarget show slightly higher values than Seq2Seq, while they also could get similarity distribution. As for the docking score, in Additional file 1: Table S3 and Additional file 1: Figure S2, DeepTarget and Seq2Seq could get close distributions on Parp1, while the better value is obtained from DeepTarget. In terms of DRD2, both in terms of mean and distribution charts, DeepTarget has the advantage of being visible in the flesh.</p>
<p>The comparison of these two methods indicated that both are capable of generating ligand molecules against protein sequences and our model has a greater advantage. On the one hand, in the generated results for untrained protein sequences, our model generates molecules with better interactions. On the other hand, drug design need screening suitable molecules from a large number of candidates, which means not simply generating small batches of molecules. In terms of Seq2Seq, the larger the size of the beam search, the more memory rises sharply and various performance degradation [45], which is not suitable for large data generation. Instead, this is not the case for generative models based on probabilistic sampling and it could generate different data in large batches. In conclusion, our model has better applicability.</p>
<h2>Discussion</h2>
<p>Most DL methods normally involve a known activity library of molecules or bind pockets of a particular protein to produce ligand. In other words, these methods require some prior information about the compounds that are active or complex self-information of the target protein. In general, the methods based on known compounds could not generate novel results. Even some proteins also do not have sufficient known molecules. In addition, the determination of the 3D structure of a protein is not an easy task, and it is quite costly. Conversely, the proposed method does not require in-depth knowledge of any kind of chemical descriptor of the active ligand or molecule nor does rely on information about the 3D structure of the protein. Consequently, the use of amino acid sequences as input to discover molecules could considerably simplify the initial phase of drug discovery.</p>
<p>Our study was to generate SMILES molecules simply on the basis of amino acid sequences. The method benefits from the recent progress in the image or text generation field, where this architecture shows state-of-the-art results. First, GAN is an unsupervised learning model whose most important feature is that it provides a method of adversarial training for deep networks. Such method helps better learn the distribution and relationship between data. Second, the products of GANs have greater diversity than machine translation. In addition, the introduction of CL greatly enhances the quality of the generated results, thus better closing the chemical space features of the same protein target. In addition, our methods have rapid process to generate a series of candidate molecules. It has been tested that our model could generate novel molecules at a rate of about 980/s on a single 40G NVIDIA A100 (Ubuntu 18.04, AMD EPYC 7742 64-Core Processor).</p>
<p>The above results show that the idea of generating molecules with interrelationships directly from protein target sequences has a certain plausibility. For the generated molecules, the generative effect of the model and the properties of the molecules are similar to the capabilities of the other models. For the target protein, the model could generate molecules that are similar to known active molecules and obtain novel new molecules. Importantly, the affinity scores of the generated molecules are not only related to the distribution of the known active molecules, but their interaction ability could be demonstrated by molecular docking.</p>
<h2>Supplementary Information</h2>
<p>The online version contains supplementary material available at https://doi. org/10.1186/s13321-023-00702-2.</p>
<p>Additional file 1: Figures and Tables of the comparison method.</p>
<h2>Author contributions</h2>
<p>YC deigned experiment and analyzed the results and wrote the paper. ZW implemented the algorithms and embellished papers. LW analyzed some of the results and plotted them. DC and XZ provided guidance and advised the project. XY and TS revised the paper and provided funding. All authors read and approved the final manuscript.</p>
<h2>Funding</h2>
<p>This work was supported in part by the New Energy and Industrial Technology Development Organization (NEDO), the JSPS KAKENHI Grant Number JP22K12144, and the JST Grant Number JPMJPF2017, the National Key Research and Development Program of China (2021YFF1201400), National Natural Science Foundation of China (22173118), Hunan Provincial Science Fund for Distinguished Young Scholars (2021JJ10068), the science and technology innovation Program of Hunan Province (2021RC4011).</p>
<h2>Availability of data and materials</h2>
<p>Code and data are available at an opensource GitHub repository at: https:// github.com/viko-3/TargetGAN.</p>
<h2>Declarations</h2>
<h2>Ethics approval and consent to participate</h2>
<p>Not applicable.</p>
<h2>Competing interests</h2>
<p>We declare no conflict of interest.</p>
<h2>Received: 31 July 2022 Accepted: 18 February 2023 Published online: 28 March 2023</h2>
<h2>References</h2>
<ol>
<li>DiMasi JA, Grabowski HG, Hansen RW (2016) Innovation in the pharmaceutical industry: new estimates of R\&amp;D costs. J Health Econ 47:20-33</li>
<li>Gao W et al (2020) Deep learning in protein structural modeling and design. Patterns 1(9):100142</li>
<li>Chen L et al (2022) Drug design and repurposing with a sequence-todrug paradigm. bbioRxiv 39:2314</li>
<li>LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436-444</li>
<li>Tong $X$ et al (2021) Generative models for De Novo drug design. J Med Chem 64(19):14011-14027</li>
<li>Lin E, Lin C-H, Lane H-Y (2020) Relevant applications of generative adversarial networks in drug design and discovery: molecular de novo design, dimensionality reduction, and de novo peptide and protein design. Molecules 25(14):3250</li>
<li>Kadurin A et al (2017) The cornucopia of meaningful leads: applying deep adversarial autoencoders for new molecule development in oncology. Oncotarget 8(7):10883</li>
<li>Segler MH et al (2018) Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Cent Sci 4(1):120-131</li>
<li>You J, Liu B, Ying Z, et al. Graph convolutional policy network for goaldirected molecular graph generation[J]. Advances in neural information processing systems, 2018, 31.</li>
<li>Luo, Y., K. Yan, and S. Ji. GraphDF: A discrete flow model for molecular graph generation. In International Conference on Machine Learning. 2021. PMLR.</li>
<li>Luo S et al (2021) A 3D generative model for structure-based drug design. Adv Neural Inf Proc Sys 34:6229-39</li>
<li>Kaelbling LP, Littman ML, Moore AW (1996) Reinforcement learning: a survey. J Artif Intell Res 4:237-285</li>
<li>Maziarka L et al (2020) Mol-CycleGAN: a generative model for molecular optimization. J Cheminf 12(1):1-18</li>
<li>Wang J et al (2021) Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nat Machine Intell 3(10):914-922</li>
<li>Krishnan SR et al (2021) Accelerating de novo drug design against novel proteins using deep learning. J Chem Inf Model 61(2):621-630</li>
<li>Merk D et al (2018) De novo design of bioactive small molecules by artificial intelligence. Mol Inf 37(1-2):1700153</li>
<li>Zhavoronkov A et al (2019) Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nat Biotechnol 37(9):1038-1040</li>
<li>Bai Q et al (2021) MolAICal: a soft tool for 3D drug design of protein targets by artificial intelligence and classical algorithm. Brief Bioinform 22(3):161</li>
<li>Trott O, Olson AJ (2010) AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. J Comput Chem 31(2):455-461</li>
<li>Skalic M et al (2019) From target to drug: generative modeling for the multimodal structure-based ligand design. Mol Pharm 16(10):4282-4291</li>
<li>Masuda T, Ragoza M, Koes DR. (2020) Generating 3d molecular structures conditional on a receptor binding site with deep generative models. arXiv preprint arXiv:2010.14442.</li>
<li>Ragoza, M, Masuda T, Koes DR. (2020). Learning a continuous representation of 3D molecular structures with deep generative models. arXiv preprint arXiv:2010.08687.</li>
<li>Xu M, Ran T, Chen H (2021) De novo molecule design through the molecular generative model conditioned by 3D information of protein binding sites. J Chem Inf Model 61(7):3240-3254</li>
<li>Skalic M et al (2019) Shape-based generative modeling for de novo drug design. J Chem Inf Model 59(3):1205-1214</li>
<li>Jumper J et al (2021) Highly accurate protein structure prediction with AlphaFold. Nature 596(7873):583-589</li>
<li>Goodfellow I et al. (2014). Generative adversarial nets. Advances in neural information processing systems. 27.</li>
<li>Wang J et al. (2022). De novo molecular design with deep molecular generative models for PPI inhibitors. Briefings in Bioinformatics. 23(4).</li>
<li>Gunel B et al. (2020). Supervised contrastive learning for pre-trained language model fine-tuning. arXiv preprint arXiv:2011.01403.</li>
<li>Gaulton A et al (2012) ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic Acids Res 40(D1):D1100-D1107</li>
<li>Vaswani A et al. (2017). Attention is all you need. Advances in neural information processing systems. 30</li>
<li>GDR HB, Sharon N, Australia EW (1984) Nomenclature and symbolism for amino acids and peptides. Pure Appl Chem 1984(56):595-624</li>
<li>Gulrajani, I et al. (2017). Improved training of wasserstein gans. Adv Neural Inform Proc Syst. 30</li>
<li>Dollar O et al (2021) Attention-based generative models for de novo molecular design. Chem Sci 12(24):8362-8372</li>
<li>Flam-Shepherd D, Zhu K, Aspuru-Guzik A (2022) Language models can learn complex molecular distributions. Nat Commun 13(1):3293</li>
<li>Tschannen M, Bachem O, Lucic M. (2018). Recent advances in autoen-coder-based representation learning. arXiv preprint arXiv:1812.05069.</li>
<li>Bjerrum EJ. (2017). SMILES enumeration as data augmentation for neural network modeling of molecules. arXiv preprint arXiv:1703.07076.</li>
<li>Chaudhary KK, Mishra N (2016) A review on molecular docking: novel tool for drug discovery. Databases 3(4):1029</li>
<li>Huang K et al (2020) DeepPurpose: a deep learning library for drug-target interaction prediction. Bioinformatics 36(22-23):5545-5547</li>
<li>Liu T et al (2007) BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities. Nucleic Acids Res 35(1):198-201</li>
<li>Friesner RA et al (2004) Glide: a new approach for rapid, accurate docking and scoring. 1. Method and assessment of docking accuracy. J Med Chem 47(7):1739-1749</li>
<li>Polykovskiy D et al (2020) Molecular sets (MOSES): a benchmarking platform for molecular generation models. Front Pharmacol 11:1931</li>
<li>Comings DE et al (1996) The dopamine D2 receptor (DRD2) gene: a genetic risk factor in smoking. Pharmacogenetics 6(1):73-79</li>
<li>Rouleau M et al (2010) PARP inhibition: PARP1 and beyond. Nat Rev Cancer 10(4):293-301</li>
<li>Grechishnikova D (2021) Transformer neural network for protein-specific de novo drug generation as a machine translation problem. Sci Rep 11(1):1-13</li>
<li>Freitag M, Al-Onaizan Y. (2017). Beam search strategies for neural machine translation. arXiv preprint arXiv:1702.01806.</li>
</ol>
<h2>Publisher's Note</h2>
<p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
<h2>Ready to submit your research? Choose BMC and benefit from:</h2>
<ul>
<li>fast, convenient online submission</li>
<li>thorough peer review by experienced researchers in your field</li>
<li>rapid publication on acceptance</li>
<li>support for research data, including large and complex data types</li>
<li>gold Open Access which fosters wider collaboration and increased citations</li>
<li>maximum visibility for your research: over 100M website views per year</li>
</ul>
<p>At BMC, research is always in progress.
Learn more biomedcentral.com/submissions</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Correspondence:</p>
<p>Yangyang Chen
chen.yangyang.xp@alumni.tsukuba.ac.jp
Dongsheng Cao
oriental-cds@163.com
Xiangxiang Zeng
xzeng@hnu.edu.cn
Xiucai Ye
yexiucai@cs.tsukuba.ac.jp
${ }^{1}$ Department of Computer Science, University of Tsukuba, Tsukuba 3058577, Japan
${ }^{2}$ College of Computer Science and Electronic Engineering, Hunan University, Changsha 410082, Hunan, People's Republic of China
${ }^{3}$ Xiangya School of Pharmaceutical Sciences, Central South University, Changsha 410013, Hunan, China
${ }^{4}$ School of Computer Science and Technology, Xidian University, Xian 710071, China
${ }^{5}$ The Interdisciplinary Graduate Program in Integrative Biotechnology and Translational Medicine, Yonsei University, Incheon 21983, Republic of Korea
${ }^{6}$ Bioinformatics and Molecular Design Research Center (BMDRC), Incheon 21983, Republic of Korea&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>(c) The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
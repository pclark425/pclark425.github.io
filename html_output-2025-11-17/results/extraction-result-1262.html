<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1262 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1262</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1262</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-222133157</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.02193v4.pdf" target="_blank">Mastering Atari with Discrete World Models</a></p>
                <p><strong>Paper Abstract:</strong> Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1262.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1262.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL agent that learns a separately trained latent world model with discrete (categorical) latent variables and trains an actor-critic purely from imagined trajectories in that latent space to achieve human-level performance on Atari and solve continuous control from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2 world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A latent dynamics model based on an RSSM: CNN image encoder -> representation model producing a vector of categorical stochastic latents concatenated with a deterministic recurrent state (GRU) -> transition predictor (prior) that predicts next latent from deterministic state and action; decoders predict image (transposed CNN), reward (MLP Gaussian mean), and discount (Bernoulli). Uses ELBO-style losses (image, reward, discount log-likelihoods) plus a KL term between posterior and learned prior. Categorical latents are trained with straight-through gradients; KL balancing (different effective learning rates for prior vs posterior) is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (sequential VAE / Recurrent State-Space Model with discrete latents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (55 tasks) and continuous control (humanoid from pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO components: image log-likelihood (reconstruction loss), reward log-likelihood, discount log-likelihood, and KL divergence between posterior q(z|h,x) and prior p(z|h); downstream policy performance (normalized Atari scores) used as task-level fidelity proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>World model was accurate enough to train policies purely inside it and reach human-level Atari performance at 200M frames; agent-level metrics: Gamer median 2.15, Gamer mean 11.33, Record mean 0.44, Clipped Record Mean 0.28 (Table 1). Also learns an accurate model for a humanoid from pixels that solves stand-up and walking (no numeric MSE/log-likelihood reported).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural latent model; discrete latent vectors are hypothesized to yield sparse/binary-like codes (e.g., 32 categoricals with 32 classes -> sparse flattened binary vector) which may be somewhat interpretable, but no concrete latent-to-concept alignments are proven in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>No dedicated interpretability method; insights come from ablations (categorical vs Gaussian latents), hypotheses about sparsity and multimodality, and qualitative inspection (videos/training curves).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>World model ~20M trainable parameters; actor and critic ~1M parameters each. Training to 200M environment frames on a single NVIDIA V100 GPU in under 10 days. Imagines 2500 latent trajectories in parallel on one GPU; during training the agent imagined ~468 billion compact states versus 50M real inputs (after action repeat).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More computationally efficient than MuZero (MuZero would take >2 months on a single GPU according to authors) and matches/exceeds top single-GPU model-free agents (Rainbow, IQN) using the same single-GPU budget. Imagining in compact latent space enables thousands of parallel rollouts (reported 2500) and leads to ~10,000x more imagined states than real inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>First agent to reach human-level performance on the full 55-game Atari benchmark while learning purely inside a separately trained world model; outperforms top single-GPU Rainbow and IQN at 200M frames. Solves humanoid stand-up/walking from pixels in continuous control experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High fidelity of the learned latent dynamics translated directly to strong task performance: learning policies entirely from imagined latent rollouts produced policies that transfer to the real environment and outperform model-free baselines. For continuous control, dynamics backpropagation was preferred and enabled solving sparse-reward humanoid tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete (categorical) latents improved task performance across many games compared to Gaussian latents but introduce the need for straight-through gradients (biased) vs reparameterized gradients (unbiased). Straight-through dynamics backprop gives low-variance but biased policy gradients; Reinforce provides unbiased but higher variance gradients — DreamerV2 mixes these depending on domain (Atari: Reinforce preferred; continuous control: dynamics backprop preferred). KL balancing shifts optimization toward learning an accurate prior at the cost of modifying the usual posterior regularization. Stopping reward gradients sometimes improved generalization but can hurt some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key design choices: use of categorical latent variables (vector of multiple categoricals) trained with straight-through estimator; RSSM with deterministic GRU and stochastic discrete latents; KL balancing (α=0.8 prior vs 0.2 posterior in KL update); β scale for KL (β=0.1 for Atari, β=1.0 for continuous control in general; paper suggests β tuning); image encoder CNN, image decoder transposed CNN, reward/discount MLPs; fixed world model during policy learning; actor/critic trained on imagined rollouts (H=15) with λ-returns (λ=0.95); parallel imagination (2500 trajectories); actor categorical actions for Atari, truncated normal for continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DreamerV1/PlaNet (which used Gaussian latents), DreamerV2's categorical latents and KL balancing substantially improved Atari performance. Compared to model-free algorithms (Rainbow, IQN, C51, DQN), DreamerV2 outperformed them on aggregate metrics using the same single-GPU budget. Compared to SimPLe (pixel-space video prediction), DreamerV2 learns in latent space and outperforms on the full Atari suite and is more computationally practical for full benchmark. Compared to MuZero, DreamerV2 is far less compute-intensive (single GPU, ~10 days vs >2 months) while remaining simpler (no MCTS) though MuZero's planning components are complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends (for Atari) categorical latents, KL balancing, β in {0.1,0.3,1,3} search, actor entropy scale η in {3e-5,1e-4,3e-4,1e-3}, discount γ in {0.99,0.999}. For Atari use Reinforce gradients (ρ=1) and η=1e-3; for continuous control use dynamics backprop (ρ=0), η≈1e-4 or smaller and β increased (authors used β=2 in humanoid experiments). Parallel latent imagination (thousands of trajectories) and fixing model during behavior learning are recommended design principles.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1262.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1262.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential latent dynamics model combining deterministic recurrent states (GRU) and stochastic latent variables to model temporal dynamics for planning and imagination in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deterministic recurrent model h_t = f(h_{t-1}, z_{t-1}, a_{t-1}) (GRU) plus a stochastic latent z_t with posterior q(z_t|h_t,x_t) and prior p(z_t|h_t); model state is concatenation of deterministic state and stochastic sample; decoders predict image, reward, discount.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (sequential VAE / recurrent state-space)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and continuous control from pixels (used within DreamerV2)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO-style losses: image reconstruction log-likelihood, reward and discount log-likelihoods, KL between posterior and prior.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Enables accurate latent predictions sufficient for learning high-performing policies in Atari and continuous control; no standalone MSE/accuracy numbers reported for RSSM beyond overall agent performance when used in DreamerV2.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Deterministic and stochastic components separate dynamics estimation and observation assimilation but model remains a neural black box; discrete latents may afford sparse encodings but no explicit interpretability mapping is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Ablation studies (categorical vs Gaussian latents, stopping image/reward gradients) used to probe role of components; no explicit latent visualization methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>RSSM is part of the 20M-parameter world model; using latent predictions allows thousands of parallel rollouts (e.g., 2500 imagined trajectories) and reduces the need to generate pixel predictions during imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Predicting in compact RSSM latent space is much more efficient than pixel-space prediction (enables huge numbers of parallel rollouts and large multipliers of imagined states compared to real frames).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used in DreamerV2 with categorical latents and KL balancing, supports policies that achieve human-level Atari performance and solve humanoid tasks from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM's design (separating deterministic recurrence and stochastic latent variables) supports long-horizon prediction and efficient imagination; fidelity in latent predictions is directly leveraged by policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Choosing discrete vs Gaussian stochastic latents changed optimization behavior and task performance; RSSM requires careful KL balancing to encourage an accurate learned prior for imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of GRU for deterministic state, explicit posterior conditioned on image, learned prior conditioned on deterministic state, separate predictors for image/reward/discount. Option to use categorical or Gaussian stochastic latents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>RSSM in latent space is more efficient than pixel-based simulators (SimPLe) and avoids costs of tree search (MuZero's MCTS), while providing structured stochastic dynamics for long-horizon imagination unlike simpler deterministic predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper shows RSSM benefits from categorical latents and KL balancing; using β scaling, KL balancing α=0.8, and stopping or allowing certain gradient flows (image vs reward) can improve generalization/performance on different tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1262.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1262.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preceding latent dynamics planner that used a similar RSSM formulation but with Gaussian latent variables; DreamerV2 builds on PlaNet's world model and replaces Gaussian latents with categorical latents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning Latent Dynamics for Planning from Pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent dynamics model (RSSM-like) with deterministic recurrent state and Gaussian stochastic latents, trained to predict future observations and rewards; used for planning in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (sequential VAE / RSSM with Gaussian latents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based control from pixels (continuous control benchmarks and earlier tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO-style losses (image and reward log-likelihoods, KL); used for planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Prior work demonstrated effective planning on continuous control tasks; DreamerV2 reports that replacing PlaNet's Gaussian latents with categorical latents improves Atari performance, implying PlaNet's fidelity was insufficient for full Atari benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Gaussian latents are continuous and generally not directly interpretable; no specific interpretability methods described here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described in this paper beyond comparison ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>PlaNet used latent-space planning to reduce costs vs pixel predictors; exact compute not stated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>PlaNet showed feasibility of latent planning; DreamerV2 claims improvements over PlaNet by changing latent type and KL treatment to improve task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>PlaNet was successful on many continuous control tasks; DreamerV2 leverages its architecture and extends it to full Atari success with discrete latents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PlaNet-style latent models enable efficient planning and imagination; DreamerV2 demonstrates that latent design (categorical vs Gaussian) can substantially affect downstream utility on harder tasks like Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Gaussian latents with reparameterization gradients vs categorical latents with straight-through gradients show different optimization properties: Gaussians may struggle to represent multimodal transitions, categoricals may encourage sparsity and better fit aggregate posterior mixtures.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>PlaNet used Gaussian stochastic latents and reparameterization gradients; DreamerV2 changed to categorical latents and straight-through gradients to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PlaNet (Gaussian) vs DreamerV2 (categorical) — DreamerV2 reports categorical latents outperform Gaussian on the majority of Atari tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper's ablations recommend categorical latents and KL balancing over PlaNet's original Gaussian latent setup for Atari-scale tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1262.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1262.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPLe (video-prediction model-based agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL method that trains a pixel-space video prediction model and uses it to train a policy (PPO) from generated frames; evaluated on subsets of Atari with smaller budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-Based Reinforcement Learning for Atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe pixel-space video prediction model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pixel-level video prediction model trained to predict next frames and used to generate synthetic experience for a model-free policy learner (PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pixel-space world model / video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (subset of 36 games in original evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel prediction quality (video prediction loss) and downstream policy performance at limited data budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>SimPLe was evaluated on fewer games and for far fewer environment steps (e.g., 400k and 2M steps) and was not competitive with full-budget benchmarks; authors note additional training did not further increase performance in original study.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Pixel-space predictors produce explicit frame predictions (interpretable as images), but this is expensive; no deeper interpretability of latent structure reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of predicted frames is available since model outputs pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Pixel-space prediction is computationally expensive per rollout and limits number of parallel rollouts compared to compact latent models.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient for large-scale training on full 55-game Atari benchmark compared to latent-space models like DreamerV2; DreamerV2 argues latent prediction enables thousands of parallel rollouts and better scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>SimPLe was not evaluated across the full 55-game Atari benchmark at the same budget and did not reach competitive performance at large scale according to the DreamerV2 authors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Pixel-space models provide detailed visual fidelity but at high compute cost and limited scalability; DreamerV2 alleges latent models better trade visual fidelity for efficient large-scale imagination useful for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel fidelity is high but computationally costly and prone to compounding errors when used for long imagined rollouts; latent-space models trade full image fidelity for more stable, efficient long-horizon predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>SimPLe chooses pixel reconstruction/prediction as primary objective and trains a policy on generated frames (PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DreamerV2 (latent-space, categorical latents) scales to full Atari benchmark and single-GPU training while SimPLe (pixel-space) was limited to fewer games and lower data budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests latent-space prediction (as in DreamerV2) is preferable for scaling and efficient imagination when target is to train policies for many games with limited compute.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1262.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1262.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (planning with a learned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning agent that learns a model of rewards and values (and implicitly dynamics) and uses Monte-Carlo Tree Search (MCTS) for planning, achieving strong results on deterministic/board-game-like domains but with high computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero learned model + MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned model that predicts rewards, values, and policy priors used inside MCTS; the model is optimized with value/return objectives rather than direct image-reconstruction objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>task-specific learned model used for planning (hybrid model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games and selected Atari games (deterministic settings)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Planning performance via MCTS plus accuracy of predicted rewards/values; not pixel reconstruction-based.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>MuZero achieves impressive performance on board games and some deterministic Atari tasks but requires extensive engineering and a vast computational budget; authors state training MuZero on Atari would require >2 months on a single GPU and implementation was not publicly available at the time.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model trained for task-specific value/policy prediction; not explicitly interpretable; MCTS planning traces are interpretable as search trees but underlying model remains neural.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>MCTS traces provide some transparency into planning decisions, but no explicit interpretability methods described here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Very high: authors of DreamerV2 estimate >2 months of single-GPU compute to train MuZero for Atari; MCTS itself is costly and hard to parallelize.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MuZero is computationally far more expensive than DreamerV2 but achieves high performance in domains where such budget is available; DreamerV2 is presented as a computationally efficient alternative achieving competitive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High performance in board games and some Atari games where heavy planning helps; not benchmarked directly in DreamerV2 due to resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's task-specific learned model + MCTS can produce very strong policies, but cost and complexity limit accessibility and scalability; DreamerV2 proposes that accurate learned latent models can achieve similar outcomes with far less compute and engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MuZero trades large compute and complex planning (MCTS) for task performance; DreamerV2 trades off pixel fidelity and per-step planning for efficient large-scale imagination in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of MCTS with a model trained on value/policy/reward predictions rather than image reconstruction; emphasis on planning through explicit search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DreamerV2, MuZero uses task-specific learned dynamics/values and MCTS yielding strong performance at far greater compute; DreamerV2 offers a simpler, more reproducible approach leveraging image-based representation learning and latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper notes MuZero's planning components are complementary to DreamerV2's world model; no specific optimal hyperparameters for MuZero are provided here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1262.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1262.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An influential early approach that learned a generative model of visual observations and used it for control; cited as related work on learning latent dynamics from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pixel generative model (VAE / RNN-based) that learns compact representations of observations which can be used for training policies in latent space; early demonstration of learning controllers from learned generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / generative pixel model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Simple control and simulated environments (vision-based RL tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel reconstruction/prediction losses and downstream policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Demonstrated feasibility of training control policies from learned generative world models on simpler tasks; not quantitatively compared to DreamerV2 within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generative pixel outputs are interpretable visually; latent representations not guaranteed to correspond to explicit semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of generated frames and latent sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Pixel-space generative models can be computationally heavy for long rollouts; DreamerV2 argues latent-space prediction is more efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>World Models demonstrated key ideas but did not scale to competitive Atari benchmarks; DreamerV2 scales these ideas to larger benchmarks using architectural and optimization changes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Effective in early demonstrations on simpler simulated environments; not comparable to DreamerV2's Atari results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows the utility of learning compact representations for control; DreamerV2 builds on this concept but adds discrete latents, KL balancing, and more scalable imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel-level generative models give visual fidelity but can be costly for planning; latent-space variants trade some visual fidelity for efficiency and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of generative VAE/RNN combination to produce latent representations and predicted frames for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Serves as conceptual predecessor to latent-imagination approaches like PlaNet and Dreamer; less scalable without the architectural and optimization refinements introduced in later work.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper; cited as inspiration for latent imagination ideas.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to Control: Learning Behaviors by Latent Imagination <em>(Rating: 2)</em></li>
                <li>Learning Latent Dynamics for Planning from Pixels <em>(Rating: 2)</em></li>
                <li>Model-Based Reinforcement Learning for Atari <em>(Rating: 2)</em></li>
                <li>World Models <em>(Rating: 2)</em></li>
                <li>Stochastic Variational Video Prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1262",
    "paper_id": "paper-222133157",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (this paper)",
            "brief_description": "A model-based RL agent that learns a separately trained latent world model with discrete (categorical) latent variables and trains an actor-critic purely from imagined trajectories in that latent space to achieve human-level performance on Atari and solve continuous control from pixels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DreamerV2 world model",
            "model_description": "A latent dynamics model based on an RSSM: CNN image encoder -&gt; representation model producing a vector of categorical stochastic latents concatenated with a deterministic recurrent state (GRU) -&gt; transition predictor (prior) that predicts next latent from deterministic state and action; decoders predict image (transposed CNN), reward (MLP Gaussian mean), and discount (Bernoulli). Uses ELBO-style losses (image, reward, discount log-likelihoods) plus a KL term between posterior and learned prior. Categorical latents are trained with straight-through gradients; KL balancing (different effective learning rates for prior vs posterior) is applied.",
            "model_type": "latent world model (sequential VAE / Recurrent State-Space Model with discrete latents)",
            "task_domain": "Atari games (55 tasks) and continuous control (humanoid from pixels)",
            "fidelity_metric": "ELBO components: image log-likelihood (reconstruction loss), reward log-likelihood, discount log-likelihood, and KL divergence between posterior q(z|h,x) and prior p(z|h); downstream policy performance (normalized Atari scores) used as task-level fidelity proxy.",
            "fidelity_performance": "World model was accurate enough to train policies purely inside it and reach human-level Atari performance at 200M frames; agent-level metrics: Gamer median 2.15, Gamer mean 11.33, Record mean 0.44, Clipped Record Mean 0.28 (Table 1). Also learns an accurate model for a humanoid from pixels that solves stand-up and walking (no numeric MSE/log-likelihood reported).",
            "interpretability_assessment": "Primarily a black-box neural latent model; discrete latent vectors are hypothesized to yield sparse/binary-like codes (e.g., 32 categoricals with 32 classes -&gt; sparse flattened binary vector) which may be somewhat interpretable, but no concrete latent-to-concept alignments are proven in the paper.",
            "interpretability_method": "No dedicated interpretability method; insights come from ablations (categorical vs Gaussian latents), hypotheses about sparsity and multimodality, and qualitative inspection (videos/training curves).",
            "computational_cost": "World model ~20M trainable parameters; actor and critic ~1M parameters each. Training to 200M environment frames on a single NVIDIA V100 GPU in under 10 days. Imagines 2500 latent trajectories in parallel on one GPU; during training the agent imagined ~468 billion compact states versus 50M real inputs (after action repeat).",
            "efficiency_comparison": "More computationally efficient than MuZero (MuZero would take &gt;2 months on a single GPU according to authors) and matches/exceeds top single-GPU model-free agents (Rainbow, IQN) using the same single-GPU budget. Imagining in compact latent space enables thousands of parallel rollouts (reported 2500) and leads to ~10,000x more imagined states than real inputs.",
            "task_performance": "First agent to reach human-level performance on the full 55-game Atari benchmark while learning purely inside a separately trained world model; outperforms top single-GPU Rainbow and IQN at 200M frames. Solves humanoid stand-up/walking from pixels in continuous control experiments.",
            "task_utility_analysis": "High fidelity of the learned latent dynamics translated directly to strong task performance: learning policies entirely from imagined latent rollouts produced policies that transfer to the real environment and outperform model-free baselines. For continuous control, dynamics backpropagation was preferred and enabled solving sparse-reward humanoid tasks.",
            "tradeoffs_observed": "Discrete (categorical) latents improved task performance across many games compared to Gaussian latents but introduce the need for straight-through gradients (biased) vs reparameterized gradients (unbiased). Straight-through dynamics backprop gives low-variance but biased policy gradients; Reinforce provides unbiased but higher variance gradients — DreamerV2 mixes these depending on domain (Atari: Reinforce preferred; continuous control: dynamics backprop preferred). KL balancing shifts optimization toward learning an accurate prior at the cost of modifying the usual posterior regularization. Stopping reward gradients sometimes improved generalization but can hurt some tasks.",
            "design_choices": "Key design choices: use of categorical latent variables (vector of multiple categoricals) trained with straight-through estimator; RSSM with deterministic GRU and stochastic discrete latents; KL balancing (α=0.8 prior vs 0.2 posterior in KL update); β scale for KL (β=0.1 for Atari, β=1.0 for continuous control in general; paper suggests β tuning); image encoder CNN, image decoder transposed CNN, reward/discount MLPs; fixed world model during policy learning; actor/critic trained on imagined rollouts (H=15) with λ-returns (λ=0.95); parallel imagination (2500 trajectories); actor categorical actions for Atari, truncated normal for continuous actions.",
            "comparison_to_alternatives": "Compared to DreamerV1/PlaNet (which used Gaussian latents), DreamerV2's categorical latents and KL balancing substantially improved Atari performance. Compared to model-free algorithms (Rainbow, IQN, C51, DQN), DreamerV2 outperformed them on aggregate metrics using the same single-GPU budget. Compared to SimPLe (pixel-space video prediction), DreamerV2 learns in latent space and outperforms on the full Atari suite and is more computationally practical for full benchmark. Compared to MuZero, DreamerV2 is far less compute-intensive (single GPU, ~10 days vs &gt;2 months) while remaining simpler (no MCTS) though MuZero's planning components are complementary.",
            "optimal_configuration": "Paper recommends (for Atari) categorical latents, KL balancing, β in {0.1,0.3,1,3} search, actor entropy scale η in {3e-5,1e-4,3e-4,1e-3}, discount γ in {0.99,0.999}. For Atari use Reinforce gradients (ρ=1) and η=1e-3; for continuous control use dynamics backprop (ρ=0), η≈1e-4 or smaller and β increased (authors used β=2 in humanoid experiments). Parallel latent imagination (thousands of trajectories) and fixing model during behavior learning are recommended design principles.",
            "uuid": "e1262.0"
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State-Space Model (RSSM)",
            "brief_description": "A sequential latent dynamics model combining deterministic recurrent states (GRU) and stochastic latent variables to model temporal dynamics for planning and imagination in latent space.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Recurrent State-Space Model (RSSM)",
            "model_description": "Deterministic recurrent model h_t = f(h_{t-1}, z_{t-1}, a_{t-1}) (GRU) plus a stochastic latent z_t with posterior q(z_t|h_t,x_t) and prior p(z_t|h_t); model state is concatenation of deterministic state and stochastic sample; decoders predict image, reward, discount.",
            "model_type": "latent world model (sequential VAE / recurrent state-space)",
            "task_domain": "Atari games and continuous control from pixels (used within DreamerV2)",
            "fidelity_metric": "ELBO-style losses: image reconstruction log-likelihood, reward and discount log-likelihoods, KL between posterior and prior.",
            "fidelity_performance": "Enables accurate latent predictions sufficient for learning high-performing policies in Atari and continuous control; no standalone MSE/accuracy numbers reported for RSSM beyond overall agent performance when used in DreamerV2.",
            "interpretability_assessment": "Deterministic and stochastic components separate dynamics estimation and observation assimilation but model remains a neural black box; discrete latents may afford sparse encodings but no explicit interpretability mapping is provided.",
            "interpretability_method": "Ablation studies (categorical vs Gaussian latents, stopping image/reward gradients) used to probe role of components; no explicit latent visualization methods reported.",
            "computational_cost": "RSSM is part of the 20M-parameter world model; using latent predictions allows thousands of parallel rollouts (e.g., 2500 imagined trajectories) and reduces the need to generate pixel predictions during imagination.",
            "efficiency_comparison": "Predicting in compact RSSM latent space is much more efficient than pixel-space prediction (enables huge numbers of parallel rollouts and large multipliers of imagined states compared to real frames).",
            "task_performance": "When used in DreamerV2 with categorical latents and KL balancing, supports policies that achieve human-level Atari performance and solve humanoid tasks from pixels.",
            "task_utility_analysis": "RSSM's design (separating deterministic recurrence and stochastic latent variables) supports long-horizon prediction and efficient imagination; fidelity in latent predictions is directly leveraged by policy learning.",
            "tradeoffs_observed": "Choosing discrete vs Gaussian stochastic latents changed optimization behavior and task performance; RSSM requires careful KL balancing to encourage an accurate learned prior for imagination.",
            "design_choices": "Use of GRU for deterministic state, explicit posterior conditioned on image, learned prior conditioned on deterministic state, separate predictors for image/reward/discount. Option to use categorical or Gaussian stochastic latents.",
            "comparison_to_alternatives": "RSSM in latent space is more efficient than pixel-based simulators (SimPLe) and avoids costs of tree search (MuZero's MCTS), while providing structured stochastic dynamics for long-horizon imagination unlike simpler deterministic predictors.",
            "optimal_configuration": "Paper shows RSSM benefits from categorical latents and KL balancing; using β scaling, KL balancing α=0.8, and stopping or allowing certain gradient flows (image vs reward) can improve generalization/performance on different tasks.",
            "uuid": "e1262.1"
        },
        {
            "name_short": "PlaNet",
            "name_full": "PlaNet (prior work)",
            "brief_description": "A preceding latent dynamics planner that used a similar RSSM formulation but with Gaussian latent variables; DreamerV2 builds on PlaNet's world model and replaces Gaussian latents with categorical latents.",
            "citation_title": "Learning Latent Dynamics for Planning from Pixels",
            "mention_or_use": "mention",
            "model_name": "PlaNet world model",
            "model_description": "Latent dynamics model (RSSM-like) with deterministic recurrent state and Gaussian stochastic latents, trained to predict future observations and rewards; used for planning in latent space.",
            "model_type": "latent world model (sequential VAE / RSSM with Gaussian latents)",
            "task_domain": "Model-based control from pixels (continuous control benchmarks and earlier tasks)",
            "fidelity_metric": "ELBO-style losses (image and reward log-likelihoods, KL); used for planning performance.",
            "fidelity_performance": "Prior work demonstrated effective planning on continuous control tasks; DreamerV2 reports that replacing PlaNet's Gaussian latents with categorical latents improves Atari performance, implying PlaNet's fidelity was insufficient for full Atari benchmark.",
            "interpretability_assessment": "Gaussian latents are continuous and generally not directly interpretable; no specific interpretability methods described here.",
            "interpretability_method": "Not described in this paper beyond comparison ablations.",
            "computational_cost": "PlaNet used latent-space planning to reduce costs vs pixel predictors; exact compute not stated in this paper.",
            "efficiency_comparison": "PlaNet showed feasibility of latent planning; DreamerV2 claims improvements over PlaNet by changing latent type and KL treatment to improve task performance.",
            "task_performance": "PlaNet was successful on many continuous control tasks; DreamerV2 leverages its architecture and extends it to full Atari success with discrete latents.",
            "task_utility_analysis": "PlaNet-style latent models enable efficient planning and imagination; DreamerV2 demonstrates that latent design (categorical vs Gaussian) can substantially affect downstream utility on harder tasks like Atari.",
            "tradeoffs_observed": "Gaussian latents with reparameterization gradients vs categorical latents with straight-through gradients show different optimization properties: Gaussians may struggle to represent multimodal transitions, categoricals may encourage sparsity and better fit aggregate posterior mixtures.",
            "design_choices": "PlaNet used Gaussian stochastic latents and reparameterization gradients; DreamerV2 changed to categorical latents and straight-through gradients to improve performance.",
            "comparison_to_alternatives": "PlaNet (Gaussian) vs DreamerV2 (categorical) — DreamerV2 reports categorical latents outperform Gaussian on the majority of Atari tasks.",
            "optimal_configuration": "Paper's ablations recommend categorical latents and KL balancing over PlaNet's original Gaussian latent setup for Atari-scale tasks.",
            "uuid": "e1262.2"
        },
        {
            "name_short": "SimPLe",
            "name_full": "SimPLe (video-prediction model-based agent)",
            "brief_description": "A model-based RL method that trains a pixel-space video prediction model and uses it to train a policy (PPO) from generated frames; evaluated on subsets of Atari with smaller budgets.",
            "citation_title": "Model-Based Reinforcement Learning for Atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe pixel-space video prediction model",
            "model_description": "Pixel-level video prediction model trained to predict next frames and used to generate synthetic experience for a model-free policy learner (PPO).",
            "model_type": "pixel-space world model / video prediction",
            "task_domain": "Atari games (subset of 36 games in original evaluation)",
            "fidelity_metric": "Pixel prediction quality (video prediction loss) and downstream policy performance at limited data budgets.",
            "fidelity_performance": "SimPLe was evaluated on fewer games and for far fewer environment steps (e.g., 400k and 2M steps) and was not competitive with full-budget benchmarks; authors note additional training did not further increase performance in original study.",
            "interpretability_assessment": "Pixel-space predictors produce explicit frame predictions (interpretable as images), but this is expensive; no deeper interpretability of latent structure reported here.",
            "interpretability_method": "Visual inspection of predicted frames is available since model outputs pixels.",
            "computational_cost": "Pixel-space prediction is computationally expensive per rollout and limits number of parallel rollouts compared to compact latent models.",
            "efficiency_comparison": "Less efficient for large-scale training on full 55-game Atari benchmark compared to latent-space models like DreamerV2; DreamerV2 argues latent prediction enables thousands of parallel rollouts and better scaling.",
            "task_performance": "SimPLe was not evaluated across the full 55-game Atari benchmark at the same budget and did not reach competitive performance at large scale according to the DreamerV2 authors.",
            "task_utility_analysis": "Pixel-space models provide detailed visual fidelity but at high compute cost and limited scalability; DreamerV2 alleges latent models better trade visual fidelity for efficient large-scale imagination useful for policy learning.",
            "tradeoffs_observed": "Pixel fidelity is high but computationally costly and prone to compounding errors when used for long imagined rollouts; latent-space models trade full image fidelity for more stable, efficient long-horizon predictions.",
            "design_choices": "SimPLe chooses pixel reconstruction/prediction as primary objective and trains a policy on generated frames (PPO).",
            "comparison_to_alternatives": "DreamerV2 (latent-space, categorical latents) scales to full Atari benchmark and single-GPU training while SimPLe (pixel-space) was limited to fewer games and lower data budgets.",
            "optimal_configuration": "Paper suggests latent-space prediction (as in DreamerV2) is preferable for scaling and efficient imagination when target is to train policies for many games with limited compute.",
            "uuid": "e1262.3"
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero (planning with a learned model)",
            "brief_description": "A planning agent that learns a model of rewards and values (and implicitly dynamics) and uses Monte-Carlo Tree Search (MCTS) for planning, achieving strong results on deterministic/board-game-like domains but with high computational cost.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MuZero learned model + MCTS",
            "model_description": "A learned model that predicts rewards, values, and policy priors used inside MCTS; the model is optimized with value/return objectives rather than direct image-reconstruction objectives.",
            "model_type": "task-specific learned model used for planning (hybrid model)",
            "task_domain": "Board games and selected Atari games (deterministic settings)",
            "fidelity_metric": "Planning performance via MCTS plus accuracy of predicted rewards/values; not pixel reconstruction-based.",
            "fidelity_performance": "MuZero achieves impressive performance on board games and some deterministic Atari tasks but requires extensive engineering and a vast computational budget; authors state training MuZero on Atari would require &gt;2 months on a single GPU and implementation was not publicly available at the time.",
            "interpretability_assessment": "Model trained for task-specific value/policy prediction; not explicitly interpretable; MCTS planning traces are interpretable as search trees but underlying model remains neural.",
            "interpretability_method": "MCTS traces provide some transparency into planning decisions, but no explicit interpretability methods described here.",
            "computational_cost": "Very high: authors of DreamerV2 estimate &gt;2 months of single-GPU compute to train MuZero for Atari; MCTS itself is costly and hard to parallelize.",
            "efficiency_comparison": "MuZero is computationally far more expensive than DreamerV2 but achieves high performance in domains where such budget is available; DreamerV2 is presented as a computationally efficient alternative achieving competitive performance.",
            "task_performance": "High performance in board games and some Atari games where heavy planning helps; not benchmarked directly in DreamerV2 due to resource constraints.",
            "task_utility_analysis": "MuZero's task-specific learned model + MCTS can produce very strong policies, but cost and complexity limit accessibility and scalability; DreamerV2 proposes that accurate learned latent models can achieve similar outcomes with far less compute and engineering.",
            "tradeoffs_observed": "MuZero trades large compute and complex planning (MCTS) for task performance; DreamerV2 trades off pixel fidelity and per-step planning for efficient large-scale imagination in latent space.",
            "design_choices": "Use of MCTS with a model trained on value/policy/reward predictions rather than image reconstruction; emphasis on planning through explicit search.",
            "comparison_to_alternatives": "Compared to DreamerV2, MuZero uses task-specific learned dynamics/values and MCTS yielding strong performance at far greater compute; DreamerV2 offers a simpler, more reproducible approach leveraging image-based representation learning and latent imagination.",
            "optimal_configuration": "Paper notes MuZero's planning components are complementary to DreamerV2's world model; no specific optimal hyperparameters for MuZero are provided here.",
            "uuid": "e1262.4"
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models",
            "brief_description": "An influential early approach that learned a generative model of visual observations and used it for control; cited as related work on learning latent dynamics from pixels.",
            "citation_title": "World Models",
            "mention_or_use": "mention",
            "model_name": "World Models (Ha & Schmidhuber)",
            "model_description": "Pixel generative model (VAE / RNN-based) that learns compact representations of observations which can be used for training policies in latent space; early demonstration of learning controllers from learned generative models.",
            "model_type": "latent world model / generative pixel model",
            "task_domain": "Simple control and simulated environments (vision-based RL tasks)",
            "fidelity_metric": "Pixel reconstruction/prediction losses and downstream policy performance.",
            "fidelity_performance": "Demonstrated feasibility of training control policies from learned generative world models on simpler tasks; not quantitatively compared to DreamerV2 within this paper.",
            "interpretability_assessment": "Generative pixel outputs are interpretable visually; latent representations not guaranteed to correspond to explicit semantics.",
            "interpretability_method": "Visual inspection of generated frames and latent sampling.",
            "computational_cost": "Pixel-space generative models can be computationally heavy for long rollouts; DreamerV2 argues latent-space prediction is more efficient.",
            "efficiency_comparison": "World Models demonstrated key ideas but did not scale to competitive Atari benchmarks; DreamerV2 scales these ideas to larger benchmarks using architectural and optimization changes.",
            "task_performance": "Effective in early demonstrations on simpler simulated environments; not comparable to DreamerV2's Atari results.",
            "task_utility_analysis": "Shows the utility of learning compact representations for control; DreamerV2 builds on this concept but adds discrete latents, KL balancing, and more scalable imagination.",
            "tradeoffs_observed": "Pixel-level generative models give visual fidelity but can be costly for planning; latent-space variants trade some visual fidelity for efficiency and scalability.",
            "design_choices": "Use of generative VAE/RNN combination to produce latent representations and predicted frames for policy learning.",
            "comparison_to_alternatives": "Serves as conceptual predecessor to latent-imagination approaches like PlaNet and Dreamer; less scalable without the architectural and optimization refinements introduced in later work.",
            "optimal_configuration": "Not specified in this paper; cited as inspiration for latent imagination ideas.",
            "uuid": "e1262.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to Control: Learning Behaviors by Latent Imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Learning Latent Dynamics for Planning from Pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Model-Based Reinforcement Learning for Atari",
            "rating": 2,
            "sanitized_title": "modelbased_reinforcement_learning_for_atari"
        },
        {
            "paper_title": "World Models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Stochastic Variational Video Prediction",
            "rating": 1,
            "sanitized_title": "stochastic_variational_video_prediction"
        }
    ],
    "cost": 0.01954475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Published as a conference paper at ICLR 2021 MASTERING ATARI WITH DISCRETE WORLD MODELS</p>
<p>Danijar Hafner 
Google Research
University of Toronto</p>
<p>Timothy Lillicrap 
Google Research
University of Toronto</p>
<p>Mohammad Norouzi 
Google Research
University of Toronto</p>
<p>Google Research 
Google Research
University of Toronto</p>
<p>Jimmy Ba 
Google Research
University of Toronto</p>
<p>Published as a conference paper at ICLR 2021 MASTERING ATARI WITH DISCRETE WORLD MODELS</p>
<p>Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.</p>
<p>DreamerV2 is the first agent that learns purely within a world model to achieve human-level Atari performance, demonstrating the high accuracy of its learned world model. DreamerV2 further outperforms the top single-GPU agents Rainbow and IQN, whose scores are provided by Dopamine (Castro et al., 2018). According to its authors, SimPLe (Kaiser et al., 2019) was only evaluated on an easier subset of 36 games and trained for fewer steps and additional training does not further increase its performance.</p>
<p>To successfully operate in unknown environments, reinforcement learning agents need to learn about their environments over time. World models are an explicit way to represent an agent's knowledge about its environment. Compared to model-free reinforcement learning that learns through trial and error, world models facilitate generalization and can predict the outcomes of potential actions to enable planning (Sutton, 1991). Capturing general aspects of the environment, world models have been shown to be effective for transfer to novel tasks (Byravan et al., 2019), directed exploration (Sekar et al., 2020), and generalization from offline datasets (Yu et al., 2020). When the inputs are high-dimensional images, latent dynamics models predict ahead in an abstract latent space (Watter et al., 2015;Ha and Schmidhuber, 2018;Hafner et al., 2018;Zhang et al., 2019). Predicting compact representations instead of images has been hypothesized to reduce accumulating errors and their small memory footprint enables thousands of parallel predictions on a single GPU (Hafner et al., 2018;2019). Leveraging this approach, the recent Dreamer agent (Hafner et al., 2019) has solved a wide range of continuous control tasks from image inputs.</p>
<p>Despite their intriguing properties, world models have so far not been accurate enough to compete with the stateof-the-art model-free algorithms on the most competitive benchmarks. The well-established Atari benchmark (Bellemare et al., 2013) historically required model-free algorithms to achieve human-level performance, such as DQN (Mnih et al., 2015), A3C (Mnih et al., 2016), or Rainbow (Hessel et al., 2018. Several attempts at learning accurate world models of Atari games have been made, without achieving competitive performance (Oh et al., 2015;Chiappa et al., 2017;Kaiser et al., 2019). On the other hand, the recently proposed MuZero agent (Schrittwieser et al., 2019) shows that planning can achieve impressive performance on board games and deterministic Atari games given extensive engineering effort and a vast computational budget. However, its implementation is not available to the public and it would require over 2 months of computation to train even one agent on a GPU, rendering it impractical for most research groups.</p>
<p>In this paper, we introduce DreamerV2, the first reinforcement learning agent that achieves humanlevel performance on the Atari benchmark by learning behaviors purely within a separately trained world model, as shown in Figure 1. Learning successful behaviors purely within the world model demonstrates that the world model learns to accurately represent the environment. To achieve this, we apply small modifications to the Dreamer agent (Hafner et al., 2019), such as using discrete latents and balancing terms within the KL loss. Using a single GPU and a single environment instance, DreamerV2 outperforms top single-GPU Atari agents Rainbow (Hessel et al., 2018) and IQN (Dabney et al., 2018), which rest upon years of model-free reinforcement learning research (Van Hasselt et al., 2015;Schaul et al., 2015;Wang et al., 2016;Bellemare et al., 2017;Fortunato et al., 2017). Moreover, aspects of these algorithms are complementary to our world model and could be integrated into the Dreamer framework in the future. To rigorously compare the algorithms, we report scores normalized by both a human gamer (Mnih et al., 2015) and the human world record (Toromanoff et al., 2019) and make a suggestion for reporting scores going forward.</p>
<p>DREAMERV2</p>
<p>We present DreamerV2, an evolution of the Dreamer agent (Hafner et al., 2019). We refer to the original Dreamer agent as DreamerV1 throughout this paper. This section describes the complete DreamerV2 algorithm, consisting of the three typical components of a model-based agent (Sutton, 1991). We learn the world model from a dataset of past experience, learn an actor and critic from imagined sequences of compact model states, and execute the actor in the environment to grow the experience dataset. In Appendix C, we include a list of changes that we applied to DreamerV1 and which of them we found to increase empirical performance.</p>
<p>WORLD MODEL LEARNING</p>
<p>World models summarize an agent's experience into a predictive model that can be used in place of the environment to learn behaviors. When inputs are high-dimensional images, it is beneficial to learn compact state representations of the inputs to predict ahead in this learned latent space (Watter et al., 2015;Karl et al., 2016;Ha and Schmidhuber, 2018). These models are called latent dynamics models. Predicting ahead in latent space not only facilitates long-term predictions, it also allows to efficiently predict thousands of compact state sequences in parallel in a single batch, without having to generate images. DreamerV2 builds upon the world model that was introduced by PlaNet (Hafner et al., 2018) and used in DreamerV1, by replacing its Gaussian latents with categorical variables.</p>
<p>Experience dataset The world model is trained from the agent's growing dataset of past experience that contains sequences of images x 1:T , actions a 1:T , rewards r 1:T , and discount factors γ 1:T . The discount factors equal a fixed hyper parameter γ = 0.999 for time steps within an episode and are set to zero for terminal time steps. For training, we use batches of B = 50 sequences of fixed length L = 50 that are sampled randomly within the stored episodes. To observe enough episode ends during training, we sample the start index of each training sequence uniformly within the episode and then clip it to not exceed the episode length minus the training sequence length.</p>
<p>Model components The world model consists of an image encoder, a Recurrent State-Space Model (RSSM;Hafner et al., 2018) to learn the dynamics, and predictors for the image, reward, and discount factor. The world model is summarized in Figure 2. The RSSM uses a sequence of deterministic recurrent states h t , from which it computes two distributions over stochastic states at each step. The posterior state z t incorporates information about the current image x t , while the prior stateẑ t aims to predict the posterior without access to the current image. The concatenation of deterministic and  Figure 2: World Model Learning. The training sequence of images x t is encoded using the CNN. The RSSM uses a sequence of deterministic recurrent states h t . At each step, it computes a posterior stochastic state z t that incorporates information about the current image x t , as well as a prior stochastic stateẑ t that tries to predict the posterior without access to the current image. Unlike in PlaNet and DreamerV1, the stochastic state of DreamerV2 is a vector of multiple categorical variables. The learned prior is used for imagination, as shown in Figure 3. The KL loss both trains the prior and regularizes how much information the posterior incorporates from the image. The regularization increases robustness to novel inputs. It also encourages reusing existing information from past steps to predict rewards and reconstruct images, thus learning long-term dependencies.</p>
<p>stochastic states forms the compact model state. From the posterior model state, we reconstruct the current image x t and predict the reward r t and discount factor γ t . The model components are:
RSSM    Recurrent model: h t = f φ (h t−1 , z t−1 , a t−1 ) Representation model: z t ∼ q φ (z t | h t , x t ) Transition predictor:ẑ t ∼ p φ (ẑ t | h t ) Image predictor:x t ∼ p φ (x t | h t , z t ) Reward predictor:r t ∼ p φ (r t | h t , z t ) Discount predictor:γ t ∼ p φ (γ t | h t , z t ).
(1) All components are implemented as neural networks and φ describes their combined parameter vector. The transition predictor guesses the next model state only from the current model state and the action but without using the next image, so that we can later learn behaviors by predicting sequences of model states without having to observe or generate images. The discount predictor lets us estimate the probability of an episode ending when learning behaviors from model predictions.</p>
<p>Neural networks The representation model is implemented as a Convolutional Neural Network (CNN; LeCun et al., 1989) followed by a Multi-Layer Perceptron (MLP) that receives the image embedding and the deterministic recurrent state. The RSSM uses a Gated Recurrent Unit (GRU;Cho et al., 2014) to compute the deterministic recurrent states. The model state is the concatenation of deterministic GRU state and a sample of the stochastic state. The image predictor is a transposed CNN and the transition, reward, and discount predictors are MLPs. We down-scale the 84 × 84 grayscale images to 64 × 64 pixels so that we can apply the convolutional architecture of DreamerV1.</p>
<p>Algorithm 1: Straight-Through Gradients with Automatic Differentiation sample = one_hot(draw(logits)) # sample has no gradient probs = softmax(logits) # want gradient of this sample = sample + probs -stop_grad(probs) # has gradient of probs We use the ELU activation function for all components of the model (Clevert et al., 2015). The world model uses a total of 20M trainable parameters.</p>
<p>Distributions The image predictor outputs the mean of a diagonal Gaussian likelihood with unit variance, the reward predictor outputs a univariate Gaussian with unit variance, and the discount predictor outputs a Bernoulli likelihood. In prior work, the latent variable in the model state was a diagonal Gaussian that used reparameterization gradients during backpropagation (Kingma and Welling, 2013;Rezende et al., 2014). In DreamerV2, we instead use a vector of several categorical variables and optimize them using straight-through gradients (Bengio et al., 2013), which are easy to implement using automatic differentiation as shown in Algorithm 1. We discuss possible benefits of categorical over Gaussian latents in the experiments section.</p>
<p>Loss function All components of the world model are optimized jointly. The distributions produced by the image predictor, reward predictor, discount predictor, and transition predictor are trained to maximize the log-likelihood of their corresponding targets. The representation model is trained to produce model states that facilitates these prediction tasks, through the expectation below. Moreover, it is regularized to produce model states with high entropy, such that the model becomes robust to many different model states during training. The loss function for learning the world model is:
L(φ) . = E q φ (z 1:T | a 1:T ,x 1:T ) T t=1 − ln p φ (x t | h t , z t ) image log loss − ln p φ (r t | h t , z t ) reward log loss − ln p φ (γ t | h t , z t ) discount log loss +β KL q φ (z t | h t , x t ) p φ (z t | h t ) KL loss .(2)
We jointly minimize the loss function with respect to the vector φ that contains all parameters of the world model using the Adam optimizer (Kingma and Ba, 2014). We scale the KL loss by β = 0.1 for Atari and by β = 1.0 for continuous control (Higgins et al., 2016).</p>
<p>KL balancing The world model loss function in Equation 2 is the ELBO or variational free energy of a hidden Markov model that is conditioned on the action sequence. The world model can thus be interpreted as a sequential VAE, where the representation model is the approximate posterior and the transition predictor is the temporal prior. In the ELBO objective, the KL loss serves two purposes: it trains the prior toward the representations, and it regularizes the representations toward the prior. However, learning the transition function is difficult and we want to avoid regularizing the representations toward a poorly trained prior. To solve this problem, we minimize the KL loss faster with respect to the prior than the representations by using different learning rates, α = 0.8 for the prior and 1 − α for the approximate posterior. We implement this technique as shown in Algorithm 2 and refer to it as KL balancing. KL balancing encourages learning an accurate prior over increasing posterior entropy, so that the prior better approximates the aggregate posterior. KL balancing is different from and orthogonal to beta-VAEs (Higgins et al., 2016).</p>
<p>BEHAVIOR LEARNING</p>
<p>DreamerV2 learns long-horizon behaviors purely within its world model using an actor and a critic. The actor chooses actions for predicting imagined sequences of compact model states. The critic accumulates the future predicted rewards to take into account rewards beyond the planning horizon. Both the actor and critic operate on top of the learned model states and thus benefit from the representations learned by the world model. The world model is fixed during behavior learning, so the actor and value gradients do not affect its representations. Not predicting images during behavior learning lets us efficiently simulate 2500 latent trajectories in parallel on a single GPU.</p>
<p>Imagination MDP To learn behaviors within the latent space of the world model, we define the imagination MPD as follows. The distribution of initial statesẑ 0 in the imagination MDP is the distribution of compact model states encountered during world model training. From there, the transition predictor p φ (ẑ t |ẑ t−1 ,â t−1 ) outputs sequencesẑ 1:H of compact model states up to the  Figure 3: Actor Critic Learning. The world model learned in Figure 2 is used for learning a policy from trajectories imagined in the compact latent space. The trajectories start from posterior states computed during model training and predict forward by sampling actions from the actor network. The critic network predicts the expected sum of future rewards for each state. The critic uses temporal difference learning on the imagined rewards. The actor is trained to maximize the critic prediction, via reinforce gradients, straight-through gradients of the world model, or a combination of them.</p>
<p>imagination horizon H = 15. The mean of the reward predictor p φ (r t |ẑ t ) is used as reward sequencer 1:H . The discount predictor p φ (γ t |ẑ t ) outputs the discount sequenceγ 1:H that is used to down-weight rewards. Moreover, we weigh the loss terms of the actor and critic by the cumulative predicted discount factors to softly account for the possibility of episode ends.</p>
<p>Model components To learn long-horizon behaviors in the imagination MDP, we leverage a stochastic actor that chooses actions and a deterministic critic. The actor and critic are trained cooperatively, where the actor aims to output actions that lead to states that maximize the critic output, while the critic aims to accurately estimate the sum of future rewards achieved by the actor from each imagined state. The actor and critic use the parameter vectors ψ and ξ, respectively:
Actor:â t ∼ p ψ (â t |ẑ t ) Critic: v ξ (ẑ t ) ≈ E p φ ,p ψ τ ≥tγ τ −tr τ .(3)
In contrast to the actual environment, the latent state sequence is Markovian, so that there is no need for the actor and critic to condition on more than the current model state. The actor and critic are both MLPs with ELU activations (Clevert et al., 2015) and use 1M trainable parameters each. The actor outputs a categorical distribution over actions and the critic has a deterministic output. The two components are trained from the same imagined trajectories but optimize separate loss functions.</p>
<p>Critic loss function</p>
<p>The critic aims to predict the discounted sum of future rewards that the actor achieves in a given model state, known as the state value. For this, we leverage temporal-difference learning, where the critic is trained toward a value target that is constructed from intermediate rewards and critic outputs for later states. A common choice is the 1-step target that sums the current reward and the critic output for the following state. However, the imagination MDP lets us generate on-policy trajectories of multiple steps, suggesting the use of n-step targets that incorporate reward information into the critic more quickly. We follow DreamerV1 in using the more general λ-target (Sutton and Barto, 2018;Schulman et al., 2015) that is defined recursively as follows:
V λ t . =r t +γ t (1 − λ)v ξ (ẑ t+1 ) + λV λ t+1 if t &lt; H, v ξ (ẑ H ) if t = H.(4)
Intuitively, the λ-target is a weighted average of n-step returns for different horizons, where longer horizons are weighted exponentially less. We set λ = 0.95 in practice, to focus more on long horizon targets than on short horizon targets. Given a trajectory of model states, rewards, and discount factors, we train the critic to regress the λ-return using a squared loss:
L(ξ) . = E p φ ,p ψ H−1 t=1 1 2 v ξ (ẑ t ) − sg(V λ t ) 2 .(5)
We optimize the critic loss with respect to the critic parameters ξ using the Adam optimizer. There is no loss term for the last time step because the target equals the critic at that step. We stop the gradients around the targets, denoted by the sg(·) function, as typical in the literature. We stabilize value learning using a target network (Mnih et al., 2015), namely, we compute the targets using a copy of the critic that is updated every 100 gradient steps.</p>
<p>Actor loss function The actor aims to output actions that maximize the prediction of long-term future rewards made by the critic. To incorporate intermediate rewards more directly, we train the actor to maximize the same λ-return that was computed for training the critic. There are different gradient estimators for maximizing the targets with respect to the actor parameters. DreamerV2 combines unbiased but high-variance Reinforce gradients with biased but low-variance straightthrough gradients. Moreover, we regularize the entropy of the actor to encourage exploration where feasible while allowing the actor to choose precise actions when necessary.</p>
<p>Learning by Reinforce (Williams, 1992) maximizes the actor's probability of its own sampled actions weighted by the values of those actions. The variance of this estimator can be reduced by subtracting the state value as baseline, which does not depend on the current action. Intuitively, subtracting the baseline centers the weights and leads to faster learning. The benefit of Reinforce is that it produced unbiased gradients and the downside is that it can have high variance, even with baseline.</p>
<p>DreamerV1 relied entirely on reparameterization gradients (Kingma and Welling, 2013;Rezende et al., 2014) to train the actor directly by backpropagating value gradients through the sequence of sampled model states and actions. DreamerV2 uses both discrete latents and discrete actions.</p>
<p>To backpropagate through the sampled actions and state sequences, we leverage straight-through gradients (Bengio et al., 2013). This results in a biased gradient estimate with low variance. The combined actor loss function is:
L(ψ) . = E p φ ,p ψ H−1 t=1 −ρ ln p ψ (â t |ẑ t ) sg(V λ t − v ξ (ẑ t )) reinforce −(1 − ρ)V λ t dynamics backprop −η H[a t |ẑ t ] entropy regularizer .(6)
We optimize the actor loss with respect to the actor parameters ψ using the Adam optimizer. We consider both Reinforce gradients and straight-through gradients, which backpropagate directly through the learned dynamics. Intuitively, the low-variance but biased dynamics backpropagation could learn faster initially and the unbiased but high-variance could to converge to a better solution. For Atari, we find Reinforce gradients to work substantially better and use ρ = 1 and η = 10 −3 . For continuous control, we find dynamics backpropagation to work substantially better and use ρ = 0 and η = 10 −4 . Annealing these hyper parameters can improve performance slightly but to avoid the added complexity we report the scores without annealing.</p>
<p>EXPERIMENTS</p>
<p>We evaluate DreamerV2 on the well-established Atari benchmark with sticky actions, comparing to four strong model-free algorithms. DreamerV2 outperforms the four model-free algorithms in all scenarios. For an extensive comparison, we report four scores according to four aggregation protocols and give a recommendation for meaningfully aggregating scores across games going forward. We also ablate the importance of discrete representations in the world model. Our implementation of DreamerV2 reaches 200M environment steps in under 10 days, while using only a single NVIDIA V100 GPU and a single environment instance. During the 200M environment steps, DreamerV2 learns its policy from 468B compact states imagined under the model, which is 10,000× more than the 50M inputs received from the real environment after action repeat. Refer to the project website for videos, the source code, and training curves in JSON format.  Table 1 for numeric scores. The standards in the literature to aggregate over tasks are shown in the left two plots. These normalize scores by a professional gamer and compute the median or mean over tasks (Mnih et al., 2015;2016). In Section 3, we point out limitations of this methodology. As a robust measure of performance, we recommend the metric in the right-most plot. We normalize scores by the human world record (Toromanoff et al., 2019) and then clip them, such that exceeding the record does not further increase the score, before averaging over tasks. . We use the scores of these agents provided by the Dopamine framework (Castro et al., 2018) that use sticky actions. These may differ from the reported results in the papers that introduce these algorithms in the deterministic Atari setup. The training time of Rainbow was reported at 10 days on a single GPU and using one environment instance.</p>
<p>Experimental setup</p>
<p>ATARI PERFORMANCE</p>
<p>The performance curves of DreamerV2 and four standard model-free algorithms are visualized in Figure 4. The final scores at 200M environment steps are shown in Table 1 and the scores on individual games are included in  The ablations highlight the benefit of using categorical over Gaussian latent variables and of using KL balancing. Moreover, they show that the world model relies on image gradients for learning its representations. Stopping reward gradients even improves performance on some tasks, suggesting that representations that are not specifically trained to predict previously experienced rewards may generalize better to new situations.</p>
<p>• Gamer Median Atari scores are commonly normalized based on a random policy and a professional gamer, averaged over seeds, and the median over tasks is reported (Mnih et al., 2015;2016). However, if almost half of the scores would be zero, the median would not be affected. Thus, we argue that median scores are not reflective of the robustness of an algorithm and results in wasted computational resources for games that will not affect the score. • Gamer Mean Compared to the task median, the task mean considers all tasks. However, the gamer performed poorly on a small number of games, such as Crazy Climber, James Bond, and Video Pinball. This makes it easy for algorithms to achieve a high normalized score on these few games, which then dominate the task mean so it is not informative of overall performance. • Record Mean Instead of normalizing based on the professional gamer, Toromanoff et al.</p>
<p>(2019) suggest to normalize based on the registered human world record of each game. This partially addresses the outlier problem but the mean is still dominated by games where the algorithms easily achieve superhuman performance. • Clipped Record Mean To overcome these limitations, we recommend normalizing by the human world record and then clipping the scores to not exceed a value of 1, so that performance above the record does not further increase the score. The result is a robust measure of algorithm performance on the Atari suite that considers performance across all games.</p>
<p>From Figure 4 and Table 1, we see that the different aggregation approaches let us examine agent performance from different angles. Interestingly, Rainbow clearly outperforms IQN in the first aggregation method but IQN clearly outperforms Rainbow in the remaining setups. DreamerV2 outperforms the model-free agents in all four metrics, with the largest margin in record normalized mean performance. Despite this, we recommend clipped record normalized mean as the most meaningful aggregation method, as it considers all tasks to a similar degree without being dominated by a small number of outlier scores. In Table 1, we also include DreamerV2 with schedules that anneal the actor entropy loss scale and actor gradient mixing over the course of training, which further increases the gamer median score of DreamerV2.</p>
<p>Individual games The scores on individual Atari games at 200M environment steps are included in Table K.1, alongside the model-free algorithms and the baselines of random play, human gamer, and human world record. We filled in reasonable values for the 2 out of 55 games that have no registered world record. Figure E.1 compares the score differences between DreamerV2 and each model-free algorithm for the individual games. DreamerV2 achieves comparable or higher performance on most games except for Video Pinball. We hypothesize that the reconstruction loss of the world model does not encourage learning a meaningful latent representation because the most important object in the game, the ball, occupies only a single pixel. One the other hand, DreamerV2 achieves the strongest improvements over the model-free agents on the games James Bond, Up N Down, and Assault.  </p>
<p>ABLATION STUDY</p>
<p>To understand which ingredients of DreamerV2 are responsible for its success, we conduct an extensive ablation study. We compare equipping the world model with categorical latents, as in DreamerV2, to Gaussian latents, as in DreamerV1. Moreover, we study the importance of KL balancing. Finally, we investigate the importance of gradients from image reconstruction and reward prediction for learning the model representations, by stopping one of the two gradient signals before entering the model states. The results of the ablation study are summarized in Figure 5 and Table 2.</p>
<p>Refer to the appendix for the score curves of the individual tasks.</p>
<p>Categorical latents Categorical latent variables outperform than Gaussian latent variables on 42 tasks, achieve lower performance on 8 tasks, and are tied on 5 tasks. We define a tie as being within 5% of another. While we do not know the reason why the categorical variables are beneficial, we state several hypotheses that can be investigated in future work:</p>
<p>• A categorical prior can perfectly fit the aggregate posterior, because a mixture of categoricals is again a categorical. In contrast, a Gaussian prior cannot match a mixture of Gaussian posteriors, which could make it difficult to predict multi-modal changes between one image and the next. • The level of sparsity enforced by a vector of categorical latent variables could be beneficial for generalization. Flattening the sample from the 32 categorical with 32 classes each results in a sparse binary vector of length 1024 with 32 active bits. • Despite common intuition, categorical variables may be easier to optimize than Gaussian variables, possibly because the straight-through gradient estimator ignores a term that would otherwise scale the gradient. This could reduce exploding and vanishing gradients. • Categorical variables could be a better inductive bias than unimodal continuous latent variables for modeling the non-smooth aspects of Atari games, such as when entering a new room, or when collected items or defeated enemies disappear from the image.</p>
<p>KL balancing KL balancing outperforms the standard KL regularizer on 44 tasks, achieves lower performance on 6 tasks, and is tied on 5 tasks. Learning accurate prior dynamics of the world model is critical because it is used for imagining latent state trajectories using policy optimization. By scaling up the prior cross entropy relative to the posterior entropy, the world model is encouraged to minimize the KL by improving its prior dynamics toward the more informed posteriors, as opposed to reducing the KL by increasing the posterior entropy. KL balancing may also be beneficial for probabilistic models with learned priors beyond world models.</p>
<p>Model gradients Stopping the image gradients increases performance on 3 tasks, decreases performance on 51 tasks, and is tied on 1 task. The world model of DreamerV2 thus heavily relies on the learning signal provided by the high-dimensional images. Stopping the reward gradients increases performance on 15 tasks, decreases performance on 22 tasks, and is tied on 18 tasks. Figure H.1 further shows that the difference in scores is small. In contrast to MuZero, DreamerV2 thus learns general representations of the environment state from image information alone. Stopping reward gradients improved performance on a number of tasks, suggesting that the representations that are not specific to previously experienced rewards may generalize better to unseen situations.  Table 3: Conceptual comparison of recent RL algorithms that leverage planning with a learned model. DreamerV2 and SimPLe learn complete models of the environment by leveraging the learning signal provided by the image inputs, while MuZero learns its model through value gradients that are specific to an individual task. The Monte-Carlo tree search used by MuZero is effective but adds complexity and is challenging to parallelize. This component is orthogonal to the world model proposed here.</p>
<p>Policy gradients Using only Reinforce gradients to optimize the policy increases performance on 18 tasks, decreases performance on 24 tasks, and is tied on 13 tasks. This shows that DreamerV2 relies mostly on Reinforce gradients to learn the policy. However, mixing Reinforce and straight-through gradients yields a substantial improvement on James Bond and Seaquest, leading to a higher gamer normalized task mean score. Using only straight-through gradients to optimize the policy increases performance on 5 tasks, decreases performance on 44 tasks, and is tied on 6 tasks. We conjecture that straight-through gradients alone are not well suited for policy optimization because of their bias.</p>
<p>RELATED WORK</p>
<p>Model-free Atari The majority of agents applied to the Atari benchmark have been trained using model-free algorithms. DQN (Mnih et al., 2015) showed that deep neural network policies can be trained using Q-learning by incorporating experience replay and target networks.  Table 3. The model directly predicts each frame from the previous four frames and receives an additional discrete latent variable as input. The authors evaluate SimPLe on a subset of Atari games for 400k and 2M environment steps, after which they report diminishing returns. Some recent model-free methods have followed the comparison at 400k steps (Srinivas et al., 2020;Kostrikov et al., 2020). However, the highest performance achieved in this data-efficient regime is a gamer normalized median score of 0.28 (Kostrikov et al., 2020) Coulom, 2006;Silver et al., 2017). The sequence model is trained purely by predicting task-specific information and does not incorporate explicit representation learning using the images, as shown in Table 3. MuZero shows that with significant engineering effort and a vast computational budget, planning can achieve impressive performance on several board games and deterministic Atari games. However, MuZero is not publicly available, and it would require over 2 months to train an Atari agent on one GPU. By comparison, DreamerV2 is a simple algorithm that achieves human-level performance on Atari on a single GPU in 10 days, making it reproducible for many researchers. Moreover, the advanced planning components of MuZero are complementary and could be applied to the accurate world models learned by DreamerV2. DreamerV2 leverages the additional learning signal provided by the input images, analogous to recent successes by semi-supervised image classification (Chen et al., 2020;He et al., 2020;Grill et al., 2020).</p>
<p>DISCUSSION</p>
<p>We A HUMANOID FROM PIXELS Figure  While the main experiments of this paper focus on the Atari benchmark with discrete actions, Dream-erV2 is also applicable to control tasks with continuous actions. For this, we the actor outputs a truncated normal distribution instead of a categorical distribution. To demonstrate the abilities of DreamerV2 for continuous control, we choose the challenging humanoid environment with only image inputs, shown in Figure A.1. We find that for continuous control tasks, dynamics backpropagation substantially outperforms reinforce gradients and thus set ρ = 0. We also set η = 10 −5 and β = 2 to further accelerate learning. We find that DreamerV2 reliably solves both the stand-up motion required at the beginning of the episode and the subsequent walking. The score is shown in Figure A . This suggests that the world model may help with solving sparse reward tasks, for example due to improved generalization, efficient policy optimization in the compact latent space enabling more actor critic updates, or because the reward predictor generalizes and thus smooths out the sparse rewards.</p>
<p>C SUMMARY OF MODIFICATIONS</p>
<p>To develop DreamerV2, we used the Dreamer agent (Hafner et al., 2019) as a starting point. This subsection describes the changes that we applied to the agent to achieve high performance on the Atari benchmark, as well as the changes that were tried but not found to increase performance and thus were not not included in DreamerV2.</p>
<p>Summary of changes that were tried and were found to help:</p>
<p>• Categorical latents Using categorical latent states using straight-through gradients in the world model instead of Gaussian latents with reparameterized gradients. • KL balancing Separately scaling the prior cross entropy and the posterior entropy in the KL loss to encourage learning an accurate temporal prior, instead of using free nats. • Reinforce only Reinforce gradients worked substantially better for Atari than dynamics backpropagation. For continuous control, dynamics backpropagation worked substantially better. • Model size Increasing the number of units or feature maps per layer of all model components, resulting in a change from 13M parameters to 22M parameters. • Policy entropy Regularizing the policy entropy for exploration both in imagination and during data collection, instead of using external action noise during data collection.</p>
<p>Summary of changes that were tried but were found to not help substantially:</p>
<p>• Binary latents Using a larger number of binary latents for the world model instead of categorical latents, which could have encouraged a more disentangled representation, was worse. • Long-term entropy Including the policy entropy into temporal-difference loss of the value function, so that the actor seeks out states with high action entropy beyond the planning horizon. • Mixed actor gradients Combining Reinforce and dynamics backpropagation gradients for learning the actor instead of Reinforce provided marginal or no benefits. • Scheduling Scheduling the learning rates, KL scale, actor entropy loss scale, and actor gradient mixing (from 0.1 to 0) provided marginal or no benefits. • Layer norm Using layer normalization in the GRU that is used as part of the RSSM latent transition model, instead of no normalization, provided no or marginal benefits.</p>
<p>Due to the large computational requirements, a comprehensive ablation study on this list of all changes is unfortunately infeasible for us. This would require 55 tasks times 5 seeds for 10 days per change to run, resulting in over 60,000 GPU hours per change. However, we include ablations for the most important design choices in the main text of the paper.   Comparison of leveraging Reinforce gradients, straight-through gradients, or both for training the actor. While Reinforce gradients are crucial, straight-through gradients are not important for most of the tasks. Nonetheless, combining both gradients yields substantial improvements on a small number of games, most notably on Seaquest. We conjecture that straight-through gradients have low variance and thus help the agent start learning, whereas Reinforce gradients are unbiased and help converging to a better solution.</p>
<p>D HYPER PARAMETERS</p>
<p>K ATARI TASK SCORES</p>
<p>Figure 1 :
1Gamer normalized median score on the Atari benchmark of 55 games with sticky actions at 200M steps.</p>
<p>Figure 4 :
4Atari performance over 200M steps. See</p>
<p>We select the 55 games that prior works in the literature from different research labs tend to agree on(Mnih et al., 2016; Brockman et al., 2016; Hessel et al., 2018; Castro  et al., 2018; Badia et al., 2020)  and recommend this set of games for evaluation going forward. We follow the evaluation protocol of Machado et al.(2018)with 200M environment steps, action repeat of 4, a time limit of 108,000 steps per episode that correspond to 30 minutes of game play, no access to life information, full action space, and sticky actions. Because the world model integrates information over time, DreamerV2 does not use frame stacking. The experiments use a single-task setup where a separate agent is trained for each game. Moreover, each agent uses only a single environment instance. We compare the algorithms based on both human gamer and human world record normalization(Toromanoff et al., 2019).Model-free baselinesWe compare the learning curves and final scores of DreamerV2 to four model-free algorithms, IQN (Dabney et al., 2018), Rainbow (Hessel et al., 2018), C51 (Bellemare et al., 2017), and DQN (Mnih et al., 2015)</p>
<p>Figure 5 :
5Clipped record normalized scores of various ablations of the DreamerV2 agent. This experiment uses a slightly earlier version of DreamerV2. The score curves for individual tasks are shown in Figure H.1.</p>
<p>Several works have extended DQN to incorporate bias correction as inDDQN (Van Hasselt et al., 2015), prioritized experience replay(Schaul et al., 2015), architectural improvements(Wang et al., 2016), and distributional value learning(Bellemare et al., 2017; Dabney et al., 2017; 2018). Besides value learning, agents based on policy gradients have targeted the Atari benchmark, such as ACER (Schulman et al.,2017a), PPO (Schulman et al., 2017a), ACKTR (Wu et al., 2017), and Reactor (Gruslys et al., 2017). Another line of work has focused on improving performance by distributing data collection, often while increasing the budget of environment steps beyond 200M (Mnih et al., 2016; Schulman et al., 2017b; Horgan et al., 2018; Kapturowski et al., 2018; Badia et al., 2020).World models Several model-based agents focus on proprioceptive inputs(Watter et al., 2015; Gal  et al., 2016; Higuera et al., 2018; Henaff et al., 2018; Chua et al., 2018; Wang et al., 2019; Wang  and Ba, 2019), model images without using them for planning(Oh et al., 2015; Krishnan et al.,  2015; Karl et al., 2016; Chiappa et al., 2017;Babaeizadeh et al., 2017; Gemici et al., 2017; Denton  and Fergus, 2018; Buesing et al., 2018; Doerr et al., 2018; Gregor and Besse, 2018), or combine the benefits of model-based and model-free approaches(Kalweit and Boedecker, 2017; Nagabandi  et al., 2017; Weber et al., 2017; Kurutach et al., 2018; Buckman et al., 2018; Ha and Schmidhuber,  2018; Wayne et al., 2018; Igl et al., 2018; Srinivas et al., 2018; Lee et al., 2019). Risi and Stanley (2019) optimize discrete latents using evolutionary search. Parmas et al. (2019) combine reinforce and reparameterization gradients. Most world model agents with image inputs have thus far been limited to relatively simple control tasks(Watter et al., 2015; Ebert et al., 2017; Ha and Schmidhuber,  2018; Hafner et al., 2018; Zhang et al., 2019; Hafner et al., 2019). We explain the two model-based approaches that were applied to Atari in detail below.SimPLe The SimPLe agent (Kaiser et al., 2019) learns a video prediction model in pixel-space and uses its predictions to train a PPO agent(Schulman et al., 2017a), as shown in</p>
<p>A. 1 :Figure A. 2 :
12Behavior learned by DreamerV2 on the Humanoid Walk task from pixel inputs only. The task is provided by the DeepMind Control Suite and uses a continuous action space with 21 dimensions. The frames show the agent inputs. Performance on the humanoid walking task from only pixel inputs.</p>
<p>. 2 .Figure B. 2 :
22To the best of our knowledge, this constitutes the first published result of solving the humanoid environment from only pixel inputs. B MONTEZUMA'S REVENGE Figure B.1: Behavior learned by DreamerV2 on the Atari game Montezuma's Revenge, that poses a hard exploration challenge. Without any explicit exploration mechanism, DreamerV2 reaches about the same performance as the exploration method ICM. Performance on the Atari game Montezuma's Revenge. While our main experiments use the same hyper parameters across all tasks, we find that DreamerV2 achieves higher performance on Montezuma's Revenge by using a lower discount factor of γ = 0.99, possibly to stabilize value learning under sparse rewards. Figure B.2 shows the resulting performance, with all other hyper parameters left at their defaults. DreamerV2 outperforms existing modelfree approaches on the hard-exploration game Montezuma's Revenge and matches the performance of the explicit exploration algorithm ICM (Pathak et al., 2017) that was applied on top of Rainbow by Taiga et al. (2019)</p>
<p>FFigure F. 1 :Figure G. 1 :Figure H. 1 :
111MODELComparison of DreamerV2 to the top model-free RL methods IQN and Rainbow. The curves show mean and standard deviation over 5 seeds. IQN and Rainbow additionally average each point over 10 evaluation episodes, explaining the smoother curves. DreamerV2 outperforms IQN and Rainbow in all four aggregated scores. While IQN and Rainbow tend to succeed on the same tasks, DreamerV2 shows a different performance profile. Comparison of DreamerV2, Gaussian instead of categorical latent variables, and no KL balancing. The ablation experiments use a slightly earlier version of the agent. The curves show mean and standard deviation across two seeds. Categorical latent variables and KL balancing both substantially improve performance across many of the tasks. The importance of the two techniques is reflected in all four aggregated scores. Comparison of leveraging image prediction, reward prediction, or both for learning the model representations. While image gradients are crucial, reward gradients are not necessary for our world model to succeed and their gradients can be stopped. Representations learned purely from images are not biased toward previously encountered rewards and outperform reward-specific representations on a number of tasks, suggesting that they may generalize better to unseen situations.</p>
<p>Table K.1. There are different approaches for aggregating the scores across the 55 games and we show that this choice can have a substantial impact on the relative performance between algorithms. To extensively compare DreamerV2 to the model-free algorithms, we consider the following four aggregation approaches:Table 1: Atari performance at 200M steps. The scores of the 55 games are aggregated using the four different protocols described in Section 3. To overcome limitations of the previous metrics, we recommend the task mean of clipped record normalized scores as a robust measure of algorithm performance, shown in the right-most column. DreamerV2 outperforms previous single-GPU agents across all metrics. The baseline scores are taken from Dopamine Baselines(Castro et al., 2018).Agent 
Gamer Median Gamer Mean Record Mean Clipped Record Mean </p>
<p>DreamerV2 
2.15 
11.33 
0.44 
0.28 
DreamerV2 (schedules) 
2.64 
10.45 
0.43 
0.28 
IQN 
1.29 
8.85 
0.21 
0.21 
Rainbow 
1.47 
9.12 
0.17 
0.17 
C51 
1.09 
7.70 
0.15 
0.15 
DQN 
0.65 
2.84 
0.12 
0.12 </p>
<p>Table 2 :
2Ablations to DreamerV2 measured by their Atari performance at 200M frames, sorted by the 
last column. The this experiment uses a slightly earlier version of DreamerV2 compared to Table 1. 
Each ablation only removes one part of the DreamerV2 agent. Discrete latent variables and KL 
balancing substantially contribute to the success of DreamerV2. Moreover, the world model relies 
on image gradients to learn general representations that lead to successful behaviors, even if the 
representations are not specifically learned for predicting past rewards. </p>
<p>that is far from human-level performance. Instead, we focus on the well-established and competitive evaluation after 200M frames, where many successful model-free algorithms are available for comparison.MuZero The MuZero agent (Schrittwieser et al., 2019) learns a sequence model of rewards and 
values (Oh et al., 2017) to solve reinforcement learning tasks via Monte-Carlo Tree Search (MCTS; </p>
<p>improves the performance of the agent. Moreover, we find the DreamerV2 relies on image information for learning generally useful representations -its performance is not impacted by whether the representations are especially learned for predicting rewards.DreamerV2 serves as proof of concept, showing that model-based RL can outperform top model-free algorithms on the most competitive RL benchmarks, despite the years of research and engineering effort that modern model-free agents rest upon. Beyond achieving strong performance on individual tasks, world models open avenues for efficient transfer and multi-task learning, sample-efficient learning on physical robots, and global exploration based on uncertainty estimates.F Ebert, C Finn, AX Lee, S Levine. Self-Supervised Visual Planning With Temporal Skip Connections. ArXiv Preprint ArXiv:1710.05268, 2017. M Fortunato, MG Azar, B Piot, J Menick, I Osband, A Graves, V Mnih, R Munos, D Hassabis, O Pietquin, et al. Noisy Networks for Exploration. ArXiv Preprint ArXiv:1706.10295, 2017. Davidson. Learning Latent Dynamics for Planning From Pixels. ArXiv Preprint ArXiv:1811.04551, 2018. D Hafner, T Lillicrap, J Ba, M Norouzi. Dream to Control: Learning Behaviors by Latent Imagination.present DreamerV2, a model-based agent that achieves human-level performance on the Atari 
200M benchmark by learning behaviors purely from the latent-space predictions of a separately 
trained world model. Using a single GPU and a single environment instance, DreamerV2 outperforms 
top model-free single-GPU agents Rainbow and IQN using the same computational budget and 
training time. To develop DreamerV2, we apply several small modifications to the Dreamer agent 
(Hafner et al., 2019). We confirm experimentally that learning a categorical latent space and using 
KL balancing Y Gal, R McAllister, CE Rasmussen. Improving Pilco With Bayesian Neural Network Dynamics 
Models. Data-Efficient Machine Learning Workshop, ICML, 2016. </p>
<p>M Gemici, CC Hung, A Santoro, G Wayne, S Mohamed, DJ Rezende, D Amos, T Lillicrap. 
Generative Temporal Models With Memory. ArXiv Preprint ArXiv:1702.04649, 2017. </p>
<p>K Gregor F Besse. Temporal Difference Variational Auto-Encoder. ArXiv Preprint ArXiv:1806.03107, 
2018. </p>
<p>JB Grill, F Strub, F Altché, C Tallec, PH Richemond, E Buchatskaya, C Doersch, BA Pires, ZD Guo, 
MG Azar, et al. Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. ArXiv 
Preprint ArXiv:2006.07733, 2020. </p>
<p>A Gruslys, W Dabney, MG Azar, B Piot, M Bellemare, R Munos. The Reactor: A Fast and Sample-
Efficient Actor-Critic Agent for Reinforcement Learning. ArXiv Preprint ArXiv:1704.04651, 
2017. </p>
<p>D Ha J Schmidhuber. World Models. ArXiv Preprint ArXiv:1803.10122, 2018. </p>
<p>D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J ArXiv Preprint ArXiv:1912.01603, 2019. </p>
<p>K He, H Fan, Y Wu, S Xie, R Girshick. Momentum Contrast for Unsupervised Visual Representation 
Learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 
2020. </p>
<p>M Henaff, WF Whitney, Y LeCun. Model-Based Planning With Discrete and Continuous Actions. 
ArXiv Preprint ArXiv:1705.07177, 2018. </p>
<p>M Hessel, J Modayil, H Van Hasselt, T Schaul, G Ostrovski, W Dabney, D Horgan, B Piot, M Azar, 
D Silver. Rainbow: Combining Improvements in Deep Reinforcement Learning. Thirty-Second 
AAAI Conference on Artificial Intelligence, 2018. </p>
<p>I Higgins, L Matthey, A Pal, C Burgess, X Glorot, M Botvinick, S Mohamed, A Lerchner. Beta-
Vae: Learning Basic Visual Concepts With a Constrained Variational Framework. International 
Conference on Learning Representations, 2016. </p>
<p>JCG Higuera, D Meger, G Dudek. Synthesizing Neural Network Controllers With Probabilistic 
Model Based Reinforcement Learning. ArXiv Preprint ArXiv:1803.02291, 2018. </p>
<p>D Horgan, J Quan, D Budden, G Barth-Maron, M Hessel, H Van Hasselt, D Silver. Distributed 
Prioritized Experience Replay. ArXiv Preprint ArXiv:1803.00933, 2018. </p>
<p>M Igl, L Zintgraf, TA Le, F Wood, S Whiteson. Deep Variational Reinforcement Learning for 
Pomdps. ArXiv Preprint ArXiv:1806.02426, 2018. </p>
<p>L Kaiser, M Babaeizadeh, P Milos, B Osinski, RH Campbell, K Czechowski, D Erhan, C Finn, 
P Kozakowski, S Levine, et al. Model-Based Reinforcement Learning for Atari. ArXiv Preprint 
ArXiv:1903.00374, 2019. </p>
<p>G Kalweit J Boedecker. Uncertainty-Driven Imagination for Continuous Deep Reinforcement 
Learning. Conference on Robot Learning, 2017. </p>
<p>S Kapturowski, G Ostrovski, J Quan, R Munos, W Dabney. Recurrent Experience Replay in 
Distributed Reinforcement Learning. International Conference on Learning Representations, 
2018. 
M Karl, M Soelch, J Bayer, P van der Smagt. Deep Variational Bayes Filters: Unsupervised Learning 
of State Space Models From Raw Data. ArXiv Preprint ArXiv:1605.06432, 2016. </p>
<p>DP Kingma J Ba. Adam: A Method for Stochastic Optimization. ArXiv Preprint ArXiv:1412.6980, 
2014. </p>
<p>DP Kingma M Welling. Auto-Encoding Variational Bayes. ArXiv Preprint ArXiv:1312.6114, 2013. </p>
<p>Table D .
DFigure E.1: Atari agent comparison. The bars show the difference in gamer normalized scores at 200M steps. DreamerV2 outperforms the four model-free algorithms IQN, Rainbow, C51, and DQN while learning behaviors purely by planning within a separately learned world model. DreamerV2 achieves higher or similar performance on all tasks besides Video Pinball, where we hypothesize that the reconstruction loss does not focus on the ball that makes up only one pixel on the screen.1: Atari hyper parameters of DreamerV2. When tuning the agent for a new task, we 
recommend searching over the KL loss scale β ∈ {0.1, 0.3, 1, 3}, actor entropy loss scale η ∈ 
{3 · 10 −5 , 10 −4 , 3 · 10 −4 , 10 −3 }, and the discount factor γ ∈ {0.99, 0.999}. The training frequency 
update should be increased when aiming for higher data-efficiency. 
E AGENT COMPARISON </p>
<p>James Bond 
Up N Down </p>
<p>Krull 
Gopher </p>
<p>Demon Attack 
Assault </p>
<p>Road Runner 
Time Pilot 
Breakout 
Asterix 
Phoenix 
Qbert 
Atlantis 
Zaxxon </p>
<p>Ice Hockey 
Wizard Of Wor 
Yars Revenge 
Name This Game 
Kung Fu Master </p>
<p>Robotank </p>
<p>Crazy Climber 
Asteroids 
Frostbite 
Centipede 
Gravitar </p>
<p>Beam Rider 
Kangaroo </p>
<p>Fishing Derby </p>
<p>Skiing 
Amidar </p>
<p>Bank Heist 
Tutankham 
Riverraid 
Bowling </p>
<p>Ms Pacman </p>
<p>Berzerk 
Pitfall 
Freeway 
Pong </p>
<p>Battle Zone 
Private Eye 
Solaris 
Montezuma Rev. 
Alien 
Seaquest 
Boxing 
Hero 
Tennis 
Enduro </p>
<p>Chopper Com. 
Venture </p>
<p>Space Invaders 
Double Dunk 
Star Gunner 
Video Pinball </p>
<p>100 </p>
<p>10 </p>
<p>0 </p>
<p>10 </p>
<p>100 </p>
<p>DreamerV2 vs IQN </p>
<p>James Bond 
Up N Down </p>
<p>Krull 
Assault 
Gopher </p>
<p>Demon Attack 
Road Runner 
Time Pilot 
Atlantis 
Breakout 
Asterix 
Phoenix 
Qbert 
Zaxxon </p>
<p>Ice Hockey 
Yars Revenge 
Kung Fu Master </p>
<p>Skiing 
Robotank </p>
<p>Wizard Of Wor 
Name This Game </p>
<p>Tennis 
Asteroids 
Gravitar </p>
<p>Beam Rider 
Frostbite </p>
<p>Crazy Climber 
Centipede 
Kangaroo </p>
<p>Fishing Derby 
Ms Pacman 
Tutankham 
Alien </p>
<p>Bank Heist 
Bowling 
Amidar </p>
<p>Battle Zone </p>
<p>Pitfall 
Freeway 
Berzerk 
Pong 
Seaquest 
Solaris 
Montezuma Rev. 
Private Eye 
Riverraid 
Boxing 
Enduro 
Hero </p>
<p>Space Invaders </p>
<p>Venture </p>
<p>Chopper Com. 
Double Dunk 
Star Gunner 
Video Pinball </p>
<p>100 </p>
<p>10 </p>
<p>0 </p>
<p>10 </p>
<p>100 </p>
<p>DreamerV2 vs Rainbow </p>
<p>James Bond 
Up N Down </p>
<p>Assault </p>
<p>Demon Attack </p>
<p>Krull 
Gopher </p>
<p>Road Runner 
Time Pilot 
Atlantis </p>
<p>Double Dunk 
Asterix 
Phoenix 
Qbert 
Zaxxon 
Breakout </p>
<p>Yars Revenge 
Ice Hockey 
Wizard Of Wor 
Kangaroo 
Robotank </p>
<p>Kung Fu Master </p>
<p>Frostbite </p>
<p>Crazy Climber 
Fishing Derby </p>
<p>Skiing 
Gravitar 
Boxing 
Asteroids </p>
<p>Beam Rider 
Amidar </p>
<p>Bank Heist 
Enduro 
Centipede </p>
<p>Battle Zone </p>
<p>Name This Game 
Ms Pacman </p>
<p>Alien 
Riverraid 
Tutankham 
Bowling 
Berzerk 
Pong 
Pitfall 
Freeway 
Private Eye 
Solaris 
Montezuma Rev. </p>
<p>Hero </p>
<p>Chopper Com. 
Tennis 
Seaquest </p>
<p>Space Invaders </p>
<p>Venture </p>
<p>Star Gunner 
Video Pinball </p>
<p>100 </p>
<p>10 </p>
<p>0 </p>
<p>10 </p>
<p>100 </p>
<p>DreamerV2 vs C51 </p>
<p>James Bond 
Up N Down </p>
<p>Assault </p>
<p>Demon Attack </p>
<p>Krull 
Gopher </p>
<p>Road Runner 
Time Pilot 
Double Dunk 
Asterix 
Atlantis 
Breakout 
Phoenix 
Qbert 
Zaxxon </p>
<p>Ice Hockey 
Frostbite </p>
<p>Yars Revenge 
Wizard Of Wor 
Crazy Climber 
Robotank </p>
<p>Kung Fu Master 
Name This Game 
Fishing Derby 
Enduro 
Boxing 
Gravitar 
Tutankham 
Tennis 
Centipede 
Asteroids 
Kangaroo 
Amidar </p>
<p>Beam Rider 
Bank Heist 
Battle Zone 
Skiing </p>
<p>Space Invaders 
Ms Pacman 
Riverraid 
Freeway 
Alien 
Hero 
Seaquest 
Bowling 
Berzerk 
Pong 
Private Eye 
Chopper Com. 
Montezuma Rev. 
Pitfall 
Venture 
Solaris </p>
<p>Star Gunner 
Video Pinball </p>
<p>100 </p>
<p>10 </p>
<p>0 </p>
<p>10 </p>
<p>100 </p>
<p>DreamerV2 vs DQN </p>
<p>https://danijar.com/dreamerv2
Acknowledgements We thank our anonymous reviewers for their feedback and Nick Rhinehart for an insightful discussion about the potential benefits of categorical latent variables.No Layer Norm Random DataFigure J.1: Comparison of DreamerV2 to a version without layer norm in the GRU and to training from experience collected over time by a uniform random policy. We find that the benefit of layer norm depends on the task at hand, increasing and decreasing performance on a roughly equal number of tasks. The comparison to random data collection highlights which of the tasks require non-trivial exploration, which can help guide future work on directed exploration using world models.Table K.1: Atari individual scores. We select the 55 games that are common among most papers in the literature. We compare the algorithms DreamerV2, IQN, and Rainbow to the baselines of random actions, DeepMind's human gamer, and the human world record. Algorithm scores are highlighted in bold when they fall within 5% of the best algorithm. Note that these scores are already averaged across seeds, whereas any aggregated scores must be computed before averaging across seeds.
Stochastic Variational Video Prediction. M Babaeizadeh, D Finn, R H Erhan, S Campbell, Levine, ArXiv:1710.11252ArXiv PreprintM Babaeizadeh, C Finn, D Erhan, RH Campbell, S Levine. Stochastic Variational Video Prediction. ArXiv Preprint ArXiv:1710.11252, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
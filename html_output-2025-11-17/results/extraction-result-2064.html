<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2064 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2064</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2064</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-281315804</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.10740v1.pdf" target="_blank">How are Scientific Concepts Birthed? Typing Rules of Concept Formation in Theoretical Physics Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> This work aims to formalize some of the ways scientific concepts are formed in the process of theoretical physics discovery. Since this may at first seem like a task beyond the scope of the exact sciences (natural and formal sciences), we begin by presenting arguments for why scientific concept formation can be formalized. Then, we introduce type theory as a natural and well-suited framework for this formalization. We formalize what we call"ways of discovering new concepts"including concept distinction, property preservation, and concept change, as cognitive typing rules. Next, we apply these cognitive typing rules to two case studies of conceptual discovery in the history of physics: Einstein's reasoning leading to the impossibility of frozen waves, and his conceptual path to the relativity of time. In these historical episodes, we recast what a physicist might informally call"ways of discovering new scientific concepts"as compositional typing rules built from cognitive typing rules - thus formalizing them as scientific discovery mechanisms. Lastly, we computationally model the type-theoretic reconstruction of Einstein's conceptual path to the relativity of time as a program synthesis task.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2064.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2064.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EinsteinProgramSynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Type-theoretic Program Synthesis Implementation (authors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python implementation of the paper's type-theoretic formalism that represents terms as strings, types as Python classes, contexts as sets, and implements cognitive typing rules as Python primitives; used to recover a ground-truth judgment reconstructing Einstein's conceptual path to the relativity of time via program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Einstein program synthesis (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neurosymbolic program-synthesis system (Python implementation of typed primitives + search policies)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>theoretical physics / cognitive modeling of concept formation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>typed program-derivations / judgments (terms paired with types)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>reconstructive / in-distribution (aims to recover a supplied ground-truth judgment rather than produce unconstrained novel discoveries)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Search over sequences (8-step sequences in experiments) of higher-level primitives (algebraic simplification, function application, cognitive typing rules) guided by one of three policies (Enumerative, Bayesian Thompson-sampling over Beta posteriors per primitive, or a Bayes-Neural hybrid policy trained on Bayesian traces).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Exact-match comparison to an oracle-provided ground-truth judgment (matching both term expression and assigned type); success recorded when a generated judgment equals the target.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported as success-rate curves (fraction of attempted programs that reached the target) versus program index, averaged over N=150 independent runs; Pure-Bayes reached 100% success (program index not numerically specified in text), Bayes-Neural most sample-efficient in the unbiased case and near-top when inductive biases are enabled, Enumeration performed worst. No absolute runtimes provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured implicitly via the success-rate curves (fraction of runs that produced the exact ground-truth judgment); specific numeric validation-accuracy/precision/recall values are not reported beyond the plotted success curves and statements about reaching 100% success for Pure-Bayes under some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not directly evaluated: experiments reconstruct an in-distribution ground-truth; authors note the current goal-match validation is narrow and that future work should allow goals defined by property-satisfaction (which would allow more novel outputs to be considered valid). No quantitative relationship between novelty and validation accuracy is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The paper highlights an asymmetry: generation explores many candidate program sequences (guided by priors, biases, and learned policies) while validation is strict (oracle exact-match). Authors discuss that this exact-match validation can be brittle for novel outputs and propose softer goals and human-like heuristics for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Partially present at the search-primitive level: Bayesian search maintains a Beta(α,β) posterior per admissible primitive (interpreted as a belief about that primitive's chance of moving closer to the goal) and updates it with goal rewards; Bayes-Neural mixes a learned policy with Thompson-sampled Beta draws. No calibrated uncertainty estimates about final generated judgments are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; no calibration metrics for confidence of final outputs are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not evaluated; experiments focus on recovering a known in-distribution target (Einstein judgment). Authors discuss desiderata for evaluating more novel/out-of-distribution discoveries in future work but provide no metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the search uses proxy goal measures (term similarity and type agreement) to score candidate steps and update posteriors; these act as proxies for successful conceptual matches rather than direct empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>experiment used oracle exact-match (no human-in-the-loop); authors recommend human assessment for broader, conceptual applications but do not quantify frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal like theoretical physics (type-theoretic formalization of conceptual steps); authors emphasize that this semi-formal domain permits symbolic/programmatic representation but is not as rigid as mathematical theorem proving.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>(1) Formalize concepts/types and discovery mechanisms to make generated concepts checkable; (2) use inductive biases and higher-level primitives to reduce combinatorial search; (3) combine Bayesian search with a neural policy (Bayes-Neural hybrid) to improve sample efficiency; (4) propose softer goal specifications (property-satisfaction) and learnable heuristics in future work to reduce brittle validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors argue that previous AI-for-science systems focus on solving predefined problems (optimization) and not on generating new problems/concepts; they critique black-box systems (e.g., AI Scientist) for producing natural-language outputs with no formal representation, increasing risk of accepting flawed concepts. In their implementation, validation is an exact-match oracle, showing a narrow validation regime relative to a broad generative search.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Within their controlled reconstruction task, search-guided generation combined with strict oracle validation successfully recovers the historical judgment (Pure-Bayes reached 100% success), suggesting that for well-specified reconstruction tasks generation and validation can be brought into alignment; however, this does not contradict the general generation-validation gap for genuinely novel, unconstrained discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported quantitatively; paper reports sample-efficiency differences (number of programs run until success) between search methods but does not provide CPU/time cost nor a ratio of validation-to-generation cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2064.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2064.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-Neural</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Neural Hybrid Search (authors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid search policy that trains a neural policy on traces generated by a Bayesian Thompson-sampling search and during search samples primitives from a depth-annealed mixture of the neural policy and Bayesian probabilities to balance exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayes-Neural hybrid search</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neurosymbolic/search policy combining Bayesian Thompson sampling and supervised neural policy</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>program synthesis for concept-formation in theoretical physics (generalizable to program-synthesis tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>sequences of primitives / program traces that yield typed judgments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution reconstruction in experiments; intended to improve sample-efficiency for compositional search (may produce moderately novel compositions in general).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Neural policy trained on Bayesian-generated traces; during search a depth-annealed mixture samples next primitive from neural policy probability and Thompson-sampled Bayesian probability; updating Beta posteriors for primitives with goal reward.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Same as overall system: generated trace is accepted if it produces a judgment exactly matching the oracle ground-truth (term+type).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described as 'most sample-efficient' in the unbiased case and 'near the top' when inductive biases are enabled; success-rate vs program-index curves averaged over N=150 runs shown in Fig. 4. Exact numeric program-index thresholds and medians are not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured via success-rate curves (fraction of runs reaching exact-match goal); specific numerical validation metrics not given beyond plot descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported; experiments target a supplied in-distribution goal so relationship to novelty is not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper highlights Bayes-Neural improves generation (sample efficiency) but validation remains an exact-match oracle; the hybrid improves generation side but does not alter the strictness of validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty over primitives is represented by Beta(α,β) posteriors used for Thompson sampling; neural policy does not provide explicit calibrated uncertainties in the reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses term-similarity and type-agreement as reward proxies to update Beta posteriors and train the neural policy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>no human-in-loop in reported experiments; human assessment suggested for conceptual generalization in discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (typed program fragments modeling theoretical-physics concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Improves generation sample efficiency via learned policy + Bayesian exploration and by using higher-level primitives and inductive biases to constrain search.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors note that better generation (Bayes-Neural) reduces search cost but validation still relies on strict ground-truth matching; they emphasize the need for more flexible validation to handle truly novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None direct — Bayes-Neural reduces generation cost but does not change validation strictness.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2064.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2064.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pure-Bayes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pure Bayesian (Thompson-sampling) Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search procedure that keeps a Beta(α,β) posterior for every admissible primitive (interpreting it as the chance of moving closer to the goal) and selects primitives by Thompson sampling, updating posteriors with a goal measure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pure-Bayes (Bayesian Thompson-sampling search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>symbolic/neurosymbolic search using per-primitive Beta posteriors and Thompson sampling</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>program synthesis for typed-concept reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>program traces / typed judgments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution reconstruction in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Thompson sampling over Beta(α,β) posteriors per primitive; at each step draw θ_p ∼ Beta(α_p,β_p) and select primitive with largest draw; update posterior with goal reward (term similarity/type agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Exact-match comparison to oracle-provided target judgment (term+type).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>In biased-search setting (Fig. 4a) Pure-Bayes achieved highest performance and reached 100% success (program-index threshold not numerically specified); in unbiased setting Bayes-Neural dominated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured indirectly by success-rate curves; specific numeric precision/recall not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Pure-Bayes demonstrates that a well-designed Bayesian search can reach full success on the reconstruction task, but validation remains exact-match and narrow, highlighting a potential brittleness for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes at primitive-selection level via Beta posteriors; no quantified uncertainty about final judgments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses term similarity and type agreement as reward proxies for Bayesian updates.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>none in experiment</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Reduces generation cost by principled Bayesian updating over primitives; authors propose combining with neural policies and inductive biases to further mitigate combinatorial explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors emphasize that even when generation succeeds (Pure-Bayes reaching 100%), validation in real conceptual discovery needs richer criteria than exact-match, so generation success does not automatically imply robust validation for novel concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Successful recovery of the historical judgment (100% success) shows generation can be aligned with validation for well-specified reconstruction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2064.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2064.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (Lu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that uses large language models, prompt engineering, and coding assistants to generate scientific questions in natural language by splitting variables, modifying pathways, or adding components; characterized as a candidate approach to the 'hard' problem of AI for science (generating new problems/concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (Lu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large-language-model-driven system with prompt engineering and code-generation assistants (black-box LLM pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific-hypothesis/question generation (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>natural-language scientific questions/hypotheses/process modifications</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>claims to generate novel scientific questions (potentially moderately novel to highly novel); paper critiques that generation is black-box and lacks formal grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLMs with prompt engineering and coding assistants generate human-interpretable outputs by heuristically splitting variables, modifying processing pathways, or adding model components.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not formally specified in the referenced work as described here; paper criticizes AI Scientist for being a black box and for not providing a formal representation or explicit validation procedures beyond human interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not quantified in this paper; authors note promising natural-language outputs but do not report success metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not quantified; paper warns that lack of formal representation makes precise validation unclear and may lead to ambiguous concepts and acceptance of flawed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported; authors express concern that black-box generation may produce empirically correct but seemingly unreasonable concepts or plausible-sounding but flawed ones, implying validation may degrade with novelty but no metrics are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper explicitly contrasts AI Scientist's strong generative capabilities with its lack of formal validation: generation is natural-language and black-box while validation/formal representation is missing, increasing risk of mistaken acceptance/rejection.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in this paper; no mention of calibrated uncertainty for AI Scientist's generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported here; capable of producing open-ended questions but validation for out-of-distribution novelty is unclear per authors' critique.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Outputs are evaluated primarily by human interpretability; no formal proxy metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Implied frequent/human-in-the-loop validation recommended because outputs are natural-language and lack formal representation; exact frequency not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>varies; outputs are natural-language hypotheses lacking formal symbolic grounding per critique (empirical / informal).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Authors propose type-theoretic formalisms and formal representations as a way to mitigate black-box generation and improve validation; they suggest that formal types and typing rules could complement or replace purely natural-language outputs to close the generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors argue AI Scientist exemplifies the generation-validation gap: it generates human-interpretable questions but offers no formal account of their derivation or formal representations for precise validation, risking acceptance of flawed or rejection of reasonable concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided in this paper; paper treats AI Scientist as promising for generation but deficient for formal validation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2064.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2064.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbolicRegression_AI-Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic regression methods (e.g., AI Feynman, AI Feynman 2.0, Udrescu & Tegmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of automated systems (symbolic regression and physics-inspired methods) that infer closed-form mathematical expressions from data; cited here as successful at rediscovering physical laws and scalars but limited with respect to concept formation which requires defining new problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Feynman: A Physics-Inspired Method for Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic regression systems (AI Feynman and variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>symbolic-regression algorithms (physics-informed heuristics, sometimes augmented by ML)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics / discovery of symbolic laws and conserved quantities</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic equations, scalar quantities, functional forms</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>typically incremental to moderately novel within the space of mathematical expressions consistent with data; primarily designed to rediscover or fit laws from data rather than invent new conceptual types.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Search/heuristic-driven compositional construction of symbolic expressions from primitives (algebraic operators, function templates) often guided by physical constraints and simplification heuristics; deep-learning variants accelerate search or propose candidate forms.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compare discovered symbolic expressions to data (fit metrics) and physical plausibility; prior work uses held-out data, Pareto-front analysis of complexity vs fit, and domain-expert assessment. In this paper symbolic-regression methods are referenced but their validation protocols are not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not specified in this paper beyond general statements that symbolic-regression approaches have 'vastly improved scalability' and rediscovered meaningful physical objects; no concrete metrics provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not discussed here; authors note symbolic regression solves well-defined optimization problems but struggles to generate new questions/concepts (hard problem).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper positions symbolic regression as strong at generating candidate equations for well-defined problems but limited at conceptual-level generation; validation for fitted laws is straightforward (data fit) whereas conceptual novelty (new types) lacks clear validation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed here for these referenced methods.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not assessed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Referenced symbolic regression literature typically uses goodness-of-fit and parsimony (complexity) as proxies; this paper notes such methods focus on optimizing solutions to predefined problems rather than inventing new problems.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>highly formal for mathematical expressions (in-distribution equation discovery); however, concept-level novelty is less formal and not well-addressed by these methods.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Authors suggest complementing symbolic-regression-style equation discovery with a type-theoretic formalism to represent and validate concept formation, and propose program synthesis with higher-level primitives and inductive biases to tackle compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors cite Battleday & Gershman (2024) and others arguing that these methods are built to solve predefined problems and therefore fail to address the hard problem of generating new conceptual scientific questions; thus generation of equations can outpace validation of conceptual novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Symbolic regression's success at rediscovering known laws shows that for well-posed problems generation and validation (via data fit) can be well-aligned, but this does not address conceptual novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>AI Feynman: A Physics-Inspired Method for Symbolic Regression <em>(Rating: 2)</em></li>
                <li>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity <em>(Rating: 2)</em></li>
                <li>Distilling Free-Form Natural Laws from Experimental Data <em>(Rating: 1)</em></li>
                <li>A Transformer Model for Symbolic Regression towards Scientific Discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2064",
    "paper_id": "paper-281315804",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "EinsteinProgramSynthesis",
            "name_full": "Type-theoretic Program Synthesis Implementation (authors)",
            "brief_description": "A Python implementation of the paper's type-theoretic formalism that represents terms as strings, types as Python classes, contexts as sets, and implements cognitive typing rules as Python primitives; used to recover a ground-truth judgment reconstructing Einstein's conceptual path to the relativity of time via program synthesis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Einstein program synthesis (this work)",
            "system_type": "neurosymbolic program-synthesis system (Python implementation of typed primitives + search policies)",
            "scientific_domain": "theoretical physics / cognitive modeling of concept formation",
            "output_type": "typed program-derivations / judgments (terms paired with types)",
            "novelty_level": "reconstructive / in-distribution (aims to recover a supplied ground-truth judgment rather than produce unconstrained novel discoveries)",
            "generation_method": "Search over sequences (8-step sequences in experiments) of higher-level primitives (algebraic simplification, function application, cognitive typing rules) guided by one of three policies (Enumerative, Bayesian Thompson-sampling over Beta posteriors per primitive, or a Bayes-Neural hybrid policy trained on Bayesian traces).",
            "validation_method": "Exact-match comparison to an oracle-provided ground-truth judgment (matching both term expression and assigned type); success recorded when a generated judgment equals the target.",
            "generation_performance": "Reported as success-rate curves (fraction of attempted programs that reached the target) versus program index, averaged over N=150 independent runs; Pure-Bayes reached 100% success (program index not numerically specified in text), Bayes-Neural most sample-efficient in the unbiased case and near-top when inductive biases are enabled, Enumeration performed worst. No absolute runtimes provided.",
            "validation_performance": "Measured implicitly via the success-rate curves (fraction of runs that produced the exact ground-truth judgment); specific numeric validation-accuracy/precision/recall values are not reported beyond the plotted success curves and statements about reaching 100% success for Pure-Bayes under some conditions.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not directly evaluated: experiments reconstruct an in-distribution ground-truth; authors note the current goal-match validation is narrow and that future work should allow goals defined by property-satisfaction (which would allow more novel outputs to be considered valid). No quantitative relationship between novelty and validation accuracy is reported.",
            "generation_validation_comparison": "The paper highlights an asymmetry: generation explores many candidate program sequences (guided by priors, biases, and learned policies) while validation is strict (oracle exact-match). Authors discuss that this exact-match validation can be brittle for novel outputs and propose softer goals and human-like heuristics for future work.",
            "uncertainty_quantification": "Partially present at the search-primitive level: Bayesian search maintains a Beta(α,β) posterior per admissible primitive (interpreted as a belief about that primitive's chance of moving closer to the goal) and updates it with goal rewards; Bayes-Neural mixes a learned policy with Thompson-sampled Beta draws. No calibrated uncertainty estimates about final generated judgments are reported.",
            "calibration_quality": "Not reported; no calibration metrics for confidence of final outputs are provided.",
            "out_of_distribution_performance": "Not evaluated; experiments focus on recovering a known in-distribution target (Einstein judgment). Authors discuss desiderata for evaluating more novel/out-of-distribution discoveries in future work but provide no metrics.",
            "validation_proxy_metrics": "Yes — the search uses proxy goal measures (term similarity and type agreement) to score candidate steps and update posteriors; these act as proxies for successful conceptual matches rather than direct empirical validation.",
            "human_validation_required": false,
            "human_validation_frequency": "experiment used oracle exact-match (no human-in-the-loop); authors recommend human assessment for broader, conceptual applications but do not quantify frequency.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal like theoretical physics (type-theoretic formalization of conceptual steps); authors emphasize that this semi-formal domain permits symbolic/programmatic representation but is not as rigid as mathematical theorem proving.",
            "gap_mitigation_strategies": "(1) Formalize concepts/types and discovery mechanisms to make generated concepts checkable; (2) use inductive biases and higher-level primitives to reduce combinatorial search; (3) combine Bayesian search with a neural policy (Bayes-Neural hybrid) to improve sample efficiency; (4) propose softer goal specifications (property-satisfaction) and learnable heuristics in future work to reduce brittle validation.",
            "evidence_supporting_gap": "Authors argue that previous AI-for-science systems focus on solving predefined problems (optimization) and not on generating new problems/concepts; they critique black-box systems (e.g., AI Scientist) for producing natural-language outputs with no formal representation, increasing risk of accepting flawed concepts. In their implementation, validation is an exact-match oracle, showing a narrow validation regime relative to a broad generative search.",
            "evidence_contradicting_gap": "Within their controlled reconstruction task, search-guided generation combined with strict oracle validation successfully recovers the historical judgment (Pure-Bayes reached 100% success), suggesting that for well-specified reconstruction tasks generation and validation can be brought into alignment; however, this does not contradict the general generation-validation gap for genuinely novel, unconstrained discovery.",
            "computational_cost_ratio": "Not reported quantitatively; paper reports sample-efficiency differences (number of programs run until success) between search methods but does not provide CPU/time cost nor a ratio of validation-to-generation cost.",
            "uuid": "e2064.0"
        },
        {
            "name_short": "Bayes-Neural",
            "name_full": "Bayes-Neural Hybrid Search (authors)",
            "brief_description": "A hybrid search policy that trains a neural policy on traces generated by a Bayesian Thompson-sampling search and during search samples primitives from a depth-annealed mixture of the neural policy and Bayesian probabilities to balance exploration and exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Bayes-Neural hybrid search",
            "system_type": "neurosymbolic/search policy combining Bayesian Thompson sampling and supervised neural policy",
            "scientific_domain": "program synthesis for concept-formation in theoretical physics (generalizable to program-synthesis tasks)",
            "output_type": "sequences of primitives / program traces that yield typed judgments",
            "novelty_level": "in-distribution reconstruction in experiments; intended to improve sample-efficiency for compositional search (may produce moderately novel compositions in general).",
            "generation_method": "Neural policy trained on Bayesian-generated traces; during search a depth-annealed mixture samples next primitive from neural policy probability and Thompson-sampled Bayesian probability; updating Beta posteriors for primitives with goal reward.",
            "validation_method": "Same as overall system: generated trace is accepted if it produces a judgment exactly matching the oracle ground-truth (term+type).",
            "generation_performance": "Described as 'most sample-efficient' in the unbiased case and 'near the top' when inductive biases are enabled; success-rate vs program-index curves averaged over N=150 runs shown in Fig. 4. Exact numeric program-index thresholds and medians are not provided in text.",
            "validation_performance": "Measured via success-rate curves (fraction of runs reaching exact-match goal); specific numerical validation metrics not given beyond plot descriptions.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported; experiments target a supplied in-distribution goal so relationship to novelty is not evaluated.",
            "generation_validation_comparison": "Paper highlights Bayes-Neural improves generation (sample efficiency) but validation remains an exact-match oracle; the hybrid improves generation side but does not alter the strictness of validation.",
            "uncertainty_quantification": "Uncertainty over primitives is represented by Beta(α,β) posteriors used for Thompson sampling; neural policy does not provide explicit calibrated uncertainties in the reported results.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not evaluated.",
            "validation_proxy_metrics": "Uses term-similarity and type-agreement as reward proxies to update Beta posteriors and train the neural policy.",
            "human_validation_required": false,
            "human_validation_frequency": "no human-in-loop in reported experiments; human assessment suggested for conceptual generalization in discussion.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal (typed program fragments modeling theoretical-physics concepts).",
            "gap_mitigation_strategies": "Improves generation sample efficiency via learned policy + Bayesian exploration and by using higher-level primitives and inductive biases to constrain search.",
            "evidence_supporting_gap": "Authors note that better generation (Bayes-Neural) reduces search cost but validation still relies on strict ground-truth matching; they emphasize the need for more flexible validation to handle truly novel outputs.",
            "evidence_contradicting_gap": "None direct — Bayes-Neural reduces generation cost but does not change validation strictness.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2064.1"
        },
        {
            "name_short": "Pure-Bayes",
            "name_full": "Pure Bayesian (Thompson-sampling) Search",
            "brief_description": "A search procedure that keeps a Beta(α,β) posterior for every admissible primitive (interpreting it as the chance of moving closer to the goal) and selects primitives by Thompson sampling, updating posteriors with a goal measure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Pure-Bayes (Bayesian Thompson-sampling search)",
            "system_type": "symbolic/neurosymbolic search using per-primitive Beta posteriors and Thompson sampling",
            "scientific_domain": "program synthesis for typed-concept reconstruction",
            "output_type": "program traces / typed judgments",
            "novelty_level": "in-distribution reconstruction in experiments",
            "generation_method": "Thompson sampling over Beta(α,β) posteriors per primitive; at each step draw θ_p ∼ Beta(α_p,β_p) and select primitive with largest draw; update posterior with goal reward (term similarity/type agreement).",
            "validation_method": "Exact-match comparison to oracle-provided target judgment (term+type).",
            "generation_performance": "In biased-search setting (Fig. 4a) Pure-Bayes achieved highest performance and reached 100% success (program-index threshold not numerically specified); in unbiased setting Bayes-Neural dominated.",
            "validation_performance": "Measured indirectly by success-rate curves; specific numeric precision/recall not reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not evaluated.",
            "generation_validation_comparison": "Pure-Bayes demonstrates that a well-designed Bayesian search can reach full success on the reconstruction task, but validation remains exact-match and narrow, highlighting a potential brittleness for novel outputs.",
            "uncertainty_quantification": "Yes at primitive-selection level via Beta posteriors; no quantified uncertainty about final judgments reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not evaluated.",
            "validation_proxy_metrics": "Uses term similarity and type agreement as reward proxies for Bayesian updates.",
            "human_validation_required": false,
            "human_validation_frequency": "none in experiment",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal",
            "gap_mitigation_strategies": "Reduces generation cost by principled Bayesian updating over primitives; authors propose combining with neural policies and inductive biases to further mitigate combinatorial explosion.",
            "evidence_supporting_gap": "Authors emphasize that even when generation succeeds (Pure-Bayes reaching 100%), validation in real conceptual discovery needs richer criteria than exact-match, so generation success does not automatically imply robust validation for novel concepts.",
            "evidence_contradicting_gap": "Successful recovery of the historical judgment (100% success) shows generation can be aligned with validation for well-specified reconstruction tasks.",
            "computational_cost_ratio": "Not provided.",
            "uuid": "e2064.2"
        },
        {
            "name_short": "AI Scientist",
            "name_full": "The AI Scientist (Lu et al., 2024)",
            "brief_description": "A referenced system that uses large language models, prompt engineering, and coding assistants to generate scientific questions in natural language by splitting variables, modifying pathways, or adding components; characterized as a candidate approach to the 'hard' problem of AI for science (generating new problems/concepts).",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (Lu et al., 2024)",
            "system_type": "large-language-model-driven system with prompt engineering and code-generation assistants (black-box LLM pipeline)",
            "scientific_domain": "general scientific-hypothesis/question generation (cross-domain)",
            "output_type": "natural-language scientific questions/hypotheses/process modifications",
            "novelty_level": "claims to generate novel scientific questions (potentially moderately novel to highly novel); paper critiques that generation is black-box and lacks formal grounding.",
            "generation_method": "LLMs with prompt engineering and coding assistants generate human-interpretable outputs by heuristically splitting variables, modifying processing pathways, or adding model components.",
            "validation_method": "Not formally specified in the referenced work as described here; paper criticizes AI Scientist for being a black box and for not providing a formal representation or explicit validation procedures beyond human interpretability.",
            "generation_performance": "Not quantified in this paper; authors note promising natural-language outputs but do not report success metrics here.",
            "validation_performance": "Not quantified; paper warns that lack of formal representation makes precise validation unclear and may lead to ambiguous concepts and acceptance of flawed outputs.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported; authors express concern that black-box generation may produce empirically correct but seemingly unreasonable concepts or plausible-sounding but flawed ones, implying validation may degrade with novelty but no metrics are provided.",
            "generation_validation_comparison": "Paper explicitly contrasts AI Scientist's strong generative capabilities with its lack of formal validation: generation is natural-language and black-box while validation/formal representation is missing, increasing risk of mistaken acceptance/rejection.",
            "uncertainty_quantification": "Not described in this paper; no mention of calibrated uncertainty for AI Scientist's generated hypotheses.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported here; capable of producing open-ended questions but validation for out-of-distribution novelty is unclear per authors' critique.",
            "validation_proxy_metrics": "Outputs are evaluated primarily by human interpretability; no formal proxy metrics reported in this paper.",
            "human_validation_required": true,
            "human_validation_frequency": "Implied frequent/human-in-the-loop validation recommended because outputs are natural-language and lack formal representation; exact frequency not specified.",
            "formal_verification_used": null,
            "domain_formalization_level": "varies; outputs are natural-language hypotheses lacking formal symbolic grounding per critique (empirical / informal).",
            "gap_mitigation_strategies": "Authors propose type-theoretic formalisms and formal representations as a way to mitigate black-box generation and improve validation; they suggest that formal types and typing rules could complement or replace purely natural-language outputs to close the generation-validation gap.",
            "evidence_supporting_gap": "Authors argue AI Scientist exemplifies the generation-validation gap: it generates human-interpretable questions but offers no formal account of their derivation or formal representations for precise validation, risking acceptance of flawed or rejection of reasonable concepts.",
            "evidence_contradicting_gap": "None provided in this paper; paper treats AI Scientist as promising for generation but deficient for formal validation.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2064.3"
        },
        {
            "name_short": "SymbolicRegression_AI-Feynman",
            "name_full": "Symbolic regression methods (e.g., AI Feynman, AI Feynman 2.0, Udrescu & Tegmark)",
            "brief_description": "A class of automated systems (symbolic regression and physics-inspired methods) that infer closed-form mathematical expressions from data; cited here as successful at rediscovering physical laws and scalars but limited with respect to concept formation which requires defining new problems.",
            "citation_title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression",
            "mention_or_use": "mention",
            "system_name": "Symbolic regression systems (AI Feynman and variants)",
            "system_type": "symbolic-regression algorithms (physics-informed heuristics, sometimes augmented by ML)",
            "scientific_domain": "physics / discovery of symbolic laws and conserved quantities",
            "output_type": "symbolic equations, scalar quantities, functional forms",
            "novelty_level": "typically incremental to moderately novel within the space of mathematical expressions consistent with data; primarily designed to rediscover or fit laws from data rather than invent new conceptual types.",
            "generation_method": "Search/heuristic-driven compositional construction of symbolic expressions from primitives (algebraic operators, function templates) often guided by physical constraints and simplification heuristics; deep-learning variants accelerate search or propose candidate forms.",
            "validation_method": "Compare discovered symbolic expressions to data (fit metrics) and physical plausibility; prior work uses held-out data, Pareto-front analysis of complexity vs fit, and domain-expert assessment. In this paper symbolic-regression methods are referenced but their validation protocols are not detailed.",
            "generation_performance": "Not specified in this paper beyond general statements that symbolic-regression approaches have 'vastly improved scalability' and rediscovered meaningful physical objects; no concrete metrics provided here.",
            "validation_performance": "Not specified in this paper.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not discussed here; authors note symbolic regression solves well-defined optimization problems but struggles to generate new questions/concepts (hard problem).",
            "generation_validation_comparison": "Paper positions symbolic regression as strong at generating candidate equations for well-defined problems but limited at conceptual-level generation; validation for fitted laws is straightforward (data fit) whereas conceptual novelty (new types) lacks clear validation procedures.",
            "uncertainty_quantification": "Not discussed here for these referenced methods.",
            "calibration_quality": "Not discussed here.",
            "out_of_distribution_performance": "Not assessed in this paper.",
            "validation_proxy_metrics": "Referenced symbolic regression literature typically uses goodness-of-fit and parsimony (complexity) as proxies; this paper notes such methods focus on optimizing solutions to predefined problems rather than inventing new problems.",
            "human_validation_required": null,
            "human_validation_frequency": "Not reported in this paper.",
            "formal_verification_used": null,
            "domain_formalization_level": "highly formal for mathematical expressions (in-distribution equation discovery); however, concept-level novelty is less formal and not well-addressed by these methods.",
            "gap_mitigation_strategies": "Authors suggest complementing symbolic-regression-style equation discovery with a type-theoretic formalism to represent and validate concept formation, and propose program synthesis with higher-level primitives and inductive biases to tackle compositionality.",
            "evidence_supporting_gap": "Authors cite Battleday & Gershman (2024) and others arguing that these methods are built to solve predefined problems and therefore fail to address the hard problem of generating new conceptual scientific questions; thus generation of equations can outpace validation of conceptual novelty.",
            "evidence_contradicting_gap": "Symbolic regression's success at rediscovering known laws shows that for well-posed problems generation and validation (via data fit) can be well-aligned, but this does not address conceptual novelty.",
            "computational_cost_ratio": "Not provided in this paper.",
            "uuid": "e2064.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression",
            "rating": 2
        },
        {
            "paper_title": "AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity",
            "rating": 2
        },
        {
            "paper_title": "Distilling Free-Form Natural Laws from Experimental Data",
            "rating": 1
        },
        {
            "paper_title": "A Transformer Model for Symbolic Regression towards Scientific Discovery",
            "rating": 1
        }
    ],
    "cost": 0.0194375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How are Scientific Concepts Birthed? Typing Rules of Concept Formation in Theoretical Physics Reasoning
12 Sep 2025</p>
<p>Omar Aguilar 
Physics Department
University of California
1156 High Street95064Santa Cruz, Santa CruzCAUSA</p>
<p>Anthony Aguirre 
Physics Department
University of California
1156 High Street95064Santa Cruz, Santa CruzCAUSA</p>
<p>How are Scientific Concepts Birthed? Typing Rules of Concept Formation in Theoretical Physics Reasoning
12 Sep 20254FAFB3AEA156EAE4AEB31101D71D94D6arXiv:2509.10740v1[physics.hist-ph]Scientific discoveryArtificial IntelligenceCognitive ScienceProgram Synthesis
This work aims to formalize some of the ways scientific concepts are formed in the process of theoretical physics discovery.Since this may at first seem like a task beyond the scope of the exact sciences (natural and formal sciences), we begin by presenting arguments for why scientific concept formation can be formalized.Then, we introduce type theory as a natural and well-suited framework for this formalization.We formalize what we call "ways of discovering new concepts" including concept distinction, property preservation, and concept change, as cognitive typing rules.Next, we apply these cognitive typing rules to two case studies of conceptual discovery in the history of physics: Einstein's reasoning leading to the impossibility of frozen waves, and his conceptual path to the relativity of time.In these historical episodes, we recast what a physicist might informally call "ways of discovering new scientific concepts" as compositional typing rules built from cognitive typing rules-thus formalizing them as scientific discovery mechanisms.Lastly, we computationally model the type-theoretic reconstruction of Einstein's conceptual path to the relativity of time as a program synthesis task.</p>
<p>Introduction</p>
<p>"Often the right questions are very simple.They invite us to become surprised about perfectly ordinary things.Things that we have taken for granted."-Noam Chomsky What is the scientific method?At first glance this question might seem perfectly ordinary.After all, we are introduced to it in middle school.We are taught that it is the instrument that enables science to be possible.It consists of four steps: observing the natural world, formulating a hypothesis to explain these observations, testing the hypothesis against evidence to determine its validity, and repeating the process as needed.Yet, as soon as one embarks on scientific research in college or graduate school, it becomes apparent that this answer is profoundly incomplete.It leaves a fundamental preceding question unaddressed: How does one arrive at a hypothesis in the first place?or more broadly:</p>
<p>How does one create a scientific concept?</p>
<p>Faced with this conundrum, a physicist may respond in one of two broad ways.They may say that the act of creating scientific concepts is subjective and almost mystical, hence inaccessible to rational analysis-and so they simply stop there, because there's nothing left to explain.Or, they may acknowledge that scientific concept formation is at the very least partially subjective, but still try to make sense of it-by reflecting on how they and other scientists create concepts in their natural creative environments, namely conversations and internal monologues, and then finding patterns in those reflections.If they take the second choice, they might recall learning early on that Newton imagined the moon as a falling apple.They might also recall learning in class that one of Einstein's first steps toward special relativity was his refusal to abandon the constancy of the speed of light.Once equipped with these canonical results in fundamental physics, they may apply their training to frontier questions in the physics of complex systems and ask-while searching for a measure of complexity-"What would an entropy-like measure look like if, instead of always increasing in a closed system, it rose to a peak and then declined?"(Aaronson, Carroll, &amp; Ouellette, 2014;Crutchfield, 1994).If one abstracts from these memories, a common thread emerges: they all involve the imposition of constraints.This, in essence, was the first concrete answer in recorded history to the question of how scientific concepts are generated-offered by the 19th century philosopher and polymath C.S. Peirce.</p>
<p>Peirce proposed that forming scientific concepts is abduction (Chomsky, 2006)-the process of imposing constraints on admissible hypotheses to generate plausible, though fallible, explanations2 .A Peircean notion of a scientific concept follows from this view: it is an entity that satisfies the constraints of the process through which it is discovered.As the 20th century began, however, interest in how scientific concepts are formed-and in defining them in terms of their formation-declined within mainstream philosophy of science, as scientific concepts came to be treated solely as ideal entities waiting to be discovered (Feigl, 1970).</p>
<p>A small resurgence of interest in scientific concept formation began in the second half of the century led by small, scattered groups of philosophers who drew on insights from the history of science (Hanson, 1958;Kuhn, 1962) and, by the 1980s, from cognitive science (Gentner, 1983;Nersessian, 1987;Thagard, 1988) as well.These combined insights gave rise to a newly established interdisciplinary field dedicated to the study of scientific cognition: the cognitive science of science (Gorman, Tweney, Gooding, &amp; Kincannon, 2005;Magnani &amp; Li, 2007;Thagard, 2012).Like Peirce, researchers in this field conceived scientific concepts in terms of how they were discovered, but this time their definition was much more precise: they treated concepts as entities shaped by the constraints of the knowledge-construction practices scientists use to generate them-such as analogies and thought experiments (Nersessian, 2008).</p>
<p>Perhaps the most detailed engagement between the cognitive science of science and theoretical physics is found in the work of Nancy Nersessian, who developed a method for studying scientific concept formation known as cognitive-historical analysis (Nersessian, 1987).The historical component of her method treats detailed records of theoretical physics practice-such as notebooks, diaries, correspondence, drafts, and publications-as empirical data and subjects them to philosophical analysis.Meanwhile, the cognitive component treats these practices as traces of cognitive mechanisms-analogy, mental modeling, and thought experiments-that allow us move from specific to general conclusions about the nature of scientific concept formation.In effect, Nersessian conceives scientific concepts as entities that satisfy the constraints imposed by discovery methods used to generate them-such as the method of physical analogy employed by Maxwell (Nersessian, 2002).</p>
<p>Since discovery methods like physical analogy are systematic procedures with a definite and finite number of steps, Nersessian's framework gives rise to the first central question of this paper:</p>
<p>Can concept formation in theoretical physics be formalized?</p>
<p>To address this question, one may be inclined to explore machine learning approaches -such as graph neural networks (Battaglia et al., 2018), symbolic regression (Udrescu et al., 2020), and transformers (Lalande et al., 2023) -that have successfully automated other aspects of scientific discovery, with the aim of adapting these techniques to model scientific concept formation.However, as discussed in (Battleday &amp; Gershman, 2024) and Section 3, these methods still face the challenge of modeling conceptual development, as they were built to solve predefined problems rather than define new ones.This suggests that we should take a step back and further clarify scientific concept formation and scientific concepts-by clarifying what has so far been left vague: what we mean by constraints.</p>
<p>As part of this effort, physicists might recall that when searching for new scientific concepts, they often use the word "type" or "kind" informally to describe what they are looking for.For instance, when trying to define a thermodynamic entropy for non-equilibrium systems ( Šafránek, Aguirre, Schindler, &amp; Deutsch, 2021), they might ask, "What type of entropy would count as thermodynamic, yet increase continuously rather than abruptly as the system transitions from one equilibrium configuration (a closed box) to another (an open box after a long time)?" ( Šafránek, Aguirre, &amp; Deutsch, 2020).In some cases, physicists even explicitly use the words "type" or "kind" in a colloquial sense when describing their discovery process in papers-just as Maxwell did when he first conceived of the magnetic field as a "type" of stress composed of pressures and tensions (Maxwell, 1865).</p>
<p>These intuitive types are not necessarily physically correct claims, but rather common-sense constraints that help narrow the space of possible concepts moving forward.Their intuitiveness stems from the fact that they are used most frequently in everyday human reasoning.For example, one way intuitive types show up in everyday reasoning is in how people interpret the question, "What time is it?"Most would respond with a time, like "2 PM" even if it's incorrect.But they wouldn't answer, "A burrito with pinto beans"-unless as a joke-because that response doesn't match the expected "type" of answer, which is time.</p>
<p>Felix Sosa has recently characterized these intuitive types as cognitive constraints that impose structure on concepts, enabling people to generate reasonable-though not necessarily correct-answers to novel questions (Sosa &amp; Ullman, 2022).Furthermore, he has proposed a natural formalization of these types as theoretical types which are the building blocks of type theory3 .This makes one wonder: if intuitive types used in everyday thought-like "bread" or "dog"-are slippery yet still conceivable as theoretical types, what about those used in scientific reasoning, such as "has 3 distinct components" which are already well-defined?This brings us to the second and more narrowly focused central question of this paper:</p>
<p>Can concept formation in theoretical physics be formalized using type theory?</p>
<p>Our answer is yes, and in this paper we set out to substantiate this claim through a type-theoretic formalization of scientific concept formation driven by detailed reconstructions of scientific reasoning from the history of physics.In this framework, intuitive types of scientific concepts are formalized as types and different "ways of discovering scientific concepts" as typing rules-thereby recasting them as discovery mechanisms.Moreover, scientific concepts are formalized both as terms and as the results of typing rules-thus defining them by how they are discovered, in the spirit of Peirce and cognitive science of science.</p>
<p>Lastly, for one familiar with programming-where types like true4 and int are standard-it is natural to wonder whether type theory can be viewed through a computational lens.This is indeed the case, as a type system can be understood as a mathematical formalization of a programming language (Cardelli, 1996), where terms correspond to programs5 and types specify how those programs may be used.This perspective allows us to recast scientific discovery as program synthesis: the inference of programs from data and constraints (Rule, Tenenbaum, &amp; Piantadosi, 2020).To illustrate this idea, we will carry out type-theoretic reconstructions of two examples of concept formation from the history of physics, and computationally implement one of them using program synthesis.</p>
<p>Overview</p>
<p>In summary, there are two main reasons to consider type theory as a mathematical language for formalizing concept formation in theoretical physics:</p>
<ol>
<li>Theoretical types provide a natural formalization of intuitive types, which theoretical physicists rely on when creating new concepts 2. Intuitive types used in theoretical physics are often more precisely defined than those used in everyday reasoning, which makes the former much easier to formalize than the latter.For example, an intuitive type like "a force with a wave-like direction" has more sharply defined properties than intuitive types like toy, dog, or sweet, which remain comparatively vague.</li>
</ol>
<p>This paper aims to provide a type-theoretic formalization of some ways in which scientific concepts are formed in theoretical physics, based on reasoning patterns reconstructed from the history of theoretical physics.To this end, the paper undertakes the following recastings:</p>
<ol>
<li>Intuitive types are formalized as theoretical types.2. "Ways of discovering new concepts"-such as concept distinction, property preservation and concept change-are formalized as elementary typing rules, which we call cognitive typing rules.3. "Ways of discovering new scientific concepts" are formalized as compositional typing rules that integrate cognitive typing rules with functional and algebraic operations.In doing so, these "ways" are recasted as discovery mechanisms.4. Scientific concepts are formalized both as terms and as the outcomes of discovery mechanisms.</li>
</ol>
<p>The formalism we develop is applied to a paradigmatic conceptual breakthrough in the history of physics: Einstein's special relativity.Specifically, we examine two key episodes in Einstein's reasoning that paved the way to his discovery.Namely, the conceptual paths that led him to reject the possibility of frozen waves and to recognize the relativity of time.</p>
<p>Einstein's reasoning leading to the impossibility of frozen waves is formalized as a distinction discovery mechanism, which combines a distinction typing rule with functional and algebraic operations.Moreover, Einstein's reasoning leading to the relativity of time is formalized as a discovery mechanism that combines property preservation and concept change typing rules with functional and algebraic operations.At a global level, this discovery mechanism reverses the roles of assumption and conclusion, which leads us to name it the assumption-conclusion switch discovery mechanism.Lastly, we use typed-program synthesis to simulate Einstein's conceptual path to the relativity of time.</p>
<p>Although many studies have modeled the discovery of symbolic equations (Udrescu et al., 2020), scalars (Zhang, Han, Wang, Car, &amp; E, 2018), vectors (Schütt, Sauceda, Kindermans, Tkatchenko, &amp; Müller, 2018), and probability distributions in physics (Noe, Olsson, Kohler, &amp; Wu, 2019), this paper is, to our knowledge, the first to formalize and model concept discovery in theoretical physics.By integrating insights from cognitive science, computer science, and the history of physics, this study offers a novel perspective that frames concept discovery in theoretical physics as a structured process, rather than an inscrutable one.This approach holds that by taking seriously the history and philosophy of physics as blueprints for the reasoning patterns behind theoretical discoveries-and by formalizing those patterns using type theory and program synthesis-we can begin to address the question of how scientific concepts are formed, scientifically.</p>
<p>The rest of the paper is organized as follows.Section 3 compares our formalism to past and current AI approaches to scientific discovery and complex systems frameworks.Section 4 provides an overview of the type theory background relevant to our goals, emphasizing intuition over rigor to ensure accessibility to researchers from diverse fields such as physics, artificial intelligence, cognitive science, psychology, and philosophy.Section 5 presents the cognitive typing rules that underpin our formal approach.In Section 6, we apply the type theory background and cognitive typing rules developed in the previous sections to two historical episodes in theoretical physics: Einstein's reasoning leading to the impossibility of frozen waves and his conceptual path to the relativity of time.These applications lead to the identification of two discovery mechanisms-distinction and assumption-conclusion switch-all of which take the form of compositional typing rules constructed from cognitive typing rules and functional and algebraic operations.Section 7 computationally models the type-theoretic reconstruction of Einstein's conceptual path to the relativity of time as a program synthesis task.Finally, Section 8 summarizes the paper, reflects on its broader implications, and outlines directions for future research.</p>
<p>Related work</p>
<p>The idea of computationally modeling scientific discovery-or using AI for science-began with Herbert Simon and Pat Langley's invention of symbolic regression, the computational inference of equations from data (Simon, 1973;Simon, Langley, &amp; Bradshaw, 1981).They implemented it using heuristics inspired by the history of early classical physics, and in the following decades, subsequent implementations-such as those using delay-coordinate embedding (Packard, Crutchfield, Farmer, &amp; Shaw, 1980) and genetic algorithms (Schmidt &amp; Lipson, 2009)-vastly improved its scalability.More recently, deep learning has not only enhanced the efficiency and scale of symbolic regression (Udrescu &amp; Tegmark, 2020), but also enabled the discovery and rediscovery of other physically meaningful mathematical objects.Among these objects one finds scalars like band gap energies that signal new topological insulators (Claussen, Bernevig, &amp; Regnault, 2020); probability distributions like the Bernoulli model used to confirm Kepler-90i as a real exoplanet (Shallue &amp; Vanderburg, 2018); and operators such as those defining the Navier-Stokes equations (Li et al., 2020).As a result, deep learning has become the dominant paradigm in AI for science.</p>
<p>Yet despite the success of both early and modern AI for science approaches in computationally modeling key aspects of the scientific discovery process, they still face a common challenge: providing a computational account of its conceptual dimension.Their difficulty in doing so stems from framing discovery solely as the optimization of solutions to well-defined questions, rather than also including the generation of new questions-a process that requires conceptual innovation.This distinction between solving well-defined problems and generating new ones was recently examined by Battleday and Gershman (Battleday &amp; Gershman, 2024), who describe them as the "easy" and "hard" problems of AI for science.</p>
<p>In their recent work, they point out that, at a high level, formulating a new scientific problem is akin to specifying both the domain-identifying the relevant phenomena-and the theoretical constraints-determining the properties that a theory explaining these phenomena must satisfy.Moreover, they note that these specifications involve not only objective aspects but also subjective ones, such as taste and style.They further note that the interaction of these objective and subjective aspects is the central object of study in the cognitive science of science, and therefore, any computational account of scientific question formation should begin with insights from this field.In this context, the present work represents a step in that direction.Moreover, this endeavor can be regarded as a fine-grained complement to coarser-grained, multi-agent computational models of scientific discovery that capture how research communities select strategies for scientific discovery and offer principled guidance on which strategies are most effective (Dubova, Moskvichev, &amp; Zollman, 2022).</p>
<p>At this point, one may wonder: what alternative AI approaches-not informed by the cognitive science of science-are currently being pursued to address the hard problem of AI for science?Battleday and Gershman point out that the only existing candidate is the AI Scientist (Lu et al., 2024), a system that uses large language models, prompt engineering, and coding assistants to generate scientific questions in natural language.Its outputs are often generated by splitting variables, modifying processing pathways, or adding new components to the model.While the AI Scientist generates human-interpretable outputs and, if sufficiently scalable, may contribute to the generation of new scientific hypotheses or concepts, the procedure by which these outputs are produced still functions largely as a black box (Mengaldo, 2024), providing no formal justification for how the results are derived.As a result, this could increase the risk of overlooking empirically correct scientific concepts that appear unreasonable, or of accepting seemingly reasonable ones that rest on flawed assumptions.</p>
<p>Moreover, although the AI Scientist produces scientific concepts in natural language, it does not currently provide a formal representation of them (Wolfram, 2024).For one seeking to formalize scientific concept formation, this makes it unclear how to define scientific concepts precisely and establish exact relationships among them.As a result, one may struggle to refine scientific concepts in a way that reliably generates reasonable, yet fallible, novel concepts or that effectively challenges deeply held assumptions.Furthermore, because the output is solely in natural language and not explicitly restricted to properties relevant in a given context, it is open to multiple interpretations-that is, it is overloaded with meaning-which can lead to ambiguous concepts (Liu et al., 2023).In sum, while the AI Scientist generates promising natural language outputs that could give rise to novel scientific concepts, it does not yet offer a formal account of how such concepts are formed-either in the processes it follows or in the outputs it produces.To address these challenges, we examine how a type-theoretic approach to scientific concept formation might help overcome them.</p>
<p>To begin with, a type-theoretic framework is a formal system built on well-defined objects of study (types) and procedures (typing rules), thereby making the process from input to output explainable rather than a black box.In this system, concepts are formalized as both terms and the outcomes of typing rules, and the concepts' relevant properties are represented as types.This formalism makes it possible to relate concepts objectively-either by assigning them the same type, or by applying rules that govern how concepts of the same or different types interact.Furthermore, the problem of meaning overload can be naturally avoided by the fact that each typing assertion is made relative to a set of assumptions, known as the context.This does not compromise type theory's syntactic expressiveness, meaning its capacity to represent concepts in varied syntactic forms (e.g., equations, hierarchical relationships, or visual constraints).In addition, both the assignment of a type to a term and the typing rules themselves act as constraints6 that guide the reasoning process.Lastly, although type theory is an abstract formalism, its syntactic expressiveness allows both its terms and types to retain the concrete features that give scientific concepts their meaning.</p>
<p>Type theory</p>
<p>Type theory is a branch of mathematical logic that formalizes the classification of mathematical objects, known as terms, by assigning them classifiers called types, which constrain how terms can be used and combined (Pierce, 2002).Originally developed to avoid logical paradoxes in set theory, it now may be regarded as a foundation for the construction of mathematics (Russell, 1903) and as a formalization of programming languages (Pierce, 2002).In the latter role, it has been used to automate basic algebraic problem solving (Poesia &amp; Goodman, 2023), as well as the formulation and verification of mathematical proofs and conjectures (Andrews et al., 1996).</p>
<p>We now aim to extend the use of type theory to model concept formation in theoretical physics reasoning.To this end, we begin by introducing type theory with an emphasis on intuition over rigor, illustrating the key ideas relevant to our purpose through examples drawn from physics.For simplicity, the type-theoretic examples in this section involve only terms that represent fully established physical concepts-those that are clearly physically interpretable and recognized as valid within currently accepted theories.In contrast, examples involving provisional concepts-constructs that merely satisfy certain physical constraints and aid reasoning, but may not be fully physical or valid within any accepted theory-will be presented in Section 6.</p>
<p>Expressions, terms and types</p>
<p>In a type system, an expression is any sequence of symbols allowed by the system's typing judgments and typing rules (Smith, 2024).Common-sense examples of this are "Colorless green ideas sleep furiously" (Chomsky, 1957) and "The square root of a triangle equals banana" which are grammatically well-formed but lack coherent, common-sense meaning.Physics examples such as Charge + Temperature or Force = Meters Seconds 2 are syntactically well-formed in the sense that they resemble mathematical equations, but they are physically meaningless-here because they violate dimensional consistency or because they define a physical quantity solely in terms of units.</p>
<p>A term is a specific kind of expression that carries semantic content-that is, it "is or does something" in the type system (Smith, 2024).More precisely, a term is an expression that can be assigned a type in the type system.Common-sense examples of terms include "a red apple", "a wooden chair" or a "barking dog"-each referring to familiar and recognizable entities.In physics, examples of terms include variables, constants, and function applications such as a particle's velocity v, the speed constant c, and the Galilean acceleration equation v t .A type classifies terms and determines their valid uses7 .This classification is made by asserting that a term t has type A, denoted as t : A, and the determination of valid use is carried out by judgments and typing rules.Common-sense examples include assigning the term "a red apple" the type fruit, or "a wooden chair" the type furniture-reflecting how we often understand things in terms of what kind they are.In physics, this corresponds to assigning the speed constant c the type physical-constant, and the Galilean acceleration expression v/t the type kinematic-quantity, where both the assignment of types to terms and the use of those terms are defined by the system's typing rules.</p>
<p>Contexts</p>
<p>A context Γ is a collection of assumptions under which an assertion can be made.For a commonsense example, making a cake occurs under a kitchen context, Γ kitchen , which assumes flour, sugar, and an oven are available.Likewise, doing laundry takes place in a laundry-preparation context, Γ laundry prep , which requires sorted clothes, detergent, and an empty washing machine.In physics, contexts encode the theoretical assumptions.For instance, in the Newtonian physics context Γ Newton , we assume Newton's second and third laws; in the classical mechanics (CM) context Γ CM , we adopt a classical mechanical framework; and in the non-relativistic context Γ non-rel , we restrict all objects to speeds far below the speed of light.</p>
<p>Typing jugements</p>
<p>A typing judgment asserts that, under a context Γ, a term t has type A, and is denoted as Γ ⊢ t : A For a common-sense example, consider a coin in two everyday contexts.In the arcade context Γ arcade -where coins serve to activate an arcade machine-we assign coins the type activator, while in the wishing-well context Γ wishing-well -where coins symbolize chance or hope-we assign them the type luck.Formally, we write Γ arcade ⊢ coin : activator and Γ wishing-well ⊢ coin : luck.</p>
<p>In physics, within the low speed classical mechanics context Γ v≪c,CM , kinetic energy of type energy takes the form KE = 1 2 mv 2 .Formally, we express this by the following typing judgment:
Γ CM ⊢ KE = 1 2 mv 2 : energy.
Now consider the physical quantity angular momentum, of type momentum, which takes different mathematical forms in two distinct theoretical physics contexts.First, within the classical mechanical (CM) context Γ CM , the angular momentum L of a rotating system is given by L = mvr.However, within the quantum mechanical (QM) context Γ QM , angular momentum is quantized and takes discrete values given by the equation L = ℓ(ℓ + 1) ℏ, where ℓ can be any non-negative integer.Formally, these judgments are denoted as
Γ CM ⊢ L = mvr : momentum and Γ QM ⊢ L = ℓ(ℓ + 1)ℏ : momentum</p>
<p>Typing rules</p>
<p>In order to specify how a typing judgment is derived, it is often defined inductively (Harper, 2016).That is, a typing judgement is defined as a collection of typing rules.Each typing rule defines a typing judgment as a conclusion derived from premises that are themselves already established judgments.Schematically, a typing rule is written in the following form8 :
premise 1 • • • premise n conclusion
Formally, a typing rule asserts that the typing judgment t : A holds under the context Γ if each of its premises t i : A i holds under the context Γ, Γ i9 , where Γ i is a local extension of Γ specific to the i th premise.This is denoted as follows:
Γ, Γ 1 ⊢ t 1 : A 1 . . . Γ, Γ n ⊢ t n : A n Γ ⊢ t : A
At this point, we offer a few remarks on the nature of typing rules.A rule with no premises holds unconditionally and is therefore called an axiom.Moreover, the definition of a typing rule implies that the premises are sufficient to derive the conclusion.Furthermore, different collections of premises can lead to the same conclusion.Thus, even if a conclusion holds, it does not mean that any specific set of premises must have held.Moreover, since Γ applies uniformly to all premises and the conclusion, it can be omitted.This yields the local form of the inductive definition, as opposed to the global form in the immediately preceding displayed rule (Harper, 2016).</p>
<p>To illustrate typing rules using familiar scenarios, consider the following examples.First, suppose that under the kitchen context Γ kitchen , the term banana has type fruit.Moreover, under the same context, the term blender has type fruit → smoothie, meaning it takes an input of type fruit and produces an output of type smoothie.From these premises, we can infer that the function application term blender(banana) has type smoothie.This is a common-sense example of a standard function application typing rule.For the second example, suppose that under the mood context Γ mood , both terms happy and sad have type emotion.From these premises, we can form the pair term (happy, sad) which may represent nostalgia or catharsis, and is assigned the type emotion.This is a commonsense example of a standard pair typing rule that preserves the type of its components.The examples are formalized as follows:</p>
<p>Γ kitchen ⊢ blender : fruit → smoothie Γ kitchen ⊢ banana : fruit Γ kitchen ⊢ blender(banana) : smoothie Γ mood ⊢ happy : emotion Γ mood ⊢ sad : emotion Γ mood ⊢ (happy, sad) : emotion The following physics examples illustrate how typing rules operate-either by directly assigning a type to a basic term or by building the type of a complex expression from the types of its parts.These include a constant rule for the gravitational constant g, a function application10 rule for uniform linear velocity v = x t and a function abstraction rule for uniform linear acceleration a = v t , as shown below:
Γ ⊢ g : real , Γ ⊢ x : pos Γ ⊢ t : time Γ ⊢ v : pos → time → vel Γ ⊢ v(x, t) : vel and Γ, v : vel, t : time ⊢ v/t : acc Γ ⊢ λv.λt.v/t : vel → time → acc</p>
<p>Equality Judgments</p>
<p>Type theory allows us to relate scientific concepts in diverse ways through distinct notions of equality.Each such notion is expressed by an equality judgment comprising three claims: two typing judgments (taken as implicit premises) and an equality assertion between the terms according to the chosen notion.A helpful intuition for distinguishing these notions of equality is captured by Bhavik Mehta's saying: "Syntactic equality is they look identical, definitional equality is they are the same, propositional equality is they turn out to be the same" (Buzzard, 2024).Below, we present their formal definitions11 .</p>
<p>Syntactic Equality.This equality holds between two terms t and t ′ when they are identical in written form.That is, they consist of the same sequence of symbols, ignoring trivial differences such as whitespace or renaming bound variables12 .For a common-sense example, consider the syntactic equality cat = cat.For a mathematical example, consider the lambda functions (λy.y + 1) and (λx.x + 1).These are syntactically equal as they differ only in the name of the bound variable.</p>
<p>Definitional Equality.This equality generalizes syntactic equality by stating that two terms t and t ′ , both of type A, are equal if they are computationally identical, meaning they reduce to the same normal form.It is written as: Γ ⊢ t ≡ t ′ : A A common-sense example of definitional equality is the equivalence between the phrases "The capital of Peru" and "Lima" since they refer to the same entity.In physics, a clear example is the equivalence between the force terms dp dt and m dv dt (assuming mass is constant), which are definitionally equal after expanding the definition of momentum p13 .This can be expressed as Γ ⊢ dp dt ≡ m dv dt : force.</p>
<p>Propositional Equality.This extends definitional equality by stating that two terms t and t ′ of the same type A are equal if their equivalence can be constructed (proven) by a sequence of computational transformations rather than direct reduction.It is expressed as: Γ ⊢ t = t ′ : A. A common-sense example of propositional equality is the equivalence between the phrases "The package at your door" and "the item you ordered online last week" as both refer to the same entity, though recognizing this requires some reasoning.In physics, an example of this is given by the energy terms F • dx and 1 2 mv 2 − 1 2 mv 2 0 , which are propositionally equal after integration.This can be written as Γ ⊢ F • dx = 1 2 mv 2 − 1 2 mv 2 0 : energy.</p>
<p>Heterogeneous Equality.This equality generalizes propositional equality by allowing terms of different types to be equal.It is expressed as: Γ ⊢ (t : A) = (t ′ : B) A common-sense example of heterogeneous equality is the equivalence between the action of waving your hand when someone walks in and saying "Hello!"-two expressions of the same intent, even though they are of different kinds.In physics, an example of heterogeneous equality is the equivalence between the gravitational force F g = GM m r 2 and centripetal force F c = F c = mv 2 r , which are equal in the case of a circular orbit under gravity.This can be expressed as Γ ⊢ F g : grav = F c : cent</p>
<p>Special Types</p>
<p>Below, we present non-elementary types that are necessary for describing scientific concept formation using type theory.</p>
<p>Intersection Type.An intersection type A ∩ B is a type that combines types A and B, such that any term of type A ∩ B has both types A and B. A common-sense example is the coin flip, which functions both as an activator in arcades and as a symbol of luck in wishing wells-giving it the intersection type activator ∩ luck.In physics, a charged pendulum swinging in Earth's magnetic field is governed by both the laws of mechanics and the laws of electrodynamics-giving it the intersection type mech ∩ elect.This can be expressed as Γ ⊢ charged-pend : mech ∩ elect.</p>
<p>Contradiction Type.The contradiction type ⊥ is a type for which no term can be constructed unless there is an inconsistency.Thus, if a derivation results in a term of type ⊥, this indicates that some of the assumptions used in the derivation are inconsistent.For a common-sense example, consider the statement "the light is both on and off" which leads to a contradiction and thus has type ⊥.For a physics example, consider the statement Q that an object is both at rest and accelerating under a net force has type ⊥, since it contradicts Newton's laws.This can be expressed as Γ Newton ⊢ Q : ⊥.</p>
<p>Equation rewrite form</p>
<p>The symbol | indicates the equation-rewrite form and means that, in t | r, the term on the left is rewritten using the equations on the right.Here, t | r reads as "t under r".The present paper adopts this metallanguage notation14 for its usefulness in symbolic manipulation, but is not standard in type theory.</p>
<p>Cognitive typing rules</p>
<p>In the previous section, we presented various examples involving fully formed concepts-those already taken as given-to illustrate how type theory works.Now, we turn to formalizing specific mechanisms of concept formation and change using type theory.We refer to the typing rules that govern concept formation and change as cognitive typing rules.From a physics perspective, these rules could also be interpreted as equations of motion for concept formation and change.In what follows, we present the cognitive typing rules needed to reconstruct the conceptual paths that led to the scientific discoveries discussed in Section 6.</p>
<p>Property preservation rules</p>
<p>Property preservation rules formalize reasoning patterns in which a concept produced by transforming another concept retains the same property i.e. intuitive type.The following rules apply to cases where the transformation is either a function or its inverse.</p>
<ol>
<li>Forward property preservation.This rule states that if the argument x has type A, then the function f (x) also has type A, as denoted below:
Γ ⊢ x : A Γ ⊢ f (x) : A
Intuitively, under this rule, if a concept has a certain property, applying a function f to it preserves that property.2. Backward property preservation.This rule states that if the function f (x) has type A, then its argument x also has type A, as denoted below:
Γ ⊢ f (x) : A Γ ⊢ x : A
Intuitively, under this rule, if a concept resulting from a function has a certain property, then the concept it was applied to must also have that property.</li>
</ol>
<p>Concept distinction rule</p>
<p>The concept distinction rule formalizes the reasoning pattern by which identical concepts with different intuitive types are distinguished from one another: one retains its type, and the other is marked inconsistent.</p>
<p>Concept distinction.This rule states that if two terms, x and y, are assigned different types A and B in separate contexts Γ 1 and Γ 2 , and are assumed to be heterogeneously equal, in the combined context Γ 1 ∪ Γ 2 , x retains its type and y is assigned the empty type ⊥, indicating an inconsistency.
Γ 1 ⊢ x : A Γ 2 ⊢ y : B x = y Γ 1 ∪ Γ 2 ⊢ x : A Γ 1 ∪ Γ 2 ⊢ y : ⊥
Intuitively, this rule captures the line of reasoning that if two concepts are considered identical but have different properties, then only one of them can exist.Notably, this typing rule differs from the conventional ones found in Section 4, as it involves defining terms (concepts) in different contexts and then uniting those contexts.</p>
<p>For a common-sense example, consider two books with the same title placed in both the science fiction and history sections of a library.According to the distinction rule, under the combined context of both sections, the librarian might assume the two entries refer to the same book and treat the differing classifications as inconsistent.Thus, the librarian might reasonably-but perhaps mistakenly-conclude that it belongs to science fiction and that its classification as history is an error.</p>
<p>Concept change rule</p>
<p>The concept change rule captures the reasoning pattern that if two concepts have different intuitive types yet are identical, one acquires the other's type as well.</p>
<p>Concept change.This rule states that if two terms, x and y, assigned different types A and B in separate contexts Γ 1 and Γ 2 , are assumed to be heterogeneously equal, then in the combined context Γ 1 ∪ Γ 2 , the term x changes its type to the intersection type A ∩ B.
Γ 1 ⊢ x : A Γ 2 ⊢ y : B x = y Γ 1 ∪ Γ 2 ⊢ x : A ∩ B Intuitively,
this rule captures the line of reasoning that if two concepts are considered identical but have different properties, then they can be treated as a single concept that possesses both properties.For a common-sense example of this rule, take avocado, which is considered savory in Peru and sweet in Brazil.Thus, depending on the cultural context, avocado may be assigned the type savory or sweet.Nonetheless someone who has experienced both cultures might treat avocado as both savory and sweet-say, by eating an avocado sandwich alongside an avocado smoothie.Notably, the rule implies that the name "avocado" does not determine the concept's meaning; it merely serves as a pointer that locates the concept.Instead, the meaning of avocado is encoded in how the concept is used-specifically, in how it relates to other concepts, as defined by the types it is assigned and the rules that govern them.</p>
<p>Conceptual paths</p>
<p>In this section, we build on the type theory background developed in Section 4 and the cognitive typing rules introduced in Section 5 to formalize the conceptual paths that led Einstein to the impossibility of frozen waves and the relativity of time.We formalize these two paths, respectively, as instances of two discovery mechanisms: distinction and assumption-conclusion switch.To clarify the kinds of concepts formalized along these conceptual paths, we begin by recalling that in Section 4, we applied typing rules to already formed common-sense and scientific concepts, and in Section 5, to in-formation common-sense concepts that were not yet fully characterized.Here, we apply them to concepts that are both scientific and still in formation-meaning they are not yet physically correct (i.e., they wouldn't appear in a textbook and may be forgotten from history)-but nonetheless serve as necessary provisional steps toward physically valid ones.</p>
<p>Conceptual path to frozen light waves impossibility</p>
<p>Setting the stage</p>
<p>At the age of sixteen, Einstein wondered what a light wave would look like if one were to travel on top of it.On the basis of his experience 15 -and perhaps influenced by Galilean relativity-he reasoned that if he traveled at the speed of light, the wave would appear frozen.Yet, he instinctively felt that a frozen wave could not be physically real.This led him to suspect that any theory permitting such a possibility had to be, at the very least, partially flawed.</p>
<p>Later, after learning Maxwell's electrodynamics, Einstein likely used a thought experiment to rule out the possibility of frozen waves 16 , as suggested by John Norton (Norton, 2004(Norton, , 2005)).The core result of this thought experiment is that frozen waves cannot exist (or at least are not likely to) because, in standard Maxwell electrodynamics, there is no way to distinguish between frozen and propagating waves at a single instant 17 .The following is a type-theoretic reconstruction of Einstein's reasoning in this thought experiment, showing how he arrived at this conclusion, along with an explanation in natural language:</p>
<p>Distinction discovery mechanism</p>
<p>Step 1. Einstein first considered a propagating wave, which we can formalize as having the type move E prop = E 0 sin(ωt − ky) : move Step 2. To examine the wave at an instantaneous moment, he evaluated it at an arbitrary specific point in time, such as t = 0 E prop = E 0 sin(−ky) : move</p>
<p>Step 3. Next, he considered a frozen wave, which is a wave constrained by two key assumptions:</p>
<p>• It undergoes a Galilean transformation.</p>
<p>• The observer moves at the speed of light.</p>
<p>15 Contrary to the popular belief that Einstein derived this thought experiment from Maxwell's equations, he himself admitted that he only learned Maxwell's electrodynamics during his university years (Norton, 2004) 16 as a first step in the process of ruling out emission theories of light 17 Einstein would have considered time at a single instant in his thought experiment, since that is how his claims about propagating and frozen waves would have been tested experimentally (Norton, 2004) Under these conditions, the wave is formalized as having the type frozen
E frozen = E 0 sin(ωt − ky) | y = y ′ + vt, t = t ′ , v = c: frozen
Step 4. Applying the transformation and velocity condition into the frozen wave
E frozen = E 0 sin(ωt ′ − k(y ′ + vt ′ )) | v = c: frozen = E 0 sin(ωt ′ − k(y ′ + ct ′ )): frozen
Step 5. Einstein substituted the fact that ω = kc in the frozen wave
E frozen = E 0 sin(kct ′ − ky ′ + kct ′ ): frozen = E 0 sin(−ky ′ ): frozen
Step 6.Since propagating and frozen waves were understood by Einstein to be physically distinct, but he found them to be mathematically equivalent, he could have concluded that they are actually physically the same.This possible judgment can be formalized using the concept change rule:
Γ 1 ⊢ E prop : move Γ 2 ⊢ E frozen : frozen E prop = E frozen Γ 1 ∪ Γ 2 ⊢ E prop : move ∩ frozen where Γ 1 = {E prop = E 0 sin(ωt − ky) : move, ω = kc} Γ 2 = {E frozen = E 0 sin(ωt − ky) | y = y ′ + vt, t = t ′ , v = c: frozen, ω = kc}
Nonetheless, he did not take this approach; instead, he remained steadfast in the conviction he had developed at the age of 16</p>
<p>Step 7. Since he knew that propagating waves exist, he would have ruled out the possibility of frozen waves to reconcile the inconsistency between the mathematics described above and his experience.This judgment can be formalized using the distinction rule:
Γ 1 ⊢ E prop : move Γ 2 ⊢ E frozen : frozen E prop = E frozen Γ 1 ∪ Γ 2 ⊢ E prop : move Γ 1 ∪ Γ 2 ⊢ E frozen : ⊥
The distinction discovery mechanism18 can be stated in its most general form, without referring to the example's specific variable names, as follows:
Γ 1 ⊢ x : A Γ 2 ⊢ y : B f (x) = g(y) Γ 1 ∪ Γ 2 ⊢ x : A Γ 1 ∪ Γ 2 ⊢ y : ⊥
where f and g are the functions that substitute the appropriate context-dependent algebraic constraints into x and y, respectively.</p>
<p>Conceptual path to the relativity of time</p>
<p>The Michelson-Morley experiment is often regarded as one of the crucial optical experiments that Einstein sought to explain in his effort to extend the principle of relativity to electrodynamics.However, according to Shankland's account of a conversation he had with Einstein, the physicist noted that he only became aware of the Michelson-Morley experiment after publishing his paper on special relativity, and that it was actually two other experiments that played this formative role:</p>
<p>"Otherwise," he said, "I would have mentioned it in my paper."He continued to say the experimental results which had influenced him most were the observations of stellar aberration and Fizeau's measurements on the speed of light in moving water."They were enough," he said.</p>
<p>What role might stellar aberration and Fizeau's experiment have played in shaping Einstein's path to special relativity?John Norton (Norton, 2004) argues that a close examination of how these results fit into Lorentz's treatment of electrodynamics, which Einstein based his reasoning on, suggests that these results provided Einstein with empirical support for believing in the relativity of time.Since Einstein analyzed both experiments similarly, we will focus solely on explaining how he reasoned from stellar aberration to the relativity of time, and ultimately on formalizing that reasoning process using typing rules.</p>
<p>Setting the stage</p>
<p>First observed by James Bradley in 1727, stellar aberration is the apparent deflection in the direction of a star's light due to the Earth's motion around the Sun19 .Einstein might have recognized in stellar aberration a well-established experiment involving both light and relative motion, and therefore a natural setting in which to pursue his extension of the principle of relativity to electrodynamics (Norton, 2004).For his first attempt, Einstein most likely relied on the Galilean transformation, but ultimately concluded that it could not succeed.This incompatibility becomes evident when one examines how a light wave behaves under this transformation, as we do now, following Norton's analysis (Norton, 2004(Norton, , 2005)).</p>
<p>We begin by examining light propagation in the star's rest frame shown in Fig. 1(a).The star emits light waves toward the Earth along the y-axis, which are denoted as f (ωt − ky).Since light wave fronts reaching Earth are virtually flat, they are plane and perpendicular to their direction of propagation.These wave fronts then reach a telescope on Earth, which moves to the right along with the Earth's motion.To ensure the starlight passes through the telescope and reaches the observer's eyepiece, the telescope must be tilted.If left upright, the trailing wall of the telescope would block the light before it reaches the eyepiece.This necessary tilt deflects the observed direction of the starlight by an amount precisely equal to stellar aberration's deflection angle θ = u c , where u is the Earth's speed as it moves around the sun and c is the speed of light.</p>
<p>For one seeking to ensure that stellar aberration is consistent with Galilean relativity, the effect should also be observed in the Earth's rest frame shown in Fig. 1(b).However, this is not the case.By applying a Galilean transformation to the star's rest frame, we arrive at the Earth's rest frame and assign the star and its emitted light a velocity of −u along the x-axis.Under this transformation, the coordinates change as t ′ = t, x ′ = x + ut, and y ′ = y.Consequently, the transformed wave remains f (ωt ′ − ky ′ ), indicating that its propagation direction is unchanged.As a result, the wave fronts remain perpendicular to the line connecting the star and Earth rather than tilting.This implies that if the star, rather than the Earth, is in motion, there is no need to tilt the telescope on Earth to observe the starlight.Thus, the effect of stellar aberration disappears when the star moves instead of the Earth, contradicting relativity.</p>
<p>To resolve this contradiction, Einstein sought an explanation of stellar aberration that depended only on the relative velocity between Earth and star, rather than on their individual velocities (Norton, 2004).He recalled from his reading of Versuch, how Lorentz had applied his transformations for fields20 and coordinates to stellar aberration, as illustrated in Fig 1(c).Similar to the Galilean transformation, the Lorentz transformation, when applied to the star's rest frame, brings Earth to rest while setting the star in motion in the opposite direction.However, under the Lorentz transformation, the coordinates change as follows: t ′ = t − ux c 2 and x ′ = x − ut where t ′ is what Lorentz referred to as the local time-an artificial but necessary coordinate that Lorentz did not consider to represent real time.As a result, the waveform transforms into f (ωt
′ − ky ′ ) = f (ω(t − ux c 2 ) − ky) = f (ωt − k ux c − ky)
, where c = ω/k and u is the Earth and star's relative velocity.Notably, this transformation deflects the wave fronts' propagation direction by an angle of u c radians, aligning it with the star's motion.That is, the Lorentz transformation rotates the wavefronts, making them no longer perpendicular to the line connecting the star and the Earth.This means that the telescope on Earth must be tilted for starlight to reach the eyepiece.Thus, stellar aberration is preserved in the Earth's frame as it depends solely on the relative motion between the Earth and the star-a result fully consistent with the principle of relativity.</p>
<p>Here, Norton (Norton, 2004) proposes that Einstein, by viewing Lorentz's application of his transformations to stellar aberration through the lens of relativity, would have followed a line of reasoning leading to the conclusion that stellar aberration provided experimental support for the physical reality of the local time-allowing it to be interpreted simply as time itself.This thought process can be regarded as a discovery mechanism in which the roles of assumption and conclusion are switched.It is this mode of discovery that we describe in natural language, alongside its corresponding type-theoretic representation.</p>
<p>Assumption-conclusion switch discovery mechanism</p>
<p>Step 1. Lorentz regarded the concept of local time, t ′ = t − ux c 2 , as a mathematical coordinate without physical interpretation, yet one that was useful for explaining stellar aberration while treating light as an electromagnetic wave.In other words, he considered local time to be a necessary artificial construct ( (Lorentz, 1895), p. 77).This seemingly qualitative remark can be formalized as the assertion that t ′ has the type art (artificial construct), denoted as t ′ : art Step 2. Now, a Lorentz transformation applied to stellar aberration yields a light waveform in the Earth's rest frame, in which time is replaced by local time as follows:
f (ωt − ky) → f ω t − ux c 2 − ky
Here, Einstein might have wondered: what does the artificial nature of local time imply for the transformed wave?Or in our language, what does the local time's type imply about the type of the transformed wave?The simplest answer-and the one Einstein seems to have adopted-is that anything derived from an artificial construct is itself artificial.This reasoning can be formalized by applying the forward property preservation rule to t ′ , as shown below: Step 4. Note that we can summarize the previous steps as follows: Under the assumption that local time is an artificial construct, the transformed waveform must also be artificial.This can be formalized by saying that under the context Γ 1 = t ′ : art,
Γ ⊢ t ′ : art Γ ⊢ g(t ′ ) : artwe have Γ 1 ⊢ f ωt − kx u c − ky : art
To simplify notation, we define the term above as f art .Thus, Γ 1 ⊢ f art : art</p>
<p>Step 5. On the other hand, Einstein recognized that, in the Earth's rest frame, the light waveform deflected by stellar aberration f (ωt − kx u c − ky) is a phenomenological observation that does not rely on theoretical assumptions.Thus, the waveform had to be empirically true rather than artificially constructed.To formalize this reasoning, we state that, under the empty context Γ 2 = ∅, the deflected waveform has type emp (empirical):</p>
<p>Γ 2 ⊢ f ωt − kx u c − ky : emp</p>
<p>For convenience, we define the term above as f emp .Thus, Γ 2 ⊢ f emp : emp</p>
<p>Step 6.Here, Einstein would have recognized that although f art is artificial, it is also equal to f emp , which is empirical.According to Norton's analysis, this suggests that, at the very least, f art could not be solely artificial but must also possess an empirical aspect.Thus, within our formalism, Einstein would have concluded that f art has both types: art and emp.This reasoning process can be formalized using the concept change rule:
Γ 1 ⊢ f art : art Γ 2 ⊢ f emp : emp f art = f emp Γ 1 ∪ Γ 2 ⊢ f art : art ∩ emp where Γ 1 = {t ′ : art} and Γ 2 = ∅
Step 7. At this point, according to Norton (Norton, 2004), Einstein reasons that since his earlier conclusion is not merely artificial but also empirically true, he can now reinterpret it as an assumption.In terms of our formalism, Einstein assumes that f art is of both types: art and emp.As a result, he begins to derive the previous steps in reverse order, while cautiously maintaining that the same type (art ∩ emp) continues to propagate backwards.Eventually, by reasoning backwards, Einstein reaches a new version of his former assumption, now framed as a conclusion: local time t ′ has type art ∩ emp.That is, local time could not be wholly artificial; it had to be, at least in part, empirically true.In other words, local time could be time itself, not merely an artifice.Einstein's reasoning process can be formalized using a backward property preservation rule as follows:</p>
<p>Γ 1 ∪ Γ 2 ⊢ f art : art ∩ emp Γ 1 ∪ Γ 2 ⊢ t ′ : art ∩ emp Because local time depends on relative velocity, it was at this moment that Einstein realized time could be relative.That realization launched special relativity.</p>
<p>Step 8. Ultimately, Einstein's full discovery mechanism of switching Lorentz' assumption and conclusion can be formalized using the following rule:
Γ 1 ⊢ t ′ : art Γ 2 ⊢ f emp : emp g(t ′ ) = f emp Γ 1 ∪ Γ 2 ⊢ t ′ : art ∩ emp
That is, under assumptions from contexts Γ 1 and Γ 2 , the concept of local time t ′ is assigned both types: art and emp.This rule builds compositionally upon the concept change and property preservation rules along with other simple mathematical operations.</p>
<p>The assumption-conclusion switch discovery mechanism can be stated in its most general form, without referring to the example's specific variable names, as follows:
Γ 1 ⊢ x : A Γ 2 ⊢ y : B g(x) = y Γ 1 ∪ Γ 2 ⊢ x : A ∩ B
where g denotes the function that transforms x into y.</p>
<p>Note that one might expect this chain of reasoning to conclude that Einstein became fully aware that time is relative, that is, that t ′ is solely of type emp (t ′ : emp).In fact, this conclusion could be reached by applying a version of the distinction distinction discovery mechanism.However, making that claim would be premature.By this point Einstein had only just begun to regard the relativity of time as a reasonable possibility and was not yet fully confident.Further work could apply Bayesian reasoning to assign probabilities over the terms in the concept-change rule, with outcomes that depend on a "boldness" prior.</p>
<p>Bridge from Type theory to Computation</p>
<p>To carry the type-theoretic formalism into a computational setting, we examine the three-way relationship among type theory, programming languages and scientific concepts introduced in Section 1.We begin with the connection between type theory and programming languages.Specifically, we note that once a type system is equipped with fixed operational semantics (step-by-step evaluation rules), the system's terms can be viewed as programs and its theoretical types as programming-language types (Plotkin, 1977).</p>
<p>Turning to the link between programming languages and scientific concepts, recent work in computational cognitive science holds that symbolic programs (code) offer the best formal representation of concepts because of their expressive power (Rule et al., 2020).As noted in Section 1, Sosa builds on this perspective by suggesting that types in programming languages may capture the cognitive constraints that impose structure in concepts and allow us to generate reasonable, though sometimes incorrect, answers (Sosa &amp; Ullman, 2022).Motivated by these three developments, we introduce two analogies-one theoretical and one computational-for formalizing scientific concept formation.</p>
<ol>
<li>Theoretical analogy: Intuitive types are formalized as theoretical types; discovery mechanisms as typing rules; and scientific concepts as terms and the conclusions of typing rules.2. Computational analogy: Intuitive types are formalized as types in a programming language; discovery mechanisms and scientific concepts as programs.</li>
</ol>
<p>These analogies provide the basis for the Python implementation of the type-theoretic reconstruction of Einstein's conceptual path to the relativity of time.</p>
<p>Program synthesis</p>
<p>In the previous sections, we introduced a type-theoretic reconstruction of scientific concept formation.In this section, the reconstruction is recast as a program-synthesis task: the inference of programs given constraints.Throughout, Einstein's conceptual path to the relativity of time serves as the running example.</p>
<p>To carry out this recasting, we implement the type-theoretic formalism (terms, types, and typing rules) as executable code.To ensure the implementation is user-friendly and practically scalable, we choose to implement the formalism in Python rather than any domain-specific language (DSL), including DSLs that become Turing complete when equipped with an escape hatch (Ellis &amp; Tavares, 2025).In what follows, each formal element is mapped to a Python counterpart.</p>
<p>Representing Type-theoretic objects in Python</p>
<p>Terms are represented as Python strings so that they can be algebraically manipulated with ease.As strings, each term can be passed directly to SymPy-Python's symbolic-mathematics library-which then parses it into a symbolic object that can undergo substitution, simplification, and other operations.This representational choice naturally supports the algebraic manipulations involved in Einstein's conceptual path to the relativity of time.Furthermore, to keep manipulations concise, we sometimes assign a name to a term.Each name is definitionally equivalent to the term it stands for.</p>
<p>Intuitive types are represented as instances of Python classes.This mirrors how Python handles both built-in and user-defined types-as instances of the built-in class Type or an explicitly defined Python class.Specifically, Einstein's intuitive types-Art, Emp, and Intersection-are implemented as instances of the class Base.Similarly, contexts are implemented as instances of the class Context, and each such instance is represented as a set of strings.</p>
<p>A judgment is represented as an instance of the class Judgment, consisting of four elements: context, name, term and type.Moreover, typing rules are implemented as Python functions that take an instance of the Judgment class as input and return a new instance of the same class as output.Notably, a backward property preservation rule simply goes back to whathever term we had immediately before; however, what we call forward property preservation rule is constrained to be a function application f , instead of any other algebraic operation or cognitive typing rule.</p>
<p>State, Goal and Solution</p>
<p>To make the implementation explicit, we frame scientific concept formation as the process of transforming an initial state into a final state that satisfies a specified goal.Within this framing, a state comprises either a single judgment or a set of judgments.The initial state is a set of judgments, and the final state is a single judgment.For simplicity, the goal is defined to be the final state itself.Hence, a solution is a sequence of operations and typing rules that transform the initial state into the final state.</p>
<p>The initial state consists of two judgments: the local time t − ux c 2 has type art, and the light waveform exhibiting stellar aberration has type emp.The final state, which also serves as the goal, is the judgment that local time t − ux c 2 is of type emp.The goal of our present implementation is simple: recover the ground-truth judgment supplied by an oracle.In future implementations, a more cognitively and historically realistic goal would be to find judgments that satisfy a given constraint on the final state, or to construct new relationships among the existing judgments.With the states, goal, and solution now clearly specified, we can now introduce the search procedures.</p>
<p>Search</p>
<p>For researchers working at the intersection of AI and the natural sciences, an intuitive way to understand the search conducted by our program-synthesis approach is to view it by analogy to the more familiar symbolic regression.Both approaches explore expressions built from primitives.In symbolic regression, those primitives are low-level arithmetic and control operators and the expressions take the form of equations, whereas in our method the primitives are typing rules and the expressions represent scientific concepts, which may, but need not, be equations.Our search relies on the primitives (mathematical operations and typing rules)21 explained in natural language and implemented as Python programs in Fig. 2. Notably, we exclude basic mathematical operations like addition and multiplication from the primitive set, opting instead for higher-level primitives-such as algebraic simplification-that better reflect the conceptual level at which a trained physicist like Einstein reasoned, while also avoiding the combinatorial explosion caused by ultra-fine primitives.</p>
<p>To assess whether Bayesian and neurosymbolic guidance further mitigate the curse of compositionality (combinatorial blow-up from composing primitives) (Spelke, 2022), three search methods are used: Enumerative, Bayesian, and Bayes-neural.These methods explore the space of eight-step sequences of mathematical operations and cognitive typing rules and identify the sequence that yields a judgment satisfying our goal.Enumerative search samples sequences by selecting primitives at each step according to a goal measure (term similarity and type agreement) and choosing the next primitive greedily or in proportion to that score.Bayesian search builds each chain by keeping a Beta(α, β) posterior for every admissible primitive (interpreted as that primitive's chance of moving closer to the goal).At each step, for each admissible primitive p we draw θ p ∼ Beta(α p , β p ) and select the primitive with the largest draw (Thompson sampling).We then update that primitive's posterior with the goal measure.Priors are initialized α = β = 1 and reset between chains unless noted.</p>
<p>Bayesian-neural search first trains a neural policy over primitives on traces generated by the pure Bayesian method using state features (goal-term similarity, goal-type match, normalized depth).During search, each step samples the next primitive from a depth-annealed mixture of the neural policy probability and a Thompson-sampled Bayesian probability (the mixture shifts from Bayesian-heavy in early steps to neural-heavy in later ones).After selecting a primitive, its Beta parameters are updated with the same goal reward used in Bayesian search.</p>
<p>Fig. 3 shows the successful eight-step sequence alongside the alternative candidates for the transition from step 1 to step 2. At each step, we select one primitive from our finite pool of valid options, but not all primitives are permitted at every stage (for example, concept change is allowed only when two judgments share the same term but have different types).Moreover, for the three search methods, we can enable or disable specific heuristics to imbue the search with human-like inductive biases or maintain a purely unbiased exploration.</p>
<p>Fig. 4 plots the success rate of each of the three search methods versus program index (i.e., the number of programs run up to that point), averaged over N = 150 independent runs.For each method, the text reports when 100% success is first reached as "about X programs (median: M , IQR: I)." Fig. 4(a) shows results with all search constraints enabled except the "backward-only after target waveform is reached" inductive bias.Under these constraints, Pure-Bayes achieves the highest performance, Bayes-Neural the next best, and Enumeration the lowest.Pure-Bayes reaches 100% success
for step = 1 to L do 15: µ ← M IX BASE + (1 − M IX BASE) • (step − 1)/(L − 1)
▷ depth anneal return None 41: end procedure</p>
<p>Conclusion</p>
<p>So how are scientific concepts birthed?This work does not offer a final answer, but it argues that a central part of the answer lies in the intuitive types that constrain and guide scientific knowledge-making practices.By reconstructing physicists' reasoning leading to concept discovery from philosophical analysis of historical records and formalizing this reasoning within a type-theoretic formalism, we have shown that these intuitive types, and their role in theoretical physics reasoning, can be articulated in a way that is explicit, structured, and computable.More specifically, using types and cognitive typing rules, we formalized two key episodes in Einstein's reasoning toward special relativity: the conceptual paths that led to his rejection of frozen waves and his reinterpretation of time as relative.In doing so, we identified and formalized two discovery mechanisms he employed: distinction and assumption-conclusion switch.Finally, we observed that combining Bayesian methods with neural network techniques to guide search through program space effectively overcomes the curse of compositionality.</p>
<p>Although much remains to be understood, this framework supports the view that scientific concept discovery is not fully mystical or unformalizable, but is at least partially a human-explainable, rule-based process.The viability of this framework hinges on the idea that the vagueness often associated with defining scientific concepts can be limited by defining them in terms of how they are used in the discovery process, and by making explicit the epistemic context in which they are discovered.Furthermore, the framework suggests that history of science and philosophical insight play an essential role-one that goes beyond motivation or decoration-in formalizing scientific concept formation by shaping how formal systems and computational models are designed to represent that very process.</p>
<p>More broadly, this type-theoretic approach offers a step toward a fine-grained computational cognitive science of science-one capable of explaining the detailed reasoning involved in scientific concept discovery.Furthermore, this work complements coarse-grained cognitive formalisms that study discovery in general or at the level of research communities-stochastic or agent-based accounts and Bayesian treatments of explanatory values-rather than individual scientists' reasoning (Dubova et al., 2022;Wojtowicz &amp; DeDeo, 2020;Wolpert, David H. and Kinney, David B., 2024).A natural next step is to investigate whether macro-level accounts of discovery strategies are recoverable from fine-grained reconstructions of individual discoveries.</p>
<p>Looking forward, this framework can be extended by reconstructing the discovery process in other historical examples from physics and science more broadly, formalizing them to uncover new typing rules-or to show that rules like concept distinction and concept change are reusable.In parallel, by examining research records and interviewing scientists, we may be able to formalize aspects of the scientific concept discovery process as it occurs in the present day.Moreover, the program synthesis search can be made more human-like by using more flexible constraints for the goal (e.g., satisfying specific properties rather than matching an exact equation), and by developing heuristics that are learnable and reusable rather than manually engineered (Poesia &amp; Goodman, 2023).This formalization of scientific concept formation would provide us with a deeper explanation of scientific concept discovery as a phenomenon in its own right.This deeper explanation, in turn, may help us make more informed decisions about which discovery mechanisms to use in specific contexts, and avoid dismissing scientific goals grounded in mechanisms that have led to successful discoveries in the past-mechanisms we might have forgotten.Thus, a type-theoretic formalism of scientific concept formation may serve as both tool AI (Aguirre, 2025) and a supportive framework for scientists-especially those early in their careers-to take conceptual risks they might otherwise hesitate to pursue.Ultimately, a deeper understanding of scientific concept formation may also inform scientific pedagogy-not only by helping people learn existing scientific concepts, but by equipping them with the tools needed to discover and create new ones themselves.</p>
<p>Code availability</p>
<p>Source code, experiment scripts, and notebooks are available at: https://github.com/omalagui/einstein program synthesis</p>
<p>Fig. 1
1
Fig. 1 Stellar aberration in the star's rest frame and in the Earth's rest frame under Galilean and Lorentz transformations.(a) In the star's rest frame, Earth moves right and the telescope must tilt in order to capture light wave fronts due to stellar aberration.(b) In the Earth's rest frame obtained via a Galilean transformation, the wavefronts remain vertical and no tilt is needed-implying the absence of stellar aberration.(c) In the Earth's rest frame obtained via a Lorentz transformation, the wavefronts tilt, restoring the need for telescope tilt and thereby preserving stellar aberration.The images used here are adapted from John Norton's website, with permission (personal communication).</p>
<p>ky : art Step 3. Simplifying expression and using k = w c , we have = f ωt − ω ux c 2 − ky : art = f ωt − kx u c − ky : art</p>
<p>Fig. 2
2
Fig. 2 Primitives that serve as the building blocks of the search.The left column names each primitive, the middle explains what each one does in natural language, and the right column shows a shortened version of the Python implementation of each (for illustration).</p>
<p>Fig. 4
4
Fig. 4 Success rate (fraction of programs reaching goal so far) versus program index (number of programs evaluated) for Enumeration (blue), Pure-Bayes (orange), and Bayes-Neural (green) search algorithms.Panels (a) and (b) show the biased and unbiased cases, respectively.In (a) all inductive-bias heuristics are enabled except "backward-only after target waveform is reached".In (b) all heuristics are off.Inductive biases greatly accelerate all search algorithms.Overall, Bayes-Neural is the most sample-efficient, dominating in the unbiased case and remaining near the top when biases are enabled.</p>
<p>Algorithm 1 Bayes-Neural Hybrid Search (Python-style pseudocode) 1: procedure RunExperiments(initState, policy, numRuns)
2:results ← [ ]3:for i = 1 to numRuns do4:results.append BayesNeuralSearch(initState, policy)5:end for6:return results7: end procedure8: procedure BayesNeuralSearch(initState, policy)9:state ← initState10:for all p ∈ P RIM IT IV ES do11:α p , β p ← 1, 112:end for13:f lags ← ∅14:
In support of this, he mentioned that in the history of science, once certain constraints were agreed upon, different scientists without communicating with each other came to the same conclusion(Chomsky, 2006) 
Type theory feels natural because it captures the core of intuitive types-basic distinctions between kinds like numbers and places-without adding unnecessary structure to what those distinctions mean. Its compositional rules reflect how we build complex intuitive types in language, and its constraints filter out incompatible ones before conscious reasoning begins.
Type names are set in monospaced font to emphasize their formal role and set them apart from the surrounding text. The same convention is used for discovery mechanism names to highlight their central place in the formalism.
Typically, functions from the lambda calculus, a compact formal language of functions, but also computer programs (e.g., in Python).
Constraints are chosen over raw data, since the former are the input that guides the conceptual thinker and prompted the kind of reasoning Maxwell used to discover his electrodynamics laws. He was not trying to fit an equation to the positions of the iron filings around a magnet, as depicted in Faraday's magnetic iron filings diagram. Instead, he developed a theory that captured the qualitative constraints revealed by the diagram(Maxwell, 1865).
In this paper's type system, a term may have more than one type.
In a typing rule, the horizontal bar denotes that the typing judgment written beneath is derived from the typing judgments above. In contrast, the entailment relation ⊢ indicates that the judgment on its right is derivable in the context on its left; the assumptions are required, yet their presence alone is not enough to derive the judgment.
 Context extension (comma)  forms the context Γ, Γi by appending Γi to Γ and requires that Γi introduce no duplicate variables. In contrast, we later use context union (∪) to merge two independently built contexts that may share variable names.
In lambda calculus, function abstraction defines a function and is written λx.t, meaning "the function that takes x and returns t". Moreover, function application uses a function and is written t1t2, meaning "apply the term t1 to the argument term t2."
While we use distinct notation here to clarify the different notions of equality, in Sections 5 and 6 the intended notion of equality will be inferred from context
Differing only in the names of bound variables is known as α-equivalence in lambda calculus
via δ-reduction, in the language of lambda calculus
"Metalevel" refers to the explanatory metalanguage used to talk about the formal system. The symbol | belongs to this meta-language; it is not part of the object language (terms and types). This convention carries no additional type-theoretic content.
This differs from the concept distinction rule in the cognitive typing rules section. Here the equality is between functions of variables (e.g., f (x) and g(y)), not between variables themselves.
To help visualize this, a useful analogy of this effect is how raindrops appear to fall at a tilted angle when observed by someone walking.
The transformations of the field quantities turn out to play no role in the final result. All that matters to track the velocity of a wave are the locations at which the field intensity drops to zero; these are unaffected by the field transformations. As a result, I can still simply represent a propagating wave as f (ωt − ky) where f stands for the multivalued field intensities.
Inspired by Peano(Poesia &amp; Goodman, 2023), we keep the number of judgments finite at each step by only allowing in-line function applications (not in-line function abstractions) and applying them solely to existing arguments.
AcknowledgementsWe are grateful to Stefano Profumo and Vidyesh Rao for helpful feedback.This research was supported by the Foundational Questions Institute and by the Faggin Presidential Chair Fund.Judgment(ctx lorentz, "tprime", "t -u<em>x/c</em><em>2", Art)Step 0Judgment(ctx lorentz, "", "f(-k</em>y + w(t -u<em>x/c</em><em>2))", Art)Step 1Judgment(ctx lorentz, "", "f(-k</em>y + t<em>w -u</em>w<em>x/c</em><em>2)", Art)Step 2Judgment(ctx lorentz, "", "f(-k</em>y + t<em>w -k</em>u<em>x/c)", Art)Step 3Step 4Step 5Step 6Judgment(ctx combined, "", "f(-k</em>y + w(t -u<em>x/c</em><em>2))", Art ∧ Emp)Step 7Judgment(ctx combined, "tprime", "t -u</em>x/c**2", Art ∧ Emp)StepToward theory-building AI-assistantsViewed more broadly, typing rules and inductive biases provided us with a way to explicitly encode aspects of Einstein's discovery style and taste preferences.Looking ahead, these rules and biases could be used to encode preferences for concrete instances of other discovery strategies: favoring simple or complex analogies(Hofstadter &amp; Sander, 2013;Maxwell, 1865); importing new mathematics into physics(Arkani-Hamed &amp; Trnka, 2014;Crutchfield &amp; Young, 1989); crafting thought experiments(Norton, 2004); or pushing a well-established theory to its ultimate consequences, à la John Wheeler's radical conservatism(Thorne, 2019).In turn, this could allow us to build theory-building AI assistants that help us transfer these discovery strategies to new domains and, when appropriate, refashion them into new mechanisms, fostering conceptual leaps.These theory-building AI assistants would serve as analogs of computer algebra systems ((Ellis, 2020), p. 185), but for conceptual work.They would support the conceptual modeling of not well understood physical phenomena, rather than merely performing symbolic manipulations of the equations that describe them.Their goal would be to aid scientists in concept discovery, support informed conceptual risk-taking, and work alongside scientists rather than act as answering machines intended to replace them (even if not very good replacements).
Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton. S Aaronson, S Carroll, L Ouellette, arXiv:1405.69032014cond-mat.stat-mech</p>
<p>Engineering the Future: What We Should Do Instead. A Aguirre, Keep the future human: Why and how we should close the gates to AGI and superintelligence, and what we should build instead. 2025</p>
<p>TPS: A theoremproving system for classical type theory. P B Andrews, M Bishop, S Issar, D Nesmith, F Pfenning, H Xi, 10.1007/BF00252180Journal of Automated Reasoning. 1631996</p>
<p>The Amplituhedron. N Arkani-Hamed, J Trnka, 10.1007/JHEP10(2014)030Journal of High Energy Physics. 30102014. 2014</p>
<p>P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, . . Pascanu, R , arXiv:1806.01261[cs.LG]Relational inductive biases, deep learning, and graph networks. 2018</p>
<p>Artificial intelligence for science: The easy and hard problems. R M Battleday, S J Gershman, arXiv:2408.14508[cs.AI]2024</p>
<p>Formalising mathematics: Release 0.1. Imperial College London. K Buzzard, 2024Xena Project</p>
<p>Type systems. A. L Cardelli, The CRC handbook of computer science and engineering. B Tucker, CRC Press1996</p>
<p>Syntactic Structures. N Chomsky, Language and Mind. Mouton, N Chomsky, Cambridge University Press1957. 20063rd ed.</p>
<p>Detection of topological materials with machine learning. N Claussen, B A Bernevig, N Regnault, 10.1103/PhysRevB.101.245117Physical Review B. 101242451172020</p>
<p>The Calculi of Emergence: Computation, Dynamics, and Induction. J P Crutchfield, 10.1016/0167-2789(94)90273-9Physica D: Nonlinear Phenomena. 751-31994</p>
<p>Inferring statistical complexity. J P Crutchfield, K Young, 10.1103/PhysRevLett.63.105Phys. Rev. Lett. 6321989</p>
<p>Against theory-motivated experimentation in science. M Dubova, A Moskvichev, K Zollman, 10.31222/osf.io/ysv2uMetaArXiv (OSF Preprints. 2022</p>
<p>Algorithms for Learning to Induce Programs (Doctoral dissertation). K Ellis, 2020Massachusetts Institute of Technology</p>
<p>Why program synthesis is next. K Ellis, Z Tavares, Machine Learning Street Talk (YouTube). 2025</p>
<p>The 'orthodox' view of theories: Remarks in defense as well as critique. H Feigl, Analyses of theories and methods of physics and psychology. M Radner, S Winokur, University of Minnesota Press19704</p>
<p>Structure-mapping: A theoretical framework for analogy. D Gentner, 10.1207/s15516709cog0702_3Cognitive Science. 721983</p>
<p>Scientific and Technological Thinking. M E Gorman, R D Tweney, D C Gooding, Kincannon, A.P.2005Lawrence Erlbaum Associates</p>
<p>Patterns of Discovery: An Inquiry into the Conceptual Foundations of Science. N R Hanson, 1958Cambridge University Press</p>
<p>Practical Foundations for Programming Languages. R Harper, 2016Cambridge University Press2nd ed.</p>
<p>Surfaces and Essences: Analogy as the Fuel and Fire of Thinking. D R Hofstadter, E Sander, 2013Basic Books</p>
<p>T S Kuhn, The Structure of Scientific Revolutions. University of Chicago Press1962</p>
<p>F Lalande, Y Matsubara, N Chiba, T Taniai, R Igarashi, Y Ushiku, arXiv 2312.04070 [cs.LG]A Transformer Model for Symbolic Regression towards Scientific Discovery. 2023</p>
<p>Z Li, N Kovachki, K Azizzadenesheli, B Liu, K Bhattacharya, A M Stuart, A Anandkumar, arXiv 2010.08895 [cs.LG]Fourier Neural Operator for Parametric Partial Differential equations. 2020</p>
<p>We're afraid language models aren't modeling ambiguity. A Liu, Z Wu, J Michael, A Suhr, P West, A Koller, . . Choi, Y , Proceedings of the 2023 conference on empirical methods in natural language processing. the 2023 conference on empirical methods in natural language processingAssociation for Computational Linguistics2023</p>
<p>Attempt of a Theory of Electrical and Optical Phenomena in Moving Bodies. H A Lorentz, 1895E. J. Brill</p>
<p>C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv 2408.06292 [cs.AI]The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024</p>
<p>Model-Based Reasoning in Science. Technology, and Medicine. Magnani, L., &amp; Li, P.642007Springer</p>
<p>On Physical Lines of Force. J C Maxwell, The Scientific Papers of James Clerk Maxwell. W D Niven, Dover Publications18651</p>
<p>Explain the Black Box for the Sake of Science: Revisiting the Scientific Method in the Era of Generative Artificial Intelligence. G Mengaldo, arXiv 2406.10557 [cs.AI]2024</p>
<p>A Cognitive-Historical Approach to Meaning in Scientific Theories. The process of science: Contemporary philosophical approaches to understanding scientific practice. N J Nersessian, 1987Martinus Nijhoff Publishers3</p>
<p>Maxwell and "The Method of Physical Analogy": Model-based Reasoning, Generic Abstraction, and Conceptual Change. Reading natural philosophy: Essays in the history and philosophy of science and mathematics. N J Nersessian, Open Court. 2002</p>
<p>Creating scientific concepts. N J Nersessian, 2008MIT Press</p>
<p>Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. F Noe, S Olsson, J Kohler, H Wu, 10.1126/science.aaw1147Science. 36564572019</p>
<p>Einstein's investigations of Galilean covariant electrodynamics prior to 1905. J D Norton, 10.1007/s00407-004-0085-6Archive for History of Exact Sciences. 5912004</p>
<p>Einstein's Special Theory of Relativity and the Problems in the Electrodynamics of Moving Bodies that Led Him to It. J D Norton, The cambridge companion to Einstein. M Janssen, C Lehner, Cambridge University Press2005</p>
<p>Geometry from a Time Series. N H Packard, J P Crutchfield, J D Farmer, R S Shaw, 10.1103/PhysRevLett.45.712Physical Review Letters. 4591980</p>
<p>Types and Programming Languages. B C Pierce, 2002MIT Press</p>
<p>LCF Considered as a Programming Language. G D Plotkin, 10.1016/0304-3975(77)90044-5Theoretical Computer Science. 51977</p>
<p>Peano: Learning Formal Mathematical Reasoning. G Poesia, N D Goodman, 10.1098/rsta.2022.0044Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 3812023. 20220044</p>
<p>The Child as Hacker. J S Rule, J B Tenenbaum, S T Piantadosi, 10.1016/j.tics.2020.07.005Trends in Cognitive Sciences. 24112020</p>
<p>B Russell, The Principles of Mathematics. Cambridge University Press1903</p>
<p>Classical dynamical coarse-grained entropy and comparison with the quantum version. D Šafránek, A Aguirre, J M Deutsch, 10.1103/PhysRevE.102.032106Physical Review E. 1023321062020</p>
<p>A Brief Introduction to Observational Entropy. D Šafránek, A Aguirre, J Schindler, J M Deutsch, 10.1007/s10701-021-00498-xFoundations of Physics. 5142021</p>
<p>Distilling Free-Form Natural Laws from Experimental Data. M Schmidt, H Lipson, 10.1126/science.1165893Science. 32459232009</p>
<p>SchNet: A deep learning architecture for molecules and materials. K T Schütt, H E Sauceda, P.-J Kindermans, A Tkatchenko, K.-R Müller, 10.1063/1.5019779The Journal of Chemical Physics. 1482417222018</p>
<p>Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90. C J Shallue, A Vanderburg, 10.3847/1538-3881/aa9e09The Astronomical Journal. 1552018</p>
<p>Does scientific discovery have a logic?. H A Simon, 10.1086/288559Philosophy of Science. 4041973</p>
<p>Scientific discovery as problem solving. H A Simon, P W Langley, G L Bradshaw, 10.1007/BF01064262Synthese. 198147</p>
<p>Church's type theory. P Smith, The Stanford Encyclopedia of Philosophy. E N Zalta, 2024. Spring 2024Stanford UniversityMetaphysics Research Lab</p>
<p>F A Sosa, T D Ullman, arXiv:2210.01634[cs.AI]Type Theory in Human-Like Learning and Inference. 2022</p>
<p>What babies know: Core knowledge and composition. E S Spelke, 2022Oxford University Press</p>
<p>Computational philosophy of science. P Thagard, 1988MIT Press</p>
<p>P Thagard, The Cognitive Science of Science: Explanation, Discovery, and Conceptual Change. MIT Press2012</p>
<p>K S Thorne, arXiv:1901.06623John Archibald Wheeler: A Biographical Memoir. 2019physics.hist-ph</p>
<p>AI Feynman 2.0: Paretooptimal symbolic regression exploiting graph modularity. S.-M Udrescu, A Tan, J Feng, O Neto, T Wu, M Tegmark, Advances in neural information processing systems. 202033</p>
<p>AI Feynman: A Physics-Inspired Method for Symbolic Regression. S.-M Udrescu, M Tegmark, 10.1126/sciadv.aay2631Science Advances. 61626312020</p>
<p>From Probability to Consilience: How Explanatory Values Implement Bayesian Reasoning. Z Wojtowicz, S Dedeo, 10.1016/j.tics.2020.09.013Trends in Cognitive Sciences. 24122020</p>
<p>Can AI Solve Science? Stephen Wolfram Writings. S Wolfram, 2024</p>
<p>A Stochastic Model of Mathematics and Science. David H Wolpert, David B Kinney, 10.1007/s10701-024-00755-9Foundations of Physics. 542024</p>
<p>Deep Potential Molecular Dynamics: A Scalable Model with the Accuracy of Quantum Mechanics. L Zhang, J Han, H Wang, R Car, E , W , 10.1103/PhysRevLett.120.143001Physical Review Letters. 120141430012018</p>            </div>
        </div>

    </div>
</body>
</html>
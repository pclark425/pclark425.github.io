<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2015 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2015</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2015</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-280338149</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.21257v1.pdf" target="_blank">CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting</a></p>
                <p><strong>Paper Abstract:</strong> Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they"understand"the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2015.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2015.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CompoST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compositional Systematicity Test (CompoST) benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled QALD (Question Answering over Linked Data) benchmark that tests systematic compositional generalization of LLMs by generating DBpedia-based SPARQL graph patterns (pitchfork/star patterns) and their verbalizations along with all connected sub-patterns; available in classic and self-contained variants and split into easy/medium/hard by proportion of pattern edges included in training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>benchmark (task-level; evaluated multiple LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model — a benchmark and experimental protocol for mapping natural language questions to executable SPARQL queries over DBpedia, designed to isolate systematic compositionality by controlling graph-pattern constituents and verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>N/A (benchmark); tasks require models to output executable SPARQL; tests compositional assembly of subgraph building blocks</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (knowledge-base question answering / semantic parsing to SPARQL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CompoST (Compositional Systematicity Test) for QALD</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an English question verbalizing a DBpedia graph pattern, generate an executable SPARQL query retrieving the correct result set. Patterns are constructed as 'pitchfork-like' (star) graphs of variable depth and breadth; dataset contains all connected sub-patterns so sufficiency of building blocks is known. Classic tasks require model to reason compositionally using training sub-patterns; self-contained tasks additionally provide an edge-set cover (examples) in the input.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Graph patterns with depth 1–3 and breadth 1–3 (paper reports experiments up to depth=3, breadth=3)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Nested logical composition of triple patterns / graph-subgraph composition (combining atomic triple-patterns into multi-hop and branching SPARQL queries)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Controlled OOD-style splits: 'easy' (train contains sub-patterns up to 75% of base pattern edges), 'medium' (50%), 'hard' (25%); test sets contain larger/novel combinations of seen primitives (novel structural patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Zero-shot prompting, few-shot prompting (in-context learning with optimized shot selection), and supervised fine-tuning; prompt-optimization methods used include COPRO and MIPROv2 (light/medium/heavy), BootstrapFewShotWithRandomSearch, LabeledFewShot; experiments run both with and without chain-of-thought prompting. Self-contained variant supplies edge-set covers (up to 10 examples) in the input.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Self-contained tasks: the input includes an explicit edge set cover of the target graph taken from the train dataset (up to 10 example items) to simulate ideal compositional information being present in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Not a single IID number; for fine-tuned models training/validation macro F1 were high (training macro F1 typically ~0.92–0.95 across datasets; validation macro F1 varied by dataset: ~0.34–0.58), indicating near-perfect behavior on seen/training-like items.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>On compositional generalization (test sets with novel combinations): classic (non-self-contained) best macro F1 (fine-tuned) = 0.45 (easy), 0.26 (medium), 0.09 (hard). Self-contained (all necessary info present) best macro F1 test scores ranged from ~0.30 up to 0.57 depending on dataset and method. Compositionality-adjusted macro F1 (where TP requires all subproblems correct) did not exceed 0.31 for classic; test compositionality F1: easy 0.31, medium 0.22, hard 0.10; self-contained compositionality F1 up to 0.57 (best case), with hard dataset best compositionality = 0.43.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Very large gaps between training and compositional test performance for fine-tuned models: e.g., training macro F1 ~0.92–0.95 to test macro F1 0.45 (easy) -> gap ≈0.47; to 0.26 (medium) -> gap ≈0.68; to 0.09 (hard) -> gap ≈0.86. Larger structural deviation from training causes steeper drops.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Single-triple (atomic) queries perform relatively well (single-triple macro F1 ≈0.51 for some few-shot runs); increasing depth and/or breadth substantially reduces performance. Fine-tuned model performance drops close to 0.01 for combined depth=3 & breadth=3 in the hard dataset (example reported). Generally, accuracy sharply declines as the number of edges to compose increases beyond training coverage (adding one extra edge already more than doubles incorrect results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>Reported breakdowns emphasize branching (breadth) and chaining (depth) effects: both increased breadth and increased depth independently hurt performance; no fine-grained split-by-phenomena percentages beyond depth/breadth reported. Compositional failures appear mainly when multiple atomic building blocks must be combined (multi-hop and branching patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared zero-shot prompting, several prompt-optimization strategies, few-shot in-context learning (various shot selection algorithms), and supervised fine-tuning; also compared classic (only question) vs self-contained (question + edge-set cover examples) variants. Findings: zero-shot performed very poorly (macro F1 0.01–0.04); few-shot improved but remained weak (single-triple best ~0.51 macro F1); fine-tuning outperformed in-context learning on classic tasks, but for self-contained tasks few-shot sometimes outperformed fine-tuning (e.g., easy self-contained: few-shot 0.57 vs fine-tuned 0.48; hard self-contained few-shot 0.51 vs fine-tuned 0.30).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Evaluated multiple off-the-shelf Transformer LLMs: Llama 3.3 (70B), GPT-4o-mini (API), Phi-4 (14B), Qwen2.5-Coder (7B), OLMo 2 (7B); relative performance: best-performing configurations were usually either Llama 3.3 (few-shot/optimized prompting) or GPT-4o-mini (fine-tuned). No architecture with explicit compositional bias was tested beyond standard Transformer LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Multiple model sizes tested (70B, 14B, 7B) but authors do not report a consistent, systematic scaling trend for compositional generalization; best performances came from either the largest tested (Llama 3.3, 70B) or GPT-4o-mini, but scaling did not eliminate the compositionality gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) LLMs struggle to systematically compose atomic SPARQL building blocks into larger queries: classic fine-tuned macro F1 drops from 0.45 (easy) to 0.26 (medium) to 0.09 (hard). 2) Even when all necessary information is provided in-input (self-contained), macro F1 never exceeded 0.57 and compositionality-adjusted F1 did not exceed 0.57 (best) and was often much lower, indicating limited compositional assembly ability. 3) Fine-tuning greatly reduces errors on training-like items (training macro F1 ≈0.92–0.95) but generalizes poorly to structurally larger/novel queries (large train→test gaps up to ≈0.86). 4) Depth and breadth of graph patterns independently and jointly reduce performance, with near-zero performance on depth=3 & breadth=3 for some fine-tuned setups.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Models commonly fail when asked to combine more atomic constituents than seen during training (even +1 edge often doubles incorrect answers). Many correct outputs on target items are not compositional (i.e., subproblems are incorrect despite overall correct result), and most non-training items violate systematicity. Errors concentrate on multi-hop chaining and branching combinations; fine-tuned models overfit to training sub-pattern sizes and do not robustly generalize to larger graph patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Models succeed when tasks are within training distribution or when all atomic building blocks and their combinations are explicitly present in training input (i.e., atomic/training-like subgraphs, up to two edges in examples). Self-contained inputs (edge-set cover examples included) and carefully selected few-shot examples improve performance, but only up to modest levels and mainly for shallower/smaller patterns.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2015.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2015.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini (eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (evaluated variant via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary Transformer-based large language model used in the paper as one of the best-performing models; employed both for in-context (few-shot/zero-shot) experiments and fine-tuning (via OpenAI API) to map NL questions to SPARQL queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model (decoder-style generative LLM); used via OpenAI API for zero-shot, few-shot prompting, and fine-tuning in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Standard Transformer attention architecture (decoder/generative LLM); no bespoke compositional modules used</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (QALD: natural language → SPARQL over DBpedia)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CompoST evaluation (fine-tuned and in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate executable SPARQL queries from verbalized DBpedia graph patterns; evaluated on classic and self-contained variants across easy/medium/hard splits measuring macro F1 and compositionality-adjusted F1.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Tested up to depth 3 and breadth 3 graph patterns</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Nested triple-pattern composition and branching (multi-hop and conjunction/branching 'and' constructs)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Easy/medium/hard splits by proportion of base pattern edges included in training (75%/50%/25%); tests novel combinations of seen primitives</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Fine-tuning on training split (supervised) and experiments with zero-shot and few-shot in-context prompting with prompt-optimization (MIPROv2, COPRO) and chain-of-thought variants; for self-contained variant fine-tuned on dataset with example edge-set covers.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Self-contained fine-tuning: model was given the relevant edge set cover of example items during fine-tuning (i.e., examples explicitly containing all necessary sub-patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Training macro F1 reported ~0.92–0.95 depending on dataset; validation macro F1 ranged (dataset-dependent) ~0.34–0.58 — indicating high performance on training-like examples.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Fine-tuned GPT-4o-mini classic test macro F1: easy 0.45, medium 0.26, hard 0.09. Compositionality-adjusted test F1 (classic): easy 0.31, medium 0.22, hard 0.10. Self-contained fine-tuned results improved in some conditions (e.g., self-contained test compositionality F1 up to ~0.53 reported for easy), but did not exceed macro F1 ≈0.57 on easiest self-contained tests.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Large: e.g., training macro F1 ~0.92 to test macro F1 0.45 (easy) -> gap ≈0.47; to 0.26 (medium) -> gap ≈0.66; to 0.09 (hard) -> gap ≈0.83–0.86.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Fine-tuned GPT-4o-mini performs almost perfectly on training data but generalizes poorly with increasing depth/breadth; in reported hard-dataset breakdown, performance approaches ~0.01 for depth=3 & breadth=3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>No per-phenomena numeric breakdown beyond depth/breadth; failures concentrated on multi-hop chaining and branching compositions (combining multiple atoms).</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to zero-shot and few-shot prompting with other LLMs (Llama 3.3, Phi-4, Qwen2.5-Coder, OLMo 2). Fine-tuning GPT-4o-mini outperformed in-context learning on classic tasks, but for self-contained tasks few-shot sometimes outperformed GPT-4o-mini fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Compared against other Transformer LLMs (Llama 3.3 70B, Phi-4 14B, Qwen2.5-Coder 7B, OLMo 2 7B). GPT-4o-mini (when fine-tuned) was the best fine-tuned model reported on classic tasks; Llama 3.3 often matched or slightly outperformed in few-shot conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>No explicit scaling study for GPT-4o-mini parameters in paper; comparisons are cross-model rather than within-model scaling analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning GPT-4o-mini substantially raises performance on training-like items but does not solve compositional generalization; test macro F1 falls dramatically as structural novelty increases (0.45→0.26→0.09 across easy→medium→hard). Compositionality-adjusted F1 similarly low (max 0.31 classic).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Fails to compose atomic building blocks into larger SPARQL queries when required to combine more edges than seen during fine-tuning; overfits to seen sub-pattern sizes and exhibits near-zero performance on deep/broad graphs beyond training coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeeds for atomic or training-like sub-patterns and when training contains ample coverage of the required sub-pattern edges; modest success when provided with self-contained edge-set cover examples (but still limited).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2015.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2015.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3.3 (eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.3 (70B) evaluated via Ollama</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large 70B-parameter Transformer LLM used as one of the best-performing models in few-shot and zero-shot prompting experiments with prompt-optimization (MIPRO variants); often the top model for in-context methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Transformer-based LLM (70B parameters as reported); used via Ollama with Q4_K_M quantization for prompt- and few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Standard Transformer architecture with attention; no additional compositional modules used</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (QALD SPARQL generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CompoST evaluation (zero-shot and few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate SPARQL queries for DBpedia questions; evaluated in zero-shot, prompt-optimized few-shot, and self-contained few-shot variants across easy/medium/hard splits.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td>Up to depth 3, breadth 3 graph patterns tested</td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>Composition of triple patterns into multi-hop and branching SPARQL queries</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>Novel structural combinations vs training coverage (easy/medium/hard by % edges in training)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>No fine-tuning for Llama 3.3 in most reported experiments; used zero-shot and few-shot prompting with prompt-optimization (MIPROv2 light/medium/heavy; COPRO) and chain-of-thought prompting; also used BootstrapFewShotWithRandomSearch for shot selection in some cases. Self-contained few-shot variants provided edge-set covers in input.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td>Self-contained few-shot experiments included up to 10 example items (edge-set cover) in input for the target pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td>Zero-shot and few-shot training/validation metrics vary; zero-shot macro F1 reported very low (e.g., 0.01–0.04). Few-shot improved results but still modest: single-triple few-shot macro F1 ~0.51 in examples; self-contained few-shot test macro F1 reached up to 0.57 (easy dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Few-shot in-context best performance occurred with Llama 3.3 for some configurations: self-contained easy test macro F1 reported ≈0.57 and compositionality F1 ≈0.52; classic few-shot performance lower (classic test macro F1 often <0.10 in zero-shot/few-shot baselines for non-self-contained tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td>Large gap between few-shot validation and held-out compositional tests: few-shot improves over zero-shot but still fails dramatically when structural novelty increases (no exact per-model train→test gaps as Llama was not fine-tuned in classic experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td>Atomic (single-triple) tasks handled reasonably well under few-shot; increasing depth/breadth reduces performance substantially—same qualitative pattern as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td>No detailed per-phenomenon breakdown beyond depth/breadth; failures concentrate on multi-hop and branching compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against GPT-4o-mini (fine-tuned and in-context), Phi-4, Qwen2.5-Coder, and OLMo 2; Llama 3.3 often achieved top in-context/few-shot results; fine-tuned GPT-4o-mini outperformed Llama on classic fine-tuned tasks but self-contained few-shot Llama sometimes beat fine-tuned GPT-4o-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Compared as an off-the-shelf Transformer LLM (70B) to smaller models (14B, 7B) and GPT-4o-mini; no structural architectural modifications tested.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Single large-size model (70B) performed best among in-context approaches, suggesting scale helps in few-shot conditions but does not eliminate compositional failures.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama 3.3 with optimized few-shot prompting is among the best in-context performers and, in self-contained setups, reaches macro F1 up to ~0.57 (easy) with compositionality F1 ~0.52, but still fails on structurally novel medium/hard compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Same failure modes as other models: inability to reliably combine atomic building blocks into larger unseen graph patterns; performance drops steeply with depth and breadth.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Performs well when shot selection is optimized and when self-contained examples (edge-set cover) are provided; works best for shallower/smaller patterns.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SCAN <em>(Rating: 2)</em></li>
                <li>CFQ <em>(Rating: 2)</em></li>
                <li>COGS <em>(Rating: 2)</em></li>
                <li>Faith and fate: limits of transformers on compositionality <em>(Rating: 2)</em></li>
                <li>Measuring and Narrowing the Compositionality Gap in Language Models <em>(Rating: 2)</em></li>
                <li>Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2015",
    "paper_id": "paper-280338149",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "CompoST",
            "name_full": "Compositional Systematicity Test (CompoST) benchmark",
            "brief_description": "A controlled QALD (Question Answering over Linked Data) benchmark that tests systematic compositional generalization of LLMs by generating DBpedia-based SPARQL graph patterns (pitchfork/star patterns) and their verbalizations along with all connected sub-patterns; available in classic and self-contained variants and split into easy/medium/hard by proportion of pattern edges included in training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "benchmark (task-level; evaluated multiple LLMs)",
            "model_description": "Not a model — a benchmark and experimental protocol for mapping natural language questions to executable SPARQL queries over DBpedia, designed to isolate systematic compositionality by controlling graph-pattern constituents and verbalizations.",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": "N/A (benchmark); tasks require models to output executable SPARQL; tests compositional assembly of subgraph building blocks",
            "task_domain": "linguistic/semantic (knowledge-base question answering / semantic parsing to SPARQL)",
            "task_name": "CompoST (Compositional Systematicity Test) for QALD",
            "task_description": "Given an English question verbalizing a DBpedia graph pattern, generate an executable SPARQL query retrieving the correct result set. Patterns are constructed as 'pitchfork-like' (star) graphs of variable depth and breadth; dataset contains all connected sub-patterns so sufficiency of building blocks is known. Classic tasks require model to reason compositionally using training sub-patterns; self-contained tasks additionally provide an edge-set cover (examples) in the input.",
            "compositional_depth": "Graph patterns with depth 1–3 and breadth 1–3 (paper reports experiments up to depth=3, breadth=3)",
            "composition_type": "Nested logical composition of triple patterns / graph-subgraph composition (combining atomic triple-patterns into multi-hop and branching SPARQL queries)",
            "split_type": "Controlled OOD-style splits: 'easy' (train contains sub-patterns up to 75% of base pattern edges), 'medium' (50%), 'hard' (25%); test sets contain larger/novel combinations of seen primitives (novel structural patterns)",
            "training_strategy": "Zero-shot prompting, few-shot prompting (in-context learning with optimized shot selection), and supervised fine-tuning; prompt-optimization methods used include COPRO and MIPROv2 (light/medium/heavy), BootstrapFewShotWithRandomSearch, LabeledFewShot; experiments run both with and without chain-of-thought prompting. Self-contained variant supplies edge-set covers (up to 10 examples) in the input.",
            "curriculum_details": null,
            "inoculation_details": "Self-contained tasks: the input includes an explicit edge set cover of the target graph taken from the train dataset (up to 10 example items) to simulate ideal compositional information being present in-context.",
            "iid_performance": "Not a single IID number; for fine-tuned models training/validation macro F1 were high (training macro F1 typically ~0.92–0.95 across datasets; validation macro F1 varied by dataset: ~0.34–0.58), indicating near-perfect behavior on seen/training-like items.",
            "compositional_performance": "On compositional generalization (test sets with novel combinations): classic (non-self-contained) best macro F1 (fine-tuned) = 0.45 (easy), 0.26 (medium), 0.09 (hard). Self-contained (all necessary info present) best macro F1 test scores ranged from ~0.30 up to 0.57 depending on dataset and method. Compositionality-adjusted macro F1 (where TP requires all subproblems correct) did not exceed 0.31 for classic; test compositionality F1: easy 0.31, medium 0.22, hard 0.10; self-contained compositionality F1 up to 0.57 (best case), with hard dataset best compositionality = 0.43.",
            "generalization_gap": "Very large gaps between training and compositional test performance for fine-tuned models: e.g., training macro F1 ~0.92–0.95 to test macro F1 0.45 (easy) -&gt; gap ≈0.47; to 0.26 (medium) -&gt; gap ≈0.68; to 0.09 (hard) -&gt; gap ≈0.86. Larger structural deviation from training causes steeper drops.",
            "performance_by_depth": "Single-triple (atomic) queries perform relatively well (single-triple macro F1 ≈0.51 for some few-shot runs); increasing depth and/or breadth substantially reduces performance. Fine-tuned model performance drops close to 0.01 for combined depth=3 & breadth=3 in the hard dataset (example reported). Generally, accuracy sharply declines as the number of edges to compose increases beyond training coverage (adding one extra edge already more than doubles incorrect results).",
            "performance_by_composition_type": "Reported breakdowns emphasize branching (breadth) and chaining (depth) effects: both increased breadth and increased depth independently hurt performance; no fine-grained split-by-phenomena percentages beyond depth/breadth reported. Compositional failures appear mainly when multiple atomic building blocks must be combined (multi-hop and branching patterns).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared zero-shot prompting, several prompt-optimization strategies, few-shot in-context learning (various shot selection algorithms), and supervised fine-tuning; also compared classic (only question) vs self-contained (question + edge-set cover examples) variants. Findings: zero-shot performed very poorly (macro F1 0.01–0.04); few-shot improved but remained weak (single-triple best ~0.51 macro F1); fine-tuning outperformed in-context learning on classic tasks, but for self-contained tasks few-shot sometimes outperformed fine-tuning (e.g., easy self-contained: few-shot 0.57 vs fine-tuned 0.48; hard self-contained few-shot 0.51 vs fine-tuned 0.30).",
            "architectural_comparison": "Evaluated multiple off-the-shelf Transformer LLMs: Llama 3.3 (70B), GPT-4o-mini (API), Phi-4 (14B), Qwen2.5-Coder (7B), OLMo 2 (7B); relative performance: best-performing configurations were usually either Llama 3.3 (few-shot/optimized prompting) or GPT-4o-mini (fine-tuned). No architecture with explicit compositional bias was tested beyond standard Transformer LLMs.",
            "scale_effects": "Multiple model sizes tested (70B, 14B, 7B) but authors do not report a consistent, systematic scaling trend for compositional generalization; best performances came from either the largest tested (Llama 3.3, 70B) or GPT-4o-mini, but scaling did not eliminate the compositionality gaps.",
            "transfer_results": null,
            "key_findings": "1) LLMs struggle to systematically compose atomic SPARQL building blocks into larger queries: classic fine-tuned macro F1 drops from 0.45 (easy) to 0.26 (medium) to 0.09 (hard). 2) Even when all necessary information is provided in-input (self-contained), macro F1 never exceeded 0.57 and compositionality-adjusted F1 did not exceed 0.57 (best) and was often much lower, indicating limited compositional assembly ability. 3) Fine-tuning greatly reduces errors on training-like items (training macro F1 ≈0.92–0.95) but generalizes poorly to structurally larger/novel queries (large train→test gaps up to ≈0.86). 4) Depth and breadth of graph patterns independently and jointly reduce performance, with near-zero performance on depth=3 & breadth=3 for some fine-tuned setups.",
            "failure_analysis": "Models commonly fail when asked to combine more atomic constituents than seen during training (even +1 edge often doubles incorrect answers). Many correct outputs on target items are not compositional (i.e., subproblems are incorrect despite overall correct result), and most non-training items violate systematicity. Errors concentrate on multi-hop chaining and branching combinations; fine-tuned models overfit to training sub-pattern sizes and do not robustly generalize to larger graph patterns.",
            "success_conditions": "Models succeed when tasks are within training distribution or when all atomic building blocks and their combinations are explicitly present in training input (i.e., atomic/training-like subgraphs, up to two edges in examples). Self-contained inputs (edge-set cover examples included) and carefully selected few-shot examples improve performance, but only up to modest levels and mainly for shallower/smaller patterns.",
            "uuid": "e2015.0"
        },
        {
            "name_short": "GPT-4o-mini (eval)",
            "name_full": "GPT-4o-mini (evaluated variant via OpenAI API)",
            "brief_description": "A proprietary Transformer-based large language model used in the paper as one of the best-performing models; employed both for in-context (few-shot/zero-shot) experiments and fine-tuning (via OpenAI API) to map NL questions to SPARQL queries.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "Transformer-based large language model (decoder-style generative LLM); used via OpenAI API for zero-shot, few-shot prompting, and fine-tuning in experiments.",
            "model_size": null,
            "is_pretrained": true,
            "architectural_features": "Standard Transformer attention architecture (decoder/generative LLM); no bespoke compositional modules used",
            "task_domain": "linguistic/semantic (QALD: natural language → SPARQL over DBpedia)",
            "task_name": "CompoST evaluation (fine-tuned and in-context)",
            "task_description": "Generate executable SPARQL queries from verbalized DBpedia graph patterns; evaluated on classic and self-contained variants across easy/medium/hard splits measuring macro F1 and compositionality-adjusted F1.",
            "compositional_depth": "Tested up to depth 3 and breadth 3 graph patterns",
            "composition_type": "Nested triple-pattern composition and branching (multi-hop and conjunction/branching 'and' constructs)",
            "split_type": "Easy/medium/hard splits by proportion of base pattern edges included in training (75%/50%/25%); tests novel combinations of seen primitives",
            "training_strategy": "Fine-tuning on training split (supervised) and experiments with zero-shot and few-shot in-context prompting with prompt-optimization (MIPROv2, COPRO) and chain-of-thought variants; for self-contained variant fine-tuned on dataset with example edge-set covers.",
            "curriculum_details": null,
            "inoculation_details": "Self-contained fine-tuning: model was given the relevant edge set cover of example items during fine-tuning (i.e., examples explicitly containing all necessary sub-patterns).",
            "iid_performance": "Training macro F1 reported ~0.92–0.95 depending on dataset; validation macro F1 ranged (dataset-dependent) ~0.34–0.58 — indicating high performance on training-like examples.",
            "compositional_performance": "Fine-tuned GPT-4o-mini classic test macro F1: easy 0.45, medium 0.26, hard 0.09. Compositionality-adjusted test F1 (classic): easy 0.31, medium 0.22, hard 0.10. Self-contained fine-tuned results improved in some conditions (e.g., self-contained test compositionality F1 up to ~0.53 reported for easy), but did not exceed macro F1 ≈0.57 on easiest self-contained tests.",
            "generalization_gap": "Large: e.g., training macro F1 ~0.92 to test macro F1 0.45 (easy) -&gt; gap ≈0.47; to 0.26 (medium) -&gt; gap ≈0.66; to 0.09 (hard) -&gt; gap ≈0.83–0.86.",
            "performance_by_depth": "Fine-tuned GPT-4o-mini performs almost perfectly on training data but generalizes poorly with increasing depth/breadth; in reported hard-dataset breakdown, performance approaches ~0.01 for depth=3 & breadth=3.",
            "performance_by_composition_type": "No per-phenomena numeric breakdown beyond depth/breadth; failures concentrated on multi-hop chaining and branching compositions (combining multiple atoms).",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to zero-shot and few-shot prompting with other LLMs (Llama 3.3, Phi-4, Qwen2.5-Coder, OLMo 2). Fine-tuning GPT-4o-mini outperformed in-context learning on classic tasks, but for self-contained tasks few-shot sometimes outperformed GPT-4o-mini fine-tuned.",
            "architectural_comparison": "Compared against other Transformer LLMs (Llama 3.3 70B, Phi-4 14B, Qwen2.5-Coder 7B, OLMo 2 7B). GPT-4o-mini (when fine-tuned) was the best fine-tuned model reported on classic tasks; Llama 3.3 often matched or slightly outperformed in few-shot conditions.",
            "scale_effects": "No explicit scaling study for GPT-4o-mini parameters in paper; comparisons are cross-model rather than within-model scaling analyses.",
            "transfer_results": null,
            "key_findings": "Fine-tuning GPT-4o-mini substantially raises performance on training-like items but does not solve compositional generalization; test macro F1 falls dramatically as structural novelty increases (0.45→0.26→0.09 across easy→medium→hard). Compositionality-adjusted F1 similarly low (max 0.31 classic).",
            "failure_analysis": "Fails to compose atomic building blocks into larger SPARQL queries when required to combine more edges than seen during fine-tuning; overfits to seen sub-pattern sizes and exhibits near-zero performance on deep/broad graphs beyond training coverage.",
            "success_conditions": "Succeeds for atomic or training-like sub-patterns and when training contains ample coverage of the required sub-pattern edges; modest success when provided with self-contained edge-set cover examples (but still limited).",
            "uuid": "e2015.1"
        },
        {
            "name_short": "Llama 3.3 (eval)",
            "name_full": "Llama 3.3 (70B) evaluated via Ollama",
            "brief_description": "A large 70B-parameter Transformer LLM used as one of the best-performing models in few-shot and zero-shot prompting experiments with prompt-optimization (MIPRO variants); often the top model for in-context methods in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.3",
            "model_description": "Large Transformer-based LLM (70B parameters as reported); used via Ollama with Q4_K_M quantization for prompt- and few-shot experiments.",
            "model_size": "70B",
            "is_pretrained": true,
            "architectural_features": "Standard Transformer architecture with attention; no additional compositional modules used",
            "task_domain": "linguistic/semantic (QALD SPARQL generation)",
            "task_name": "CompoST evaluation (zero-shot and few-shot)",
            "task_description": "Generate SPARQL queries for DBpedia questions; evaluated in zero-shot, prompt-optimized few-shot, and self-contained few-shot variants across easy/medium/hard splits.",
            "compositional_depth": "Up to depth 3, breadth 3 graph patterns tested",
            "composition_type": "Composition of triple patterns into multi-hop and branching SPARQL queries",
            "split_type": "Novel structural combinations vs training coverage (easy/medium/hard by % edges in training)",
            "training_strategy": "No fine-tuning for Llama 3.3 in most reported experiments; used zero-shot and few-shot prompting with prompt-optimization (MIPROv2 light/medium/heavy; COPRO) and chain-of-thought prompting; also used BootstrapFewShotWithRandomSearch for shot selection in some cases. Self-contained few-shot variants provided edge-set covers in input.",
            "curriculum_details": null,
            "inoculation_details": "Self-contained few-shot experiments included up to 10 example items (edge-set cover) in input for the target pattern.",
            "iid_performance": "Zero-shot and few-shot training/validation metrics vary; zero-shot macro F1 reported very low (e.g., 0.01–0.04). Few-shot improved results but still modest: single-triple few-shot macro F1 ~0.51 in examples; self-contained few-shot test macro F1 reached up to 0.57 (easy dataset).",
            "compositional_performance": "Few-shot in-context best performance occurred with Llama 3.3 for some configurations: self-contained easy test macro F1 reported ≈0.57 and compositionality F1 ≈0.52; classic few-shot performance lower (classic test macro F1 often &lt;0.10 in zero-shot/few-shot baselines for non-self-contained tasks).",
            "generalization_gap": "Large gap between few-shot validation and held-out compositional tests: few-shot improves over zero-shot but still fails dramatically when structural novelty increases (no exact per-model train→test gaps as Llama was not fine-tuned in classic experiments).",
            "performance_by_depth": "Atomic (single-triple) tasks handled reasonably well under few-shot; increasing depth/breadth reduces performance substantially—same qualitative pattern as other models.",
            "performance_by_composition_type": "No detailed per-phenomenon breakdown beyond depth/breadth; failures concentrate on multi-hop and branching compositions.",
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against GPT-4o-mini (fine-tuned and in-context), Phi-4, Qwen2.5-Coder, and OLMo 2; Llama 3.3 often achieved top in-context/few-shot results; fine-tuned GPT-4o-mini outperformed Llama on classic fine-tuned tasks but self-contained few-shot Llama sometimes beat fine-tuned GPT-4o-mini.",
            "architectural_comparison": "Compared as an off-the-shelf Transformer LLM (70B) to smaller models (14B, 7B) and GPT-4o-mini; no structural architectural modifications tested.",
            "scale_effects": "Single large-size model (70B) performed best among in-context approaches, suggesting scale helps in few-shot conditions but does not eliminate compositional failures.",
            "transfer_results": null,
            "key_findings": "Llama 3.3 with optimized few-shot prompting is among the best in-context performers and, in self-contained setups, reaches macro F1 up to ~0.57 (easy) with compositionality F1 ~0.52, but still fails on structurally novel medium/hard compositions.",
            "failure_analysis": "Same failure modes as other models: inability to reliably combine atomic building blocks into larger unseen graph patterns; performance drops steeply with depth and breadth.",
            "success_conditions": "Performs well when shot selection is optimized and when self-contained examples (edge-set cover) are provided; works best for shallower/smaller patterns.",
            "uuid": "e2015.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SCAN",
            "rating": 2
        },
        {
            "paper_title": "CFQ",
            "rating": 2
        },
        {
            "paper_title": "COGS",
            "rating": 2
        },
        {
            "paper_title": "Faith and fate: limits of transformers on compositionality",
            "rating": 2
        },
        {
            "paper_title": "Measuring and Narrowing the Compositionality Gap in Language Models",
            "rating": 2
        },
        {
            "paper_title": "Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures",
            "rating": 2
        }
    ],
    "cost": 0.019312,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting
28 Jul 2025</p>
<p>David Maria Schmidt daschmidt@techfak.uni-bielefeld.de 
Technical Faculty
Semantic Computing Group
CITEC
Bielefeld University
BielefeldGermany</p>
<p>Raoul Schubert raoul.schubert@uni-bielefeld.de 
Technical Faculty
Semantic Computing Group
CITEC
Bielefeld University
BielefeldGermany</p>
<p>Philipp Cimiano cimiano@techfak.uni-bielefeld.de 
Technical Faculty
Semantic Computing Group
CITEC
Bielefeld University
BielefeldGermany</p>
<p>CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting
28 Jul 20250148CF1B37D73E56778E0AB3DE17B0EBarXiv:2507.21257v1[cs.AI]CompositionalityLarge Language ModelsQuestion Answering over Linked DataSemantic Web
Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts.Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries.An open question is how systematic this interpretation process is.Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional.For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization.Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks.This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they "understand" the atomic parts.We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning.Our results show that performance in terms of macro F1 degrades from 0.45 over 0.26 down to 0.09 with increasing deviation from the samples optimized on.Even when all necessary information was provided to the model in the input, the F1 scores do not exceed 0.57 for the dataset of lowest complexity.We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.</p>
<p>Introduction</p>
<p>In light of rapidly increasing capabilities of large language models (LLMs), some even proclaiming the "age of LLMs" [4], the question arises where the limits of current LLMs lie.One central question is whether LLMs are able to truly work and reason in a compositional way.For example, to cite a classic example from Zoltán G. Szabó [45], a compositional system understanding both "brown dog" and "black cat" should understand "brown cat" as well.More precisely, Szabó divides compositionality into two sub-properties, namely productivity, describing the ability to produce utterances never encountered before, and systematicity, describing the ability to understand other combinations of known constituents.As figuring out what LLMs have seen during pre-training is notoriously hard if at all possible, in this paper, we focus on a structured investigation of systematicity.</p>
<p>The question of whether LLMs can work in a compositional way has been investigated from different perspectives already, e.g., from a more (complexity-) theoretical point of view [8,14,29,44,55], or with focus on arithmetics [12,28] or logic puzzles [12].Many tasks and works deal with compositionality in one way or another, either directly [12,20,37], or indirectly by exploring multi-step compositional tasks [30,40,49,53].While some of these also investigate question answering [37], the task of mapping natural language questions into SPARQL queries (i.e., Question Answering over Linked Data, QALD) [43], to the best of our knowledge, still lacks a structured investigation of the abilities of LLMs to interpret questions in terms of a formal query language in a systematic way.</p>
<p>As discussed by Dziri et al. [12], many tasks do not require as much compositional behavior as it initially seems, leaving room for overfitting and shortcut learning.This raises the need for specialized datasets, testing the compositional power of LLMs in isolation for different domains.Therefore, in this paper, we present CompoST, a Compositional Systematicity Test dataset for QALD.In comparison to other datasets, this dataset allows us to test the degree of systematicity of LLM behavior in a controlled setting where irrelevant confounders are removed, so that we can focus on the assessment of the systematicity in isolation from other capabilities.In order to create a well-founded dataset, we first operationalize the concept of compositionality w.r.t.[45] and then generate a corresponding dataset to test the sub-property of systematicity.Finally, we conduct a broad set of zero-shot, few-shot and fine-tuning experiments with this dataset.More precisely, we delve into the following research questions: RQ1.How can compositionality be operationalized for the question answering over linked data context?RQ2.Do large language models satisfy the defined property of systematicity when using in-context learning?RQ3.Does fine-tuning improve the abilities of large language models to work in a compositional manner?</p>
<p>2 Related Work</p>
<p>Compositional Generalization in NLP</p>
<p>Compositional generalization in terms of the ability to interpret new combinations of known components has emerged as a critical challenge in evaluating the generalization capacity of neural models.Datasets such as SCAN [26], CFQ [22] and COGS [25] have been designed to test this ability under controlled syntactic and semantic conditions: SCAN focuses on mapping simple commands to actions in a synthetic navigation task, CFQ tests generalization over complex SPARQLlike logical forms derived from Freebase and COGS evaluates structural generalization from surface syntax to abstract meaning representations.Unlike QALD tasks, these datasets do not require the generation of executable queries or the linking of expressions to real-world entities.In SCAN, Lake and Baroni [26] observed that RNN-based models failed to generalize compositionally, succeeding only when test sequences were very similar to those seen during training.Keysers et al. [22] found that Transformer models performed well on CFQ when trained and tested on similar compositions, but accuracy sharply declined when they were tested on structurally novel combinations.Kim and Linzen [25] reported that models trained on COGS often failed to generalize across syntactic alternations, such as the dative shift.Furrer et al. [15] showed that even models such as BERT [10] underperform on compositional generalization in Freebase-style question answering, and Dessì and Baroni [9] demonstrated similar shortcomings in convolutional architectures on synthetic symbolic tasks.These results suggest that, across architectures, including large-scale Transformer models, there remains a consistent difficulty with systematic generalization to previously unseen combinations of known elements.Recent work has proposed taxonomies and metrics to distinguish compositional generalization from mere pattern matching [20], which emphasizes the need for benchmarks that isolate compositional competence from surface-level generalization.While this work focuses on a systematicity definition similar to ours, they use an artificial language for a translation task instead of QALD as a task and do not use pre-trained LLMs.</p>
<p>Large Language Models and Compositionality</p>
<p>LLMs have shown impressive zero-shot and few-shot performance across diverse NLP tasks, however their compositional capabilities do still remain contested.Although prompt engineering and in-context learning have extended the apparent flexibility of LLMs [6,48], there are still limitations in their ability to generalize from known linguistic or logical primitives to new combinations [18,36].For instance, Petty et al. [35] have found that increasing transformer depth yields diminishing returns for compositional generalization tasks.In a similar vein, Yang et al. [51] have observed that training LLMs on higher-order compositional instructions improves performance on simpler tasks, but not vice versa.Furthermore, Ismayilzada et al. [21] have reported that instruction-fine-tuned multilingual models struggle when it comes to morphological compositional generalization in agglutinative languages such as Turkish and Finnish, particularly when applied to novel word roots.These findings suggest a discrepancy between LLMs' surface-level fluency and their deeper semantic systematicity.Dziri et al. [12] investigate the limits of Transformer-based architectures on compositional reasoning tasks, e.g., multiplication and logic puzzles.They demonstrate that even advanced LLMs exhibit systematic failures when faced with reasoning chains that are beyond the complexity seen during training.Although Dziri et al. also investigate the compositional abilities of LLMs, they focus on different tasks, use a set of older and less diverse models as well as a more implicit compositionality definition.Press et al. [37] propose an empirical framework for quantifying what they call the "compositionality gap" in language models.By introducing diagnostic datasets and evaluation metrics, they showcase how current models often rely on statistical heuristics rather than exhibiting true compositional understanding.They also test question answering as a task, although not QALD.Moreover, the problems tested in [37] are limited in size and complexity by mostly focusing on 2-hop question answering, together with older and less diverse tested LLMs.In contrast, our approach generalizes from that 2-hop setting to a general sub-graph-based notion for determining the parts of a question that allows to easily scale the complexity of the benchmarks.</p>
<p>Compositionality in QALD</p>
<p>QALD represents a natural way of testing for compositional generalization, as building SPARQL queries involves, among other challenges, logical composition, nesting relations, and linking entities.Previous approaches in semantic parsing have addressed these challenges by explicitly modeling the structure of SPARQL queries derived from natural language [11,34,46,47,52].However, these systems typically require domain-specific grammars or supervision, which greatly limits their scalability.Recent investigations have focused on the capacity of LLMs to answer knowledge base questions without explicit parsing.In contrast, Schmidt et al. [41,42] aim to combine the strengths of both LLMs as well as symbolic approaches in a compositional QALD pipeline.Their system works by utilizing lexical entries and explicit meaning representations together with neural parts for, e.g., dependency parsing or selecting the most promising SPARQL query.</p>
<p>While LLMs perform reasonably on atomic questions (i.e., involving a single RDF triple), they do still struggle with multi-hop or nested constructions [7,17,33], which are prevalent in QALD.Recent studies suggest that these failures may stem from LLMs' limited ability to systematically map natural language inputs to the compositional structures that are required for accurate knowledge base reasoning.For instance, Chen et al. [8] demonstrate that integrating explicit knowledge graphs enhances LLM performance on multi-hop question answering tasks, at the same time indicating that LLMs alone struggle with such compositional reasoning.In a similar vein, Zhao et al. [54] find that LLMs possess the necessary knowledge components but fail to combine them effectively in novel contexts, pointing out a deficiency in systematic compositionality.As such, this paper positions itself at the intersection of these concerns: operationalizing compositionality through limited linguistic units, DBpedia-based property verbalizations, and testing whether LLMs exhibit systematic generalization w.r.t.QALD.By grounding the experimental design in a formal notion of compositionality and providing the CompoST dataset, our aim is to offer a controlled, yet scalable, approach to evaluating this dimension of model behavior.</p>
<p>Methods</p>
<p>Operationalization of Compositionality</p>
<p>For our definition of compositionality in a QALD context, we rely on work by Zoltán G. Szabó [45], where compositionality is described as consisting of two sub-properties, namely productivity and systematicity.We slightly generalize the definitions and arguments made in that work to also include generative LLMs.These definitions are also similar to the systematicity and productivity tests described in [20].The definitions used in this work are, therefore, as follows:</p>
<p>Definition 1 (Productivity [45]).The property of productivity is satisfied iff an agent can understand1 complex expressions it never encountered before.</p>
<p>Definition 2 (Systematicity [45]).The property of systematicity is satisfied iff an agent that understands a number of complex expressions e 1 , . . ., e n also understands all other complex expressions that can be built up from the constituents of e 1 , . . ., e n using syntactic rules employed in building up their structures.</p>
<p>As we want to operationalize compositionality and develop a benchmark dataset for LLMs, we need to think of ways to test these definitions.For Def. 1, an obvious challenge in applying it to LLMs is that i) for many models, the datasets used for pre-training are not publicly available, and ii) even if they are, e.g., for models like OLMo [31], they are huge and it is hardly verifiable whether an expression is or is not part of the training data beyond exact string matches.We therefore focus on the systematicity property as stated in Def. 2 only.</p>
<p>In order to operationalize Def. 2, there are multiple ways in which the term "constituents" can be interpreted in the QALD context.On the one hand, this could be interpreted as constituents w.r.t.natural language, i.e., words or sentence structures, and, on the other hand, w.r.t.SPARQL, syntax elements like triple patterns.In this paper, we focus on investigating the behavior of LLMs w.r.t.SPARQL constituents and verbalize the corresponding SPARQL queries with only slight linguistic variations.In this way, we want to test the basic abilities of LLMs to work in a compositional, i.e., systematic, manner, without having to generalize over many potential verbalizations.</p>
<p>CompoST Dataset Creation</p>
<p>For creating a dataset to test the systematicity property, we thus need pairs of questions and SPARQL queries (named "benchmark item" or just "item" in the following) such that we know that the sufficiency criterion is fulfilled, i.e., we know whether or not a set of benchmark items contains all necessary constituents to build another specific item.This requirement also prevents us from using existing QALD datasets.QALD benchmarks typically consist of a large number of mostly different questions, but lack two aspects that are necessary to test compositional systematicity.More precisely, they a) usually do not have large numbers of different combinations of the same "basic building blocks" and b) do not have known relations between two benchmark items that would allow us to determine whether the sufficiency criterion is satisfied or not.Thus, we create a new dataset that meets these requirements.For our dataset, this is achieved by first generating graph patterns of a certain size and structure, then fetching instances of that pattern from a DBpedia snapshot from December 2022, and finally verbalizing the full pattern as well as all connected sub-patterns in the same structured fashion.For each resulting set of benchmark items, we know whether the sufficiency criterion is satisfied if the graph patterns connected to a set of items are an (exact) edge set cover of the considered other item's graph pattern.More precisely, we use "pitchfork-like"/star patterns of variable depth and breadth.An instance for depth 3 and breadth 3 is given in Fig. 1.These pattern instances as well as all connected sub-graphs of those patterns form the basis of our benchmark dataset.However, the pattern instance of Fig. 1 still needs to be translated into SPARQL queries and corresponding verbalizations.For this, we rely on 1070 hand-crafted Lemon lexical entries [27] for 147 DBpedia properties, roughly corresponding to the properties covered in QALD-9 [46].Of these entries, 245 are NounPPFrame entries for verbalizing DBpedia properties and 825 are about verbalizing different rdf:type, dbo:country, dbo:nationality and dbo:industry patterns, e.g., "?v dbo:nationality dbr:Germany" as "is Ger-  man".For this, we reused and extended the lexicon of Schmidt et al. [41,42] as well as parts of the code for parsing and using the lexical entries.</p>
<p>A Lemon lexical entry consists of a canonical form of the respective word or phrase, the corresponding preposition (if applicable) as well as the associated DBpedia property, among other things.Based on this information, we can verbalize a given "pitchfork-like" graph pattern such that we ask for ?result.Examples are given in Fig. 2, illustrating how a set of items can contain all necessary information to compose another item.By using the information from 1 and 2, a compositional system should be able to generate the bottom query.</p>
<p>As the structure of these questions and queries is quite simple and monotonous, we additionally increased the diversity of our dataset in two ways: i) inverting the direction of some triples in the pattern and ii) adding verbalizations of rdf:type or, e.g., dbo:nationality values to the query.For further details about the dataset generation process, we published the corresponding generation code in our software artifact.All in all, the generated questions can be described by the grammar in Extended Backus-Naur Form (EBNF, [3]) presented in Fig. 3.However, that grammar generates a super-set of the sentences that can actually be generated by our approach.Therefore, some additional constraints enforced during the dataset generation are omitted.For example, the choice of "works in ...", "is a", "is an" or just "is" depends on the respective canonical form, the corresponding property, as well as the specific lexical entry type.</p>
<p>For the final dataset, patterns with the following depth and breadth combinations of the basic pattern were used: breadth 2 &amp; depth 2, breadth 2 &amp; depth 3, breadth 3 &amp; depth 2, and breadth 3 &amp; depth 3.For each, we searched for matching pattern instances in DBpedia and, based on the instances found, generated a verbalization of those base pattern instances as well as of all connected sub-patterns of the respective instances.In cases of multiple available lexical entries for a property, we chose the entries consistently between the base pattern and its sub-patterns by seeding the random choices accordingly.Similarly, we fetched all rdf:type, dbo:country, dbo:nationality and dbo:industry objects for all entities of the instance and selected a random sample of random size of the available objects for which a corresponding lexical entry was available.This sample was then fixed for that pattern instance and verbalized as a "that" clause for all (sub-)patterns that include the respective node at an appropriate position.All those choices are necessary in order to ensure that, as Def. 2 states, all necessary constituents are present in the sub-patterns such that one should be able to compose those parts to verbalizations of larger (sub-)patterns.</p>
<p>The generated instances, their verbalizations and the corresponding SPARQL queries were then split equally among three datasets of varying difficulty, with every dataset having in total 75 base pattern instances as well as the corresponding sub-graphs.In total, this makes 2803 question-query-pairs per dataset.The splitting was done based on the portion of the number of pattern edges (thus, "that" clause edges were excluded from that calculation) up to which sub-patterns were added to the training split.More precisely, they were split into easy (sub-patterns with up to 75% of the total base pattern edges were included in the train split, e.g., for breadth 3 &amp; depth 3 with a total of 9 edges, patterns with up to 6 edges were included in train), medium (50%) and hard (25%, with an absolute minimum of two edges to cover all phenomena, such as chaining triples or branching with "and").For the validation and test splits, we chose the lower and upper half of the remaining data sorted by the number of base pattern edges, respectively.</p>
<p>Additionally, based on these datasets, we created additional variants where an edge set cover of the target pattern, i.e., all information necessary to compose the correct answer, is given in the input.In the following, these variations will be called self-contained tasks, as no knowledge beyond the input is necessary to solve it.Analogously, the original tasks are called classic tasks.The selfcontained tasks were created by taking the validation items for the new train and the test items for the new test datasets together with an edge set cover of the target pattern taken from the train dataset, comprising up to 10 examples2 .</p>
<p>With these datasets, which together comprise CompoST, we investigate the ability of models to generalize beyond what they have seen either during prompt optimization, in the given examples or during fine-tuning.A system which is able to truly generalize in a compositional way would be expected to show almost no performance degradation, as all necessary information is already in sub-patterns with as few as one triple, or two triples to cover combination phenomena.</p>
<p>Experiments</p>
<p>In the following, we describe the experiments conducted with CompoST.</p>
<p>Zero-Shot Prompting: As a first baseline, we consider zero-shot prompting.In order to run the experiments in a structured manner together with, e.g., prompt optimization techniques, we use the DSPy framework [23,24] for our in-context learning experiments.We evaluate a broad set of models, namely Llama 3.3 (70B) [16], Phi-4 (14B) [1], Qwen2.5-Coder(7B) [19,50], OLMo 2 (7B) [31] and GPT-4o-mini [32].As optimization techniques, we used none, COPRO, MIPROv2 (light, medium and heavy) together with plain as well as chain of thought prompting.In each experiment, the model is given a question from the dataset and expected to generate a corresponding SPARQL query for DBpedia.Finally, the results of both queries are compared and the F 1 scores are calculated.The optimization strategies are given the training split and can optimize the prompt based on an evaluation method that calculates the F 1 score for a predicted query compared to the gold standard.</p>
<p>Few-Shot Prompting: As generating SPARQL queries for natural language questions is a hard task, one might argue that, besides prompt optimization, examples are necessary for the model to properly understand the task.Therefore, we use MIPROv2 (light, medium and heavy), LabeledFewShot, BootstrapFew-Shot and BootstrapFewShotWithRandomSearch choosing the shots given to the model, as well as (in case of MIPROv2) optimizing the prompt at the same time, again paired with plain or chain of thought prompting.In these experiments, the models are additionally given a number of examples, i.e., "shots", chosen by the respective optimization strategies.For choosing those shots, the strategies are also given the SPARQL queries for the train split and have the same information available as the zero-shot experiments.For the self-contained tasks, we evaluated MIPROv2 (light, medium and heavy) with and without chain of thought prompting, as the given shots are a fixed part of the input already.</p>
<p>Fine-Tuning: For particularly hard tasks, even with the most recent models, it might be necessary to apply fine-tuning when in-context learning fails.Therefore, we also conducted experiments with fine-tuned models.In particular, we fine-tuned GPT-4o-mini, OLMo 2 and Qwen2.5-Coder.Although hardware limitations were no issue for GPT-4o-mini fine-tuning because of the OpenAI API usage, we chose the two 7B parameter models OLMo 2 and Qwen2.5-Coderover Llama 3.3 and Phi-4 as this was the largest model size we could fine-tune in reasonable time with our available hardware.For the classic tasks, these models were fine-tuned on a zero-shot setting, i.e., given a basic task description 3 to-gether with the input question and fine-tuned to output just the SPARQL query.For self-contained tasks, we fine-tuned the best-performing model of the classic tasks, namely GPT-4o-mini.In this setting, the model was additionally given the relevant edge set cover of example items.Thus, the model is fine-tuned to compose the answer based on question-query-pairs that are given to the model.</p>
<p>Experimental Settings</p>
<p>For all in-context learning experiments, we used the OpenAI API for GPT-4omini, evaluating the gpt-4o-mini-2024-07-18 version, and Ollama4 to serve all other models.More precisely, we used the default Ollama models with Q4_K_M quantization, i.e., Llama 3.3 with 70B parameters, Phi-4 with 14B parameters, and Qwen2.5-Coder with 7B parameters.Additionally, we used the 7b-1124-instruct-q4_K_M version of OLMo 2. The models used for (classic) finetuning were GPT-4o-mini as well as Qwen2.5-Coder(Qwen/Qwen2.5-Coder-7B-Instruct)and OLMo 2 (allenai/OLMo-2-1124-7B-Instruct).For fine-tuning GPT-4o-mini, we used the OpenAI API, leaving all parameters to "auto".For the other models, we performed a hyperparameter search using Optuna [2] and fine-tuned the models using PyTorch Lightning [13] with DeepSpeed [38].For our experiments, we had up to 10 Nvidia A40 GPUs available, utilizing nodes with up to four A40 GPUs paired with 120 CPU cores and 443GB of RAM.With quantization and DeepSpeed [38], we were able to fine-tune the given 7B models on a single node.The prompting experiments not using the OpenAI API were conducted with Ollama instances running on a single A40 GPU each, grouping experiments by models accordingly to avoid reloading.</p>
<p>To measure the "raw" performance, we get the results for both the gold standard, as well as the generated SPARQL query of a DBpedia snapshot from December 2022 5 .We then compare the two result sets and calculate true positives, false positives, etc. in the standard way.Based on the confusion matrix, we can then calculate F 1 scores at a macro level (calculating F 1 score per question and using the mean of the respective scores as the final score).The F 1 scores for single questions are available to the DSPy [23,24] optimizers.</p>
<p>Additionally, we calculate a second set of scores focusing more on the compositionality aspect, evaluating whether or not the model behaves in a compositional way.For this, we define the confusion matrix values w.r.t.expectations of compositional behavior.More precisely, this means that true positives (TP) are those correctly-answered benchmark items for which all sub-problems, i.e., all sub-patterns of the respective target graph pattern, are correctly answered as well.False positives (FP) are all other correctly-answered tasks, i.e., where one or more sub-problems are wrong.Finally, false negatives (FN) are those incorrect answers for which an edge set cover of the target graph is correctly answered, but the actual question is still answered incorrectly.
I = C ∪ W TP = {i ∈ C | ∀i ′ ∈ I : i ′ ⊆ i ⇒ i ′ ∈ C. } FP = C \ TPFN = {i ∈ W | ∃ α ⊆ C : i ′ ∈α edges(i ′ ) = edges(i) ∧ ∀i ′ ∈ α : i ′ ⊂ i. . }
with I being the set of all benchmark items (represented by their graph pattern instance), C the set of correctly-answered items and W the set of items with a wrong answer.Furthermore, edges(i ∈ I) returns the set of edges of the graph pattern instance of a benchmark item.Based on this, the compositionalityadjusted F 1 score is then calculated per question and aggregated as a macro F 1 score for the resulting compositionality F 1 score displayed in Table 1.</p>
<p>Results</p>
<p>As we conducted over 400 experiments in total, this section will only discuss the results of the best-performing models (w.r.t.overall macro F 1 score) and optimization strategies per category.As shown in Table 1, the best-performing models (in terms of macro F 1 score) are either Llama 3.3 or GPT-4o-mini across all categories and datasets.Additionally, for in-context learning, MIPROv2 prompt optimization works best in most cases, with medium few-shot being the only exception, achieving the best results for BootstrapFewShotWithRandomSearch.However, fine-tuning outperforms in-context learning in all cases in terms of macro F 1 scores for the classic tasks.For the self-contained task variants, it is slightly different.Here, few-shot experiments outperform the fine-tuned approaches for the easy (0.57 vs. 0.48) and hard (0.51 vs. 0.30) datasets.Considering the different datasets with different portions of the total edge count being present in training data, it can generally be observed that the models perform worse the more the tasks differ from their training samples.This effect is especially strong for the fine-tuned models, with, e.g., fine-tuned test macro F 1 scores degrading from 0.45 for easy over 0.29 for medium to 0.09 for hard.Notably, in all datasets, all relevant information is part of the training data.Thus, only the number and complexity of components that need to be composed differs.</p>
<p>For the compositionality F 1 score, all classic configurations show poor performance.No approach achieves a higher test score than 0.31, which even degrades to 0.22 for medium and 0.10 for hard.But also for self-contained configurations, where the models are optimized for composing the pieces already present in the input, the compositionality F 1 score does not exceed 0.57 for the test split, with just 0.43 being the best result for the hard dataset.Comparing in-context learning and fine-tuning, no clear trend is observable.For the easy dataset, the fine-tuned approach's compositionality F 1 score is slightly higher than for the best few-shot approach (0.53 vs. 0.52), although the macro F 1 score is lower (0.48 vs. 0.57).In contrast, the opposite is true for the medium and hard datasets.</p>
<p>The performance of the models depending on the depth and breadth of the given question and respective SPARQL query is further examined in Fig. 4 by the example of the hard dataset.As we observed in Fig. 4a, the single-triple queries work comparably well, whereas increasing depth and breadth substantially hurts performance.The fine-tuned approach, see Fig. 4b, works almost perfectly for the training data and successfully generalizes to breadth 3, however, increasing both depth and breadth degrades performance down to 0.01 for depth and breadth 3. Fig. 5: Results for a depth and breadth 3 graph pattern instance and connected sub-patterns taken from the best fine-tuned medium approach.Arrows indicate source contains target node pattern, omitting nodes with &gt; 1 edge difference.Fig. 5 shows all results for a single graph pattern instance of depth and breadth 3 as well as the corresponding sub-graphs from the medium dataset for the best fine-tuned approach.The illustration can be understood similarly as a Hasse diagram [5,39] for the "subset of" relation between two graphs.The nodes in the bottom line correspond to single-triple questions, e.g., "Who is the president of X?", while the second-bottom line is one triple richer, e.g., "Who is the father of the president of X?".Thus, the figure illustrates the questions that can be built from the "atomic" building blocks up to the whole pattern instance.We can observe that data that were available during training plus up to two edges work relatively well, but almost all predictions beyond that are incorrect.Similarly, most non-training results violate the systematicity property.</p>
<p>Discussion</p>
<p>Before we conclude to answer our research questions, we will first go through the results with the hypothesis that LLMs do work in a systematic (Def.2) manner and see whether this view is compatible with our empirical results.</p>
<p>Zero-Shot Experiments: In an ideal scenario, the models have already seen sufficient amounts of data for DBpedia during pre-training, such that it only needs a suitable explanation of the task to perform well.To ensure the explanation is not the problem here, we evaluated multiple prompt optimization techniques to find the best prompt for each tested model.However, the poor results (macro F 1 between 0.01 and 0.04) suggest that our assumptions are not true and the models need more information to successfully deal with QALD.</p>
<p>Few-Shot Experiments: Thus, the natural next step when zero-shot prompting fails is to additionally provide examples/"shots" to the model.However, we can observe in Table 1 as well as Fig. 4 that, while it improves results, the absolute scores are still poor.Even for single-triple questions, the macro F 1 score is only 0.51 in Fig. 4a.The test compositionality scores are not much better either, with scores between 0.00 and 0.07.Similarly to the zero-shot experiments, the weak performance as well as the low compositionality scores question the hypothesis that LLMs work in a compositionally systematic manner.</p>
<p>Fine-Tuning Experiments: One reason that makes models struggle with the QALD task could be that, in order to construct a correct DBpedia SPARQL query for a specific question, one also needs to know how to deal with peculiarities in the mapping from natural language to DBpedia vocabulary.The peculiarities could be learned by a model through fine-tuning, as the model then has the chance to learn to connect certain entities and properties to certain natural language expressions.As all entities, properties and "atomic" natural language expressions are by design part of the training data, ambiguity problems should be minimal after fine-tuning.Although the results of (classic) fine-tuning presented in Table 1 are much better overall, there are a few trends suggesting that the model has substantial problems to generalize beyond training data.First, the macro F 1 performance degrades to almost few-shot level as the deviation of samples from the training examples grows (i.e., for easy, test performance jumps from 0.09 to 0.45, but for hard only from 0.08 to 0.09).This impression is corroborated by the results presented in Fig. 4b, where the top left three rectangles, which represent the training data, achieve almost perfect results, whereas almost all other examples slightly outside the training data already display substantial performance drops.This impression that the models lack a proper generalization is further corroborated by Fig. 5.As we can see from the green nodes in the graph, most of the training split results are correct.However, this performance severely degrades when adding edges beyond the training data size.Thus, even when adding one additional edge, the number of respective incorrect results more than doubles and continues to grow.This indicates that the models struggle to generalize on a compositional level, contradicting our hypothesis.</p>
<p>Self-Contained Few-Shot and Fine-Tuning Experiments: Especially for the incontext learning experiments, we might discard the assumption that the models have already seen the relevant data from DBpedia such that we need to provide all necessary information in the input.This is what we did with the self-contained experiments, which contain an edge set cover of the target graph pattern, i.e., the respective questions and queries, in the input.Although this eases the task a lot and comes at the risk of overfitting during prompt optimization or fine-tuning, it also focuses the task on the core ability we want to test, namely systematicity.Thus, self-contained tasks provide almost optimal conditions for a model to show its abilities to work in a compositional way.</p>
<p>However, the performance improvements in Table 1 are not as large as one would expect assuming our initial hypothesis that the models indeed work in a compositional, systematic manner.The best-performing self-contained experiments in all cases outperform the classic experiments in terms of test macro F 1 scores and are less affected by the portion of edges given as training samples, i.e., the easy, medium and hard datasets.This might be due to the fact that all relevant information is given in the input and always with (up to) 10 shots, only with varying closeness of the given examples to the target question due to the edge portion limit.Notably, fine-tuning achieves worse results for the easy and hard datasets than the few-shot approach (0.48 vs. 0.57 and 0.30 vs. 0.51).Only for medium the fine-tuning approach works better (0.55 vs. 0.48).It is not clear what this deviation is caused by.One possibility would be that the fine-tuning process overfits stronger to the training dataset and thus has more problems generalizing to larger target questions, whereas for in-context learning, only the prompt is optimized, with less potential to overfit.Regarding compositionality scores, we see better overall results as well, but with slight differences between approaches.For fine-tuning, the test compositionality F 1 score is in all cases slightly higher than the "raw" macro F 1 score, whereas it is lower for few-shot prompting.This might suggest that fine-tuning leads to better compositional behavior than few-shot prompting.Nevertheless, given the discussed simplifications and all scores being no greater than 0.57 for both macro and compositionality F 1 score, this raises questions how compositionally LLMs are able to work and may indicate a fundamental weakness, especially considering the observations for higher depths, breadths, or edge counts.Overall, our results are in line with those in Dziri et al. [12], indicating fundamental weaknesses in LLMs w.r.t.compositional reasoning.</p>
<p>Research Questions: Considering RQ1, we presented an operationalization of compositionality in terms of systematicity in Def. 2 and Section 3.1.When it comes to whether or not LLMs satisfy the property defined in Def. 2 w.r.t.RQ2, our results indicate that the LLMs tested do not satisfy the property in a strict sense.Considering the merely mediocre scores even under optimal conditions, this at least heavily questions the general abilities to work in a compositional way w.r.t. the QALD domain.For RQ3, we have seen promising results that finetuning slightly improves the compositional behavior of LLMs, however, without achieving scores that would justify the assumption that LLMs actually satisfy systematicity in terms of Def. 2 when fine-tuned accordingly.</p>
<p>Limitations</p>
<p>Our study has multiple limitations that frame the scope and interpretation of our results.First, the dataset used is synthetic and constructed with a controlled set of lexical entries and structural templates.This therefore limits the diversity of surface verbalizations.However, we see this primarily as an advantage in the sense that by minimizing linguistic variation, we can reduce confounding factors such as paraphrasing and noise from formulation differences, thereby isolating the core ability of models to generalize compositionally.We aimed to address shortcut learning and overfitting by running diverse experiments with and without optimization/fine-tuning.If the limited linguistic diversity had a substantial impact on the (optimization) performance, this would have been visible in the results in one way or another.If the optimizations would hurt performance, this would usually be visible by experiments with non-optimized models performing better.If the optimization found an easy shortcut for the datasets, the scores would have been very high.All in all, both is not the case, such that we assume the simplicity to have a positive if any impact on the results.Secondly, it has to be highlighted that our dataset is built on a curated but ultimately limited subset of DBpedia.Although the coverage is relatively large, it does not represent the full heterogeneity of the DBpedia knowledge graph.Furthermore, DBpedia exhibits certain idiosyncrasies, particularly in its type definitions and predicate choices for particular information, that may result in generated benchmark items that appear to be "unnatural".Generalization to other knowledge bases, such as Wikidata, remains to be tested in future work.Additionally, we conducted only a limited set of fine-tuning experiments due to resource constraints.More experiments or even specialized training techniques for compositionality may yield further novel insights.Finally, we emphasize that our evaluation focuses on necessary conditions for compositionality.Success on our task does not definitively demonstrate that a model reasons compositionally in a systematic fashion.However, failure to generalize in our reduced and highly controlled setting strongly suggests a lack of compositional generalization capabilities.</p>
<p>Conclusion</p>
<p>In this paper, we presented the CompoST dataset for testing systematic compositionality for the QALD task, together with benchmark results for a diverse set of current LLMs and optimization strategies.Our results suggest that LLMs face limitations when it comes to systematic compositional behavior, with increasing performance degradations the larger the deviation from the training data is.For example, the best-performing classic experiment achieves a test macro F 1 score of 0.45 for the easy dataset, but only 0.09 for hard.Even for self-contained experiments, with all necessary information in the input, only test macro F 1 scores between 0.30 and 0.57 are achieved.All in all, our results do not justify the assumption that LLMs work in a compositional manner.Future work could further investigate whether there are more specialized training techniques not covered by this paper which might improve their ability to work in a compositional manner.Supplemental Material Statement: Source code and dataset are available at https://doi.org/10.5281/zenodo.16312287.</p>
<p>Fig. 1 :
1
Fig. 1: RDF graph of SPARQL graph pattern instance with depth of 3 and breadth of 3 with concrete entities and properties from DBpedia.</p>
<p>Fig. 2 :
2
Fig. 2: Example question-query-pairs. 1 and 2 together contain all relevant information to compose the bottom query.Only triple patterns of queries shown.</p>
<p>Fig. 4 :
4
Fig. 4: Macro F 1 scores of best approaches of the respective category for the hard dataset, grouped by graph pattern depth and breadth.Training data comprised samples up to two edges, i.e., up to breadth 2, depth 1 and depth 2, breadth 1.</p>
<ol>
<li>Question: Who is the spouse of Michelle Obama and parent of Malia Obama?SPARQL: dbr:Michelle_Obama dbo:spouse ?result.dbr:Malia_Obama dbo:parent ?result. 2. Question: Who is the spouse of the child of Marian Shields Robinson?SPARQL: dbr:Marian_Shields_Robinson dbo:child ?v1. ?v1 dbo:spouse ?result.
Question: Who is the spouse of the child of Marian Shields Robinson and parent of Malia Obama?SPARQL: dbr:Marian_Shields_Robinson dbo:child ?v1. ?v1 dbo:spouse ?result. dbr:Malia_Obamadbo:parent ?result.</li>
</ol>
<p>Table 1 :
1
Performance of the best-performing model and optimization strategy for each category.<em> = self-contained tasks, BFRS = BootstrapFewShotWith-RandomSearch, CoT = Chain of Thought, MIPRO: H = Heavy, M = Medium
ExperimentBest ConfigurationMacro F1Compositionality F1ModelOptimization Train Validation Test Train Validation TestEasy (75%)zero-shot Llama 3.3 MIPROH+CoT 0.170.040.01 0.100.000.00few-shot Llama 3.3 MIPROM+CoT 0.230.120.09 0.150.000.00fine-tuned GPT-4o-mini fine-tuning0.920.580.45 0.790.490.31few-shot</em> Llama 3.3MIPROM0.560.57 0.720.52fine-tuned<em> GPT-4o-mini fine-tuning0.720.48 0.840.53Medium (50%)zero-shot Llama 3.3MIPROH0.190.030.01 0.140.000.16few-shot Llama 3.3BFRS CoT0.290.150.09 0.180.020.03fine-tuned GPT-4o-mini fine-tuning0.940.570.26 0.780.600.22few-shot</em> Llama 3.3MIPROH0.680.48 0.760.33fine-tuned<em> GPT-4o-mini fine-tuning0.920.55 0.950.57Hard (25%)zero-shot Llama 3.3 MIPROH+CoT 0.250.070.04 0.080.180.06few-shot GPT-4o-mini MIPROM+CoT 0.360.130.08 0.110.090.07fine-tuned GPT-4o-mini fine-tuning0.950.340.09 0.610.450.10few-shot</em> Llama 3.3MIPROM0.890.51 0.920.43fine-tuned* GPT-4o-mini fine-tuning0.990.30 0.990.41
As the original definition refers to "understand" in a human context, we use the following working definition of understanding for generative LLMs in this paper: An LLM understands a sentence w.r.t. some knowledge graph iff it can generate a correct SPARQL query for that respective sentence and knowledge graph.
In some cases, there were less than 10 smaller samples related to the target pattern and thus given in the input. For further details, please refer to our software artifact.
Given a natural language question about a specific entity, generate a SPARQL query that retrieves relevant information from DBpedia.
https://ollama.com/
https://databus.dbpedia.org/dbpedia/collections/dbpedia-snapshot-2022-12
Acknowledgments.This work is partially funded by the Ministry of Culture and Science of the State of North Rhine-Westphalia under grant no NW21-059A (SAIL).Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article.
. M Abdin, J Aneja, H Behl, S Bubeck, R Eldan, S Gunasekar, M Harrison, R J Hewett, M Javaheripi, P Kauffmann, J R Lee, Y T Lee, Y Li, W Liu, C C T Mendes, A Nguyen, E Price, G D Rosa, O Saarikivi, A Salim, S Shah, X Wang, R Ward, Y Wu, D Yu, C Zhang, Y Zhang, 10.48550/arXiv.2412.08905arXiv:2412.08905Dec 2024Phi-4 Technical Report</p>
<p>Optuna: a nextgeneration hyperparameter optimization framework. T Akiba, S Sano, T Yanase, T Ohta, M Koyama, Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery and data mining. the 25th ACM SIGKDD international conference on knowledge discovery and data mining2019</p>
<p>The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference. J W Backus, 25:09 +0200Information processing, proceedings of the 1st international conference on information processing. Parisparis 15-20 june 1959. 1959. 26 Jul 2019 12tex.bibsource: dblp computer science bibliography</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, S Wachter, 10.1038/s42254-023-00581-4Nature Reviews Physics. 55May 2023Nature Publishing Group</p>
<p>Lattice Theory, revised ed. G Birkhoff, American mathematical society colloquium publications. 194825</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. 332020</p>
<p>Multi-hop Question Answering over Knowledge Graphs using Large Language Models. A Chakraborty, 10.48550/arXiv.2404.19234arXiv:2404.19234Apr 2024</p>
<p>Theoretical limitations of multi-layer Transformer. L Chen, B Peng, H Wu, arXiv:2412.02975[cs.LG]2024</p>
<p>CNNs found to jump around more skillfully than RNNs: Compositional Generalization in Seq2seq Convolutional Networks. R Dessì, M Baroni, 10.18653/v1/P19-1381Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M W Chang, K Lee, K Toutanova, 10.48550/arXiv.1810.04805arXiv:1810.04805May 2019</p>
<p>LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and DBpedia. M Dubey, D Banerjee, A Abdelkawi, J Lehmann, C Ghidini, O Hartig, M Maleshkova, V Svátek, I Cruz, A Hogan, J Song, M Lefrançois, 10.1007/978-3-030-30796-7_5The Semantic Web -ISWC 2019. Lecture Notes in Computer Science. F Gandon, ChamSpringer International Publishing201911779</p>
<p>Faith and fate: limits of transformers on compositionality. N Dziri, X Lu, M Sclar, X L Li, L Jiang, B Y Lin, P West, C Bhagavatula, R Le Bras, J D Hwang, S Sanyal, S Welleck, X Ren, A Ettinger, Z Harchaoui, Y Choi, Proceedings of the 37th international conference on neural information processing systems. Nips '23. the 37th international conference on neural information processing systems. Nips '23Red Hook, NY, USACurran Associates Inc20233081number of pages: 40 Place: New Orleans, LA, USA tex</p>
<p>The PyTorch Lightning team: PyTorch lightning. W Falcon, 10.5281/zenodo.3828935Mar 2019</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. G Feng, B Zhang, Y Gu, H Ye, D He, L Wang, Advances in neural information processing systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures. D Furrer, M V Zee, N Scales, N Schärli, 10.48550/arXiv.2007.08970arXiv:2007.08970Sep 2021</p>
<p>A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, A Rodriguez, A Gregerson, A Spataru, B Roziere, B Biron, B Tang, B Chern, C Caucheteux, C Nayak, C Bi, C Marra, C Mcconnell, C Keller, C Touret, C Wu, C Wong, C C Ferrer, C Nikolaidis, D Allonsius, D Song, D Pintz, D Livshits, D Wyatt, D Esiobu, D Choudhary, D Mahajan, D Garcia-Olano, D Perino, D Hupkes, E Lakomkin, E Albadawy, E Lobanova, E Dinan, E M Smith, F Radenovic, F Guzmán, F Zhang, G Synnaeve, G Lee, G L Anderson, G Thattai, G Nail, G Mialon, G Pang, G Cucurell, H Nguyen, H Korevaar, H Xu, H Touvron, I Zarov, I A Ibarra, I Kloumann, I Misra, I Evtimov, J Zhang, J Copet, J Lee, J Geffert, J Vranes, J Park, J Mahadeokar, J Shah, J V D Linde, J Billock, J Hong, J Lee, J Fu, J Chi, J Huang, J Liu, J Wang, J Yu, J Bitton, J Spisak, J Park, J Rocca, J Johnstun, J Saxe, J Jia, K V Alwala, K Prasad, K Upasani, K Plawiak, K Li, K Heafield, K Stone, K El-Arini, K Iyer, K Malik, K Chiu, K Bhalla, K Lakhotia, L Rantala-Yeary, L V D Maaten, L Chen, L Tan, L Jenkins, L Martin, L Madaan, L Malo, L Blecher, L Landzaat, L D Oliveira, M Muzzi, M Pasupuleti, M Singh, M Paluri, M Kardas, M Tsimpoukelli, M Oldham, M Rita, M Pavlova, M Kambadur, M Lewis, M Si, M K Singh, M Hassan, N Goyal, N Torabi, N Bashlykov, N Bogoychev, N Chatterji, N Zhang, O Duchenne, O Çelebi, P Alrassy, P Zhang, P Li, P Vasic, P Weng, P Bhargava, P Dubal, P Krishnan, P S Koura, P Xu, Q He, Q Dong, R Srinivasan, R Ganapathy, R Calderer, R S Cabral, R Stojnic, R Raileanu, R Maheswari, R Girdhar, R Patel, R Sauvestre, R Polidoro, R Sumbaly, R Taylor, R Silva, R Hou, R Wang, S Hosseini, S Chennabasappa, S Singh, S Bell, S S Kim, S Edunov, S Nie, S Narang, S Raparthy, S Shen, S Wan, S Bhosale, S Zhang, S Vandenhende, S Batra, S Whitman, S Sootla, S Collot, S Gururangan, S Borodinsky, T Herman, T Fowler, T Sheasha, T Georgiou, T Scialom, T Speckbacher, T Mihaylov, T Xiao, U Karn, V Goswami, V Gupta, V Ramanathan, V Kerkez, V Gonguet, V Do, V Vogeti, V Albiero, V Petrovic, W Chu, W Xiong, W Fu, W Meers, X Martinet, X Wang, X Wang, X E Tan, X Xia, X Xie, X Jia, X Wang, Y Goldschlag, Y Gaur, Y Babaei, Y Wen, Y Song, Y Zhang, Y Li, Y Mao, Z D Coudert, Z Yan, Z Chen, Z Papakipos, A Singh, A Srivastava, A Jain, A Kelsey, A Shajnfeld, A Gangidi, A Victoria, A Goldstand, A Menon, A Sharma, A Boesenberg, A Baevski, A Feinstein, A Kallet, A Sangani, A Teo, A Yunus, A Lupu, A Alvarado, A Caples, A Gu, A Ho, A Poulton, A Ryan, A Ramchandani, A Dong, A Franco, A Goyal, A Saraf, A Chowdhury, A Gabriel, A Bharambe, A Eisenman, A Yazdan, B James, B Maurer, B Leonhardi, B Huang, B Loyd, B D Paola, B Paranjape, B Liu, B Wu, B Ni, B Hancock, B Wasti, B Spence, B Stojkovic, B Gamido, B Montalvo, C Parker, C Burton, C Mejia, C Liu, C Wang, C Kim, C Zhou, C Hu, C H Chu, C Cai, C Tindal, C Feichtenhofer, C Gao, D Civin, D Beaty, D Kreymer, D Li, D Adkins, D Xu, D Testuggine, D David, D Parikh, D Liskovich, D Foss, D Wang, D Le, D Holland, E Dowling, E Jamil, E Montgomery, E Presani, E Hahn, E Wood, E T Le, E Brinkman, E Arcaute, E Dunbar, E Smothers, F Sun, F Kreuk, F Tian, F Kokkinos, F Ozgenel, F Caggioni, F Kanayet, F Seide, G M Florez, G Schwarz, G Badeer, G Swee, G Halpern, G Herman, G Sizov, Guangyi, Zhang, G Lakshminarayanan, H Inan, H Shojanazeri, H Zou, H Wang, H Zha, H Habeeb, H Rudolph, H Suk, H Aspegren, H Goldman, H Zhan, I Damlaj, I Molybog, I Tufanov, I Leontiadis, I E Veliche, I Gat, J Weissman, J Geboski, J Kohli, J Lam, J Asher, J B Gaya, J Marcus, J Tang, J Chan, J Zhen, J Reizenstein, J Teboul, J Zhong, J Jin, J Yang, J Cummings, J Carvill, J Shepard, J Mcphie, J Torres, J Ginsburg, J Wang, K Wu, U , K H Saxena, K Khandelwal, K Zand, K Matosich, K Veeraraghavan, K Michelena, K Li, K Jagadeesh, K Huang, K Chawla, K Huang, K Chen, L Garg, L , A , L Silva, L Bell, L Zhang, L Guo, L Yu, L Moshkovich, L Wehrstedt, L Khabsa, M Avalani, M Bhatt, M Mankus, M Hasson, M Lennie, M Reso, M Groshev, M Naumov, M Lathi, M Keneally, M Liu, M Seltzer, M L Valko, M Restrepo, M Patel, M Vyatskov, M Samvelyan, M Clark, M Macey, M Wang, M Hermoso, M J Metanat, M Rastegari, M Bansal, M Mehta, N Laptev, N P Dong, N Cheng, N Chernoguz, O Hart, O Salpekar, O Kalinli, O Kent, P Parekh, P Saab, P Balaji, P Rittner, P Bontrager, P Roux, P Dollar, P Zvyagina, P Ratanchandani, P Yuvraj, P Liang, Q Alao, R Rodriguez, R Ayub, R Murthy, R Nayani, R Mitra, R Parthasarathy, R Li, R Hogan, R Battey, R Wang, R Howes, R Rinott, R Mehta, S Siby, S Bondu, S J Datta, S Chugh, S Hunt, S Dhillon, S Sidorov, S Pan, S Mahajan, S Verma, S Yamamoto, S Ramaswamy, S Lindsay, S Lindsay, S Feng, S Lin, S Zha, S C Patil, S Shankar, S Zhang, S Zhang, S Wang, S Agarwal, S Sajuyigbe, S Chintala, S Max, S Chen, S Kehoe, S Satterfield, S Govindaprasad, S Gupta, S Deng, S Cho, S Virk, S Subramanian, S Choudhury, S Goldman, S Remez, T Glaser, T Best, T Koehler, T Robinson, T Li, T Zhang, T Matthews, T Chou, T Shaked, T Vontimitta, V Ajayi, V Montanez, V Mohan, V Kumar, V S Mangla, V Ionescu, V Poenaru, V Mihailescu, V T Ivanov, V Li, W Wang, W Jiang, W Bouaziz, W Constable, W Tang, X Wu, X Wang, X Wu, X Gao, X Kleinman, Y Chen, Y Hu, Y Jia, Y Qi, Y Li, Y Zhang, Y Zhang, Y Adi, Y Nam, Y Yu, Wang, Y Zhao, Y Hao, Y Qian, Y Li, Y He, Z Rait, Z Devito, Z Rosnbrick, Z Wen, Z Yang, Z Zhao, Z Ma, 10.48550/arXiv.2407.21783arXiv:2407.21783The Llama 3 Herd of Models. Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N.,Nov 2024</p>
<p>J He, N Hu, W Long, J Chen, J Z Pan, 10.48550/arXiv.2412.17032arXiv:2412.17032MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New and Tail Knowledge. Jan 2025</p>
<p>Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations. J Herzig, P Shaw, M W Chang, K Guu, P Pasupat, Y Zhang, 10.48550/arXiv.2104.07478arXiv:2104.07478Apr 2021</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Dang, arXiv:2409.12186others: Qwen2. 5-coder technical report. 2024arXiv preprint</p>
<p>Compositionality Decomposed: How do Neural Networks Generalise. D Hupkes, V Dankers, M Mul, E Bruni, 10.1613/jair.1.11674Journal of Artificial Intelligence Research. 67Apr 2020</p>
<p>Evaluating Morphological Compositional Generalization in Large Language Models. M Ismayilzada, D Circi, J Sälevä, H Sirin, A Köksal, B Dhingra, A Bosselut, D Ataman, L V Plas, 10.48550/arXiv.2410.12656arXiv:2410.12656Feb 2025</p>
<p>Measuring Compositional Generalization: A Comprehensive Method on Realistic Data. D Keysers, N Schärli, N Scales, H Buisman, D Furrer, S Kashubin, N Momchev, D Sinopalnikov, L Stafiniak, T Tihon, D Tsarkov, X Wang, M V Zee, O Bousquet, 10.48550/arXiv.1912.09713arXiv:1912.09713Jun 2020</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. O Khattab, K Santhanam, X L Li, D Hall, P Liang, C Potts, M Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>DSPy: Compiling declarative language model calls into selfimproving pipelines. O Khattab, A Singhvi, P Maheshwari, Z Zhang, K Santhanam, S Vardhamanan, S Haq, A Sharma, T T Joshi, H Moazam, H Miller, M Zaharia, C Potts, The Twelfth International Conference on Learning Representations. 2024</p>
<p>COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. N Kim, T Linzen, 10.48550/arXiv.2010.05465arXiv:2010.05465Oct 2020</p>
<p>B Lake, M Baroni, Generalization without Systematicity:On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. 2018</p>
<p>Linking lexical resources and ontologies on the semantic web with lemon. J P Mccrae, D Spohr, P Cimiano, Proceedings of the 8th extended semantic web conference on The semantic web: research and applications (ESWC). the 8th extended semantic web conference on The semantic web: research and applications (ESWC)20116643</p>
<p>Transformers can do arithmetic with the right embeddings. S Mcleish, A Bansal, A Stein, N Jain, J Kirchenbauer, B R Bartoldson, B Kailkhura, A Bhatele, J Geiping, A Schwarzschild, T Goldstein, A Globerson, L Mackey, D Belgrave, A Fan, U Paquet, J Tomczak, Advances in neural information processing systems. C Zhang, Curran Associates, Inc202437</p>
<p>The parallelism tradeoff: Limitations of log-precision transformers. W Merrill, A Sabharwal, 10.1162/tacl_a_00562Transactions of the Association for Computational Linguistics. 11Jun 2023</p>
<p>Show Your Work: Scratchpads for Intermediate Computation with Language Models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, 10.48550/arXiv.2112.00114arXiv:2112.00114Nov 2021</p>
<p>T Olmo, P Walsh, L Soldaini, D Groeneveld, K Lo, S Arora, A Bhagia, Y Gu, S Huang, M Jordan, N Lambert, D Schwenk, O Tafjord, T Anderson, D Atkinson, F Brahman, C Clark, P Dasigi, N Dziri, M Guerquin, H Ivison, P W Koh, J Liu, S Malik, W Merrill, L J V Miranda, J Morrison, T Murray, C Nam, V Pyatkin, A Rangapur, M Schmitz, S Skjonsberg, D Wadden, C Wilhelm, M Wilson, L Zettlemoyer, A Farhadi, N A Smith, H Hajishirzi, 10.48550/arXiv.2501.00656arXiv:2501.00656OLMo 2 Furious. Jan 2025</p>
<p>. Achiam Openai, J Adler, S Agarwal, S Ahmad, L Akkaya, I Aleman, F L Almeida, D Altenschmidt, J Altman, S Anadkat, S Avila, R Babuschkin, I Balaji, S Balcom, V Baltescu, P Bao, H Bavarian, M Belgum, J Bello, I Berdine, J Bernadett-Shapiro, G Berner, C Bogdonoff, L Boiko, O Boyd, M Brakman, A L Brockman, G Brooks, T Brundage, M Button, K Cai, T Campbell, R Cann, A Carey, B Carlson, C Carmichael, R Chan, B Chang, C Chantzis, F Chen, D Chen, S Chen, R Chen, J Chen, M Chess, B Cho, C Chu, C Chung, H W Cummings, D Currier, J Dai, Y Decareaux, C Degry, T Deutsch, N Deville, D Dhar, A Dohan, D Dowling, S Dunning, S Ecoffet, A Eleti, A Eloundou, T Farhi, D Fedus, L Felix, N Fishman, S P Forte, J Fulford, I Gao, L Georges, E Gibson, C Goel, V Gogineni, T Goh, G Gontijo-Lopes, R Gordon, J Grafstein, M Gray, S Greene, R Gross, J Gu, S S Guo, Y Hallacy, C Han, J Harris, J He, Y Heaton, M Heidecke, J Hesse, C Hickey, A Hickey, W Hoeschele, P Houghton, B Hsu, K Hu, S Hu, X Huizinga, J Jain, S Jain, S Jang, J Jiang, A Jiang, R Jin, H Jin, D Jomoto, S Jonn, B Jun, H Kaftan, T Kaiser, L Kamali, A Kanitscheider, I Keskar, N S Khan, T Kilpatrick, L Kim, J W Kim, C Kim, Y Kirchner, J H Kiros, J Knight, M Kokotajlo, D Kondraciuk, L Kondrich, A Konstantinidis, A Kosic, K Krueger, G Kuo, V Lampe, M Lan, I Lee, T Leike, J Leung, J Levy, D Li, C M Lim, R Lin, M Lin, S Litwin, M Lopez, T Lowe, R Lue, P Makanju, A Malfacini, K Manning, S Markov, T Markovski, Y Martin, B Mayer, K Mayne, A Mcgrew, B Mckinney, S M Mcleavey, C Mcmillan, P Mcneil, J Medina, D Mehta, A Menick, J Metz, L Mishchenko, A Mishkin, P Monaco, V Morikawa, E Mossing, D Mu, T Murati, M Murk, O Mély, D Nair, A Nakano, R Nayak, R Neelakantan, A Ngo, R Noh, H Ouyang, L O'keefe, C Pachocki, J Paino, A Palermo, J Pantuliano, A Parascandolo, G Parish, J Parparita, E Passos, A Pavlov, M Peng, A Perelman, A Peres, F D A B Petrov, M Pinto, H P D O Michael, Pokorny, M Pokrass, V H Pong, T Powell, A Power, B Power, E Proehl, R Puri, A Radford, J Rae, A Ramesh, C Raymond, F Real, K Rimbach, C Ross, B Rotsted, H Roussez, N Ryder, M Saltarelli, T Sanders, S Santurkar, G Sastry, H Schmidt, D Schnurr, J Schulman, D Selsam, K Sheppard, T Sherbakov, J Shieh, S Shoker, P Shyam, S Sidor, E Sigler, M Simens, J Sitkin, K Slama, I Sohl, B Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N Tezak, M B Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C J Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, R Zellers, C Zhang, M Zhang, S Zhao, T Zheng, J Zhuang, W Zhuk, 10.48550/arXiv.2303.08774arXiv:2303.08774Mar 2024Zoph, B.: GPT-4 Technical Report</p>
<p>HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs. P Panda, A Agarwal, C Devaguptapu, M Kaul, P Ap, 10.18653/v1/2024.acl-long.717Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers. A Perevalov, D Diefenbach, R Usbeck, A Both, 10.1109/ICSC52841.2022.00045Jan 2022IEEE Computer Society</p>
<p>The Impact of Depth on Compositional Generalization in Transformer Language Models. J Petty, S Steenkiste, I Dasgupta, F Sha, D Garrette, T Linzen, 10.18653/v1/2024.naacl-long.402Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJun 20241</p>
<p>Meaning without reference in large language models. S T Piantadosi, F Hill, 10.48550/arXiv.2208.02957arXiv:2208.02957Aug 2022</p>
<p>Measuring and Narrowing the Compositionality Gap in Language Models. O Press, M Zhang, S Min, L Schmidt, N Smith, M Lewis, 10.18653/v1/2023.findings-emnlp.378Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, Association for Computational LinguisticsDec 2023Singapore</p>
<p>DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. J Rasley, S Rajbhandari, O Ruwase, Y He, 10.1145/3394486.3406703Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningNew York, NY, USAAssociation for Computing MachineryAug 202020</p>
<p>Graphs and Order: The Role of Graphs in the Theory of Ordered Sets and Its Applications. I Rival, 10.1007/978-94-009-5315-4_3https://doi.org/10.1007/978-94-009-5315-4_3Rival, I.1985SpringerNetherlands; DordrechtThe Diagram</p>
<p>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. A Saparov, H He, 10.48550/arXiv.2210.01240arXiv:2210.01240Mar 2023</p>
<p>Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System. D M Schmidt, M F Elahi, P Cimiano, C Badenes-Olmedo, I Novalija, E Daga, L Stork, R G Pillai, L Dierickx, B Kruit, V Degeler, J Moreira, B Zhang, R Alharbi, Y He, A Graciotti, A M Tirado, V Presutti, Joint Proceedings of Posters, Demos, Workshops, and Tutorials of the 24th International Conference on Knowledge Engineering and Knowledge Management. E Motta, Amsterdam, NetherlandsCEUR2024. Nov 20243967CEUR Workshop Proceedings</p>
<p>Lexicalization is all you need: Examining the impact of lexical knowledge in a compositional QALD system. D M Schmidt, M F Elahi, P Cimiano, M Alam, M Rospocher, M Van Erp, L Hollink, Knowledge engineering and knowledge management. G A Gesese, ChamSpringer Nature Switzerland2025</p>
<p>Question answering on linked data: Challenges and future directions. S Shekarpour, K M Endris, A Jaya Kumar, D Lukovnikov, K Singh, H Thakkar, C Lange, Proceedings of the 25th international conference companion on world wide web (WWW). the 25th international conference companion on world wide web (WWW)2016</p>
<p>What formal languages can transformers express? A survey. L Strobl, W Merrill, G Weiss, D Chiang, D Angluin, 10.1162/tacl_a_00663Transactions of the Association for Computational Linguistics. 122024MIT Press</p>
<p>The case for compositionality. Z G Szabó, The oxford handbook of compositionality. M Werning, W Hinzen, E Machery, Oxford University Press2012</p>
<p>9th challenge on question answering over linked data (QALD-9. R Usbeck, R H Gusmita, A C N Ngomo, M Saleem, Joint proceedings of the 4th workshop on semantic deep learning (SemDeep-4) and NLIWoD4: Natural language interfaces for the web of data (NLIWOD-4) and 9th question answering over linked data challenge (QALD-9) co-located with 17th international semantic web conference (ISWC). 2018California, United States of America</p>
<p>QALD-10 -the 10th challenge on question answering over linked data. R Usbeck, X Yan, A Perevalov, L Jiang, J Schulz, A Kraft, C Möller, J Huang, J Reineke, A C N Ngomo, M Saleem, A Both, Feb 2023IOS Press BVSemantic Web</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates IncNov 202222</p>
<p>NaturalProver: Grounded Mathematical Proof Generation with Language Models. S Welleck, J Liu, X Lu, H Hajishirzi, Y Choi, Advances in Neural Information Processing Systems. Dec 202235</p>
<p>A Yang, B Yang, B Hui, B Zheng, B Yu, C Zhou, C Li, C Li, D Liu, F Huang, arXiv:2407.10671others: Qwen2 technical report. 2024arXiv preprint</p>
<p>Exploring Compositional Generalization of Large Language Models. H Yang, H Lu, W Lam, D Cai, 10.18653/v1/2024.naacl-srw.3Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City; MexicoAssociation for Computational Linguistics20244Student Research Workshop)</p>
<p>Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base. W T Yih, M W Chang, X He, J Gao, 10.3115/v1/P15-1128Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, China20151</p>
<p>Unveiling Transformers with LEGO: a synthetic reasoning task. Y Zhang, A Backurs, S Bubeck, R Eldan, S Gunasekar, T Wagner, 10.48550/arXiv.2206.04301arXiv:2206.04301Feb 2023</p>
<p>Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems. J Zhao, J Tong, Y Mou, M Zhang, Q Zhang, X Huang, 10.18653/v1/2024.emnlp-main.915Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Limits of deep learning: Sequence modeling through the lens of complexity theory. N Zubić, F Soldá, A Sulser, D Scaramuzza, arXiv:2405.16674[cs.LG]2025</p>            </div>
        </div>

    </div>
</body>
</html>
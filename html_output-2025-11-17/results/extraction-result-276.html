<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-276 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-276</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-276</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-258840942</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14201v1.pdf" target="_blank">Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e276.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e276.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (fine-tuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-7B model instruction-fine-tuned on ~1M synthetic arithmetic examples (with CoT for some tasks) that achieves state-of-the-art performance on BIG-bench arithmetic sub-tasks and near-perfect zero-shot accuracy on large-number addition/subtraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (LLaMA-7B base)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (integer arithmetic; includes multi-digit multiplication/division solved via CoT decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>addition/subtraction up to 16-digit operands; multiplication limited so product ≤ 12 digits in training; division where quotient ≤ 6 digits and dividend < 12 digits (training setting)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised instruction fine-tuning on synthetic dataset (~1M QA pairs), LoRA parameter-efficient tuning (r=64) on LLaMA-7B; zero-shot direct-answer generation for learnable tasks; trained to produce chain-of-thought (CoT) intermediate steps for unlearnable tasks (decomposition into subtasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>State-of-the-art on BIG-bench arithmetic sub-task; zero-shot Goat-7B achieves near-perfect accuracy on large-number addition and subtraction (described as near-perfect/close to 100% on unseen test set under training distribution). For multi-digit multiplication/division, direct-answer accuracy is near-zero without CoT but becomes high when using the proposed CoT decomposition (substantial improvement, outperforming GPT-4's long multiplication/division by a large margin). Exact numeric tables are reported in the paper (Table 3) but precise percentages are described qualitatively as near-perfect for addition/subtraction and large gains for CoT-enabled multiplication/division.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>The paper attributes Goat's success primarily to LLaMA's consistent per-digit tokenization which preserves digit alignment and enables the model to learn digitwise arithmetic patterns. For unlearnable composite tasks (multi-digit multiplication/division), performance is enabled by decomposing the task into learnable subtasks (extraction, split, expansion, n-digit×1-digit product, term-by-term addition) and supervising the model to generate interpretable CoT steps, effectively teaching an algorithmic procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance degrades with number of digits when testing beyond the training distribution (limited extrapolation). Training/fine-tuning rapidly yields high accuracy for learnable tasks (e.g., 8-digit addition: near-perfect in ~1.5 hours on A10 with 100K instances). Using CoT enables learnability of tasks that otherwise show essentially zero exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without CoT, exact-string-match accuracy is effectively zero for multi-digit multiplication/division. When other models fail, common errors include misalignment of digits, incorrect copying of numbers, and incorrect intermediate n-digit×1-digit multiplication results; extrapolation outside the training distribution also causes degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared Goat (fine-tuned LLaMA-7B) versus GPT-4 (API), PaLM-540B (few-shot reported), and other open models (Bloom, OPT, GPT-NeoX, Pythia) fine-tuned on same dataset. Ablations: with vs without CoT and removing individual CoT steps (split, expansion, product, adding-term-by-term).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Supervised fine-tuning of a model with consistent digit-level tokenization (LLaMA-7B) can produce near-perfect zero-shot addition/subtraction; harder composite tasks (multi-digit multiplication/division) become solvable when supervised to generate intermediate, human-interpretable CoT that decomposes the computation into learnable subtasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e276.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e276.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source pretrained decoder-only transformer model whose digit-wise tokenization (each digit typically becomes a separate token) is central to Goat's success on arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (used as the base model for fine-tuning in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>same as Goat experiments when fine-tuned (addition/subtraction ≤16 digits, multiplication product ≤12 digits, division quotient ≤6 digits)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>pretrained LLaMA base model fine-tuned via supervised instruction tuning on synthetic arithmetic dataset; LoRA used for parameter-efficient tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>As a base, LLaMA fine-tuned (Goat) produced near-perfect addition/subtraction and, with CoT, high-accuracy multiplication/division; other LLaMA-based variants were shown capable when tokenization preserved digits. (Quantitative percentages reported for Goat which builds on LLaMA-7B.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>LLaMA's consistent tokenization that splits digits into individual tokens preserves digit alignment, greatly facilitating the learning of digitwise arithmetic patterns and carry management; this tokenization property is identified as a key mechanism enabling successful learning via supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>The paper argues that the success is tied to tokenization rather than simply model size; other larger models with inconsistent tokenization failed to match performance after identical fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If tokenization merges digits inconsistently (as in some other models), the model struggles or fails to learn multi-digit arithmetic even for relatively small digit counts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared LLaMA's tokenization and fine-tuning results to other LLMs (Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo) fine-tuned on the same dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Consistent digit-level tokenization (as in LLaMA) is a critical enabler for learning arithmetic by direct supervised fine-tuning; architecture/size alone is insufficient if tokenization breaks digit consistency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e276.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e276.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art closed-source large language model used as a baseline in this paper; shows strong general performance but poor exact-match accuracy on large-number arithmetic under standard prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (evaluated on same prompts and test questions as Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>BIG-bench style multi-digit arithmetic including large-number addition/subtraction and multi-digit multiplication/division (16D+8D, 16D+16D, multi-digit multiply/divide scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot direct prompting, few-shot prompting, chain-of-thought prompting ('Solve it step by step' appended), and few-shot with the authors' decomposition examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Under exact string match, GPT-4's accuracy is reported as almost identically zero on tasks involving large numbers; performs reasonably on some addition tasks (8D+8D, 16D+16D) but fails many 16D+8D cases; chain-of-thought prompting yields only marginal improvements for long multiplication/division; few-shot prompting with the paper's decomposition method improves GPT-4 performance but still inferior to Goat with CoT in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Authors hypothesize GPT-4 has a limited effective working memory for these composite arithmetic computations and inconsistent tokenization of numbers (digit tokenization not always per-digit) which leads to digit alignment and copying errors; GPT-4 often does not benefit from intermediate steps (CoT) and sometimes produces incorrect intermediate steps but correct final answers, suggesting it does not reliably use intermediate supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance does not reliably improve via CoT prompting; accuracy often degrades with increased digit counts and when inputs fall outside few-shot/context distribution; authors reference claims that GPT-4 has short working memory impacting composite arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Common errors: misalignment of corresponding digits, incorrect copying of numbers, incorrect intermediate n-digit×1-digit multiplication results, and poor utilization of CoT leading to incorrect final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared GPT-4 (zero/few-shot, with/without CoT) against Goat-7B (fine-tuned), and tested few-shot with the paper's decomposition; main comparisons center on CoT vs no-CoT and fine-tuned vs prompt-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Despite strong general abilities, GPT-4 struggles on exact-string multi-digit arithmetic; it benefits little from naive CoT prompting for multiplication/division and exhibits errors linked to tokenization and working-memory-like limitations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e276.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e276.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other open LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bloom / OPT / GPT-NeoX / Pythia / GPT-J / GPT-Neo (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A group of open-source LLMs which the authors fine-tuned on the same synthetic arithmetic dataset but found they could not match LLaMA/Goat performance, attributed largely to inconsistent number tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo (as a group)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>various autoregressive transformer variants (not all specified)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (same tasks used to evaluate Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>same synthetic tasks as Goat when fine-tuned, including multi-digit addition and other sub-tasks (details as per Goat training ranges)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>identical supervised fine-tuning on the synthetic arithmetic dataset with same hyperparameters to compare learnability and convergence</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>These models struggle on arithmetic tasks: higher training loss and lower test accuracy than LLaMA when fine-tuned identically; some tasks that are learnable for LLaMA (e.g., multi-digit addition) remain difficult or fail for these models (e.g., near-zero for ≥5-digit addition in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Inconsistent subword/token-level number representations (tokenization that does not split digits consistently) makes it hard for these models to learn digitwise arithmetic patterns; tokenization irregularity forces the model to represent variable-length digit sequences with inconsistent embeddings, hindering learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Even with identical fine-tuning, larger parameter count does not compensate for poor tokenization; these models showed poorer convergence and required more training to overfit small enumerated domains (e.g., exhaustive 2-digit × 2-digit table) than LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High loss during fine-tuning, inability to learn direct-answer mapping for multi-digit arithmetic, severe degradation for larger digit counts due to tokenization irregularity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly (fine-tuned under identical settings) against LLaMA-7B (Goat) to isolate the impact of tokenization and model differences.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Tokenization matters: open models with inconsistent number tokenization fail to learn multi-digit arithmetic with the same ease as LLaMA under identical fine-tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e276.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e276.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT decomposition (Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought decomposition for arithmetic (extraction, split, expansion, product, adding term-by-term)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised CoT method proposed/used in this paper that decomposes unlearnable arithmetic tasks (multi-digit multiplication/division) into a sequence of learnable subtasks and supervises the model to output those intermediate steps before the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT decomposition (task supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-digit multiplication (decomposed), multi-digit division (modified slow division via partial remainders), also used for complex composite arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>used for multiplication tasks like 4D×4D, 6D÷3D, and other multi-digit cases defined in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised generation of intermediate steps (extraction, split, expansion using distributive law, compute n-digit×1-digit products, add term-by-term) inserted before final numeric output; for division a slow-subtractive division CoT based on partial remainders and per-iteration n-digit×1-digit multiplication + comparison is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Without CoT, exact-match accuracy for multi-digit multiplication/division remains at zero in experiments; with full CoT the model achieves high exact-match accuracy (substantial gains; ablations show CoT is necessary). Ablation shows 'adding term by term' is the most crucial step for multiplication accuracy, while omission of product computation in division ablates performance severely.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Decomposition converts an unlearnable composite mapping into a sequence of learnable subtasks, aligning with theoretical results (Wies et al. 2022). Supervised intermediate steps provide the model with explicit algorithmic structure (e.g., distributive expansion, iterative subtraction for division), effectively scaffolding a procedural solution the model can follow.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>CoT enables rapid learning: tasks that would require many epochs to overfit when trained end-to-end can reach comparable accuracy with CoT in far fewer epochs (e.g., 2-digit×2-digit comparable accuracy in 1 epoch with CoT vs ~10 epochs to overfit without).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Removing the 'adding term by term' step causes large drops in multiplication accuracy; omission of 'product' computation step in division also degrades performance. CoT length and training distribution still limit extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared full CoT vs ablated CoT (no split, no expansion, no adding-term-by-term, no product) and vs no CoT (direct answer). Also compared to GPT-4's default long-multiplication/division CoT and to few-shot prompting with decomposition examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Supervised CoT that decomposes composite arithmetic into simple, learnable subtasks is necessary and sufficient to enable high-accuracy multi-digit multiplication/division from LLaMA-7B; particular intermediate steps (term-by-term addition) are critical.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e276.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e276.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Number tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number tokenization / digit-wise tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tokenization strategy for numbers (per-digit tokens vs variable-length subword tokens) which the paper identifies as a primary mechanistic factor determining whether a model can learn arithmetic via supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>number tokenization (digit-level vs subword)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>affects all integer arithmetic tasks evaluated (addition, subtraction, multiplication, division)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>pertinent for multi-digit numbers (2-digit and above up to 16-digit in experiments); tokenization inconsistency especially harmful as digit count grows</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>analysis and controlled fine-tuning comparisons across models with different tokenization schemes; authors show LLaMA tokenizes digits consistently while many other LLMs do not.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Models with consistent per-digit tokenization (LLaMA) learn multi-digit addition/subtraction to near-perfect accuracy under supervised fine-tuning; models with inconsistent tokenization (Bloom, OPT, GPT-NeoX, Pythia) fail or show much worse convergence and accuracy even when fine-tuned identically.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>When each digit maps to a consistent token embedding, the model can learn digitwise arithmetic patterns and carries; inconsistent tokenization forces the model to memorize variable-length digit embeddings and hinders learning of positional digit relationships and carry propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Tokenization effect outweighs naive scaling: larger models with poor tokenization still perform worse on arithmetic than smaller models with digit-level tokenization when fine-tuned identically.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Inconsistent tokenization causes inability to align corresponding digits across numbers, leading to systematic errors in addition/subtraction carries and catastrophic failures on larger digit counts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Tokenization of LLaMA vs GPT-4 vs Pythia vs GPT-NeoX vs Bloom (Table 5); empirical fine-tuning comparisons to assess impact.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Digit-consistent tokenization is a primary enabler for learning exact integer arithmetic via supervised fine-tuning; without it, even large models struggle to learn multi-digit operations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Sub-task decomposition enables learning in sequence to sequence tasks <em>(Rating: 2)</em></li>
                <li>Limitations of language models in arithmetic and symbolic induction <em>(Rating: 2)</em></li>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 2)</em></li>
                <li>Have you seen that number? investigating extrapolation in question answering models <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-276",
    "paper_id": "paper-258840942",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Goat-7B",
            "name_full": "Goat (fine-tuned LLaMA-7B)",
            "brief_description": "A LLaMA-7B model instruction-fine-tuned on ~1M synthetic arithmetic examples (with CoT for some tasks) that achieves state-of-the-art performance on BIG-bench arithmetic sub-tasks and near-perfect zero-shot accuracy on large-number addition/subtraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goat-7B",
            "model_size": "7B",
            "model_architecture": "decoder-only transformer (LLaMA-7B base)",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (integer arithmetic; includes multi-digit multiplication/division solved via CoT decomposition)",
            "number_range_or_complexity": "addition/subtraction up to 16-digit operands; multiplication limited so product ≤ 12 digits in training; division where quotient ≤ 6 digits and dividend &lt; 12 digits (training setting)",
            "method_or_intervention": "supervised instruction fine-tuning on synthetic dataset (~1M QA pairs), LoRA parameter-efficient tuning (r=64) on LLaMA-7B; zero-shot direct-answer generation for learnable tasks; trained to produce chain-of-thought (CoT) intermediate steps for unlearnable tasks (decomposition into subtasks).",
            "performance_result": "State-of-the-art on BIG-bench arithmetic sub-task; zero-shot Goat-7B achieves near-perfect accuracy on large-number addition and subtraction (described as near-perfect/close to 100% on unseen test set under training distribution). For multi-digit multiplication/division, direct-answer accuracy is near-zero without CoT but becomes high when using the proposed CoT decomposition (substantial improvement, outperforming GPT-4's long multiplication/division by a large margin). Exact numeric tables are reported in the paper (Table 3) but precise percentages are described qualitatively as near-perfect for addition/subtraction and large gains for CoT-enabled multiplication/division.",
            "mechanistic_insight": "The paper attributes Goat's success primarily to LLaMA's consistent per-digit tokenization which preserves digit alignment and enables the model to learn digitwise arithmetic patterns. For unlearnable composite tasks (multi-digit multiplication/division), performance is enabled by decomposing the task into learnable subtasks (extraction, split, expansion, n-digit×1-digit product, term-by-term addition) and supervising the model to generate interpretable CoT steps, effectively teaching an algorithmic procedure.",
            "performance_scaling": "Performance degrades with number of digits when testing beyond the training distribution (limited extrapolation). Training/fine-tuning rapidly yields high accuracy for learnable tasks (e.g., 8-digit addition: near-perfect in ~1.5 hours on A10 with 100K instances). Using CoT enables learnability of tasks that otherwise show essentially zero exact-match accuracy.",
            "failure_modes": "Without CoT, exact-string-match accuracy is effectively zero for multi-digit multiplication/division. When other models fail, common errors include misalignment of digits, incorrect copying of numbers, and incorrect intermediate n-digit×1-digit multiplication results; extrapolation outside the training distribution also causes degradation.",
            "comparison_baseline": "Compared Goat (fine-tuned LLaMA-7B) versus GPT-4 (API), PaLM-540B (few-shot reported), and other open models (Bloom, OPT, GPT-NeoX, Pythia) fine-tuned on same dataset. Ablations: with vs without CoT and removing individual CoT steps (split, expansion, product, adding-term-by-term).",
            "key_finding": "Supervised fine-tuning of a model with consistent digit-level tokenization (LLaMA-7B) can produce near-perfect zero-shot addition/subtraction; harder composite tasks (multi-digit multiplication/division) become solvable when supervised to generate intermediate, human-interpretable CoT that decomposes the computation into learnable subtasks.",
            "uuid": "e276.0"
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA (7B)",
            "brief_description": "An open-source pretrained decoder-only transformer model whose digit-wise tokenization (each digit typically becomes a separate token) is central to Goat's success on arithmetic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_size": "7B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (used as the base model for fine-tuning in experiments)",
            "number_range_or_complexity": "same as Goat experiments when fine-tuned (addition/subtraction ≤16 digits, multiplication product ≤12 digits, division quotient ≤6 digits)",
            "method_or_intervention": "pretrained LLaMA base model fine-tuned via supervised instruction tuning on synthetic arithmetic dataset; LoRA used for parameter-efficient tuning.",
            "performance_result": "As a base, LLaMA fine-tuned (Goat) produced near-perfect addition/subtraction and, with CoT, high-accuracy multiplication/division; other LLaMA-based variants were shown capable when tokenization preserved digits. (Quantitative percentages reported for Goat which builds on LLaMA-7B.)",
            "mechanistic_insight": "LLaMA's consistent tokenization that splits digits into individual tokens preserves digit alignment, greatly facilitating the learning of digitwise arithmetic patterns and carry management; this tokenization property is identified as a key mechanism enabling successful learning via supervised fine-tuning.",
            "performance_scaling": "The paper argues that the success is tied to tokenization rather than simply model size; other larger models with inconsistent tokenization failed to match performance after identical fine-tuning.",
            "failure_modes": "If tokenization merges digits inconsistently (as in some other models), the model struggles or fails to learn multi-digit arithmetic even for relatively small digit counts.",
            "comparison_baseline": "Compared LLaMA's tokenization and fine-tuning results to other LLMs (Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo) fine-tuned on the same dataset.",
            "key_finding": "Consistent digit-level tokenization (as in LLaMA) is a critical enabler for learning arithmetic by direct supervised fine-tuning; architecture/size alone is insufficient if tokenization breaks digit consistency.",
            "uuid": "e276.1"
        },
        {
            "name_short": "GPT-4 (baseline)",
            "name_full": "GPT-4",
            "brief_description": "A state-of-the-art closed-source large language model used as a baseline in this paper; shows strong general performance but poor exact-match accuracy on large-number arithmetic under standard prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (evaluated on same prompts and test questions as Goat)",
            "number_range_or_complexity": "BIG-bench style multi-digit arithmetic including large-number addition/subtraction and multi-digit multiplication/division (16D+8D, 16D+16D, multi-digit multiply/divide scenarios)",
            "method_or_intervention": "zero-shot direct prompting, few-shot prompting, chain-of-thought prompting ('Solve it step by step' appended), and few-shot with the authors' decomposition examples",
            "performance_result": "Under exact string match, GPT-4's accuracy is reported as almost identically zero on tasks involving large numbers; performs reasonably on some addition tasks (8D+8D, 16D+16D) but fails many 16D+8D cases; chain-of-thought prompting yields only marginal improvements for long multiplication/division; few-shot prompting with the paper's decomposition method improves GPT-4 performance but still inferior to Goat with CoT in many cases.",
            "mechanistic_insight": "Authors hypothesize GPT-4 has a limited effective working memory for these composite arithmetic computations and inconsistent tokenization of numbers (digit tokenization not always per-digit) which leads to digit alignment and copying errors; GPT-4 often does not benefit from intermediate steps (CoT) and sometimes produces incorrect intermediate steps but correct final answers, suggesting it does not reliably use intermediate supervision.",
            "performance_scaling": "Performance does not reliably improve via CoT prompting; accuracy often degrades with increased digit counts and when inputs fall outside few-shot/context distribution; authors reference claims that GPT-4 has short working memory impacting composite arithmetic.",
            "failure_modes": "Common errors: misalignment of corresponding digits, incorrect copying of numbers, incorrect intermediate n-digit×1-digit multiplication results, and poor utilization of CoT leading to incorrect final answers.",
            "comparison_baseline": "Compared GPT-4 (zero/few-shot, with/without CoT) against Goat-7B (fine-tuned), and tested few-shot with the paper's decomposition; main comparisons center on CoT vs no-CoT and fine-tuned vs prompt-based approaches.",
            "key_finding": "Despite strong general abilities, GPT-4 struggles on exact-string multi-digit arithmetic; it benefits little from naive CoT prompting for multiplication/division and exhibits errors linked to tokenization and working-memory-like limitations.",
            "uuid": "e276.2"
        },
        {
            "name_short": "Other open LLMs",
            "name_full": "Bloom / OPT / GPT-NeoX / Pythia / GPT-J / GPT-Neo (fine-tuned)",
            "brief_description": "A group of open-source LLMs which the authors fine-tuned on the same synthetic arithmetic dataset but found they could not match LLaMA/Goat performance, attributed largely to inconsistent number tokenization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bloom, OPT, GPT-NeoX, Pythia, GPT-J, GPT-Neo (as a group)",
            "model_size": null,
            "model_architecture": "various autoregressive transformer variants (not all specified)",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (same tasks used to evaluate Goat)",
            "number_range_or_complexity": "same synthetic tasks as Goat when fine-tuned, including multi-digit addition and other sub-tasks (details as per Goat training ranges)",
            "method_or_intervention": "identical supervised fine-tuning on the synthetic arithmetic dataset with same hyperparameters to compare learnability and convergence",
            "performance_result": "These models struggle on arithmetic tasks: higher training loss and lower test accuracy than LLaMA when fine-tuned identically; some tasks that are learnable for LLaMA (e.g., multi-digit addition) remain difficult or fail for these models (e.g., near-zero for ≥5-digit addition in prior work).",
            "mechanistic_insight": "Inconsistent subword/token-level number representations (tokenization that does not split digits consistently) makes it hard for these models to learn digitwise arithmetic patterns; tokenization irregularity forces the model to represent variable-length digit sequences with inconsistent embeddings, hindering learning.",
            "performance_scaling": "Even with identical fine-tuning, larger parameter count does not compensate for poor tokenization; these models showed poorer convergence and required more training to overfit small enumerated domains (e.g., exhaustive 2-digit × 2-digit table) than LLaMA.",
            "failure_modes": "High loss during fine-tuning, inability to learn direct-answer mapping for multi-digit arithmetic, severe degradation for larger digit counts due to tokenization irregularity.",
            "comparison_baseline": "Compared directly (fine-tuned under identical settings) against LLaMA-7B (Goat) to isolate the impact of tokenization and model differences.",
            "key_finding": "Tokenization matters: open models with inconsistent number tokenization fail to learn multi-digit arithmetic with the same ease as LLaMA under identical fine-tuning.",
            "uuid": "e276.3"
        },
        {
            "name_short": "CoT decomposition (Goat)",
            "name_full": "Chain-of-Thought decomposition for arithmetic (extraction, split, expansion, product, adding term-by-term)",
            "brief_description": "A supervised CoT method proposed/used in this paper that decomposes unlearnable arithmetic tasks (multi-digit multiplication/division) into a sequence of learnable subtasks and supervises the model to output those intermediate steps before the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CoT decomposition (task supervision)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "multi-digit multiplication (decomposed), multi-digit division (modified slow division via partial remainders), also used for complex composite arithmetic",
            "number_range_or_complexity": "used for multiplication tasks like 4D×4D, 6D÷3D, and other multi-digit cases defined in experiments",
            "method_or_intervention": "supervised generation of intermediate steps (extraction, split, expansion using distributive law, compute n-digit×1-digit products, add term-by-term) inserted before final numeric output; for division a slow-subtractive division CoT based on partial remainders and per-iteration n-digit×1-digit multiplication + comparison is used.",
            "performance_result": "Without CoT, exact-match accuracy for multi-digit multiplication/division remains at zero in experiments; with full CoT the model achieves high exact-match accuracy (substantial gains; ablations show CoT is necessary). Ablation shows 'adding term by term' is the most crucial step for multiplication accuracy, while omission of product computation in division ablates performance severely.",
            "mechanistic_insight": "Decomposition converts an unlearnable composite mapping into a sequence of learnable subtasks, aligning with theoretical results (Wies et al. 2022). Supervised intermediate steps provide the model with explicit algorithmic structure (e.g., distributive expansion, iterative subtraction for division), effectively scaffolding a procedural solution the model can follow.",
            "performance_scaling": "CoT enables rapid learning: tasks that would require many epochs to overfit when trained end-to-end can reach comparable accuracy with CoT in far fewer epochs (e.g., 2-digit×2-digit comparable accuracy in 1 epoch with CoT vs ~10 epochs to overfit without).",
            "failure_modes": "Removing the 'adding term by term' step causes large drops in multiplication accuracy; omission of 'product' computation step in division also degrades performance. CoT length and training distribution still limit extrapolation.",
            "comparison_baseline": "Compared full CoT vs ablated CoT (no split, no expansion, no adding-term-by-term, no product) and vs no CoT (direct answer). Also compared to GPT-4's default long-multiplication/division CoT and to few-shot prompting with decomposition examples.",
            "key_finding": "Supervised CoT that decomposes composite arithmetic into simple, learnable subtasks is necessary and sufficient to enable high-accuracy multi-digit multiplication/division from LLaMA-7B; particular intermediate steps (term-by-term addition) are critical.",
            "uuid": "e276.4"
        },
        {
            "name_short": "Number tokenization",
            "name_full": "Number tokenization / digit-wise tokenization",
            "brief_description": "Tokenization strategy for numbers (per-digit tokens vs variable-length subword tokens) which the paper identifies as a primary mechanistic factor determining whether a model can learn arithmetic via supervised fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "number tokenization (digit-level vs subword)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "affects all integer arithmetic tasks evaluated (addition, subtraction, multiplication, division)",
            "number_range_or_complexity": "pertinent for multi-digit numbers (2-digit and above up to 16-digit in experiments); tokenization inconsistency especially harmful as digit count grows",
            "method_or_intervention": "analysis and controlled fine-tuning comparisons across models with different tokenization schemes; authors show LLaMA tokenizes digits consistently while many other LLMs do not.",
            "performance_result": "Models with consistent per-digit tokenization (LLaMA) learn multi-digit addition/subtraction to near-perfect accuracy under supervised fine-tuning; models with inconsistent tokenization (Bloom, OPT, GPT-NeoX, Pythia) fail or show much worse convergence and accuracy even when fine-tuned identically.",
            "mechanistic_insight": "When each digit maps to a consistent token embedding, the model can learn digitwise arithmetic patterns and carries; inconsistent tokenization forces the model to memorize variable-length digit embeddings and hinders learning of positional digit relationships and carry propagation.",
            "performance_scaling": "Tokenization effect outweighs naive scaling: larger models with poor tokenization still perform worse on arithmetic than smaller models with digit-level tokenization when fine-tuned identically.",
            "failure_modes": "Inconsistent tokenization causes inability to align corresponding digits across numbers, leading to systematic errors in addition/subtraction carries and catastrophic failures on larger digit counts.",
            "comparison_baseline": "Tokenization of LLaMA vs GPT-4 vs Pythia vs GPT-NeoX vs Bloom (Table 5); empirical fine-tuning comparisons to assess impact.",
            "key_finding": "Digit-consistent tokenization is a primary enabler for learning exact integer arithmetic via supervised fine-tuning; without it, even large models struggle to learn multi-digit operations.",
            "uuid": "e276.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2,
            "sanitized_title": "investigating_the_limitations_of_transformers_with_simple_arithmetic_tasks"
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Sub-task decomposition enables learning in sequence to sequence tasks",
            "rating": 2,
            "sanitized_title": "subtask_decomposition_enables_learning_in_sequence_to_sequence_tasks"
        },
        {
            "paper_title": "Limitations of language models in arithmetic and symbolic induction",
            "rating": 2,
            "sanitized_title": "limitations_of_language_models_in_arithmetic_and_symbolic_induction"
        },
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 2,
            "sanitized_title": "evaluating_transformer_language_models_on_arithmetic_operations_using_number_decomposition"
        },
        {
            "paper_title": "Have you seen that number? investigating extrapolation in question answering models",
            "rating": 2,
            "sanitized_title": "have_you_seen_that_number_investigating_extrapolation_in_question_answering_models"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        }
    ],
    "cost": 0.01623175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</p>
<p>Tiedong Liu tiedong.liu@u.nus.edu 
National University of Singapore
National University of Singapore</p>
<p>Bryan Kian 
National University of Singapore
National University of Singapore</p>
<p>Hsiang Low lowkh@comp.nus.edu.sg 
National University of Singapore
National University of Singapore</p>
<p>Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks</p>
<p>We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-ofthe-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation. 1</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown remarkable proficiency across a wide range of natural language processing (NLP) tasks (Brown et al., 2020;Chowdhery et al., 2022;Thoppilan et al., 2022). Notably, GPT-4 (OpenAI, 2023) has achieved state-of-the-art performances in such tasks. However, it is surprising that such powerful language models still struggle with elementary arithmetic tasks. The performance of GPT-4 in 1 https://github.com/liutiedong/goat. arithmetic tasks, particularly multiplication and division of large numbers, currently remains far from optimal, with accuracy levels trending toward zero.</p>
<p>In this paper, we present Goat, a fine-tuned language model that is GOod at Arithmetic Tasks. Goat achieves state-of-the-art performance in elementary arithmetic, including addition, subtraction, multiplication, and division of integers. We adopt an end-to-end supervised instruction-finetuning paradigm on LLaMA (Touvron et al., 2023), leveraging a synthetically generated dataset containing around 1 million samples. Unlike previous research on arithmetic computation (Lee and Kim, 2023;Nogueira et al., 2021;Nye et al., 2021;Qian et al., 2022;Zhou et al., 2022b), our study demonstrates that through supervised fine-tuning alone and without applying any special techniques, our model is capable of generating direct answers for largenumber addition and subtraction with near-perfect accuracy in a zero-shot setting. We attribute this exceptional arithmetic ability to LLaMA's consistent tokenization of numbers and show that this is almost impossible to achieve for previous LLMs such as Bloom (Scao et al., 2022), OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), Pythia (Biderman et al., 2023), etc. However, the model encounters significant difficulties when generating direct answers for arithmetic tasks like large-number multiplication and division. To overcome this challenge, we propose an approach that categorizes various arithmetic tasks into learnable and unlearnable tasks, subsequently decomposing the unlearnable tasks, such as multidigit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. Our approach ensures that the intermediate supervision which facilitates the model's learning is also easily understandable and interpretable by humans. We fine-tune our model to generate the proposed CoT before generating the final answer, similar to sketchpad (Nye et al., 2021). Our method outperforms GPT-4's long multiplication and long division methods by a large margin. We assess the performance of our model using BIG-bench (Srivastava et al., 2022) arithmetic sub-task, and provide a comprehensive evaluation of the effectiveness of our proposed method. Our findings suggest that the model can learn the pattern and generalize to unseen data instead of purely memorizing the computation. Additionally, Goat-7B can be conveniently trained using Low-Rank Adaptation (LoRA) (Hu et al., 2021) technique on a 24GB VRAM GPU, making it easily reproducible for other researchers.</p>
<p>To summarize, our contributions include:</p>
<p>• Our model achieves state-of-the-art performance on various elementary arithmetic tasks, including addition, subtraction, multiplication, and division of positive integers (Section 4).</p>
<p>We show that an open-sourced model finetuned on a synthetically generated dataset has the potential to achieve even higher accuracy on arithmetic tasks compared to GPT-4.</p>
<p>• To the best of our knowledge, we are the first to demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to generate direct answers for certain elementary arithmetic tasks, such as large-number addition and subtraction, without applying any special techniques (Section 3.3). Previously effective chain-of-thought (CoT) methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary. The impressive performance is mainly attributed to LLaMA's consistent tokenization of numbers.</p>
<p>• To solve large-number multiplication and division, we propose a novel decomposition method based on the learnability of the task, leveraging basic arithmetic principles to ensure human interpretability (Section 3.4).</p>
<p>• We systematically investigate the proposed decomposition method and demonstrate its effectiveness (Section 5). We conduct thorough experiments on the decomposition steps in a fully synthetic environment by mitigating many hard-to-control aspects of natural language. Our experimental setup offers an ideal platform to study the impact of CoT and intermediate supervision.</p>
<p>• Our end-to-end instruction tuning pipeline can be easily integrated into existing instructiontuned language models (Chiang et al., 2023;Taori et al., 2023) and potentially enhance their mathematical reasoning for math word problems. We release the model, dataset, and script for generating the dataset.</p>
<p>2 Related Work</p>
<p>Instruction Tuning</p>
<p>Instruction tuning (Chung et al., 2022;Ouyang et al., 2022;Sanh et al., 2021) is a technique used to align pretrained language models with human instructions. It enables targeted customization of LLMs to specific tasks, enhancing their ability to generate more accurate and contextually relevant responses and improving the zero-shot performance. The dataset used for instruction tuning can be human-written (Ouyang et al., 2022), machinegenerated (Peng et al., 2023;Taori et al., 2023;, or collected from web (Geng et al., 2023). Recently, there has been extensive research on fine-tuning LLaMA (Touvron et al., 2023) for various downstream tasks using instruction tuning (Chiang et al., 2023;Geng et al., 2023;Taori et al., 2023;Xu et al., 2023;Yunxiang et al., 2023). Creating high-quality instruction tuning datasets can be expensive and time-consuming. In this study, we utilize a simple Python program to generate input-output pairs for arithmetic tasks.</p>
<p>Arithmetic Reasoning</p>
<p>Arithmetic reasoning has been a topic of interest in NLP research for many years (Lu et al., 2022). Recently, the use of pretrained models (Brown et al., 2020;OpenAI, 2023) has shown great capabilities in solving math word problems. Particularly, chain of thought (CoT) (Kojima et al., 2022;Zhou et al., 2022a) provides the model with the intermediate steps to derive the final answer. However, studies have shown that LLMs struggle with basic arithmetic computation and often make arithmetic mistakes, even though the reasoning process is correct (Cobbe et al., 2021;Gao et al., 2022;Schick et al., 2023). Consequently, one key challenge of arithmetic reasoning, aside from mapping natural language to arithmetic expressions, is how to compute the generated arithmetic expressions with high accuracy.</p>
<p>Arithmetic Computation</p>
<p>Recent studies have explored using external tools to evaluate arithmetic expressions. Toolformer (Schick et al., 2023) and GSM8K (Cobbe et al., 2021) invoke an external calculator to compute the generated arithmetic expression. PoT (Chen et al., 2022) and PAL (Gao et al., 2022) generate programs that can be executed to produce the final answer. While arithmetic can be solved using calculators or programs easily, the ability to perform arithmetic computation is a remarkable trait of human intelligence, and we anticipate LLMs should possess this ability as well.</p>
<p>Previous studies have evaluated the arithmetic abilities of LLMs. Nogueira et al. (2021) have evaluated addition and subtraction tasks. Muffo et al. (2022) have further examined 2-digit multiplication. Yuan et al. (2023) have tested different types of arithmetic operations. CoT seems to be a promising solution for arithmetic computation as well. Similar to humans, autoregressive language model may rely on intermediate supervision to generate the final answer. Scratchpad (Nye et al., 2021) finetunes the language models to produce CoT before generating an answer, and has demon-strated effectiveness on 8-digit addition. However, we show that previously effective CoT methods, such as those used for addition in sketchpad (Nye et al., 2021) and LM Tutor (Qian et al., 2022), are no longer necessary for certain arithmetic tasks like addition. By leveraging simple supervised finetuning alone, our model can perform addition and subtraction with sufficiently high accuracy. For challenging tasks like large-number multiplication and division, previous studies (Muffo et al., 2022;Lee and Kim, 2023) either fail to compute or are inefficient. Furthermore, our model is trained endto-end such that it can follow human instructions.</p>
<p>Method</p>
<p>Language Model</p>
<p>LLaMA (Touvron et al., 2023) is a collection of open-source pretrained language models trained on trillions of tokens using publicly available datasets, and achieves state-of-the-art performance on many benchmarks.</p>
<p>Previous studies (Kim et al., 2021;Nogueira et al., 2021) have shown that tokenization is important for LLM's arithmetic ability. Many commonlyused subword tokenization techniques today are not ideal to represent numbers. However, LLaMA splits each digit into an individual token (Yuan et al., 2023), thereby ensuring consistent tokenization of numbers, as shown in Appendix B.</p>
<p>The selection of language models is crucial to our work. We believe the remarkable arithmetic ability demonstrated in this work is mainly attributed to LLaMA's consistent tokenization of numbers. We experimentally verify that other LLMs, such as Bloom, OPT, GPT-NeoX, and Pythia, finetuned on the same arithmetic dataset, cannot match LLaMA's arithmetic ability.</p>
<p>Learnability of Arithmetic Tasks</p>
<p>Wies et al. (2022) have provided a theoretical analysis on the use of intermediate supervision for solving composite tasks. Specifically, they have shown that for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple subtasks, unlearnable composite problems can become learnable by using intermediate supervision or stepby-step CoT.</p>
<p>Building upon their analysis, we first experimentally categorize learnable and unlearnable tasks. In the context of arithmetic computation, learnable tasks generally refer to those for which the model can be successfully trained to generate direct answers, achieving sufficiently high accuracy within a predefined number of training epochs. Conversely, unlearnable tasks are those that the model struggles to learn and generate direct answers correctly even with extensive training. While the exact reason behind the varying learnability of tasks is not yet fully understood and requires further investigation, we hypothesize that it is associated with the complexity of the underlying pattern and the size of working memory required for completing the task (Bubeck et al., 2023).</p>
<p>We experimentally examine the learnability of these tasks by fine-tuning the model specifically for each task in a simplified synthetic environment (Table 7). Our recognized learnable and unlearnable tasks are listed in Table 1.</p>
<p>The categorization of tasks also aligns with human perception. With practice, humans can mentally calculate the addition and subtraction of two large numbers, writing down the final numerical answer directly from the left (most significant figure) to the right (least significant figure) without the need for sketchpad. However, mentally solving large-number multiplication and division is undeniably a challenging task.</p>
<p>We also observe that our classification of tasks is consistent with the performance of GPT-4. In particular, GPT-4 excels in generating direct answers for large-number addition and subtraction. However, its accuracy significantly drops when it comes to multi-digit multiplication and division tasks. Our observation aligns with the claim made by Bubeck et al. (2023) that GPT-4 has a short working memory and performs poorly on composite arithmetic tasks. This is particularly evident in the case of multiplication, which involves multiple steps of addition. The inability of powerful models like GPT-4 to directly solve unlearnable tasks may suggest that generating direct answers for such tasks is extremely challenging, even with extensive training.</p>
<p>It is noteworthy that a task that is learnable for LLaMA may not necessarily be learnable for other LLMs, which is validated in our experiments in Section 5.3. Furthermore, not all tasks classified as unlearnable are entirely impossible for the model to learn. For instance, 2-digit by 2-digit multiplication is considered an unlearnable task in our case. However, the model can still learn to generate the direct answer by overfitting to the training set, which contains an exhaustive enumeration of all possible 2-digit multiplication. Nevertheless, the process takes nearly 10 epochs to achieve around 90% accuracy. In contrast, by inserting our proposed CoT before the final answer, the model can achieve comparable accuracy in 2-digit multiplication with only 1 epoch of training. These findings align with the claim (Wies et al., 2022) that the presence of intermediate supervision facilitates the learning process.</p>
<p>Addition and Subtraction</p>
<p>Addition and subtraction tasks are learnable, as with supervised fine-tuning alone, the model exhibits a remarkable ability to accurately generate direct numerical answers. The model successfully captures the underlying patterns of the arithmetic operations. This is evident from the model's near-perfect accuracy on the unseen test set, despite being trained on a very limited subset of the data. It is worth mentioning that addition and subtraction operations do not require the use of CoT. This contrasts with previous studies that have employed CoT for addition and subtraction tasks (Lee and Kim, 2023;Nye et al., 2021;Qian et al., 2022).</p>
<p>Multiplication</p>
<p>We experimentally verify that n-digit by 1-digit multiplication is learnable. In contrast, multi-digit multiplication poses significant challenges for the model, suggesting it to be an unlearnable task. To overcome this issue, we adopt a similar strategy used in sketchpad (Nye et al., 2021), which finetunes the LLMs to generate CoT before generating the answer. Specifically, we propose a CoT that decomposes the multi-digit multiplication into a series of 5 learnable sub-tasks: (1) extraction: extract the arithmetic expression from the natural language instruction, (2) split: split the smaller number of the two into place values, (3) expansion: expand the sum based on the distributive property, (4) product: compute each product simultaneously, and (5) adding term by term: add the first two terms and copy the rest, and the final sum is obtained.</p>
<p>Consider the example in Fig. 1. Firstly, the arithmetic expression 397 × 4429 is extracted from the instruction, which can be considered as a "copying" task. Secondly, 397×4429 = 4429×(300+90+7) involves two learnable tasks. The larger number of the two is placed in front and then the smaller one is split, which is similar to "ordering" and "split" learnable tasks. The ordering ensures that there are fewer summation terms in the next step, thereby reducing the CoT length. Thirdly, the sum is expanded using distributive law: 4429 × (300 + 90 + 7) = 4429 × 300 + 4429 × 90 + 4429 × 7, which is similar to "copying" task. Next, 4429 × 300 + 4429 × 90 + 4429 × 7 = 1328700 + 398610 + 31003 where the products are computed at once by applying "multiplication n-digit by 1-digit" with zeros copied at the end of each product. Finally, we take the sum of the first two terms at each step, and copy the rest terms, leveraging "addition" and "copying". Hence, a composite unlearnable task is broken down into simpler tasks that are all learnable.</p>
<p>Division</p>
<p>Similarly, we observe that n-digit by 1-digit division is learnable. However, multi-digit division is unlearnable. We design a novel CoT leveraging a modified slow division method based on the following recurrence equation
R j − D × (q n−(j+1) × 10 j ) = R j+1
where R j is the j-th partial remainder of the division, q n−(j+1) is the digit of the quotient in position n − (j + 1) numbered from least significant 0 to most significant n − 1, n is the number of digits in the quotient, and D is the divisor. Specifically, the main idea is to subtract multiples of the divisor from the dividend until the remainder is less than the divisor.</p>
<p>Here is a detailed breakdown of the CoT used in Fig. 1. Consider the first iteration (first equation). The first step 8914−64×100 requires the model to copy the dividend and the divisor, and subsequently generate a number q n−(j+1) × 10 j such that the product of q n−(j+1) × 10 j and the divisor D is less than or equal to the partial remainder R j . This inherently involves two learnable tasks: "n-digit by 1digit multiplication" and "comparison". We experimentally show that this composite task is learnable. The second step 8914 − 64 × 100 = 8914 − 6400 mainly involves a "copying" task and an "n-digit by 1-digit multiplication" task. The third step 8914 − 6400 = 2514 leverages "subtraction". The process iterates until the leftover is less than the divisor, which implies the model has to implicitly learn comparison. Finally, the model generates the quotient by combining all q n−(j+1) 's in previous iterations, which can be considered as the inverse of the "split" task, and finally copies the remainder if it is not zero.</p>
<p>A summary of prompts and expected output for various tasks are shown in Table 2.</p>
<p>Settings</p>
<p>In this paper, we consider the addition and subtraction of two positive integers with each containing up to 16 digits. It is worth noting that the result of subtraction can be negative. To limit the maximum generated sequence length, we consider the multiplication of two positive integers whose product falls within 12 digits, and the division of two positive integers resulting in a quotient within 6 digits where the dividend is less than 12 digits. Since we focus on arithmetic tasks of integers, we aim  to obtain the least positive remainder in the case when it is not divisible. In Section 5.2, we present an analysis showcasing the limited extrapolation capabilities of finetuned LLMs. Consequently, input data that falls outside the distribution of the training data is unlikely to yield reasonable answers. Our method potentially applies to numbers with more digits, though the training cost will increase correspondingly.</p>
<p>Dataset</p>
<p>We generate the dataset synthetically using a Python script. The dataset consists of around 1 million question-answer pairs. The answer contains the proposed CoT as well as the final numerical output. The numbers are randomly generated, hence ensuring a very low probability of instances being duplicated, although small numbers may be sampled multiple times. We sample from log space to ensure the numbers are equally likely to be sampled from different orders of magnitude, which is similar to the sampling method used by Lee and Kim (2023). The details of the dataset are presented in Appendix F.</p>
<p>Fine-tuning</p>
<p>To enable the model to solve arithmetic problems based on instructions and facilitate natural language question answering, we generate hundreds of instruction templates using ChatGPT (Table 6). During the instruction tuning process, we randomly select a template for each arithmetic input from the training set, and fine-tune LLaMA-7B similar to the method used in Alpaca (Taori et al., 2023). We apply various techniques to enhance the model's adaptability to diverse question formats, such as randomly removing spaces between numbers and symbols in the arithmetic expression, replacing "*" with "x" or "times", etc.</p>
<p>Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU. In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy. The training hyperparameters are listed in Appendix A.</p>
<p>Experiments</p>
<p>We evaluate our model using BIG-bench arithmetic dataset (Srivastava et al., 2022), as well as our extra selected tasks. The results are shown in Table 3. Notably, in a zero-shot setting, Goat-7B achieves comparable or even higher accuracy on BIG-bench compared to the few-shot PaLM-540B.</p>
<p>Metric</p>
<p>We first compute the accuracy based on the standard exact string match (Appendix C). We observe that GPT-4's accuracy under exact string match is almost identically zero on tasks involving large numbers. However, in many cases where the final answer is incorrect, the majority of digits in the generated answer align with the target number, with only a few digits being incorrect. Inspired by recent study on the emergent abilities of LLMs (Schaeffer et al., 2023), we include a digit match metric that can reflect the per-token error rate of the output, as each digit is uniquely represented by a token in LLaMA.</p>
<p>Task</p>
<p>BIG-bench</p>
<p>Extra Tasks   ADD  1D  2D  3D  4D  5D  8D+8D 16D+8D 16D+16D GPT  Table 3: The result of GPT-4 and Goat-7B on BIG-bench Arithmetic sub-task and extra selected arithmetic tasks, using metrics Exact String Match/Digit Match (Appendix C), shown in percentage. We test GPT-4 and Goat with exactly the same questions and prompts. We evaluate GPT-4 using the API version on May 10th. For Big-bench tasks, nD refers the n-digit by n-digit operation, except for division where nD means n-digit by m-digit where m ≤ n. BIG-bench only includes division operation without remainder, whereas in extra tasks we include the cases where the remainder is not zero and ask GPT-4 to output the answer in "quotient R remainder" format. It should be noted that we exclude the BIG-bench test data from our training dataset as much as possible, although the overlap is unavoidable for operations involving small numbers.</p>
<p>Comparison</p>
<p>Comparing the performance of Goat and GPT-4 for large-number multiplication and division may seem unfair, as GPT-4 generates direct answers while Goat relies on CoT. Hence, we also evaluate GPT-4's performance with CoT by appending "Solve it step by step" at the end of each prompt. By default, GPT-4 uses long multiplication and long division methods. However, we observe that generating CoT only leads to marginal improvement in accuracy. In some cases, the intermediate steps from long multiplication and division are incorrect, but surprisingly the final answer is correct. This implies that GPT-4 does not effectively take advantage of intermediate supervision from CoT to improve the final output. We identify the following 3 common errors from GPT-4's solution, which results in incorrect final answers: (1) the alignment of corresponding digits, (2) copying of numbers, and (3) the intermediate result from n-digit by 1-digit multiplication.</p>
<p>Additionally, we observe that GPT-4 performs reasonably well on 8D +8D and 16D +16D tasks, but fails on most 16D + 8D tasks, though intuitively 16D + 8D should be relatively easier than 16D+16D. While the exact reason for this remains unclear, one possible factor could be GPT-4's inconsistent number tokenization (Table 5), which makes it difficult to align the corresponding digits of two numbers. full CoT no split no expansion no adding term by term no CoT Figure 2: Accuracy (exact string match) against the number of samples seen during the training of 4D × 4D task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>Here we want to study the usefulness and effectiveness of each intermediate decomposition step. Specifically, for multiplication (Fig. 2), we com- full CoT no product no CoT Figure 3: Accuracy (exact string match) against the number of samples seen during the training of 6D ÷ 3D task. Evaluated on the same randomly generated unseen test set using training checkpoints.</p>
<p>pare the accuracy of 4-digit by 4-digit multiplication by removing one particular step in the CoT, including split, expansion, adding term by term (referring to G), as well as no CoT. For division (Fig. 3), we compare the accuracy of 6-digit by 3-digit division after removing the middle step that computes the product (referring to G), as well as no CoT. To minimize the impact caused by natural language, we conduct an ablation study in a simplified synthetic environment (Table 7). The multiplication results suggest that the "adding term by term" step plays a crucial role in obtaining the final answer. In contrast, the "split" and "expand" steps have minimal impact, and can potentially be omitted for generating more concise CoT. This can be attributed to the nature of these two intermediate steps, which primarily involve simple and learnable tasks like copying and comparison. Nevertheless, we still retain these steps to ensure human interpretability.</p>
<p>The accuracy of exact string match without CoT remains consistently at zero for both 4D × 4D multiplication and 6D ÷ 3D division. This further showcases the validity of our approach, as breaking down complex arithmetic tasks into a series of learnable tasks can indeed facilitate the training process for LLMs.</p>
<p>Extrapolation</p>
<p>Extrapolation refers to the ability of the model to predict data that lies out-of-distribution (OOD) of training data. We test addition for numbers larger than those in the training data distribution. The results reveal that the model has limited extrapolation capabilities. There is a gradual drop in accuracy, as the test set deviates further from the training set. This observation is consistent with the result reported in (Kim et al., 2021), highlighting a limitation of our fine-tuned model and underscoring the significance of training data distribution.  Figure 4: Accuracy against the number of digits for the addition task. The model is trained up to 16D+16D, and tested on 17D+17D onward.</p>
<p>Comparison with Other LLMs</p>
<p>We conduct comprehensive experiments on a variety of LLMs, including Bloom, OPT, GPT-J, GPT-NeoX, and Pythia. These models are fine-tuned using the identical dataset as that for Goat, maintaining consistency in the training hyperparameters. Our experiment shows that they all struggle with arithmetic tasks. Even for tasks that are considered learnable for LLaMA, such as multi-digit addition, the loss during fine-tuning is significantly higher than that of LLaMA. The observation underscores the claim made in (Nogueira et al., 2021) that tokenization is a crucial factor in the performance of arithmetic tasks.</p>
<p>Few-shot Prompting with GPT-4</p>
<p>GPT-4 demonstrates powerful in-context learning abilities. We further examine the effectiveness of our proposed decomposition method for solving large-number multiplication and division by using few-shot prompting with GPT-4 (see Appendix H). We observe that our decomposition method allows GPT-4 to generate correct answers more frequently than using its default long multiplication and division methods. This further supports the effectiveness and validity of our approach. Examples of the prompt and output are shown in Appendix H.</p>
<p>Limitations</p>
<p>Humans are capable of performing multiplication and division on arbitrarily large numbers, providing sufficient time and space for calculations. In contrast, LLMs often suffer from extrapolation prob-lems. The models are unlikely to generate reasonable answers if the input deviates significantly from the distribution of training data. To enhance the human interpretability of intermediate supervision, we use the straightforward CoT that follows simple basic arithmetic rules. However, this design may not be the most efficient way to facilitate the final answer generation. There are potentially more suitable multiplication and division algorithms for the model to learn. Besides, our research only focuses on elementary arithmetic operations involving integers. Nevertheless, we anticipate that our method could be applicable to decimal computation as well.</p>
<p>Conclusion</p>
<p>In summary, we demonstrate the feasibility that supervised fine-tuning alone can enable LLMs to perform certain basic arithmetic operations with high accuracy. With our proposed CoT, our model achieves state-of-the-art performance on various elementary arithmetic tasks. Our research offers an excellent platform for investigating the mechanism of working memory and the influence of intermediate supervision on text generation. Our method can be easily integrated with other instruction-tuned LLMs and has the potential to further enhance arithmetic reasoning abilities in solving math word problems.</p>
<p>A Hyperparameters</p>
<p>Hyperparameter Value batch size 128 learning rate 0.0003 lora r 64 lora alpha 64 lora target module q, v, k, o lora dropout 0.05 epoch 1  Nogueira et al. (2021) demonstrate that models with inconsistent tokenization of numbers barely learn the addition of 2-digit numbers, and it completely fails to learn the addition of larger numbers. Specifically, it has an accuracy of zero for 5 digits or more. They attribute this failure to the lack of systematic tokenization of individual digits. For instance, "123" might be tokenized as "12" and "3", while "234" might be tokenized as "2" and "34". Consequently, the model is required to learn that the embedding of a token may represent either a single digit or two digits and so on. Hence, it might be challenging for the model to learn to map an embedding to a number when the number of digits it represents changes irregularly. In Table 5, we compare number tokenization across different LLMs.</p>
<p>B Tokenization</p>
<p>C Metric</p>
<p>Exact string match is defined as 1 if the output string exactly matches the target string, and 0 otherwise. Then we take the average of exact string match for each task. Char error rate (CER) is defined as the percentage of characters that were incorrectly predicted. We compute CER using Python torchmetrics package. Then we define digit match accuracy as 1 − cer. We include this metric because, for difficult tasks, the exact string match could be identically zero, making it hard to evaluate the performance. In many cases, both GPT-4 and Goat may have very few incorrect digits in the middle of the generated answer, and the number of digits in the generated answer generally matches the target number. What is the answer to {arithmetic}? . . . . . . Table 6: Example templates to fine-tune arithmetic tasks with natural language instructions, generated by ChatGPT. During training, {arithmetic} is replaced by the randomly generated arithmetic expression, like 3425 * 5823.</p>
<p>D Simplified Synthetic Environment</p>
<p>We use the simplified synthetic environment to study the effectiveness of various CoT, by avoiding many hard-to-control aspects of natural languages. The difference between this and Goat is that we use a more structured prompt without any instruction template and a straightforward completion of the task. This enables easy comparison between the model's performance on different tasks, allowing us to examine the learnability of various sub-tasks and explore the effectiveness of the proposed CoT. The input and output examples for the simplified synthetic environment are shown in Table 7.</p>
<p>E Special Cases</p>
<p>In general, multi-digit multiplication and division are considered unlearnable, and we use the decomposition method to solve them. However, some special cases within multi-digit multiplication and division are learnable, and in these cases, we omit CoT and generate the direct answer:</p>
<p>• For multiplication, one of the two numbers contains only one non-zero digit, such as 857483 × 400 = 342993200. This type of task is similar to learnable n-digit by 1-digit multiplication, with the zeros being copied at the end of the product.</p>
<p>• The dividend is equal to the divisor. In that case, the quotient is identically one. For example, 358 ÷ 358 = 1.</p>
<p>• The dividend is less than the divisor. In that case, the quotient is zero and the remainder equals the dividend. For example, 423 ÷ 968 = 0 R 423.</p>
<p>F Dataset</p>
<p>In general, it is difficult to determine the optimal proportion for each task. The number and composition of data samples also depend on the problem settings (see Section 3.6). We empirically find that n-digit by 1-digit multiplication and division may be easier than other tasks, as it requires fewer samples to reach the same level of accuracy as other tasks during task-specific fine-tuning in the simplified synthetic environment. It is noteworthy that the data samples are all randomly generated, so the probability of the occurrence of duplicated samples is very low for large numbers. Therefore, the train-  Table 7: Examples of input and output for training and testing in the simplified synthetic environment, which is used for testing the learnability of sub-tasks and ablation studies. Specifically, "+", "-", "*", and "\" are used for addition, subtraction, multiplication, and division, respectively. Space is inserted between numbers and symbols. The input and output are formatted to mitigate the influence of natural language.  ing loss can reflect the test accuracy on unseen the test set, if the dataset is only trained for one epoch.</p>
<p>Since the synthetic dataset can be generated very easily, we first create a dataset that contains a sufficient number of data samples for training and then observe the training loss and apply early stopping. We observe that the training loss does not show any significant decrease after training on about one million samples. It should be noted that convergence also depends on other hyper-parameters such as batch size and learning rate. Hence, it is recommended to use a dataset larger than what is necessary and terminate the training process when the training loss no longer decreases.</p>
<p>G Ablation Study</p>
<p>We name the steps (shown in the box below) as (1) extraction, (2) split, (3) expansion, (4) product, and (5, 6, . . . ) adding term by term. The ablation study is performed by removing one particular step while keeping other steps unchanged. We exclude the (1) "extraction" and (4) "product" steps from the ablation study as it is crucial for multi-digit multiplication.</p>
<p>Multiplication</p>
<p>Calculate 397 x 4429 \nAnswer:</p>
<p>× 4429</p>
<p>(1) = 4429 × (300 + 90 + 7)</p>
<p>(2) = 4429 × 300 + 4429 × 90 + 4429 × 7</p>
<p>(3)</p>
<p>= 1328700 + 398610 + 31003 (4) = 1727310 + 31003 (5) = 1758313</p>
<p>For division, the ablation study is performed by removing the middle step (bold) that computes the product for all iterations, while keeping other steps unchanged.</p>
<p>Division</p>
<p>What is 8914/64? \nAnswer: </p>
<p>Figure 1 :
1Example of Goat's response on addition, subtraction, multiplication, and division tasks. Prompts are marked in bold, followed by Goat-7B's response.</p>
<p>Figure 5 :
5Composition of tasks in the dataset.</p>
<p>8914 − 64 × 100 = 8914 − 6400 =2514 2514 − 64 × 30 = 2514 − 1920 =594 594 − 64 × 9 = 594 − 576 =18Therefore, 8914 ÷ 64 = 139 R 18</p>
<p>Table 2 :
2Examples of prompts and targets for fine-tuning LLaMA. "\nAnswer: " is appended at the end of each prompt. It should be noted that there are a few special cases when CoT is not required (see Appendix E).</p>
<p>Table 4 :
4Hyperparameters for fine-tuning LLaMA-7B.
H Few-shot Prompting with  Here are two examples showing 3-shot prompting using GPT-4.3-Shot prompting for multiplicationPrompt:Here are 3 examples of question-solution pairs for solving arithmetic multiplication problems. Solve the last question using the same format given in the solutions."question": "235658 * 91204 = ", "solution": "235658 * 91204 = 235658 * (90000 + 1000 + 200 + 4) = 235658 * 90000 + 235658 * 1000 + 235658 * 200 + 235658 * 4 = 21209220000 + 235658000 + 47131600 + 942632 = 21444878000 + 47131600 + 942632 = 21492009600 + 942632 = 21492952232" "question": "59909636 * 6070 = ", "solution": "59909636 * 6070 = 59909636 * (6000 + 70) = 59909636 * 6000 + 59909636 * 70 = 359457816000 + 4193674520 = 363651490520" "question": "7661 * 9461 = ", "solution": "7661 * 9461 = 9461 * (7000 + 600 + 60 + 1) = 9461 * 7000 + 9461 * 600 + 9461 * 60 + 9461 * 1 = 66227000 + 5676600 + 567660 + 9461 = 71903600 + 567660 + 9461 = 72471260 + 9461 = 72480721" "question": "58061 * 3569 = ",GPT-4:To solve the multiplication problem of 58061 multiplied by 3569 in the same format, we'll distribute 58061 across the sum of the digits of 3569 multiplied by their respective places. So, "solution": "58061 * 3569 = 58061 * (3000 + 500 + 60 + 9) = 58061 * 3000 + 58061 * 500 + 58061 * 60 + 58061 * 9 = 174183000 + 29030500 + 3483660 + 522549 = 203213500 + 3483660 + 522549 = 206697160 + 522549 = 207219709"GPT-4:To solve the division problem of 262122 divided by 591 in the same format, we'll subtract multiples of 591 from 262122 until we reach a number less than 591, then that's our remainder. So, "solution": "262122 -591 * 400 = 262122 -236400 = 25722\n25722 -591 * 40 = 25722 -23640 = 2082\n2082 -591 * 3 = 2082 -1773 = 309\nTherefore, 262122 / 591 = 443 R 309"
Pythia: A suite for analyzing large language models across training and scaling. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, O&apos; Kyle, Eric Brien, Mohammad Aflah Hallahan, Shivanshu Khan, Purohit, Edward Usvsn Sai Prashanth, Raff, arXiv:2304.01373arXiv preprintStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mo- hammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>GPT-NeoX-20B: An opensource autoregressive language model. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach, 10.18653/v1/2022.bigscience-1.9Proceedings of BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language Models. BigScience Episode #5 -Workshop on Challenges &amp; Perspectives in Creating Large Language ModelsAssociation for Computational Linguisticsvirtual+DublinSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro- hit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An open- source autoregressive language model. In Proceed- ings of BigScience Episode #5 -Workshop on Chal- lenges &amp; Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe- ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general in- telligence: Early experiments with gpt-4.</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.12588arXiv preprintWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reason- ing for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Vicuna: An open-source chatbot impressing gpt-4 with 90%<em> chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%</em> chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Dehghani, arXiv:2210.11416Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprintHyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, arXiv:2211.10435Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprintLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Gra- ham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Xinyang Geng, Arnav Gudibande, Hao Liu, Blog post. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal- lace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic re- search. Blog post, April, 1.</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adap- tation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Have you seen that number? investigating extrapolation in question answering models. Jeonghwan Kim, Giwon Hong, Kyung-Min Kim, Junmo Kang, Sung-Hyon Myaeng, 10.18653/v1/2021.emnlp-main.563Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsJeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo Kang, and Sung-Hyon Myaeng. 2021. Have you seen that number? investigating extrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7031-7037, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.11916arXiv preprintTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Recursion of thought: Divide and conquer reasoning with language models. Soochan Lee, Gunhee Kim, Soochan Lee and Gunhee Kim. 2023. Recursion of thought: Divide and conquer reasoning with language models.</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, arXiv:2212.10535arXiv preprintPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. 2022. A survey of deep learn- ing for mathematical reasoning. arXiv preprint arXiv:2212.10535.</p>
<p>Evaluating transformer language models on arithmetic operations using number decomposition. Matteo Muffo, Aldo Cocco, Enrico Bertino, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources AssociationMatteo Muffo, Aldo Cocco, and Enrico Bertino. 2022. Evaluating transformer language models on arith- metic operations using number decomposition. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 291-297, Mar- seille, France. European Language Resources Asso- ciation.</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, arXiv:2102.13019arXiv preprintRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021. Investigating the limitations of transform- ers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. arXiv preprintMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratch- pads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>. OpenAI. 2023. Gpt-4 technical report. OpenAI. 2023. Gpt-4 technical report.</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprintBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal- ley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan, arXiv:2208.05051Limitations of language models in arithmetic and symbolic induction. arXiv preprintJing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 2022. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051.</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, H Stephen, Lintang Bach, Zaid Sutawika, Antoine Alyafeai, Arnaud Chaffin, Teven Le Stiegler, Arun Scao, Raja, arXiv:2110.08207arXiv preprintVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training en- ables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Angela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ilić, Roman Hesslow, Alexandra Sasha Castagné, François Luccioni, Yvon, arXiv:2211.05100Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, El- lie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Are emergent abilities of large language models a mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, arXiv:2304.15004arXiv preprintRylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language mod- els a mirage? arXiv preprint arXiv:2304.15004.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, arXiv:2302.04761Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, GitHub repositoryRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. GitHub repos- itory.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applica- tions. arXiv preprint arXiv:2201.08239.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and effi- cient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, A Noah, Daniel Smith, Hannaneh Khashabi, Hajishirzi, arXiv:2212.10560arXiv preprintYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al- isa Liu, Noah A Smith, Daniel Khashabi, and Han- naneh Hajishirzi. 2022. Self-instruct: Aligning lan- guage model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Sub-task decomposition enables learning in sequence to sequence tasks. Noam Wies, Yoav Levine, Amnon Shashua, arXiv:2204.02892arXiv preprintNoam Wies, Yoav Levine, and Amnon Shashua. 2022. Sub-task decomposition enables learning in sequence to sequence tasks. arXiv preprint arXiv:2204.02892.</p>
<p>Baize: An open-source chat model with parameter-efficient tuning on self-chat data. Canwen Xu, Daya Guo, Nan Duan, Julian Mcauley, arXiv:2304.01196arXiv preprintCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.02015arXiv preprintZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023. How well do large lan- guage models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015.</p>
<p>Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, Zhang You, arXiv:2303.14070arXiv preprintLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070.</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, arXiv:2205.01068Mona Diab, Xian Li, Xi Victoria LinarXiv preprintet al. 2022. Opt: Open pre-trained transformer language modelsSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, arXiv:2205.10625Quoc Le, and Ed Chi. 2022aarXiv preprintDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022a. Least-to-most prompting enables complex reason- ing in large language models. arXiv preprint arXiv:2205.10625.</p>
<p>Model Number Tokenization LLaMA 74815. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, arXiv:2211.09066Teaching algorithmic reasoning via incontext learning. 74arXiv preprint7 [1, 29871, 29955] GPT-4 74815 [20338, 868] 7481 [20338, 16] 748 [20338] , 39373, 996] 7481 [2, 406, 34490] 748 [2, 39373] 74 [2, 5243] 7 [2, 406] Pythia 74815 [24, 2385, 1010] GPT-NeoX-20B 7481 [24, 34474] MPT-7B 748 [24, 2385Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022b. Teaching algorithmic reasoning via in- context learning. arXiv preprint arXiv:2211.09066. Model Number Tokenization LLaMA 74815 [1, 29871, 29955, 29946, 29947, 29896, 29945] 7481 [1, 29871, 29955, 29946, 29947, 29896] 748 [1, 29871, 29955, 29946, 29947] 74 [1, 29871, 29955, 29946] 7 [1, 29871, 29955] GPT-4 74815 [20338, 868] 7481 [20338, 16] 748 [20338] , 39373, 996] 7481 [2, 406, 34490] 748 [2, 39373] 74 [2, 5243] 7 [2, 406] Pythia 74815 [24, 2385, 1010] GPT-NeoX-20B 7481 [24, 34474] MPT-7B 748 [24, 2385] 74</p>
<p>. Gpt-J 74815, 48246, 1314] GPT-Neo 7481 [22, 40271GPT-J 74815 [48246, 1314] GPT-Neo 7481 [22, 40271]</p>
<p>Comparison of number tokenization of various LLMs. It should be noted that ChatGLM also splits each digit into an individual token. Evaluating ChatGLM's arithmetic abilities will be left as future work. 5Table 5: Comparison of number tokenization of various LLMs. It should be noted that ChatGLM also splits each digit into an individual token. Evaluating ChatGLM's arithmetic abilities will be left as future work.</p>            </div>
        </div>

    </div>
</body>
</html>
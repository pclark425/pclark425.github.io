<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3320 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3320</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3320</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-0744580e75a74357e466a57082c85cb42f548aa9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0744580e75a74357e466a57082c85cb42f548aa9" target="_blank">The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection with additional 1.84 million rationales across 1,060 tasks, and shows that instruction tuning with coT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks.</p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B&11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3320.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3320.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoT-T5 (Flan-T5 fine-tuned with CoT COLLECTION)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Flan-T5 model (3B and 11B variants) fine-tuned on the paper's CoT COLLECTION to produce chain-of-thought (step-by-step) rationales before answers, improving zero-shot and few-shot reasoning compared to the Flan-T5 baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flan-T5 (instruction-tuned T5) additionally fine-tuned on 1.84M machine-augmented rationales across 1,060 tasks (the CoT COLLECTION); trained to emit a rationale r before the answer y, using the trigger 'Let's think step by step' during training and eval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) prompting', 'Chain-of-Thought fine-tuning (CoT FT)', 'Few-shot adaptation with LoRA (parameter-efficient fine-tuning)', 'Direct evaluation (no rationale)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT prompting: generate a step-by-step rationale before the final answer using the phrase 'Let's think step by step'. CoT fine-tuning: supervised fine-tuning to generate rationale+answer sequences from the CoT COLLECTION. LoRA: low-rank adapters applied during few-shot fine-tuning to adapt while preserving CoT ability. Direct evaluation: standard label-verbalizer classification without intermediate rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style focused (Chain-of-Thought across many tasks) — the paper applies a single primary reasoning style (CoT) but across a diverse set of tasks and languages; diversity comes from the task distribution of training data rather than mixing qualitatively different reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BIG-Bench-Hard (BBH), P3 Eval (subset), MGSM (subset), domain-specific few-shot tasks (LEDGAR, CaseHold, MedNLI, PubMedQA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>BBH: a set of challenging zero-shot reasoning tasks; P3 Eval: evaluation subset from instruction tuning benchmarks; MGSM: multilingual grade-school math benchmark; domain-specific tasks: legal and medical classification/QA datasets used in few-shot adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Zero-shot BBH (CoT evaluation) improvement over Flan-T5: +4.34% (CoT-T5-3B) and +2.60% (CoT-T5-11B) average accuracy. In few-shot domain tasks (64-shot adaptation with LoRA): CoT-T5-3B + LoRA CoT FT average = 64.02% vs Flan-T5-3B + LoRA FT = 60.19% (delta +3.83%). CoT-T5-11B + LoRA CoT FT average = 68.96% vs Flan-T5-11B + LoRA FT = 66.59% (delta +2.37%). In P3 (T0 setup) CoT FT on T0-3B subset improved T0-3B by +8.65% average on 11 P3 datasets. Multilingual (MGSM subset): mT0/mT5 fine-tuned with CoT data produced 2x–10x relative improvements on average across five languages compared to non-CoT baselines (table reports concrete improvements per language; e.g., MT0-3.7B + CoT FT yields 7.6% for Korean vs MT0 baseline 0.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Explicit comparisons: (1) CoT evaluation vs Direct evaluation — CoT-T5 improves CoT-based accuracy substantially while maintaining or improving Direct eval in some settings (CoT-T5-11B improved Direct eval). (2) CoT fine-tuning on many tasks (1,060) vs few CoT tasks (9) or vs more instances on few tasks: training across many tasks (even 10K instances over 1,060 tasks) outperforms many more instances concentrated in only 9 CoT tasks (180K instances), showing breadth of task diversity matters more than raw instance count. (3) In few-shot adaptation, LoRA applied to CoT-T5 preserves and leverages CoT ability better than full fine-tuning in that setting. (4) CoT-T5 few-shot performance surpasses ICL baselines using larger proprietary models (e.g., ChatGPT/Claude) when demonstrations are limited by context length.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning smaller LMs on a large, diverse set of chain-of-thought rationales enables them to perform step-by-step reasoning in zero-shot and few-shot settings; breadth of task diversity in CoT fine-tuning is more important than the sheer number of instances per task; LoRA parameter-efficient fine-tuning preserves CoT ability for few-shot adaptation and yields strong performance; CoT fine-tuning can close some of the gap between smaller LMs and much larger LMs on reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT fine-tuning sometimes slightly degrades Direct evaluation (e.g., CoT-T5-3B had a 0.96% Direct-eval degradation while CoT-eval improved). For some task types (sentiment, coreference, WSD) CoT rationales generated by large LMs can be short/uninformative and were excluded; arithmetic reasoning rationales from the teacher LLM (Codex) sometimes degenerated and required filtering. Also, for Flan-T5, regular full fine-tuning sometimes outperforms applying LoRA; CoT fine-tuning didn't uniformly help every base model without adaptation (i.e., benefits depend on base model and fine-tuning recipe).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3320.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3320.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5 (instruction-tuned T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned variant of T5 used as the base model in experiments; served as the main baseline to evaluate the impact of CoT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 family model instruction-tuned on a collection of instruction tasks (Flan/FLAN Collection). Evaluated in 3B and 11B sizes in this paper as the baseline before additional CoT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct evaluation (no rationale)', 'CoT prompting (at eval; often fails for smaller LMs without CoT fine-tuning)', 'Instruction tuning (pre-existing)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Direct evaluation: use label verbalizers to map LM outputs to options. CoT prompting: append 'Let's think step by step' to elicit rationales — smaller Flan-T5 often fails to produce adequate rationales without being CoT fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single primary style in baseline (instruction tuned) — the paper contrasts this with CoT fine-tuned variants rather than mixing multiple reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BBH, P3 Eval, MGSM, domain-specific few-shot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same benchmarks used across the paper for comparison (see CoT-T5 entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Baseline metrics: Flan-T5-3B total avg (Table 1) ≈ 35.60; Flan-T5-11B total avg ≈ 39.78. Under CoT evaluation, CoT-T5 improved above these baselines by +4.34% (3B) and +2.60% (11B) on BBH average accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared against CoT-T5: Flan-T5 without CoT fine-tuning performs worse on CoT evaluation and often fails to generate rationales with the trigger phrase; regular fine-tuning on a target few-shot task sometimes outperform CoT fine-tuning for Flan-T5, indicating interactions between base model and CoT recipe.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Flan-T5 benefits from CoT fine-tuning but the gains depend on model size and fine-tuning recipe; simply prompting Flan-T5 with CoT triggers is often insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Flan-T5 when CoT fine-tuned (naively) can underperform full fine-tuning on some few-shot tasks; Flan-T5 often generates no rationales even with the CoT trigger phrase, motivating CoT fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3320.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3320.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex (used as teacher for rationale augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary OpenAI model (Codex) used by the authors to generate chain-of-thought rationales (machine-augmented) for the CoT COLLECTION; outputs were filtered to ensure quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A family of OpenAI transformer models adapted for code generation and language tasks; used here with in-context learning prompts (6–8 demonstrations per task family) to generate rationales conditioned on ground-truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['In-context learning (ICL) to produce Chain-of-Thought rationales', 'Prompt engineering (label-first demonstrations)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The authors use task-family demonstrations to prompt Codex to output rationale r conditioned on instance and gold label, sometimes ordering label before rationale to improve quality. Generated multiple candidate rationales and applied filtering (presence of answer token, max length, de-duplication, code filter).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single method (ICL-based CoT generation) but applied across a highly diverse set of 1,060 tasks to create varied rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Rationale augmentation across Flan Collection tasks (1,060 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Codex was used to generate CoT rationales for inputs/labels sampled from a wide set of instruction tasks (P3, FLAN, SNI etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Not an evaluation benchmark result, but authors report Codex produced sufficiently high-quality rationales after filtering; they also compared rationale quality from Codex vs Bard and Claude (Appendix A) and report comparable ROSCOE scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Authors note reproducibility concerns (Codex decommissioned) and performed limited comparisons with Bard and Claude to show other LLMs can generate usable rationales; also mention later work used GPT-4 to augment even more rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICL with a strong proprietary LLM (Codex) can produce large-scale CoT rationales, enabling CoT fine-tuning of smaller open models; ordering the label before the rationale in demonstrations improved rationale quality; filtering is necessary to remove degenerate/code outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Codex sometimes degenerated into writing code; arithmetic rationales were sometimes low quality and required filtering; reliance on proprietary models raises reproducibility concerns (Codex support ended).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3320.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3320.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive language model used as a zero-/few-shot baseline in the paper comparisons; provides context for how CoT fine-tuned smaller models compare to much larger, proprietary LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's autoregressive model (175B parameters) used in zero-shot and few-shot baselines; evaluated in BBH and other benchmarks as a point of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['In-context learning (ICL)', 'CoT prompting (where applicable)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-3 is evaluated in zero-shot and few-shot settings, sometimes with standard prompts; CoT prompting historically shown to help larger LMs, and GPT-3 appears in baseline tables of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-family (ICL / prompting) in this paper's comparisons — CoT-style prompting can be applied to GPT-3 but the paper uses it as a baseline rather than extensively experimenting with multiple methods on GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BBH, P3 Eval, MGSM (reference baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See CoT-T5 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported in baseline tables: GPT-3 total avg (Table 1 / Table 3 contexts) shows middling performance (e.g., BBH total avg numbers shown in tables: 38.30 in one column), used to contextualize CoT-T5 improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT-T5 (smaller) outperforms or rivals GPT-3 on some benchmarks after CoT fine-tuning (e.g., T5 + CoT FT outperformed GPT-3 in some settings in Table 1), demonstrating that instruction/CoT fine-tuning of smaller models can partially close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LMs like GPT-3 remain strong but the paper shows smaller LMs fine-tuned with diverse CoT data can outperform or match GPT-3 on some reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Noted that GPT-3 sometimes performs worse than CoT-fine-tuned smaller models on CoT evaluation in this paper's reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3320.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3320.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT / Claude (ICL baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI) and Claude (Anthropic) used as in-context learning baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary chat LMs used as ICL baselines for few-shot domain adaptation experiments; compared against smaller fine-tuned models using CoT rationales and LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT; Claude</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary instruction-following/chat LMs (ChatGPT by OpenAI, Claude by Anthropic) used with demonstrations up to maximum context length (ICL) or with CoT prompting to provide baselines for few-shot domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['In-context learning (append demonstrations)', 'CoT prompting (use of rationales/demonstrations)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ICL: append the available demonstrations (up to context length) and rely on model to generalize; CoT prompting: include chain-of-thought demonstrations where applicable. In the paper, demonstrations for ICL are limited by long input lengths in domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style usage (ICL with or without CoT demos); the models themselves are not re-trained in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Few-shot domain-specific adaptation (LEDGAR, CaseHold, MedNLI, PubMedQA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Domain-specific classification/QA tasks (64 training examples sampled) where provisioning demonstrations for ICL is constrained by long input lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ChatGPT + ICL total avg = 54.98%; ChatGPT + CoT prompting total avg = 39.70% (CoT prompting degraded ChatGPT on these long domain tasks). Claude + ICL total avg = 60.85%; Claude + CoT prompting total avg = 51.74%. CoT-T5 + LoRA CoT FT outperformed ChatGPT/Claude ICL baselines in these few-shot settings (CoT-T5-3B + LoRA CoT FT = 64.02%). The paper reports CoT-T5 outperforming ChatGPT by +13.98% (abstract claim) in some comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>ICL with chat models underperforms CoT fine-tuned smaller models in the few-shot long-input domain tasks because demonstrations cannot all be appended; CoT prompting as used with chat models sometimes performed worse than plain ICL in these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning (CoT FT + LoRA) on a small model tailored with CoT rationales can beat large proprietary chat models using ICL when context/demonstration capacity is limited (long inputs). CoT prompting on chat models is not guaranteed to help and can even hurt on long, domain-specific inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT prompting reduced performance for ChatGPT and Claude on the few-shot domain tasks (e.g., ChatGPT CoT PT 39.70% vs ChatGPT ICL 54.98%), indicating that applying a single prompting trick to large chat LMs is not universally beneficial, particularly in long-input regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3320.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3320.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mT0 / mT5 (multilingual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mT0-3.7B and mT5-3.7B (multilingual instruction-tuned / pretrained models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multilingual models evaluated for multilingual CoT adaptation: the paper fine-tunes mT0 and mT5 on translated subsets of CoT COLLECTION to show CoT reasoning can be taught in other languages with limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>mT0-3.7B; mT5-3.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>mT5: multilingual T5 pretrained on many languages; mT0: multilingual instruction-tuned variant. Both used at ~3.7B size and CoT fine-tuned on translated CoT data (60k-80k instances per target language) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈3.7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['CoT fine-tuning on translated rationales', 'Zero-shot CoT evaluation in target languages']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Translate a subset of CoT COLLECTION into target languages (Korean, Russian, French, Chinese, Japanese) and CoT fine-tune mT0/mT5 on that single-language data to teach step-by-step reasoning in that language.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single method (CoT) applied cross-lingually; the experiment tests language adaptation rather than mixing reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MGSM (multilingual grade-school math subset of 5 languages)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MGSM contains math/logic reasoning problems across many languages; the subset and CoT FT test step-by-step math reasoning in underrepresented languages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Baseline smaller models often near zero on low-resource languages (e.g., Flan-T5-3B: ko=0.0, zh=0.0, ja=0.0). After CoT FT: MT5-3.7B + CoT FT yields non-trivial gains (e.g., ko 3.2%, zh 6.0%, ja 7.6%); MT0-3.7B + CoT FT yields larger gains (e.g., ko 7.6%, ru 10.4%, fr 15.6%, zh 11.2%, ja 11.0%). Authors report relative improvements of 2x–10x on average across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Multilingual CoT FT on a single language (60k–80k instances) substantially improves low-shot reasoning in that language compared to non-CoT baselines; however, the paper did not explore cross-lingual CoT transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small multilingual LMs can be taught to do CoT reasoning in specific target languages with a modest amount of translated CoT data; CoT FT substantially raises near-zero baselines in underrepresented languages.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Training on only one language per model; cross-lingual transfer of CoT capabilities not demonstrated. Gains depend on language and baseline — some languages still have low absolute accuracy despite improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3320.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3320.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling / Task-Diversity Finding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Importance of task diversity versus instance count in CoT fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis (ablation) showing that breadth of tasks in CoT fine-tuning matters more than sheer number of CoT instances per task for improving generalization on reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT fine-tuned models (Flan-T5 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Controlled experiments: sample different numbers of instances and tasks from CoT COLLECTION (e.g., 10K instances across 1,060 tasks; 100K across same; compare to 180K instances across only 9 CoT tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applies to 3B and 11B Flan-T5 variants in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought fine-tuning (with varying dataset breadth and size)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Vary the number of distinct tasks and total instances used for CoT fine-tuning to measure BBH performance: small instance budget spread across many tasks vs large instance budget concentrated in few tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>This finding concerns data diversity rather than method diversity: the reasoning method (CoT) is fixed while training data task diversity varies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>BBH (Big-Bench Hard) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>BBH assesses zero-shot reasoning generalization over diverse challenges; used here to quantify gains from dataset design.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Result: 10K instances across 1,060 tasks outperformed 180K instances across 9 tasks on BBH CoT evaluation (Figure 3), demonstrating that wider task coverage yields better zero-shot generalization than many examples of a few tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct ablation comparing task breadth vs instance count — confirms task diversity is more impactful for CoT generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For CoT fine-tuning, training on many different tasks (diversity) is more effective than training on many more instances of a smaller number of CoT tasks; this supports building CoT datasets with broad task coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No counter-example inside paper; authors note this was observed under their training setup and encourage further study on optimal task-vs-instance tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>The Flan Collection: Designing data and methods for effective instruction tuning <em>(Rating: 2)</em></li>
                <li>Scaling instruction-finetuned language models <em>(Rating: 2)</em></li>
                <li>Specializing smaller language models towards multi-step reasoning <em>(Rating: 1)</em></li>
                <li>Orca: Progressive learning from complex explanation traces of gpt-4 <em>(Rating: 1)</em></li>
                <li>Mammoth: Building math generalist models through hybrid instruction tuning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3320",
    "paper_id": "paper-0744580e75a74357e466a57082c85cb42f548aa9",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "CoT-T5",
            "name_full": "CoT-T5 (Flan-T5 fine-tuned with CoT COLLECTION)",
            "brief_description": "A Flan-T5 model (3B and 11B variants) fine-tuned on the paper's CoT COLLECTION to produce chain-of-thought (step-by-step) rationales before answers, improving zero-shot and few-shot reasoning compared to the Flan-T5 baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CoT-T5",
            "model_description": "Flan-T5 (instruction-tuned T5) additionally fine-tuned on 1.84M machine-augmented rationales across 1,060 tasks (the CoT COLLECTION); trained to emit a rationale r before the answer y, using the trigger 'Let's think step by step' during training and eval.",
            "model_size": "3B, 11B",
            "reasoning_methods": [
                "Chain-of-Thought (CoT) prompting",
                "Chain-of-Thought fine-tuning (CoT FT)",
                "Few-shot adaptation with LoRA (parameter-efficient fine-tuning)",
                "Direct evaluation (no rationale)"
            ],
            "reasoning_methods_description": "CoT prompting: generate a step-by-step rationale before the final answer using the phrase 'Let's think step by step'. CoT fine-tuning: supervised fine-tuning to generate rationale+answer sequences from the CoT COLLECTION. LoRA: low-rank adapters applied during few-shot fine-tuning to adapt while preserving CoT ability. Direct evaluation: standard label-verbalizer classification without intermediate rationales.",
            "diversity_of_methods": "Similar style focused (Chain-of-Thought across many tasks) — the paper applies a single primary reasoning style (CoT) but across a diverse set of tasks and languages; diversity comes from the task distribution of training data rather than mixing qualitatively different reasoning styles.",
            "reasoning_task_name": "BIG-Bench-Hard (BBH), P3 Eval (subset), MGSM (subset), domain-specific few-shot tasks (LEDGAR, CaseHold, MedNLI, PubMedQA)",
            "reasoning_task_description": "BBH: a set of challenging zero-shot reasoning tasks; P3 Eval: evaluation subset from instruction tuning benchmarks; MGSM: multilingual grade-school math benchmark; domain-specific tasks: legal and medical classification/QA datasets used in few-shot adaptation.",
            "performance_by_method": "Zero-shot BBH (CoT evaluation) improvement over Flan-T5: +4.34% (CoT-T5-3B) and +2.60% (CoT-T5-11B) average accuracy. In few-shot domain tasks (64-shot adaptation with LoRA): CoT-T5-3B + LoRA CoT FT average = 64.02% vs Flan-T5-3B + LoRA FT = 60.19% (delta +3.83%). CoT-T5-11B + LoRA CoT FT average = 68.96% vs Flan-T5-11B + LoRA FT = 66.59% (delta +2.37%). In P3 (T0 setup) CoT FT on T0-3B subset improved T0-3B by +8.65% average on 11 P3 datasets. Multilingual (MGSM subset): mT0/mT5 fine-tuned with CoT data produced 2x–10x relative improvements on average across five languages compared to non-CoT baselines (table reports concrete improvements per language; e.g., MT0-3.7B + CoT FT yields 7.6% for Korean vs MT0 baseline 0.0%).",
            "comparison_of_methods": "Explicit comparisons: (1) CoT evaluation vs Direct evaluation — CoT-T5 improves CoT-based accuracy substantially while maintaining or improving Direct eval in some settings (CoT-T5-11B improved Direct eval). (2) CoT fine-tuning on many tasks (1,060) vs few CoT tasks (9) or vs more instances on few tasks: training across many tasks (even 10K instances over 1,060 tasks) outperforms many more instances concentrated in only 9 CoT tasks (180K instances), showing breadth of task diversity matters more than raw instance count. (3) In few-shot adaptation, LoRA applied to CoT-T5 preserves and leverages CoT ability better than full fine-tuning in that setting. (4) CoT-T5 few-shot performance surpasses ICL baselines using larger proprietary models (e.g., ChatGPT/Claude) when demonstrations are limited by context length.",
            "key_findings": "Fine-tuning smaller LMs on a large, diverse set of chain-of-thought rationales enables them to perform step-by-step reasoning in zero-shot and few-shot settings; breadth of task diversity in CoT fine-tuning is more important than the sheer number of instances per task; LoRA parameter-efficient fine-tuning preserves CoT ability for few-shot adaptation and yields strong performance; CoT fine-tuning can close some of the gap between smaller LMs and much larger LMs on reasoning benchmarks.",
            "counter_examples_or_negative_results": "CoT fine-tuning sometimes slightly degrades Direct evaluation (e.g., CoT-T5-3B had a 0.96% Direct-eval degradation while CoT-eval improved). For some task types (sentiment, coreference, WSD) CoT rationales generated by large LMs can be short/uninformative and were excluded; arithmetic reasoning rationales from the teacher LLM (Codex) sometimes degenerated and required filtering. Also, for Flan-T5, regular full fine-tuning sometimes outperforms applying LoRA; CoT fine-tuning didn't uniformly help every base model without adaptation (i.e., benefits depend on base model and fine-tuning recipe).",
            "uuid": "e3320.0",
            "source_info": {
                "paper_title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Flan-T5",
            "name_full": "Flan-T5 (instruction-tuned T5)",
            "brief_description": "An instruction-tuned variant of T5 used as the base model in experiments; served as the main baseline to evaluate the impact of CoT fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5",
            "model_description": "T5 family model instruction-tuned on a collection of instruction tasks (Flan/FLAN Collection). Evaluated in 3B and 11B sizes in this paper as the baseline before additional CoT fine-tuning.",
            "model_size": "3B, 11B",
            "reasoning_methods": [
                "Direct evaluation (no rationale)",
                "CoT prompting (at eval; often fails for smaller LMs without CoT fine-tuning)",
                "Instruction tuning (pre-existing)"
            ],
            "reasoning_methods_description": "Direct evaluation: use label verbalizers to map LM outputs to options. CoT prompting: append 'Let's think step by step' to elicit rationales — smaller Flan-T5 often fails to produce adequate rationales without being CoT fine-tuned.",
            "diversity_of_methods": "Single primary style in baseline (instruction tuned) — the paper contrasts this with CoT fine-tuned variants rather than mixing multiple reasoning styles.",
            "reasoning_task_name": "BBH, P3 Eval, MGSM, domain-specific few-shot tasks",
            "reasoning_task_description": "Same benchmarks used across the paper for comparison (see CoT-T5 entry).",
            "performance_by_method": "Baseline metrics: Flan-T5-3B total avg (Table 1) ≈ 35.60; Flan-T5-11B total avg ≈ 39.78. Under CoT evaluation, CoT-T5 improved above these baselines by +4.34% (3B) and +2.60% (11B) on BBH average accuracy.",
            "comparison_of_methods": "Compared against CoT-T5: Flan-T5 without CoT fine-tuning performs worse on CoT evaluation and often fails to generate rationales with the trigger phrase; regular fine-tuning on a target few-shot task sometimes outperform CoT fine-tuning for Flan-T5, indicating interactions between base model and CoT recipe.",
            "key_findings": "Flan-T5 benefits from CoT fine-tuning but the gains depend on model size and fine-tuning recipe; simply prompting Flan-T5 with CoT triggers is often insufficient.",
            "counter_examples_or_negative_results": "Flan-T5 when CoT fine-tuned (naively) can underperform full fine-tuning on some few-shot tasks; Flan-T5 often generates no rationales even with the CoT trigger phrase, motivating CoT fine-tuning.",
            "uuid": "e3320.1",
            "source_info": {
                "paper_title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Codex (teacher)",
            "name_full": "OpenAI Codex (used as teacher for rationale augmentation)",
            "brief_description": "A proprietary OpenAI model (Codex) used by the authors to generate chain-of-thought rationales (machine-augmented) for the CoT COLLECTION; outputs were filtered to ensure quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Codex (OpenAI)",
            "model_description": "A family of OpenAI transformer models adapted for code generation and language tasks; used here with in-context learning prompts (6–8 demonstrations per task family) to generate rationales conditioned on ground-truth labels.",
            "model_size": null,
            "reasoning_methods": [
                "In-context learning (ICL) to produce Chain-of-Thought rationales",
                "Prompt engineering (label-first demonstrations)"
            ],
            "reasoning_methods_description": "The authors use task-family demonstrations to prompt Codex to output rationale r conditioned on instance and gold label, sometimes ordering label before rationale to improve quality. Generated multiple candidate rationales and applied filtering (presence of answer token, max length, de-duplication, code filter).",
            "diversity_of_methods": "Single method (ICL-based CoT generation) but applied across a highly diverse set of 1,060 tasks to create varied rationales.",
            "reasoning_task_name": "Rationale augmentation across Flan Collection tasks (1,060 tasks)",
            "reasoning_task_description": "Codex was used to generate CoT rationales for inputs/labels sampled from a wide set of instruction tasks (P3, FLAN, SNI etc.).",
            "performance_by_method": "Not an evaluation benchmark result, but authors report Codex produced sufficiently high-quality rationales after filtering; they also compared rationale quality from Codex vs Bard and Claude (Appendix A) and report comparable ROSCOE scores.",
            "comparison_of_methods": "Authors note reproducibility concerns (Codex decommissioned) and performed limited comparisons with Bard and Claude to show other LLMs can generate usable rationales; also mention later work used GPT-4 to augment even more rationales.",
            "key_findings": "ICL with a strong proprietary LLM (Codex) can produce large-scale CoT rationales, enabling CoT fine-tuning of smaller open models; ordering the label before the rationale in demonstrations improved rationale quality; filtering is necessary to remove degenerate/code outputs.",
            "counter_examples_or_negative_results": "Codex sometimes degenerated into writing code; arithmetic rationales were sometimes low quality and required filtering; reliance on proprietary models raises reproducibility concerns (Codex support ended).",
            "uuid": "e3320.2",
            "source_info": {
                "paper_title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3 (baseline)",
            "name_full": "GPT-3 (175B)",
            "brief_description": "A large autoregressive language model used as a zero-/few-shot baseline in the paper comparisons; provides context for how CoT fine-tuned smaller models compare to much larger, proprietary LMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "OpenAI's autoregressive model (175B parameters) used in zero-shot and few-shot baselines; evaluated in BBH and other benchmarks as a point of comparison.",
            "model_size": "175B",
            "reasoning_methods": [
                "In-context learning (ICL)",
                "CoT prompting (where applicable)"
            ],
            "reasoning_methods_description": "GPT-3 is evaluated in zero-shot and few-shot settings, sometimes with standard prompts; CoT prompting historically shown to help larger LMs, and GPT-3 appears in baseline tables of the paper.",
            "diversity_of_methods": "Single-family (ICL / prompting) in this paper's comparisons — CoT-style prompting can be applied to GPT-3 but the paper uses it as a baseline rather than extensively experimenting with multiple methods on GPT-3.",
            "reasoning_task_name": "BBH, P3 Eval, MGSM (reference baselines)",
            "reasoning_task_description": "See CoT-T5 entry.",
            "performance_by_method": "Reported in baseline tables: GPT-3 total avg (Table 1 / Table 3 contexts) shows middling performance (e.g., BBH total avg numbers shown in tables: 38.30 in one column), used to contextualize CoT-T5 improvements.",
            "comparison_of_methods": "CoT-T5 (smaller) outperforms or rivals GPT-3 on some benchmarks after CoT fine-tuning (e.g., T5 + CoT FT outperformed GPT-3 in some settings in Table 1), demonstrating that instruction/CoT fine-tuning of smaller models can partially close the gap.",
            "key_findings": "Large LMs like GPT-3 remain strong but the paper shows smaller LMs fine-tuned with diverse CoT data can outperform or match GPT-3 on some reasoning benchmarks.",
            "counter_examples_or_negative_results": "Noted that GPT-3 sometimes performs worse than CoT-fine-tuned smaller models on CoT evaluation in this paper's reported experiments.",
            "uuid": "e3320.3",
            "source_info": {
                "paper_title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT / Claude (ICL baselines)",
            "name_full": "ChatGPT (OpenAI) and Claude (Anthropic) used as in-context learning baselines",
            "brief_description": "Proprietary chat LMs used as ICL baselines for few-shot domain adaptation experiments; compared against smaller fine-tuned models using CoT rationales and LoRA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT; Claude",
            "model_description": "Proprietary instruction-following/chat LMs (ChatGPT by OpenAI, Claude by Anthropic) used with demonstrations up to maximum context length (ICL) or with CoT prompting to provide baselines for few-shot domain tasks.",
            "model_size": null,
            "reasoning_methods": [
                "In-context learning (append demonstrations)",
                "CoT prompting (use of rationales/demonstrations)"
            ],
            "reasoning_methods_description": "ICL: append the available demonstrations (up to context length) and rely on model to generalize; CoT prompting: include chain-of-thought demonstrations where applicable. In the paper, demonstrations for ICL are limited by long input lengths in domain tasks.",
            "diversity_of_methods": "Single-style usage (ICL with or without CoT demos); the models themselves are not re-trained in this study.",
            "reasoning_task_name": "Few-shot domain-specific adaptation (LEDGAR, CaseHold, MedNLI, PubMedQA)",
            "reasoning_task_description": "Domain-specific classification/QA tasks (64 training examples sampled) where provisioning demonstrations for ICL is constrained by long input lengths.",
            "performance_by_method": "ChatGPT + ICL total avg = 54.98%; ChatGPT + CoT prompting total avg = 39.70% (CoT prompting degraded ChatGPT on these long domain tasks). Claude + ICL total avg = 60.85%; Claude + CoT prompting total avg = 51.74%. CoT-T5 + LoRA CoT FT outperformed ChatGPT/Claude ICL baselines in these few-shot settings (CoT-T5-3B + LoRA CoT FT = 64.02%). The paper reports CoT-T5 outperforming ChatGPT by +13.98% (abstract claim) in some comparisons.",
            "comparison_of_methods": "ICL with chat models underperforms CoT fine-tuned smaller models in the few-shot long-input domain tasks because demonstrations cannot all be appended; CoT prompting as used with chat models sometimes performed worse than plain ICL in these datasets.",
            "key_findings": "Fine-tuning (CoT FT + LoRA) on a small model tailored with CoT rationales can beat large proprietary chat models using ICL when context/demonstration capacity is limited (long inputs). CoT prompting on chat models is not guaranteed to help and can even hurt on long, domain-specific inputs.",
            "counter_examples_or_negative_results": "CoT prompting reduced performance for ChatGPT and Claude on the few-shot domain tasks (e.g., ChatGPT CoT PT 39.70% vs ChatGPT ICL 54.98%), indicating that applying a single prompting trick to large chat LMs is not universally beneficial, particularly in long-input regimes.",
            "uuid": "e3320.4",
            "source_info": {
                "paper_title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "mT0 / mT5 (multilingual)",
            "name_full": "mT0-3.7B and mT5-3.7B (multilingual instruction-tuned / pretrained models)",
            "brief_description": "Multilingual models evaluated for multilingual CoT adaptation: the paper fine-tunes mT0 and mT5 on translated subsets of CoT COLLECTION to show CoT reasoning can be taught in other languages with limited data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "mT0-3.7B; mT5-3.7B",
            "model_description": "mT5: multilingual T5 pretrained on many languages; mT0: multilingual instruction-tuned variant. Both used at ~3.7B size and CoT fine-tuned on translated CoT data (60k-80k instances per target language) in experiments.",
            "model_size": "≈3.7B",
            "reasoning_methods": [
                "CoT fine-tuning on translated rationales",
                "Zero-shot CoT evaluation in target languages"
            ],
            "reasoning_methods_description": "Translate a subset of CoT COLLECTION into target languages (Korean, Russian, French, Chinese, Japanese) and CoT fine-tune mT0/mT5 on that single-language data to teach step-by-step reasoning in that language.",
            "diversity_of_methods": "Single method (CoT) applied cross-lingually; the experiment tests language adaptation rather than mixing reasoning styles.",
            "reasoning_task_name": "MGSM (multilingual grade-school math subset of 5 languages)",
            "reasoning_task_description": "MGSM contains math/logic reasoning problems across many languages; the subset and CoT FT test step-by-step math reasoning in underrepresented languages.",
            "performance_by_method": "Baseline smaller models often near zero on low-resource languages (e.g., Flan-T5-3B: ko=0.0, zh=0.0, ja=0.0). After CoT FT: MT5-3.7B + CoT FT yields non-trivial gains (e.g., ko 3.2%, zh 6.0%, ja 7.6%); MT0-3.7B + CoT FT yields larger gains (e.g., ko 7.6%, ru 10.4%, fr 15.6%, zh 11.2%, ja 11.0%). Authors report relative improvements of 2x–10x on average across languages.",
            "comparison_of_methods": "Multilingual CoT FT on a single language (60k–80k instances) substantially improves low-shot reasoning in that language compared to non-CoT baselines; however, the paper did not explore cross-lingual CoT transfer.",
            "key_findings": "Small multilingual LMs can be taught to do CoT reasoning in specific target languages with a modest amount of translated CoT data; CoT FT substantially raises near-zero baselines in underrepresented languages.",
            "counter_examples_or_negative_results": "Training on only one language per model; cross-lingual transfer of CoT capabilities not demonstrated. Gains depend on language and baseline — some languages still have low absolute accuracy despite improvements.",
            "uuid": "e3320.5",
            "source_info": {
                "paper_title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Scaling / Task-Diversity Finding",
            "name_full": "Importance of task diversity versus instance count in CoT fine-tuning",
            "brief_description": "An analysis (ablation) showing that breadth of tasks in CoT fine-tuning matters more than sheer number of CoT instances per task for improving generalization on reasoning benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CoT fine-tuned models (Flan-T5 experiments)",
            "model_description": "Controlled experiments: sample different numbers of instances and tasks from CoT COLLECTION (e.g., 10K instances across 1,060 tasks; 100K across same; compare to 180K instances across only 9 CoT tasks).",
            "model_size": "applies to 3B and 11B Flan-T5 variants in experiments",
            "reasoning_methods": [
                "Chain-of-Thought fine-tuning (with varying dataset breadth and size)"
            ],
            "reasoning_methods_description": "Vary the number of distinct tasks and total instances used for CoT fine-tuning to measure BBH performance: small instance budget spread across many tasks vs large instance budget concentrated in few tasks.",
            "diversity_of_methods": "This finding concerns data diversity rather than method diversity: the reasoning method (CoT) is fixed while training data task diversity varies.",
            "reasoning_task_name": "BBH (Big-Bench Hard) evaluation",
            "reasoning_task_description": "BBH assesses zero-shot reasoning generalization over diverse challenges; used here to quantify gains from dataset design.",
            "performance_by_method": "Result: 10K instances across 1,060 tasks outperformed 180K instances across 9 tasks on BBH CoT evaluation (Figure 3), demonstrating that wider task coverage yields better zero-shot generalization than many examples of a few tasks.",
            "comparison_of_methods": "Direct ablation comparing task breadth vs instance count — confirms task diversity is more impactful for CoT generalization.",
            "key_findings": "For CoT fine-tuning, training on many different tasks (diversity) is more effective than training on many more instances of a smaller number of CoT tasks; this supports building CoT datasets with broad task coverage.",
            "counter_examples_or_negative_results": "No counter-example inside paper; authors note this was observed under their training setup and encourage further study on optimal task-vs-instance tradeoffs.",
            "uuid": "e3320.6",
            "source_info": {
                "paper_title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "The Flan Collection: Designing data and methods for effective instruction tuning",
            "rating": 2
        },
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 2
        },
        {
            "paper_title": "Specializing smaller language models towards multi-step reasoning",
            "rating": 1
        },
        {
            "paper_title": "Orca: Progressive learning from complex explanation traces of gpt-4",
            "rating": 1
        },
        {
            "paper_title": "Mammoth: Building math generalist models through hybrid instruction tuning",
            "rating": 1
        }
    ],
    "cost": 0.02206975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The CoT COLLECTION: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning</h1>
<p>Seungone Kim ${ }^{1,2 * \dagger}$ Se June Joo ${ }^{1 * \dagger}$ Doyoung Kim ${ }^{1}$ Joel Jang ${ }^{1,3}$<br>Seonghyeon $\mathrm{Ye}^{1}$ Jamin Shin ${ }^{2 \dagger}$ Minjoon Seo ${ }^{1 \dagger}$<br>KAIST AI ${ }^{1}$ NAVER AI Lab ${ }^{2}$ University of Washington ${ }^{3}$<br>{seungone, sejune, minjoon}@kaist.ac.kr jayshin.nlp@gmail.com</p>
<h4>Abstract</h4>
<p>Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instructiontuning dataset called the CoT COLLECTION, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B \&amp; 11B) with CoT COLLECTION enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of $+4.34 \%$ (Flan-T5 3B) and $+2.60 \%$ (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with COT COLLECTION allows LMs to possess stronger fewshot learning capabilities on 4 domain-specific tasks, resulting in an improvement of $+2.24 \%$ (Flan-T5 3B) and $+2.37 \%$ (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a $+13.98 \%$ margin. Our code, the CoT COLLECTION data, and model checkpoints are publicly available ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Language models (LMs) pre-trained on massive text corpora can adapt to downstream tasks in both zero-shot and few-shot learning settings by incorporating task instructions and demonstrations (Brown et al., 2020; Wei et al., 2021; Sanh et al., 2021; Mishra et al., 2022; Wang et al., 2022b; Iyer et al., 2022; Liu et al., 2022b; Chung et al., 2022; Longpre et al., 2023; Ye et al., 2023). One approach that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>has been particularly effective in enabling LMs to excel at a multitude of tasks is Chain-of-Thought (CoT) prompting, making LMs generate a rationale to derive its final prediction in a sequential manner (Wei et al., 2022b; Kojima et al., 2022; Zhou et al., 2022; Zhang et al., 2022; Yao et al., 2023).</p>
<p>While CoT prompting works effectively for large LMs with more than 100 billion parameters, it does not necessarily confer the same benefits to smaller LMs (Tay et al., 2022; Suzgun et al., 2022; Wei et al., 2022a; Chung et al., 2022). The requirement of a large number of parameters consequently results in significant computational cost and accessibility issues (Kaplan et al., 2020; Min et al., 2022; Liu et al., 2022b; Mhlanga, 2023; Li et al., 2023).</p>
<p>Recent work has focused on empowering relatively smaller LMs to effectively solve novel tasks as well, primarily through fine-tuning with rationales (denoted as CoT fine-tuning) and applying CoT prompting on a single target task (Shridhar et al., 2022; Ho et al., 2022; Fu et al., 2023). However, solving a single task does not adequately address the issue of generalization to a broad range of unseen tasks. While Chung et al. (2022) leverage 9 publicly available CoT tasks during instruction tuning to solve multiple unseen tasks, the imbalanced ratio compared to 1,827 tasks used for direct finetuning results in poor CoT results across smaller LMs (Longpre et al., 2023). In general, the community still lacks a comprehensive strategy to fully leverage CoT prompting to solve multiple unseen novel tasks in the context of smaller LMs.</p>
<p>To bridge this gap, we present the CoT COLLECTION, an instruction tuning dataset that augments 1.84 million rationales from the FLAN Collection (Longpre et al., 2023) across 1,060 tasks. We fine-tune Flan-T5 (3B \&amp; 11B) using CoT COLLECTION and denote the resulting model as CoTT5. We perform extensive comparisons of CoT-T5 and Flan-T5 under two main scenarios: (1) zeroshot learning and (2) few-shot learning.</p>
<p>In the zero-shot learning setting, CoT-T5 (3B \&amp; 11B) outperforms Flan-T5 (3B \&amp; 11B) by $+4.34 \%$ and $+2.60 \%$ on average accuracy across 27 datasets from the Big Bench Hard (BBH) benchmark (Suzgun et al., 2022) when evaluated with CoT prompting. During ablation experiments, we show that CoT fine-tuning T0 (3B) (Sanh et al., 2021) on a subset of the CoT Collection, specifically 163 training tasks used in T0, shows a performance increase of $+8.65 \%$ on average accuracy across 11 datasets from the P3 Evaluation benchmark. Moreover, we translate 80K instances of CoT COLLECTION into 5 different languages (French, Japanese, Korean, Russian, Chinese) and observe that CoT fine-tuning mT0 (3B) (Muennighoff et al., 2022) on each language results in $2 \mathrm{x} \sim 10 \mathrm{x}$ performance improvement on average accuracy across all 5 languages from the MGSM benchmark (Shi et al., 2022).</p>
<p>In the few-shot learning setting, where LMs must adapt to new tasks with a minimal number of instances, CoT-T5 (3B \&amp; 11B) exhibits a $+2.24 \%$ and $+2.37 \%$ improvement on average compared to using Flan-T5 (3B \&amp; 11B) as the base model on 4 different domain-specific tasks ${ }^{2}$. Moreover, it demonstrates $+13.98 \%$ and $+8.11 \%$ improvement over ChatGPT (OpenAI, 2022) and Claude (Anthropic, 2023) that leverages ICL with demonstrations up to the maximum input length.</p>
<p>Our contributions are summarized as follows:</p>
<ul>
<li>We introduce CoT COLLECTION, a new instruction dataset that includes 1.84 million rationales across 1,060 tasks that could be used for applying CoT fine-tuning to LMs.</li>
<li>With CoT Collection, we fine-tune FlanT5, denoted as CoT-T5, which shows a nontrivial boost in zero-shot and few-shot learning capabilities with CoT Prompting.</li>
<li>For ablations, we show that CoT fine-tuning could improve the CoT capabilities of LMs in low-compute settings by using a subset of CoT COLLECTION and training on (1) smaller number of tasks (T0 setting; 163 tasks) and (2) smaller amount of instances in 5 different languages (French, Japanese, Korean, Russian, Chinese; 80K instances).</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>2 Related Works</h2>
<h3>2.1 Chain-of-Thought (CoT) Prompting</h3>
<p>Wei et al. (2022b) propose Chain of Thought (CoT) Prompting, a technique that triggers the model to generate a rationale before the answer. By generating a rationale, large LMs show improved reasoning abilities when solving challenging tasks. Kojima et al. (2022) show that by appending the phrase 'Let's think step by step', large LMs could perform CoT prompting in a zero-shot setting. Different work propose variants of CoT prompting such as automatically composing CoT demonstrations (Zhang et al., 2022) and performing a finegrained search through multiple rationale candidates with a tree search algorithm (Yao et al., 2023). While large LMs could solve novel tasks with CoT Prompting, Chung et al. (2022) and Longpre et al. (2023) show that this effectiveness does not necessarily hold for smaller LMs. In this work, we aim to equip smaller LMs with the same capabilities by instruction tuning on large amount of rationales.</p>
<h3>2.2 Improving Zero-shot Generalization</h3>
<p>Previous work show that instruction tuning enables generalization to multiple unseen tasks (Wei et al., 2021; Sanh et al., 2021; Aribandi et al., 2021; Ouyang et al., 2022; Wang et al., 2022b; Xu et al., 2022). Different work propose to improve instruction tuning by enabling cross-lingual generalization (Muennighoff et al., 2022), improving label generalization capability (Ye et al., 2022), and training modular, expert LMs (Jang et al., 2023). Meanwhile, a line of work shows that CoT fine-tuning could improve the reasoning abilities of LMs on a single-seen task (Zelikman et al., 2022; Shridhar et al., 2022; Ho et al., 2022; Fu et al., 2023). As a follow-up study, we CoT fine-tune on 1,060 instruction tasks and observe a significant improvement in terms of zero-shot generalization on multiple tasks.</p>
<h3>2.3 Improving Few-Shot Learning</h3>
<p>For adapting LMs to new tasks with a few instances, recent work propose advanced parameter efficient fine-tuning (PEFT) methods, where a small number of trainable parameters are added (Hu et al., 2021; Lester et al., 2021; Liu et al., 2021, 2022b; Asai et al., 2022; Liu et al., 2022c). In this work, we show that a simple recipe of (1) applying LoRA (Hu et al., 2021) to a LM capable of performing CoT reasoning and (2) CoT fine-tuning on a target task results in strong few-shot performance.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of the overall task group and dataset source of where we obtained the instances to augment the rationales in CoT COLLECTION. Compared to the 9 datasets that provide publicly available rationales (included within 'Flan-T5 ExQA', 'Flan-T5 Arithmetic', 'Flan-T5 MCQA', 'Flan-T5 NLI' from the red box), we generate ~51.29 times more rationales (1.84 million rationales) and ~117.78 times more task variants (1,060 tasks).</p>
<h2>3 The CoT Collection</h2>
<p>Despite its effectiveness for CoT fine-tuning, rationales data still remains scarce. To the best of our knowledge, recent work mostly rely on 9 publicly available NLP datasets<sup>3</sup> for fine-tuning with rationales (Zelikman et al., 2022; Shridhar et al., 2022; Chung et al., 2022; Ho et al., 2022; Longpre et al., 2023; Fu et al., 2023). This is due to the difficulty in gathering human-authored rationales (Kim et al., 2023). To this end, we create CoT COLLECTION, an instruction-tuning dataset that includes 1.84 million rationales augmented across 1,060 tasks<sup>4</sup>. In this section, we explain the datasets we select to augment into rationales and how we perform the overall augmentation process.</p>
<h3>Broad Overview</h3>
<p>Given an input $X = [I, z]$ composed of an instruction $I$, and an instance $z$ along with the answer $y$, we obtain a rationale $r$ by applying in-context learning (ICL) with a large LM. Note that this differs from previous works which focused on generating new instances $z$ using large LMs (West et al., 2022; Liu et al., 2022a; Kim et al., 2022; Honovich et al., 2022; Wang et al., 2022a; Taori et al., 2023; Chiang et al., 2023) while we extend it to generating new rationales $r$.</p>
<h3>Source Dataset Selection</h3>
<p>As a source dataset to extract rationales, we choose the Flan Collection (Longpre et al., 2023), consisting of 1,836 diverse NLP tasks from P3 (Sanh et al., 2021), Super-NaturalInstructions (Wang et al., 2022b), Flan (Wei et al., 2021), and some additional dialogue &amp; code datasets. We choose 1,060 tasks, narrowing our focus following the criteria as follows:</p>
<ul>
<li><strong>Generation tasks</strong> with long outputs are excluded since the total token length of appending $r$ and $y$ exceeds the maximum output token length (512 tokens) during training.</li>
<li><strong>Datasets</strong> that are not publicly available such as DeepMind Coding Contents and Dr Repair (Yasunaga and Liang, 2020) are excluded.</li>
<li><strong>Datasets</strong> where the input and output do not correspond to each other in the huggingface datasets (Lhoest et al., 2021) are excluded.</li>
<li>When a dataset appears in common across different sources, we prioritize using the task from P3 first, followed by SNI, and Flan.</li>
<li>During preliminary experiments, we find that for tasks such as sentiment analysis, sentence</li>
</ul>
<p><sup>3</sup>The 9 available datasets are QASC (Khot et al., 2020), AQuA (Amini et al., 2019), GSM8K (Cobbe et al., 2021), QED (Lamm et al., 2021), StrategyQA (Geva et al., 2021), SenseMaking (Wang et al., 2019), CREAK (Onoe et al., 2021), e-SNLI (Camburu et al., 2018), ECQA (Aggarwal et al., 2021).</p>
<p><sup>4</sup>Following Sanh et al. (2021), we use the notion of 'task' referring to each prompt applied to a dataset.</p>
<p>completion, coreference resolution, and word disambiguation, rationales generated by large LMs are very short and uninformative. We exclude these tasks to prevent negative transfer during multitask learning <em>Aribandi et al. (2021); Jang et al. (2023)</em>.</p>
<h4>Creating Demonstrations for ICL</h4>
<p>We first create prompts to apply in-context learning (ICL) with large LMs for augmenting the instances in the selected tasks with rationales. Preparing demonstrations $\mathcal{D}^{t}$ for each task $t$ is the most straightforward, but it becomes infeasible to prepare demonstrations for each task as the number of tasks gets larger. Instead, we assign each task $t$ to $T_{k}$, a family of tasks that shares a similar task format such as multiple choice QA, closed book QA, and dialogue generation. Each family of tasks share $\mathcal{D}^{T_{k}}$, which consists of $6 \sim 8$ demonstrations. These $6 \sim 8$ demonstrations for each task group $T_{k}$ is manually created by 3 of the authors in this paper. Specifically, given 136 instances sampled from Flan Collection, two annotators are assigned to write a rationale, and the other third annotator conducts an A/B testing between the two options. We manually create $\mathcal{D}^{T_{k}}$ across $k=26$ task groups. We include the prompts for all of the different task groups in Appendix D.</p>
<h4>Rationale Augmentation</h4>
<p>We use the OpenAI Codex to augment rationales. Formally, given $(X_{i}^{t}, y_{i}^{t})$, the $i^{t h}$ instance of a task $t$, the goal is to generate corresponding rationale $r_{i}^{t}$. Note that during preliminary experiments, we found that ordering the label in front of the rationale within the demonstration $\mathcal{D}^{T_{k}}$ was crucial to generate good quality rationales. We conjecture this is because ordering the label in front of the rationale loosens the need for the large LM to solve the underlying task and only focus on generating a rationale. However, we also found that in some tasks such as arithmetic reasoning, large LMs fail to generate good-quality rationales. To mitigate this issue, we apply filtering to the augmented rationales. We provide the criteria used for the filtering phase and</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: MCQA Prompt used to augment rationales from P3 dataset. Through ICL, the large LM generates a rationale that is conditioned on the ground-truth label.
the filtered cases at Appendix B. Also, we include analysis of the diversity and quality of CoT COLLECTION compared to the existing 9 CoT tasks and human-authored rationales in Appendix A.</p>
<h2>4 Experiments</h2>
<p>For our main experiments, we use Flan-T5 <em>Chung et al. (2022)</em> as our base model, and obtain CoT-T5 by CoT fine-tuning on the CoT COLLECTION. Formally, given $X_{i}^{t}$, the goal of CoT fine-tuning is to sequentially generate the rationale $r_{i}^{t}$ and answer $y_{i}^{t}$. To indicate that $r_{i}^{t}$ should be generated before $y_{i}^{t}$, the trigger phrase 'Let's think step by step' is added during both training and evaluation. We mostly follow the details for training and evaluation from Chung et al. (2022), and provide additional details in Appendix C. In this section, we show how training on CoT COLLECTION enhances zero-shot generalization capabilities (Section 4.2) and few-shot adaptation capabilities (Section 4.3).</p>
<h3>4.1 Evaluation</h3>
<p>We evaluate under two different evaluation methods: Direct Evaluation and CoT Evaluation. For Direct Evaluation on classification tasks, we follow previous works using verbalizers, choosing the option with the highest probability through comparison of logit values <em>Schick and Schütze (2021)</em>.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>CoT</th>
<th>Direct</th>
<th>Total Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5-LM-3B</td>
<td>26.68</td>
<td>26.96</td>
<td>26.82</td>
</tr>
<tr>
<td>T0-3B</td>
<td>26.64</td>
<td>27.45</td>
<td>27.05</td>
</tr>
<tr>
<td>TK-INSTRUCT-3B</td>
<td>29.86</td>
<td>29.90</td>
<td>29.88</td>
</tr>
<tr>
<td>TK-INSTRUCT-11B</td>
<td>33.60</td>
<td>30.71</td>
<td>32.16</td>
</tr>
<tr>
<td>T0-11B</td>
<td>31.83</td>
<td>33.57</td>
<td>32.70</td>
</tr>
<tr>
<td>Flan-T5-3B</td>
<td>34.06</td>
<td>37.14</td>
<td>35.60</td>
</tr>
<tr>
<td>GPT-3 (175B)</td>
<td>38.30</td>
<td>33.60</td>
<td>38.30</td>
</tr>
<tr>
<td>Flan-T5-11B</td>
<td>38.57</td>
<td>40.99</td>
<td>39.78</td>
</tr>
<tr>
<td>T5-3B + CoT FT</td>
<td>37.95</td>
<td>35.52</td>
<td>36.74</td>
</tr>
<tr>
<td>CoT-T5-3B</td>
<td>38.40</td>
<td>36.18</td>
<td>37.29</td>
</tr>
<tr>
<td>T5-11B + CoT FT</td>
<td>40.02</td>
<td>38.76</td>
<td>39.54</td>
</tr>
<tr>
<td>CoT-T5-11B</td>
<td>42.20</td>
<td>42.56</td>
<td>42.38</td>
</tr>
</tbody>
</table>
<p>Sanh et al., 2021; Ye et al., 2022; Jang et al., 2023), and measure the accuracy. For generation tasks, we directly compare the LM's prediction with the answer and measure the EM score.</p>
<p>When evaluating with CoT Evaluation, smaller LMs including Flan-T5 often do not generate any rationales even with the trigger phrase 'Let's think step by step'. Therefore, we adopt a hard constraint of requiring the LM to generate $r_{i}^{t}$ with at least a minimum length of 8 tokens. In classification tasks, we divide into two steps where the LM first generates $r_{i}^{t}$, and then verbalizers are applied with a indicator phrase ‘[ANSWER]’ inserted between $r_{i}^{t}$ and the possible options. For generation tasks, we extract the output coming after the indicator phrase. Accuracy metric is used for classification tasks while EM metric is used for generation tasks.</p>
<h3>4.2 Zero-shot Generalization</h3>
<p>In this subsection, we show how training with CoT COLLECTION could effectively improve the LM's ability to solve unseen tasks. We have three difference experimental set-ups, testing different aspects: Setup #1: training on the entire 1060 tasks in CoT COLLECTION and evaluating the reasoning capabilities of LMs with the Bigbench Hard (BBH) benchmark (Suzgun et al., 2022), Setup #2: training only on 163 tasks that T0 (Sanh et al., 2021) used for training (a subset of the CoT COLLECTION), and evaluating the linguistic capabilities of LMs with the P3 evaluation benchmark (Sanh et al., 2021), and Setup #3: training with a translated, subset version of CoT COLLECTION for each five different languages and evaluating how LMs could perform CoT reasoning in multilingual settings using the MGSM benchmark (Shi et al., 2022).</p>
<p>Setup #1: CoT Fine-tuning with 1060 CoT Tasks We first perform experiments with our main model, CoT-T5, by training Flan-T5 on the entire CoT COLLECTION and evaluate on the BBH benchmark (Suzgun et al., 2022). In addition to evaluating Flan-T5, we compare the performances of different baselines such as (1) T5-LM (Raffel et al., 2020): the original base model of Flan-T5, (2) T0 (Sanh et al., 2021): an instruction-tuned LM trained with P3 instruction dataset, (3) TkInstruct (Wang et al., 2022b): an instruction-tuned LM trained with SNI instruction dataset, and (4) GPT-3 (Brown et al., 2020): a pre-trained LLM with 175B parameters. For ablation purposes, we also train T5-LM with CoT COLLECTION (de-</p>
<p>Table 1: Evaluation performance on all the 27 unseen datasets from BBH benchmark, including generation tasks. All evaluations are held in a zero-shot setting. The best comparable performances are bolded and second best underlined.
noted as 'T5 + CoT FT'). Note that FLAN Collection includes 15 million instances, hence $\sim 8$ times larger compared to our COT COLLECTION.</p>
<p>The results on BBH benchmark are shown across Table 1 and Table 2. In Table 1, CoT-T5 (3B \&amp; 11B) achieves a $+4.34 \%$ and $+2.60 \%$ improvement over Flan-T5 (3B \&amp; 11B) with CoT Evaluation. Surprisingly, while CoT-T5-3B CoT performance improves $+4.34 \%$ with the cost of $0.96 \%$ degradation in Direct Evalution, CoT-T5-11B's Direct Evaluation performance even improves, resulting in a $+2.57 \%$ total average improvement. Since CoT COLLECTION only includes instances augmented with rationales, these results show that CoT fine-tuning could improve the LM's capabilities regardless of the evaluation method. Also, T5-3B + CoT FT and T5-11B + CoT FT outperforms Flan-T5-3B and Flan-T5-11B by a $+1.45 \%$ and $+3.89 \%$ margin, respectively, when evaluated with CoT evaluation. Moreover, T5-3B + CoT FINE-TUNING outperforms $\sim 4$ times larger models such as T0-11B and Tk-Instruct-11B in both Direct and CoT Evaluation. The overall results indicate that (1) CoT fine-tuning on a diverse number of tasks enables smaller LMs to outperform larger LMs and (2) training with FLAN Collection and CoT Collection provides complementary improvements to LMs under different evaluation methods; CoT-T5 obtains good results across both evaluation methods by training on both datasets.</p>
<p>In Table 2, CoT-T5-11B obtains same or better results on 15 out of 23 tasks when evaluated with Direct evaluation, and 17 out of 23 tasks when evaluated with CoT Evaluation compared to Flan-</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>CoT-T5-11B</th>
<th></th>
<th>FLAN-T5-11B</th>
<th></th>
<th>Vicuna-13B</th>
<th>ChatGPT</th>
<th>CODEX</th>
<th>GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CoT</td>
<td>Direct</td>
<td>CoT</td>
<td>Direct</td>
<td>Direct</td>
<td>Direct</td>
<td>Direct</td>
<td>Direct</td>
</tr>
<tr>
<td>Boolean Expressions</td>
<td>65.6</td>
<td>59.2</td>
<td>51.6</td>
<td>56.8</td>
<td>40.8</td>
<td>82.8</td>
<td>88.4</td>
<td>77.6</td>
</tr>
<tr>
<td>Causal Judgment</td>
<td>60.4</td>
<td>60.2</td>
<td>58.3</td>
<td>61.0</td>
<td>42.2</td>
<td>57.2</td>
<td>63.6</td>
<td>59.9</td>
</tr>
<tr>
<td>Date Understanding</td>
<td>52.0</td>
<td>51.0</td>
<td>46.8</td>
<td>54.8</td>
<td>10.0</td>
<td>42.8</td>
<td>63.6</td>
<td>74.8</td>
</tr>
<tr>
<td>Disambiguation QA</td>
<td>63.4</td>
<td>68.2</td>
<td>63.2</td>
<td>67.2</td>
<td>18.4</td>
<td>57.2</td>
<td>67.2</td>
<td>69.2</td>
</tr>
<tr>
<td>Formal Fallacies</td>
<td>51.2</td>
<td>55.2</td>
<td>54.4</td>
<td>55.2</td>
<td>47.2</td>
<td>53.6</td>
<td>52.4</td>
<td>64.4</td>
</tr>
<tr>
<td>Geometric Shapes</td>
<td>22.0</td>
<td>10.4</td>
<td>12.4</td>
<td>21.2</td>
<td>3.6</td>
<td>25.6</td>
<td>32.0</td>
<td>40.8</td>
</tr>
<tr>
<td>Hyperbation</td>
<td>65.2</td>
<td>64.2</td>
<td>55.2</td>
<td>70.8</td>
<td>44.0</td>
<td>69.2</td>
<td>60.4</td>
<td>62.8</td>
</tr>
<tr>
<td>Logical Deduction (5)</td>
<td>48.2</td>
<td>54.4</td>
<td>51.2</td>
<td>53.6</td>
<td>4.8</td>
<td>38.8</td>
<td>32.4</td>
<td>66.8</td>
</tr>
<tr>
<td>Logical Deduction (7)</td>
<td>52.4</td>
<td>60.6</td>
<td>57.6</td>
<td>60.0</td>
<td>1.2</td>
<td>39.6</td>
<td>26.0</td>
<td>66.0</td>
</tr>
<tr>
<td>Logical Deduction (3)</td>
<td>55.4</td>
<td>75.0</td>
<td>66.4</td>
<td>74.4</td>
<td>16.8</td>
<td>60.4</td>
<td>52.8</td>
<td>94.0</td>
</tr>
<tr>
<td>Movie Recommendation</td>
<td>44.6</td>
<td>52.8</td>
<td>32.4</td>
<td>36.4</td>
<td>43.4</td>
<td>55.4</td>
<td>84.8</td>
<td>79.5</td>
</tr>
<tr>
<td>Navigate</td>
<td>59.0</td>
<td>60.0</td>
<td>60.8</td>
<td>61.6</td>
<td>46.4</td>
<td>55.6</td>
<td>50.4</td>
<td>68.8</td>
</tr>
<tr>
<td>Penguins in a Table</td>
<td>39.1</td>
<td>41.8</td>
<td>41.8</td>
<td>41.8</td>
<td>15.1</td>
<td>45.9</td>
<td>66.4</td>
<td>76.7</td>
</tr>
<tr>
<td>Reasoning Colored Obj.</td>
<td>32.6</td>
<td>33.2</td>
<td>22.8</td>
<td>23.2</td>
<td>12.0</td>
<td>47.6</td>
<td>67.6</td>
<td>84.8</td>
</tr>
<tr>
<td>Ruin Names</td>
<td>42.8</td>
<td>41.6</td>
<td>31.6</td>
<td>34.4</td>
<td>15.7</td>
<td>56.0</td>
<td>75.2</td>
<td>89.1</td>
</tr>
<tr>
<td>Salient Trans Err.</td>
<td>43.8</td>
<td>49.2</td>
<td>35.6</td>
<td>49.2</td>
<td>2.0</td>
<td>40.8</td>
<td>62.0</td>
<td>62.4</td>
</tr>
<tr>
<td>Snarks</td>
<td>67.7</td>
<td>66.2</td>
<td>59.5</td>
<td>70.2</td>
<td>28.1</td>
<td>59.0</td>
<td>61.2</td>
<td>87.6</td>
</tr>
<tr>
<td>Sports Understanding</td>
<td>64.8</td>
<td>66.4</td>
<td>56.0</td>
<td>60.0</td>
<td>48.4</td>
<td>79.6</td>
<td>72.8</td>
<td>84.4</td>
</tr>
<tr>
<td>Temporal Sequences</td>
<td>27.4</td>
<td>28.8</td>
<td>24.4</td>
<td>28.8</td>
<td>16.0</td>
<td>35.6</td>
<td>77.6</td>
<td>98.0</td>
</tr>
<tr>
<td>Tracking Shuff Obj. (5)</td>
<td>20.0</td>
<td>13.2</td>
<td>19.6</td>
<td>15.2</td>
<td>9.2</td>
<td>18.4</td>
<td>20.4</td>
<td>25.2</td>
</tr>
<tr>
<td>Tracking Shuff Obj. (7)</td>
<td>18.4</td>
<td>9.6</td>
<td>13.2</td>
<td>12.0</td>
<td>5.6</td>
<td>15.2</td>
<td>14.4</td>
<td>25.2</td>
</tr>
<tr>
<td>Tracking Shuff Obj. (3)</td>
<td>41.8</td>
<td>31.2</td>
<td>28.8</td>
<td>24.4</td>
<td>23.2</td>
<td>31.6</td>
<td>37.6</td>
<td>42.4</td>
</tr>
<tr>
<td>Web of Lies</td>
<td>57.0</td>
<td>51.6</td>
<td>52.8</td>
<td>50.0</td>
<td>41.2</td>
<td>56.0</td>
<td>51.6</td>
<td>49.6</td>
</tr>
<tr>
<td>Average</td>
<td>47.60</td>
<td>48.00</td>
<td>43.32</td>
<td>47.05</td>
<td>23.30</td>
<td>48.90</td>
<td>52.80</td>
<td>67.40</td>
</tr>
</tbody>
</table>
<p>Table 2: Evaluation performance on 23 unseen classification datasets from BBH benchmark. Scores of Vicuna, ChatGPT, Codex (teacher model of CoT-T5), GPT-4 are obtained from Chung et al. (2022) and Mukherjee et al. (2023). Evaluations are held in a zero-shot setting. The best comparable performances are bolded and second best underlined among the open-sourced LMs.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Natural Language Inference</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Sentence Completion</th>
<th></th>
<th></th>
<th>Coreference Resolut.</th>
<th>WSD</th>
<th>Total Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>RTE</td>
<td>CB</td>
<td>AN. R1</td>
<td>AN. R2</td>
<td>AN. R3</td>
<td>COPA</td>
<td>Hellasw.</td>
<td>StoryC.</td>
<td>Winogr.</td>
<td>WSC</td>
<td>WiC</td>
</tr>
<tr>
<td>T5-3B <em>Raffel et al. (2020)</em></td>
<td>53.03</td>
<td>34.34</td>
<td>32.89</td>
<td>33.76</td>
<td>33.82</td>
<td>54.88</td>
<td>27.00</td>
<td>48.16</td>
<td>50.64</td>
<td>54.09</td>
<td>50.30</td>
</tr>
<tr>
<td>T0-3B <em>Sanh et al. (2021)</em></td>
<td>60.61</td>
<td>48.81</td>
<td>35.10</td>
<td>33.27</td>
<td>33.52</td>
<td>75.13</td>
<td>27.18</td>
<td>84.91</td>
<td>59.91</td>
<td>65.00</td>
<td>51.27</td>
</tr>
<tr>
<td>RoE-3B <em>Jang et al. (2023)</em></td>
<td>64.01</td>
<td>43.57</td>
<td>35.49</td>
<td>34.64</td>
<td>31.22</td>
<td>79.25</td>
<td>34.60</td>
<td>86.33</td>
<td>61.60</td>
<td>62.21</td>
<td>52.97</td>
</tr>
<tr>
<td>KiC-770M <em>Pan et al. (2022)</em></td>
<td>74.00</td>
<td>67.90</td>
<td>36.30</td>
<td>35.00</td>
<td>37.60</td>
<td>85.30</td>
<td>29.60</td>
<td>94.40</td>
<td>55.30</td>
<td>65.40</td>
<td>52.40</td>
</tr>
<tr>
<td>Flipped-3B <em>Ye et al. (2022)</em></td>
<td>71.05</td>
<td>57.74</td>
<td>39.99</td>
<td>37.05</td>
<td>37.73</td>
<td>89.88</td>
<td>41.64</td>
<td>95.88</td>
<td>58.56</td>
<td>58.37</td>
<td>50.42</td>
</tr>
<tr>
<td>GPT-3 (175B) <em>Brown et al. (2020)</em></td>
<td>63.50</td>
<td>46.40</td>
<td>34.60</td>
<td>35.40</td>
<td>34.50</td>
<td>91.00</td>
<td>78.90</td>
<td>83.20</td>
<td>70.20</td>
<td>65.40</td>
<td>45.92</td>
</tr>
<tr>
<td>T0-11B <em>Sanh et al. (2021)</em></td>
<td>80.83</td>
<td>70.12</td>
<td>43.56</td>
<td>38.68</td>
<td>41.26</td>
<td>90.02</td>
<td>33.58</td>
<td>92.40</td>
<td>59.94</td>
<td>61.45</td>
<td>56.58</td>
</tr>
<tr>
<td>T5-3B + CoT FT - Eval w/ Direct</td>
<td>69.96</td>
<td>58.69</td>
<td>37.58</td>
<td>36.00</td>
<td>37.44</td>
<td>84.59</td>
<td>40.92</td>
<td>90.47</td>
<td>55.40</td>
<td>64.33</td>
<td>51.53</td>
</tr>
<tr>
<td>T0-3B + CoT FT - Eval w/ Direct</td>
<td>80.79</td>
<td>65.00</td>
<td>39.49</td>
<td>35.13</td>
<td>38.58</td>
<td>88.27</td>
<td>41.04</td>
<td>92.13</td>
<td>56.40</td>
<td>65.96</td>
<td>53.60</td>
</tr>
<tr>
<td>T5-3B + CoT FT - Eval w/ CoT</td>
<td>80.61</td>
<td>69.17</td>
<td>40.24</td>
<td>36.67</td>
<td>40.17</td>
<td>90.10</td>
<td>41.08</td>
<td>93.00</td>
<td>56.47</td>
<td>55.10</td>
<td>56.73</td>
</tr>
<tr>
<td>T0-3B + CoT FT - Eval w/ CoT</td>
<td>80.25</td>
<td>72.62</td>
<td>41.71</td>
<td>37.22</td>
<td>41.89</td>
<td>90.88</td>
<td>39.50</td>
<td>94.47</td>
<td>57.47</td>
<td>50.58</td>
<td>54.27</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation performance on 11 different unseen P3 dataset <em>Sanh et al. (2021)</em> categorized into 4 task categories. We report the direct performance of the baselines since they were not CoT fine-tuned on instruction data. The best comparable performances are bolded and second best underlined. We exclude Flan-T5 and CoT-T5 since they were trained on the unseen tasks (tasks from FLAN and SNI overlap with the P3 Eval datasets), breaking unseen task assumption.</p>
<p>T5-11B. Interestingly, Vicuna <em>Chiang et al. (2023)</em>, a LM trained on long-form dialogues between users and GPT models, perform much worse compared to both CoT-T5 and Flan-T5. We conjecture that training on instruction datasets from existing academic benchmarks consisting CoT Collection and Flan Collection is more effective in enabling LMs to solve reasoning tasks compared to chat LMs.</p>
<h4>Setup #2: CoT Fine-tuning with 163 CoT Tasks</h4>
<p>(T0 Setup) To examine whether the effect of CoT fine-tuning is dependent on large number of tasks and instances, we use the P3 training subset from the CoT Collection consisted of 644K instances from 163 tasks, and apply CoT fine-tuning to T0 (3B) <em>Sanh et al. (2021)</em> and T5-LM (3B) <em>Raffel et al. (2020)</em>. Note that T0 is trained with 12M instances, hence $\sim 18.63$ times larger. Then, we evaluate on the P3 evaluation benchmark which consists of 11 different NLP datasets. In addition to the baselines from the previous section (T5-LM, T0, and GPT-3), we also include LMs that are trained on the same T0 setup for comparison such as, (1) RoE <em>Jang et al. (2023)</em>: a modular expert LM that retrieves different expert models depending on the unseen task, (2) KiC <em>Pan et al. (2022)</em>: a retrieval-augmented model that</p>
<p>is instruction-tuned to retrieve knowledge from a KB memory, and (3) Flipped <em>Ye et al. (2022)</em>: an instruction-tuned model that is trained to generate the instruction in order to resolve the LM overfitting to the output label as baseline models.</p>
<p>The results are shown in Table 3. Surprisingly, T5-3B + CoT FT outperforms T0-3B by a +8.24% margin when evaluated with CoT Evaluation, while using ~18.63 times less instances. This supports that CoT fine-tuning is <em>data efficient</em>, being effective even with less number of instances and tasks. Moreover, T0-3B + CoT FT improves T0-3B by +8.65% on average accuracy. When compared with T0-11B with ~4 times more number of parameters, it achieves better performance at sentence completion, and word sense disambiguation (WSD) tasks, and obtains similar performances at natural language inference and coreference resolution tasks.</p>
<h4>Setup #3: Multilingual Adaptation with CoT Fine-tuning</h4>
<p>In previous work, <em>Shi et al. (2022)</em> proposed MGSM, a multilingual reasoning benchmark composed of 10 different languages. In this subsection, we conduct a toy experiment to examine whether CoT fine-tuning could enable LMs to reason step-by-step in multilingual settings as well, using a subset of 5 languages (Korean, Russian, French, Chinese, Japanese) from MGSM.</p>
<p>In Table 4, current smaller LMs can be divided into three categories: (1) Flan-T5, a LM that is CoT fine-tuned with mostly English instruction data, (2) MT5 <em>Xue et al. (2021)</em>, a LM pretrained on diverse languages, but isn't instruction tuned or CoT fine-tuned, (3) MT0 <em>Muennighoff et al. (2022)</em>, a LM that is instruction-tuned on diverse languages, but isn't CoT fine-tuned. In relatively underrepresented languages such as Korean, Japanese, and Chinese, all three LMs get close to zero accuracy.</p>
<p>A natural question arises whether training a multilingual LM that could reason step-by-step on different languages is viable. As a preliminary research, we examine whether CoT Fine-tuning on a single language with a small amount of CoT data could enable LMs to avoid achieving near zero score such as Korean, Chinese and Japanese subsets of MGSM. Since there is no publicly available multilingual instruction dataset, we translate 60K ~ 80K instances from CoT Collection for each 5 languages using ChatGPT <em>OpenAI (2022)</em>, and CoT fine-tune mT5 and mT0 on each of them.</p>
<p>The results are shown in Table 4. Across all the 5 different languages, CoT fine-tuning brings</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>ko</th>
<th>ru</th>
<th>fr</th>
<th>zh</th>
<th>ja</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flan-T5-3B</td>
<td>0.0</td>
<td>2.8</td>
<td>7.2</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>Flan-T5-11B</td>
<td>0.0</td>
<td>5.2</td>
<td>13.2</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>MT5-3.7B</td>
<td>0.0</td>
<td>1.2</td>
<td>2.0</td>
<td>0.8</td>
<td>0.8</td>
</tr>
<tr>
<td>MT0-3.7B</td>
<td>0.0</td>
<td>4.8</td>
<td>7.2</td>
<td>1.6</td>
<td>2.4</td>
</tr>
<tr>
<td>GPT-3 (175B)</td>
<td>0.0</td>
<td>4.4</td>
<td>10.8</td>
<td>6.8</td>
<td>0.8</td>
</tr>
<tr>
<td>MT5-3.7B + CoT FT</td>
<td>3.2</td>
<td>6.8</td>
<td>9.6</td>
<td>6.0</td>
<td>7.6</td>
</tr>
<tr>
<td>MT0-3.7B + CoT FT</td>
<td>7.6</td>
<td>10.4</td>
<td>15.6</td>
<td>11.2</td>
<td>11.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Evaluation performance on MGSM benchmark <em>Shi et al. (2022)</em> across 5 languages (Korean, Russian, French, Chinese, Japanese, respectively). All evaluations are held in a zero-shot setting with CoT Evaluation except GPT-3 using a 6-Shot prompt for ICL. The best comparable performances are <strong>bolded</strong> and second best underlined. Note that 'MT5-3.7B + CoT FT' and 'MT0-3.7B + CoT FT' are trained on a single language instead of multiple languages as mT5 and mT0.</p>
<p>about non-trivial gains in performance. Even for relatively low-resource languages such as Korean Japanese, and Chinese, CoT fine-tuning on the specific language allows the underlying LM to perform mathematical reasoning in the target language, which are considered very difficult <em>Shi et al. (2022)</em>. Considering that only a very small number of instances were used for language-specific adaptation (60k-80k), CoT fine-tuning shows potential for efficient language adaptation.</p>
<p>However, it is noteworthy that we limited our setting to training/evaluating on a single target language, without exploring the cross-lingual transfer of CoT capabilities among varied languages. The chief objective of this experimentation was to ascertain if introducing a minimal volume of CoT data could facilitate effective adaptation to the target language, specifically when addressing reasoning challenges. Up to date, no hypothesis has suggested that training with CoT in various languages could enable cross-lingual transfer of CoT abilities among different languages. We identify this as a promising avenue for future exploration.</p>
<h3>4.3 Few-shot Generalization</h3>
<p>In this subsection, we show how CoT-T5 performs in a few-shot adaptation setting where a limited number of instances from the target task can be used for training, which is sometimes more likely in real-world scenarios.</p>
<h4>Dataset Setup</h4>
<p>We choose 4 domain-specific datasets from legal and medical domains including LEDGAR <em>Tuggener et al. (2020)</em>, Case Hold <em>Zheng et al. (2021)</em>, MedNLI <em>Romanov and Shivade (2018)</em>, and PubMedQA <em>Jin et al. (2019)</em>. To simulate a few-shot setting, we randomly sample 64 instances from the train split of each dataset.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>#Train Param</th>
<th>Ledgar</th>
<th>Case Hold</th>
<th>MedNLI</th>
<th>PubmedQA</th>
<th>Total Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flan-T5-3B + Full FT.</td>
<td>2.8B</td>
<td>52.60</td>
<td>61.40</td>
<td>66.82</td>
<td>66.28</td>
<td>61.78</td>
</tr>
<tr>
<td>Flan-T5-3B + Full CoT FT.</td>
<td>2.8B</td>
<td>53.60</td>
<td>58.80</td>
<td>65.89</td>
<td>65.89</td>
<td>61.05</td>
</tr>
<tr>
<td>CoT-T5-3B + Full CoT FT. (Ours)</td>
<td>2.8B</td>
<td>51.90</td>
<td>60.60</td>
<td>67.16</td>
<td>68.12</td>
<td>61.95</td>
</tr>
<tr>
<td>Flan-T5-3B + LoRA FT.</td>
<td>2.35M</td>
<td>53.20</td>
<td>58.80</td>
<td>61.60</td>
<td>67.18</td>
<td>60.19</td>
</tr>
<tr>
<td>Flan-T5-3B + LoRA CoT FT.</td>
<td>2.35M</td>
<td>51.20</td>
<td>61.60</td>
<td>62.59</td>
<td>66.06</td>
<td>60.36</td>
</tr>
<tr>
<td>CoT-T5-3B + LoRA CoT FT. (Ours)</td>
<td>2.35M</td>
<td>54.80</td>
<td>63.60</td>
<td>68.00</td>
<td>69.66</td>
<td>64.02</td>
</tr>
<tr>
<td>Flan-T5-11B + LoRA FT.</td>
<td>4.72M</td>
<td>55.30</td>
<td>64.90</td>
<td>75.91</td>
<td>70.25</td>
<td>66.59</td>
</tr>
<tr>
<td>Flan-T5-11B + LoRA CoT FT.</td>
<td>4.72M</td>
<td>52.10</td>
<td>65.50</td>
<td>71.63</td>
<td>71.60</td>
<td>65.21</td>
</tr>
<tr>
<td>CoT-T5-11B + LoRA CoT FT. (Ours)</td>
<td>4.72M</td>
<td>56.10</td>
<td>68.30</td>
<td>78.02</td>
<td>73.42</td>
<td>68.96</td>
</tr>
<tr>
<td>Claude (Anthropic, 2023) + ICL</td>
<td>0</td>
<td>55.70</td>
<td>57.20</td>
<td>75.94</td>
<td>54.58</td>
<td>60.85</td>
</tr>
<tr>
<td>Claude (Anthropic, 2023) + CoT PT.</td>
<td>0</td>
<td>34.80</td>
<td>43.60</td>
<td>76.51</td>
<td>52.06</td>
<td>51.74</td>
</tr>
<tr>
<td>ChatGPT (OpenAI, 2022) + ICL</td>
<td>0</td>
<td>51.70</td>
<td>32.10</td>
<td>70.53</td>
<td>65.59</td>
<td>54.98</td>
</tr>
<tr>
<td>ChatGPT (OpenAI, 2022) + CoT PT.</td>
<td>0</td>
<td>51.00</td>
<td>18.90</td>
<td>63.71</td>
<td>25.22</td>
<td>39.70</td>
</tr>
</tbody>
</table>
<p>Table 5: Evaluation performance on 4 domain-specific datasets. FT. denotes Fine-tuning, CoT FT. denotes CoT fine-tuning, and CoT PT. denotes CoT Prompting. The best comparable performances are bolded and second best underlined. For a few-shot adaptation, we use 64 randomly sampled instances from each dataset.</p>
<p>We report the average accuracy across 3 runs with different random seeds. We augment rationales for the 64 training instances using the procedure described in Section 3 for the rationale augmentation phase, utilizing the MCQA prompt from P3 dataset. In an applied setting, practitioners could obtain rationales written by human experts.</p>
<p>Training Setup We compare Flan-T5 &amp; CoT-T5, across 3B and 11B scale and explore 4 different approaches for few-shot adaptation: (1) regular finetuning, (2) CoT fine-tuning, (3) LoRA fine-tuning, and (4) LoRA CoT fine-tuning. When applying Lora, we use a rank of 4 and train for 1K steps following <em>Liu et al. (2022b)</em>. This results in training 2.35M parameters for 3B scale models and 4.72M parameters for 11B scale models. Also, we include Claude (Anthropic, 2023) and ChatGPT (OpenAI, 2022) as ICL baselines by appending demonstrations up to maximum context length. Specifically, For CoT prompting, the demonstrations are sampled among 64 augmented rationales are used.</p>
<p>Effect of LoRA The experimental results are shown in Table 5. Overall, CoT fine-tuning CoT-T5 integrated with LoRA obtains the best results overall. Surprisingly for Flan-T5, applying full fine-tuning obtains better performance compared to its counterpart using LoRA fine-tuning. However, when using CoT-T5, LoRA achieves higher performance compared to full fine-tuning. We conjecture this to be the case because introducing only a few parameters enables CoT-T5 to maintain the CoT ability acquired during CoT fine-tuning.</p>
<p>Fine-tuning vs. CoT Fine-tuning While CoT fine-tuning obtains similar or lower performance compared to regular fine-tuning in Flan-T5, CoT-T5 achieves higher performance with CoT finetuning compared to Flan-T5 regular fine-tuning. This results in CoT-T5 in combination with CoT fine-tuning showing the best performance in few-shot adaptation setting.</p>
<p>Fine-tuning vs. ICL Lastly, fine-tuning methods obtain overall better results compared to ICL methods utilizing much larger, proprietary LLMs. We conjecture this to be the case due to the long input length of legal and medical datasets, making appending all available demonstrations (64) impossible. While increasing the context length could serve as a temporary solution, it would still mean that the inference time will increase quadratically in proportion to the input length, which makes ICL computationally expensive.</p>
<h2>5 Analysis of of CoT Fine-tuning</h2>
<p>In this section, we conduct experiments to address the following two research questions:</p>
<ul>
<li>For practitioners, is it more effective to augment CoT rationales across diverse tasks or more instances with a fixed number of tasks?</li>
<li>During CoT fine-tuning, does the LM maintain its performance on in-domain tasks without any catastrophic forgetting?</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Scaling plot of increasing the number of instances within the CoT COLLECTION compared to using the existing 9 CoT datasets. Even with less number of instances, maintaining a wider range of tasks is crucial to improve the CoT abilities of an underlying LLM.</p>
<h3>5.1 Scaling the number of tasks &amp; instances</h3>
<p>In our main experiments, we used a large number of instances (1.84M) across a large number of tasks (1,060) to apply CoT fine-tuning. A natural question arises: "Is it more effective to increase the number of tasks or the number of instances?" To address this question, we conduct an experiment of randomly sampling a small number of instances within the CoT COLLECTION and comparing the BBH performance with (1) a baseline that is only CoT fine-tuned with the existing 9 CoT tasks and (2) CoT-T5 that fully utilizes all the 1.84M instances. Specifically, we sample 10K, 100K instances within the CoT COLLECTION and for the 9 CoT tasks, we fully use all the 180K instances. As CoT-T5, we use Flan-T5 as our base model and use the same training configuration and evaluation setting (CoT Eval) during our experiments.</p>
<p>The results are shown in Figure 3, where surprisingly, only using 10K instances across 1,060 tasks obtains better performance compared to using 180K instances across 9 tasks. This shows that maintaining a wide range of tasks is more crucial compared to increasing the number of instances.</p>
<h3>5.2 In-domain Task Accuracy of CoT-T5</h3>
<p>It is well known that LMs that are fine-tuned on a wide range of tasks suffer from catastrophic forgetting <em>Chen et al. (2020); Jang et al. (2021, 2023)</em>, a phenomenon where an LM improves its performance on newly learned tasks while the performance on previously learned tasks diminishes.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: In-domain task accuracy with CoT evaluation. CoT Fine-tuning with the CoT COLLECTION also improves accuracy on in-domain tasks as well.</p>
<p>While CoT-T5 uses the same tasks as its base model (Flan-T5), we also check whether CoT fine-tuning on a wide range of tasks could possibly harm performance. For this purpose, we use the test set of 5 tasks within the CoT COLLECTION, namely ANLI-R1, ANLI-R2, ANLI-R3, RTE, and Winogrande. Note that this differs with the Setup #2 in the main experiments in that we use different base models (T0 vs Flan-T5), and the tasks are already used for CoT fine-tuning.</p>
<p>Results are shown in Figure 4, where CoT-T5 consistently improves in-domain accuracy on the learned tasks as well. However, we conjecture that this is because we used the exact same task that Flan-T5 used to CoT fine-tuned CoT-T5. Adding additional tasks that were not used to train Flan-T5 and CoT-T5 could show different results, and we leave additional exploration of catastrophic forgetting during CoT fine-tuning to future work.</p>
<h3>6 Conclusion</h3>
<p>In this work, we show that augmenting rationales from an instruction tuning data using LLMs (Open AI Codex), and CoT fine-tuning could improve the reasoning capabilities of smaller LMs. Specifically, we construct CoT COLLECTION, a large-scale instruction-tuning dataset with 1.84M CoT rationales extracted across 1,060 NLP tasks. With our dataset, we CoT fine-tune Flan-T5 and obtain CoT-T5, which shows better zero-shot generalization performance and serves as a better base model when training with few number of instances. We hope CoT COLLECTION could be beneficial in the development of future strategies for advancing the capabilities of LMs with CoT fine-tuning.</p>
<h2>Limitations</h2>
<p>Recently, there has been a lot of focus on distilling the ability to engage in dialogues with longform outputs in the context of instruction following (Taori et al., 2023; Chiang et al., 2023). Since our model CoT-T5 is not trained to engage in dialogues with long-form responses from LLMs, it does not necessarily possess the ability to be applied in chat applications. In contrast, our work focuses on improving the zero-shot and few-shot capabilities by training on academic benchmarks (CoT COLLECTION, Flan Collection), where LMs trained with chat data lack on. Utilizing both longform chat data from LLMs along with instruction data from academic tasks has been addressed in future work (Wang et al., 2023). Moreover, various applications have been introduced by using the FEEDBACK COLLECTION to train advanced chat models ${ }^{7}$.</p>
<p>Also, since CoT-T5 uses Flan-T5 as a base model, it doesn't have the ability to perform step-by-step reasoning in diverse languages. Exploring how to efficiently and effectively train on CoT data from multiple languages is also a promising and important line of future work. While Shi et al. (2022) has shown that large LMs with more than 100B parameters have the ability to write CoT in different languages, our results show that smaller LMs show nearly zero accuracy when solving math problems in different languages. While CoT finetuning somehow shows slight improvement, a more comprehensive strategy of integrating the ability to write CoT in diverse language would hold crucial.</p>
<p>In terms of reproducibility, it is extremely concerning that proprietary LLMs shut down such as the example of the Codex, the LLM we used for rationale augmentation. We provide additional analysis on how different LLMs could be used for this process in Appendix A. Also, there is room of improvement regarding the quality of our dataset by using more powerful LLMs such as GPT-4 and better prompting techniques such as Tree of Thoughts (ToT) (Yao et al., 2023). This was examined by later work in Mukherjee et al. (2023) which used GPT-4 to augment 5 million rationales and Yue et al. (2023) which mixed Chain-of-Thoughts and Program of Thoughts (PoT) during fine-tuning. Using rationales extracted using Tree of Thoughts (Yao et al., 2023) could also be explored in future work.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>References</h2>
<p>Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for commonsenseqa: New dataset and models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3050-3065.</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $2357-2367$.</p>
<p>Anthropic. 2023. Claude. https: //www.anthropic.com/index/ introducing-claude.</p>
<p>Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. 2021. Ext5: Towards extreme multitask scaling for transfer learning. arXiv preprint arXiv:2111.10952.</p>
<p>Akari Asai, Mohammadreza Salehi, Matthew E Peters, and Hannaneh Hajishirzi. 2022. Attentional mixtures of soft prompt tuning for parameter-efficient multi-task knowledge sharing. arXiv preprint arXiv:2205.11961.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31.</p>
<p>Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. 2020. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7870-7881.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919.</p>
<p>Google. 2023. Bard. https:// blog.google/technology/ai/ bard-google-ai-search-updates/.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017.</p>
<p>Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2023. Exploring the benefits of training expert language models over instruction tuning. arXiv preprint arXiv:2302.03202.</p>
<p>Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. 2021. Towards continual knowledge learning of language models. arXiv preprint arXiv:2110.03215.</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567-2577.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. Qasc: A dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8082-8090.</p>
<p>Hyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, et al. 2022. Soda: Million-scale dialogue distillation with social commonsense contextualization. arXiv preprint arXiv:2212.10465.</p>
<p>Seungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae, and Jinyoung Yeo. 2023. Cotever: Chain of thought prompting annotation toolkit for explanation verification. arXiv preprint arXiv:2303.03628.</p>
<p>Nikita Kitaev, Steven Cao, and Dan Klein. 2019. Multilingual constituency parsing with self-attention and pre-training. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3499-3505.</p>
<p>Nikita Kitaev and Dan Klein. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676-2686.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. 2021. Qed: A framework and dataset for explanations in question answering. Transactions of the Association for computational Linguistics, 9:790-806.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059.</p>
<p>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. 2021. Datasets: A community library for natural language processing. arXiv preprint arXiv:2109.02846.</p>
<p>Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. 2023. Multi-step jailbreaking privacy attacks on chatgpt. arXiv preprint arXiv:2304.05197.</p>
<p>Alisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. 2022a. Wanli: Worker and ai collaboration for natural language inference dataset creation. arXiv preprint arXiv:2201.05955.</p>
<p>Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022b. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950-1965.</p>
<p>Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022c. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61-68.</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv preprint arXiv:2103.10385.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688.</p>
<p>David Mhlanga. 2023. Open ai in education, the responsible and ethical use of chatgpt towards lifelong learning. Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning (February 11, 2023).</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. Metaicl: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791-2809.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707.</p>
<p>Yasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and Greg Durrett. 2021. Creak: A dataset for commonsense reasoning over entity knowledge. arXiv preprint arXiv:2109.01653.</p>
<p>OpenAI. 2022. ChatGPT. https://openai.com/ blog/chatgpt.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. 2022. Knowledge-incontext: Towards knowledgeable semi-parametric language models. arXiv preprint arXiv:2210.16433.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.</p>
<p>Alexey Romanov and Chaitanya Shivade. 2018. Lessons from natural language inference in the clinical domain. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1586-1596.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269.</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057.</p>
<p>Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2022. Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. arXiv preprint arXiv:2212.00193.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131.</p>
<p>Don Tuggener, Pius Von Däniken, Thomas Peetz, and Mark Cieliebak. 2020. Ledgar: a large-scale multilabel corpus for text classification of legal provisions in contracts. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 12351241 .</p>
<p>Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? a pilot study for sense making and explanation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages $4020-4026$.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. URL https://arxiv. org/abs/2204.07705.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Jason Wei, Yi Tay, and Quoc V Le. 2022a. Inverse scaling can become u-shaped. arXiv preprint arXiv:2211.02011.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. Symbolic knowledge distillation: from general language models to commonsense models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4602-4625.</p>
<p>Haike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng, and Zhilin Yang. 2022. A universal discriminator for zero-shot generalization. arXiv preprint arXiv:2211.08099.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Michihiro Yasunaga and Percy Liang. 2020. Graphbased, self-supervised program repair from diagnostic feedback. In International Conference on Machine Learning, pages 10799-10808. PMLR.</p>
<p>Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo. 2023. In-context instruction learning. arXiv preprint arXiv:2302.14691.</p>
<p>Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, and Minjoon Seo. 2022. Guess the instruction! making language models stronger zero-shot learners. arXiv preprint arXiv:2210.02969.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D Goodman. 2022. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Lucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. 2021. When does pretraining help? assessing self-supervised learning for</p>
<p>law and the casehold dataset of 53,000+ legal holdings. In Proceedings of the eighteenth international conference on artificial intelligence and law, pages $159-168$.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">A Analysis of CoT Collection</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Non-cherry picked rationales within CoT Col- <br> LECTION are shown in Table 8. We perform an analysis regarding the quality, diversity, and reproducibility of rationale within the CoT COLLECTION.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Diversity of Rationales To take a look into the diversity of CoT COLLECTION, we use Berkeley Neural Parser (Kitaev and Klein, 2018; Kitaev et al., 2019) and parse rationales. More specifically, the verb which is closest to the root of the parse tree along the noun object is extracted. We compare this with the rationales from the 9 CoT datasets used in Chung et al. (2022). As shown in Figure 5, CoT COLLECTION have diverse textual formats included compared to the 9 existing CoT datasets that have a high proportion assigned to 'answer question' and 'consider following'.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Quality of Rationales To ensure the quality of CoT COLLECTION, we use ROSCOE Golovneva et al., 2022, a suite of metrics designed to evaluate rationales under different criteria within semantic alignment, semantic similarity, logical inference, language coherence. We compare with human-authored rationales obtained during Prompt Creation in Section 3. The 13 ROSCOE scores are shown in Table 6. The results show that CoT COLLECTION include CoT rationales that are faithful, less repetitive, informative, and logical even when compared to human-authored rationales. Yet, we find that machine-generated rationales tend to have higher perplexity, leading to lower language coherence scores. We conjecture this is because including diverse textual formats leads may result in relatively higher perplexity (Holtzman et al., 2019).</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Is CoT COLLECTION Reproducible? One could doubt whether CoT COLLECTION is reproducible due to the usage of OpenAI model in the process of CoT rationale augmentation ${ }^{8}$. In this section, we test different LLMs to generate 150 rationales randomly sampled from COT COLLECTION, and compare the ROSCOE score (Golovneva et al., 2022) in order to assess the quality. We use Bard (Google, 2023), Claude (Anthropic, 2023), for comparing with OpenAI Codex. The comparison of quality is shown in Figure 6. The results</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 6: Comparison of the quality between human-authored rationales and machine-generated rationales. 13 label-free metrics from ROSCOE (Golovneva et al., 2022) is used.
show that different LLMs are able to produce high quality rationales in terms of semantic alignment and language coherence.</p>
<h2>B Filtering CoT COLLECTION</h2>
<p>Filtering After generating multiple rationales, we filter to ensure high quality. We apply the following criteria to filter instances:</p>
<ul>
<li>We exclude rationales that do not include the ground truth answer when splitted by white spaces. While a rationale that doesn't include the answer isn't necessarily a bad rationale, we found it is effective to exclude inconsistent ones.</li>
<li>We exclude CoT rationales that exceed the maximum output length, where we constrain the sum of $r$ and $y$ to be shorter than 512 tokens.</li>
<li>We exclude rationales that are identical to previously augmented ones during our process.</li>
<li>We exclude rationales that include repetitive sentences within the context.</li>
</ul>
<p>We further include the filtered instances in Table 9 .</p>
<p>Also, we found that in many cases, Codex degenerates and starts writing code after the rationale. To prevent inclusion of code snippets, we apply additional filtering based on trigger tokens that abundantly appear in the start of the code. The list of trigger tokens are as follows:</p>
<div class="codehilite"><pre><span></span><code>CODE_FILTER = [
    &quot;\n&#39;\n\n&#39;’&quot;, &quot;\n&#39;\n&quot;, &quot;\n&#39;’\n&quot;,
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    &quot;\n&#39;&#39;&#39; \n&quot;, &quot;\n&#39;&#39;&#39;&quot;, &quot;\n\n \n&#39;&quot;,
    &quot;\n\n \n&#39;&quot;, &quot;\n\n \nimport&quot;,
    &quot;\n&#39;;\n\n&quot;, &quot;\&quot;\n\n&quot;,
    &quot;[examp&quot;, &quot;[Examp&quot;, &quot;\n&#39;;\n\n&quot;,
    &quot;&#39;&#39;&#39; \n&quot;, &quot;\n&#39;\n&quot;, &quot;\n\n&#39;&#39;&quot;,
    &quot;\n&#39;&#39;&quot;, &quot;\&quot;\&quot;\&quot;\n&quot;, &quot;\n\n\t&quot;,
    &quot;\n#&quot;, &quot;\&quot;;\n&quot;, &quot;\&quot;\n\t&quot;,
    &quot;print(&quot;, &quot;\n &quot;, &quot;\&quot;\n&#39;&#39;&#39;&quot;,
    &quot;&#39;&#39;&#39; \nimport&quot;, &quot;\&quot;\n\n\t&quot;,
    &quot;\n\n &quot;, &quot;\n\t\t&quot;, &quot;\t &quot;,
    &quot;\&quot;\n }\n&quot;, &quot;\n\n ####&quot;,
    &quot;\n\n \t&#39;)\n]&quot;, &quot;\n&lt;/block&gt;\n\n&quot;,
    &quot;\n\n <span class="gs">*/\n&quot;,</span>
<span class="gs">    &quot;\&quot;\n\n \n \t&#39;;&quot;, &quot;\n\n \t&quot;,</span>
<span class="gs">    &quot;\&quot;\n\n \t *</span>/&quot;, &quot;\&quot;;\n\n }&quot;,
    &quot;\n\n \t\t&quot;, &quot;\&quot;\n&#39;, \n\t]&quot;,
    &quot;]]\n\t&quot;, &quot;\&quot;\n\n&#39;&quot;, &quot;\&quot;\n&#39;&#39;&#39; \n\n&quot;,
    &quot;\n\n OR&quot;, &quot;\n \n&quot;
}
</code></pre></div>

<h1>C Training and Evaluation Details of CoT-T5</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Batch size</th>
<th style="text-align: center;">LR</th>
<th style="text-align: center;">Optimizer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">CoT-T5-3B</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">CoT-T5-11B</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">Adafactor</td>
</tr>
</tbody>
</table>
<p>Table 7: Hyperparameters used for fine-tuning CoT-T5.</p>
<p>We mostly follow the fine-tuning details of Chung et al. (2022) to train CoT-T5. The hyperparameters used for training CoT-T5 are shown in Table 7. We find 3B and 11B sized LMs converge well using different optimizers. While CoT-T5-3B tends to converge well using AdamW, CoT-T5-11B is well optimized using Adafactor. For both sizes, we train with 1 epoch using CoT COLLECTION which takes 1 day (3B) and 7 days (11B) when 8 A 100 (80GB) GPUs are used. For both settings, we use a gradient accumulation step of 8 .</p>
<p>For sampling training instances, we sample instances from Flan Collection (Longpre et al., 2023) by using the proportion of $23.94 \%$ (FLAN), $30.85 \%(\mathrm{P} 3), 7.89 \%$ (Existing 9 CoT datasets), $25.47 \%(\mathrm{SNI})$ and $11.85 \%$ (other dialogue \&amp; code datasets). This is done by sampling 400 instances (FLAN), 300 instances (P3), 150 instances (SNI), 4000 instances (Existing 9 CoT datasets), and 300 instances (other dialogue \&amp; code datasets), respectively. We generate 5 rationales per instance and then apply filtering, leading to the final set of COT COLLECTION, which is consisted of 1.84 million instances and rationales across 1,060 tasks.</p>
<p>During evaluation, we found that using nucleus
sampling (Holtzman et al., 2019) with $p=0.8$ and no_repeat_n_gram $=3$ was very effective in generating good-quality rationales.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The top 20 common root verbs (inner circle) and their top 4 noun objects (outer circle) within the rationales of the 9 CoT tasks used in Chung et al. (2022) (left side) and our CoT Collection with 1,060 tasks (right side).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison of the quality of augmenting CoT rationales using different LLMs by reference-free ROSCOE scores. Semantic Alignment(SA), Semantic Similarity(SS), Logical Inference(LI), Coherence(LC).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question \&amp; Instruction</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Read the following context and choose the best option to answer the question. <br> Context: From the triple loop she executed after five minutes, he suspected that she had managed to sneak off for flying more often than during the summers and wondered just how diligent George was about practicing on his own time. <br> Question: What is a triple loop? <br> Options: <br> A. None of the above choices. <br> B. 3 round trips on a plane route. <br> C. 3 neckties worn on flights. <br> D. A feature pulled off by making 3 loops in the air.</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">A triple loop is a maneuver in which an aircraft makes three loops. Based on the context, it can be inferred that a triple loop involves making 3 loops in the air. Since this is what A means, D should be chosen as the correct answer.</td>
</tr>
<tr>
<td style="text-align: center;">Read the passage below and choose the right answer to the following question (choices are more or less ). <br> On a hot day, an endotherm may give off more heat by increasing blood flow to the surface of the body. <br> After Sarah finishes her walk, she notices that her blood is not really pumping that hard. Sarah can know that since it's a cool day, she's giving off what kind of heat compared to usual more or less?</td>
<td style="text-align: center;">less</td>
<td style="text-align: center;">The given passage states that an endotherm gives off more heat by increasing blood flow to the surface of its body. As such, it can be inferred that decreased blood flow corresponds with less heat being given off. Since Sarah notices that her blood is not really pumping hard on a cool day, she is giving off less heat compared to usual.</td>
</tr>
<tr>
<td style="text-align: center;">Read this and answer the question. <br> Pubertal development also affects circulatory and respiratory systems as an adolescents' heart and lungs increase in both size and capacity. These changes lead to increased strength and tolerance for exercise. Sex differences are apparent as males tend to develop "larger hearts and lungs, higher systolic blood pressure, a lower resting heart rate, <br> a greater capacity for carrying oxygen to the blood, a greater power for neutralizing the chemical products of muscular exercise, higher blood hemoglobin and more red blood cells. <br> Which sex tends to develop larger hearts and lungs?</td>
<td style="text-align: center;">males</td>
<td style="text-align: center;">The last sentence of the paragraph states that males tend to develop larger hearts and lungs.</td>
</tr>
<tr>
<td style="text-align: center;">In this task, you are given a string of characters. You need to remove duplicate characters from the string if any, and return the resulting string.</td>
<td style="text-align: center;">xXGFxHYtafOiyWLZQfpw</td>
<td style="text-align: center;">There are no duplicate characters in the given string, so the resulting string will be the same as the input. Therefore, the output will be "xXGFxHYtafOiyWLZQfpw".</td>
</tr>
<tr>
<td style="text-align: center;">In this task you will be given a list of numbers and you should remove all duplicates in the list. If every number is repeated in the list an empty list should be returned. Your list should be numbers inside brackets, just like the given list.</td>
<td style="text-align: center;">${5,1,0}$</td>
<td style="text-align: center;">The given list is ${3,5,1,0,3}$. <br> We can see that '3' appears in the list twice. Therefore, we need to keep only one of the ' 3 's and remove the other one. The resulting list is ${5,1,0}$.</td>
</tr>
<tr>
<td style="text-align: center;">How to keep your phone cool in the car <br> Keep your phone on a vent mount with air conditioning on if you need to use it. <br> If you need to use your phone while you're in your car, purchase a mount that attaches to one of the dashboard air vents. Turn on the air conditioner so it keeps your phone cold. <br> Can we draw the following conclusion? keep your phone and tablet cool while mounted on an air conditioner vent if you need to use them while in the car. <br> OPTIONS: <br> -Yes <br> - It's impossible to say <br> - No</td>
<td style="text-align: center;">It's impossible to say</td>
<td style="text-align: center;">While the passage does mention that your phone should be mounted on an air vent, it doesn't specify whether this is referring to a car or home AC. Additionally, we don't know from the context if other devices such as tablets can also be kept cool using this method. <br> So, "It's impossible to say"</td>
</tr>
<tr>
<td style="text-align: center;">Give me a possible correct answer to the question "who nominated stephen g. breyer?"</td>
<td style="text-align: center;">Bill Clinton</td>
<td style="text-align: center;">Stephen G. Breyer was a nominee for the Supreme Court by Bill Clinton and was confirmed on July 29, 1994. Although he wasn't nominated in 1998 or 1999, this is still a valid answer because it happened within 5 years of those dates before.</td>
</tr>
<tr>
<td style="text-align: center;">Write down the solution for this math problem: <br> Solve $291 \times c-264 \times c=189$ for c.</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$\begin{gathered} \text { STEP 1. } 291 \times c-264 \times c=189 \ \text { STEP 2. } 27 \times c-189 \ \text { STEP 3. } c-7 \end{gathered}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Example of rationales within CoT COLLECTION including tasks such as Multiple choice QA (MCQA), Extractive QA (ExQA), Closed-book QA (CBQA), Formal Logic, Natural Language Inference (NLI), and Arithmetic.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">CoT Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">You are given a math word problem <br> and you are supposed to only use subtraction on the numbers <br> embedded in the text to answer the following question <br> and then only report the final numerical answer. <br> Context: Jake has 8 fewer peaches and 10 more apples than Steven. <br> Steven has 11 apples and 18 peaches. <br> Question: How many more peaches than apples does Steven have?</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">To solve this problem, <br> we need to find the number of apples and peaches that Steven has. <br> From the text, we know that Jake has 8 fewer peaches <br> than Steven and 10 more apples than Steven. <br> We also know that Steven has <br> 11 apples and 18 peaches. <br> Using these clues we can write two equations: <br> Steven's peaches $=$ Jake's Peaches +8 <br> Steven's Apples $=$ Jake's Apples -10 <br> We can then add these two equations together: <br> (Steven's Peaches) $=$ (Stevens' Apples) $=$ (Jake's Peaches +8 ) + (Jake's Apples-10) <br> $(18+11) <em>$ (Jake 's Peaches +8 ) + (Jakes'Apples-10) <br> $29=$ Jakes'peach+Jakes'applees-2 <br> $\mathrm{a}=29-2 \times 27$ <br> $\mathrm{b}=(18+11)-\mathrm{a}$ <br> $\mathrm{c}=8 / 2$ <br> $\mathrm{nd}=\mathrm{c}^{</em>} 2$</td>
</tr>
<tr>
<td style="text-align: center;">What is the solution? <br> Solve $690^{*} \mathrm{c}+14495=-4825$ for c .</td>
<td style="text-align: center;">$-28$</td>
<td style="text-align: center;">$690^{<em>} \mathrm{c}+14495=-4825$ <br> Subtracting 4825 from both sides: <br> $690^{</em>} \mathrm{c}+9669=0$ <br> Dividing both sides by 690 : <br> $\mathrm{c}=-14.02391304348$</td>
</tr>
</tbody>
</table>
<p>Table 9: Two representative rationale examples that are filtered with our filtering criteria. It demonstrates that not including the ground truth token within the rationale could indicate that the LLM failed to generate a consistent rationale, often leading to degeneration. This phenomenon is mostly found on math datasets.</p>
<h2>D Prompts used for Rationale Augmentation</h2>
<p>Among the 26 demonstrations used for each family of tasks when augmenting CoT rationales, we include 4 examples, other demonstrations for task families can be seen at this link.</p>
<h2>[Example 1]</h2>
<p>[Instruction and Question]
Skylar had stopped breathing but Lee held Skylar's arms anyway, in vain. Given the context: Lee want to do what next?
Possible answers: beg the doctors to try again, get her up and walking home, bring Skylar back
[Options]</p>
<ul>
<li>beg the doctors to try again</li>
<li>get her up and walking home</li>
<li>bring Skylar back
[Answer]
beg the doctors to try again</li>
</ul>
<h2>[Rationale]</h2>
<p>The context of the situation is that Skylar has stopped breathing and Lee is holding Skylar's arms. This suggests that Skylar has received medical attention and is no longer alive.
Based on this, Lee would want to beg the doctors to try again, as this would be the best way to potentially save Skylar's life.
The options of getting Skylar up and walking home or bringing Skylar back do not align with the context, as they would not address the immediate medical emergency at hand.</p>
<p>So the answer is to beg the doctors to try again.</p>
<h2>[Example 2]</h2>
<p>[Instruction and Question]
Do you think the right answer to the question "what can run on alcoholic fermentation of the glucose in corn or other plants?" is "bicycles"?
[Options]</p>
<ul>
<li>No</li>
<li>Yes
[Answer]
No</li>
</ul>
<h2>[Rationale]</h2>
<p>Alcoholic fermentation is a process that occurs in living organisms, most commonly yeast, to convert glucose into energy. This process does not produce enough energy to power something like a bicycle.</p>
<p>So the answer is No.</p>
<h2>[Example 3]</h2>
<p>[Instruction and Question]
Eventually, the cyst can get so large that part of the wall weakens and fluid begins to leak out ; that is, the cyst ruptures. It is similar to popping a water balloon. When this happens, the fluid may irritate the pelvic lining and cause pain, often severe. Most of the time the pain begins to gradually subside over the course of the next few days.
What type of doctor might the person have had to see?</p>
<ul>
<li>A paediatrician - A heart surgeon - a gynecologist - None of the above choices
[Options]</li>
<li>A paediatrician</li>
<li>A heart surgeon</li>
<li>a gynecologist</li>
<li>None of the above choices
[Answer]</li>
<li>gynecologist
[Rationale]
The context mentions a cyst in the pelvic region and the symptoms of the cyst rupturing, including pain and fluid leakage. This suggests that the issue is related to the reproductive or urinary system. Given that the issue is related to the reproductive or urinary system, it is likely that the person would have had to see a specialist in these areas.
Out of the given options, "a gynecologist" is the most likely specialist that the person would have had to see, as gynecologists specialize in female reproductive health, including issues related to the uterus, ovaries, and other reproductive organs.</li>
</ul>
<p>In conclusion, based on the given context, the most likely answer is a gynecologist.</p>
<p>[Example 4]
[Instruction and Question]
Pitmore.</p>
<ul>
<li>Get some seeds</li>
<li>Pick a spot to plant them</li>
<li>Dig a hole in the dirt</li>
<li>Put the seed in the hole</li>
<li>Pour some water on the seed and hole</li>
<li>Cover up the hole</li>
<li>Press down on it</li>
<li>Spray some plant food on it.</li>
</ul>
<p>Question:
Suppose if no water is available happens, how will it affect vegetables?</p>
<ul>
<li>A: more</li>
<li>B: less
[Options]</li>
<li>A</li>
<li>B
[Answer]
B
[Rationale]
The process of planting seeds requires water, which is essential for the growth of plants. Water helps transport nutrients, regulate temperature, and absorb minerals from the soil. If no water is available, the seed will not germinate and the plant will not grow properly, leading to fewer vegetables. Hence, the absence of water will result in "less" vegetables, making the answer "B".</li>
</ul>
<h2>[Example 5]</h2>
<p>[Instruction and Question]
Read the following conversation and answer the question.
M: I'm worried about my sister, Lucy.
W: Why is that?
M: She thinks she's too fat.
W: Is she?
M: No, but she keeps skipping meals. Then, she only eats chips and drinks cola.
W: I used to do that. It's called binge. It was no fan!
M: Why did you stop doing it?
W: Well, my doctor told me to eat when I'm hungry. She said, "Eat till you're full or you'll eat too much later." She said a lot of girls ruin their health this way.
M: Did she say what to eat?
W: She said, "Eat fruit, vegetables, meats and grains. Have regular meals and snacks. Get exercise, too."</p>
<p>Question: What's the relationship between the speakers?</p>
<ul>
<li>Doctor and patient.</li>
<li>Friends.</li>
<li>Brother and sister.
[Options]</li>
<li>Doctor and patient</li>
<li>Friends</li>
<li>Brother and sister
[Answer]
Friends
[Rationale]
The conversation between two people, a man(M) and a woman(W), is discussing the man's sister who has a negative view of her own body and is skipping meals. The woman shares that she used to engage in similar behavior, and explains that her doctor advised her to eat a balanced diet and exercise.</li>
</ul>
<p>Among the three options,</p>
<ul>
<li>First, since the woman is sharing her experience of what the doctor told her, it is unlikely she would be a medical professional. So the answer couldn't be doctor and patient.</li>
<li>Second, given that the man is talking about his sister, the woman is unlikely to be the man's sister.</li>
<li>Lastly, because the conversation is informal and the speakers are discussing a personal matter it is likely that the relationship between the speakers is that of friends.</li>
</ul>
<p>Therefore, the answer is Friends.</p>
<p>[Example 6]
[Instruction and Question]
Question: The best place for the troublemaker to sneak a smoke while he was in where, was in the bathroom?</p>
<h2>Choices:</h2>
<ul>
<li>playground</li>
<li>school</li>
<li>at hotel</li>
<li>theatre</li>
<li>convenience store</li>
</ul>
<p>The rationale to choose ""school"" as the answer is that:
[Options]
Not Given
[Answer]
smokers traditionally use school bathrooms</p>
<p>[Rationale]
Here's the rationale: "smokers traditionally use school bathrooms".
[Example 7]
[Instruction and Question]
So I was looking at quotes for our tehirts for ice hockey, and I came across one that sums up how I'm feeling right now. When you talk about an injury and the kind of depression you go through, it's not just because you're out of shape and you can't go out and play. You're missing a part of you.</p>
<p>Why is the narrator so down in the dumps?
Pick the best answer from the following options:
A. They are unable to play ice curling
B. They are hurt
C. They are unable to play ice skating
D. They are unable to play in the yard
[Options]
$-\mathrm{A}$
$-\mathrm{B}$
$-\mathrm{C}$
$-\mathrm{D}$
[Answer]
B
[Rationale]
The quote in the question mentions the narrator's feelings about an injury and the depression that follows. The narrator says that it's not just about being out of shape and unable to play, but rather they feel like they are missing a part of themselves. This implies that the reason for their depression is linked to their physical inability to participate in a sport or activity they enjoy. Based on the context, it can be inferred that the narrator is upset because they are hurt and unable to play ice hockey.
So, the answer is B.
[Example 8]
[Instruction and Question]
Q. What does the ubiquitous generator use to generate currents?</p>
<h2>Choices:</h2>
<ul>
<li>inertia - water - gravity - magnetism
[Options]</li>
<li>inertia</li>
<li>water</li>
<li>gravity</li>
<li>magnetism
[Answer]
magnetism
[Rationale]
A generator is a device that converts mechanical energy into electrical energy. This is done by using a magnetic field to induce a current in the generator's windings. This current is then sent to an external circuit, where it can be used to power electrical devices. The generator uses magnetism to generate currents, so the answer is magnetism.
[Example 9]
[Instruction and Question]</li>
</ul>
<h2>Prompts 1: Demonstration used for tasks in family of</h2>
<p>MCQA (T0)</p>
<h2>[Example 1]</h2>
<p>[Instruction and Question]
Skylar had stopped breathing but Lee held Skylar's arms anyway, in vain. Given the context: Lee want to do what next?
Possible answers: beg the doctors to try again, get her up and walking home, bring Skylar back
[Options]
- beg the doctors to try again
- get her up and walking home
- bring Skylar back
[Answer]
beg the doctors to try again
[Rationale]
The context of the situation is that Skylar has stopped breathing and Lee is holding Skylar's arms. This suggests that Skylar has received medical attention and is no longer alive.
Based on this, Lee would want to beg the doctors to try again, as this would be the best way to potentially save Skylar's life.
The options of getting Skylar up and walking home or bringing Skylar back do not align with the context, as they would not address the immediate medical emergency at hand.</p>
<p>So the answer is to beg the doctors to try again.
[Example 2]
[Instruction and Question]
Do you think the right answer to the question "what can run on alcoholic fermentation of the glucose in corn or other plants?" is "bicycle?"
[Options]
$-\mathrm{No}$
- Yes
[Answer]
No
[Rationale]
Alcoholic fermentation is a process that occurs in living organisms, most commonly yeast, to convert glucose into energy. This process does not produce enough energy to power something like a bicycle.</p>
<p>So the answer is No.
[Example 3]
[Instruction and Question]
Eventually, the cyst can get so large that part of the wall weakens and fluid begins to leak out, that is, the cyst ruptures. It is similar to popping a water balloon. When this happens, the fluid may irritate the pelvic lining and cause pain, often severe. Most of the time the pain begins to gradually subside over the course of the next few days.
What type of doctor might the person have had to see?</p>
<ul>
<li>A paediatrician - A heart surgeon - a gynecologist - None of the above choices
[Options]</li>
<li>A paediatrician</li>
<li>A heart surgeon</li>
<li>a gynecologist</li>
<li>None of the above choices
[Answer]
a gynecologist
[Rationale]
The context mentions a cyst in the pelvic region and the symptoms of the cyst rupturing, including pain and fluid leakage. This suggests that the issue is related to the reproductive or urinary system. Given that the issue is related to the reproductive or urinary system, it is likely that the person would have had to see a specialist in these areas.
Out of the given options, "a gynecologist" is the most likely specialist that the person would have had to see, as gynecologists specialize in female reproductive health, including issues related to the uterus, ovaries, and other reproductive organs.</li>
</ul>
<p>In conclusion, based on the given context, the most likely answer is a gynecologist.
[Example 4]
[Instruction and Question]
Process:
- Get some seeds
- Pick a spot to plant them
- Dig a hole in the dirt
- Put the seed in the hole
- Pour some water on the seed and hole
- Cover up the hole
- Press down on it
- Spray some plant food on it.</p>
<p>Question:
suppose if no water is available happens, how will it affect vegetables?</p>
<ul>
<li>A: more</li>
<li>B: less
[Options]
$-\mathrm{A}$
$-\mathrm{B}$
[Answer]
B
[Rationale]
The process of planting seeds requires water, which is essential for the growth of plants. Water helps transport nutrients, regulate temperature, and absorb minerals from the soil. If no water is available, the seed will not germinate and the plant will not grow properly, leading to fewer vegetables. Hence, the absence of water will result in "less" vegetables, making the answer "B".
[Example 5]
[Instruction and Question]
Read the following conversation and answer the question.
M. I'm worried about my sister, Lucy.
W. Why is that?
M. She thinks she's too fat.
W. Is she?
M. No, but she keeps skipping meals. Then, she only eats chips and drinks cola.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Moreover, OpenAI announced to stop its support on Codex model starting from June, 2023.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4342 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4342</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4342</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-281243949</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.08032v1.pdf" target="_blank">SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Scientific literature is growing exponentially, creating a critical bottleneck for researchers to efficiently synthesize knowledge. While general-purpose Large Language Models (LLMs) show potential in text processing, they often fail to capture scientific domain-specific nuances (e.g., technical jargon, methodological rigor) and struggle with complex scientific tasks, limiting their utility for interdisciplinary research. To address these gaps, this paper presents SciGPT, a domain-adapted foundation model for scientific literature understanding and ScienceBench, an open source benchmark tailored to evaluate scientific LLMs. Built on the Qwen3 architecture, SciGPT incorporates three key innovations: (1) low-cost domain distillation via a two-stage pipeline to balance performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention mechanism that cuts memory consumption by 55\% for 32,000-token long-document reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to bridge interdisciplinary knowledge gaps. Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in core scientific tasks including sequence labeling, generation, and inference. It also exhibits strong robustness in unseen scientific tasks, validating its potential to facilitate AI-augmented scientific discovery.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4342.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4342.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGPT-RE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGPT Relation Extraction (ScienceBench RE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A relation-extraction component within SciGPT/ScienceBench that uses a Qwen3-8B-based LLM fine-tuned on domain-specific corpora to extract entity-relation triples (causal, compositional, compatibility) from scientific papers and patents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciGPT Relation Extraction (ScienceBench RE)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Sequence-labeling / relation-extraction pipeline implemented by fine-tuning the Qwen3-8B base model with a two-stage supervised fine-tuning (SFT) curriculum followed by Direct Preference Optimization (DPO). Stage 1 SFT emphasizes structured understanding tasks (NER, RE, knowledge linking) using curated instances; Stage 2 addresses generation-heavy tasks. Training used QLoRA/LoRA (rank 64), batch sizes and learning schedules noted in the paper. The RE task is framed as extracting (head entity, relation type, tail entity) triples from full-text or long fragments, evaluated against human-annotated gold labels in ScienceBench.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B (Qwen3 architecture, ~7B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multidisciplinary scientific literature (Science papers and patent documents; includes biomedical and other domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>1,200 RE samples from Science papers (task description); dataset table reports 35,180 RE instances (zh) as part of ScienceBench</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Entity relationships and relational patterns (causal, compositional, compatibility relations) — extraction of structured relational patterns rather than explicit physical scaling laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured entity-relation triples (head, relation type, tail); token-level sequence labels for entities</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation against annotated gold labels using Micro-F1; expert-driven assessments and pairwise few-shot comparisons versus GPT-4 as reported in ScienceBench</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Micro-F1: 0.667 for SciGPT on RE (ScienceBench); compared to GPT-4 Micro-F1 0.385 (reported 73.2% relative improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to GPT-4 on ScienceBench RE: SciGPT Micro-F1 0.667 vs GPT-4 0.385 (reported in paper). No other baseline details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Performance depends on domain coverage of training data and ontologies; degradation observed in niche fields with scarce data; potential sensitivity to annotation quality and long-context truncation (max sequence length used during SFT = 1024 tokens); paper notes need for ontologies to bridge interdisciplinary gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4342.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGPT-RelPredict</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGPT Relationship Prediction (ScienceBench Relationship Predict)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SciGPT task that predicts innovative knowledge relations and classifies involved entities and types from scientific and patent texts using the domain-adapted Qwen3-8B model fine-tuned on labeled relation-prediction examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciGPT Relationship Prediction (ScienceBench)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Framed as an information-extraction + classification problem: given a text fragment, the model outputs the extracted innovative entities and their types (a list of entity-type pairs). Implemented by supervised fine-tuning on curated relation-prediction examples in Stage 1 SFT and refined with DPO preference data; evaluated by accuracy/F1 against human annotations in ScienceBench.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B (Qwen3 architecture, ~7B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Academic papers and patent documents (multidisciplinary)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Task description: 600 text fragments from academic and patent documents; dataset table reports 885 (zh) instances for Relation Predict</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Predicted innovative relations / knowledge claims (qualitative relations and structured novel-knowledge mentions); not explicit numerical/physical laws in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured lists of entities and entity types (textual structured output)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Accuracy/F1 computed against human-annotated ground truth; DPO training uses human-annotated preference pairs and GPT-4 judged AI pairs for cross-validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy: 0.52650 for SciGPT on Relationship Predict (ScienceBench) versus GPT-4 0.334 (reported in paper / Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4 (accuracy 0.334) — SciGPT shows higher accuracy (0.52650). No other numerical baselines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limited dataset sizes for novel-knowledge prediction; sensitivity to annotation definitions of 'innovative knowledge'; declines in performance in low-data niche domains; evaluation limited to labelled fragments rather than end-to-end experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4342.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGPT-KFusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGPT Knowledge Fusion (ScienceBench Knowledge Fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SciGPT capability to integrate cross-domain classification systems and identify equivalence/inclusion relationships across different domain taxonomies, implemented and evaluated within ScienceBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciGPT Knowledge Fusion (ScienceBench)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Model ingests a 'main classification' and related classification systems, then outputs fused mappings (equivalence, inclusion, conflicts) using supervised fine-tuning on cross-domain classification samples; uses domain ontologies (e.g., MeSH, CAS identifiers) during adaptation to improve mappings. Training pipeline includes Stage 1 SFT for structured linking tasks and DPO for preference-aligned outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B (Qwen3 architecture, ~7B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (multiple disciplines; datasets drawn from different fields' classification systems)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset contains 500 classification-system samples (ScienceBench Knowledge Fusion task); not specified as full papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Identification of equivalence/inclusion relationships between classification systems (structural knowledge fusion rather than numeric laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured mappings between classification entries (textual structured outputs indicating equivalence/inclusion/conflict)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>F1 accuracy versus annotated ground truth in ScienceBench; expert annotations used in dataset creation/evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>F1: 0.5576 (reported in body) / 0.558 (Table 2) for SciGPT on Knowledge Fusion; GPT-4 reported F1 0.461 (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4 (F1 0.461); SciGPT shows ~12.4% absolute improvement (0.5576 vs 0.461) per paper text</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Handling repetition, conflicts, and vague boundaries in classification systems; relies on availability and quality of domain ontologies and standardized terms; performance degrades when ontologies are incomplete or inconsistent across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4342.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciGPT-KLink</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciGPT Knowledge Linking / Knowledge Link Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SciGPT subtask for linking entities across documents and ontologies (knowledge linking) to support cross-document reasoning and cross-domain entity alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciGPT Knowledge Linking</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Supervised fine-tuning on curated knowledge-linking instances (Stage 1 SFT) with ontology integration; model outputs cross-document/entity-to-ontology links. The pipeline uses LoRA-based adaptation and DPO preference tuning for higher fidelity in terminological consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen3-8B (Qwen3 architecture, ~7B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multidisciplinary scientific literature (including biomedical and patent domains where ontologies like MeSH and CAS were integrated)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset entry for Knowledge Linking: 23,110 (zh) instances listed in the paper's data table; not specified as number of full papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Cross-document entity equivalence/links (structural knowledge linking rather than explicit quantitative laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured entity links and ontology mappings</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>F1 evaluation against annotated ground truth in ScienceBench; expert verification used in data curation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Knowledge Linking F1: 0.683 (SciGPT) vs GPT-4 0.491 (Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared with GPT-4 baseline (F1 0.491) showing substantial improvement for SciGPT (0.683)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Depends on completeness and coverage of integrated ontologies; multilingual alignment challenges; potential for incorrect mapping when metadata is incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4342.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system (title cited in references) for iterative research idea generation over scientific literature; mentioned in related work but not described in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Researchagent (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as a prior system for iterative idea generation from literature; SciGPT paper does not provide operational details, dataset sizes, or evaluation metrics for this system.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4342.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autosurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system/paper that demonstrates automatic survey-writing from literature using LLMs; mentioned in related work without operational detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Autosurvey (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as an example of LLMs applied to automated literature survey generation; no implementation details or performance numbers are provided in the SciGPT paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4342.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZeroShot-Hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are zero shot hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced claim/paper (title listed) that LLMs can propose hypotheses in zero-shot settings; cited by the authors as evidence that LLMs can learn meaningful world models and propose hypotheses from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero shot hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero-shot hypothesis proposing (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in related work to support the premise that LLMs can generate hypotheses from text; SciGPT paper does not describe the method's specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4342.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scipip</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scipip: An llm-based scientific paper idea proposer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced LLM-based system (title listed) for proposing scientific paper ideas from literature; cited in the related work list without further detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scipip: An llm-based scientific paper idea proposer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Scipip (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as an example of applying LLMs to generate research ideas from literature; no operational implementation or evaluation details are provided in SciGPT paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4342.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated-lit-review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated literature research and review-generation method based on large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work (title listed) describing automated literature research and review generation using LLMs; included in the related-work citations without operational detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated literature research and review-generation method based on large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Automated literature research and review-generation (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as prior art for literature review automation with LLMs; SciGPT paper does not provide method specifics, datasets, or metrics for this referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4342.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced toolkit for scientific literature review based on LLMs; cited in the related-work list but not described further in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LitLLM (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as an LLM toolkit for literature review tasks; no implementation, domain, or evaluation details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4342.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cycleresearcher: Improving automated research via automated review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system for improving automated research through automated review using LLMs; listed in related work without further details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cycleresearcher: Improving automated research via automated review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Cycleresearcher (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as prior work on automated review to improve automated research workflows; SciGPT paper does not provide operational specifics or metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4342.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4342.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScilitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scilitllm: How to adapt llms for scientific literature understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work on adapting LLMs for scientific literature understanding; cited in related work but not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scilitllm: How to adapt llms for scientific literature understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ScilitLLM (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as an example of adapting LLMs for literature understanding; the SciGPT paper does not supply details about its methods or results.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Scipip: An llm-based scientific paper idea proposer <em>(Rating: 2)</em></li>
                <li>Autosurvey: Large language models can automatically write surveys <em>(Rating: 2)</em></li>
                <li>Automated literature research and review-generation method based on large language models <em>(Rating: 2)</em></li>
                <li>Litllm: A toolkit for scientific literature review <em>(Rating: 1)</em></li>
                <li>Cycleresearcher: Improving automated research via automated review <em>(Rating: 1)</em></li>
                <li>Scilitllm: How to adapt llms for scientific literature understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4342",
    "paper_id": "paper-281243949",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "SciGPT-RE",
            "name_full": "SciGPT Relation Extraction (ScienceBench RE)",
            "brief_description": "A relation-extraction component within SciGPT/ScienceBench that uses a Qwen3-8B-based LLM fine-tuned on domain-specific corpora to extract entity-relation triples (causal, compositional, compatibility) from scientific papers and patents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SciGPT Relation Extraction (ScienceBench RE)",
            "method_description": "Sequence-labeling / relation-extraction pipeline implemented by fine-tuning the Qwen3-8B base model with a two-stage supervised fine-tuning (SFT) curriculum followed by Direct Preference Optimization (DPO). Stage 1 SFT emphasizes structured understanding tasks (NER, RE, knowledge linking) using curated instances; Stage 2 addresses generation-heavy tasks. Training used QLoRA/LoRA (rank 64), batch sizes and learning schedules noted in the paper. The RE task is framed as extracting (head entity, relation type, tail entity) triples from full-text or long fragments, evaluated against human-annotated gold labels in ScienceBench.",
            "llm_model_used": "Qwen3-8B (Qwen3 architecture, ~7B parameters)",
            "scientific_domain": "Multidisciplinary scientific literature (Science papers and patent documents; includes biomedical and other domains)",
            "number_of_papers": "1,200 RE samples from Science papers (task description); dataset table reports 35,180 RE instances (zh) as part of ScienceBench",
            "type_of_quantitative_law": "Entity relationships and relational patterns (causal, compositional, compatibility relations) — extraction of structured relational patterns rather than explicit physical scaling laws",
            "extraction_output_format": "Structured entity-relation triples (head, relation type, tail); token-level sequence labels for entities",
            "validation_method": "Evaluation against annotated gold labels using Micro-F1; expert-driven assessments and pairwise few-shot comparisons versus GPT-4 as reported in ScienceBench",
            "performance_metrics": "Micro-F1: 0.667 for SciGPT on RE (ScienceBench); compared to GPT-4 Micro-F1 0.385 (reported 73.2% relative improvement)",
            "baseline_comparison": "Compared directly to GPT-4 on ScienceBench RE: SciGPT Micro-F1 0.667 vs GPT-4 0.385 (reported in paper). No other baseline details provided.",
            "challenges_limitations": "Performance depends on domain coverage of training data and ontologies; degradation observed in niche fields with scarce data; potential sensitivity to annotation quality and long-context truncation (max sequence length used during SFT = 1024 tokens); paper notes need for ontologies to bridge interdisciplinary gaps.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4342.0",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "SciGPT-RelPredict",
            "name_full": "SciGPT Relationship Prediction (ScienceBench Relationship Predict)",
            "brief_description": "A SciGPT task that predicts innovative knowledge relations and classifies involved entities and types from scientific and patent texts using the domain-adapted Qwen3-8B model fine-tuned on labeled relation-prediction examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SciGPT Relationship Prediction (ScienceBench)",
            "method_description": "Framed as an information-extraction + classification problem: given a text fragment, the model outputs the extracted innovative entities and their types (a list of entity-type pairs). Implemented by supervised fine-tuning on curated relation-prediction examples in Stage 1 SFT and refined with DPO preference data; evaluated by accuracy/F1 against human annotations in ScienceBench.",
            "llm_model_used": "Qwen3-8B (Qwen3 architecture, ~7B parameters)",
            "scientific_domain": "Academic papers and patent documents (multidisciplinary)",
            "number_of_papers": "Task description: 600 text fragments from academic and patent documents; dataset table reports 885 (zh) instances for Relation Predict",
            "type_of_quantitative_law": "Predicted innovative relations / knowledge claims (qualitative relations and structured novel-knowledge mentions); not explicit numerical/physical laws in the paper",
            "extraction_output_format": "Structured lists of entities and entity types (textual structured output)",
            "validation_method": "Accuracy/F1 computed against human-annotated ground truth; DPO training uses human-annotated preference pairs and GPT-4 judged AI pairs for cross-validation",
            "performance_metrics": "Accuracy: 0.52650 for SciGPT on Relationship Predict (ScienceBench) versus GPT-4 0.334 (reported in paper / Table 2)",
            "baseline_comparison": "Compared to GPT-4 (accuracy 0.334) — SciGPT shows higher accuracy (0.52650). No other numerical baselines provided.",
            "challenges_limitations": "Limited dataset sizes for novel-knowledge prediction; sensitivity to annotation definitions of 'innovative knowledge'; declines in performance in low-data niche domains; evaluation limited to labelled fragments rather than end-to-end experimental validation.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4342.1",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "SciGPT-KFusion",
            "name_full": "SciGPT Knowledge Fusion (ScienceBench Knowledge Fusion)",
            "brief_description": "A SciGPT capability to integrate cross-domain classification systems and identify equivalence/inclusion relationships across different domain taxonomies, implemented and evaluated within ScienceBench.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SciGPT Knowledge Fusion (ScienceBench)",
            "method_description": "Model ingests a 'main classification' and related classification systems, then outputs fused mappings (equivalence, inclusion, conflicts) using supervised fine-tuning on cross-domain classification samples; uses domain ontologies (e.g., MeSH, CAS identifiers) during adaptation to improve mappings. Training pipeline includes Stage 1 SFT for structured linking tasks and DPO for preference-aligned outputs.",
            "llm_model_used": "Qwen3-8B (Qwen3 architecture, ~7B parameters)",
            "scientific_domain": "Cross-domain (multiple disciplines; datasets drawn from different fields' classification systems)",
            "number_of_papers": "Dataset contains 500 classification-system samples (ScienceBench Knowledge Fusion task); not specified as full papers",
            "type_of_quantitative_law": "Identification of equivalence/inclusion relationships between classification systems (structural knowledge fusion rather than numeric laws)",
            "extraction_output_format": "Structured mappings between classification entries (textual structured outputs indicating equivalence/inclusion/conflict)",
            "validation_method": "F1 accuracy versus annotated ground truth in ScienceBench; expert annotations used in dataset creation/evaluation",
            "performance_metrics": "F1: 0.5576 (reported in body) / 0.558 (Table 2) for SciGPT on Knowledge Fusion; GPT-4 reported F1 0.461 (paper)",
            "baseline_comparison": "Compared to GPT-4 (F1 0.461); SciGPT shows ~12.4% absolute improvement (0.5576 vs 0.461) per paper text",
            "challenges_limitations": "Handling repetition, conflicts, and vague boundaries in classification systems; relies on availability and quality of domain ontologies and standardized terms; performance degrades when ontologies are incomplete or inconsistent across domains.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4342.2",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "SciGPT-KLink",
            "name_full": "SciGPT Knowledge Linking / Knowledge Link Extraction",
            "brief_description": "A SciGPT subtask for linking entities across documents and ontologies (knowledge linking) to support cross-document reasoning and cross-domain entity alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SciGPT Knowledge Linking",
            "method_description": "Supervised fine-tuning on curated knowledge-linking instances (Stage 1 SFT) with ontology integration; model outputs cross-document/entity-to-ontology links. The pipeline uses LoRA-based adaptation and DPO preference tuning for higher fidelity in terminological consistency.",
            "llm_model_used": "Qwen3-8B (Qwen3 architecture, ~7B parameters)",
            "scientific_domain": "Multidisciplinary scientific literature (including biomedical and patent domains where ontologies like MeSH and CAS were integrated)",
            "number_of_papers": "Dataset entry for Knowledge Linking: 23,110 (zh) instances listed in the paper's data table; not specified as number of full papers",
            "type_of_quantitative_law": "Cross-document entity equivalence/links (structural knowledge linking rather than explicit quantitative laws)",
            "extraction_output_format": "Structured entity links and ontology mappings",
            "validation_method": "F1 evaluation against annotated ground truth in ScienceBench; expert verification used in data curation",
            "performance_metrics": "Knowledge Linking F1: 0.683 (SciGPT) vs GPT-4 0.491 (Table 2)",
            "baseline_comparison": "Compared with GPT-4 baseline (F1 0.491) showing substantial improvement for SciGPT (0.683)",
            "challenges_limitations": "Depends on completeness and coverage of integrated ontologies; multilingual alignment challenges; potential for incorrect mapping when metadata is incomplete.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4342.3",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Researchagent",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "An LLM-based system (title cited in references) for iterative research idea generation over scientific literature; mentioned in related work but not described in detail in this paper.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "method_name": "Researchagent (as cited)",
            "method_description": "Cited as a prior system for iterative idea generation from literature; SciGPT paper does not provide operational details, dataset sizes, or evaluation metrics for this system.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.4",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Autosurvey",
            "name_full": "Autosurvey: Large language models can automatically write surveys",
            "brief_description": "A referenced system/paper that demonstrates automatic survey-writing from literature using LLMs; mentioned in related work without operational detail in this paper.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys",
            "mention_or_use": "mention",
            "method_name": "Autosurvey (as cited)",
            "method_description": "Mentioned as an example of LLMs applied to automated literature survey generation; no implementation details or performance numbers are provided in the SciGPT paper.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.5",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ZeroShot-Hypothesis",
            "name_full": "Large language models are zero shot hypothesis proposers",
            "brief_description": "A referenced claim/paper (title listed) that LLMs can propose hypotheses in zero-shot settings; cited by the authors as evidence that LLMs can learn meaningful world models and propose hypotheses from text.",
            "citation_title": "Large language models are zero shot hypothesis proposers",
            "mention_or_use": "mention",
            "method_name": "Zero-shot hypothesis proposing (as cited)",
            "method_description": "Mentioned in related work to support the premise that LLMs can generate hypotheses from text; SciGPT paper does not describe the method's specifics.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.6",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Scipip",
            "name_full": "Scipip: An llm-based scientific paper idea proposer",
            "brief_description": "A referenced LLM-based system (title listed) for proposing scientific paper ideas from literature; cited in the related work list without further detail in this paper.",
            "citation_title": "Scipip: An llm-based scientific paper idea proposer",
            "mention_or_use": "mention",
            "method_name": "Scipip (as cited)",
            "method_description": "Cited as an example of applying LLMs to generate research ideas from literature; no operational implementation or evaluation details are provided in SciGPT paper.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.7",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Automated-lit-review",
            "name_full": "Automated literature research and review-generation method based on large language models",
            "brief_description": "A referenced work (title listed) describing automated literature research and review generation using LLMs; included in the related-work citations without operational detail in this paper.",
            "citation_title": "Automated literature research and review-generation method based on large language models",
            "mention_or_use": "mention",
            "method_name": "Automated literature research and review-generation (as cited)",
            "method_description": "Mentioned as prior art for literature review automation with LLMs; SciGPT paper does not provide method specifics, datasets, or metrics for this referenced work.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.8",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "Litllm: A toolkit for scientific literature review",
            "brief_description": "A referenced toolkit for scientific literature review based on LLMs; cited in the related-work list but not described further in this paper.",
            "citation_title": "Litllm: A toolkit for scientific literature review",
            "mention_or_use": "mention",
            "method_name": "LitLLM (as cited)",
            "method_description": "Cited as an LLM toolkit for literature review tasks; no implementation, domain, or evaluation details are provided in this paper.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.9",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "CycleResearcher",
            "name_full": "Cycleresearcher: Improving automated research via automated review",
            "brief_description": "A referenced system for improving automated research through automated review using LLMs; listed in related work without further details in this paper.",
            "citation_title": "Cycleresearcher: Improving automated research via automated review",
            "mention_or_use": "mention",
            "method_name": "Cycleresearcher (as cited)",
            "method_description": "Mentioned as prior work on automated review to improve automated research workflows; SciGPT paper does not provide operational specifics or metrics.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.10",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "ScilitLLM",
            "name_full": "Scilitllm: How to adapt llms for scientific literature understanding",
            "brief_description": "A referenced work on adapting LLMs for scientific literature understanding; cited in related work but not detailed in this paper.",
            "citation_title": "Scilitllm: How to adapt llms for scientific literature understanding",
            "mention_or_use": "mention",
            "method_name": "ScilitLLM (as cited)",
            "method_description": "Cited as an example of adapting LLMs for literature understanding; the SciGPT paper does not supply details about its methods or results.",
            "llm_model_used": null,
            "scientific_domain": null,
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": null,
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": null,
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4342.11",
            "source_info": {
                "paper_title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Scipip: An llm-based scientific paper idea proposer",
            "rating": 2,
            "sanitized_title": "scipip_an_llmbased_scientific_paper_idea_proposer"
        },
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Automated literature research and review-generation method based on large language models",
            "rating": 2,
            "sanitized_title": "automated_literature_research_and_reviewgeneration_method_based_on_large_language_models"
        },
        {
            "paper_title": "Litllm: A toolkit for scientific literature review",
            "rating": 1,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Cycleresearcher: Improving automated research via automated review",
            "rating": 1,
            "sanitized_title": "cycleresearcher_improving_automated_research_via_automated_review"
        },
        {
            "paper_title": "Scilitllm: How to adapt llms for scientific literature understanding",
            "rating": 1,
            "sanitized_title": "scilitllm_how_to_adapt_llms_for_scientific_literature_understanding"
        }
    ],
    "cost": 0.01516855,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery
September 11, 2025</p>
<p>Fengyu She shefengyu@foxmail.com 
Ziyi Wan
Jingmian WangChang Wang</p>
<p>Nan Wang 
Ziyi Wan
Jingmian WangChang Wang</p>
<p>Hongfei Wu 
Ziyi Wan
Jingmian WangChang Wang</p>
<p>SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery
September 11, 202590B3DD1FA47C91E285666623572F2296arXiv:2509.08032v1[cs.CL]
Scientific literature is growing exponentially, creating a critical bottleneck for researchers to efficiently synthesize knowledge.While general-purpose Large Language Models (LLMs) show potential in text processing, they often fail to capture scientific domain-specific nuances (e.g., technical jargon, methodological rigor) and struggle with complex scientific tasks, limiting their utility for interdisciplinary research.To address these gaps, this paper presents SciGPT-a domain-adapted foundation model for scientific literature understanding-and ScienceBench, an open-source benchmark tailored to evaluate scientific LLMs.Built on the Qwen3 architecture, SciGPT incorporates three key innovations: (1) low-cost domain distillation via a two-stage pipeline to balance performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention mechanism that cuts memory consumption by 55% for 32,000-token long-document reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to bridge interdisciplinary knowledge gaps.Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in core scientific tasks including sequence labeling, generation, and inference.It also exhibits strong robustness in unseen scientific tasks, validating its potential to facilitate AI-augmented scientific discovery.</p>
<p>Introduction</p>
<p>The exponential growth of scientific literature-over 70 million peer-reviewed articles published annually-has created a critical bottleneck for researchers seeking to synthesize knowledge efficiently.Large language models (LLMs) have demonstrated transformative potential in this domain, with capabilities to encode specialized terminology, reason over multi-document contexts, and generate actionable insights.Recent studies suggest LLMs are not merely reproducing surface statistics but learning meaningful world models [1,2,3], while their performance follows a scaling law where more pretraining data and parameters yield better results [4,5,6,7].However, adapting LLMs to scientific tasks requires addressing two systemic challenges.</p>
<p>General-purpose LLMs often fail to capture nuanced methodologies, technical jargon, or citation patterns unique to scientific writing.These limitations hinder their ability to support interdisciplinary research, where integrating knowledge across fields (e.g., linking biomedical findings to materials science applications) demands deep contextual understanding.For instance, a model tasked with designing experiments at the intersection of chemistry and machine learning must simultaneously master domain-specific constraints and methodological synergies [8,9,10].Scientific documents frequently exceed 10,000 words and span diverse disciplines, requiring architectures optimized for long-context reasoning while preserving cross-domain semantic coherence.For example, analyzing full-length patent filings or comparative studies in clinical trials demands not only memory-efficient processing but also precise retention of methodological details critical to downstream validation [11,12,13].</p>
<p>Existing benchmarks like PatentBench highlight the importance of domain-centric evaluation frameworks, yet no standardized benchmark exists for scientific literature.[14] Current approaches rely on simplified tasks (e.g., abstract summarization) or short-answer metrics, which inadequately reflect real-world research demands such as full-paper critique or interdisciplinary hypothesis formulation.</p>
<p>To bridge this gap, we present SciGPT, a foundation model for multidisciplinary scientific document processing, and ScienceBench.SciGPT builds on the qwen3 architecture but incorporates three key innovations tailored to scientific workflows: Low-cost distillation via Qwen-DS Leveraging a twostage domain adaptation pipeline, SciGPT combines pretraining on open-access publications with fine-tuning, enabling scalable deployment for resource-constrained settings.This approach directly addresses the challenge of interdisciplinary knowledge fusion [15,16].SMoE-based long-document processing An attention mechanism with sparse mixture-of-experts (SMoE) reduces key-value cache memory consumption by 55%, supporting efficient reasoning over 32,000-token documents while preserving methodological rigor.This is critical for tasks like patent-to-research paper alignment, where crossreferencing spans thousands of tokens.Knowledge-aware adaptation, by integrating domain-specific ontologies (e.g., MeSH terms in biomedicine, Chemical Abstracts Service identifiers) into training data [17,18,19], SciGPT explicitly bridges knowledge gaps between disciplines.This enables applications like cross-domain entity linking.</p>
<p>ScienceBench evaluates models across six dimensions: factual accuracy, methodological rigor, crossreference coherence, user interaction quality, computational efficiency, and interdisciplinary generalization.Experimental results show SciGPT performs significantly better than GPT-4 on complex tasks such as "designing experiments from literature gaps" and "translating technical protocols into lay summaries," with a marked improvement in both task completion quality and alignment with scientific workflow demands.By open-sourcing both the model and benchmark, we aim to catalyze advancements in AI-augmented scientific discovery while establishing rigorous standards for domain-specific LLM development.</p>
<p>Methodology</p>
<p>To construct and train the SciGPT model for scientific scenarios, we first integrated academic papers, patent documents, and expert-validated synthetic data to build a multi-source scientific training dataset.Subsequently, we defined core scientific tasks including Named Entity Recognition (NER), Relation Extraction (RE), and cross-domain knowledge fusion, and specifically proposed the Science Benchmarks framework to evaluate the model's scientific capabilities.</p>
<p>Data Collection</p>
<p>To construct a comprehensive scientific corpus, we aggregated heterogeneous resources from three primary sources: (1) public scientific corpora, (2) domain-specific repositories, and (3) synthetic data generated through rule-based augmentation and GPT-4-assisted paraphrasing.The dataset spans 796,981 instruction-response pairs across patent analysis (18.7% of total data) and science paper processing (53.5%), with the remaining 27.8% dedicated to general dialogue and cross-domain reasoning.For multilingual coverage, we curated English-Chinese technical documents from patent filings and academic publications.To maintain scientific rigor, all training instances underwent verification for factual accuracy and structural integrity.</p>
<p>Data Cleaning: To construct domain-specific training data for scientific natural language processing, we implemented a systematic pipeline across variety heterogeneous data modalities.The data cleaning process began with hybrid filtering, integrated rule-based methods (keyword blacklists, language proportion thresholds) with GPT-4 classification to remove non-scientific content, toxic language, and personally identifiable information via regular expressions.Metadata validation subsequently filtered documents with incomplete abstracts or invalid Digital Object Identifiers to reducing dataset noise, fuzzy deduplication detected near-duplicates via MinHash comparisons of full-text sections.After cleaning, data source proportions were balanced to create a diverse dataset comprising academic literature, patent documents, and synthetic data.Finally, training samples were sequenced by difficulty</p>
<p>Scientific Tasks and Benchmarks</p>
<p>To address the lack of specialization in scientific domain evaluation, we propose ScienceBench, a comprehensive benchmark that for the first time incorporates task-specific 场景定义 tailored to real scientific workflows, along with clear data sources and annotation standards.ScienceBench covers nine core tasks, each with detailed scenario descriptions, data provenance, and evaluation metrics, designed to comprehensively assess the capabilities of scientific LLMs.</p>
<p>Named Entity Recognition (NER): Predefined technical entities in scientific literature are identified with strict matching requirements.The evaluation is conducted using the F1 score, emphasizing precise alignment of entity text, type, and cross-sentence span, while ensuring terminology consistency.</p>
<p>从以下文本中识别实体，返回实体提及及其类型 "需要额外的序列生成逻辑,需要序列生成模块的执行效 率极高,解决上述问题,需要说明的是,不冲突的,不同测 试和应用需求,不同的测,解决了多通道传输链路硬件测 试序,解决了高速串行数据传输通道测试难度,解决了序 "</p>
<p>{\"mentions\": [{\"mention\": \"高速\", \"type\": \"速 度\"}, {\"mention\": \"多通道\", \"type\": \"数量\"}, {\"mention\": \"执行效率\", \"type\": \"运行效率\"}, {\"mention\": \"不冲突\", \"type\": \"兼容性\"}, {\"mention\": \"高速串行数据传输通道测试\", \"type\": \"测量难度\"}, {\"mention\": \"传输链路\", \"type\": \"运行效率\"}, {\"mention\": \"串行\", \"type\": \"运行效率\"}, {\"mention\": \"测试难度\", \"type\": \"测量难度\"}]}</p>
<p>Named Entity Recognition</p>
<p>使用能够产生1.5kW辐射能的一个或多个石英钨灯泡 的烘箱，其中显着部分是在0.4至0.7μm波长范围内的 光能，其将高强度可见光波辐射直接照射到食品上。 光源可以位于食物的上方和下方，并且烤箱的内壁优 选地是高反射性的以将光能反射到食物上。 可见光源 的强度是可自动控制的，并且可以在烹饪周期内变化。</p>
<p>An oven using one or more quartz tungsten light bulbs capable of producing 1.5 kW of radiant energy of which a significant portion is light energy in the 0. "{\"relations\": [{\"from\": \"微小卫星\", \"to\": \"芯片 \", \"relation\": \"similar\"}, {\"from\": \"微小卫星\", \"to\": \"芯片\", \"relation\": \"similar\"}]}"</p>
<p>Relationship Complete</p>
<p>Figure 2: Examples of questions.</p>
<p>The dataset comprises 500 samples sourced from Chinese and English academic papers and patent documents, each containing annotated entities and their corresponding types.</p>
<p>Relation Extraction (RE): Extracts entity relationship triples (head entity, relation type, tail entity) with a focus on causal (e.g., "gene A regulates protein B"), compositional (e.g., "material X consists of component Y"), and compatibility relations (e.g., "method A is applicable to dataset B").Data includes 1,200 samples from Science papers, evaluated by Micro-F1.</p>
<p>Abstractive Summarization: Compresses complex method descriptions (e.g., experimental procedures, patent claims) into high-precision summaries while retaining key parameters and logical dependencies.ROUGE-L is used for evaluation.The dataset contains 500 Chinese and English scientific literature fragments, each with a corresponding high-quality summary as a reference.</p>
<p>Machine Translation: Achieve accurate cross-lingual conversion of scientific literature, ensuring the accuracy of numerical values and patent terms.BLEU-4 is adopted for evaluation.The dataset covers 300 Chinese-English parallel scientific paper abstracts and patent abstracts, totaling 600 samples.</p>
<p>Relationship Complete: Infer implicit entity associations from the context.The dataset includes 400 samples, all selected from academic papers and patent documents.Each sample contains the entity associations to be inferred and relevant contextual information, with accuracy used for evaluation.</p>
<p>Semantic Matching: Focus on judging whether the technical features of different patents are similar, requiring precise comparison of technical elements in patents.The input is a list of technical features of two patents, and the matching result is obtained by analyzing the overlapping points and differences of the technical features.The dataset contains 800 sets of patent comparison samples selected from patent databases, each sample is annotated with detailed matching points and differences, and F1 is used for evaluation.</p>
<p>Relation Prediction: Predict innovative knowledge in the text, extract the involved entities and their corresponding types, and construct a list of entity types of innovative knowledge.The input is a text containing innovative knowledge, and the output is entity information including "mentioned content" and "type".This task requires accurate identification of innovative elements and their attributes in the text, and clear classification of entities.The dataset contains 600 text fragments from academic and patent documents, each annotated with corresponding entities and types, and F1 accuracy is used for evaluation.</p>
<p>Knowledge Fusion: Focus on the integration of cross-domain classification systems, maintain the core position of the main classification, and identify equivalent or inclusion relationships between different systems.The input is the main classification, and the output is the knowledge fusion result, clarifying the association between the main classification and related classifications.This task needs to handle issues such as repetition, conflict, and vague boundaries in the classification system, and use standardized terms for integration.The dataset contains 500 sets of classification system samples from different fields, each with clear main classification and related classification information, and F1 accuracy is used for evaluation.</p>
<p>Topic Modeling: Identify core topics and technical features from technical texts, and extract key topic words.The input is a technical text, and the output is a list of professional terms in a mix of Chinese and English, which is required to contain 3-7 core topic words, accurately reflecting the core technical concepts and key innovation points in the text.This task emphasizes the accurate extraction of professional domain terms and maintains the standardization of terms in the original text.The dataset includes 700 Chinese and English technical documents, each annotated with corresponding core topic words, and BLEU is used for evaluation.</p>
<p>Summary to Title: Generate appropriate titles based on paper abstracts, requiring accurate summarization of the core content of the abstracts and highlighting research focuses.The input is a paper abstract, and the output is a concise and precise title that can reflect the main research objects and contents of the abstract.This task needs to grasp key information in the abstract, such as research topics, research contents, and research results.The dataset contains 400 paper abstracts from different disciplines and their corresponding titles, with evaluation based on the matching degree and information coverage between the title and the abstract, including BLEU scores and manual ratings.</p>
<p>Training</p>
<p>We pretrained SciGPT models based on the Qwen3-8B, a robust multilingual language model pretrained on diverse corpora including web texts, academic literature, code, and domain-specific resources.Qwen3-8B's 7 billion parameters balance computational efficiency and performance, making it well-suited for resource-constrained environments while retaining strong cross-domain generalization capabilities.Its proven superiority in natural language understanding and generation tasks on public benchmarks, combined with multilingual and multi-modal support, aligns with the cross-lingual requirements of scientific literature analysis.Building on this foundation, we designed a structured finetuning pipeline consisting of two stages: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).This pipeline explicitly encodes domain knowledge while optimizing for humanaligned generation through preference learning, aimed at enhancing scientific literature understanding, domain-specific reasoning, and task-oriented generation.</p>
<p>Supervised Fine-Tuning</p>
<p>Given the inherent complexity of scientific texts, including technical ambiguity and domain-specific knowledge requirements.we developed a structured two-stage supervised instruction fine-tuning (SFT) framework for SciGPT.Which systematically builds foundational competencies before advancing to complex we developed a structured two-stage supervised instruction fine-tuning (SFT) framework for SciGPT.generativetasks.The Stage 1 focuses on structured understanding tasks requiring precise technical language parsing and metadata extraction, including patent named entity recognition, relation extraction from SCI papers, knowledge linking operations, and English-Chinese cross-lingual alignment.This phase incorporates 340,000 curated instances specifically selected to strengthen the model's ability to resolve domain-specific ambiguities in terminology, maintain consistency across multilingual scientific expressions, and extract structured information from heterogeneous document formats.In the Stage 2, the model transitioned to generation-intensive tasks, including abstract-to-title summarization, QA, and general-domain dialogue generation.Here, the model integrated retrospective data from Stage 1 with 490,000 new instances covering scientific paper summary generation, patent abstract titling, and dialogue datasets.This staged approach prioritized simpler structured tasks first, ensuring robust knowledge grounding before advancing to complex generative tasks that require coherence and contextual reasoning.</p>
<p>The training process utilized one A800 and one L40s GPUs with QLoRA for memory-efficient finetuning, ensuring scalability while maintaining performance.Key hyperparameters included a batch size of 12 per GPU, a learning rate of 2 × 10 −4 , and a warmup ratio of 0.1, with a max sequence length of 1024 tokens to handle long-form scientific content.LoRA parameters were set to a rank of 64 and dropout of 0.05 to optimize adaptation.</p>
<p>Direct Preference Optimization</p>
<p>Our DPO framework constructs a hybrid dataset P consisting of 9,000 high -quality preference pairs, integrating human expertise and AI -generated feedback to enhance model performance.The dataset is composed of two parts: 3,000 human -annotated pairs from domain experts.The annotation criteria are comprehensive, covering multiple dimensions such as factual consistency with source documents, accuracy of technical term usage, and rigor of logical reasoning.In this regard, annotators are required to meticulously compare the text with the original scientific literature to ensure that all information is accurate.For example, when dealing with a scientific paper on named entity recognition task, the annotated content about entities and relationships must precisely match the details in the paper.</p>
<p>The use of scientific terminology must conform to the authoritative definitions within relevant disciplinary fields.If an annotation involves terms in the field of biology, like "gene expression", it should be used in accordance with the established biological nomenclature.Logical reasoning rigor is judged based on the common paradigms of scientific research.For instance, in an argument about the causal relationship between two scientific phenomena, the reasoning process should follow the principles of scientific induction and deduction.</p>
<p>The remaining 6,000 AI-generated pairs utilize GPT-4 as a judgment system.Cross-validation is carried out to ensure the agreement rate with human annotators.Special attention is paid to key elements in the terminology standardization and professional accuracy of machine translation tasks.</p>
<p>In a patent document involving technical translation between Chinese and English, the AI-generated content should use domain-specific terms correctly, for example, accurately translating "权利要求 书" as "claims" and "优先权" as "priority right" in patent contexts.The consistency of technical expression is also crucial.If an AI-generated pair contains the translation of a mechanical structure description, it should adhere to the professional norms of mechanical engineering terminology and maintain the precision of parameter descriptions (e.g., "公差范围±0.02mm"should be translated as "tolerance range ±0.02mm" without altering numerical values or technical connotations).</p>
<p>At the mathematical modeling level, DPO directly optimizes the policy probability distribution through binary preference comparisons (x, y w , y l ), where y w represents an output superior to y l .The objective function is:
L DPO (θ) = − log σ β −1 [log p θ (y w |x) − log p θ (y l |x)]
While σ is the sigmoid function, which maps the input value to the range between 0 and 1, used to measure the likelihood of preference.β is a hyperparameter that controls the deviation from the reference strategy.Its optimal value is determined through experiments to balance the model's exploration of new preferences and its utilization of existing experience.p θ (y w |x) and p θ (y l |x) respectively represent the probabilities of the better output y w and the less -good output y l given the input x under the current model parameters θ.This objective function aims to maximize the probability of the better output and minimize the probability of the less -good output, thus guiding the model to generate results that are more in line with scientific task preferences.</p>
<p>The implementation of DPO employs the AdamW optimizer, with an initial learning rate set to 5 × 10 −5 , linear warm -up within 500 steps, and then uses the cosine annealing strategy for 3 training epochs.All experiments are conducted on A800 GPUs with a batch size of 64.The final evaluation metrics include preference accuracy, factual consistency (measured through domain -specific question -answering probes), and mathematical notation coherence scores.In the evaluation of factual consistency, a series of question -answering tests covering different scientific fields are designed.The model's output is required not only to be reasonable in expression but also to precisely match known scientific facts.</p>
<p>Results</p>
<p>To comprehensively assess SciGPT's capabilities in scientific literature understanding and knowledge discovery, we evaluated its performance across three dimensions: task-specific performance on Sci-enceBench, practical utility in domain-specific professional scenarios, and robustness in handling unseen tasks.</p>
<p>Performance on ScienceBench</p>
<p>We conducted a systematic few-shot evaluation of SciGPT on ScienceBench, comparing it with GPT-4 using both standard NLP metrics (e.g., F1, BLEU) and expert-driven assessments.The results (Table 2 and Figure 4) demonstrate SciGPT's domain-specific advantages:</p>
<p>Sequence Labeling Tasks: SciGPT achieved a Micro-F1 score of 0.7466 in Named Entity Recognition (NER), outperforming GPT-4 by 8.3% (vs.GPT-4's 0.6902).In Relation Extraction (RE), its Micro-F1 score of 0.667 was 73.2% higher than GPT-4's 0.385, reflecting superior mastery of scientific entity relationships.This advantage stems from SciGPT's specialized training on domain-specific ontologies, enabling precise parsing of technical terminology.</p>
<p>Generation Tasks: In scientific machine translation, SciGPT's BLEU-4 score of 0.7735 exceeded GPT-4's 0.691 by 11.9%, particularly excelling in preserving numerical precision and discipline-specific jargon.For Abstract-to-Title generation, its ROUGE-L score of 0.7621 matched GPT-4's performance, indicating strong ability to distill core scientific contributions.</p>
<p>Inference Tasks: SciGPT achieved 0.5576 F1 in Knowledge Fusion, outperforming GPT-4 by 12.4%, showcasing its strength in integrating cross-domain classification systems.In Semantic Matching, its 0.6262 F1 score surpassed GPT-4's 0.586, highlighting better capability to identify technical similarities between patents and research papers.</p>
<p>We employed ChatGPT-4 as a third-party judge.Through carefully designed input prompts, we requested ChatGPT-4 to evaluate and score the responses generated by SciGPT and ChatGPT-4o for the same question.Each pair of outputs from different models was evaluated twice with their positions swapped in the prompts.The final scores were calculated by averaging the two evaluations.Finally, we counted the number of wins and losses for both models.The results are shown in Figure 4. Our SciGPT model demonstrated significantly superior performance compared to ChatGPT-4o in machine translation and technical terminology parsing tasks, indicating that SciGPT has the potential to serve as a scientific literature assistant, aiding researchers in efficiently processing academic literature translation, accurately identifying technical terms, and quickly organizing core ideas of literature.</p>
<p>In summarization tasks, the proportion of ties was very high, suggesting that GPT-4 struggled to clearly distinguish the differences in scientific literature summarization capabilities between SciGPT and ChatGPT-4o.</p>
<p>Robustness and Generalization</p>
<p>Consistent with its demonstrated strong robustness and generalization capabilities, SciGPT exhibits notable strengths in adapting to unseen scientific scenarios and maintaining performance stability across diverse tasks.Specifically, the model achieves competitive results on previously unseen tasks-such as newly published relation extraction datasets-proving its ability to learn transferable domainspecific patterns.This is reflected in its effective generalization across common scientific subfields: for instance, when applied to cross-domain tasks (e.g., biomedical entity recognition), SciGPT retains an F1 score of over 0.6, outperforming general-purpose models by 8%-12% in similar scenarios.Despite these strengths, SciGPT still faces challenges in two extreme scenarios.First, in highly niche fields with scarce training data (e.g., material synthesis), its generalization performance degrades significantly: the F1 score in cross-domain named entity recognition (NER) tasks drops below 0.48, as the model lacks sufficient exposure to the unique terminology and knowledge structures of these domains.</p>
<p>Conclusions and Future Works</p>
<p>In this work, we present SciGPT, a domain-specific large language model tailored for scientific literature understanding and knowledge discovery, along with a comprehensive evaluation of its performance on ScienceBench.Through a structured training pipeline encompassing domain-adapted pretraining, supervised fine-tuning (SFT), and direct preference optimization (DPO), with the architectural optimizations based on the Qwen3 framework, SciGPT achieves superior performance in scientific domain tasks.The key conclusions of this study are summarized as follows:</p>
<p>First, the proposed training methodology for scientific domain LLMs demonstrates significant effectiveness.Evaluation results show that SciGPT outperforms general-purpose models like GPT-4 across multiple core tasks in the scientific domain, particularly in sequence labeling tasks such as named entity recognition and relation extraction, as well as in cross-lingual scientific text translation.These results validate that targeted domain adaptation, including the integration of scientific terminology, citation patterns, and disciplinary logic, enables SciGPT to better meet the demands of scientific literature processing.</p>
<p>Second, Qwen3-8B based optimization strategies contribute to a favorable balance between performance and efficiency.SciGPT exhibits efficient memory usage in long-document processing scenarios, maintaining high accuracy in multi-step reasoning and knowledge fusion tasks while reducing inference costs.This advantage is crucial for practical applications in scientific research, where handling lengthy documents such as research papers and technical reports is common.</p>
<p>Third, SciGPT demonstrates strong robustness and generalization capabilities.Its ability to achieve competitive performance on unseen tasks (e.g., newly published relation extraction datasets) indicates that the model has learned transferable domain-specific patterns, highlighting the potential for broader application across diverse scientific subfields.</p>
<p>Future work will focus on addressing the current limitations of SciGPT, particularly in tasks requiring condensation of interdisciplinary knowledge into concise outputs.We plan to enhance the model's ability to synthesize complex scientific concepts through advanced prompt engineering and specialized fine-tuning on interdisciplinary reasoning datasets.Additionally, we will explore the integration of multi-modal scientific information (e.g., figures, formulas, and experimental data) to further expand SciGPT's capabilities in comprehensive scientific papers analysis.Finally, efforts will be made to improve the model's interpretability, ensuring that its outputs and reasoning processes align with the rigorous standards of scientific research.</p>
<p>Figure 3 :
3
Figure 3: Schematic of large language model SciGPT.</p>
<p>Figure 4 :
4
Figure 4: performance of SciGPT models on ScienceBench</p>
<p>Table 1 :
1
Statistics of our final instruction-tuning data.
General Data6%Web CorpusDialogue Data25%14%Math, 1%Others, 2%Code, 1%QA, 0.33%Scientific Paper 25%Patent 28%Figure 1: The distribution of different categories of pretraining data for SciGPT.and quality, prioritizing high-quality data in early stages to minimize catastrophic forgetting. Thisstaged learning strategy focused initially on foundational scientific concepts and terminology, graduallyprogressing to complex tasks like relationship prediction and scientific paper machine translate in latertraining phases.Knowledge DomainTask TypeData sizeNamed Entity Recognition1,000(en) 1,000(zh)Abstract to Title50,176(en)PatentAbstract Extract Machine Translation1,632(en) 2,000(zh) 164,000(en) 164,000(zh)Relation Predict885(zh)Knowledge Extract885(zh)Summary to Topic30,000(zh)Summary to Title30,000(zh)Title to Keywords30,000(zh)Topic and Summary to Title30,000(zh)Sciences PaperSemantic Matching7,569(zh)Relation Extraction35,180(zh)Knowledge Linking23,110(zh)Knowledge Fusion1,643(zh)Relationship Complete900(zh)Topic Modeling6,891(zh)GeneralGeneral Dialogue Data Other490,000 91Total796,981</p>
<p>4 to 0.7 μm wavelength range impinges high intensity visible light wave radiation directly onto a food item. Light sources can be positioned above and below the food item and the inner walls of the oven are preferably highly reflective to reflect light energy onto the food. The intensity of the visible light source is automatically controllable and can be varied throughout the cooking cycle
Machine TranslateSummary to Title根据论文摘要生成标题对于我国当下是否已经存在具有法律拘束力的宪法解释行为或结果,宪法学界存在不同认识。部分学者认为我国已经产生了具有法律拘束力的实质性宪法解释文件,但实质性宪法解释的判断方法和标准尚无学术共识。宪法解释是否已经成为实体与程序相统一的、具有法律拘束力的宪法实施制度,仍需从法理上进行严谨和细致的分析。结合宪法解释制度的法律事实特征来看,我国现行宪法第67条确立的宪法解释制度,在宪法实施的实践中已经具备了构成完整法律事实意义上的宪法解释的实体性要件,并且通过具有法律拘束力的法律文件发挥着实质性宪法解释的制度功能,但其程序性要件还不完备。宪法解释制度今后完善的重点,应当是全面和系统地整合各种具有法律拘束力的宪法解释文件,通过统一、规范的宪法解释程序发布正式的宪法解释令,构建判断方法科学、认定标准清晰、结构体系严密的完整法律事实意义上的宪法解释制度。法律事实理论视角下的实质性宪法解释
请作为技术文本分析专家，完成以下关系补充任务， 根据文本的意思，进行关系补全 "{'content': '一种微 小卫星用芯片的筛选方法，本发明涉及微小卫星领域， 特别地，涉及一种微小卫星用芯片的筛选方法', 'mentions': ['微小卫星', '芯片']}"</p>
<p>Table 2 :
2
Performance comparison of SciGPT with GPT-4 on ScienceBench tasks
TaskDatasetSciGPT GPT-4 Evaluation MetricsNamed Entity Recognition0.8280.585F1Sequence LabelingRelation Extract Abstractive Summarization0.667 0.7670.556 0.542F1 RougeKnowledge Linking0.6830.491F1Topic Modeling0.5000.387Coherence ScoreAbstract-to-Title0.7620.511RougeGenerationMachine Translation0.7740.668BLEURelationship Predict0.52650.334AccuracyInferenceKnowledge Fusion0.5580.461F1Semantic Matching0.62620.586F1</p>
<p>Language models represent space and time. W Gurnee, M Tegmark, arXiv:2310.022072023arXiv preprint</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, Galactica: A large language model for science. 2022</p>
<p>Automated literature research and review-generation method based on large language models. S Wu, X Ma, D Luo, L Li, X Shi, X Chang, X Lin, R Luo, C Pei, C Du, Z.-J Zhao, J Gong, National Science Review. 2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, 2025</p>
<p>Autosurvey: Large language models can automatically write surveys. Y Wang, Q Guo, W Yao, H Zhang, X Zhang, Z Wu, M Zhang, X Dai, M Zhang, Q Wen, W Ye, S Zhang, Y Zhang, 2024</p>
<p>Large language models are zero shot hypothesis proposers. B Qi, K Zhang, H Li, K Tian, S Zeng, Z.-R Chen, B Zhou, 2023</p>
<p>Scipip: An llm-based scientific paper idea proposer. W Wang, L Gu, L Zhang, Y Luo, Y Dai, C Shen, L Xie, B Lin, X He, J Ye, 2025</p>
<p>Litllm: A toolkit for scientific literature review. S Agarwal, G Sahu, A Puri, I H Laradji, K D Dvijotham, J Stanley, L Charlin, C Pal, 2025</p>
<p>Cycleresearcher: Improving automated research via automated review. Y Weng, M Zhu, G Bao, H Zhang, J Wang, Y Zhang, L Yang, 2025</p>
<p>Scilitllm: How to adapt llms for scientific literature understanding. S Li, J Huang, J Zhuang, Y Shi, X Cai, M Xu, X Wang, L Zhang, G Ke, H Cai, 2025</p>
<p>Distilling reasoning capabilities into smaller language models. K Shridhar, A Stolfo, M Sachan, arXiv:2212.001932022arXiv preprint</p>
<p>Multilinear mixture of experts: Scalable expert specialization through factorization. J Oldfield, M Georgopoulos, G G Chrysos, C Tzelepis, Y Panagakis, M A Nicolaou, J Deng, I Patras, arXiv:2402.125502024arXiv preprint</p>
<p>Sparsely activated mixture-of-experts are robust multi-task learners. S Gupta, S Mukherjee, K Subudhi, E Gonzalez, D Jose, A H Awadallah, J Gao, arXiv:2204.076892022arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Fingpt: Democratizing internet-scale data for financial large language models. X.-Y Liu, G Wang, D Zha, arXiv:2307.104852023arXiv preprint</p>
<p>Similarity estimation techniques from rounding algorithms. M S Charikar, Proceedings of the thiry-fourth annual ACM symposium on Theory of computing. the thiry-fourth annual ACM symposium on Theory of computing2002</p>
<p>Textbooks are all you need. S Gunasekar, Y Zhang, J Aneja, C C T Mendes, A Del Giorno, S Gopi, M Javaheripi, P Kauffmann, G De Rosa, O Saarikivi, arXiv:2306.116442023arXiv preprint</p>
<p>Effective long-context scaling of foundation models. W Xiong, J Liu, I Molybog, H Zhang, P Bhargava, R Hou, L Martin, R Rungta, K A Sankararaman, B Oguz, arXiv:2309.160392023arXiv preprint</p>
<p>Openchat: Advancing open-source language models with mixed-quality data. G Wang, S Cheng, X Zhan, X Li, S Song, Y Liu, arXiv:2309.112352023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
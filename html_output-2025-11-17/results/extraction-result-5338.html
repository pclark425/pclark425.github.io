<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5338 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5338</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5338</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-271769292</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.13988v6.pdf" target="_blank">Machine Psychology</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) show increasingly advanced emergent capabilities and are being incorporated across various societal domains. Understanding their behavior and reasoning abilities therefore holds significant importance. We argue that a fruitful direction for research is engaging LLMs in behavioral experiments inspired by psychology that have traditionally been aimed at understanding human cognition and behavior. In this article, we highlight and summarize theoretical perspectives, experimental paradigms, and computational analysis techniques that this approach brings to the table. It paves the way for a"machine psychology"for generative artificial intelligence (AI) that goes beyond performance benchmarks and focuses instead on computational insights that move us toward a better understanding and discovery of emergent abilities and behavioral patterns in LLMs. We review existing work taking this approach, synthesize best practices, and highlight promising future directions. We also highlight the important caveats of applying methodologies designed for understanding humans to machines. We posit that leveraging tools from experimental psychology to study AI will become increasingly valuable as models evolve to be more powerful, opaque, multi-modal, and integrated into complex real-world settings.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5338.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5338.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binz & Schulz 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applied cognitive-psychology-inspired behavioral tests to GPT-3 and reported that the model exhibits some of the same heuristics and reasoning biases found in humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer large language model trained for next-token prediction (reference: Brown et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Heuristics and biases tasks (cognitive-psychology decision-making tests)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Decision-making / reasoning (heuristics & biases)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Tasks drawn from heuristics-and-biases literature designed to reveal reliance on mental shortcuts (e.g., framing effects, belief bias, other classic cognitive-bias paradigms).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative finding: GPT-3 'displays some of the same cognitive biases observed in people' (no quantitative scores given in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans typically show systematic biases on these tasks (qualitative baseline); no specific numeric baselines reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 exhibited patterns qualitatively similar to human participants on the tested heuristics-and-biases tasks; the review does not report statistical comparisons or exact accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The review notes potential confounds such as small sample sizes in early studies and the possibility of training-data contamination or sensitivity to prompt wording that could mimic human-like responding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5338.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hagendorff et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compared earlier LLMs and ChatGPT on classical cognitive-bias tasks and reported that human-like biases present in earlier models are largely absent in ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (and earlier LLMs referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational large language model family (latest-generation models referenced as ChatGPT in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Heuristics and biases tasks (same family of decision-making / reasoning tests)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Decision-making / reasoning (heuristics & biases)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Psychology test stimuli originally designed to elicit human reasoning biases; applied to language models to inspect bias signatures.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported pattern: human-like biases were visible in earlier LLMs but 'largely disappeared' in ChatGPT (qualitative statement; no numeric scores reported in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans continue to exhibit these biases on the original tasks (no numeric baseline provided in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Where earlier LLMs mirrored human biases, ChatGPT diverged by not showing the same biases; the review suggests a qualitative mismatch rather than a simple stronger/weaker score comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors discuss multiple possible causes: improved reasoning or instruction tuning, test stimuli no longer challenging for modern models, and potential contamination/leakage into training data; quantitative details are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5338.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dasgupta et al. 2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models show human-like content effects on reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigated how semantic content (familiar/believable vs. unfamiliar/abstract formulations) affects LLM logical-reasoning accuracy and found content effects similar to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models show human-like content effects on reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various; not specified in-detail in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative LLMs evaluated on logical-reasoning problems with manipulated semantic content.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Logical reasoning problems with semantic content manipulations (content-effect tests)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Reasoning (belief/content effects)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Sets of logical reasoning items presented in versions that vary in semantic plausibility or familiarity to measure content effects on reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: LLMs 'reason more accurately about familiar, believable, or grounded situations' than about unfamiliar, unbelievable, or abstract problems.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans also show better reasoning on believable/grounded content versus abstract versions (no numeric baselines in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs exhibit the same directional content effect as humans (i.e., similar qualitative pattern), but the review does not report exact accuracy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The review flags potential confounds such as exposure to similar content during model training (training-data contamination) and sensitivity to prompt formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5338.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sap et al. 2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early evaluations showed that models such as GPT-3 struggle on theory-of-mind (ToM) tasks, indicating limitations in inferring others' mental states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (examples of early models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM evaluated on social-intelligence / theory-of-mind tasks; specifics not detailed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Theory-of-mind tasks (false-belief and related social inference tests)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Social cognition / theory of mind</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic developmental psychology tasks that measure ability to infer others' beliefs, intentions, and false beliefs from vignettes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: early models like GPT-3 'struggle to solve theory of mind tasks' (no numeric accuracy reported in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Typically developing human adults/older children succeed on many canonical ToM tasks; exact baselines not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Early LLMs underperformed relative to expected human competence on ToM tasks according to the cited work; the review does not provide numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Subsequent work (see entries below) shows improvement in later models; methodological caveats include sensitivity to prompt wording and task formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5338.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Strachan et al. 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Testing theory of mind in large language models and humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent evaluations comparing LLMs and humans on ToM tasks show later LLMs improving and reliably inferring unobservable mental states in others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Testing theory of mind in large language models and humans</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Later-generation LLMs (multiple models evaluated; not enumerated in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art LLMs evaluated on developmental and higher-order theory-of-mind paradigms; the review summarizes the qualitative trend of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Theory-of-mind test battery (developmental and higher-order ToM tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Social cognition / theory of mind</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Tasks adapted from human developmental psychology (e.g., false-belief tasks and higher-order recursive belief tasks) used to probe models' ability to infer beliefs and mental states.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: later models demonstrate 'increasing ability to reliably infer unobservable mental states' (no quantitative scores provided in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human responses on these ToM tasks are used as comparative benchmarks in the cited work; specific numbers/demographics not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Trend reported: gap between early LLMs and humans on ToM tasks narrows with later models; some models approach human-like performance on specific ToM measures per the cited studies.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The review notes robustness concerns: some setups are sensitive to trivial input changes (see Ullman 2023), and performance depends strongly on prompt/task formulation and model family.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5338.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Duijn et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct empirical comparison of 11 state-of-the-art LLMs to children aged 7–10 on advanced theory-of-mind tests to identify strengths and weaknesses across models and developmental benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>11 state-of-the-art LLMs (collection evaluated in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A suite of contemporary LLMs compared head-to-head against child developmental benchmarks on advanced ToM tasks; model specifics not enumerated in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Advanced theory-of-mind tests (developmental/comparative battery)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Social cognition / theory of mind</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Advanced ToM tasks used in developmental psychology adapted to evaluate LLMs and compare performance to children aged 7–10.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Review notes a head-to-head comparison was performed but does not provide numeric results; study compares model outputs to child performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children aged 7–10 performance used as an explicit baseline in the cited work; the review does not provide the numeric baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>The cited work frames direct comparisons between multiple LLMs and children; the review does not report the outcome magnitudes or statistical details.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Review emphasizes cross-model variability and dependence on task wording; specific strengths/weaknesses are left to the primary study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5338.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Street 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs achieve adult human performance on higher-order theory of mind tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported claim that some LLMs reach adult-human-level performance on higher-order (recursive) theory-of-mind tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLMs achieve adult human performance on higher-order theory of mind tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (specific models not enumerated in the review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Later-generation large language models evaluated on recursive/higher-order ToM tasks; exact architectures are not detailed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Higher-order theory-of-mind tasks (recursive belief reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Social cognition / theory of mind (higher-order)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Tasks requiring multiple levels of nested belief reasoning (e.g., beliefs about beliefs) used to probe higher-order social inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Claimed result (per cited arXiv): LLMs achieve adult human performance on these higher-order ToM tasks; review provides this qualitative statement but no numeric detail.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Adult human performance is the comparator indicated by the cited claim; numeric baselines are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Cited claim states parity with adult humans on the evaluated higher-order ToM tasks; the review does not present accompanying statistical details or task specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The review cautions that ToM performance can be sensitive to task alterations and prompting, and independent replication/robustness checks are necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5338.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ullman 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrated that LLM ToM task performance can be brittle — trivial changes to inputs or task framing can cause models to fail even when they previously appeared successful.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (various; specific models discussed in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs evaluated for robustness to small, superficial perturbations of theory-of-mind task inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Theory-of-mind tasks with perturbation/robustness checks</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Social cognition / theory of mind (robustness testing)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Standard ToM vignettes modified by trivial alterations (e.g., wording changes, irrelevant distractors) to test model robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Finding: models that succeeded on canonical ToM prompts could fail under trivial alterations; performance is not robust across minor input changes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans are typically robust to many trivial surface changes in ToM vignettes; the review does not provide numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs can show brittle behavior relative to humans: apparent successes may not generalize under small task perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Highlights prompt-sensitivity and the need for careful controls to avoid overestimating LLM competence on cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5338.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5338.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogBench (Coda-Forno et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogBench: a large language model walks into a psychology lab</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedural, large-scale testbed/battery (CogBench) developed to run psychology lab-style experiments on LLMs in a way that mitigates static-test contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CogBench: a large language model walks into a psychology lab</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CogBench (evaluation platform); various LLMs evaluated using this platform in the cited work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A procedural and/or automatically generated battery designed to present varied psychology-style tasks to LLMs to reduce risk of training-data contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CogBench battery (multiple cognitive-psychology tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Multiple (reasoning, memory, social cognition, etc.) — a modular psychology testbed</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Procedurally generated or diverse sets of psychology-inspired tasks designed to probe multiple cognitive constructs while limiting reuse/contamination of static stimuli.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>The review describes CogBench as a tool/benchmark; specific aggregate performance outcomes are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not reported in the review summary; CogBench is presented as a framework that could support human–model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not provided here; CogBench is described as enabling robust comparisons but the review does not include concrete comparative results.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>CogBench is highlighted as less susceptible to contamination due to procedural generation, but the review does not detail empirical findings from it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Psychology', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 2)</em></li>
                <li>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs <em>(Rating: 2)</em></li>
                <li>Testing theory of mind in large language models and humans <em>(Rating: 2)</em></li>
                <li>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests <em>(Rating: 2)</em></li>
                <li>LLMs achieve adult human performance on higher-order theory of mind tasks <em>(Rating: 2)</em></li>
                <li>Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks <em>(Rating: 2)</em></li>
                <li>CogBench: a large language model walks into a psychology lab <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5338",
    "paper_id": "paper-271769292",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "Binz & Schulz 2023",
            "name_full": "Using cognitive psychology to understand GPT-3",
            "brief_description": "Applied cognitive-psychology-inspired behavioral tests to GPT-3 and reported that the model exhibits some of the same heuristics and reasoning biases found in humans.",
            "citation_title": "Using cognitive psychology to understand GPT-3",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer large language model trained for next-token prediction (reference: Brown et al., 2020).",
            "model_size": null,
            "cognitive_test_name": "Heuristics and biases tasks (cognitive-psychology decision-making tests)",
            "cognitive_test_type": "Decision-making / reasoning (heuristics & biases)",
            "cognitive_test_description": "Tasks drawn from heuristics-and-biases literature designed to reveal reliance on mental shortcuts (e.g., framing effects, belief bias, other classic cognitive-bias paradigms).",
            "llm_performance": "Qualitative finding: GPT-3 'displays some of the same cognitive biases observed in people' (no quantitative scores given in this review).",
            "human_baseline_performance": "Humans typically show systematic biases on these tasks (qualitative baseline); no specific numeric baselines reported in this review.",
            "performance_comparison": "GPT-3 exhibited patterns qualitatively similar to human participants on the tested heuristics-and-biases tasks; the review does not report statistical comparisons or exact accuracies.",
            "notable_differences_or_limitations": "The review notes potential confounds such as small sample sizes in early studies and the possibility of training-data contamination or sensitivity to prompt wording that could mimic human-like responding.",
            "uuid": "e5338.0",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Hagendorff et al. 2023",
            "name_full": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
            "brief_description": "Compared earlier LLMs and ChatGPT on classical cognitive-bias tasks and reported that human-like biases present in earlier models are largely absent in ChatGPT.",
            "citation_title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (and earlier LLMs referenced)",
            "model_description": "Instruction-tuned conversational large language model family (latest-generation models referenced as ChatGPT in the review).",
            "model_size": null,
            "cognitive_test_name": "Heuristics and biases tasks (same family of decision-making / reasoning tests)",
            "cognitive_test_type": "Decision-making / reasoning (heuristics & biases)",
            "cognitive_test_description": "Psychology test stimuli originally designed to elicit human reasoning biases; applied to language models to inspect bias signatures.",
            "llm_performance": "Reported pattern: human-like biases were visible in earlier LLMs but 'largely disappeared' in ChatGPT (qualitative statement; no numeric scores reported in the review).",
            "human_baseline_performance": "Humans continue to exhibit these biases on the original tasks (no numeric baseline provided in the review).",
            "performance_comparison": "Where earlier LLMs mirrored human biases, ChatGPT diverged by not showing the same biases; the review suggests a qualitative mismatch rather than a simple stronger/weaker score comparison.",
            "notable_differences_or_limitations": "Authors discuss multiple possible causes: improved reasoning or instruction tuning, test stimuli no longer challenging for modern models, and potential contamination/leakage into training data; quantitative details are not provided in this review.",
            "uuid": "e5338.1",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Dasgupta et al. 2022",
            "name_full": "Language models show human-like content effects on reasoning",
            "brief_description": "Investigated how semantic content (familiar/believable vs. unfamiliar/abstract formulations) affects LLM logical-reasoning accuracy and found content effects similar to humans.",
            "citation_title": "Language models show human-like content effects on reasoning",
            "mention_or_use": "mention",
            "model_name": "Large language models (various; not specified in-detail in this review)",
            "model_description": "Generative LLMs evaluated on logical-reasoning problems with manipulated semantic content.",
            "model_size": null,
            "cognitive_test_name": "Logical reasoning problems with semantic content manipulations (content-effect tests)",
            "cognitive_test_type": "Reasoning (belief/content effects)",
            "cognitive_test_description": "Sets of logical reasoning items presented in versions that vary in semantic plausibility or familiarity to measure content effects on reasoning performance.",
            "llm_performance": "Qualitative: LLMs 'reason more accurately about familiar, believable, or grounded situations' than about unfamiliar, unbelievable, or abstract problems.",
            "human_baseline_performance": "Humans also show better reasoning on believable/grounded content versus abstract versions (no numeric baselines in this review).",
            "performance_comparison": "LLMs exhibit the same directional content effect as humans (i.e., similar qualitative pattern), but the review does not report exact accuracy comparisons.",
            "notable_differences_or_limitations": "The review flags potential confounds such as exposure to similar content during model training (training-data contamination) and sensitivity to prompt formulation.",
            "uuid": "e5338.2",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Sap et al. 2022",
            "name_full": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "brief_description": "Early evaluations showed that models such as GPT-3 struggle on theory-of-mind (ToM) tasks, indicating limitations in inferring others' mental states.",
            "citation_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (examples of early models)",
            "model_description": "Autoregressive transformer LLM evaluated on social-intelligence / theory-of-mind tasks; specifics not detailed in the review.",
            "model_size": null,
            "cognitive_test_name": "Theory-of-mind tasks (false-belief and related social inference tests)",
            "cognitive_test_type": "Social cognition / theory of mind",
            "cognitive_test_description": "Classic developmental psychology tasks that measure ability to infer others' beliefs, intentions, and false beliefs from vignettes.",
            "llm_performance": "Qualitative: early models like GPT-3 'struggle to solve theory of mind tasks' (no numeric accuracy reported in the review).",
            "human_baseline_performance": "Typically developing human adults/older children succeed on many canonical ToM tasks; exact baselines not reported here.",
            "performance_comparison": "Early LLMs underperformed relative to expected human competence on ToM tasks according to the cited work; the review does not provide numerical comparisons.",
            "notable_differences_or_limitations": "Subsequent work (see entries below) shows improvement in later models; methodological caveats include sensitivity to prompt wording and task formulation.",
            "uuid": "e5338.3",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Strachan et al. 2024",
            "name_full": "Testing theory of mind in large language models and humans",
            "brief_description": "Recent evaluations comparing LLMs and humans on ToM tasks show later LLMs improving and reliably inferring unobservable mental states in others.",
            "citation_title": "Testing theory of mind in large language models and humans",
            "mention_or_use": "mention",
            "model_name": "Later-generation LLMs (multiple models evaluated; not enumerated in this review)",
            "model_description": "State-of-the-art LLMs evaluated on developmental and higher-order theory-of-mind paradigms; the review summarizes the qualitative trend of improvement.",
            "model_size": null,
            "cognitive_test_name": "Theory-of-mind test battery (developmental and higher-order ToM tasks)",
            "cognitive_test_type": "Social cognition / theory of mind",
            "cognitive_test_description": "Tasks adapted from human developmental psychology (e.g., false-belief tasks and higher-order recursive belief tasks) used to probe models' ability to infer beliefs and mental states.",
            "llm_performance": "Qualitative: later models demonstrate 'increasing ability to reliably infer unobservable mental states' (no quantitative scores provided in the review).",
            "human_baseline_performance": "Human responses on these ToM tasks are used as comparative benchmarks in the cited work; specific numbers/demographics not provided in this review.",
            "performance_comparison": "Trend reported: gap between early LLMs and humans on ToM tasks narrows with later models; some models approach human-like performance on specific ToM measures per the cited studies.",
            "notable_differences_or_limitations": "The review notes robustness concerns: some setups are sensitive to trivial input changes (see Ullman 2023), and performance depends strongly on prompt/task formulation and model family.",
            "uuid": "e5338.4",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Duijn et al. 2023",
            "name_full": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "brief_description": "Direct empirical comparison of 11 state-of-the-art LLMs to children aged 7–10 on advanced theory-of-mind tests to identify strengths and weaknesses across models and developmental benchmarks.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "mention",
            "model_name": "11 state-of-the-art LLMs (collection evaluated in the cited work)",
            "model_description": "A suite of contemporary LLMs compared head-to-head against child developmental benchmarks on advanced ToM tasks; model specifics not enumerated in this review.",
            "model_size": null,
            "cognitive_test_name": "Advanced theory-of-mind tests (developmental/comparative battery)",
            "cognitive_test_type": "Social cognition / theory of mind",
            "cognitive_test_description": "Advanced ToM tasks used in developmental psychology adapted to evaluate LLMs and compare performance to children aged 7–10.",
            "llm_performance": "Review notes a head-to-head comparison was performed but does not provide numeric results; study compares model outputs to child performance.",
            "human_baseline_performance": "Children aged 7–10 performance used as an explicit baseline in the cited work; the review does not provide the numeric baselines.",
            "performance_comparison": "The cited work frames direct comparisons between multiple LLMs and children; the review does not report the outcome magnitudes or statistical details.",
            "notable_differences_or_limitations": "Review emphasizes cross-model variability and dependence on task wording; specific strengths/weaknesses are left to the primary study.",
            "uuid": "e5338.5",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Street 2024",
            "name_full": "LLMs achieve adult human performance on higher-order theory of mind tasks",
            "brief_description": "Reported claim that some LLMs reach adult-human-level performance on higher-order (recursive) theory-of-mind tasks.",
            "citation_title": "LLMs achieve adult human performance on higher-order theory of mind tasks",
            "mention_or_use": "mention",
            "model_name": "LLMs (specific models not enumerated in the review summary)",
            "model_description": "Later-generation large language models evaluated on recursive/higher-order ToM tasks; exact architectures are not detailed in the review.",
            "model_size": null,
            "cognitive_test_name": "Higher-order theory-of-mind tasks (recursive belief reasoning)",
            "cognitive_test_type": "Social cognition / theory of mind (higher-order)",
            "cognitive_test_description": "Tasks requiring multiple levels of nested belief reasoning (e.g., beliefs about beliefs) used to probe higher-order social inference.",
            "llm_performance": "Claimed result (per cited arXiv): LLMs achieve adult human performance on these higher-order ToM tasks; review provides this qualitative statement but no numeric detail.",
            "human_baseline_performance": "Adult human performance is the comparator indicated by the cited claim; numeric baselines are not provided in this review.",
            "performance_comparison": "Cited claim states parity with adult humans on the evaluated higher-order ToM tasks; the review does not present accompanying statistical details or task specifics.",
            "notable_differences_or_limitations": "The review cautions that ToM performance can be sensitive to task alterations and prompting, and independent replication/robustness checks are necessary.",
            "uuid": "e5338.6",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Ullman 2023",
            "name_full": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
            "brief_description": "Demonstrated that LLM ToM task performance can be brittle — trivial changes to inputs or task framing can cause models to fail even when they previously appeared successful.",
            "citation_title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
            "mention_or_use": "mention",
            "model_name": "Large language models (various; specific models discussed in the cited work)",
            "model_description": "LLMs evaluated for robustness to small, superficial perturbations of theory-of-mind task inputs.",
            "model_size": null,
            "cognitive_test_name": "Theory-of-mind tasks with perturbation/robustness checks",
            "cognitive_test_type": "Social cognition / theory of mind (robustness testing)",
            "cognitive_test_description": "Standard ToM vignettes modified by trivial alterations (e.g., wording changes, irrelevant distractors) to test model robustness.",
            "llm_performance": "Finding: models that succeeded on canonical ToM prompts could fail under trivial alterations; performance is not robust across minor input changes.",
            "human_baseline_performance": "Humans are typically robust to many trivial surface changes in ToM vignettes; the review does not provide numeric comparisons.",
            "performance_comparison": "LLMs can show brittle behavior relative to humans: apparent successes may not generalize under small task perturbations.",
            "notable_differences_or_limitations": "Highlights prompt-sensitivity and the need for careful controls to avoid overestimating LLM competence on cognitive tasks.",
            "uuid": "e5338.7",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "CogBench (Coda-Forno et al. 2024)",
            "name_full": "CogBench: a large language model walks into a psychology lab",
            "brief_description": "A procedural, large-scale testbed/battery (CogBench) developed to run psychology lab-style experiments on LLMs in a way that mitigates static-test contamination.",
            "citation_title": "CogBench: a large language model walks into a psychology lab",
            "mention_or_use": "mention",
            "model_name": "CogBench (evaluation platform); various LLMs evaluated using this platform in the cited work",
            "model_description": "A procedural and/or automatically generated battery designed to present varied psychology-style tasks to LLMs to reduce risk of training-data contamination.",
            "model_size": null,
            "cognitive_test_name": "CogBench battery (multiple cognitive-psychology tasks)",
            "cognitive_test_type": "Multiple (reasoning, memory, social cognition, etc.) — a modular psychology testbed",
            "cognitive_test_description": "Procedurally generated or diverse sets of psychology-inspired tasks designed to probe multiple cognitive constructs while limiting reuse/contamination of static stimuli.",
            "llm_performance": "The review describes CogBench as a tool/benchmark; specific aggregate performance outcomes are not provided in this review.",
            "human_baseline_performance": "Not reported in the review summary; CogBench is presented as a framework that could support human–model comparisons.",
            "performance_comparison": "Not provided here; CogBench is described as enabling robust comparisons but the review does not include concrete comparative results.",
            "notable_differences_or_limitations": "CogBench is highlighted as less susceptible to contamination due to procedural generation, but the review does not detail empirical findings from it.",
            "uuid": "e5338.8",
            "source_info": {
                "paper_title": "Machine Psychology",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
            "rating": 2,
            "sanitized_title": "humanlike_intuitive_behavior_and_reasoning_biases_emerged_in_large_language_models_but_disappeared_in_chatgpt"
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 2,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        },
        {
            "paper_title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
            "rating": 2,
            "sanitized_title": "neural_theoryofmind_on_the_limits_of_social_intelligence_in_large_lms"
        },
        {
            "paper_title": "Testing theory of mind in large language models and humans",
            "rating": 2,
            "sanitized_title": "testing_theory_of_mind_in_large_language_models_and_humans"
        },
        {
            "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "rating": 2,
            "sanitized_title": "theory_of_mind_in_large_language_models_examining_performance_of_11_stateoftheart_models_vs_children_aged_710_on_advanced_tests"
        },
        {
            "paper_title": "LLMs achieve adult human performance on higher-order theory of mind tasks",
            "rating": 2,
            "sanitized_title": "llms_achieve_adult_human_performance_on_higherorder_theory_of_mind_tasks"
        },
        {
            "paper_title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "CogBench: a large language model walks into a psychology lab",
            "rating": 2,
            "sanitized_title": "cogbench_a_large_language_model_walks_into_a_psychology_lab"
        }
    ],
    "cost": 0.01978125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>August 9, 2024</p>
<p>Thilo Hagendorff thilo.hagendorff@iris.uni-stuttgart.de 
Ishita Dasgupta 
Google Deepmind 
Marcel Binz 
Stephanie C Y Chan 
Andrew Lampinen 
Jane X Wang 
Zeynep Akata 
Eric Schulz </p>
<p>University of Stuttgart</p>
<p>Helmholtz Institute for Human-Centered AI</p>
<p>TU Munich</p>
<p>Helmholtz Institute for Human-Centered AI</p>
<p>August 9, 202439D43FBC831219BFD2B082919FBC4062arXiv:2303.13988v6[cs.CL]
Large language models (LLMs) show increasingly advanced emergent capabilities and are being incorporated across various societal domains.Understanding their behavior and reasoning abilities therefore holds significant importance.We argue that a fruitful direction for research is engaging LLMs in behavioral experiments inspired by psychology that have traditionally been aimed at understanding human cognition and behavior.In this article, we highlight and summarize theoretical perspectives, experimental paradigms, and computational analysis techniques that this approach brings to the table.It paves the way for a "machine psychology" for generative artificial intelligence (AI) that goes beyond performance benchmarks and focuses instead on computational insights that move us toward a better understanding and discovery of emergent abilities and behavioral patterns in LLMs.We review existing work taking this approach, synthesize best practices, and highlight promising future directions.We also highlight the important caveats of applying methodologies designed for understanding humans to machines.We posit that leveraging tools from experimental psychology to study AI will become increasingly valuable as models evolve to be more powerful, opaque, multi-modal, and integrated into complex real-world settings.</p>
<p>Introduction</p>
<p>Recent advances in computing power, data availability, and machine learning algorithms have yielded powerful artificial intelligence systems that are used in almost all parts of society.Among these, large language models (LLMs), gigantic neural network architectures trained on large amounts of text, have seen a particularly meteoric rise in their influence.The ability of LLMs to interface directly with natural language has made them accessible to the public in a way that was not seen before, leading to widespread adoption with millions of daily users (Gemini Team et al., 2024;Anthropic, 2024;OpenAI, 2022;OpenAI, 2023a).Also contributing to their rise in influence is that LLMs are wide-ranging in the kinds of tasks they can do -from writing text or code to calling functions, accessing the Internet, retrieving external information, reasoning about complex problems, and many more (Bubeck et al., 2023;Lo et al., 2022;Elkins and Chun, 2020).Recently, LLMs have been extended to interact with other modalities such as vision and speech (Fei et al., 2022;Radford et al., 2023).The ever-growing capabilities of these systems make them challenging but also increasingly important to characterize and understand, especially since these expanding capabilities also bring greater potential for unforeseen harm (Bommasani et al., 2021;Hagendorff, 2024b;Weidinger et al., 2022;Bender et al., 2021;Schramowski et al., 2022).Understanding behavioral patterns and emergent abilities in LLMs requires explaining their operating principles.Of the approaches focused on explaining AI systems, many rely on trying to understand the inner workings of these neural networks.This approach, often termed mechanistic interpretability, seeks to investigate LLMs by analyzing how their weights and activation patterns implement the observable behavior.It uses simplifications in terms of data, the model, or both, that make causal interventions possible and the internal mechanisms easier to characterize (Stolfo et al., 2023;Conmy et al., 2023;Wang, Variengien, et al., 2022;Gao et al., 2024).A related set of approaches draws inspiration more directly from neuroscience to characterize broader correlational similarities and differences between the internal processing of LLMs and humans (Hosseini and Fedorenko, 2023;Kumar et al., 2022).</p>
<p>In contrast, this review focuses on the class of approaches that directly study the behavior of LLMs, analyzing relationships between inputs and outputs instead of inspecting the inner workings.This approach includes not only analyses of static trained models, but also experimental manipulations of inputs both during and after training.It also encompasses analyses of inputs and outputs that reveal insights about internal mechanisms, even if those internal mechanisms are not directly inspected.For this set of approaches, experiments can be inspired by human psychology, cognitive science, and the behavioral sciences.This is what we want to term machine psychology (see Figure 1).Over several decades, the mentioned disciplines have developed a wide range of methods and frameworks to understand and characterize observable intelligent behaviors in human and non-human animals (Edwards, 1954;Festinger and Katz, 1953), much of which can now be adapted to LLMs as well.</p>
<p>Thus far, the research community has responded to the challenges of understanding behavioral patterns and growing capabilities in LLMs in several ways (Schwartz, 2022;Zhao, Chen, et al., 2023).The traditional machine learning benchmark-driven approach has released new datasets that capture specific aspects only recently seen emerging in models (Srivastava et al., 2022;Hendrycks et al., 2021;Zellers et al., 2019).Traditional benchmarking aims primarily to enable the community to compare and optimize LLM performance.In contrast, machine psychology research is not primarily interested in increasing (or measuring) an LLM's performance, but rather in understanding behavioral patterns.While traditional natural language processing benchmarks measure abilities such as translation, numerical reasoning, or factual accuracy, machine psychology is also interested in how these observable abilities indirectly reflect the underlying constructs and algorithms (Frank et al., 2024).Understanding these constructs lets us make new predictions about e.g.how the model will generalize, how it will perform with different training data, and specific failure modes.</p>
<p>The relative importance of behavior-based inspection (or psychology) versus internal inspection (or neuroscience) has been a long-standing debate (Jonas and Kording, 2017).We believe that both approaches have value for understanding both humans and LLMs.Directly inspecting LLMs' behavior, however, does come with multiple advantages.The behavior of LLMs is expressed at the interface of the model, where human users interact, and thus is what we ultimately care about the most (Binz and Schulz, 2023;Chang and Bergen, 2024;Ivanova, 2023).Such behavior is often too complex to predict purely from our current mechanistic understanding of model weights and activation patterns (Grön et al., 2003).Many interesting behaviors are only displayed by large models with billions of parameters (Kaplan et al., 2020;Wei, Tay, et al., 2022), and behavioral methods in psychology that treat behavior directly as the experimental variable of interest scale gracefully with model size.Another practical advantage is that these behavioral approaches can easily be applied by the broader academic community to closed-source state-of-the-art models whose internal workings are not disclosed to the public.</p>
<p>In this article, we review and chart future directions in this emerging field of directly modeling LLM behavior.We outline how established behavioral sciences can guide and inform our understanding of LLMs, and discuss important caveats for when and how to apply methods to LLMs, given that they were originally developed for humans and animals.In the first section, we discuss the theoretical frameworks developed and used in psychology to organize our understanding of intelligence and intelligent behaviors.We then review the many empirical paradigms that have been developed to study and characterize different aspects of intelligent behavior.Finally, we discuss and make recommendations for robust empirical methods both for designing experiments and analyzing behavioral data.We end the article by discussing the potentials and limitations of conducting machine psychology experiments with increasingly capable black-box models.</p>
<p>Theory: Evaluation paradigms for understanding intelligent systems</p>
<p>The traditional framework in machine learning algorithms has revolved around benchmark datasets (Bowman et al., 2015;Russakovsky et al., 2015).These datasets are designed to require specific capabilities (e.g.object recognition, sentiment analysis, etc.) for good performance.Researchers train on a train dataset and evaluate on a held-out test dataset that was not seen during training.This framework does not generalize well to large-scale foundation models for two reasons.First, when using Internet-scale training data for models, this split has become harder to maintain (Li and Flanigan, 2023;Khan et al., 2023).Second, foundation models are only directly trained for next-token prediction but exhibit many other "intelligent" behaviors that can, with some reservations (Schaeffer et al., 2023), be considered emergent.For example, practitioners did not explicitly encode or train for a transformer LLM's ability to learn from a few examples in context (Brown et al., 2020), but it nonetheless arose from the machine learning architecture, data, and learning signal (Chan et al., 2022;Oswald et al., 2023).Emergent behaviors can be difficult to study through the lens of the components that gave rise to it (Anderson, 1972), and the ones that emerge can seem surprising (Wei, Tay, et al., 2022) -the most interesting evaluations are not 'held-out' exemplars of the training task.</p>
<p>Researchers have therefore started building test-only benchmarks -i.e.smaller scale datasets unsuitable for training and intended solely as a test set -to investigate model capabilities, e.g. the BIG-bench comprising more than 200 tests (Srivastava et al., 2022), the Abstraction and Reasoning Challenge (Chollet et al., 2020), as well as many others (Ivanova et al., 2024;Mazumder et al., 2024).In several cases, these benchmarks already resemble evaluation frameworks from the behavioral sciences (Bubeck et al., 2023) -like personality tests, intelligence tests, implicit association tests, etc. that are applied to humans -which similarly do not follow the train-test paradigm.They also tend to fall into two categories.Some evaluations focus on scalar performance metrics, e.g.intelligence quotients.Others focus on characterizing behavior, i.e. the questions are not designed with accuracy in mind, but designed to elicit responses that reveal behavioral strategies, or underlying constructs.In this review, we focus on test-only evaluations that provide this latter kind of understanding, as a novel evaluation paradigm that is starting to gain traction in the machine learning field.</p>
<p>Several such diagnostic evaluations have been developed even for pre-LLM models where, despite the models being trained for specific tasks, how to solve them is not specified.Such diagnostic datasets were used to expose the ways in which learned systems solved tasks -often counter to human intuitions (Geirhos et al., 2020;McCoy, Pavlick, et al., 2019;Hermann and Lampinen, 2020;Dasgupta et al., 2022;Singla and Feizi, 2021).Researchers have also made the case for borrowing from ethology, a branch of zoology that studies the behavior of non-human animals, to explain machine behavior in machine learning systems (Rahwan et al., 2019).However, in the era of LLMs, not only are the how unspecified, but the model abilities themselves are neither directly known nor intentionally engineered.Furthermore, since LLMs can be evaluated via natural language, this can enhance or replace comparatively simpler methods from ethology.This has led to the widespread adoption of language-based diagnostic evaluations, making it easier and more intuitive for practitioners to develop relevant tests.</p>
<p>However, this comes with important caveats.In trying to shed light on the workings of a black-box system that can produce language, it is tempting to use the simplest approach of asking the system about it.Self-report measures have been extensively used in psychology as well; but their reliability is questionable in humans (Jobe, 2003) as well as LLMs.Properties that such measures usually consider, such as personality, morality, or clinical disorders, are famously sensitive to prompting (Dominguez-Olmedo et al., 2023;Röttger et al., 2024); to the extent that several recent works even simulate groups of humans of different social groups, opinions, and personalities with differently prompted LLMs (Salewski et al., 2023;Park et al., 2022;Argyle et al., 2023;Shanahan et al., 2023).There remains value in using self-report stimuli from psychology -for example, to characterize behavior on a default prompt, as well as to understand how steerable (i.e.sensitive to prompting) models are along these dimensions.But results drawn from these measures should be taken contextually (e.g. as a property of a specific system prompt on a model) instead of as a fundamental or general property of the LLM itself.</p>
<p>In contrast, the empirical tradition in psychology is significantly different from self-reports.This tradition has yielded lasting understanding of natural intelligence (Frank et al., 2024), and is the tradition we argue is the most amenable for transferring insights to machine psychology.In this paradigm, externally observed behavior continues to be the measured experimental variable, but stimuli are designed such that different observed behaviors map onto and measure different internal representations, capabilities, or constructs -like compositionality, theory of mind, logic, causality, etc.A key principle is that experiments are hypothesis-driven: if the agent has representation or construct X, we would expect to see behavior Y, otherwise we would see behavior Z.We highlight two key principles from this tradition that are crucial to keep in mind when performing and interpreting machine psychology evaluations.First, does seeing behavior Y reliably imply having the construct X? To answer this, the design of a good control is crucial -to ensure that behavior Y does not have another explanation and does, in fact, implicate X.A large part of experimental psychology has been coming up with the right controls for these subtle constructs (Boring, 1954), and has been providing a valuable foundation for future research in machine psychology.Second, does the absence of behavior Y indicate the absence of the construct X?This is a more subtle question.Research in psychology often grapples with the fact that human performance can be noisy or biased; for example, humans may make mistakes even on an easy calculation, or produce ungrammatical language colloquially.These should not be taken to mean that they lack the abstract capability for math or language.These inconsistencies led to the concept of the performance-competence distinction (e.g.Chomsky, 1965): that the way humans perform in a particular situation may not fully capture their underlying competence.More recent work has suggested that similar issues apply when assessing the capabilities of machine learning systems (Firestone, 2020), and particularly LLMs (Lampinen, 2022).</p>
<p>Paradigms: The many aspects of intelligent behavior</p>
<p>There are many aspects of intelligent behavior, each of which has been studied by different sub-fields of the behavioral sciences.Each of these has developed domain-specific empirical paradigms.While some of these sub-fields (e.g.motor learning) and paradigms (e.g.pupillometry) are not directly transferable to LLMs since they rely on the existence of a physical body, several of these paradigms are purely linguistic and can be easily transferred.As LLMs expand in the kinds of stimuli they can interpret -e.g.visual (OpenAI, 2023b;Zhang, Huang, et al., 2024;Gemini Team et al., 2024) -and the ways in which they can interact with the world -e.g.embodiment and tool use (Mialon et al., 2023) -, the space of transferable paradigms increases.Humans also interact with several modalities, and the paradigms developed to understand us often compare and integrate these modalities (Schulze Buschoff et al., 2023) -e.g. the Stroop test which spans vision and reading capabilities (Scarpina and Tagini, 2017).</p>
<p>In this article, we focus on language-based tests, since these are the most widely used in the current research landscape.Moreover, we believe that even in light of the growing trend toward multi-modal models, language will remain a primary modality due to its fundamental role in models' reasoning processes.We concentrate on four research areas that can inform distinct strands in machine psychology research: heuristics and biases, social interactions, the psychology of language, and learning.Apart from these four areas, there are, of course, multiple other domains of psychology that can also provide valuable paradigms for, for instance when investigating creativity in LLMs (Stevenson et al., 2022), clinical psychology (Li, Li, et al., 2022), moral behavior (Khandelwal et al., 2024), and others.</p>
<p>Heuristics and biases</p>
<p>The heuristics and biases framework is one of the most influential research paradigms in psychology (Gigerenzer and Gaissmaier, 2011;Tversky and Kahneman, 1974).Heuristics are mental shortcuts that simplify reasoning or decision-making processes, and this field studies how such shortcuts can help explain both the successes and the biases in human behavior.The large existing literature on heuristics and biases in humans is a fertile ground for examining such shortcuts in the newest generation of LLMs -whose capabilities now overlap more with the human abilities this literature studies.Binz and Schulz (2023) were among the first to use this paradigm to better understand the decision-making processes of LLMs.They found that GPT-3 (Brown et al., 2020) displays some of the same cognitive biases observed in people.Several other works have also been done in this vein (Jones and Steinhardt, 2022;Yax et al., 2024;Hagendorff et al., 2023;Macmillan-Scott and Musolesi, 2024;Schulze Buschoff et al., 2023;Hayes et al., 2024;Coda-Forno, Binz, Wang, et al., 2024).Interestingly, there is evidence from several studies showing that, while the previous generation of models frequently exhibited human-like heuristics and biases, they have largely disappeared in the latest generation of LLMs (Chen, Liu, et al., 2023;Hagendorff et al., 2023).The test stimuli were originally designed to be challenging for human study participants and possibly no longer challenge the growing reasoning abilities in LLMs.This could also be due to leakage into the training set -we discuss this challenge in the section on design and analysis.</p>
<p>The literature on heuristics and biases also suggests that how a problem is phrased can influence how people solve it (Cheng and Holyoak, 1985;Tversky and Kahneman, 1981).It is well-known that LLMs are also susceptible to similar manipulations.For example, Dasgupta et al. (2022) have investigated whether LLMs are affected by the semantic content of logical reasoning problems using several existing tasks from the literature.They found that, like people, LLMs reason more accurately about familiar, believable, or grounded situations, compared to unfamiliar, unbelievable, or abstract problems.Likewise, Schubert et al., 2024 have shown that how LLMs learn in-context depends on the problem formulation.</p>
<p>Finally, people do not simply apply arbitrary heuristics.Instead, they use heuristics that are adapted to the problems they encounter during their everyday interactions with the world (Todd and Gigerenzer, 2012).In the context of LLMs, one can look at how the properties of the training data shape their behavior.For example, Chan et al., 2022 have demonstrated that the presence of in-context learning in LLMs can be traced back to data distributional properties such as burstiness, where items appear in clusters rather than being uniformly distributed over time, and the presence of large numbers of rarely occurring classes.Researchers also proposed that one should try to understand LLMs through the problem they are trained to solve, similarly to how behavioral scientists attempt to understand human cognition through the lens of ecological rationality (Todd and Gigerenzer, 2012;McCoy, Yao, et al., 2023;Jagadish et al., 2024).</p>
<p>Social interactions</p>
<p>Traditionally, developmental psychology explores how humans develop cognitively, socially, and emotionally throughout their lives.This includes studying the various factors that influence development, such as social intelligence or social skills.By applying paradigms from this area of developmental psychology to LLMs, researchers can gain deeper insights into how these models manage complex social interactions.In particular, once LLMs are deployed as chat agents, they should become versed in modeling human communicators.Therefore, it is important to assess the level of social intelligence in LLMs.One example in this context is the application of theory of mind tests to LLMs, where researchers use tasks from human experiments, such as those famously conducted by Wimmer and Perner (1983) and Perner et al. (1987).While early experiments with models such as GPT-3 showed that they struggle to solve theory of mind tasks (Sap et al., 2022), later models demonstrate an increasing ability to reliably infer unobservable mental states in others (Strachan et al., 2024;Holterman and Deemter, 2023;Moghaddam and Honey, 2023).Further related research examines how LLM performance on theory of mind tests compares to that of children (Duijn et al., 2023), LLM ability to handle higher-order theory of mind tasks requiring recursive reasoning about multiple mental states (Street et al., 2024), or measures the robustness of theory of mind test setups against distracting alterations in the tasks LLMs receive as inputs (Ullman, 2023).As theory of mind tests measure, among other things, the ability to understand false beliefs, further research has explored the emerging capability of LLMs to induce false beliefs in other agents (Hagendorff, 2024a), or how LLMs trade off various communicative values like honesty and helpfulness (Liu et al., 2024) -these investigations also contribute to understanding and improving alignment with human values for AI safety (Ji et al., 2023).</p>
<p>The space of relevant paradigms increases as LLMs are allowed to interact through self-reflection (Nair et al., 2023), self-instruction (Wang, Wei, et al., 2022), or in swarms (Zhuge et al., 2023).For example, researchers looked at cooperative and coordinative behavior in LLMs playing games, revealing persistent behavioral signatures in the models (Akata et al., 2023).Similarly, researchers investigated cooperative or competitive LLMs behavior in psychologyinspired dilemma situations to assess the ability of LLMs to participate in real-world negotiations (Phelps and Russell, 2024).Another study, which is influenced by works in human social psychology, looked at how multiple LLMs form and evolve networks, investigating micro-level network principles such as preferential attachment or triadic closure, as well as macro-level principles such as community structures (Papachristou and Yuan, 2024).In sum, machine psychology can reveal patterns of social behavior and interaction among LLMs, individually and collectively, be it for problem solving or world simulation (Guo et al., 2024).By drawing from human developmental psychology and social dynamics, researchers can better understand and design LLMs that navigate complex social interactions and exhibit advanced social skills.</p>
<p>Psychology of language</p>
<p>A long history of work has studied the psychology of how humans use and understand language, ranging from how they use semantic and syntactic features to understand a sentence to how they use pragmatic inferences in a discourse context to help interpret what someone has said.Correspondingly, a long-standing body of work has studied how language processing models capture these features of human language processing.Early connectionist works studied these topics in simple recurrent predictive models (Elman, 1991;McClelland et al., 1989); more recently, researchers have applied similar techniques to study LLMs.A wide range of work has studied what models learn about syntax (Linzen and Baroni, 2021), often using methods from psycholinguistics.For example, Wilcox et al. (2023) used psycholinguistics-inspired surprisal measures to show that LLMs learn filler-gap dependencies, a challenging syntactic structure.Other researchers have used related measures to study what LLMs learn about the semantics of entailment (Merrill et al., 2024).Moreover, researchers used psycholinguistic techniques like priming to study how models represent and process language (Prasad et al., 2019;Sinclair et al., 2022), and methods like deconfounded stimuli to identify where models may rely on semantic heuristics rather than syntax (McCoy, Pavlick, et al., 2019).Several recent works (Hu, Floyd, et al., 2023;Ruis, Khan, et al., 2023) studied pragmatic judgments of LLMs, and found that larger models, as well as those with instruction tuning, tend to better approximate human responses and error patterns -though some deficiencies remain.In another study, researchers examined long-form analogies generated by ChatGPT, finding that AI-generated analogies lack some human-like psycholinguistic properties (Seals and Shalin, 2023), particularly in text cohesion, language, and readability.Furthermore, researchers applied garden path sentences -sentences that lead the reader to initially interpret them incorrectly due to their ambiguous structure -to LLMs, showing that the models respond similarly to humans (Aher et al., 2023;Christianson et al., 2001).At a higher level, some researchers have drawn inspiration from aspects of human language development to attempt to identify the causes of the relative data inefficiency of language models (Warstadt et al., 2023;Frank, 2023).In each of these cases, methods and ideas from psychology and psycholinguistics provide guidance on how to assess processes through language behaviors in LLMs, potentially by drawing comparisons between LLMs and humans.</p>
<p>Learning</p>
<p>The psychology of learning is concerned with how individuals acquire and retain knowledge and skills.At first blush, it may appear that experimental paradigms for the study of learning are less applicable to LLMs, given that the aim of behavioral experiments is often to help uncover the underlying learning algorithm -whereas for LLMs the learning algorithms used in training are designed and already known.However, the behavioral sciences can still benefit from the study of LLMs in this context, since LLMs exhibit learning abilities that were not explicitly designed into the models (they are emergent), and thus one does not understand the underlying learning algorithm.In particular, LLMs exhibit emergent in-context learning -the ability to learn from context (the prompt) without requiring any gradient-based updates in weights (Brown et al., 2020).Understanding in-context learning is a burgeoning field that is rapidly gaining in importance, given the increasing size of LLMs context windows and consequent gains in capabilities, e.g. the capability to learn an entire language from context alone (Munkhdalai et al., 2024;Gemini Team et al., 2024), or the ability to overcome safety fine-tuning (Anil et al., 2024;Zheng et al., 2024).</p>
<p>Uncovering the implicit learning algorithm implemented by in-context learning is a burgeoning research field, and utilizes many of the methods common in cognitive science.For example, multiple studies have compared the outputs of transformer in-context learning with the outputs of hypothesized learning algorithms (Oswald et al., 2023;Akyürek et al., 2022).This is a staple of cognitive modeling, and could potentially benefit even further from model comparison procedures from psychology and statistics (Yang, 2006;Arlot and Celisse, 2010;Vrieze, 2012).Recent work in cognitive science has used machine learning to discover new theories of human decision-making (Peterson et al., 2021) -it might be interesting to apply related approaches to in-context learning as well.Researchers might also benefit from considering particular models as normative starting points (Niv, 2009).</p>
<p>Researchers may also wish to understand other interesting and important characteristics of learning, such as inductive biases and generalization, the data dependence of learning, and the dynamics of learning over time.These characteristics are often not obvious even in cases where the learning algorithm is known, and thus researchers would like to understand them not only for in-context learning, but also for other forms of LLM learning, e.g.self-supervised gradient-based learning, reinforcement learning (Ouyang et al., 2022), or "fast" memory retrieval (Borgeaud et al., 2022;Lewis et al., 2020).</p>
<p>To characterize inductive biases and generalization of LLMs, researchers have borrowed both concepts and experimental paradigms from cognitive sciences (Schubert et al., 2024;Coda-Forno, Binz, Akata, et al., 2023) and Bayesian inference (Xie et al., 2022).Studies utilized paradigms for measuring systematic generalization to characterize those capabilities in LLMs, and as inspiration to improve these abilities (Lake and Baroni, 2023;Ruis, Andreas, et al., 2022).Webb et al. (2023) created novel variants of classic analogy problems from cognitive science, in order to examine the analogical capabilities of large language models.Chan et al. (2022) have borrowed ideas and experimental paradigms on "rule-based" vs. "exemplar-based" generalization to characterize the inductive biases of in-weights vs. in-context learning in transformers.Furthermore, researchers borrowed paradigms and measures from developmental psychology to characterize the domains where LLM inductive biases may match those of children, and where they may fall short (including in causal reasoning and innovation) (Kosoy et al., 2023;Yiu et al., 2023).</p>
<p>To characterize the data dependence of in-context learning, existing work has drawn inspiration from research in developmental psychology on skewed and bursty distributions (Chan et al., 2022).An important aspect of data dependence is the structure of data over time (during training).AI researchers have long drawn inspiration from curriculum learning in human and non-human animals to better understand how to structure training data so that earlier learning on easier tasks can scaffold later learning on harder tasks (Bengio et al., 2009).There remain many areas of behavioral research on learning that may serve as rich sources of inspiration on data dependence, e.g. research on repetition and spacing (Dempster, 1989), working memory (Baddeley, 2010;Chai et al., 2018), blocking vs. interleaving tasks (Carvalho and Goldstone, 2015), and continual learning (Greco et al., 2019).Data dependence is particularly interesting for LLMs because text training data (being sourced largely from unstructured web-scale corpora) is very different from the structured training data typically used for traditional discriminatory machine learning techniques, and because data is one of the major levers one can manipulate in training LLMs to adjust their behaviors.</p>
<p>Design and analysis: Good behavioral experimentation</p>
<p>Computer science has not historically been an empirical science.While machine learning (especially since the era of neural network models) has been significantly driven by empirical rather than theoretical work, the settings under which those protocols were developed -a test set that is fixed for all practitioners and is effectively infinitely large -no longer hold in the small test-only behavioral experiments setting.Current LLMs are famously sensitive to small changes in prompt structure or they rely on shallow syntactic heuristics (McCoy, Pavlick, et al., 2019), and studies that are not careful about testing the robustness of their conclusions risk being spurious and non-generalizable.Psychology too has had its own share of reproducibility crises (Open Science Collaboration, 2015;Haibe-Kains et al., 2020), and machine psychology should not share the same fate.In this section, we provide recommendations for sound methodologies in behavioral test settings with LLMs, which should be valuable to practitioners in the field of machine psychology.</p>
<p>Prompting methods and biases</p>
<p>Many studies conducted in the field of machine psychology have a significant shortcoming in common, namely that they do not avoid training data contamination.They use prompts from existing psychology studies and apply them to LLMs without changing their wording, task orders, etc.In this way, LLMs are likely to have already experienced identical or similar tasks during training, thus causing LLMs to simply reproduce known token patterns.When adopting test frameworks from psychology -meaning vignettes, cognitive tasks, or other test setups -researchers must ensure that LLMs have never seen the tests before and go beyond mere memorization.Hence, prompts may indeed be structurally like already existing tasks, but they should contain new wordings, agents, orders, actions, etc.That being said, some experiments may be procedurally generated (instead of consisting of a static dataset), which makes them inherently less susceptible to data contamination issues (Coda-Forno, Binz, Wang, et al., 2024).</p>
<p>Another common shortcoming of several existing machine psychology studies is that they rely on small sample sizes or convenience samples, meaning non-systematic sequences of prompts.Sampling biases in the used benchmarks or task datasets, which are especially prevalent in small sample sizes, can diminish the quality of machine psychology studies.This is because slight changes in prompts can change model outputs significantly.Because of this high sensitivity to prompt wording, it is important to test multiple versions of one task and to create representative samples, meaning batteries of varied prompts.Only in this way can one reliably measure whether a certain behavior is systematically reoccurring and generalizable (Yarkoni, 2022).Furthermore, LLMs can succumb to various biases influencing the processing of prompts (Zhao, Wallace, et al., 2021;Chan et al., 2022).Recency biases in LLMs, for instance, lead to a tendency to rely more heavily on information appearing toward the end of prompts.LLMs can also possess a common token bias, meaning that models are biased toward outputting tokens that are common in their training data.Moreover, majority label biases can cause LLMs to be skewed towards labels, classes, or examples that are frequent in a few-shot learning setting.Technical biases like these can at least in part be controlled for when designing prompts or prompt variations that tend to avoid triggering them.If this is not done, LLMs may rely on shortcuts exploiting such biases.</p>
<p>Eliciting capabilities with prompts</p>
<p>The standard prompt design, comprising a vignette plus an open-or close-ended question or task, can be enhanced by prefixes or suffixes eliciting improved reasoning capabilities in LLMs.On the other hand, omitting such prefixes and suffixes can lead to underestimations of the model's capabilities.Although it is likely that most specific prompt augmentations have a positive influence on one kind of task but not another, reducing our ability to systematically understand LLM behavior, a few prompt design approaches have nonetheless been found to confer broader performance benefits.Most notably, (zero-shot) chain-of-thought prompting (Wei, Wang, et al., 2022;Kojima et al., 2022) -which simply adds "Let's think step by step" at the end of a prompt -improves reasoning performance.This can be extended even further by generating multiple chain-of-thought reasoning paths and taking the majority response as the final one (Wang, Wei, et al., 2022).Similar to chain-of-thought prompting is least-to-most prompting, which also decomposes problems into a set of subproblems to increase accuracy in LLMs (Zhou et al., 2022).Yet another approach is to frame questions in a multiple-choice format.This was shown to improve reasoning capabilities in some cases (Kadavath et al., 2022), but can also limit them because LLMs might be prompted to provide brief responses, thereby circumventing reasoning in the process of prompt completion.Nevertheless, many prominent NLP benchmarks use multiple choice formats instead of open-ended questions.Here, one must keep in mind that different expressions of the same concept compete for probability, which can lower the chances of selecting the correct answer (Holtzman et al., 2021).Moreover, one has to consider potential recency biases, which require neutralizing this effect by shuffling the order of answers in multiple test runs to cover all possible combinations.Another method to increase reasoning is to utilize the ability for few-shot learning in LLMs (Brown et al., 2020), where the LLM's performance improves after repeated exposure to a given task.Moreover, self-reflection, meaning the automated, recursive criticizing and subsequent self-improvement of LLM outputs by the LLM itself, is a further technique that can improve reasoning abilities (Nair et al., 2023;Kim et al., 2023).Regarding improvements in symbolic or numeric reasoning, another technique is to prompt LLMs to use code for solving tasks (Zhang, Ge, et al., 2024).Eventually, all mentioned methods to improve reasoning can be not just leveraged for machine psychology; they can also become objects of study themselves.</p>
<p>Setting parameters and evaluating outputs</p>
<p>LLMs come with a variety of parameters researchers can set.For example, most models come in a variety of sizes.Analyses across different sizes are valuable: while the largest ones usually have the highest capabilities, some recent works find "inverse-scaling" (McKenzie et al., 2023).Moreover, temperature settings control randomness.If exact reproducibility is required, studies should use temperature 0 or assign a seed to ensure complete determinacy.However, this can be prone to (intentional or unintentional) biases in seed choice.The effect of temperature on capabilities is not established (Renze and Guven, 2024), and reporting averages or "best of K" -considering all the responses over K samples that meet certain simple criteria, e.g.formatting (Chen, Tworek, et al., 2021) -is valuable.</p>
<p>After conducting the experiments, a list of LLM responses must be evaluated and compared with the ground truth.The simplest case is when the results can be framed and scored as a multiple-choice question -though even in this case, scoring the answers so that the model responds directly inline, rather than selecting a choice, can yield more signal (Hu and Levy, 2023).If possible, multiple scoring methods should be compared, to evaluate whether the effects are dependent on the scoring method (Tsvilodub et al., 2024).If the questions must be answered with free generations, the evaluation process can still be automated if the results exhibit sufficient simplicity and regularity, meaning that the LLM responses are similar to the ground truth strings in terms of length and wording, which is particularly common when using masked language models.Methods such as testing word overlaps with regular expressions or using metrics such as the F1 score can be employed.State-of-the-art LLMs, however, tend to produce highly variable and comprehensive outputs, which can complicate classification.While stop sequences, token limits, or prompt instructions that interrupt further text generation can facilitate classification by promoting output uniformity, they also improperly constrain LLM behavior.Therefore, researchers are increasingly relying on LLM-based evaluations of outputs where a single model or multiple stacked model instances perform the classification using carefully crafted instructions.Although this method might still be inaccurate for very comprehensive outputs, a solution is to instruct the LLM under scrutiny to output its final answer or summary after a specific string sequence like "####" (Cobbe et al., 2021).This approach allows the LLM to reason during verbose prompt completions, which is necessary for many prompt engineering techniques such as chain-of-thought reasoning.The classification then only involves processing the string following "####".If this method still proves to be unreliable, evaluations might have to be performed manually, possibly by hiring research assistants or contractors.Following the evaluation, a statistical analysis can be carried out.</p>
<p>Discussion</p>
<p>Machine psychology provides a new approach to explaining AI.Instead of interpreting a neural network's design components (Barredo Arrieta et al., 2019), one analyzes the relationships between inputs and outputs, i.e. prompt design and prompt completion.Although this may allow the identification of hitherto unknown abilities or behavioral traits in LLMs, interpreting LLM responses comes with a challenge.A strong tendency exists to confer mental concepts or psychological terms to LLMs that were hitherto reserved for human and animal minds.This tendency manifests in common terms like "machine learning," but will become more prevalent in machine psychology when concepts such as reasoning (Huang and Chang, 2022), intuition (Hagendorff et al., 2023), creativity (Stevenson et al., 2022), intelligence (Webb et al., 2023), personality (Miotto et al., 2022), mental illnesses (Li, Li, et al., 2022), etc. are transferred to LLMs.In this context, researchers have demanded caution by stressing that the underlying neural mechanisms for these concepts are different in humans and machines (Shanahan, 2022;Mahowald et al., 2024).Moreover, many psychological concepts are normatively laden and can foster mismatches in expectations between AI experts and the public regarding machine capabilities (Shevlin and Halina, 2019).Nevertheless, the problem that many abilities in LLMs cannot be reasonably grasped by only referring to the inner workings of their neural architecture remains.</p>
<p>By adopting a concept from ethnography, one could call such an approach "thin descriptions" (Ryle, 1971;Geertz, 1973), meaning that one only explains internal representations in AI systems, for instance via activation atlases, which visualize how different parts of a neural network respond to various inputs (Carter et al., 2019).In this sense, LLMs simply hijack humans' intuitions to explain machine behavior patterns by using psychological or other anthropocentric terms.Contrary to thin descriptions, though, there are "thick descriptions."They imply using psychological terms to add a layer of explainability.LLMs are, like the human brain, black boxes to some extent.By applying psychological terms to them, the explanatory power increases, even if no direct neural correlates to these terms exist.This holds for humans, too, where mental terms used to explain behavior do not directly correlate with specific sets of neural activations.By postulating (mental) unobservable states, be it with regard to brains or artificial neural networks, one increases explanatory resources (Sellars, 1997).Thick descriptions help in making sense of LLMs when thin descriptions are insufficient to explain behavioral patterns.Thin descriptions assume that LLMs merely possess syntax or a statistical capacity to associate words (Searle, 1980;Floridi and Chiriatti, 2020;Bender et al., 2021), but not semantics.Thick descriptions, though, assume that LLMs show patterns and regularities that go beyond mere syntax.These patterns can be explained by means of machine psychology.</p>
<p>Beyond potential habituations regarding the use of terminology borrowed from psychology in the context of machines, machine psychology, as a nascent field of research, aims to identify behavioral patterns, emergent abilities, and mechanisms of decision-making and reasoning in LLMs by treating them as participants in psychology experiments.This new discipline of evaluating LLMs will become even more important when taking multimodal or augmented LLMs into account, meaning LLMs that are allowed to interact with images, external information sources, sensory data, physical objects, and various other tools (Mialon et al., 2023;Schick et al., 2023;Ma et al., 2024).Moreover, once test settings for machine psychology are established, researchers can investigate how LLMs develop over time by applying the same tasks multiple times, yielding longitudinal data.This data can serve as a baseline to extrapolate trends regarding the development of reasoning abilities in LLMs.Such estimations may be increasingly important for AI safety and AI alignment research to predict future behavioral potentials in LLMs.By gaining a deeper understanding of these potentials, machine psychology is providing a new approach to AI explainability as well as an important addition to traditional benchmarking methods in natural language processing.</p>
<p>Figure 1 :
1
Figure 1: Overview of key concepts of machine psychology.</p>
<p>Author contributionsTH and ID conceptualized and led the initial design of the manuscript.TH and ID wrote the initial drafts, with contributions from MB, SCYC, AL, JW, ZA, and ES to flesh out the sections and create the figure.All authors assisted with iterations and edited and reviewed the paper.
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. Aher, Rosa I Gati, Adam Arriaga, Kalai Tauman, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Playing repeated games with Large Language Models. Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz, arXiv2023</p>
<p>What learning algorithm is in-context learning? Investigations with linear models. Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou, arXiv2022</p>
<p>More is different: Broken symmetry and the nature of the hierarchical structure of science. Philip W Anderson, Science. 1771972</p>
<p>Out of One, Many: Using Language Models to Simulate Human Samples. Cem Anil, Political Analysis. 312024. 2024. 2023Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku</p>
<p>A survey of cross-validation procedures for model selection. Sylvain Arlot, Alain Celisse, Statistics Surveys. 42010</p>
<p>Working memory. Alan Baddeley, Current Biology. 202010</p>
<p>Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI. Barredo Arrieta, Alejandro , Information Fusion. 582019</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine Learning2009</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 12062023</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, arXiv2021</p>
<p>Improving Language Models by Retrieving from Trillions of Tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, Proceedings of the 39th International Conference on Machine Learning. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, Sivan Sabato, the 39th International Conference on Machine Learning2022162</p>
<p>The Nature and History of Experimental Control. Edwin G Boring, The American Journal of Psychology. 671954</p>
<p>A large annotated corpus for learning natural language inference. Samuel R Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lluís Màrquez, Chris Callison-Burch, Jian Su, the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Language Models are Few-Shot Learners. Tom Brown, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Sparks of Artificial General Intelligence: Early experiments with GPT-4. Sébastien Bubeck, arXiv2023</p>
<p>Exploring Neural Networks with Activation Atlases. Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, Chris Olah, Distill. 432019</p>
<p>The benefits of interleaved and blocked study: different tasks benefit from different schedules of study. Paulo F Carvalho, L Robert, Goldstone, Psychonomic Bulletin &amp; Review. 222015</p>
<p>Working Memory From the Psychological and Neurosciences Perspectives: A Review. Wen Chai, Aini Jia, Abd Ismafairus, Jafri Hamid, Abdullah Malin, Frontiers in Psychology. 92018</p>
<p>Data Distributional Properties Drive Emergent In-Context Learning in Transformers. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James Mcclelland, Felix Hill, Advances in Neural Information Processing Systems. 352022</p>
<p>Language Model Behavior: A Comprehensive Survey. Tyler A Chang, Benjamin K Bergen, Computational Linguistics. 502024</p>
<p>Evaluating Large Language Models Trained on Code. Mark Chen, Jerry Tworek, arXiv2021</p>
<p>The emergence of economic rationality of GPT. Yiting Chen, Tracy Xiao Liu, You Shan, Songfa Zhong, Proceedings of the National Academy of Sciences. 120e23162051202023</p>
<p>Pragmatic reasoning schemas. Patricia W Cheng, Keith J Holyoak, Cognitive Psychology. 171985</p>
<p>Abstraction and Reasoning Challenge. François Chollet, Katherine Tong, Walter Reade, Julia Elliott, 2020</p>
<p>Aspects of the Theory of Syntax. Noam Chomsky, 1965MIT Press</p>
<p>Thematic Roles Assigned along the Garden Path Linger. Christianson, Andrew Kiel, John F Hollingworth, Fernanda Halliwell, Ferreira, Cognitive Psychology. 422001</p>
<p>Training Verifiers to Solve Math Word Problems. Karl Cobbe, arXiv2021</p>
<p>Meta-in-context learning in large language models. Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, Eric Schulz, Advances in Neural Information Processing Systems. 362023</p>
<p>CogBench: a large language model walks into a psychology lab. Julian Coda-Forno, Marcel Binz, Jane X Wang, Eric Schulz, arXiv2024</p>
<p>Towards Automated Circuit Discovery for Mechanistic Interpretability. Arthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Language models show human-like content effects on reasoning. Dasgupta, Andrew K Ishita, Lampinen, C Y Stephanie, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv2022</p>
<p>Spacing effects and their implications for theory and practice. Frank N Dempster, Educational Psychology Review. 11989</p>
<p>Questioning the Survey Responses of Large Language Models. Ricardo Dominguez-Olmedo, Moritz Hardt, Celestine Mendler-Dünner, arXiv2023</p>
<p>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Max J Duijn, Van, Tom Bram Van Dijk, Kouwenhoven, Marco R Werner De Valk, Peter Spruit, Van Der Putten, Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL). Jing Jiang, David Reitter, Shumin Deng, the 27th Conference on Computational Natural Language Learning (CoNLL)2023Children Aged 7-10 on Advanced Tests</p>
<p>Statistical Methods for the Behavioral Sciences. Allen L Edwards, 1954Rinehart</p>
<p>Can GPT-3 Pass a Writer's Turing Test. Katherine Elkins, Jon Chun, Journal of Cultural Analytics. 52020</p>
<p>Distributed representations, simple recurrent networks, and grammatical structure. Jeffrey L Elman, Machine Learning. 71991</p>
<p>Towards artificial general intelligence via a multimodal foundation model. Nanyi Fei, Nature Communications. 132022</p>
<p>Performance vs. competence in human-machine comparisons. Leon Festinger, Ed, Ed Daniel, Katz, Proceedings of the National Academy of Sciences. Holt, Rinehart and Winston1171953. 2020Research methods in the behavioral sciences</p>
<p>GPT-3: Its Nature, Scope, Limits, and Consequences. Luciano Floridi, Massimo Chiriatti, Minds and Machines. 302020</p>
<p>Bridging the data gap between children and large language models. Michael C Frank, Trends in Cognitive Sciences. 27112023</p>
<p>Experimentology: An Open Science Approach to Experimental Psychology Methods. Michael C Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom E Hardwicke, Robert D Hawkins, Maya B Mathur, Rondeline Williams, 2024MIT Press</p>
<p>Scaling and evaluating sparse autoencoders. Leo Gao, Tom Dupré La Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, Jeffrey Wu, arXiv2024</p>
<p>The Interpretation of Cultures: Selected Essays. Clifford Geertz, 1973Basic Books</p>
<p>Gemini Team et al. "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, arXivNature Machine Intelligence. 22020. 2024Shortcut learning in deep neural networks</p>
<p>Heuristic decision making. Gerd Gigerenzer, Wolfgang Gaissmaier, Annual Review of Psychology. 622011</p>
<p>Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering. Claudio Greco, Barbara Plank, Raquel Fernández, Raffaella Bernardi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Anna Korhonen, David Traum, Lluís Màrquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Alike performance during nonverbal episodic learning from diversely imprinted neural networks. Georg Grön, David Schul, Volker Bretschneider, Matthias W Wunderlich, Riepe, European Journal of Neuroscience. 182003</p>
<p>Large Language Model based Multi-Agents: A Survey of Progress and Challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv2024</p>
<p>Deception abilities emerged in large language models. Thilo Hagendorff, Proceedings of the National Academy of Sciences. 1212024</p>
<p>Mapping the Ethics of Generative AI: A Comprehensive Scoping Review. arXiv2024</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Thilo Hagendorff, Sarah Fabi, Michal Kosinski, Nature Computational Science. 32023</p>
<p>Transparency and reproducibility in artificial intelligence. Benjamin Haibe-Kains, Nature. 5862020</p>
<p>Relative Value Biases in Large Language Models. William M Hayes, Nicolas Yax, Stefano Palminteri, arXiv2024</p>
<p>Measuring Mathematical Problem Solving With the MATH Dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, Jacob Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems. 2021</p>
<p>What shapes feature representations? Exploring datasets, architectures, and training. Katherine Hermann, Andrew Lampinen, 34th Conference on Neural Information Processing Systems. 2020</p>
<p>Does ChatGPT have Theory of Mind?. Bart Holterman, Kees Van Deemter, In: arXiv. 2023</p>
<p>Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, Luke Zettlemoyer, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language. Eghbal A Hosseini, Evelina Fedorenko, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, 202336</p>
<p>A fine-grained comparison of pragmatic language understanding in humans and language models. Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, Edward Gibson, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Prompting is not a substitute for probability measurements in large language models. Jennifer Hu, Roger P Levy, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Towards Reasoning in Large Language Models: A Survey. Jie Huang, Kevin Chen, -Chuan Chang, arXiv2022</p>
<p>Running cognitive evaluations on large language models: The do's and the don'ts. Anna A Ivanova, arXiv2023</p>
<p>Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models. Anna A Ivanova, arXiv2024</p>
<p>Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks. Akshay K Jagadish, Julian Coda-Forno, Mirko Thalmann, Eric Schulz, Marcel Binz, arXiv2024</p>
<p>AI Alignment: A Comprehensive Survey. Jiaming Ji, arXiv2023</p>
<p>Cognitive psychology and self-reports: models and methods. Jared B Jobe, Quality of Life Research. 122003</p>
<p>Could a Neuroscientist Understand a Microprocessor. Eric Jonas, Konrad Paul, Kording , PLOS Computational Biology. 132017</p>
<p>Capturing failures of large language models via human cognitive biases. Erik Jones, Jacob Steinhardt, Advances in Neural Information Processing Systems. 352022</p>
<p>Language Models (Mostly) Know What They Know. Saurav Kadavath, arXiv2022</p>
<p>Scaling Laws for Neural Language Models. Jared Kaplan, arXiv2020</p>
<p>xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. Mohammad Khan, M Saiful Abdullah Matin, Xuan Long Bari, Weishi Do, Md Wang, Shafiq Rizwan Parvez, Joty, arXiv2023</p>
<p>Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test. Aditi Khandelwal, Utkarsh Agarwal, Kumar Tanmay, Monojit Choudhury, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European Chapterthe Association for Computational Linguistics. Association for Computational Linguistics2024</p>
<p>Language Models can Solve Computer Tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv2023</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa, arXiv2022</p>
<p>Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. Eliza Kosoy, Emily Rose Reagan, Leslie Lai, Alison Gopnik, Danielle Krettek Cobb, NeurIPS Workshop: AI Meets Moral Philosophy and Moral Psychology. 2023</p>
<p>Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model. Sreejan Kumar, Theodore R Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth A Norman, Thomas L Griffiths, Robert D Hawkins, Samuel A Nastase, BioRxiv. 2022</p>
<p>Human-like systematic generalization through a meta-learning neural network. Brenden M Lake, Marco Baroni, Nature. 6232023</p>
<p>Can language models handle recursively nested grammatical structures? A case study on comparing models and humans. Andrew Lampinen, Kyle, arXiv2022</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Task Contamination: Language Models May Not Be Few-Shot Anymore. Changmao Li, Jeffrey Flanigan, arXiv2023</p>
<p>Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological Perspective. Xingxuan Li, Yutong Li, Linlin Liu, Lidong Bing, Shafiq Joty, arXiv2022</p>
<p>Syntactic Structure from Deep Learning. Tal Linzen, Marco Baroni, Annual Review of Linguistics. 712021</p>
<p>How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?. Ryan Liu, Theodore R Sumers, Ishita Dasgupta, Thomas L Griffiths, In: arXiv. 2024</p>
<p>GPoeT-2: A GPT-2 Based Poem Generator. Kai - Lo, Rami Ling, Philipp Ariss, Kurz, arXiv2022</p>
<p>DrEureka: Language Model Guided Sim-To-Real Transfer. Yecheng Ma, RSSWilliam Jason, RSSHung-Ju Liang, RSSSam Wang, RSSYuke Wang, RSSLinxi Zhu, RSSOsbert Fan, RSSDinesh Bastani, RSSJayaraman, RSSRobotics: Science and Systems. 2024</p>
<p>(Ir)rationality and cognitive biases in large language models. Olivia Macmillan-Scott, Mirco Musolesi, Royal Society Open Science. 112024</p>
<p>Dissociating language and thought in large language models. Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Trends in Cognitive Sciences. 2862024</p>
<p>DataPerf: Benchmarks for Data-Centric AI Development. Mark Mazumder, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, 202436</p>
<p>Sentence comprehension: A parallel distributed processing approach. Jay L Mcclelland, Mark St John, Roman Taraban, Language and Cognitive Processes. 41989</p>
<p>Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve. R Mccoy, Shunyu Thomas, Dan Yao, Matthew Friedman, Thomas L Hardy, Griffiths, arXiv2023</p>
<p>Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. Tom Mccoy, Ellie Pavlick, Tal Linzen, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Inverse Scaling: When Bigger Isn't Better. Ian R Mckenzie, arXiv2023</p>
<p>Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment. William Merrill, Zhaofeng Wu, Norihito Naka, Yoon Kim, Tal Linzen, arXiv2024</p>
<p>Augmented Language Models: a Survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta , arXiv2023</p>
<p>Who is GPT-3? An Exploration of Personality, Values and Demographics. Marilù Miotto, Nicola Rossberg, Bennett Kleinberg, arXiv2022</p>
<p>Boosting Theory-of-Mind Performance in Large Language Models via Prompting. Shima Moghaddam, Christopher J Rahimi, Honey, arXiv2023</p>
<p>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal, arXiv2024</p>
<p>DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. Varun Nair, Elliot Schumacher, Geoffrey Tso, Anitha Kannan, arXiv2023</p>
<p>Estimating the reproducibility of psychological science. Yael Niv, Science. 3492009. 2015Reinforcement learning in the brain</p>
<p>Optimizing Language Models for Dialogue. Openai, Chatgpt, 2022visited on 02/13/2023</p>
<p>. 2023GPT-4V(ision) System Card. 2023Technical Reportvisited on 03/19/2023. visited on 10/13/2023</p>
<p>Transformers learn in-context by gradient descent. Johannes Oswald, Eyvind Von, Ettore Niklasson, João Randazzo, Alexander Sacramento, Andrey Mordvintsev, Max Zhmoginov, Vladymyrov, Proceedings of the 40th International Conference on Machine Learning. 1464. JMLR, 2023. the 40th International Conference on Machine Learning. 1464. JMLR, 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Advances in Neural Information Processing Systems. 202235</p>
<p>Network Formation and Dynamics Among Multi-LLMs. Marios Papachristou, Yuan Yuan, arXiv2024</p>
<p>Social Simulacra: Creating Populated Prototypes for Social Computing Systems. Joon Park, Lindsay Sung, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Three-year-olds' difficulty with false belief: The case for a conceptual deficit. Josef Perner, Susan R Leekam, Heinz Wimmer, The British Journal of Developmental Psychology. 51987</p>
<p>Using large-scale experiments and machine learning to discover theories of human decision-making. Joshua C Peterson, D David, Mayank Bourgin, Daniel Agrawal, Thomas L Reichman, Griffiths, 2021</p>
<p>The Machine Psychology of Cooperation: Can GPT models operationalise prompts for altruism, cooperation, competitiveness and selfishness in economic games?. Steve Phelps, Yvan I Russell, In: arXiv. 2024</p>
<p>Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models. Grusha Prasad, Marten Van Schijndel, Tal Linzen, 23rd Conference on Computational Natural Language Learning. CoNLL2019. 2019</p>
<p>Robust speech recognition via large-scale weak supervision. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever, International Conference on Machine Learning. 2023</p>
<p>Machine behaviour. Iyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh Bongard, Jean-François Bonnefon, Cynthia , Nature. 5682019</p>
<p>The Effect of Sampling Temperature on Problem Solving in Large Language Models. Matthew Renze, Erhan Guven, arXiv2024</p>
<p>Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models. Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy, arXiv2024</p>
<p>Improving Systematic Generalization Through Modularity and Augmentation. Laura Ruis, Jacob Andreas, Brenden M Lake, arXiv2022</p>
<p>The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs. Laura Ruis, Akbir Eline, Stella Khan, Sara Biderman, Tim Hooker, Edward Rocktäschel, Grefenstette, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, 202336</p>
<p>ImageNet Large Scale Visual Recognition Challenge. Olga Russakovsky, International Journal of Computer Vision. 1152015</p>
<p>. Gilbert Ryle, 1971HutchinsonCollected Papers</p>
<p>In-Context Impersonation Reveals Large Language Models' Strengths and Biases. Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata, arXiv2023</p>
<p>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. Maarten Sap, Le Ronan, Daniel Bras, Yejin Fried, Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2022</p>
<p>The Stroop Color and Word Test. Federica Scarpina, Sofia Tagini, Frontiers in Psychology. 82017</p>
<p>Are Emergent Abilities of Large Language Models a Mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, Proceedings of the 37th International Conference on Neural Information Processing Systems. the 37th International Conference on Neural Information Processing SystemsCurran Associates Inc2425. 2023</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv2023</p>
<p>Large pre-trained language models contain human-like biases of what is right and wrong to do. Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, Kristian Kersting, Nature Machine Intelligence. 432022</p>
<p>In-context learning agents are asymmetric belief updaters. Johannes A Schubert, K Akshay, Marcel Jagadish, Eric Binz, Schulz, arXiv2024</p>
<p>Visual cognition in multimodal large language models. Schulze Buschoff, M Luca, Elif Akata, Matthias Bethge, Eric Schulz, arXiv2023</p>
<p>Should artificial intelligence be interpretable to humans. Matthew D Schwartz, Nature Reviews Physics. 42022</p>
<p>Long-form analogies generated by chatGPT lack human-like psycholinguistic properties. S M Seals, Valerie L Shalin, arXiv2023</p>
<p>Minds, brains, and programs. John R Searle, Behavioral and Brain Sciences. 5681980</p>
<p>Empiricism and the Philosophy of Mind. Wilfrid Sellars, 1997Harvard University Press</p>
<p>Talking About Large Language Models. Murray Shanahan, arXiv2022</p>
<p>Role play with large language models. Murray Shanahan, Kyle Mcdonell, Laria Reynolds, Nature. 6232023</p>
<p>Apply rich psychological terms in AI with care. Henry Shevlin, Marta Halina, Nature Machine Intelligence. 12019</p>
<p>Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations. Arabella Sinclair, Jaap Jumelet, Willem Zuidema, Raquel Fernández, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Causal ImageNet: How to discover spurious features in Deep Learning?. Sahil Singla, Soheil Feizi, In: arXiv. 2021</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, arXiv2022</p>
<p>Putting GPT-3's Creativity to the (Alternative Uses) Test. Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, Han Van Der Maas, arXiv2022</p>
<p>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Testing theory of mind in large language models and humans. James W A Strachan, Nature Human Behaviour. 82024</p>
<p>LLMs achieve adult human performance on higher-order theory of mind tasks. Winnie Street, arXiv2024</p>
<p>Predictions from language models for multiplechoice tasks are not robust under variation of scoring methods. Peter M Todd, Gerd Gigerenzer, ; Tsvilodub, Hening Polina, Sharon Wang, Michael Grosch, Franke, arXivEcological Rationality: Intelligence in the World. Oxford University Press2012. 2024</p>
<p>Judgment under Uncertainty: Heuristics and Biases. Amos Tversky, Daniel Kahneman, Science. 1851974</p>
<p>The Framing of Decisions and the Psychology of Choice. Science. 2111981</p>
<p>Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. Tomer Ullman, arXiv2023</p>
<p>Model selection and psychological theory: a discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). Scott I Vrieze, Psychological Methods. 172012</p>
<p>Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, arXiv2022</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv2022</p>
<p>Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora. Alex Warstadt, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. Alex Warstadt, the BabyLM Challenge at the 27th Conference on Computational Natural Language LearningAssociation for Computational Linguistics2023</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 792023</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Transactions on Machine Learning Research. 2022</p>
<p>Chain of Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Le Quoc, Denny Zhou, arXiv2022</p>
<p>Taxonomy of Risks posed by Language Models. Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John , Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. the 2022 ACM Conference on Fairness, Accountability, and TransparencyAssociation for Computing Machinery2022</p>
<p>Using Computational Models to Test Syntactic Learnability. Ethan Wilcox, Richard Gotlieb, Roger Futrell, Levy, Linguistic Inquiry. 2023</p>
<p>Beliefs about beliefs: representation and constraining function of wrong beliefs in young children's understanding of deception. H Wimmer, Perner, Cognition. 1311983</p>
<p>An Explanation of In-context Learning as Implicit Bayesian Inference. Sang Xie, Aditi Michael, Percy Raghunathan, Tengyu Liang, Ma, International Conference on Learning Representations. 2022</p>
<p>COMPARING LEARNING METHODS FOR CLASSIFICATION. Yuhong Yang, Statistica Sinica. 22006</p>
<p>The generalizability crisis. Tal Yarkoni, Behavioral and Brain Sciences. 452022</p>
<p>Studying and improving reasoning in humans and machines. Nicolas Yax, Hernan Anlló, Stefano Palminteri, Communications Psychology. 212024</p>
<p>Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet). Eunice Yiu, Eliza Kosoy, Alison Gopnik, Perspectives on Psychological Science. 002023</p>
<p>HellaSwag: Can a Machine Really Finish Your Sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, In: Annual Meeting of the Association for Computational Linguistics. 2019</p>
<p>Vision-Language Models for Vision Tasks: A Survey. Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu, arXiv2024</p>
<p>Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning. Tianhua Zhang, Jiaxin Ge, Findings of the Association for Computational Linguistics: NAACL 2024. Kevin Duh, Helena Gomez, Steven Bethard, 2024</p>
<p>Explainability for Large Language Models: A Survey. Haiyan Zhao, Hanjie Chen, F Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du, ACM Transactions on Intelligent Systems and Technology. 152023</p>
<p>Calibrate Before Use: Improving Few-Shot Performance of Language Models. Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, arXiv2021</p>
<p>Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses. Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin, arXiv2024</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi , arXiv2022</p>
<p>Mindstorms in Natural Language-Based Societies of Mind. Mingchen Zhuge, arXiv2023</p>            </div>
        </div>

    </div>
</body>
</html>
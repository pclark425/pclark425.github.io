<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1275 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1275</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1275</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060" target="_blank">Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper develops a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions on the problem of intrinsically-motivated learning.</p>
                <p><strong>Paper Abstract:</strong> The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1275.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1275.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIM-Empowerment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Variational Information Maximisation for Empowerment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent algorithm that maximises mutual information between K-step action sequences and resulting future states (empowerment) via a variational lower bound, optimising a decoder q and a source policy approximation h_theta end-to-end from pixels or continuous state inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Empowerment-driven agent (stochastic variational information maximisation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent uses a convolutional neural network state representation ( ConvNet -> 100-dim state ), an autoregressive variational decoder q_xi(a | s, s') (Gaussian/categorical per-step parameterised by two-layer nets), and a directed source model h_theta(a | s) plus scalar psi_theta(s) to approximate the optimal (unnormalised) source. Training alternates: maximum-likelihood updates for q_xi and squared-error regression for r_theta = ln h_theta + psi_theta to match beta * ln q; optimisation performed by minibatch stochastic gradient descent (Adagrad) using on-policy interaction data. Behaviour uses one-step greedy expected-empowerment (or optionally value iteration or using omega as behaviour).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information-theoretic adaptive exploration (empowerment / mutual information maximisation) via a variational lower bound</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent adapts its experimental design (action-selection distribution) by iteratively maximising a variational lower bound on I(a; s' | s): (1) fit decoder q_xi to predict action sequences given observed (s,s') pairs (ML), (2) compute energy u(s,a)=E_{p(s'|s,a)}[ln q_xi(a|s,s')] and approximate the unnormalised optimal source ω*(a|s) ∝ exp(β u) with a normalised directed model h_theta(a|s) and scalar psi_theta(s) via squared-error regression. Updates are performed online from environment interaction (Monte Carlo expectation over p(s'|s,a)), so the source distribution continuously adapts to recent observations to preferentially sample action sequences that increase mutual information (i.e., that discriminate reachable future states).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple: grid-like rooms/mazes (static and dynamic), lava-flow maze, two-rooms with key-door, corridor-with-bricks, and continuous 3D predator-prey (MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially-/fully-observable via pixels or low-dimensional continuous state; discrete action settings (typically 5 actions: up, down, left, right, noop) and continuous action settings (2D force); environments include static layouts, movable objects (combinatorial state structure), non-stationary/dynamic elements (flowing lava), sparse/no external rewards, resetting on failure, and an adversarial moving predator in continuous physics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies across experiments: some discrete environments with ≤ 400 enumerated states (used for exact MI comparison); visual inputs were 20×20 images processed to P=100-dim state; planning horizon K=5; discrete action branching N≈5 per step (so N^K = 5^5); continuous predator-prey: continuous 6+ dimensional states (position, velocity, angular momentum for agent and predator) and 2D continuous action.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative and quantitative matches to exact MI in small static environments: empowerment heatmaps match exact Blahut-Arimoto results; reported correlation coefficient = 1.00 and R^2 = 0.90 for the two-rooms environment. Empirical qualitative successes in dynamic settings: agent discovers tool-like behaviour (moves boxes to increase empowerment), collects key then prefers states near door, builds wall to stop lava flow, and learns to evade predator in continuous physics. No cumulative-reward, success-rate, or sample-count numeric metrics provided beyond these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not numerically quantified in the paper; training is online with minibatch stochastic gradient updates from interaction data. No explicit sample/episode counts to reach convergence reported.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via optimisation of the source ω under an entropy constraint (control via inverse temperature β) — ω is shaped to maximise mutual information while limiting entropy; behaviour policy used for acting is typically one-step greedy expected-empowerment (exploitation of empowerment), while ω (the optimized source) can be used as an exploration policy but has high variance (designed to uniformly explore reachable terminal states). The constraint on H(a) prevents degenerate solutions and controls exploration breadth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against exact Blahut-Arimoto (BA) mutual information computation for small discrete environments; qualitatively contrasted with prior intrinsic-motivation measures (references to Salge et al., Klyubin et al., Wissner-Gross & Freer). The paper also discusses model-based variants: importance-sampling BA approximation and reparameterised stochastic-backprop (model-based) optimisation, which were found less efficient and thus not central to main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) A scalable variational lower bound on mutual information enables empowerment computation directly from pixels and in continuous domains, avoiding exponential enumeration required by Blahut-Arimoto. 2) Variational empowerment reproduces exact empowerment landscapes in small static environments (heatmap agreement; corr=1.00, R^2=0.90). 3) The method scales to dynamic environments with movable objects and non-stationary hazards, producing plausible intrinsic behaviours (tool use, key collection, wall-building to stop lava, predator evasion). 4) Computational complexity scales favorably (quadratic in hidden-layer sizes, linear in other factors) and is compatible with GPU-accelerated gradient descent. 5) The optimized source ω* yields an efficient exploration distribution that approximately visits terminal states uniformly (intuition and analytic discussion in deterministic discrete case).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) No numeric sample-efficiency or learning-curve metrics reported; claims are primarily qualitative except for the exact-versus-variational comparison. 2) Intrinsic curiosity limited by planning horizon K (K=5 used); paper does not explore effect of varying K on behaviour. 3) Using ω as the behaviour policy produces high-variance actions (may be undesirable in acting). 4) Model-based variants (importance sampling, reparametrisation) require many samples or were less efficient and thus not adopted. 5) Exact BA still required for verification in small domains; scalability claims are algorithmic/complexity-based and demonstrated qualitatively in larger/dynamic settings rather than via standardized quantitative benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_code_or_hyperparams</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning', 'publication_date_yy_mm': '2015-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The IM algorithm: a variational approach to information maximization <em>(Rating: 2)</em></li>
                <li>Empowerment: A universal agent-centric measure of control <em>(Rating: 2)</em></li>
                <li>Empowerment for continuous agent-environment systems <em>(Rating: 2)</em></li>
                <li>Changing the environment based on empowerment as intrinsic motivation <em>(Rating: 2)</em></li>
                <li>Causal entropic forces <em>(Rating: 1)</em></li>
                <li>How can we define intrinsic motivation? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1275",
    "paper_id": "paper-ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "VIM-Empowerment",
            "name_full": "Stochastic Variational Information Maximisation for Empowerment",
            "brief_description": "An agent algorithm that maximises mutual information between K-step action sequences and resulting future states (empowerment) via a variational lower bound, optimising a decoder q and a source policy approximation h_theta end-to-end from pixels or continuous state inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Empowerment-driven agent (stochastic variational information maximisation)",
            "agent_description": "Agent uses a convolutional neural network state representation ( ConvNet -&gt; 100-dim state ), an autoregressive variational decoder q_xi(a | s, s') (Gaussian/categorical per-step parameterised by two-layer nets), and a directed source model h_theta(a | s) plus scalar psi_theta(s) to approximate the optimal (unnormalised) source. Training alternates: maximum-likelihood updates for q_xi and squared-error regression for r_theta = ln h_theta + psi_theta to match beta * ln q; optimisation performed by minibatch stochastic gradient descent (Adagrad) using on-policy interaction data. Behaviour uses one-step greedy expected-empowerment (or optionally value iteration or using omega as behaviour).",
            "adaptive_design_method": "Information-theoretic adaptive exploration (empowerment / mutual information maximisation) via a variational lower bound",
            "adaptation_strategy_description": "The agent adapts its experimental design (action-selection distribution) by iteratively maximising a variational lower bound on I(a; s' | s): (1) fit decoder q_xi to predict action sequences given observed (s,s') pairs (ML), (2) compute energy u(s,a)=E_{p(s'|s,a)}[ln q_xi(a|s,s')] and approximate the unnormalised optimal source ω*(a|s) ∝ exp(β u) with a normalised directed model h_theta(a|s) and scalar psi_theta(s) via squared-error regression. Updates are performed online from environment interaction (Monte Carlo expectation over p(s'|s,a)), so the source distribution continuously adapts to recent observations to preferentially sample action sequences that increase mutual information (i.e., that discriminate reachable future states).",
            "environment_name": "Multiple: grid-like rooms/mazes (static and dynamic), lava-flow maze, two-rooms with key-door, corridor-with-bricks, and continuous 3D predator-prey (MuJoCo)",
            "environment_characteristics": "Partially-/fully-observable via pixels or low-dimensional continuous state; discrete action settings (typically 5 actions: up, down, left, right, noop) and continuous action settings (2D force); environments include static layouts, movable objects (combinatorial state structure), non-stationary/dynamic elements (flowing lava), sparse/no external rewards, resetting on failure, and an adversarial moving predator in continuous physics.",
            "environment_complexity": "Varies across experiments: some discrete environments with ≤ 400 enumerated states (used for exact MI comparison); visual inputs were 20×20 images processed to P=100-dim state; planning horizon K=5; discrete action branching N≈5 per step (so N^K = 5^5); continuous predator-prey: continuous 6+ dimensional states (position, velocity, angular momentum for agent and predator) and 2D continuous action.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative and quantitative matches to exact MI in small static environments: empowerment heatmaps match exact Blahut-Arimoto results; reported correlation coefficient = 1.00 and R^2 = 0.90 for the two-rooms environment. Empirical qualitative successes in dynamic settings: agent discovers tool-like behaviour (moves boxes to increase empowerment), collects key then prefers states near door, builds wall to stop lava flow, and learns to evade predator in continuous physics. No cumulative-reward, success-rate, or sample-count numeric metrics provided beyond these comparisons.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Not numerically quantified in the paper; training is online with minibatch stochastic gradient updates from interaction data. No explicit sample/episode counts to reach convergence reported.",
            "exploration_exploitation_tradeoff": "Balanced via optimisation of the source ω under an entropy constraint (control via inverse temperature β) — ω is shaped to maximise mutual information while limiting entropy; behaviour policy used for acting is typically one-step greedy expected-empowerment (exploitation of empowerment), while ω (the optimized source) can be used as an exploration policy but has high variance (designed to uniformly explore reachable terminal states). The constraint on H(a) prevents degenerate solutions and controls exploration breadth.",
            "comparison_methods": "Compared against exact Blahut-Arimoto (BA) mutual information computation for small discrete environments; qualitatively contrasted with prior intrinsic-motivation measures (references to Salge et al., Klyubin et al., Wissner-Gross & Freer). The paper also discusses model-based variants: importance-sampling BA approximation and reparameterised stochastic-backprop (model-based) optimisation, which were found less efficient and thus not central to main experiments.",
            "key_results": "1) A scalable variational lower bound on mutual information enables empowerment computation directly from pixels and in continuous domains, avoiding exponential enumeration required by Blahut-Arimoto. 2) Variational empowerment reproduces exact empowerment landscapes in small static environments (heatmap agreement; corr=1.00, R^2=0.90). 3) The method scales to dynamic environments with movable objects and non-stationary hazards, producing plausible intrinsic behaviours (tool use, key collection, wall-building to stop lava, predator evasion). 4) Computational complexity scales favorably (quadratic in hidden-layer sizes, linear in other factors) and is compatible with GPU-accelerated gradient descent. 5) The optimized source ω* yields an efficient exploration distribution that approximately visits terminal states uniformly (intuition and analytic discussion in deterministic discrete case).",
            "limitations_or_failures": "1) No numeric sample-efficiency or learning-curve metrics reported; claims are primarily qualitative except for the exact-versus-variational comparison. 2) Intrinsic curiosity limited by planning horizon K (K=5 used); paper does not explore effect of varying K on behaviour. 3) Using ω as the behaviour policy produces high-variance actions (may be undesirable in acting). 4) Model-based variants (importance sampling, reparametrisation) require many samples or were less efficient and thus not adopted. 5) Exact BA still required for verification in small domains; scalability claims are algorithmic/complexity-based and demonstrated qualitatively in larger/dynamic settings rather than via standardized quantitative benchmarks.",
            "sample_code_or_hyperparams": null,
            "uuid": "e1275.0",
            "source_info": {
                "paper_title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning",
                "publication_date_yy_mm": "2015-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The IM algorithm: a variational approach to information maximization",
            "rating": 2
        },
        {
            "paper_title": "Empowerment: A universal agent-centric measure of control",
            "rating": 2
        },
        {
            "paper_title": "Empowerment for continuous agent-environment systems",
            "rating": 2
        },
        {
            "paper_title": "Changing the environment based on empowerment as intrinsic motivation",
            "rating": 2
        },
        {
            "paper_title": "Causal entropic forces",
            "rating": 1
        },
        {
            "paper_title": "How can we define intrinsic motivation?",
            "rating": 1
        }
    ],
    "cost": 0.00979475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning</h1>
<p>Shakir Mohamed and Danilo J. Rezende<br>Google DeepMind, London<br>{shakir, danilor}@google.com</p>
<h4>Abstract</h4>
<p>The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm - an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.</p>
<h2>1 Introduction</h2>
<p>The problem of measuring and harnessing dependence between random variables is an inescapable statistical problem that forms the basis of a large number of applications in machine learning, including rate distortion theory [4], information bottleneck methods [26], population coding [1], curiositydriven exploration [24, 19], model selection [3], and intrinsically-motivated reinforcement learning [20]. In all these problems the core quantity that must be reasoned about is the mutual information. In general, the mutual information (MI) is intractable to compute and few existing algorithms are useful for realistic applications. The received algorithm for estimating mutual information is the Blahut-Arimoto algorithm [29] that effectively solves for the MI by enumeration - an approach with exponential complexity that is not suitable for modern machine learning applications. By combining the best current practice from variational inference with that of deep learning, we bring the generality and scalability seen in other problem domains to information maximisation problems. We provide a new approach for maximisation of the mutual information that has significantly lower complexity, allows for computation with high-dimensional sensory inputs, and that allows us to exploit modern computational resources.
The technique we derive is generally applicable, but we shall describe and develop our approach by focussing on one popular and increasingly topical application of the mutual information: as a measure of 'empowerment' in intrinsically-motivated reinforcement learning. Reinforcement learning (RL) has seen a number of successes in recent years that has now established it as a practical, scalable solution for realistic agent-based planning and decision making [15, 12]. A limitation of the standard RL approach is that an agent is only able to learn using external rewards obtained from its environment; truly autonomous agents will often exist in environments that lack such external rewards or in environments where rewards are sparsely distributed. Intrinsically-motivated reinforcement learning [23] attempts to address this shortcoming by equipping an agent with a number of internal drives or intrinsic reward signals, such as hunger, boredom or curiosity that allows the agent to continue to explore, learn and act meaningfully in a reward-sparse world. There are many</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Perception-action loop separating environment into internal and external facets.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Computational graph for variational information maximisation.
ways in which to formally define internal drives, but what all such definitions have in common is that they, in some unsupervised fashion, allow an agent to reason about the value of information in the action-observation sequences it experiences. The mutual information allows for exactly this type of reasoning and forms the basis of one popular intrinsic reward measure, known as empowerment.</p>
<p>Our paper begins by describing the framework we use for online and self-motivated learning (section 2) and then describes the general problem associated with mutual information estimation and empowerment (section 3). We then make the following contributions:</p>
<ul>
<li>We develop stochastic variational information maximisation, a new algorithm for scalable estimation of the mutual information and channel capacity that is applicable to both discrete and continuous settings.</li>
<li>We combine variational information optimisation and tools from deep learning to develop a scalable algorithm for intrinsically-motivated reinforcement learning, demonstrating a new application of the variational theory for problems in reinforcement learning and decision making.</li>
<li>We demonstrate that empowerment-based behaviours obtained using variational information maximisation match those using the exact computation. We then apply our algorithms to a broad range of high-dimensional problems for which it is not possible to compute the exact solution, but for which we are able to act according to empowerment - learning directly from pixel information.</li>
</ul>
<h1>2 Intrinsically-motivated Reinforcement Learning</h1>
<p>Intrinsically- or self-motivated learning attempts to address the question of where rewards come from and how they are used by an autonomous agent. Consider an online learning system that must model and reason about its incoming data streams and interact with its environment. This perception-action loop is common to many areas such as active learning, process control, black-box optimisation, and reinforcement learning. An extended view of this framework was presented by Singh et al. [23], who describe the environment as factored into external and internal components (figure 1). An agent receives observations and takes actions in the external environment. Importantly, the source and nature of any reward signals are not assumed to be provided by an oracle in the external environment, but is moved to an internal environment that is part of the agent's decisionmaking system; the internal environment handles the efficient processing of all input data and the choice and computation of an appropriate internal reward signal.</p>
<p>There are two important components of this framework: the state representation and the critic. We are principally interested in vision-based self-motivated systems, for which there are no solutions currently developed. To achieve this, our state representation system is a convolutional neural network [13]. The critic in figure 1 is responsible for providing intrinsic rewards that allow the agent to act under different types of internal motivations, and is where information maximisation enters the intrinsically-motivated learning problem.</p>
<p>The nature of the critic and in particular, the reward signal it provides is the main focus of this paper. A wide variety of reward functions have been proposed, and include: missing information or Bayesian surprise, which uses the KL divergence to measure the change in an agents internal belief after the observation of new data [8, 22]; measures based on prediction errors of future states such predicted $L_{1}$ change, predicted mode change or probability gain [16], or salient event prediction [23]; and measures based on information-theoretic quantities such as predicted information gain (PIG) [14], causal entropic forces [28] or empowerment [21]. The paper by Oudeyer \&amp; Kaplan [18]</p>
<p>currently provides the widest singular discussion of the breadth of intrinsic motivation measures. Although we have a wide choice of intrinsic reward measures, none of the available informationtheoretic approaches are efficient to compute or scalable to high-dimensional problems: they require either knowledge of the true transition probability or summation over all configurations of the state space, which is not tractable for complex environments or when the states are large images.</p>
<h1>3 Mutual Information and Empowerment</h1>
<p>The mutual information is a core information-theoretic quantity that acts as a general measure of dependence between two random variables $\mathbf{x}$ and $\mathbf{y}$, defined as:</p>
<p>$$
\mathcal{I}(\mathbf{x}, \mathbf{y})=\mathbb{E}_{p(y \mid x) p(x)}\left[\log \left(\frac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x}) p(\mathbf{y})}\right)\right]
$$</p>
<p>where the $p(\mathbf{x}, \mathbf{y})$ is a joint distribution over the random variables, and $p(\mathbf{x})$ and $p(\mathbf{y})$ are the corresponding marginal distributions. $\mathbf{x}$ and $\mathbf{y}$ can be many quantities of interest: in computational neuroscience they are the sensory inputs and the spiking population code; in telecommunications they are the input signal to a channel and the received transmission; when learning exploration policies in RL, they are the current state and the action at some time in the future, respectively.
For intrinsic motivation, we use an internal reward measure referred to as empowerment [11, 21] that is obtained by searching for the maximal mutual information $\mathcal{I}(\cdot, \cdot)$, conditioned on a starting state $\mathbf{s}$, between a sequence of $K$ actions $\mathbf{a}$ and the final state reached $\mathbf{s}^{\prime}$ :</p>
<p>$$
\mathcal{E}(\mathbf{s})=\max <em _omega="\omega">{\omega} \mathcal{I}^{\omega}\left(\mathbf{a}, \mathbf{s}^{\prime} \mid \mathbf{s}\right)=\max </em>\right)\right]
$$} \mathbb{E}_{p\left(s^{\prime} \mid a, s\right) \omega(a \mid s)}\left[\log \left(\frac{p\left(\mathbf{a}, \mathbf{s}^{\prime} \mid \mathbf{s}\right)}{\omega(\mathbf{a} \mid \mathbf{s}) p\left(\mathbf{s}^{\prime} \mid \mathbf{s}\right)</p>
<p>where $\mathbf{a}=\left{a_{1}, \ldots, a_{K}\right}$ is a sequence of $K$ primitive actions $a_{k}$ leading to a final state $\mathbf{s}^{\prime}$, and $p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s}\right)$ is the $K$-step transition probability of the environment. $p\left(\mathbf{a}, \mathbf{s}^{\prime} \mid \mathbf{s}\right)$ is the joint distribution of action sequences and the final state, $\omega(\mathbf{a} \mid \mathbf{s})$ is a distribution over $K$-step action sequences, and $p\left(\mathbf{s}^{\prime} \mid \mathbf{s}\right)$ is the joint probability marginalised over the action sequence.
Equation (2) is the definition of the channel capacity in information theory and is a measure of the amount of information contained in the action sequences a about the future state $\mathbf{s}^{\prime}$. This measure is compelling since it provides a well-grounded, task-independent measure for intrinsic motivation that fits naturally within the framework for intrinsically motivated learning described by figure 1. Furthermore, empowerment, like the state- or action-value function in reinforcement learning, assigns a value $\mathcal{E}(\mathbf{s})$ to each state $\mathbf{s}$ in an environment. An agent that seeks to maximise this value will move towards states from which it can reach the largest number of future states within its planning horizon $K$. It is this intuition that has led authors to describe empowerment as a measure of agent 'preparedness', or as a means by which an agent may quantify the extent to which it can reliably influence its environment - motivating an agent to move to states of maximum influence [21].
An empowerment-based agent generates an open-loop sequence of actions $K$ steps into the future - this is only used by the agent for its internal planning using $\omega(\mathbf{a} \mid \mathbf{s})$. When optimised using (2), the distribution $\omega(\mathbf{a} \mid \mathbf{s})$ becomes an efficient exploration policy that allows for uniform exploration of the state space reachable at horizon $K$, and is another compelling aspect of empowerment (we provide more intuition for this in appendix A). But this policy is not what is used by the agent for acting: when an agent must act in the world, it follows a closed-loop policy obtained by a planning algorithm using the empowerment value (e.g., Q-learning); we expand on this in sect. 4.3. A further consequence is that while acting, the agent is only 'curious' about parts of its environment that can be reached within its internal planning horizon $K$. We shall not explore the effect of the horizon in this work, but this has been widely-explored and we defer to the insights of Salge et al. [21].</p>
<h2>4 Scalable Information Maximisation</h2>
<p>The mutual information (MI) as we have described it thus far, whether it be for problems in empowerment, channel capacity or rate distortion, hides two difficult statistical problems. Firstly, computing the MI involves expectations over the unknown state transition probability. This can be seen by rewriting the MI in terms of the difference between conditional entropies $H(\cdot)$ as:</p>
<p>$$
\mathcal{I}\left(\mathbf{a}, \mathbf{s}^{\prime} \mid \mathbf{s}\right)=H(\mathbf{a} \mid \mathbf{s})-H\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)
$$</p>
<p>where $H(\mathbf{a} \mid \mathbf{s})=-\mathbb{E}<em p_left_s_prime="p\left(s^{\prime">{\omega(a \mid s)}[\log \omega(\mathbf{a} \mid \mathbf{s})]$ and $H(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s})=-\mathbb{E}</em>\right)$,} \mid a, s\right) \omega(a \mid s)}[\log p\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)]$. This computation requires marginalisation over the $K$-step transition dynamics of the environment $p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s</p>
<p>which is unknown in general. We could estimate this distribution by building a generative model of the environment, and then use this model to compute the MI. Since learning accurate generative models remains a challenging task, a solution that avoids this is preferred (and we also describe one approach for model-based empowerment in appendix B).
Secondly, we currently lack an efficient algorithm for MI computation. There exists no scalable algorithm for computing the mutual information that allows us to apply empowerment to highdimensional problems and that allow us to easily exploit modern computing systems. The current solution is to use the Blahut-Arimoto algorithm [29], which essentially enumerates over all states, thus being limited to small-scale problems and not being applicable to the continuous domain. More scalable non-parametric estimators have been developed [7, 6]: these have a high memory footprint or require a very large number of observations, any approximation may not be a bound on the MI making reasoning about correctness harder, and they cannot easily be composed with existing (gradient-based) systems that allow us to design a unified (end-to-end) system. In the continuous domain, Monte Carlo integration has been proposed [10], but applications of Monte Carlo estimators can require a large number of draws to obtain accurate solutions and manageable variance. We have also explored Monte Carlo estimators for empowerment and describe an alternative importance sampling-based estimator for the MI and channel capacity in appendix B.1.</p>
<h1>4.1 Variational Information Lower Bound</h1>
<p>The MI can be made more tractable by deriving a lower bound to it and maximising this instead here we present the bound derived by Barber \&amp; Agakov [1]. Using the entropy formulation of the MI (3) reveals that bounding the conditional entropy component is sufficient to bound the entire mutual information. By using the non-negativity property of the KL divergence, we obtain the bound:</p>
<p>$$
\begin{gathered}
\mathrm{KL}[p(x \mid y) | q(x \mid y)] \geq 0 \Rightarrow H(x \mid y) \leq-\mathbb{E}<em _xi="\xi">{p(x \mid y)}[\log q</em>(x \mid y)] \
\mathcal{I}^{\omega}(\mathbf{s})=H(\mathbf{a} \mid \mathbf{s})-H(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}) \geq H(\mathbf{a})+\mathbb{E}<em _theta="\theta">{p\left(s^{\prime} \mid a, s\right) \omega</em>)
\end{gathered}
$$}(a \mid s)}[\log q_{\xi}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)]=\mathcal{I}^{\omega, q}(\mathbf{s</p>
<p>where we have introduced a variational distribution $q_{\xi}(\cdot)$ with parameters $\xi$; the distribution $\omega_{\theta}(\cdot)$ has parameters $\theta$. This bound becomes exact when $q_{\xi}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)$ is equal to the true action posterior distribution $p\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)$. Other lower bounds for the mutual information are also possible: Jaakkola \&amp; Jordan [9] present a lower bound by using the convexity bound for the logarithm; Brunel \&amp; Nadal [2] use a Gaussian assumption and appeal to the Cramer-Rao lower bound.</p>
<p>The bound (4) is highly convenient (especially when compared to other bounds) since the transition probability $p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s}\right)$ appears linearly in the expectation and we never need to evaluate its probability - we can thus evaluate the expectation directly by Monte Carlo using data obtained by interaction with the environment. The bound is also intuitive since we operate using the marginal distribution on action sequences $\omega_{\theta}(\mathbf{a} \mid \mathbf{s})$, which acts as a source (exploration distribution), the transition distribution $p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s}\right)$ acts as an encoder (transition distribution) from $\mathbf{a}$ to $\mathbf{s}^{\prime}$, and the variational distribution $q_{\xi}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)$ conveniently acts as a decoder (planning distribution) taking us from $\mathbf{s}^{\prime}$ to $\mathbf{a}$.</p>
<h3>4.2 Variational Information Maximisation</h3>
<p>A straightforward optimisation procedure based on (4) is an alternating optimisation for the parameters of the distributions $q_{\xi}(\cdot)$ and $\omega_{\theta}(\cdot)$. Barber \&amp; Agakov [1] made the connection between this approach and the generalised EM algorithm and refer to it as the IM (information maximisation) algorithm and we follow the same optimisation principle. From an optimisation perspective, the maximisation of the bound $\mathcal{I}^{\omega, q}(\mathbf{s})$ in (4) w.r.t. $\omega(\mathbf{a} \mid \mathbf{s})$ can be ill-posed (e.g., in Gaussian models, the variances can diverge). We avoid such divergent solutions by adding a constraint on the value of the entropy $H(\mathbf{a})$, which results in the constrained optimisation problem:</p>
<p>$$
\hat{\mathcal{E}}(\mathbf{s})=\max <em _omega_="\omega," q="q">{\omega, q} \mathcal{I}^{\omega, q}(\mathbf{s}) \text { s.t. } H(\mathbf{a} \mid \mathbf{s})&lt;\epsilon, \hat{\mathcal{E}}(\mathbf{s})=\max </em>} \mathbb{E<em _xi="\xi">{p\left(\mathbf{s}^{\prime} \mid \mathbf{s}, s\right) \omega(\mathbf{s} \mid s)}\left[-\frac{1}{\beta} \ln \omega(\mathbf{a} \mid \mathbf{s})+\ln q</em>\right)\right]
$$}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s</p>
<p>where $\mathbf{a}$ is the action sequence performed by the agent when moving from $\mathbf{s}$ to $\mathbf{s}^{\prime}$ and $\beta$ is an inverse temperature (which is a function of the constraint $\epsilon$ ).</p>
<p>At all times we use very general source and decoder distributions formed by complex non-linear functions using deep networks, and use stochastic gradient ascent for optimisation. We refer to our approach as stochastic variational information maximisation to highlight that we do all our computation on a mini-batch of recent experience from the agent. The optimisation for the decoder $q_{\xi}(\cdot)$ becomes a maximum likelihood problem, and the optimisation for the source $\omega_{\theta}(\cdot)$ requires computation of an unnormalised energy-based model, which we describe next. We summmarise the overall procedure in algorithm 1.</p>
<h1>4.2.1 Maximum Likelihood Decoder</h1>
<p>The first step of the alternating optimisation is the optimisation of equation (5) w.r.t. the decoder $q$, and is a supervised maximum likelihood problem. Given a set of data from past interactions with the environment, we learn a distribution from the start and termination states $\mathbf{s}, \mathbf{s}^{\prime}$, respectively, to the action sequences a that have been taken. We parameterise the decoder as an auto-regressive distribution over the $K$-step action sequence:</p>
<p>$$
q_{\xi}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)=q\left(a_{1} \mid \mathbf{s}, \mathbf{s}^{\prime}\right) \prod_{k=2}^{K} q\left(a_{k} \mid f_{\xi}\left(a_{k-1}, \mathbf{s}, \mathbf{s}^{\prime}\right)\right)
$$</p>
<p>We are free to choose the distributions $q\left(a_{k}\right)$ for each action in the sequence, which we choose as categorical distributions whose mean parameters are the result of the function $f_{\xi}(\cdot)$ with parameters $\xi . f$ is a non-linear function that we specify using a two-layer neural network with rectified-linear activation functions. By maximising this log-likelihood, we are able to make stochastic updates to the variational parameters $\xi$ of this distribution. The neural network models used are expanded upon in appendix D.</p>
<h3>4.2.2 Estimating the Source Distribution</h3>
<p>Given a current estimate of the decoder $q$, the variational solution for the distribution $\omega(\mathbf{a} \mid \mathbf{s})$ computed by solving the functional derivative $\delta \mathcal{I}^{\omega}(s) / \delta \omega(\mathbf{a} \mid \mathbf{s})=0$ under the constraint that $\sum_{a} \omega(\mathbf{a} \mid \mathbf{s})=1$, is given by $\omega^{\star}(\mathbf{a} \mid \mathbf{s})=\frac{1}{Z(s)} \exp (\hat{u}(\mathbf{s}, \mathbf{a}))$, where $u(\mathbf{s}, \mathbf{a})=\mathbb{E}<em _xi="\xi">{p\left(s^{\prime} \mid s, a\right)}\left[\ln q</em>)$.
The distribution $\omega^{\star}(\mathbf{a} \mid \mathbf{s})$ is implicitly defined as an unnormalised distribution - there are no direct mechanisms for sampling actions or computing the normalising function $Z(\mathbf{s})$ for such distributions. We could use Gibbs or importance sampling, but these solutions are not satisfactory as they would require several evaluations of the unknown function $u(\mathbf{s}, \mathbf{a})$ per decision per state. We obtain a more convenient problem by approximating the unnormalised distribution $\omega^{\star}(\mathbf{a} \mid \mathbf{s})$ by a normalised (directed) distribution $h_{\theta}(\mathbf{a} \mid \mathbf{s})$. This is equivalent to approximating the energy term $\hat{u}(\mathbf{s}, \mathbf{a})$ by a function of the log-likelihood of the directed model, $r_{\theta}$ :}\left(\mathbf{a} \mid \mathbf{s}, \mathbf{s}^{\prime}\right)\right], \hat{u}(\mathbf{s}, \mathbf{a})=\beta u(\mathbf{s}, \mathbf{a})$ and $Z(\mathbf{s})=\sum_{a} e^{\hat{u}(s, a)}$ is a normalisation term. By substituting this optimal distribution into the original objective (5) we find that it can be expressed in terms of the normalisation function $Z(\mathbf{s})$ only, $\mathcal{E}(s)=\frac{1}{\beta} \log Z(\mathbf{s</p>
<p>$$
\omega^{\star}(\mathbf{a} \mid \mathbf{s}) \approx h_{\theta}(\mathbf{a} \mid \mathbf{s}) \Rightarrow \hat{u}(\mathbf{s}, \mathbf{a}) \approx r_{\theta}(\mathbf{s}, \mathbf{a}) ; \quad r_{\theta}(\mathbf{s}, \mathbf{a})=\ln h_{\theta}(\mathbf{a} \mid \mathbf{s})+\psi_{\theta}(\mathbf{s})
$$</p>
<p>We introduced a scalar function $\psi_{\theta}(\mathbf{s})$ into the approximation, but since this is not dependent on the action sequence a it does not change the approximation (7), and can be verified by substituting (7) into $\omega^{\star}(\mathbf{a} \mid \mathbf{s})$. Since $h_{\theta}(\mathbf{a} \mid \mathbf{s})$ is a normalised distribution, this leaves $\psi_{\theta}(\mathbf{s})$ to account for the normalisation term $\log Z(\mathbf{s})$, verified by substituting $\omega^{\star}(\mathbf{a} \mid \mathbf{s})$ and (7) into (5). We therefore obtain a cheap estimator of empowerment $\mathcal{E}(\mathbf{s}) \approx \frac{1}{\beta} \psi_{\theta}(\mathbf{s})$.
To optimise the parameters $\theta$ of the directed model $h_{\theta}$ and the scalar function $\psi_{\theta}$ we can minimise any measure of discrepancy between the two sides of the approximation (7). We minimise the squared error, giving the loss function $L\left(h_{\theta}, \psi_{\theta}\right)$ for optimisation as:</p>
<p>$$
L\left(h_{\theta}, \psi_{\theta}\right)=\mathbb{E}<em _xi="\xi">{p\left(s^{\prime} \mid s, A\right)}\left[\left(\beta \ln q</em>\right]
$$}\left(\mathbf{a} \mid \mathbf{s}, \mathbf{s}^{\prime}\right)-r_{\theta}(\mathbf{s}, \mathbf{a})\right)^{2</p>
<p>At convergence of the optimisation, we obtain a compact function with which to compute the empowerment that only requires forward evaluation of the function $\psi . h_{\theta}(\mathbf{a} \mid \mathbf{s})$ is parameterised using an auto-regressive distribution similar to (18), with conditional distributions specified by deep networks. The scalar function $\psi_{\theta}$ is also parameterised using a deep network. Further details of these networks are provided in appendix D.</p>
<h3>4.3 Empowerment-based Behaviour policies</h3>
<p>Using empowerment as an intrinsic reward measure, an agent will seek out states of maximal empowerment. We can treat the empowerment value $\mathcal{E}(\mathbf{s})$ as a state-dependent reward and can then utilise any standard planning algorithm, e.g., Q-learning, policy gradients or Monte Carlo search. We use the simplest planning strategy by using a one-step greedy empowerment maximisation. This amounts to choosing actions $a=\arg \max <em p_left_s_prime="p\left(s^{\prime">{a} \mathcal{C}(\mathbf{s}, a)$, where $\mathcal{C}(\mathbf{s}, a)=\mathbb{E}</em>)]$. This policy does not account for the effect of actions beyond the planning horizon $K$. A natural enhancement is to use value iteration [25] to allow the agent to take actions by maximising its long term (potentially} \mid s, a\right)}[\mathcal{E}(\mathbf{s</p>
<p>Algorithm 1: Stochastic Variational Information Maximisation for Empowerment</p>
<p>Parameters: $\xi$ variational, $\lambda$ convolutional, $\theta$ source
while not converged do
$\mathbf{x} \leftarrow{$ Read current state $}$
$\mathbf{s}=\operatorname{ConvNet}<em _lambda="\lambda">{\lambda}(\mathbf{x}){$ Compute state repr. $}$
$A \sim \omega(\mathbf{a} \mid \mathbf{s}){$ Draw action sequence. $}$
Obtain data $\left(\mathbf{x}, \mathbf{a}, \mathbf{x}^{\prime}\right){$ Acting in env. $}$
$\mathbf{s}^{\prime}=\operatorname{ConvNet}</em>\right){$ Compute state repr. $}$
$\Delta \xi \propto \nabla_{\xi} \log q_{\xi}\left(\mathbf{a} \mid \mathbf{s}, \mathbf{s}^{\prime}\right)(18)$
$\Delta \theta \propto \nabla_{\theta} L\left(h_{\theta}, \psi_{\theta}\right)(8)$
$\Delta \lambda \propto \nabla_{\lambda} \log q_{\xi}\left(\mathbf{a} \mid \mathbf{s}, \mathbf{s}^{\prime}\right)+\nabla_{\lambda} L\left(h_{\theta}, \psi_{\theta}\right)$
end while
$\mathcal{E}(\mathbf{s})=\frac{1}{\beta} \psi_{\theta}(\mathbf{s}) \quad{$ Empowerment $}$
}\left(\mathbf{x}^{\prime<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparing exact vs approximate empowerment. Heat maps: empowerment in 3 environments: two rooms, cross room, two-rooms; Scatter plot: agreement for two-rooms.
discounted) empowerment. A third approach would be to use empowerment as a potential function and the difference between the current and previous state's empowerment as a shaping function with in the planning [17]. A fourth approach is one where the agent uses the source distribution $\omega(\mathbf{a} \mid \mathbf{s})$ as its behaviour policy. The source distribution has similar properties to the greedy behaviour policy and can also be used, but since it effectively acts as an empowered agents internal exploration mechanism, it has a large variance (it is designed to allow uniform exploration of the state space). Understanding this choice of behaviour policy is an important line of ongoing research.</p>
<h1>4.4 Algorithm Summary and Complexity</h1>
<p>The system we have described is a scalable and general purpose algorithm for mutual information maximisation and we summarise the core components using the computational graph in figure 2 and in algorithm 1. The state representation mechanism used throughout is obtained by transforming raw observations $\mathbf{x}, \mathbf{x}^{\prime}$ to produce the start and final states $\mathbf{s}, \mathbf{s}^{\prime}$, respectively. When the raw observations are pixels from vision, the state representation is a convolutional neural network [13, 15], while for other observations (such as continuous measurements) we use a fully-connected neural network we indicate the parameters of these models using $\lambda$. Since we use a unified loss function, we can apply gradient descent and backpropagate stochastic gradients through the entire model allowing for joint optimisation of both the information and representation parameters. For optimisation we use a preconditioned optimisation algorithm such as Adagrad [5].
The computational complexity of empowerment estimators involves the planning horizon $K$, the number of actions $N$, and the number of states $S$. For the exact computation we must enumerate over the number of states, which for grid-worlds is $S \propto D^{2}$ (for $D \times D$ grids), or for binary images is $S=$ $2^{D^{2}}$. The complexity of using the Blahut-Arimoto (BA) algorithm is $O\left(N^{K} S^{2}\right)=O\left(N^{K} D^{4}\right)$ for grid worlds or $O\left(N^{K} 2^{2 D^{2}}\right)$ for binary images. The BA algorithm, even in environments with a small number of interacting objects becomes quickly intractable, since the state space grows exponentially with the number of possible interactions, and is also exponential in the planning horizon. In contrast, our approach deals directly on the image dimensions. Using visual inputs, the convolutional network produces a vector of size $P$, upon which all subsequent computation is based, consisting of an $L$ layer neural network. This gives a complexity for state representation of $O\left(D^{2} P+L P^{2}\right)$. The autoregressive distributions have complexity of $O\left(H^{2} K N\right)$, where $H$ is the size of the hidden layer. Thus, our approach has at most quadratic complexity in the size of the hidden layers used and linear in other quantities, and matches the complexity of any currently employed large-scale vision-based models. In addition, since we use gradient descent throughout, we are able to leverage the power of GPUs and distributed gradient computations.</p>
<h2>5 Results</h2>
<p>We demonstrate the use of empowerment and the effectiveness of variational information maximisation in two types of environments. Static environments consists of rooms and mazes in different configurations in which there are no objects with which the agent can interact, or other moving ob-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Empowerment for a room environment, showing a) an empty room, b) room with an obstacle c) room with a moveable box, d) room with row of moveable boxes.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Left: empowerment landscape for agent and key scenario. Yellow is the key and green is the door. Right: Agent in a corridor with flowing lava. The agent places a bricks to stem the flow of lava.
jects. The number of states in these settings is equal to the number of locations in the environment, so is still manageable for approaches that rely on state enumeration. In dynamic environments, aspects of the environment change, such as flowing lava that causes the agent to reset, or a predator that chases the agent. For the most part, we consider discrete action settings in which the agent has five actions (up, down, left, right, do nothing). The agent may have other actions, such as picking up a key or laying down a brick. There are no external rewards available and the agent must reason purely using visual (pixel) information. For all these experiments we used a horizon of $K=5$.</p>
<h1>5.1 Effectiveness of the MI Bound</h1>
<p>We first establish that the use of the variational information lower bound results in the same behaviour as that obtained using the exact mutual information in a set of static environments. We consider environments that have at most 400 discrete states and compute the true mutual information using the Blahut-Arimoto algorithm. We compute the variational information bound on the same environment using pixel information (on $20 \times 20$ images). To compare the two approaches we look at the empowerment landscape obtained by computing the empowerment at every location in the environment and show these as heatmaps. For action selection, what matters is the location of the maximum empowerment, and by comparing the heatmaps in figure 3, we see that the empowerment landscape matches between the exact and the variational solution, and hence, will lead to the same agent-behaviour.</p>
<p>In each image in figure 3, we show a heat-map of the empowerment for each location in the environment. We then analyze the point of highest empowerment: for the large room it is in the centre of the room; for the cross-shaped room it is at the centre of the cross, and in a two-rooms environment, it is located near both doors. In addition, we show that the empowerment values obtained by our method constitute a close approximation to the true empowerment for the two-rooms environment (correlation coeff $=1.00, R^{2}=0.90$ ). These results match those by authors such as Klyubin et al. [11] (using empowerment) and Wissner-Gross \&amp; Freer [28] (using a different information-theoretic measure - the causal entropic force). The advantage of the variational approach is clear from this discussion: we are able to obtain solutions of the same quality as the exact computation, we have far more favourable computational scaling (one that is not exponential in the size of the state space and planning horizon), and we are able to plan directly from pixel information.</p>
<h3>5.2 Dynamic Environments</h3>
<p>Having established the usefulness of the bound and some further understanding of empowerment, we now examine the empowerment behaviour in environments with dynamic characteristics. Even in small environments, the number of states becomes extremely large if there are objects that can be moved, or added and removed from the environment, making enumerative algorithms (such as BA) quickly infeasible, since we have an exponential explosion in the number of states. We first reproduce an experiment from Salge et al. [21, $\S 4.5 .3$ ] that considers the empowered behaviour of an agent in a room-environment, a room that: is empty, has a fixed box, has a moveable box, has a row of moveable boxes. Salge et al. [21] explore this setup to discuss the choice of the state representation, and that not including the existence of the box severely limits the planning ability of the agent. In our approach, we do not face this problem of choosing the state representation, since the agent will reason about all objects that appear within its visual observations, obviating the need for hand-designed state representations. Figure 4 shows that in an empty room, the empowerment is uniform almost everywhere except close to the walls; in a room with a fixed box, the fixed box limits the set of future reachable states, and as expected, empowerment is low around the box; in a room where the box can be moved, the box can now be seen as a tool and we have high empowerment near the box; similarly, when we have four boxes in a row, the empowerment is highest around the</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Empowerment planning in a lava-filled maze environment. Black panels show the path taken by the agent.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Predator (red) and agent (blue) scenario. Panels 1, 6 show the 3D simulation. Other panels show a trace of the path that the predator and prey take at points on its trajectory. The blue/red shows path history; cyan shows the direction to the maximum empowerment.
boxes. These results match those of Salge et al. [21] and show the effectiveness of reasoning from pixel information directly.</p>
<p>Figure 6 shows how planning with empowerment works in a dynamic maze environment, where lava flows from a source at the bottom that eventually engulfs the maze. The only way the agent is able to safeguard itself, is to stem the flow of lava by building a wall at the entrance to one of the corridors. At every point in time $t$, the agent decides its next action by computing the expected empowerment after taking one action. In this environment, we show the planning for all 9 available actions and a bar graph with the empowerment values for each resulting state. The action that leads to the highest empowerment is taken and is indicated by the black panels ${ }^{1}$.</p>
<p>Figure 5(left) shows two-rooms separated by a door. The agent is able to collect a key that allows it to open the door. Before collecting the key, the maximum empowerment is in the region around the key, once the agent has collected the key, the region of maximum empowerment is close to the door ${ }^{2}$. Figure 5(right) shows an agent in a corridor and must protect itself by building a wall of bricks, which it is able to do successfully using the same empowerment planning approach described for the maze setting.</p>
<h1>5.3 Predator-Prey Scenario</h1>
<p>We demonstrate the applicability of our approach to continuous settings, by studying a simple 3D physics simulation [27], shown in figure 7. Here, the agent (blue) is followed by a predator (red) and is randomly reset to a new location in the environment if caught by the predator. Both the agent and the predator are represented as spheres in the environment that roll on a surface with friction. The state is the position, velocity and angular momentum of the agent and the predator, and the action is a 2D force vector. As expected, the maximum empowerment lies in regions away from the predator, which results in the agent learning to escape the predator ${ }^{3}$.</p>
<h2>6 Conclusion</h2>
<p>We have developed a new approach for scalable estimation of the mutual information by exploiting recent advances in deep learning and variational inference. We focussed specifically on intrinsic motivation with a reward measure known as empowerment, which requires at its core the efficient computation of the mutual information. By using a variational lower bound on the mutual information, we developed a scalable model and efficient algorithm that expands the applicability of empowerment to high-dimensional problems, with the complexity of our approach being extremely favourable when compared to the complexity of the Blahut-Arimoto algorithm that is currently the standard. The overall system does not require a generative model of the environment to be built, learns using only interactions with the environment, and allows the agent to learn directly from visual information or in continuous state-action spaces. While we chose to develop the algorithm in terms of intrinsic motivation, the mutual information has wide applications in other domains, all which stand to benefit from a scalable algorithm that allows them to exploit the abundance of data and be applied to large-scale problems.
Acknowledgements: We thank Daniel Polani for invaluable guidance and feedback.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>[1] Barber, D. and Agakov, F. The IM algorithm: a variational approach to information maximization. In NIPS, volume 16, pp. 201, 2004.
[2] Brunel, N. and Nadal, J. Mutual information, Fisher information, and population coding. Neural Computation, 10(7):1731-1757, 1998.
[3] Buhmann, J. M., Chehreghani, M. H., Frank, M., and Streich, A. P. Information theoretic model selection for pattern analysis. Workshop on Unsupervised and Transfer Learning, 2012.
[4] Cover, T. M. and Thomas, J. A. Elements of information theory. John Wiley \&amp; Sons, 1991.
[5] Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121-2159, 2011.
[6] Gao, S., Steeg, G. V., and Galstyan, A. Efficient estimation of mutual information for strongly dependent variables. arXiv:1411.2003, 2014.
[7] Gretton, A., Herbrich, R., and Smola, A. J. The kernel mutual information. In ICASP, volume 4, pp. IV-880, 2003.
[8] Itti, L. and Baldi, P. F. Bayesian surprise attracts human attention. In NIPS, pp. 547-554, 2005.
[9] Jaakkola, T. S. and Jordan, M. I. Improving the mean field approximation via the use of mixture distributions. In Learning in graphical models, pp. 163-173. 1998.
[10] Jung, T., Polani, D., and Stone, P. Empowerment for continuous agent-environment systems. Adaptive Behavior, 19(1):16-39, 2011.
[11] Klyubin, A. S., Polani, D., and Nehaniv, C. L. Empowerment: A universal agent-centric measure of control. In IEEE Congress on Evolutionary Computation, pp. 128-135, 2005.
[12] Koutník, J., Schmidhuber, J., and Gomez, F. Evolving deep unsupervised convolutional networks for vision-based reinforcement learning. In GECCO, pp. 541-548, 2014.
[13] LeCun, Y. and Bengio, Y. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361:310, 1995.
[14] Little, D. Y. and Sommer, F. T. Learning and exploration in action-perception loops. Frontiers in neural circuits, 7, 2013.
[15] Mnih, V., Kavukcuoglu, K., and Silver, D., et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
[16] Nelson, J. D. Finding useful questions: on Bayesian diagnosticity, probability, impact, and information gain. Psychological review, 112(4):979, 2005.
[17] Ng, Andrew Y, Harada, Daishi, and Russell, Stuart. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, 1999.
[18] Oudeyer, P. and Kaplan, F. How can we define intrinsic motivation? In International conference on epigenetic robotics, 2008.
[19] Rubin, J., Shamir, O., and Tishby, N. Trading value and information in MDPs. In Decision Making with Imperfect Decision Makers, pp. 57-74. 2012.
[20] Salge, C., Glackin, C., and Polani, D. Changing the environment based on empowerment as intrinsic motivation. Entropy, 16(5):2789-2819, 2014.
[21] Salge, C., Glackin, C., and Polani, D. Empowerment-an introduction. In Guided SelfOrganization: Inception, pp. 67-114. 2014.
[22] Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE Trans. Autonomous Mental Development, 2(3):230-247, 2010.
[23] Singh, S. P., Barto, A. G., and Chentanez, N. Intrinsically motivated reinforcement learning. In NIPS, 2005.
[24] Still, S. and Precup, D. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139-148, 2012.
[25] Sutton, R. S. and Barto, A. G. Introduction to reinforcement learning. MIT Press, 1998.
[26] Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method. In Allerton Conference on Communication, Control, and Computing, 1999.
[27] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), pp. 5026-5033, 2012.
[28] Wissner-Gross, A. D. and Freer, C. E. Causal entropic forces. Phys. Rev. Let., 110(16), 2013.
[29] Yeung, R. W. The Blahut-Arimoto algorithms. In Information Theory and Network Coding, pp. 211-228. 2008.</p>
<h1>A Empowerment as Path-counting</h1>
<p>We can obtain a further intuitive understanding of empowerment by examining analytical properties of equation (2). For simplicity, we focus on deterministic and discrete environments. In this setting, the transition probability $p\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a}\right)$ is a delta distribution $p\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a}\right)=\delta\left(\mathbf{s}^{\prime}-T(\mathbf{s}, \mathbf{a})\right)$, where $T(\mathbf{s}, \mathbf{a})$ is a transition function that starts in state $\mathbf{s}$, executes the action sequence $\mathbf{a}$ and provides the resulting state. Solving equation (2) for the optimal source distribution $\omega(\mathbf{a} \mid \mathbf{s})$, using Blahut-Arimoto [4], yields the fixed-point iteration:</p>
<p>$$
\omega^{(k+1)}(\mathbf{a} \mid \mathbf{s})=\frac{1}{Z(\mathbf{s})} \frac{\omega^{(k)}(\mathbf{a} \mid \mathbf{s})}{\sum_{A^{\prime}: T\left(s, A^{\prime}\right)=T(s, A)} \omega^{(k)}\left(\mathbf{a}^{\prime} \mid \mathbf{s}\right)}
$$</p>
<p>where $Z(\mathbf{s})$ is a normalising constant. By starting the recursion (9) with a uniform distribution, $\omega^{(1)}(\mathbf{a} \mid \mathbf{s}) \propto 1$, the solution to the recursion at the next iteration $\omega^{(2)}(\mathbf{a} \mid \mathbf{s})$ is: $\omega^{(2)}(\mathbf{a} \mid \mathbf{s}) \propto \frac{1}{n(\mathbf{a}, \mathbf{s})}$, where $n(\mathbf{a}, \mathbf{s})$ is the number of alternative action-paths $\mathbf{a}^{\prime}$ that terminate at the same state as the action-path $\mathbf{a}$, starting from the state $\mathbf{s}$. We can also relate the quantity $n(\mathbf{a}, \mathbf{s})$ with a more intuitive number, $n\left(\mathbf{s}^{\prime}, \mathbf{s}\right)$, which is the number of different action-paths that brings the agent from state $\mathbf{s}$ to the state $\mathbf{s}^{\prime}, n(\mathbf{a}, \mathbf{s})=\sum_{s^{\prime}} p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s}\right) n\left(\mathbf{s}^{\prime}, \mathbf{s}\right)$.
If $\omega^{(k)}(\mathbf{a} \mid \mathbf{s})=\omega^{(k)}\left(\mathbf{a}^{\prime} \mid \mathbf{s}\right)$, for all $\mathbf{a}^{\prime}$ such that $T\left(\mathbf{s}, \mathbf{a}^{\prime}\right)=T(\mathbf{s}, \mathbf{a})$ then the r.h.s. of (9) does not depend on $\omega^{(k)}(\mathbf{a} \mid \mathbf{s})$. Since the function $n(\mathbf{a}, \mathbf{s})$ satisfies this property, we conclude that $\omega^{(2)}(\mathbf{a} \mid \mathbf{s})$ is a fixed point of (9). The optimal source distribution and resulting empowerment are:</p>
<p>$$
\omega^{(\infty)}(\mathbf{a} \mid \mathbf{s})=\frac{1}{Z(s)} \frac{1}{n(\mathbf{a}, \mathbf{s})} ; \quad \mathcal{E}(\mathbf{s})=\log Z(\mathbf{s})=\log \sum_{A} \frac{1}{n(\mathbf{a}, \mathbf{s})}=\log n(\mathbf{s})
$$</p>
<p>where $n(\mathbf{s})$ is the number of different states that can be reached from state $\mathbf{s}$ at horizon $K$. We demonstrate this reasoning using a tree-structured environment in figure 8.
An agent selecting its actions uniformly will in general not visit all terminal states uniformly, unless every action-path terminates at a different state. However, if an agent selects its actions according the the distribution $\omega^{(\infty)}(\mathbf{a} \mid \mathbf{s})$ it will visit terminal states uniformly. This can be seen by computing the marginal distribution of terminal states $p\left(\mathbf{s}^{\prime} \mid \mathbf{s}\right)=\sum_{A} \delta\left(\mathbf{s}^{\prime}-T(\mathbf{s}, \mathbf{a})\right) \omega^{(\infty)}(\mathbf{a} \mid \mathbf{s})=\frac{1}{n(s)}$. Therefore the distribution $\omega^{(\infty)}(\mathbf{a} \mid \mathbf{s})$ can be seen as an efficient exploration policy, that allows the agent to explore all states uniformly. This adds to a similar analysis presented by Salge et al. [3, §4.4.6].
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Tree-structured environment. Each node represents a reachable state and showing possible transitions by taking the up or down action.</p>
<h2>B Model-based Empowerment</h2>
<p>The approach we described in the main text was 'model-free' in the sense that it did not use a model of the transition dynamics of the environment. Building accurate transition models can be hard and much success in RL has been achieved with model-free methods. Ideally, we would like to use a model-based method, since this will allow for reasoning about task-independent aspects of the world, allow for transfer learning across domains and potentially faster learning. We describe here model-based approaches that we developed for empowerment. These methods were not as efficient for reasons which we describe below, and hence were not part of our main text.</p>
<h1>B. 1 Importance Sampling Estimator</h1>
<p>The most-generic model-based empowerment method is to approximate the empowerment using generic importance-sampling estimator. We assume that a model of the environment $p\left(\mathbf{s}^{\prime} \mid a_{i}, \mathbf{s}\right)$ is available, but at this point will not specify how this model is obtained.
We generate $S$ samples $a_{i}$ from the source distribution $\omega^{t}(\mathbf{a} \mid \mathbf{s})$ with importance weights $\alpha_{t, i}$ ( $\left.\sum_{i} \alpha_{t, i}=1, \alpha_{t, i}&gt;0 \forall i=1 \ldots S\right)$ at iteration $t$,</p>
<p>$$
\left{\mathbf{a}<em i="i" t_="t,">{i}, \alpha</em>)
$$}\right} \sim \omega^{t}(\mathbf{a} \mid \mathbf{s</p>
<p>The samples $a_{i}$ are kept constant through the optimization and only the importance weights are adapted to maximize the MI. Additionally, for each action-sequence sample $a_{i}$ we generate $J$ futurestate samples $\mathbf{s}<em i="i">{k, i}^{\prime}$ from the transition model $p\left(\mathbf{s}^{\prime} \mid a</em>\right)$,}, \mathbf{s</p>
<p>$$
\mathbf{s}<em i="i">{k, i}^{\prime} \sim p\left(\mathbf{s}^{\prime} \mid a</em>\right) \forall i=1 \ldots S, k=1 \ldots J
$$}, \mathbf{s</p>
<p>We could approximate all quantities required to compute the MI from the samples $a_{i}$ and $\mathbf{s}<em i="i" t_="t,">{k, i}^{\prime}$, but we shall instead, directly approximate the Blahut-Arimoto iteration. For this we compute a distortion $D</em>$ at the $t$-th iteration using:</p>
<p>$$
D_{t, i} \approx \frac{1}{J} \sum_{k=1}^{J} \ln \frac{p\left(\mathbf{s}<em i="i">{k, i}^{\prime} \mid a</em>
$$}\right)}{p_{t}\left(\mathbf{s}_{k, i}^{\prime}\right)</p>
<p>where $p_{t}\left(\mathbf{s}^{\prime}\right)=\sum_{i} p\left(\mathbf{s}^{\prime} \mid a_{i}, \mathbf{s}\right) \alpha_{t, i}$ and find a new set of normalized weights $\alpha_{t+1, i}$ such that $\omega^{t+1}(\mathbf{a} \mid \mathbf{s})$ best approximates the Blahut-Arimoto update. This yields a simple update rule for the importance weights,</p>
<p>$$
\ln \alpha_{t+1, i}=\ln \alpha_{t, i}+D_{t, i}-c_{t+1}
$$</p>
<p>where $c_{t+1}=\sum_{i} \alpha_{t, i} \exp D_{t, i}$ is a normalizing constant. The algorithmic complexity of the update (14) is $O(S)$, but the cost of computing the distortion (13) scales as $O\left(J S^{2}\right)$. This approach is applicable to the continuous domain, but typically requires a large number of samples for accurate estimation of the empowerment.</p>
<h2>B. 2 Efficient Optimisation</h2>
<p>In the case of smooth transition models $p\left(\mathbf{s}^{\prime} \mid a_{i}, \mathbf{s}\right)$ and policy $\omega_{\theta}(\mathbf{a} \mid \mathbf{s})$, a more efficient algorithm can be derived using stochastic backpropagation [2, 1] in which we rewrite both the model and policy as $\mathbf{s}^{\prime}=f\left(a_{i}, \mathbf{s}, \xi_{m}\right)$ and $\mathbf{a}=h_{\theta}\left(\mathbf{s}, \xi_{p}\right)$ where $\xi_{m}, \xi_{p} \sim \mathcal{N}(0, \mathbf{I})$ and $f$ and $h$ are differentiable functions. Using this representation, we can rewrite the variational objective function 4 as an expectation under $\xi_{m}, \xi_{p}$ :</p>
<p>$$
\mathcal{I}^{\omega, q}(\mathbf{s})=\mathbb{E}<em m="m">{\xi</em>\right)\right]
$$}, \xi_{p}}\left[\log q_{\xi}\left(h_{\theta}\left(\mathbf{s}, \xi_{p}\right)\right] f\left(h_{\theta}\left(\mathbf{s}, \xi_{p}\right), \mathbf{s}, \xi_{m}\right), \mathbf{s}\right)-\log \omega_{\theta}\left(h_{\theta}\left(\mathbf{s}, \xi_{p}\right) \mid \mathbf{s</p>
<p>The reparametrised bound 15 can now be optimized with respect to the parameters $\theta$ of the policy using stochastic gradient ascent.</p>
<h2>C Deriving the Blahut-Arimoto Iterations from the Variational Bound</h2>
<p>Here we show that the Blahut-Arimoto algorithm can be derived from the variational bound (8). The variational distribution $q(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s})$ that maximises the bound (8) is the posterior distribution over actions given present and future states,</p>
<p>$$
q^{*}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)=p\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right) \propto p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s}\right) \omega(\mathbf{a} \mid \mathbf{s})
$$</p>
<p>The Blahut-Arimoto algorithm is obtained by replacing equation (16) in equation (10) and rearranging the terms:</p>
<p>$$
\begin{aligned}
\omega_{t+1}(\mathbf{a} \mid \mathbf{s}) &amp; \propto \exp \left(\beta \mathbb{E}<em t="t">{p\left(s^{\prime} \mid s, A\right)}\left[\ln q</em>\right)\right]\right) \
&amp; \propto \exp \left(\beta \mathbb{E}}\left(\mathbf{a} \mid \mathbf{s}, \mathbf{s}^{\prime<em t="t">{p\left(s^{\prime} \mid s, A\right)}\left[\ln p</em>\right)\right]\right) \
&amp; \propto \exp \left(\beta \mathbb{E}}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s<em t="t">{p\left(s^{\prime} \mid s, A\right)}\left[\ln \frac{p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s}\right) \omega</em>\right]\right) \
&amp; \propto \omega_{t}(\mathbf{a} \mid \mathbf{s}) \exp \left(\beta \mathbb{E}}(\mathbf{a} \mid \mathbf{s})}{p_{t}\left(\mathbf{s}^{\prime} \mid \mathbf{s}\right)<em t="t">{p\left(s^{\prime} \mid s, A\right)}\left[\ln \frac{p\left(\mathbf{s}^{\prime} \mid \mathbf{a}, \mathbf{s}\right)}{p</em>\right]\right)
\end{aligned}
$$}\left(\mathbf{s}^{\prime} \mid \mathbf{s}\right)</p>
<h1>D Neural Network Description</h1>
<h2>D. 1 State representation using convolutional networks</h2>
<p>For observations that are images, we make use of a convolutional network to obtain a state representation. We use the same convolutional network for all experiments. After each convolution we apply a rectified non-linearity. For all experiments, we make use a 10 filters for each layer of the convolution. The first convolution consists of $4 \times 4$ kernels with a stride of 1 , and the second convolution consists of $3 \times 3$ kernels with a stride of 2 . The output of the convolution is passed through a fully connected layer with 100 hidden units, followed by a rectified non-linearity. This 100-dimensional representation is what forms the state representation $\mathbf{s}, \mathbf{s}^{\prime}$ used for the variational information maximisation components that follow this processing stage.</p>
<h2>D. 2 Parameterisation of other networks</h2>
<p>We also use neural networks in the parameterisation of the decoder distribution $q_{\xi}\left(\mathbf{a} \mid \mathbf{s}, \mathbf{s}^{\prime}\right)$ and in the directed model $h_{\theta}(\mathbf{a} \mid \mathbf{s})$. This distribution is of the form:</p>
<p>$$
q_{\xi}\left(\mathbf{a} \mid \mathbf{s}^{\prime}, \mathbf{s}\right)=q\left(a_{1} \mid \mathbf{s}, \mathbf{s}^{\prime}\right) \prod_{k=2}^{K} q\left(a_{k} \mid f_{\xi}\left(a_{k-1}, \mathbf{s}, \mathbf{s}^{\prime}\right)\right)
$$</p>
<p>where we must specify the form of the per action distributions $q\left(a_{k}\right)$. This distribution is Gaussian distribution whose mean and variance are parameterised by a two-layer neural network:</p>
<p>$$
\begin{aligned}
q\left(a_{k}\right) &amp; =\mathcal{N}\left(a_{k} \mid \mu_{\xi}\left(a_{k-1}, \mathbf{s}, \mathbf{s}^{\prime}\right), \sigma_{\xi}^{2}\left(a_{k-1}, \mathbf{s}, \mathbf{s}^{\prime}\right)\right) \
\mu_{\xi}\left(a_{k-1}, \mathbf{s}, \mathbf{s}^{\prime}\right) &amp; =g\left(W_{\mu} \eta+b\right) \
\log \sigma_{\xi}\left(a_{k-1}, \mathbf{s}, \mathbf{s}^{\prime}\right) &amp; =g\left(W_{\sigma} \eta+b\right) \
\eta &amp; =\ell\left(W_{2} g\left(W_{1} x+b_{1}\right)+b_{2}\right)
\end{aligned}
$$</p>
<p>where $\eta$ is a two-layer neural network which forms the shared component of the distribution. $g(\cdot)$ is an element-wise non-linearity, which is the rectified non-linearity in our case: $\operatorname{Rect}(x)=\max (0, x)$. The scalar function $\psi_{\theta}(\mathbf{s})$, also is specified by a two-layer neural network.</p>
<h2>References</h2>
<p>[1] Kingma, Diederik P and Welling, Max. Stochastic gradient vb and the variational auto-encoder. International Conference on Learning Representations, 2014.
[2] Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic back-propagation and variational inference in deep latent gaussian models. In International Conference on Machine Learning, 2014.
[3] Salge, C., Glackin, C., and Polani, D. Empowerment--an introduction. In Guided SelfOrganization: Inception, pp. 67-114. 2014.
[4] Yeung, R. W. The Blahut-Arimoto algorithms. In Information Theory and Network Coding, pp. 211-228. 2008.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>1 Video: http://youtu.be/eA9jVDa7O38 2 Video: http://youtu.be/eSAIJ0isc3Y
3 Videos: http://youtu.be/tMiiXXPirAQ;http://youtu.be/LV5jYY-JFpE&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
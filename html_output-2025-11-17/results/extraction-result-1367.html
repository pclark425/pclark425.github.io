<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1367 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1367</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1367</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-225ab689f41cef1dc18237ef5dab059a49950abf</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/225ab689f41cef1dc18237ef5dab059a49950abf" target="_blank">Curiosity-Driven Exploration by Self-Supervised Prediction</a></p>
                <p><strong>Paper Venue:</strong> 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</p>
                <p><strong>Paper TL;DR:</strong> This work forms curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model, which scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and ignores the aspects of the environment that cannot affect the agent.</p>
                <p><strong>Paper Abstract:</strong> In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward; 2) exploration with no extrinsic reward; and 3) generalization to unseen scenarios (e.g. new levels of the same game).</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1367.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1367.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VizDoom (DoomMyWayHome-v0)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VizDoom 3-D navigation (DoomMyWayHome-v0)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A first-person 3-D navigation benchmark (based on Doom) consisting of rooms connected by corridors; used to study sparse-reward navigation where the agent must reach a fixed goal (a vest) from various spawn locations. Evaluated with intrinsic-curiosity-driven agents (ICM+ A3C) and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>VizDoom (DoomMyWayHome-v0)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3-D first-person navigation in a Doom map: rooms connected by corridors; agent sees raw pixel frames (grayscale 42x42 stacked 4-frame state) and has discrete actions (move forward, move left, move right, no-op). Domain: simulated indoor navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td>approx. 350 time steps (paper: 'it takes approximately 350 steps for an optimal policy to reach the vest location from the farthest room')</td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>9 rooms connected by corridors; connectivity described qualitatively (rooms and corridors forming a sparse, corridor-linked map). Not quantified (no average degree reported).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>9 rooms (paper: 'The map consists of 9 rooms connected by corridors'); 17 possible spawn locations in the 'dense' setup; episode length max 2100 time steps; farthest room 350 steps from goal.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ICM + A3C (and baselines: A3C, ICM-pixels + A3C, TRPO, VIME+TRPO, random)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A3C asynchronous advantage actor-critic policy with a CNN + LSTM (LSTM 256 units) policy head; Intrinsic Curiosity Module (ICM) provides intrinsic rewards via inverse dynamics (action prediction) and forward dynamics prediction in learned feature space (φ dim = 288); ICM-pixels variant predicts pixels directly. Training used 20 asynchronous workers.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>steps-to-goal / convergence rate; coverage (rooms visited) for no-reward experiments; sample efficiency (speed of learning/convergence); success rate (percent of runs reaching goal).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Sparse-reward convergence: ICM + A3C achieved 100% mean (100% median) at convergence (table). TRPO: 26.0% mean (0.0% median). A3C baseline: 0.0%. VIME+TRPO: 46.1% (mean) (sparse setup). In very-sparse case ICM achieved a perfect score in 66% of random runs (text). Coverage: ICM-trained no-reward agent often traversed many rooms and sometimes entire map within 2100 steps (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Sparse (at convergence): ICM + A3C = 100% (mean, median); A3C = 0%; VIME+TRPO = 46.1% mean; TRPO = 26.0% mean. Very-sparse: ICM perfect score in 66% of runs (text).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Curiosity-driven policy (ICM) paired with a memory-enabled actor-critic (A3C+LSTM) performs best; policies that use learned forward/inverse dynamics features for intrinsic reward outperform pixel-prediction curiosity and vanilla epsilon-greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Performance degrades as effective graph distance (distance from spawn to goal) increases: when spawn distribution moves farther from goal (dense -> sparse -> very-sparse), baseline A3C performance falls off drastically while ICM retains strong performance. Pixel-space curiosity (ICM-pixels) works in dense/sparse but fails in very-sparse and is brittle to visual distractors. ICM is robust to nuisance visual dynamics and generalizes better to novel textures/maps. Longer shortest-path distances (higher diameter) make exploration harder for simple/random policies; ICM mitigates this effect by providing directed intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper varies 'topological difficulty' by changing spawn location distributions (17 uniform spawn locations = 'dense', fixed distant spawns = 'sparse' or 'very-sparse') and by testing on different maps/textures. Findings: (1) as spawn-to-goal distance increases, vanilla A3C fails while ICM agents still learn; (2) pixel-based curiosity is brittle to varied textures and added noise; (3) pretraining on one map (train topology) and fine-tuning on a different test map speeds learning (pretrained ICM fine-tuned on test map outperforms scratch).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies relying on curiosity-driven intrinsic rewards plus memory (LSTM) learn directed corridor-navigation behaviors (avoid bumping into walls, traverse rooms), whereas reactive/random policies get stuck. The ICM's learned embedding focuses on controllable factors, leading to robust exploratory policies; however, when curiosity saturates (agent has explored accessible areas), policy updates degrade ('boredom' / lack of intrinsic signal). Long-range required action sequences (notably absent in VizDoom experiments) would require longer memory/skill primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity-Driven Exploration by Self-Supervised Prediction', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1367.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1367.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Super Mario Bros (Gym-SMB levels)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Super Mario Bros (OpenAI Gym interface levels; Level-1 pretrain, Levels 2-3 test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic 2D platformer levels used to study exploration without extrinsic reward: agents receive only intrinsic curiosity reward and are evaluated by horizontal distance progressed (percentage of level) and ability to discover behaviors like jumping and defeating enemies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Super Mario Bros (Gym wrapper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Side-scrolling 2D platformer levels (Level-1 through Level-3) with continuous horizontal progress, obstacles (pits, pipes), enemies, and actions requiring multi-frame sequences; domain: arcade game navigation/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not modeled as an explicit graph in the paper; environment is a continuous 2D level with sequential progression and local branching (pipes/enemies); topology described in terms of level segments and obstacles rather than a node-edge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Levels with varying horizontal length; evaluation reported as 'distance covered' (e.g., Level-1 scratch mean 711 ± 59.3 units after 1.5M iterations). Specific obstacles: a pit at ~38% of Level-1 blocks further progress without a long jump (requires long action sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ICM + A3C (curiosity-only training), and baseline 'scratch' curiosity-only agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A3C policy with convolutional input processing and LSTM-based temporal model; Intrinsic Curiosity Module (ICM) computing intrinsic reward from forward model prediction error in learned inverse-model feature space; action space reparameterized to 14 joint-button actions; action-repeat used during training (repeat=6).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>horizontal distance progressed in level (mean distance, percentage of episodes crossing distance thresholds >200, >400, >600), ability to cross level segments, sample-efficiency (iterations to reach coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Level-1 (curiosity-only): Scratch (1.5M iters) mean distance = 711 ± 59.3; % distance >200 = 50.0%; % distance >400 = 35.0%; % distance >600 = 35.8%. Transfer: running Level-1 policy 'as is' on Level-2 yields mean 31.9 ± 4.2; fine-tuned (pretrain on L1 then 1.5M finetune on L2) mean = 466 ± 37.9 with % distance>200 = 64.2% (table). On Level-3 run-as-is sometimes performs better due to visual similarity (mean 319.3 ± 9.7).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Level-1: % distance >200 = 50.0% (scratch); % distance >400 = 35.0%; % distance >600 = 35.8%. Transfer fine-tuned on Level-2: % distance >200 = 64.2%; % distance >400 = 63.6%; % distance >600 = 42.6% (from table).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Curiosity-driven exploration policy (ICM) with memory and the ability to perform repeated action sequences; hierarchical/skill-based policies would likely be necessary to reliably execute long multi-frame sequences (e.g., long jump over pit).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>The presence of long challenging segments (a pit requiring a precise, long sequence of actions at ~38%) creates a 'curiosity blockade' where the agent cannot reach new states and intrinsic reward vanishes, halting learning; visual-domain differences (e.g., night vs day levels) affect 'run-as-is' transfer, but fine-tuning with curiosity recovers performance faster than training from scratch. Thus connectivity bottlenecks (narrow bridges/pits requiring precise sequences) reduce exploration effectiveness unless the policy can perform required action primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper compares Level-1 pretraining vs training-from-scratch across Levels 2 and 3: (1) Pretraining on Level-1 then fine-tuning on Level-2 yields faster/better exploration than training from scratch on Level-2; (2) Running Level-1 policy 'as is' performs poorly on Level-2 (different appearance) but reasonably well on Level-3 (visual similarity); (3) Fine-tuning on Level-3 can sometimes degrade performance because the agent reaches known region and intrinsic reward collapses, causing policy degeneration ('curiosity blockade' / boredom).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that acquire general low-level skills (moving, jumping, killing enemies) in easier levels (curriculum) transfer and accelerate exploration in harder levels; exploration policies fail at topology bottlenecks that require long deterministic action chains (e.g., long jumps), indicating a need for memory, persistence or temporally-extended skills (motor primitives) for high-diameter/sequence-requiring regions; when intrinsic reward saturates (agent has seen reachable states), policy updates can degenerate without extrinsic signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Curiosity-Driven Exploration by Self-Supervised Prediction', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to navigate in complex environments <em>(Rating: 2)</em></li>
                <li>VIME: Variational information maximizing exploration <em>(Rating: 2)</em></li>
                <li>Vizdoom: A doom-based ai research platform for visual reinforcement learning <em>(Rating: 2)</em></li>
                <li>Unifying count-based exploration and intrinsic motivation <em>(Rating: 1)</em></li>
                <li>Incentivizing exploration in reinforcement learning with deep predictive models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1367",
    "paper_id": "paper-225ab689f41cef1dc18237ef5dab059a49950abf",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "VizDoom (DoomMyWayHome-v0)",
            "name_full": "VizDoom 3-D navigation (DoomMyWayHome-v0)",
            "brief_description": "A first-person 3-D navigation benchmark (based on Doom) consisting of rooms connected by corridors; used to study sparse-reward navigation where the agent must reach a fixed goal (a vest) from various spawn locations. Evaluated with intrinsic-curiosity-driven agents (ICM+ A3C) and baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "VizDoom (DoomMyWayHome-v0)",
            "environment_description": "3-D first-person navigation in a Doom map: rooms connected by corridors; agent sees raw pixel frames (grayscale 42x42 stacked 4-frame state) and has discrete actions (move forward, move left, move right, no-op). Domain: simulated indoor navigation.",
            "graph_diameter": "approx. 350 time steps (paper: 'it takes approximately 350 steps for an optimal policy to reach the vest location from the farthest room')",
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "9 rooms connected by corridors; connectivity described qualitatively (rooms and corridors forming a sparse, corridor-linked map). Not quantified (no average degree reported).",
            "environment_size": "9 rooms (paper: 'The map consists of 9 rooms connected by corridors'); 17 possible spawn locations in the 'dense' setup; episode length max 2100 time steps; farthest room 350 steps from goal.",
            "agent_name": "ICM + A3C (and baselines: A3C, ICM-pixels + A3C, TRPO, VIME+TRPO, random)",
            "agent_description": "A3C asynchronous advantage actor-critic policy with a CNN + LSTM (LSTM 256 units) policy head; Intrinsic Curiosity Module (ICM) provides intrinsic rewards via inverse dynamics (action prediction) and forward dynamics prediction in learned feature space (φ dim = 288); ICM-pixels variant predicts pixels directly. Training used 20 asynchronous workers.",
            "exploration_efficiency_metric": "steps-to-goal / convergence rate; coverage (rooms visited) for no-reward experiments; sample efficiency (speed of learning/convergence); success rate (percent of runs reaching goal).",
            "exploration_efficiency_value": "Sparse-reward convergence: ICM + A3C achieved 100% mean (100% median) at convergence (table). TRPO: 26.0% mean (0.0% median). A3C baseline: 0.0%. VIME+TRPO: 46.1% (mean) (sparse setup). In very-sparse case ICM achieved a perfect score in 66% of random runs (text). Coverage: ICM-trained no-reward agent often traversed many rooms and sometimes entire map within 2100 steps (qualitative).",
            "success_rate": "Sparse (at convergence): ICM + A3C = 100% (mean, median); A3C = 0%; VIME+TRPO = 46.1% mean; TRPO = 26.0% mean. Very-sparse: ICM perfect score in 66% of runs (text).",
            "optimal_policy_type": "Curiosity-driven policy (ICM) paired with a memory-enabled actor-critic (A3C+LSTM) performs best; policies that use learned forward/inverse dynamics features for intrinsic reward outperform pixel-prediction curiosity and vanilla epsilon-greedy.",
            "topology_performance_relationship": "Performance degrades as effective graph distance (distance from spawn to goal) increases: when spawn distribution moves farther from goal (dense -&gt; sparse -&gt; very-sparse), baseline A3C performance falls off drastically while ICM retains strong performance. Pixel-space curiosity (ICM-pixels) works in dense/sparse but fails in very-sparse and is brittle to visual distractors. ICM is robust to nuisance visual dynamics and generalizes better to novel textures/maps. Longer shortest-path distances (higher diameter) make exploration harder for simple/random policies; ICM mitigates this effect by providing directed intrinsic rewards.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper varies 'topological difficulty' by changing spawn location distributions (17 uniform spawn locations = 'dense', fixed distant spawns = 'sparse' or 'very-sparse') and by testing on different maps/textures. Findings: (1) as spawn-to-goal distance increases, vanilla A3C fails while ICM agents still learn; (2) pixel-based curiosity is brittle to varied textures and added noise; (3) pretraining on one map (train topology) and fine-tuning on a different test map speeds learning (pretrained ICM fine-tuned on test map outperforms scratch).",
            "policy_structure_findings": "Policies relying on curiosity-driven intrinsic rewards plus memory (LSTM) learn directed corridor-navigation behaviors (avoid bumping into walls, traverse rooms), whereas reactive/random policies get stuck. The ICM's learned embedding focuses on controllable factors, leading to robust exploratory policies; however, when curiosity saturates (agent has explored accessible areas), policy updates degrade ('boredom' / lack of intrinsic signal). Long-range required action sequences (notably absent in VizDoom experiments) would require longer memory/skill primitives.",
            "uuid": "e1367.0",
            "source_info": {
                "paper_title": "Curiosity-Driven Exploration by Self-Supervised Prediction",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Super Mario Bros (Gym-SMB levels)",
            "name_full": "Super Mario Bros (OpenAI Gym interface levels; Level-1 pretrain, Levels 2-3 test)",
            "brief_description": "Classic 2D platformer levels used to study exploration without extrinsic reward: agents receive only intrinsic curiosity reward and are evaluated by horizontal distance progressed (percentage of level) and ability to discover behaviors like jumping and defeating enemies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Super Mario Bros (Gym wrapper)",
            "environment_description": "Side-scrolling 2D platformer levels (Level-1 through Level-3) with continuous horizontal progress, obstacles (pits, pipes), enemies, and actions requiring multi-frame sequences; domain: arcade game navigation/exploration.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Not modeled as an explicit graph in the paper; environment is a continuous 2D level with sequential progression and local branching (pipes/enemies); topology described in terms of level segments and obstacles rather than a node-edge graph.",
            "environment_size": "Levels with varying horizontal length; evaluation reported as 'distance covered' (e.g., Level-1 scratch mean 711 ± 59.3 units after 1.5M iterations). Specific obstacles: a pit at ~38% of Level-1 blocks further progress without a long jump (requires long action sequence).",
            "agent_name": "ICM + A3C (curiosity-only training), and baseline 'scratch' curiosity-only agents",
            "agent_description": "A3C policy with convolutional input processing and LSTM-based temporal model; Intrinsic Curiosity Module (ICM) computing intrinsic reward from forward model prediction error in learned inverse-model feature space; action space reparameterized to 14 joint-button actions; action-repeat used during training (repeat=6).",
            "exploration_efficiency_metric": "horizontal distance progressed in level (mean distance, percentage of episodes crossing distance thresholds &gt;200, &gt;400, &gt;600), ability to cross level segments, sample-efficiency (iterations to reach coverage).",
            "exploration_efficiency_value": "Level-1 (curiosity-only): Scratch (1.5M iters) mean distance = 711 ± 59.3; % distance &gt;200 = 50.0%; % distance &gt;400 = 35.0%; % distance &gt;600 = 35.8%. Transfer: running Level-1 policy 'as is' on Level-2 yields mean 31.9 ± 4.2; fine-tuned (pretrain on L1 then 1.5M finetune on L2) mean = 466 ± 37.9 with % distance&gt;200 = 64.2% (table). On Level-3 run-as-is sometimes performs better due to visual similarity (mean 319.3 ± 9.7).",
            "success_rate": "Level-1: % distance &gt;200 = 50.0% (scratch); % distance &gt;400 = 35.0%; % distance &gt;600 = 35.8%. Transfer fine-tuned on Level-2: % distance &gt;200 = 64.2%; % distance &gt;400 = 63.6%; % distance &gt;600 = 42.6% (from table).",
            "optimal_policy_type": "Curiosity-driven exploration policy (ICM) with memory and the ability to perform repeated action sequences; hierarchical/skill-based policies would likely be necessary to reliably execute long multi-frame sequences (e.g., long jump over pit).",
            "topology_performance_relationship": "The presence of long challenging segments (a pit requiring a precise, long sequence of actions at ~38%) creates a 'curiosity blockade' where the agent cannot reach new states and intrinsic reward vanishes, halting learning; visual-domain differences (e.g., night vs day levels) affect 'run-as-is' transfer, but fine-tuning with curiosity recovers performance faster than training from scratch. Thus connectivity bottlenecks (narrow bridges/pits requiring precise sequences) reduce exploration effectiveness unless the policy can perform required action primitives.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper compares Level-1 pretraining vs training-from-scratch across Levels 2 and 3: (1) Pretraining on Level-1 then fine-tuning on Level-2 yields faster/better exploration than training from scratch on Level-2; (2) Running Level-1 policy 'as is' performs poorly on Level-2 (different appearance) but reasonably well on Level-3 (visual similarity); (3) Fine-tuning on Level-3 can sometimes degrade performance because the agent reaches known region and intrinsic reward collapses, causing policy degeneration ('curiosity blockade' / boredom).",
            "policy_structure_findings": "Policies that acquire general low-level skills (moving, jumping, killing enemies) in easier levels (curriculum) transfer and accelerate exploration in harder levels; exploration policies fail at topology bottlenecks that require long deterministic action chains (e.g., long jumps), indicating a need for memory, persistence or temporally-extended skills (motor primitives) for high-diameter/sequence-requiring regions; when intrinsic reward saturates (agent has seen reachable states), policy updates can degenerate without extrinsic signals.",
            "uuid": "e1367.1",
            "source_info": {
                "paper_title": "Curiosity-Driven Exploration by Self-Supervised Prediction",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to navigate in complex environments",
            "rating": 2
        },
        {
            "paper_title": "VIME: Variational information maximizing exploration",
            "rating": 2
        },
        {
            "paper_title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Unifying count-based exploration and intrinsic motivation",
            "rating": 1
        },
        {
            "paper_title": "Incentivizing exploration in reinforcement learning with deep predictive models",
            "rating": 1
        }
    ],
    "cost": 0.012991999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Curiosity-driven Exploration by Self-supervised Prediction</h1>
<p>Deepak Pathak ${ }^{1}$ Pulkit Agrawal ${ }^{1}$ Alexei A. Efros ${ }^{1}$ Trevor Darrell ${ }^{1}$</p>
<h4>Abstract</h4>
<p>In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.</p>
<h2>1. Introduction</h2>
<p>Reinforcement learning algorithms aim at learning policies for achieving target tasks by maximizing rewards provided by the environment. In some scenarios, these rewards are supplied to the agent continuously, e.g. the running score in an Atari game (Mnih et al., 2015), or the distance between a robot arm and an object in a reaching task (Lillicrap et al., 2016). However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or miss-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Discovering how to play Super Mario Bros without rewards. (a) Using only curiosity-driven exploration, the agent makes significant progress in Level-1. (b) The gained knowledge helps the agent explore subsequent levels much faster than when starting from scratch. Watch the video at http://pathak22. github.io/noreward-rl/
ing altogether, and it is not possible to construct a shaped reward function. This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state. Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.</p>
<p>As human agents, we are accustomed to operating with rewards that are so sparse that we only experience them once or twice in a lifetime, if at all. To a three-year-old enjoying a sunny Sunday afternoon on a playground, most trappings of modern life - college, good job, a house, a family are so far into the future, they provide no useful reinforcement signal. Yet, the three-year-old has no trouble entertaining herself in that playground using what psychologists call intrinsic motivation (Ryan, 2000) or curiosity (Silvia, 2012). Motivation/curiosity have been used to explain the need to explore the environment and discover novel states. The French word flâneur perfectly captures the notion of a curiosity-driven observer, the "deliberately aimless pedestrian, unencumbered by any obligation or sense of urgency" (Cornelia Otis Skinner). More generally, curiosity is a way of learning new skills which might come handy for pursuing rewards in the future.</p>
<p>Similarly, in reinforcement learning, intrinsic motivation/rewards become critical whenever extrinsic rewards are sparse. Most formulations of intrinsic reward can be grouped into two broad classes: 1) encourage the agent to explore "novel" states (Bellemare et al., 2016; Lopes</p>
<p>et al., 2012; Poupart et al., 2006) or, 2) encourage the agent to perform actions that reduce the error/uncertainty in the agent's ability to predict the consequence of its own actions (i.e. its knowledge about the environment) (Houthooft et al., 2016; Mohamed \&amp; Rezende, 2015; Schmidhuber, 1991; 2010; Singh et al., 2005; Stadie et al., 2015).</p>
<p>Measuring "novelty" requires a statistical model of the distribution of the environmental states, whereas measuring prediction error/uncertainty requires building a model of environmental dynamics that predicts the next state $\left(s_{t+1}\right)$ given the current state $\left(s_{t}\right)$ and the action $\left(a_{t}\right)$ executed at time $t$. Both these models are hard to build in highdimensional continuous state spaces such as images. An additional challenge lies in dealing with the stochasticity of the agent-environment system, both due to the noise in the agent's actuation, which causes its end-effectors to move in a stochastic manner, and, more fundamentally, due to the inherent stochasticity in the environment. To give the example from (Schmidhuber, 2010), if the agent receiving images as state inputs is observing a television screen displaying white noise, every state will be novel and it would be impossible to predict the value of any pixel in the future. Other examples of such stochasticity include appearance changes due to shadows from other moving entities, presence of distractor objects, or other agents in the environment whose motion is not only hard to predict but is also irrelevant to the agent's goals. Somewhat different, but related, is the challenge of generalization across physically (and perhaps also visually) distinct but functionally similar parts of an environment, which is crucial for largescale problems. One proposed solution to all these problems is to only reward the agent when it encounters states that are hard to predict but are "learnable" (Schmidhuber, 1991). However, estimating learnability is a non-trivial problem (Lopes et al., 2012).</p>
<p>This work belongs to the broad category of methods that generate an intrinsic reward signal based on how hard it is for the agent to predict the consequences of its own actions, i.e. predict the next state given the current state and the executed action. However, we manage to escape most pitfalls of previous prediction approaches with the following key insight: we only predict those changes in the environment that could possibly be due to the actions of our agent or affect the agent, and ignore the rest. That is, instead of making predictions in the raw sensory space (e.g. pixels), we transform the sensory input into a feature space where only the information relevant to the action performed by the agent is represented. We learn this feature space using self-supervision - training a neural network on a proxy inverse dynamics task of predicting the agent's action given its current and next states. Since the neural network is only required to predict the action, it has no incentive to represent within its feature embedding space the factors of vari-
ation in the environment that do not affect the agent itself. We then use this feature space to train a forward dynamics model that predicts the feature representation of the next state, given the feature representation of the current state and the action. We provide the prediction error of the forward dynamics model to the agent as an intrinsic reward to encourage its curiosity.</p>
<p>The role of curiosity has been widely studied in the context of solving tasks with sparse rewards. In our opinion, curiosity has two other fundamental uses. Curiosity helps an agent explore its environment in the quest for new knowledge (a desirable characteristic of exploratory behavior is that it should improve as the agent gains more knowledge). Further, curiosity is a mechanism for an agent to learn skills that might be helpful in future scenarios. In this paper, we evaluate the effectiveness of our curiosity formulation in all three of these roles.</p>
<p>We first compare the performance of an A3C agent (Mnih et al., 2016) with and without the curiosity signal on 3-D navigation tasks with sparse extrinsic reward in the VizDoom environment. We show that a curiosity-driven intrinsic reward is crucial in accomplishing these tasks (see Section 4.1). Next, we show that even in the absence of any extrinsic rewards, a curious agent learns good exploration policies. For instance, an agent trained only with curiosity as its reward is able to cross a significant portion of Level-1 in Super Mario Bros. Similarly in VizDoom, the agent learns to walk intelligently along the corridors instead of bumping into walls or getting stuck in corners (see Section 4.2). A question that naturally follows is whether the learned exploratory behavior is specific to the physical space that the agent trained itself on, or if it enables the agent to perform better in unseen scenarios too? We show that the exploration policy learned in the first level of Mario helps the agent explore subsequent levels faster (shown in Figure 1), while the intelligent walking behavior learned by the curious VizDoom agent transfers to a completely new map with new textures (see Section 4.3). These results suggest that the proposed method enables an agent to learn generalizable skills even in the absence of an explicit goal.</p>
<h2>2. Curiosity-Driven Exploration</h2>
<p>Our agent is composed of two subsystems: a reward generator that outputs a curiosity-driven intrinsic reward signal and a policy that outputs a sequence of actions to maximize that reward signal. In addition to intrinsic rewards, the agent optionally may also receive some extrinsic reward from the environment. Let the intrinsic curiosity reward generated by the agent at time $t$ be $r_{t}^{i}$ and the extrinsic reward be $r_{t}^{e}$. The policy sub-system is trained to maximize the sum of these two rewards $r_{t}=r_{t}^{i}+r_{t}^{e}$, with $r_{t}^{e}$ mostly (if not always) zero.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. The agent in state $s_{t}$ interacts with the environment by executing an action $a_{t}$ sampled from its current policy $\pi$ and ends up in the state $s_{t+1}$. The policy $\pi$ is trained to optimize the sum of the extrinsic reward $\left(r_{t}^{e}\right)$ provided by the environment $E$ and the curiosity based intrinsic reward signal $\left(r_{t}^{i}\right)$ generated by our proposed Intrinsic Curiosity Module (ICM). ICM encodes the states $s_{t}, s_{t+1}$ into the features $\phi\left(s_{t}\right), \phi\left(s_{t+1}\right)$ that are trained to predict $a_{t}$ (i.e. inverse dynamics model). The forward model takes as inputs $\phi\left(s_{t}\right)$ and $a_{t}$ and predicts the feature representation $\hat{\phi}\left(s_{t+1}\right)$ of $s_{t+1}$. The prediction error in the feature space is used as the curiosity based intrinsic reward signal. As there is no incentive for $\phi\left(s_{t}\right)$ to encode any environmental features that can not influence or are not influenced by the agent's actions, the learned exploration strategy of our agent is robust to uncontrollable aspects of the environment.</p>
<p>We represent the policy $\pi\left(s_{t} ; \theta_{P}\right)$ by a deep neural network with parameters $\theta_{P}$. Given the agent in state $s_{t}$, it executes the action $a_{t} \sim \pi\left(s_{t} ; \theta_{P}\right)$ sampled from the policy. $\theta_{P}$ is optimized to maximize the expected sum of rewards,</p>
<p>$$
\max <em P="P">{\theta</em>}} \mathbb{E<em t="t">{\pi\left(s</em>\right]
$$} ; \theta_{P}\right)}\left[\Sigma_{t} r_{t</p>
<p>Unless specified otherwise, we use the notation $\pi(s)$ to denote the parameterized policy $\pi\left(s ; \theta_{P}\right)$. Our curiosity reward model can potentially be used with a range of policy learning methods; in the experiments discussed here, we use the asynchronous advantage actor critic policy gradient (A3C) (Mnih et al., 2016) for policy learning. Our main contribution is in designing an intrinsic reward signal based on prediction error of the agent's knowledge about its environment that scales to high-dimensional continuous state spaces like images, bypasses the hard problem of predicting pixels and is unaffected by the unpredictable aspects of the environment that do not affect the agent.</p>
<h3>2.1. Prediction error as curiosity reward</h3>
<p>Making predictions in the raw sensory space (e.g. when $s_{t}$ corresponds to images) is undesirable not only because it is hard to predict pixels directly, but also because it is unclear if predicting pixels is even the right objective to optimize. To see why, consider using prediction error in the pixel space as the curiosity reward. Imagine a scenario where the agent is observing the movement of tree leaves in a breeze. Since it is inherently hard to model breeze, it is even harder to predict the pixel location of each leaf.</p>
<p>This implies that the pixel prediction error will remain high and the agent will always remain curious about the leaves. But the motion of the leaves is inconsequential to the agent and therefore its continued curiosity about them is undesirable. The underlying problem is that the agent is unaware that some parts of the state space simply cannot be modeled and thus the agent can fall into an artificial curiosity trap and stall its exploration. Novelty-seeking exploration schemes that record the counts of visited states in a tabular form (or their extensions to continuous state spaces) also suffer from this issue. Measuring learning progress instead of prediction error has been proposed in the past as one solution (Schmidhuber, 1991). Unfortunately, there are currently no known computationally feasible mechanisms for measuring learning progress.</p>
<p>If not the raw observation space, then what is the right feature space for making predictions so that the prediction error provides a good measure of curiosity? To answer this question, let us divide all sources that can modify the agent's observations into three cases: (1) things that can be controlled by the agent; (2) things that the agent cannot control but that can affect the agent (e.g. a vehicle driven by another agent), and (3) things out of the agent's control and not affecting the agent (e.g. moving leaves). A good feature space for curiosity should model (1) and (2) and be unaffected by (3). This latter is because, if there is a source of variation that is inconsequential for the agent, then the agent has no incentive to know about it.</p>
<h3>2.2. Self-supervised prediction for exploration</h3>
<p>Instead of hand-designing a feature representation for every environment, our aim is to come up with a general mechanism for learning feature representations such that the prediction error in the learned feature space provides a good intrinsic reward signal. We propose that such a feature space can be learned by training a deep neural network with two sub-modules: the first sub-module encodes the raw state $\left(s_{t}\right)$ into a feature vector $\phi\left(s_{t}\right)$ and the second submodule takes as inputs the feature encoding $\phi\left(s_{t}\right), \phi\left(s_{t+1}\right)$ of two consequent states and predicts the action $\left(a_{t}\right)$ taken by the agent to move from state $s_{t}$ to $s_{t+1}$. Training this neural network amounts to learning function $g$ defined as:</p>
<p>$$
\hat{a}<em t="t">{t}=g\left(s</em>\right)
$$}, s_{t+1} ; \theta_{I</p>
<p>where, $\hat{a}<em t="t">{t}$ is the predicted estimate of the action $a</em>$ are trained to optimize,}$ and the the neural network parameters $\theta_{I</p>
<p>$$
\min <em I="I">{\theta</em>}} L_{I}\left(\hat{a<em t="t">{t}, a</em>\right)
$$</p>
<p>where, $L_{I}$ is the loss function that measures the discrepancy between the predicted and actual actions. In case $a_{t}$ is discrete, the output of $g$ is a soft-max distribution across all possible actions and minimizing $L_{I}$ amounts to maximum likelihood estimation of $\theta_{I}$ under a multinomial distribution. The learned function $g$ is also known as the inverse dynamics model and the tuple $\left(s_{t}, a_{t}, s_{t+1}\right)$ required to learn $g$ is obtained while the agent interacts with the environment using its current policy $\pi(s)$.
In addition to inverse dynamics model, we train another neural network that takes as inputs $a_{t}$ and $\phi\left(s_{t}\right)$ and predicts the feature encoding of the state at time step $t+1$,</p>
<p>$$
\hat{\phi}\left(s_{t+1}\right)=f\left(\phi\left(s_{t}\right), a_{t} ; \theta_{F}\right)
$$</p>
<p>where $\hat{\phi}\left(s_{t+1}\right)$ is the predicted estimate of $\phi\left(s_{t+1}\right)$ and the neural network parameters $\theta_{F}$ are optimized by minimizing the loss function $L_{F}$ :</p>
<p>$$
L_{F}\left(\phi\left(s_{t}\right), \hat{\phi}\left(s_{t+1}\right)\right)=\frac{1}{2}\left|\hat{\phi}\left(s_{t+1}\right)-\phi\left(s_{t+1}\right)\right|_{2}^{2}
$$</p>
<p>The learned function $f$ is also known as the forward dynamics model. The intrinsic reward signal $r_{t}^{i}$ is computed as,</p>
<p>$$
r_{t}^{i}=\frac{\eta}{2}\left|\hat{\phi}\left(s_{t+1}\right)-\phi\left(s_{t+1}\right)\right|_{2}^{2}
$$</p>
<p>where $\eta&gt;0$ is a scaling factor. In order to generate the curiosity based intrinsic reward signal, we jointly optimize the forward and inverse dynamics loss described in equations 3 and 5 respectively. The inverse model learns a feature space that encodes information relevant for predicting the agent's actions only and the forward model makes predictions in this feature space. We refer to this proposed
curiosity formulation as Intrinsic Curiosity Module (ICM). As there is no incentive for this feature space to encode any environmental features that are not influenced by the agent's actions, our agent will receive no rewards for reaching environmental states that are inherently unpredictable and its exploration strategy will be robust to the presence of distractor objects, changes in illumination, or other nuisance sources of variation in the environment. See Figure 2 for illustration of the formulation.</p>
<p>The use of inverse models has been investigated to learn features for recognition tasks (Agrawal et al., 2015; Jayaraman \&amp; Grauman, 2015). Agrawal et al. (2016) constructed a joint inverse-forward model to learn feature representation for the task of pushing objects. However, they only used the forward model as a regularizer for training the inverse model features, while we make use of the error in the forward model predictions as the curiosity reward for training our agent's policy.</p>
<p>The overall optimization problem that is solved for learning the agent is a composition of equations 1,3 and 5 and can be written as,</p>
<p>$$
\min <em P="P">{\theta</em>}, \theta_{I}, \theta_{F}}\left[-\lambda \mathbb{E<em t="t">{\pi\left(s</em>\right]
$$} ; \theta_{F}\right)}\left[\Sigma_{t} r_{t}\right]+(1-\beta) L_{I}+\beta L_{F</p>
<p>where $0 \leq \beta \leq 1$ is a scalar that weighs the inverse model loss against the forward model loss and $\lambda&gt;0$ is a scalar that weighs the importance of the policy gradient loss against the importance of learning the intrinsic reward signal.</p>
<h2>3. Experimental Setup</h2>
<p>To evaluate our curiosity module on its ability to improve exploration and provide generalization to novel scenarios, we will use two simulated environments. This section describes the details of the environments and the experimental setup.</p>
<p>Environments The first environment we evaluate on is the VizDoom (Kempka et al., 2016) game. We consider the Doom 3-D navigation task where the action space of the agent consists of four discrete actions - move forward, move left, move right and no-action. Our testing setup in all the experiments is the 'DoomMyWayHome-v0' environment which is available as part of OpenAI Gym (Brockman et al., 2016). Episodes are terminated either when the agent finds the vest or if the agent exceeds a maximum of 2100 time steps. The map consists of 9 rooms connected by corridors and the agent is tasked to reach some fixed goal location from its spawning location. The agent is only provided a sparse terminal reward of +1 if it finds the vest and zero otherwise. For generalization experiments, we pre-train on</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Frames from VizDoom 3-D environment which agent takes as input: (a) Usual 3-D navigation setup; (b) Setup when uncontrollable noise is added to the input.
a different map with different random textures from (Dosovitskiy &amp; Koltun, 2016) and each episode lasts for 2100 time steps. Sample frames from VizDoom are shown in Figure 3a, and maps are explained in Figure 4. It takes approximately 350 steps for an optimal policy to reach the vest location from the farthest room in this map (sparse reward).</p>
<p>Our second environment is the classic Nintendo game Super Mario Bros (Paquette, 2016). We consider four levels of the game: pre-training on the first level and showing generalization on the subsequent levels. In this setup, we reparametrize the action space of the agent into 14 unique actions following (Paquette, 2016). This game is played using a joystick allowing for multiple simultaneous button presses, where the duration of the press affects what action is being taken. This property makes the game particularly hard, e.g. to make a long jump over tall pipes or wide gaps, the agent needs to predict the same action up to 12 times in a row, introducing long-range dependencies. All our experiments on Mario are trained using curiosity signal only, without any reward from the game.</p>
<p>Training details All agents in this work are trained using visual inputs that are pre-processed in manner similar to (Mnih et al., 2016). The input RGB images are converted into gray-scale and re-sized to $42 \times 42$. In order to model temporal dependencies, the state representation $\left(s_{t}\right)$ of the environment is constructed by concatenating the current frame with the three previous frames. Closely following (Mnih et al., 2015; 2016), we use action repeat of four during training time in VizDoom and action repeat of six in Mario. However, we sample the policy without any action repeat during inference. Following the asynchronous training protocol in A3C, all the agents were trained asynchronously with twenty workers using stochastic gradient descent. We used ADAM optimizer with its parameters not shared across the workers.</p>
<p>A3C architecture The input state $s_{t}$ is passed through a sequence of four convolution layers with 32 filters each,
<img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Train Map Scenario
(b) Test Map Scenario</p>
<p>Figure 4. Maps for VizDoom 3-D environment: (a) For generalization experiments (c.f. Section 4.3), map of the environment where agent is pre-trained only using curiosity signal without any reward from environment. 'S' denotes the starting position. (b) Testing map for VizDoom experiments. Green star denotes goal location. Blue dots refer to 17 agent spawning locations in the map in the "dense" case. Rooms 13, 17 are the fixed start locations of agent in "sparse" and "very sparse" reward cases respectively. Note that textures are also different in train and test maps.
kernel size of 3 x 3 , stride of 2 and padding of 1 . An exponential linear unit (ELU; (Clevert et al., 2015)) is used after each convolution layer. The output of the last convolution layer is fed into a LSTM with 256 units. Two seperate fully connected layers are used to predict the value function and the action from the LSTM feature representation.</p>
<p>Intrinsic Curiosity Module (ICM) architecture The intrinsic curiosity module consists of the forward and the inverse model. The inverse model first maps the input state $\left(s_{t}\right)$ into a feature vector $\phi\left(s_{t}\right)$ using a series of four convolution layers, each with 32 filters, kernel size 3 x 3 , stride of 2 and padding of 1 . ELU non-linearity is used after each convolution layer. The dimensionality of $\phi\left(s_{t}\right)$ (i.e. the output of the fourth convolution layer) is 288 . For the inverse model, $\phi\left(s_{t}\right)$ and $\phi\left(s_{t+1}\right)$ are concatenated into a single feature vector and passed as inputs into a fully connected layer of 256 units followed by an output fully connected layer with 4 units to predict one of the four possible actions. The forward model is constructed by concatenating $\phi\left(s_{t}\right)$ with $a_{t}$ and passing it into a sequence of two fully connected layers with 256 and 288 units respectively. The value of $\beta$ is 0.2 , and $\lambda$ is 0.1 . The Equation (7) is minimized with learning rate of $1 e-3$.</p>
<p>Baseline Methods 'ICM + A3C' denotes our full algorithm which combines intrinsic curiosity model with A3C. Across different experiments, we compare our approach with three baselines. First is the vanilla 'A3C' algorithm with $\epsilon$-greedy exploration. Second is 'ICM-pixels + A3C', which is a variant of our ICM without the inverse model, and has curiosity reward dependent only on the forward model loss in predicting next observation in pixel space. To design this, we remove the inverse model layers and append</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Comparing the performance of the A3C agent with no curiosity (blue) against the curiosity in pixel space agent (green) and the proposed curious ICM-A3C agent (orange) as the hardness of the exploration task is gradually increased from left to right. Exploration becomes harder with larger distance between the initial and goal locations: "dense", "sparse" and "very sparse". The results depict that succeeding on harder exploration task becomes progressively harder for the baseline A3C, whereas the curious A3C is able to achieve good score in all the scenarios. Pixel based curiosity works in dense and sparse but fails in very sparse reward setting. The protocol followed in the plots involves running three independent runs of each algorithm. Darker line represents mean and shaded area represents mean $\pm$ standard error of mean. We did not perform any tuning of random seeds.
deconvolution layers to the forward model. ICM-pixels is close to ICM in architecture but incapable of learning embedding that is invariant to the uncontrollable part of environment. Note that ICM-pixels is representative of previous methods which compute information gain by directly using the observation space (Schmidhuber, 2010; Stadie et al., 2015). We show that directly using observation space for computing curiosity is significantly worse than learning an embedding as in ICM. Finally, we include comparison with state-of-the-art exploration methods based on variational information maximization (VIME) (Houthooft et al., 2016) which is trained with TRPO.</p>
<h2>4. Experiments</h2>
<p>We qualitatively and quantitatively evaluate the performance of the learned policy with and without the proposed intrinsic curiosity signal in two environments, VizDoom and Super Mario Bros. Three broad settings are evaluated: a) sparse extrinsic reward on reaching a goal (Section 4.1); b) exploration with no extrinsic reward (Section 4.2); and c) generalization to novel scenarios (Section 4.3). In VizDoom generalization is evaluated on a novel map with novel textures, while in Mario it is evaluated on subsequent game levels.</p>
<h3>4.1. Sparse Extrinsic Reward Setting</h3>
<p>We perform extrinsic reward experiments on VizDoom using 'DoomMyWayHome-v0' setup described in Section 3. The extrinsic reward is sparse and only provided when the agent finds the goal (a vest) located at a fixed location in the map. We systematically varied the difficulty of this goaldirected exploration task by varying the distance between
the initial spawning location of the agent and the location of the goal. A larger distance means that the chances of reaching the goal location by random exploration is lower and consequently the reward is said to be sparser.</p>
<p>Varying the degree of reward sparsity: We consider three setups with "dense", "sparse" and "very-sparse" rewards (see Figure 4b). In these settings, the reward is always terminal and the episode terminates upon reaching goal or after a maximum of 2100 steps. In the "dense" reward case, the agent is randomly spawned in any of the 17 possible spawning locations uniformly distributed across the map. This is not a hard exploration task because sometimes the agent is randomly initialized close to the goal and therefore by random $\epsilon$-greedy exploration it can reach the goal with reasonably high probability. In the "sparse" and "very sparse" reward cases, the agent is always spawned in Room-13 and Room-17 respectively which are 270 and 350 steps away from the goal under an optimal policy. A long sequence of directed actions is required to reach the goals from these rooms, making these settings hard goal directed exploration problems.</p>
<p>Results shown in Figure 5 indicate that while the performance of the baseline A3C degrades with sparser rewards, curious A3C agents are superior in all cases. In the "dense" reward case, curious agents learn much faster indicating more efficient exploration of the environment as compared to $\epsilon$-greedy exploration of the baseline agent. One possible explanation of the inferior performance of ICM-pixels in comparison to ICM is that in every episode the agent is spawned in one out of seventeen rooms with different textures. It is hard to learn a pixel-prediction model as the number of textures increases.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Evaluating the robustness of ICM to the presence of uncontrollable distractors in the environment. We created such a distractor by replacing $40 \%$ of the visual observation of the agent by white noise (see Figure 3b). The results show that while ICM succeeds most of the times, the pixel prediction model struggles.</p>
<p>In the "sparse" reward case, as expected, the baseline A3C agent fails to solve the task, while the curious A3C agent is able to learn the task quickly. Note that ICM-pixels and ICM have similar convergence because, with a fixed spawning location of the agent, the ICM-pixels encounters the same textures at the starting of each episode which makes learning the pixel-prediction model easier as compared to the "dense" reward case. Finally, in the "very sparse" reward case, both the A3C agent and ICM-pixels never succeed, while the ICM agent achieves a perfect score in $66 \%$ of the random runs. This indicates that ICM is better suited than ICM-pixels and vanilla A3C for hard goal directed exploration tasks.</p>
<p>Robustness to uncontrollable dynamics For testing the robustness of the proposed ICM formulation to changes in the environment that do not affect the agent, we augmented the agent's observation with a fixed region of white noise which made up $40 \%$ of the image (see Figure 3b). In VizDoom 3-D navigation, ideally the agent should be unaffected by this noise as the noise does not affect the agent in anyway and is merely a nuisance. Figure 6 compares the performance of ICM against some baseline methods on the "sparse" reward setup described above. While, the proposed ICM agent achieves a perfect score, ICM-pixels suffers significantly despite having succeeded at the "sparse reward" task when the inputs were not augmented with any noise (see Figure 5b). This indicates that in contrast to ICM-pixels, ICM is insensitive to nuisance changes in the environment.</p>
<p>Comparison to TRPO-VIME We now compare our curious agent against variational information maximization agent trained with TRPO (Houthooft et al., 2016) for the</p>
<p>VizDoom "sparse" reward setup described above. TRPO is in general more sample efficient than A3C but takes a lot more wall-clock time. We do not show these results in plots because TRPO and A3C have different setups. The hyperparameters and accuracy for the TRPO and VIME results follow from the concurrent work (Fu et al., 2017). Despite the sample efficiency of TRPO, we see that our ICM agents work significantly better than TRPO and TRPO-VIME, both in terms of convergence rate and accuracy. Results are shown in the Table below:</p>
<table>
<thead>
<tr>
<th>Method <br> ("sparse" reward setup)</th>
<th>Mean (Median) Score <br> (at convergence)</th>
</tr>
</thead>
<tbody>
<tr>
<td>TRPO</td>
<td>$26.0 \%(0.0 \%)$</td>
</tr>
<tr>
<td>A3C</td>
<td>$0.0 \%(0.0 \%)$</td>
</tr>
<tr>
<td>VIME + TRPO</td>
<td>$46.1 \%(27.1 \%)$</td>
</tr>
<tr>
<td>ICM + A3C</td>
<td>$\mathbf{1 0 0 . 0 \% ( 1 0 0 . 0 \% )}$</td>
</tr>
</tbody>
</table>
<p>As a sanity check, we replaced the curiosity network with random noise sources and used them as the curiosity reward. We performed systematic sweep across different distribution parameters in the "sparse" reward case: uniform, Gaussian and Laplacian. We found that none of these agents were able to reach the goal showing that our curiosity module does not learn degenerate solutions.</p>
<h3>4.2. No Reward Setting</h3>
<p>A good exploration policy is one which allows the agent to visit as many states as possible even without any goals. In the case of 3-D navigation, we expect a good exploration policy to cover as much of the map as possible; in the case of playing a game, we expect it to visit as many game states as possible. In order to test if our agent can learn a good exploration policy, we trained it on VizDoom and Mario without any rewards from the environment. We then evaluated what portion of the map was explore (for VizDoom), and how much progress it made (for Mario) in this setting. To our surprise, we have found that in both cases, the noreward agent was able to perform quote well (see video at http://pathak22.github.io/noreward_rl/).</p>
<p>VizDoom: Coverage during Exploration. An agent trained with no extrinsic rewards was able to learn to navigate corridors, walk between rooms and explore many rooms in the 3-D Doom environment. On many occasions the agent traversed the entire map and reached rooms that were farthest away from the room it was initialized in. Given that the episode terminates in 2100 steps and farthest rooms are over 250 steps away (for an optimally-moving agent), this result is quite remarkable, demonstrating that it is possible to learn useful skills without the requirement of any external supervision of rewards. Example explorations are shown in Figure 7. The first 3 maps show our agent ex-</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Each column in the figure shows the visitation pattern of an agent exploring the environment. The red arrow shows the initial location and orientation of the agent at the start of the episode. Each room that the agent visits during its exploration of maximum 2100 steps has been colored. The first three columns (with maps colored in yellow) show the exploration strategy of an agent trained with curiosity driven internal reward signal only. The last two columns show the rooms visited by an agent conducting random exploration. The results clearly show that the curious agent trained with intrinsic rewards explores a significantly larger number of rooms as compared to a randomly exploring agent.
plore a much larger state space without any extrinsic signal, compared to a random exploration agent (last two maps), which often has hard time getting around local minima of state spaces, e.g. getting stuck against a wall and not able to move (see video).</p>
<p>Mario: Learning to play with no rewards. We train our agent in the Super Mario World using only curiosity based signal. Without any extrinsic reward from environment, our Mario agent can learn to cross over $30 \%$ of Level-1. The agent received no reward for killing or dodging enemies or avoiding fatal events, yet it automatically discovered these behaviors (see video). One possible reason is because getting killed by the enemy will result in only seeing a small part of the game space, making its curiosity saturate. In order to remain curious, it is in the agent's interest to learn how to kill and dodge enemies so that it can reach new parts of the game space. This suggests that curiosity provides indirect supervision for learning interesting behaviors.</p>
<p>To the best of our knowledge, this is the first demonstration where the agent learns to navigate in a 3D environment and discovers how to play a game by making use of relatively complex visual imagery directly from pixels, without any extrinsic rewards. There are several prior works that use reinforcement learning to navigate in 3D environments from pixel inputs or playing ATARI games such as (Mirowski et al., 2017; Mnih et al., 2015; 2016), but they rely on intermediate external rewards provided by the environment.</p>
<h3>4.3. Generalization to Novel Scenarios</h3>
<p>In the previous section we showed that our agent learns to explore large parts of the space where its curiosity-driven exploration policy was trained. However, it remains unclear whether the agent has done this by learning "generalized skills" for efficiently exploring its environment, or if it simply memorized the training set. In other words we would like to know, when exploring a space, how much of the learned behavior is specific to that particular space and how much is general enough to be useful in novel scenar-
ios? To investigate this question, we train a no reward exploratory behavior in one scenario (e.g. Level-1 of Mario) and then evaluate the resulting exploration policy in three different ways: a) apply the learned policy "as is" to a new scenario; b) adapt the policy by fine-tuning with curiosity reward only; c) adapt the policy to maximize some extrinsic reward. Happily, in all three cases, we observe some promising generalization results:</p>
<p>Evaluate "as is": We evaluate the policy trained by maximizing curiosity on Level-1 of Mario on subsequent levels without adapting the learned policy in any way. We measure the distance covered by the agent as a result of executing this policy on Levels 1, 2, and 3, as shown in Table 1. We note that the policy performs surprisingly well on Level 3, suggesting good generalization, despite the fact that Level-3 has different structures and enemies compared to Level-1. However, note that the running "as is" on Level2 does not do well. At first, this seems to contradict the generalization results on Level-3. However, note that Level-3 has similar global visual appearance (day world with sunlight) to Level-1, whereas Level-2 is significantly different (night world). If this is indeed the issue, then it should be possible to quickly adapt the exploration policy to Level-2 with a little bit of "fine-tuning". We will explore this below.</p>
<p>Fine-tuning with curiosity only: From Table 1 we see that when the agent pre-trained (using only curiosity as reward) on Level-1 is fine-tuned (using only curiosity as reward) on Level-2 it quickly overcomes the mismatch in global visual appearance and achieves a higher score than training from scratch with the same number of iterations. Interestingly, training "from scratch" on Level-2 is worse than the fine-tuned policy, even when training for more iterations than pre-training + fine-tuning combined. One possible reason is that Level-2 is more difficult than Level1, so learning the basic skills such as moving, jumping, and killing enemies from scratch is much more dangerous than in the relative "safety" of Level-1. This result, therefore might suggest that first pre-training on an earlier level</p>
<table>
<thead>
<tr>
<th>Level Ids</th>
<th>Level-1</th>
<th>Level-2</th>
<th></th>
<th></th>
<th></th>
<th>Level-3</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>Scratch</td>
<td>Run as is</td>
<td>Fine-tuned</td>
<td>Scratch</td>
<td>Scratch</td>
<td>Run as is</td>
<td>Fine-tuned</td>
<td>Scratch</td>
<td>Scratch</td>
</tr>
<tr>
<td>Iterations</td>
<td>1.5M</td>
<td>0</td>
<td>1.5M</td>
<td>1.5M</td>
<td>3.5M</td>
<td>0</td>
<td>1.5M</td>
<td>1.5M</td>
<td>5.0M</td>
</tr>
<tr>
<td>Mean $\pm$ stderr</td>
<td>$711 \pm 59.3$</td>
<td>$31.9 \pm 4.2$</td>
<td>$466 \pm 37.9$</td>
<td>$399.7 \pm 22.5$</td>
<td>$455.5 \pm 33.4$</td>
<td>$319.3 \pm 9.7$</td>
<td>$97.5 \pm 17.4$</td>
<td>$11.8 \pm 3.3$</td>
<td>$42.2 \pm 6.4$</td>
</tr>
<tr>
<td>$\%$ distance $&gt;200$</td>
<td>$50.0 \pm 0.0$</td>
<td>0</td>
<td>$64.2 \pm 5.6$</td>
<td>$88.2 \pm 3.3$</td>
<td>$69.6 \pm 5.7$</td>
<td>$50.0 \pm 0.0$</td>
<td>$1.5 \pm 1.4$</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>$\%$ distance $&gt;400$</td>
<td>$35.0 \pm 4.1$</td>
<td>0</td>
<td>$63.6 \pm 6.6$</td>
<td>$33.2 \pm 7.1$</td>
<td>$51.9 \pm 5.7$</td>
<td>$8.4 \pm 2.8$</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>$\%$ distance $&gt;600$</td>
<td>$35.8 \pm 4.5$</td>
<td>0</td>
<td>$42.6 \pm 6.1$</td>
<td>$14.9 \pm 4.4$</td>
<td>$28.1 \pm 5.4$</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Table 1. Quantitative evaluation of the agent trained to play Super Mario Bros. using only curiosity signal without any rewards from the game. Our agent was trained with no rewards in Level-1. We then evaluate the agent's policy both when it is run "as is", and further fine-tuned on subsequent levels. The results are compared to settings when Mario agent is train from scratch in Level-2,3 using only curiosity without any extrinsic rewards. Evaluation metric is based on the distance covered by the Mario agent.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Performance of ICM + A3C agents on the test set of VizDoom in the "very sparse" reward case. Fine-tuned models learn the exploration policy without any external rewards on the training maps and are then fine-tuned on the test map. The scratch models are directly trained on the test map. The fine-tuned ICM + A3C significantly outperforms ICM + A3C indicating that our curiosity formulation is able to learn generalizable exploration policies. The pixel prediction based ICM agent completely fail. Note that textures are also different in train and test.
and then fine-tuning on a later one produces a form of curriculum which aids learning and generalization. In other words, the agent is able to use the knowledge it acquired by playing Level-1 to better explore the subsequent levels. Of course, the game designers do this on purpose to allow the human players to gradually learn to play the game.</p>
<p>However, interestingly, fine-tuning the exploration policy pre-trained on Level-1 to Level-3 deteriorates the performance, compared to running "as is". This is because Level3 is very hard for the agent to cross beyond a certain point - the agent hits a curiosity blockade and is unable to make any progress. As the agent has already learned about parts of the environment before the hard point, it receives almost no curiosity reward and as a result it attempts to update its policy with almost zero intrinsic rewards and the policy slowly degenerates. This behavior is vaguely analogous to boredom, where if the agent is unable to make progress it gets bored and stops exploring.</p>
<p>Fine-tuning with extrinsic rewards: If it is the case that the agent has actually learned useful exploratory behavior, then it should be able to learn quicker than starting from scratch even when external rewards are provided by environment. We perform this evaluation on VizDoom where we pre-train the agent using curiosity reward on a map showed in Figure 4a. We then test on the "very sparse" reward setting of 'DoomMyWayHome-v0' environment which uses a different map with novel textures (see Figure 4b) as described earlier in Section 4.1.</p>
<p>Results in Figure 8 show that the ICM agent pre-trained only with curiosity and then fine-tuned with external reward learns faster and achieves higher reward than an ICM agent trained from scratch to jointly maximize curiosity and the external rewards. This result confirms that the learned exploratory behavior is also useful when the agent is required to achieve goals specified by the environment. It is also worth noting that ICM-pixels does not generalize to this test environment. This indicates that the proposed mechanism of measuring curiosity is significantly better for learning skills that generalize as compared to measuring curiosity in the raw sensory space.</p>
<h2>5. Related Work</h2>
<p>Curiosity-driven exploration is a well studied topic in the reinforcement learning literature and a good summary can be found in (Oudeyer \&amp; Kaplan, 2009; Oudeyer et al., 2007). Schmidhuber (1991; 2010) and Sun et al. (2011) use surprise and compression progress as intrinsic rewards. Classic work of Kearns et al. (1999) and Brafman et al. (2002) propose exploration algorithms polynomial in the number of state space parameters. Others have used empowerment, which is the information gain based on entropy of actions, as intrinsic rewards (Klyubin et al., 2005; Mohamed \&amp; Rezende, 2015). Stadie et al. (2015) use prediction error in the feature space of an auto-encoder as a measure of interesting states to explore. State visitation counts have also been investigated for exploration (Bellemare et al., 2016; Oh et al., 2015; Tang et al., 2016). Osband et al. (2016) train multiple value functions and makes</p>
<p>use of bootstrapping and Thompson sampling for exploration. Many approaches measure information gain for exploration (Little \&amp; Sommer, 2014; Still \&amp; Precup, 2012; Storck et al., 1995). Houthooft et al. (2016) use an exploration strategy that maximizes information gain about the agent's belief of the environment's dynamics. Our approach of jointly training forward and inverse models for learning a feature space has similarities to (Agrawal et al., 2016; Jordan \&amp; Rumelhart, 1992; Wolpert et al., 1995), but these works use the learned models of dynamics for planning a sequence of actions instead of exploration. The idea of using a proxy task to learn a semantic feature embedding has been used in a number of works on self-supervised learning in computer vision (Agrawal et al., 2015; Doersch et al., 2015; Goroshin et al., 2015; Jayaraman \&amp; Grauman, 2015; Pathak et al., 2016; Wang \&amp; Gupta, 2015).</p>
<p>Concurrent work: A number of interesting related papers have appeared on Arxiv while the present work was in submission. Sukhbaatar et al. (2017) generates supervision for pre-training via asymmetric self-play between two agents to improve data efficiency during fine-tuning. Several methods propose improving data efficiency of RL algorithms using self-supervised prediction based auxiliary tasks (Jaderberg et al., 2017; Shelhamer et al., 2017). Fu et al. (2017) learn discriminative models, and Gregor et al. (2017) use empowerment based measure to tackle exploration in sparse reward setups.</p>
<h2>6. Discussion</h2>
<p>In this work we propose a mechanism for generating curiosity-driven intrinsic reward signal that scales to high dimensional visual inputs, bypasses the difficult problem of predicting pixels and ensures that the exploration strategy of the agent is unaffected by nuisance factors in the environment. We demonstrate that our agent significantly outperforms the baseline A3C with no curiosity, a recently proposed VIME (Houthooft et al., 2016) formulation for exploration, and a baseline pixel-predicting formulation.</p>
<p>In VizDoom our agent learns the exploration behavior of moving along corridors and across rooms without any rewards from the environment. In Mario our agent crosses more than $30 \%$ of Level-1 without any rewards from the game. One reason why our agent is unable to go beyond this limit is the presence of a pit at $38 \%$ of the game that requires a very specific sequence of 15-20 key presses in order to jump across it. If the agent is unable to execute this sequence, it falls in the pit and dies, receiving no further rewards from the environment. Therefore it receives no gradient information indicating that there is a world beyond the pit that could potentially be explored. This issue is somewhat orthogonal to developing models of curiosity, but presents a challenging problem for policy learning.</p>
<p>It is common practice to evaluate reinforcement learning approaches in the same environment that was used for training. However, we feel that it is also important to evaluate on a separate "testing set" as well. This allows us to gauge how much of what has been learned is specific to the training environment (i.e. memorized), and how much might constitute "generalizable skills" that could be applied to new settings. In this paper, we evaluate generalization in two ways: 1) by applying the learned policy to a new scenario "as is" (no further learning), and 2) by finetuning the learned policy on a new scenario (we borrow the pre-training/fine-tuning nomenclature from the deep feature learning literature). We believe that evaluating generalization is a valuable tool and will allow the community to better understand the performance of various reinforcement learning algorithms. To further aid in this effort, we will make the code for our algorithm, as well as testing and environment setups freely available online.</p>
<p>An interesting direction of future research is to use the learned exploration behavior/skill as a motor primitive/lowlevel policy in a more complex, hierarchical system. For example, our VizDoom agent learns to walk along corridors instead of bumping into walls. This could be a useful primitive for a navigation system.</p>
<p>While the rich and diverse real world provides ample opportunities for interaction, reward signals are sparse. Our approach excels in this setting and converts unexpected interactions that affect the agent into intrinsic rewards. However our approach does not directly extend to the scenarios where "opportunities for interactions" are also rare. In theory, one could save such events in a replay memory and use them to guide exploration. However, we leave this extension for future work.</p>
<p>Acknowledgements: We would like to thank Sergey Levine, Evan Shelhamer, Saurabh Gupta, Phillip Isola and other members of the BAIR lab for fruitful discussions and comments. We thank Jacob Huh for help with Figure 2 and Alexey Dosovitskiy for VizDoom maps. This work was supported in part by NSF IIS-1212798, IIS-1427425, IIS1536003, IIS-1633310, ONR MURI N00014-14-1-0671, Berkeley DeepDrive, equipment grant from Nvidia, and the Valrhona Reinforcement Learning Fellowship.</p>
<h2>References</h2>
<p>Agrawal, Pulkit, Carreira, Joao, and Malik, Jitendra. Learning to see by moving. In $I C C V, 2015$.</p>
<p>Agrawal, Pulkit, Nair, Ashvin, Abbeel, Pieter, Malik, Jitendra, and Levine, Sergey. Learning to poke by poking: Experiential learning of intuitive physics. NIPS, 2016.</p>
<p>Bellemare, Marc, Srinivasan, Sriram, Ostrovski, Georg,</p>
<p>Schaul, Tom, Saxton, David, and Munos, Remi. Unifying count-based exploration and intrinsic motivation. In NIPS, 2016.</p>
<p>Brafman, Ronen I and Tennenholtz, Moshe. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. JMLR, 2002.</p>
<p>Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig, Schneider, Jonas, Schulman, John, Tang, Jie, and Zaremba, Wojciech. Openai gym. arXiv:1606.01540, 2016.</p>
<p>Clevert, Djork-Arné, Unterthiner, Thomas, and Hochreiter, Sepp. Fast and accurate deep network learning by exponential linear units (elus). arXiv:1511.07289, 2015.</p>
<p>Doersch, Carl, Gupta, Abhinav, and Efros, Alexei A. Unsupervised visual representation learning by context prediction. In ICCV, 2015.</p>
<p>Dosovitskiy, Alexey and Koltun, Vladlen. Learning to act by predicting the future. ICLR, 2016.</p>
<p>Fu, Justin, Co-Reyes, John D, and Levine, Sergey. Ex2: Exploration with exemplar models for deep reinforcement learning. arXiv:1703.01260, 2017.</p>
<p>Goroshin, Ross, Bruna, Joan, Tompson, Jonathan, Eigen, David, and LeCun, Yann. Unsupervised feature learning from temporal data. arXiv:1504.02518, 2015.</p>
<p>Gregor, Karol, Rezende, Danilo Jimenez, and Wierstra, Daan. Variational intrinsic control. ICLR Workshop, 2017.</p>
<p>Houthooft, Rein, Chen, Xi, Duan, Yan, Schulman, John, De Turck, Filip, and Abbeel, Pieter. Vime: Variational information maximizing exploration. In NIPS, 2016.</p>
<p>Jaderberg, Max, Mnih, Volodymyr, Czarnecki, Wojciech Marian, Schaul, Tom, Leibo, Joel Z, Silver, David, and Kavukcuoglu, Koray. Reinforcement learning with unsupervised auxiliary tasks. ICLR, 2017.</p>
<p>Jayaraman, Dinesh and Grauman, Kristen. Learning image representations tied to ego-motion. In ICCV, 2015.</p>
<p>Jordan, Michael I and Rumelhart, David E. Forward models: Supervised learning with a distal teacher. Cognitive science, 1992.</p>
<p>Kearns, Michael and Koller, Daphne. Efficient reinforcement learning in factored mdps. In IJCAI, 1999.</p>
<p>Kempka, Michał, Wydmuch, Marek, Runc, Grzegorz, Toczek, Jakub, and Jaśkowski, Wojciech. Vizdoom: A doom-based ai research platform for visual reinforcement learning. arXiv:1605.02097, 2016.</p>
<p>Klyubin, Alexander S, Polani, Daniel, and Nehaniv, Chrystopher L. Empowerment: A universal agentcentric measure of control. In Evolutionary Computation, 2005.</p>
<p>Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan. Continuous control with deep reinforcement learning. ICLR, 2016.</p>
<p>Little, Daniel Y and Sommer, Friedrich T. Learning and exploration in action-perception loops. Closing the Loop Around Neural Systems, 2014.</p>
<p>Lopes, Manuel, Lang, Tobias, Toussaint, Marc, and Oudeyer, Pierre-Yves. Exploration in model-based reinforcement learning by empirically estimating learning progress. In NIPS, 2012.</p>
<p>Mirowski, Piotr, Pascanu, Razvan, Viola, Fabio, Soyer, Hubert, Ballard, Andy, Banino, Andrea, Denil, Misha, Goroshin, Ross, Sifre, Laurent, Kavukcuoglu, Koray, et al. Learning to navigate in complex environments. ICLR, 2017.</p>
<p>Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al. Human-level control through deep reinforcement learning. Nature, 2015.</p>
<p>Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi, Graves, Alex, Lillicrap, Timothy P, Harley, Tim, Silver, David, and Kavukcuoglu, Koray. Asynchronous methods for deep reinforcement learning. In ICML, 2016.</p>
<p>Mohamed, Shakir and Rezende, Danilo Jimenez. Variational information maximisation for intrinsically motivated reinforcement learning. In NIPS, 2015.</p>
<p>Oh, Junhyuk, Guo, Xiaoxiao, Lee, Honglak, Lewis, Richard L, and Singh, Satinder. Action-conditional video prediction using deep networks in atari games. In NIPS, 2015.</p>
<p>Osband, Ian, Blundell, Charles, Pritzel, Alexander, and Van Roy, Benjamin. Deep exploration via bootstrapped dqn. In NIPS, 2016.</p>
<p>Oudeyer, Pierre-Yves and Kaplan, Frederic. What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 2009.</p>
<p>Oudeyer, Pierre-Yves, Kaplan, Frdric, and Hafner, Verena V. Intrinsic motivation systems for autonomous mental development. Evolutionary Computation, 2007.</p>
<p>Paquette, Philip. Super mario bros. in openai gym. github:ppaquette/gym-super-mario, 2016.</p>
<p>Pathak, Deepak, Krahenbuhl, Philipp, Donahue, Jeff, Darrell, Trevor, and Efros, Alexei A. Context encoders: Feature learning by inpainting. In CVPR, 2016.</p>
<p>Poupart, Pascal, Vlassis, Nikos, Hoey, Jesse, and Regan, Kevin. An analytic solution to discrete bayesian reinforcement learning. In ICML, 2006.</p>
<p>Ryan, Richard; Deci, Edward L. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary Educational Psychology, 2000.</p>
<p>Schmidhuber, Jurgen. A possibility for implementing curiosity and boredom in model-building neural controllers. In From animals to animats: Proceedings of the first international conference on simulation of adaptive behavior, 1991.</p>
<p>Schmidhuber, Jürgen. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE Transactions on Autonomous Mental Development, 2010.</p>
<p>Shelhamer, Evan, Mahmoudieh, Parsa, Argus, Max, and Darrell, Trevor. Loss is its own reward: Self-supervision for reinforcement learning. arXiv:1612.07307, 2017.</p>
<p>Silvia, Paul J. Curiosity and motivation. In The Oxford Handbook of Human Motivation, 2012.</p>
<p>Singh, Satinder P, Barto, Andrew G, and Chentanez, Nuttapong. Intrinsically motivated reinforcement learning. In NIPS, 2005.</p>
<p>Stadie, Bradly C, Levine, Sergey, and Abbeel, Pieter. Incentivizing exploration in reinforcement learning with deep predictive models. NIPS Workshop, 2015.</p>
<p>Still, Susanne and Precup, Doina. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 2012.</p>
<p>Storck, Jan, Hochreiter, Sepp, and Schmidhuber, Jürgen. Reinforcement driven information acquisition in nondeterministic environments. In ICANN, 1995.</p>
<p>Sukhbaatar, Sainbayar, Kostrikov, Ilya, Szlam, Arthur, and Fergus, Rob. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv:1703.05407, 2017.</p>
<p>Sun, Yi, Gomez, Faustino, and Schmidhuber, Jürgen. Planning to be surprised: Optimal bayesian exploration in dynamic environments. In $A G I, 2011$.</p>
<p>Tang, Haoran, Houthooft, Rein, Foote, Davis, Stooke, Adam, Chen, Xi, Duan, Yan, Schulman, John, De Turck, Filip, and Abbeel, Pieter. # exploration: A study of
count-based exploration for deep reinforcement learning. arXiv:1611.04717, 2016.</p>
<p>Wang, Xiaolong and Gupta, Abhinav. Unsupervised learning of visual representations using videos. In ICCV, 2015.</p>
<p>Wolpert, Daniel M, Ghahramani, Zoubin, and Jordan, Michael I. An internal model for sensorimotor integration. Science-AAAS-Weekly Paper Edition, 1995.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ University of California, Berkeley. Correspondence to: Deepak Pathak $&lt;$ pathak@berkeley.edu $&gt;$.</p>
<p>Proceedings of the $34^{\text {th }}$ International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W\&amp;CP. Copyright 2017 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
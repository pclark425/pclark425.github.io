<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5176 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5176</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5176</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-1754976</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1010.4222v1.pdf" target="_blank">Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System</a></p>
                <p><strong>Paper Abstract:</strong> We describe a mathematical models of grounded symbols in the brain. It also serves as a computational foundations for Perceptual Symbol System (PSS). This development requires new mathematical methods of dynamic logic (DL), which have overcome limitations of classical artificial intelligence and connectionist approaches. The paper discusses these past limitations, relates them to combinatorial complexity (exponential explosion) of algorithms in the past, and further to the static nature of classical logic. The new mathematical theory, DL, is a process-logic. A salient property of this process is evolution of vague representations into crisp. The paper first applies it to one aspect of PSS: situation learning from object perceptions. Then we relate DL to the essential PSS mechanisms of concepts, simulators, grounding, productivity, binding, recursion, and to the mechanisms relating grounded and amodal symbols. We discuss DL as a general theory describing the process of cognition on multiple levels of abstraction. We also discuss the implications of this theory for interactions between cognition and language, mechanisms of language grounding, and possible role of language in grounding abstract cognition. The developed theory makes experimental predictions, and will impact future theoretical developments in cognitive science, including knowledge representation, and perception-cognition interaction. Experimental neuroimaging evidence for DL and PSS in brain imaging is discussed as well as future research directions.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5176.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5176.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perceptual Symbol System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theory (Barsalou) proposing that conceptual knowledge is grounded in modal/perceptual representations (simulations) reactivated from sensorimotor systems, rather than stored as abstract amodal symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Perceptual Symbol Systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Perceptual Symbol System (PSS)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are represented functionally as multimodal, distributed simulators that reenact portions of prior perceptual, motor, and introspective states; conceptual processing operates by partial re‑enactment (simulation) of these perceptual traces rather than by manipulation of detached amodal tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>perceptual / multimodal distributed simulators (process-based representations)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>grounded in perception and action; multimodal/distributed; dynamic (simulation); supports type-token interpretation, compositionality via assemblage of lower-level perceptual elements, productivity and recursion via simulator composition; often unconscious.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Behavioral and neuroimaging literature cited (Barsalou and others) showing modality-specific reactivation during conceptual tasks; within this paper, Bar et al. (2006) fMRI/MEG evidence of early top-down (vague) activations facilitating object recognition is invoked as consistent with PSS simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>PSS claim that high-level abstract concepts remain grounded by cascading simulations is questioned: statistical sufficiency for grounding at higher, very abstract levels is doubtful; the paper notes that abstract concepts may require additional grounding mechanisms (e.g., language), which remains to be empirically established.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>categorization, concept learning, situation recognition, imagery, simulation-based reasoning, language grounding, planning and imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted explicitly with amodal symbolic / classical logic approaches: PSS emphasizes modality, dynamics, and simulation, avoiding combinatorial complexity that arises for static amodal-symbolic accounts; compared to connectionist approaches, PSS emphasizes structured simulators and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Simulators that generate top-down priming signals to reinstantiate distributed perceptual patterns; matching of top-down simulators with bottom-up input via an interpretive process (here formalized by Dynamic Logic); assembler processes that create higher-level types (situations) from lower-level tokens (objects/features); simulation supports inference and mental imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How high-level abstract concepts become reliably grounded remains open; the degree and mechanisms by which language supplements or scaffolds grounding need direct empirical testing; precise neural implementation and the availability of simulator content to consciousness require further empirical specification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5176.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5176.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Logic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mathematical/process-level formalism (Perlovsky) that models cognition as an evolution from vague, distributed representations toward crisp, specific ones via iterative matching between top-down models and bottom-up signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Dynamic Logic (DL)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>A process model in which concept-models (simulators) start as vague probabilistic/top‑down expectations and iteratively refine parameters and associations with bottom-up data by maximizing a similarity (likelihood-like) measure; representations are dynamic trajectories (vague→crisp) rather than static tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>dynamic, probabilistic / process-based distributed representations (vague-to-crisp simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>vague initial states; graded association variables that progressively sharpen; iterative top-down/bottom-up interaction; scalable (claims overcoming combinatorial complexity); produces final states that approximate amodal logical symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Computational simulations in the paper: (i) detection of structured patterns ('smile'/'frown') in severe noise, (ii) situation learning simulations (1000-object space) where DL clustered situations rapidly; neuroimaging results (Bar et al. 2006) showing early vague top-down activity are cited as concordant evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>While DL is claimed to avoid combinatorial complexity and is supported by simulations, questions remain about generalization limits (e.g., failure regimes when noise increases), dependence on initializations and priors, and direct neural-level confirmation beyond correlational imaging; some empirical falsifiability conditions are discussed but require systematic psychophysical/neuroimaging testing across modalities and abstraction levels.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>object recognition under noise, situation learning (context learning), perceptual inference, multimodal concept formation, symbolic functions (binding, composition) via dynamic processes, pattern detection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to classical symbolic/logical systems (static, combinatorially exploding) and to fuzzy‑logic (which still requires logical choice of fuzziness), DL is presented as combining structural advantages of logic with connectionist dynamics and as a principled way to avoid combinatorial explosion that hampers other models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Iterative update of association weights f(m|n) (analogous to posterior responsibilities) and model parameters p_m via closed-form update rules/differential equations; maximization of a global similarity (likelihood) without enumerating combinatorial assignments; emergent crisp categorizations when associations converge to near‑0/1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Scalability to all real-world concept domains and higher-level abstractions needs empirical validation; how exactly DL maps to biological mechanisms (beyond abstract top-down/bottom-up mapping) remains an open question; sensitivity to model specification and potential failure modes require formal characterization and experimental tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5176.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5176.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PSS Simulators (conceptual simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Function-level entities in PSS: multimodal processual constructs that reinstantiate fragments of perceptual/motor/introspective states to represent concepts and support simulation-based cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Perceptual Symbol Systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Simulators (PSS simulators)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Simulators are procedural, representation-generating mechanisms that, when activated, recreate patterns of sensorimotor and introspective activations associated with prior experiences, thereby functionally implementing concepts as reenactable perceptual states.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>procedural/perceptual distributed representations (simulations that generate modal activation patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>multimodal; can be partial/vague; compositional (higher-level simulators instantiate collections of lower-level simulators); support mental imagery, prediction, and planning; operate mostly unconsciously until they produce a crisp outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Behavioral evidence for simulation-based effects (e.g., effects of perceptual processing on conceptual tasks) and neuroimaging showing modality-specific reactivation; the paper's DL simulations formalize how multiple simulators interact and converge.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Direct measurement and manipulation of simulator content and boundaries is difficult; how simulators support abstract, non-experiential concepts is unresolved and may rely on language-mediated grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>conceptual combination, situation recognition, mental imagery, prediction, language comprehension via simulation of events, planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Different from amodal-symbol accounts which treat concepts as discrete static tokens; closer to embodied and grounded cognition frameworks; DL provides a computational instantiation of how simulators might operate.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Activation (top-down) of distributed perceptual patterns; competition and cooperation among simulators via graded association variables; consolidation into stable representations when bottom-up and top-down match sufficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Granularity and boundaries of simulators vs. lower-level perceptual features; the role of cultural/linguistic input in forming simulators for abstract concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5176.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5176.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AmodalSymbols</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Amodal Symbolic Representation (classical symbolic approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional cognitive-science account that represents concepts as language-like, modality-neutral tokens (feature lists, semantic networks, frames) manipulated via logical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Amodal symbolic representation / classical symbolic theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is represented as discrete, modality‑independent symbols (e.g., lexical entries, logical predicates, feature lists) manipulated by logical/compositional rules; these are static and detached from sensorimotor systems.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>amodal symbolic / token-like (language-like, propositional representations)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>discrete tokens; compositional and rule-governed; supports explicit logical inference and symbolic manipulation; static (non-processual) representations.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Historically motivated by the convenience of formalization and AI systems; some high-level linguistic and logical capacities appear compatible with amodal tokens; however, empirical brain-level support for amodal-only representations is weak per Barsalou and cited critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Lack of strong empirical evidence that conceptual representations are modality-neutral; combinatorial complexity and formal-logic-based algorithms face tractability problems (curse of dimensionality, Gödel-related issues) as argued in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>symbolic reasoning, formal logic, some rule-based AI systems, classical semantic memory models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with PSS/DL: amodal symbols are static and logically manipulable but suffer computational CC and lack grounding; PSS/DL present grounded, dynamic alternatives that can recover logical-like final states from process dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Manipulation of discrete tokens via rules/inference procedures; less emphasis on perceptual reactivation or graded matching processes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How amodal symbols obtain grounding in perception and action; computational tractability for large-scale real-world concept combination without task-specific rules; reconciling symbolic operations with pervasive unconscious, nonlogical cognitive processing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5176.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5176.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DualModel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Model (language-cognition interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed functional architecture positing that each mental model has two neurally connected parts — a language model and a cognitive model — where language representations guide the ontogeny of cognitive (grounded) representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Dual model (language-guided cognitive model development)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Every conceptual model consists of a linked pair: a language-side representation learned from the linguistic environment (often acquired early and robustly) and a cognitive-side simulator learned from experience; language-side models scaffold and guide the acquisition of cognitive simulators, especially for abstract concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>paired representations: amodal/linguistic + grounded/cognitive simulators (dual-linked format)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>separable but linked language and cognitive parts; language models are strongly culturally grounded and can be acquired with limited direct experience; cognitive models develop more slowly and are experience-grounded; the linkage supports bootstrapping of abstract concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Argumentation in the paper based on developmental observations (children speak about abstract cultural content early but lack corresponding grounded use) and modeling references (Perlovsky and collaborators); motivates testable predictions about neural connectivity and developmental trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Direct empirical evidence for the hypothesized pairing and the specific neural connectivity pattern is not fully established; the relative contribution of language vs. experience across concept types needs systematic empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>language acquisition, abstract concept learning, cultural knowledge transmission, cognitive development.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Differs from pure PSS (which assumes grounding cascades up the hierarchy) by explicitly positing linguistic scaffolding for higher-level cognition; contrasts with purely amodal accounts by maintaining grounded cognitive representations alongside language.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Language acquisition produces language-model representations; these guide the learning/updating of cognitive simulators via neural connections (bootstrapping), enabling acquisition of abstract concepts that lack sufficient direct perceptual statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Empirical tests required: existence of paired language-cognition representations, developmental timing, neural connectivity, and causal role of language in forming cognitive representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5176.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5176.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VagueToCrisp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vague-to-Crisp representational process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A central process-level claim that conceptual representations start as vague, distributed probabilistic associations and become crisp, specific, and (approximately) logical through iterative top‑down/bottom‑up matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Vague-to-Crisp process</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Representations at functional level are initially broad, uncertain distributions over possible constituent elements; perception/cognition iteratively sharpens these distributions (association variables) as models are fit to data, culminating in discrete-like (0/1) associations corresponding to conscious, amodal-like tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>graded probabilistic/distributed representations becoming discrete (processual transition)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>graded associations (f variables); temporally unfolding sharpening; explains how subjective logical certainty emerges from unconscious nonlogical dynamics; reduces combinatorial search by soft assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Computational simulations in paper; cited neuroimaging (Bar et al. 2006) showing early vague top-down activations preceding object recognition interpreted as neural echo of vague-to-crisp dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Direct temporal/causal mapping from graded association variables to behavioral decisions across tasks needs more empirical work; boundary conditions (when sharpening fails or yields incorrect crystallizations) require specification.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>rapid visual recognition, situation comprehension, decision-making under uncertainty, learning of compositional concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Offers a principled alternative to discrete assignment (hard clustering) and to fuzzy logic (which lacks formal dynamic procedure for crystallization); similar in spirit to soft-assignment EM-like methods but emphasizes process-level psychological interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Soft assignment of bottom-up signals to multiple models via normalized association variables; parameter update rules that average inputs weighted by associations; convergence to sharp assignments when models adequately explain data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How the brain implements the required normalization and iterative updates biologically; timing constraints and whether the process generalizes across modalities and high-level abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5176.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5176.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bar2006</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-down facilitation of visual recognition (Bar et al. 2006)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical neuroimaging finding that orbitofrontal cortex (OFC) shows early activation (around 130 ms) that precedes activity in object-recognition regions and facilitates visual recognition via top-down, vague projections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Top-down facilitation of visual recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Top-down facilitation / gist-driven prediction (empirical finding)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Early cortical signals (from OFC) provide coarse/vague predictions ('gist') to visual cortex, which pre-sensitizes representations and speeds recognition; these top-down activations are temporally earlier than some object-selective activations and are functionally facilitatory.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>coarse/vague predictive perceptual representations (top-down priors)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>temporal precedence of top-down vague signals; low-spatial-frequency / coarse content; enhances processing of relevant bottom-up inputs; largely unconscious.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Combined fMRI and MEG evidence in Bar et al. (2006) demonstrating OFC activation ~130 ms after early visual cortex but ~50 ms before fusiform object-recognition activations, consistent with top-down priming of object recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Extent to which similar timing and mechanisms hold for higher-level abstract simulations is unspecified; causal manipulations (e.g., perturbing OFC) to test necessity are more difficult and were not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>rapid object recognition, contextual facilitation, predictive perception research.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Consistent with PSS/DL's claim of top-down simulators being vague early in processing; challenges strictly feedforward models of recognition and supports predictive-coding-like frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Generation of coarse predictions by OFC that bias sensory processing via top-down projections; interaction with bottom-up inputs to yield a sharpened perceptual outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Generalization to non-visual modalities and to cognition above the object level; mechanistic linking to the DL formalism requires further empirical bridging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5176.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5176.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BindingDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binding mechanisms in Dynamic Logic (flat and hierarchical binding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DL-predicted functional binding mechanisms: (i) hierarchical binding where lower-level features are bound by higher-level simulators, and (ii) 'flat' binding where relations are learned as co-equal elements at the same level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>DL binding mechanisms (flat and hierarchical)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Binding of elements into structured concepts can be achieved either by hierarchical simulators (higher-level models that integrate lower-level tokens) or by learning relation-markers that reside at the same level as entities (flat binding); DL formalism supports both mechanisms via probabilistic association learning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>compositional distributed representations with two binding formats: hierarchical (nested simulators) and flat (co‑located relational markers)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>flexible binding strategy selection; learned relations treated as objects in the same representational level (flat) or as higher-level parametric structure (hierarchical); supports compositionality and context-sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Computational arguments and DL simulations demonstrating ability to assemble situations from object components; the paper references related theoretical work (Edelman & Intrator) and suggests experimental tests.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Empirical verification of the two distinct binding mechanisms and their distribution across cognitive domains is outstanding; neurophysiological signatures distinguishing flat vs hierarchical binding are not yet established.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>object-part binding, scene/situation recognition, relational reasoning, language compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Provides an explicit process-account alternative to ad-hoc binding solutions in connectionist models and to symbolic binding via variables/indices; extends PSS by specifying two implementable binding formats.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Association variables assign lower-level tokens to higher-level simulators (hierarchical binding); alternatively, relations are introduced as elements in the same representational vector and acquired via the same DL parameter updates (flat binding).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Which mechanism the brain uses for specific relation types; role of language and culture in shaping flat vs hierarchical binding; empirical discriminants between the two binding implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System', 'publication_date_yy_mm': '2010-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Perceptual Symbol Systems <em>(Rating: 2)</em></li>
                <li>Top-down facilitation of visual recognition <em>(Rating: 2)</em></li>
                <li>Fuzzy Dynamic Logic <em>(Rating: 2)</em></li>
                <li>Cognitively inspired neural network for recognition of situations <em>(Rating: 2)</em></li>
                <li>The proactive brain: using analogies and associations to generate predictions <em>(Rating: 1)</em></li>
                <li>Vague-to-Crisp' Neural Mechanism of Perception <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5176",
    "paper_id": "paper-1754976",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "PSS",
            "name_full": "Perceptual Symbol System",
            "brief_description": "A theory (Barsalou) proposing that conceptual knowledge is grounded in modal/perceptual representations (simulations) reactivated from sensorimotor systems, rather than stored as abstract amodal symbols.",
            "citation_title": "Perceptual Symbol Systems",
            "mention_or_use": "use",
            "theory_or_model_name": "Perceptual Symbol System (PSS)",
            "theory_or_model_description": "Concepts are represented functionally as multimodal, distributed simulators that reenact portions of prior perceptual, motor, and introspective states; conceptual processing operates by partial re‑enactment (simulation) of these perceptual traces rather than by manipulation of detached amodal tokens.",
            "representation_format_type": "perceptual / multimodal distributed simulators (process-based representations)",
            "key_properties": "grounded in perception and action; multimodal/distributed; dynamic (simulation); supports type-token interpretation, compositionality via assemblage of lower-level perceptual elements, productivity and recursion via simulator composition; often unconscious.",
            "empirical_support": "Behavioral and neuroimaging literature cited (Barsalou and others) showing modality-specific reactivation during conceptual tasks; within this paper, Bar et al. (2006) fMRI/MEG evidence of early top-down (vague) activations facilitating object recognition is invoked as consistent with PSS simulators.",
            "empirical_challenges": "PSS claim that high-level abstract concepts remain grounded by cascading simulations is questioned: statistical sufficiency for grounding at higher, very abstract levels is doubtful; the paper notes that abstract concepts may require additional grounding mechanisms (e.g., language), which remains to be empirically established.",
            "applied_domains_or_tasks": "categorization, concept learning, situation recognition, imagery, simulation-based reasoning, language grounding, planning and imagination.",
            "comparison_to_other_models": "Contrasted explicitly with amodal symbolic / classical logic approaches: PSS emphasizes modality, dynamics, and simulation, avoiding combinatorial complexity that arises for static amodal-symbolic accounts; compared to connectionist approaches, PSS emphasizes structured simulators and interpretability.",
            "functional_mechanisms": "Simulators that generate top-down priming signals to reinstantiate distributed perceptual patterns; matching of top-down simulators with bottom-up input via an interpretive process (here formalized by Dynamic Logic); assembler processes that create higher-level types (situations) from lower-level tokens (objects/features); simulation supports inference and mental imagery.",
            "limitations_or_open_questions": "How high-level abstract concepts become reliably grounded remains open; the degree and mechanisms by which language supplements or scaffolds grounding need direct empirical testing; precise neural implementation and the availability of simulator content to consciousness require further empirical specification.",
            "uuid": "e5176.0",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "DL",
            "name_full": "Dynamic Logic",
            "brief_description": "A mathematical/process-level formalism (Perlovsky) that models cognition as an evolution from vague, distributed representations toward crisp, specific ones via iterative matching between top-down models and bottom-up signals.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Dynamic Logic (DL)",
            "theory_or_model_description": "A process model in which concept-models (simulators) start as vague probabilistic/top‑down expectations and iteratively refine parameters and associations with bottom-up data by maximizing a similarity (likelihood-like) measure; representations are dynamic trajectories (vague→crisp) rather than static tokens.",
            "representation_format_type": "dynamic, probabilistic / process-based distributed representations (vague-to-crisp simulators)",
            "key_properties": "vague initial states; graded association variables that progressively sharpen; iterative top-down/bottom-up interaction; scalable (claims overcoming combinatorial complexity); produces final states that approximate amodal logical symbols.",
            "empirical_support": "Computational simulations in the paper: (i) detection of structured patterns ('smile'/'frown') in severe noise, (ii) situation learning simulations (1000-object space) where DL clustered situations rapidly; neuroimaging results (Bar et al. 2006) showing early vague top-down activity are cited as concordant evidence.",
            "empirical_challenges": "While DL is claimed to avoid combinatorial complexity and is supported by simulations, questions remain about generalization limits (e.g., failure regimes when noise increases), dependence on initializations and priors, and direct neural-level confirmation beyond correlational imaging; some empirical falsifiability conditions are discussed but require systematic psychophysical/neuroimaging testing across modalities and abstraction levels.",
            "applied_domains_or_tasks": "object recognition under noise, situation learning (context learning), perceptual inference, multimodal concept formation, symbolic functions (binding, composition) via dynamic processes, pattern detection.",
            "comparison_to_other_models": "Compared to classical symbolic/logical systems (static, combinatorially exploding) and to fuzzy‑logic (which still requires logical choice of fuzziness), DL is presented as combining structural advantages of logic with connectionist dynamics and as a principled way to avoid combinatorial explosion that hampers other models.",
            "functional_mechanisms": "Iterative update of association weights f(m|n) (analogous to posterior responsibilities) and model parameters p_m via closed-form update rules/differential equations; maximization of a global similarity (likelihood) without enumerating combinatorial assignments; emergent crisp categorizations when associations converge to near‑0/1.",
            "limitations_or_open_questions": "Scalability to all real-world concept domains and higher-level abstractions needs empirical validation; how exactly DL maps to biological mechanisms (beyond abstract top-down/bottom-up mapping) remains an open question; sensitivity to model specification and potential failure modes require formal characterization and experimental tests.",
            "uuid": "e5176.1",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "Simulators",
            "name_full": "PSS Simulators (conceptual simulators)",
            "brief_description": "Function-level entities in PSS: multimodal processual constructs that reinstantiate fragments of perceptual/motor/introspective states to represent concepts and support simulation-based cognition.",
            "citation_title": "Perceptual Symbol Systems",
            "mention_or_use": "use",
            "theory_or_model_name": "Simulators (PSS simulators)",
            "theory_or_model_description": "Simulators are procedural, representation-generating mechanisms that, when activated, recreate patterns of sensorimotor and introspective activations associated with prior experiences, thereby functionally implementing concepts as reenactable perceptual states.",
            "representation_format_type": "procedural/perceptual distributed representations (simulations that generate modal activation patterns)",
            "key_properties": "multimodal; can be partial/vague; compositional (higher-level simulators instantiate collections of lower-level simulators); support mental imagery, prediction, and planning; operate mostly unconsciously until they produce a crisp outcome.",
            "empirical_support": "Behavioral evidence for simulation-based effects (e.g., effects of perceptual processing on conceptual tasks) and neuroimaging showing modality-specific reactivation; the paper's DL simulations formalize how multiple simulators interact and converge.",
            "empirical_challenges": "Direct measurement and manipulation of simulator content and boundaries is difficult; how simulators support abstract, non-experiential concepts is unresolved and may rely on language-mediated grounding.",
            "applied_domains_or_tasks": "conceptual combination, situation recognition, mental imagery, prediction, language comprehension via simulation of events, planning.",
            "comparison_to_other_models": "Different from amodal-symbol accounts which treat concepts as discrete static tokens; closer to embodied and grounded cognition frameworks; DL provides a computational instantiation of how simulators might operate.",
            "functional_mechanisms": "Activation (top-down) of distributed perceptual patterns; competition and cooperation among simulators via graded association variables; consolidation into stable representations when bottom-up and top-down match sufficiently.",
            "limitations_or_open_questions": "Granularity and boundaries of simulators vs. lower-level perceptual features; the role of cultural/linguistic input in forming simulators for abstract concepts.",
            "uuid": "e5176.2",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "AmodalSymbols",
            "name_full": "Amodal Symbolic Representation (classical symbolic approach)",
            "brief_description": "A traditional cognitive-science account that represents concepts as language-like, modality-neutral tokens (feature lists, semantic networks, frames) manipulated via logical rules.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Amodal symbolic representation / classical symbolic theory",
            "theory_or_model_description": "Conceptual knowledge is represented as discrete, modality‑independent symbols (e.g., lexical entries, logical predicates, feature lists) manipulated by logical/compositional rules; these are static and detached from sensorimotor systems.",
            "representation_format_type": "amodal symbolic / token-like (language-like, propositional representations)",
            "key_properties": "discrete tokens; compositional and rule-governed; supports explicit logical inference and symbolic manipulation; static (non-processual) representations.",
            "empirical_support": "Historically motivated by the convenience of formalization and AI systems; some high-level linguistic and logical capacities appear compatible with amodal tokens; however, empirical brain-level support for amodal-only representations is weak per Barsalou and cited critiques.",
            "empirical_challenges": "Lack of strong empirical evidence that conceptual representations are modality-neutral; combinatorial complexity and formal-logic-based algorithms face tractability problems (curse of dimensionality, Gödel-related issues) as argued in the paper.",
            "applied_domains_or_tasks": "symbolic reasoning, formal logic, some rule-based AI systems, classical semantic memory models.",
            "comparison_to_other_models": "Contrasted with PSS/DL: amodal symbols are static and logically manipulable but suffer computational CC and lack grounding; PSS/DL present grounded, dynamic alternatives that can recover logical-like final states from process dynamics.",
            "functional_mechanisms": "Manipulation of discrete tokens via rules/inference procedures; less emphasis on perceptual reactivation or graded matching processes.",
            "limitations_or_open_questions": "How amodal symbols obtain grounding in perception and action; computational tractability for large-scale real-world concept combination without task-specific rules; reconciling symbolic operations with pervasive unconscious, nonlogical cognitive processing.",
            "uuid": "e5176.3",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "DualModel",
            "name_full": "Dual Model (language-cognition interaction)",
            "brief_description": "A proposed functional architecture positing that each mental model has two neurally connected parts — a language model and a cognitive model — where language representations guide the ontogeny of cognitive (grounded) representations.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Dual model (language-guided cognitive model development)",
            "theory_or_model_description": "Every conceptual model consists of a linked pair: a language-side representation learned from the linguistic environment (often acquired early and robustly) and a cognitive-side simulator learned from experience; language-side models scaffold and guide the acquisition of cognitive simulators, especially for abstract concepts.",
            "representation_format_type": "paired representations: amodal/linguistic + grounded/cognitive simulators (dual-linked format)",
            "key_properties": "separable but linked language and cognitive parts; language models are strongly culturally grounded and can be acquired with limited direct experience; cognitive models develop more slowly and are experience-grounded; the linkage supports bootstrapping of abstract concepts.",
            "empirical_support": "Argumentation in the paper based on developmental observations (children speak about abstract cultural content early but lack corresponding grounded use) and modeling references (Perlovsky and collaborators); motivates testable predictions about neural connectivity and developmental trajectories.",
            "empirical_challenges": "Direct empirical evidence for the hypothesized pairing and the specific neural connectivity pattern is not fully established; the relative contribution of language vs. experience across concept types needs systematic empirical validation.",
            "applied_domains_or_tasks": "language acquisition, abstract concept learning, cultural knowledge transmission, cognitive development.",
            "comparison_to_other_models": "Differs from pure PSS (which assumes grounding cascades up the hierarchy) by explicitly positing linguistic scaffolding for higher-level cognition; contrasts with purely amodal accounts by maintaining grounded cognitive representations alongside language.",
            "functional_mechanisms": "Language acquisition produces language-model representations; these guide the learning/updating of cognitive simulators via neural connections (bootstrapping), enabling acquisition of abstract concepts that lack sufficient direct perceptual statistics.",
            "limitations_or_open_questions": "Empirical tests required: existence of paired language-cognition representations, developmental timing, neural connectivity, and causal role of language in forming cognitive representations.",
            "uuid": "e5176.4",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "VagueToCrisp",
            "name_full": "Vague-to-Crisp representational process",
            "brief_description": "A central process-level claim that conceptual representations start as vague, distributed probabilistic associations and become crisp, specific, and (approximately) logical through iterative top‑down/bottom‑up matching.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Vague-to-Crisp process",
            "theory_or_model_description": "Representations at functional level are initially broad, uncertain distributions over possible constituent elements; perception/cognition iteratively sharpens these distributions (association variables) as models are fit to data, culminating in discrete-like (0/1) associations corresponding to conscious, amodal-like tokens.",
            "representation_format_type": "graded probabilistic/distributed representations becoming discrete (processual transition)",
            "key_properties": "graded associations (f variables); temporally unfolding sharpening; explains how subjective logical certainty emerges from unconscious nonlogical dynamics; reduces combinatorial search by soft assignments.",
            "empirical_support": "Computational simulations in paper; cited neuroimaging (Bar et al. 2006) showing early vague top-down activations preceding object recognition interpreted as neural echo of vague-to-crisp dynamics.",
            "empirical_challenges": "Direct temporal/causal mapping from graded association variables to behavioral decisions across tasks needs more empirical work; boundary conditions (when sharpening fails or yields incorrect crystallizations) require specification.",
            "applied_domains_or_tasks": "rapid visual recognition, situation comprehension, decision-making under uncertainty, learning of compositional concepts.",
            "comparison_to_other_models": "Offers a principled alternative to discrete assignment (hard clustering) and to fuzzy logic (which lacks formal dynamic procedure for crystallization); similar in spirit to soft-assignment EM-like methods but emphasizes process-level psychological interpretation.",
            "functional_mechanisms": "Soft assignment of bottom-up signals to multiple models via normalized association variables; parameter update rules that average inputs weighted by associations; convergence to sharp assignments when models adequately explain data.",
            "limitations_or_open_questions": "How the brain implements the required normalization and iterative updates biologically; timing constraints and whether the process generalizes across modalities and high-level abstractions.",
            "uuid": "e5176.5",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "Bar2006",
            "name_full": "Top-down facilitation of visual recognition (Bar et al. 2006)",
            "brief_description": "An empirical neuroimaging finding that orbitofrontal cortex (OFC) shows early activation (around 130 ms) that precedes activity in object-recognition regions and facilitates visual recognition via top-down, vague projections.",
            "citation_title": "Top-down facilitation of visual recognition",
            "mention_or_use": "use",
            "theory_or_model_name": "Top-down facilitation / gist-driven prediction (empirical finding)",
            "theory_or_model_description": "Early cortical signals (from OFC) provide coarse/vague predictions ('gist') to visual cortex, which pre-sensitizes representations and speeds recognition; these top-down activations are temporally earlier than some object-selective activations and are functionally facilitatory.",
            "representation_format_type": "coarse/vague predictive perceptual representations (top-down priors)",
            "key_properties": "temporal precedence of top-down vague signals; low-spatial-frequency / coarse content; enhances processing of relevant bottom-up inputs; largely unconscious.",
            "empirical_support": "Combined fMRI and MEG evidence in Bar et al. (2006) demonstrating OFC activation ~130 ms after early visual cortex but ~50 ms before fusiform object-recognition activations, consistent with top-down priming of object recognition.",
            "empirical_challenges": "Extent to which similar timing and mechanisms hold for higher-level abstract simulations is unspecified; causal manipulations (e.g., perturbing OFC) to test necessity are more difficult and were not reported here.",
            "applied_domains_or_tasks": "rapid object recognition, contextual facilitation, predictive perception research.",
            "comparison_to_other_models": "Consistent with PSS/DL's claim of top-down simulators being vague early in processing; challenges strictly feedforward models of recognition and supports predictive-coding-like frameworks.",
            "functional_mechanisms": "Generation of coarse predictions by OFC that bias sensory processing via top-down projections; interaction with bottom-up inputs to yield a sharpened perceptual outcome.",
            "limitations_or_open_questions": "Generalization to non-visual modalities and to cognition above the object level; mechanistic linking to the DL formalism requires further empirical bridging.",
            "uuid": "e5176.6",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        },
        {
            "name_short": "BindingDL",
            "name_full": "Binding mechanisms in Dynamic Logic (flat and hierarchical binding)",
            "brief_description": "DL-predicted functional binding mechanisms: (i) hierarchical binding where lower-level features are bound by higher-level simulators, and (ii) 'flat' binding where relations are learned as co-equal elements at the same level.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "DL binding mechanisms (flat and hierarchical)",
            "theory_or_model_description": "Binding of elements into structured concepts can be achieved either by hierarchical simulators (higher-level models that integrate lower-level tokens) or by learning relation-markers that reside at the same level as entities (flat binding); DL formalism supports both mechanisms via probabilistic association learning.",
            "representation_format_type": "compositional distributed representations with two binding formats: hierarchical (nested simulators) and flat (co‑located relational markers)",
            "key_properties": "flexible binding strategy selection; learned relations treated as objects in the same representational level (flat) or as higher-level parametric structure (hierarchical); supports compositionality and context-sensitivity.",
            "empirical_support": "Computational arguments and DL simulations demonstrating ability to assemble situations from object components; the paper references related theoretical work (Edelman & Intrator) and suggests experimental tests.",
            "empirical_challenges": "Empirical verification of the two distinct binding mechanisms and their distribution across cognitive domains is outstanding; neurophysiological signatures distinguishing flat vs hierarchical binding are not yet established.",
            "applied_domains_or_tasks": "object-part binding, scene/situation recognition, relational reasoning, language compositionality.",
            "comparison_to_other_models": "Provides an explicit process-account alternative to ad-hoc binding solutions in connectionist models and to symbolic binding via variables/indices; extends PSS by specifying two implementable binding formats.",
            "functional_mechanisms": "Association variables assign lower-level tokens to higher-level simulators (hierarchical binding); alternatively, relations are introduced as elements in the same representational vector and acquired via the same DL parameter updates (flat binding).",
            "limitations_or_open_questions": "Which mechanism the brain uses for specific relation types; role of language and culture in shaping flat vs hierarchical binding; empirical discriminants between the two binding implementations.",
            "uuid": "e5176.7",
            "source_info": {
                "paper_title": "Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System",
                "publication_date_yy_mm": "2010-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Perceptual Symbol Systems",
            "rating": 2,
            "sanitized_title": "perceptual_symbol_systems"
        },
        {
            "paper_title": "Top-down facilitation of visual recognition",
            "rating": 2,
            "sanitized_title": "topdown_facilitation_of_visual_recognition"
        },
        {
            "paper_title": "Fuzzy Dynamic Logic",
            "rating": 2,
            "sanitized_title": "fuzzy_dynamic_logic"
        },
        {
            "paper_title": "Cognitively inspired neural network for recognition of situations",
            "rating": 2,
            "sanitized_title": "cognitively_inspired_neural_network_for_recognition_of_situations"
        },
        {
            "paper_title": "The proactive brain: using analogies and associations to generate predictions",
            "rating": 1,
            "sanitized_title": "the_proactive_brain_using_analogies_and_associations_to_generate_predictions"
        },
        {
            "paper_title": "Vague-to-Crisp' Neural Mechanism of Perception",
            "rating": 1,
            "sanitized_title": "vaguetocrisp_neural_mechanism_of_perception"
        }
    ],
    "cost": 0.019835,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System</p>
<p>Leonid Perlovsky leonid@seas.harvard.edu 
Leonid Perlovsky
Harvard University</p>
<p>Roman Ilin roman.ilin@hanscom.af.mil. 
Leonid Perlovsky
Harvard University</p>
<p>Grounded Symbols in the Brain Computational Foundations for Perceptual Symbol System
3C435618FF653212B79D97AF15F104BFperceptual symbol systemcognitionperceptionknowledge representationgrounded symbolslanguagedynamic logic
We describe a mathematical models of grounded symbols in the brain.It also serves as a computational foundations for Perceptual Symbol System (PSS).This development requires new mathematical methods of dynamic logic (DL), which have overcome limitations of classical artificial intelligence and connectionist approaches.The paper discusses these past limitations, relates them to combinatorial complexity (exponential explosion) of algorithms in the past, and further to the static nature of classical logic.The new mathematical theory, DL, is a process-logic.A salient property of this process is evolution of vague representations into crisp.The paper first applies it to one aspect of PSS: situation learning from object perceptions.Then we relate DL to the essential PSS mechanisms of concepts, simulators, grounding, productivity, binding, recursion, and to the mechanisms relating grounded and amodal symbols.We discuss DL as a general theory describing the process of cognition on multiple levels of abstraction.We also discuss the implications of this theory for interactions between cognition and language, mechanisms of language grounding, and possible role of language in grounding abstract cognition.The developed theory makes experimental predictions, and will impact future theoretical developments in cognitive science, including knowledge representation, and perception-cognition interaction.Experimental neuroimaging evidence for DL and PSS in brain imaging is discussed as well as future research directions.</p>
<p>Introduction. PSS, challenge of computational model</p>
<p>Perceptual symbol system (PSS) grounds cognition in perception (Barsalou 1999)."Grounded cognition… rejects the standard view that amodal symbols represent knowledge in semantic memory" (Barsalou 1999).PSS emphasized the roles of simulation in cognition."Simulation is the reenactment of perceptual, motor, and introspective states acquired during experience with the world, body, and mind… when knowledge is needed to represent a category (e.g., chair), multimodal representations captured during experiences… are reactivated to simulate how the brain represented perception, action, and introspection associated with it."Simulation is an essential computational mechanism in the brain.The best known case of these simulation mechanisms is mental imagery (e.g., Kosslyn 1980;1994).According to PSS cognition supports action.Simulation is a central mechanism of PSS, yet rarely, if ever, they recreate full experiences.Using the mechanism of simulators, which approximately correspond to concepts and types in amodal theories, PSS implements the standard symbolic functions of type-token binding, inference, productivity, recursion, and propositions.Using these mechanisms PSS retains the symbolic functionality."Thus, PSS is a synthetic approach that integrates traditional theories with grounded theories."(Barsalou 1999;2005;2007).</p>
<p>According to Barsalou, during the Cognitive Revolution in the middle of the last century, cognitive scientists were inspired by new forms of representation "based on developments in logic, linguistics, statistics, and computer science."They adopted amodal representations, such as feature lists, semantic networks, and frames (Barsalou &amp; Hale 1993).Little empirical evidence supports amodal symbolic mechanisms (Barsalou 1999).It seems that amodal symbols were adopted largely because they promised to provide "elegant and powerful formalisms for representing knowledge, because they captured important intuitions about the symbolic character of cognition, and because they could be implemented in artificial intelligence."As we discuss in the next section, these promises were unfulfilled due to fundamental mathematical difficulties.</p>
<p>There is a number of past and ongoing developments of computational implementations of PSS (Cangelosi et al. 2000;Cangelosi &amp; Riga 2006) and references therein.Yet, computational models for PSS (Barsalou 1999;2007) require new mathematical methods different from traditional artificial intelligence, pattern recognition, or connectionist methods.The reason is that the traditional methods encountered combinatorial complexity (CC), an irresolvable computational difficulty, when attempting to model complex systems.Cognitive modeling requires learning combinations of perceptual features and objects or events (Perlovsky 1994;1997;2001;2002a,b;2006b;2007a).</p>
<p>The aim of this article is to develop a realistic and scalable mathematical model of grounded symbols and formalization of PSS based on a new computational technique of dynamic logic, DL (Perlovsky 2006a,b).Although the developed mathematical formalism is quite general, here we first concentrate on just one example of PSS mechanism: a mathematical description of models and simulators for forming and enacting representations of situations (higher level symbols) from perceptions of objects (lower level symbols), and then we discuss its general applicability.In addition to simulators, we consider concepts, grounding, binding, dynamic aspect of PSS (DIPSS), abstract concepts, the mechanism of amodal symbols within PSS, and the role of logic.</p>
<p>Section 2 considers past mathematical difficulties, relates it to classical logic, and introduces a new computational technique of dynamic logic (DL), which overcomes past computational limitations.Whereas classical logic is a static logic of statements (e.g., "if A then B"), DL describes a process capable of modeling the main components of PSS, including simulators.Section 3 illustrates the important properties of DL.Section 4 illustrates how DL models essential mechanisms of PSS considering an example of learning situations from objects (a difficult problem due to its inherent combinatorial complexity).Section 5 discusses DL as a general mechanism of interacting bottom-up and top-down signals, applicable to all levels of cognitive processing., We discuss the representation of abstract concepts, the role of language, and how it is relevant to modeling PSS.Section 6 continues this discussion concentrating specifically on DL modeling amodal vs. perceptual symbols.Section 7 discusses experimental evidence confirming DL predictions of the mind mechanism, and formulates further predictions that could be tested experimentally in the near future.Section 8 describes future theoretical research as well as proposed verifiable experimental predictions of DL.</p>
<p>Overcoming past mathematical difficulties</p>
<p>According to modern neuroscience, object perception involves bottom-up signals from sensory organs and top-down signals from internal mind's representations (memories) of objects.During perception, the mind matches subsets of bottom-up signals corresponding to objects with representations of object in the mind (and top-down signals).This produces object recognition; it activates brain signals leading to mental and behavioral responses (Grossberg 1982;Kosslyn 1994;Bar et al 2006;Schacter &amp; Addis 2007).This section briefly summarizes mathematical development in artificial intelligence, pattern recognition, and other computational methods used in cognitive science for modeling brain-mind processes.We discuss the fundamental difficulties preventing mathematical modeling of perception, cognition, PSS, and the role of DL in overcoming these difficulties.</p>
<p>Computational complexity since the 1950s</p>
<p>Developing mathematical descriptions of the very first recognition step in this seemingly simple association-recognition-understanding process has not been easy, a number of difficulties have been encountered during the past fifty years.These difficulties were summarized under the notion of combinatorial complexity (CC) (Perlovsky 1998b).CC refers to multiple combinations of bottom-up and top-down signals, or more generally to combinations of various elements in a complex system; for example, recognition of a scene often requires concurrent recognition of its multiple elements that could be encountered in various combinations.CC is computationally prohibitive because the number of combinations is very large: for example, consider 100 elements (not too large a number); the number of combinations of 100 elements is 100 100 , exceeding the number of all elementary particle events in life of the Universe; no computer would ever be able to compute that many combinations.Although, the story might sound "old," we concentrate here on those aspects of mathematical modeling of the brain-mind, which remain current and affect thinking in computational modeling and in cognitive science of many scientists today.</p>
<p>The problem of CC was first identified in pattern recognition and classification research in the 1960s and was named "the curse of dimensionality" (Bellman 1961).It seemed that adaptive self-learning algorithms and neural networks could learn solutions to any problem 'on their own', if provided with a sufficient number of training examples.The following decades of developing adaptive statistical pattern recognition and neural network algorithms led to a conclusion that the required number of training examples often was combinatorially large.This remains true about recent generation of algorithms and neural networks, which are much more powerful than those in the 1950s and 60s.Training had to include not only every object in its multiple variations, angles, etc., but also combinations of objects.Thus, self-learning approaches encountered CC of learning requirements.</p>
<p>Rule systems were proposed in the 1970's to solve the problem of learning complexity (Minsky 1975;Winston 1984).Minsky suggested that learning was a premature step in artificial intelligence; Newton "learned" Newtonian laws, most of scientists read them in the books.Therefore Minsky has suggested, knowledge ought to be input in computers "ready made" for all situations and artificial intelligence would apply these known rules.Rules would capture the required knowledge and eliminate a need for learning.Chomsky's original ideas concerning mechanisms of language grammar related to deep structure (Chomsky 1972) were also based on logical rules.Rule systems work well when all aspects of the problem can be predetermined.However in the presence of variability, the number of rules grew; rules became contingent on other rules and combinations of rules had to be considered.The rule systems encountered CC of rules.</p>
<p>In the 1980s, model systems were proposed to combine advantages of learning and rules-models by using adaptive models (Nevatia &amp; Binford 1977;Bonnisone et al. 1991;Perlovsky 1987;1988;1991;1994). Existing knowledge was to be encapsulated in models and unknown aspects of concrete situations were to be described by adaptive parameters.Along similar lines went the principles and parameters idea of Chomsky (1981).Fitting models to data (top-down to bottomup signals) required selecting data subsets corresponding to various models.The number of subsets, however, is combinatorially large.A general popular algorithm for fitting models to the data, multiple hypothesis testing (Singer, Sea, &amp; Housewright 1974) is known to face CC of computations.Model-based approaches encountered computational CC (N and NP complete algorithms).</p>
<p>Logic, CC, and amodal symbols.</p>
<p>Amodal symbols and perceptual symbols described by PSS differ not only in their representations in the brain, but also in their properties that are mathematically modeled in this paper.This mathematically fundamental difference and its relations to CC of matching bottomup and top-down signals are the subjects of this section.</p>
<p>The fundamental reasons for CC are related to the use of formal logic by algorithms and neural networks (Perlovsky 1996a;2001;Marchal 2005).Logic serves as a foundation for many approaches to cognition and linguistics; it underlies most of computational algorithms.But its influence extends far beyond, affecting cognitive scientists, psychologists, and linguists, who do not use complex mathematical algorithms for modeling the mind.All of us operate under the influence of formal logic that is more than 2000 years old, making a more or less conscious assumption that the mechanisms of logic serve as the basis of our cognition.As discussed in section 7, our minds are unconscious about its illogical foundations.We are mostly conscious about a small part of the mind mechanisms, which is approximately logical.Our intuitions, therefore, are unconsciously affected by the bias toward logic.Even when the laboratory data drive our thinking away from logical mechanisms we are having difficulties overcoming the logical bias (Grossberg 1988;Bar et al 2006;Perlovsky 1997Perlovsky , 2000Perlovsky , 2001Perlovsky , 2004Perlovsky , 2006bPerlovsky , 2007cPerlovsky , 2009cPerlovsky , 2010b,c),c).</p>
<p>The relationships between logic, cognition, and language have been a source of longstanding controversy.The widely accepted story is that Aristotle founded logic as a fundamental mind mechanism, and only during the recent decades science overcame this influence.I would like to emphasize the opposite side of this story.Aristotle assumed a close relationship between logic and language.He emphasized that logical statements should not be formulated too strictly and language inherently contains the necessary degree of precision.According to Aristotle, logic serves to communicate already made decisions (Perlovsky 2007).The mechanism of the mind relating language, cognition, and the world Aristotle described as forms.Today we call similar mechanisms mental representations, or concepts, or simulators in the mind.Aristotelian forms are similar to Plato's ideas with a marked distinction, forms are dynamic: their initial states, before learning, are different from their final states of concepts (1995 / IV BCE).Aristotle emphasized that initial states of forms, forms-as-potentialities, are not logical (i.e.vague), but their final forms, forms-as-actualities, attained in the result of learning, are logical.This fundamental idea was lost during millennia of philosophical arguments.As discussed below this Aristotelian process of dynamic forms corresponds to Barsalou idea of PSS simulators, and in this paper we describe the mathematical model, DL, for this process.</p>
<p>The founders of formal logic emphasized a contradiction between logic and language.In the 19 th century George Boole and the great logicians following him, including Gottlob Frege, Georg Cantor, David Hilbert, and Bertrand Russell (see Davis 2000, and references therein) eliminated the uncertainty of language from mathematics, and founded formal mathematical logic, the foundation of the current classical logic.Hilbert developed an approach named formalism, which rejected intuition as a matter of scientific investigation and formally defined scientific objects in terms of axioms or rules.In 1900 he formulated famous Entscheidungsproblem: to define a set of logical rules sufficient to prove all past and future mathematical theorems.This was a part of "Hilbert's program," which entailed formalization of the entire human thinking and language.Formal logic ignored the dynamic nature of Aristotelian forms and rejected the uncertainty of language.Hilbert was sure that his logical theory described mechanisms of the mind."The fundamental idea of my proof theory is none other than to describe the activity of our understanding, to make a protocol of the rules according to which our thinking actually proceeds."(see Hilbert 1928).However, Hilbert's vision of formalism explaining mysteries of the human mind came to an end in the 1930s, when Gödel (1932Gödel ( /1994) ) proved internal inconsistency of formal logic.This development called Gödel theory is considered among most fundamental mathematical results of the previous century.Logic, that was believed to be a sure way to derive truths, turned out to be basically flawed.This is a reason why theories of cognition and language based on formal logic are inherently flawed.</p>
<p>There is a close relation between logic and CC.It turned out that combinatorial complexity of algorithms is a finite-system manifestation of the Gödel's theory (Perlovsky 1996a).If Gödelian theory is applied to finite systems (all practically used or discussed systems, such as computers and brain-mind, are finite), CC is the result, instead of the fundamental inconsistency.Multivalued logic and fuzzy logic were proposed to overcome limitations related to logic (Zadeh 1965;Kecman 2001).Yet the mathematics of multivalued logic is no different in principle from formal logic (Perlovsky 2006b).Fuzzy logic uses logic to set a degree of fuzziness.</p>
<p>Correspondingly, it encounters a difficulty related to the degree of fuzziness: if too much fuzziness is specified, the solution does not achieve a needed accuracy, and if too little, it becomes similar to formal logic.If logic is used to find the appropriate fuzziness for every model at every processing step, then the result is CC.The mind has to make concrete decisions, for example one either enters a room or does not; this requires a computational procedure to move from a fuzzy state to a concrete one.But fuzzy logic does not have a formal procedure for this purpose; fuzzy systems treat this decision on an ad-hoc logical basis.</p>
<p>Is logic still possible after Gödel's proof of its incompleteness?The contemporary state of this field was reviewed in (Marchal 2005).It appears that logic after Gödel is much more complicated and much less logical than was assumed by founders of artificial intelligence.CC cannot be solved within logic.Penrose thought that Gödel's results entail incomputability of the mind processes and testify for a need for new physics "correct quantum gravitation," which would resolve difficulties in logic and physics (Penrose 1994).An opposite position in (Perlovsky 2001;2006a;b;c) is that incomputability of logic does not entail incomputability of the mind.These publications add mathematical arguments to Aristotelian view that logic is not the basic mechanism of the mind.</p>
<p>To summarize, various manifestations of CC are all related to formal logic and Gödel theory.</p>
<p>Rule systems rely on formal logic in a most direct way.Even mathematical approaches specifically designed to counter limitations of logic, such as fuzzy logic and the second wave of neural networks (developed after the 1980s) rely on logic at some algorithmic steps.Selflearning algorithms and neural networks rely on logic in their training or learning procedures: every training example is treated as a separate logical statement.Fuzzy logic systems rely on logic for setting degrees of fuzziness.CC of mathematical approaches to the mind is related to the fundamental inconsistency of logic.Therefore logical inspirations, leading early cognitive scientists to amodal brain mechanisms, could not realize their hopes for mathematical models of the brain-mind.</p>
<p>Why did the outstanding mathematicians of the 19 th and early 20 th c. believed in logic to be the foundation of the mind?Even more surprising is the belief in logic after Gödel.Gödelian theory was long recognized among most fundamental mathematical results of the 20 th c.How is it possible that outstanding minds, including founders of artificial intelligence, and many cognitive scientists and philosophers of mind insisted that logic and amodal symbols implementing logic in the mind are adequate and sufficient?The answer, in our opinion, might be in the "conscious bias."As we discuss in section 7, non-logical operations making up more than 99.9% of the mind functioning are not accessible to consciousness (Bar et al 2006).However, our consciousness functions in a way that makes us unaware of this.In subjective consciousness we usually experience perception and cognition as logical.Our intuitions are "consciously biased."This is why amodal logical symbols, which describe a tiny fraction of the mind mechanisms, have seemed to many the foundation of the mind (Grossberg 1988;Bar et al 2006;Perlovsky 1997Perlovsky , 2000Perlovsky , 2001Perlovsky , 2002bPerlovsky , 2004Perlovsky , 2006aPerlovsky ,b, 2007cPerlovsky , 2009cPerlovsky , 2010b,c),c).</p>
<p>Another aspect of logic relevant to PSS is that it lacks dynamics; it is about static statements such as "this is a chair."Classical logic is good at modeling structured statements and relations, yet it misses the dynamics of the mind and faces CC, when attempts to match bottom-up and topdown signals.The essentially dynamic nature of the mind is not represented in mathematical foundations of logic.Dynamic logic discussed in the next section is a logic-process.It overcomes CC by automatically choosing the appropriate degree of fuzziness-vagueness for every mind's concept at every moment.DL combines advantages of logical structure and connectionist dynamics.This dynamics mathematically represents the learning process of Aristotelian forms (which are different from classical logic as discussed in this section) and serves as a foundation for PSS concepts and simulators.</p>
<p>Dynamic logic-process</p>
<p>DL models perception as an interaction between bottom-up and top-down signals (Perlovsky 2001;2006a,b;2007c;2009c;2010c).This section concentrates on the basic relationship between the brain processes and the mathematics of DL.To concentrate on this relationship, we much simplify the discussion of the brain structures.We discuss visual recognition of objects as if the retina and the visual cortex each consists of a single processing layer of neurons where recognition occurs (which is not true, detailed relationship of the DL process to brain is considered in given references).Perception consists of the association-matching of bottom-up and top-down signals.Sources of top-down signals are mental representations, memories of objects created by previous simulators (Barsalou 1999); these representations model the patterns of bottom-up signals.In this way they are concepts (of objects), symbols of a higher order than bottom-up signals; we call them concepts or models.In perception processes the models are modified by learning and new models are formed; since an object is never encountered exactly the same as previously, perception is always a learning process.The DL processes along with concept-representations are mathematical models of the PSS simulators.Maximization of a similarity measure is a mathematical model of an unconditional drive to improve the correspondence between bottom-up and top-down signals (representations-models).</p>
<p>In biology and psychology it was discussed as curiosity, cognitive dissonance, or a need for knowledge since the 1950s (Harlow 1950;Festinger 1957;Cacioppo &amp; Petty 1982).This process involves knowledge-related emotions evaluating satisfaction of this drive for knowledge (Grossberg &amp; Levine 1987;Perlovsky 2001Perlovsky , 2000cPerlovsky , 2006a,b;,b;2007c;Perlovsky, Bonniot-Cabanac, &amp; Cabanac 2010).In computational intelligence it is even more ubiquitous, every mathematical learning procedure, algorithm, or neural network maximizes some similarity measure.In the process of learning, mental concept-models are constantly modified.From time to time a system forms a new concept, while retaining an old one as well; alternatively, old concepts are sometimes merged or discarded.</p>
<p>The DL learning process, let us repeat, consists in estimating parameters of concept-models (mental representations) and associating subsets of bottom-up signals with top-down signals originating from these models-concepts by maximizing a similarity.Although a similarity contains combinatorially many items, DL maximizes it without combinatorial complexity (Perlovsky 1996b;1997;2001;2006a,b;2007c;2010c) as follows.First, vague-fuzzy association variables are defined, which give a measure of correspondence between each signal and each model.They are defined similarly to the a posteriori Bayes probabilities, they range between 0 and 1, and as a result of learning they converge to the probabilities, under certain conditions.Often the association variables are close to bell-shapes.</p>
<p>The DL process is defined by a set of differential equations given in the above references; together with models discussed later it gives a mathematical description of the PSS simulators.</p>
<p>To keep the paper self-consistent we summarize these equations in Appendix 1.Those interested in mathematical details can read the Appendix.However, basic principles of DL can be adequately understood from a conceptual description and examples in this and following sections.As a mathematical model of perception-cognitive processes, DL is a process described by differential equations given in the Appendix; in particular, fuzzy association variables f associate bottom-up signals and top-down models-representations.Among unique DL properties is an autonomous dependence of association variables on models-representations: in the processes of perception and cognition, as models improve and become similar to patterns in the bottom-up signals, the association variables become more selective, more similar to deltafunctions.Whereas initial association variables are vague and associate near all bottom-up signals with virtually any top-down model-representations, in the processes of perception and cognition association variables are becoming specific, "crisp", and associate only appropriate signals.This we call a process "from vague to crisp."(The exact mathematical definition of crisp corresponds to values of f = 0 or 1; values of f in between 0 and 1 correspond to various degrees of vagueness.)</p>
<p>DL processes mathematically model PSS simulators and not static amodal signals.Another unique aspect of DL is that it explains how logic appears in the human mind; how illogical dynamic PSS simulators give rise of classical logic, and what is the role of amodal symbols.This is discussed throughout the paper, and also in specific details in section 6.</p>
<p>An essential aspect of DL, mentioned above, is that associations between models and data (topdown and bottom-up signals) are uncertain and dynamic; their uncertainty matches uncertainty of parameters of the models and both change in time during perception and cognition processes.As the model parameters improve, the associations become crisp.In this way the DL model of simulator-processes avoids combinatorial complexity because there is no need to consider separately various combinations of bottom-up and top-down signals.Instead, all combinations are accounted for in the DL simulator-processes.Let us repeat, that initially, the models do not match the data.The association variables are not the narrow logical variables 0, or 1, or nearly logical, instead they are wide functions (across top-down and bottom-up signals).In other words, they are vague, initially they take near homogeneous values across the data (across bottom-up and top-down signals); they associate all the representation-models (through simulator processes) with all the input signals (Perlovsky 2001;2006b;2009c).Here we conceptually describe the DL process as applicable to visual perception, taking approximately 160 ms, according to the reference below.Gradually, the DL simulator-processes improve matching, models better fit data, the errors become smaller, the bell-shapes concentrate around relevant patterns in the data (objects), and the association variables tend to 1 for correctly matched signal patterns and models, and 0 for others.These 0 or 1 associations are logical decisions.In this way classical logic appears from vague states and illogical processes.Thus certain representations get associated with certain subsets of signals (objects are recognized and concepts formed logically or approximately logically).This process "from vague-to-crisp" that matches bottom-up and topdown signals has been independently conceived and demonstrated in brain imaging research to take place in human visual system (Bar, Kassam, Ghuman, Boshyan, Schmid, Dale, Hamalainen, Marinkovic, Schacter, Rosen, &amp; Halgren 2006).Thus DL models PSS simulators, describes how logic appears from illogical processes, and actually models perception mechanisms of the brainmind.</p>
<p>Mathematical convergence of the DL process was proven in (Perlovsky 2001).It follows that the simulator-process of perception or cognition assembles objects or concepts among bottom-up signals, which are most similar in terms of the similarity measure.Despite a combinatorially large number of items in the similarity, a computational complexity of DL is relatively low, it is linear in the number of signals, and therefore could be implemented by a physical system, like a computer or brain.</p>
<p>Example of DL, object perception in noise</p>
<p>The purpose of this section is to illustrate the DL processes, multiple simulators running in parallel as described above; our purpose here is not to illustrate DL-PSS relations.Therefore we use a simple example, still unsolvable by other methods, (mathematical details are omitted, they could be found in Linnehan, Mutz, Perlovsky, Weijers, Schindler, &amp; Brockett 2003).In this example, DL searches for patterns in noise.Finding patterns below noise can be an exceedingly complex problem.If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data.However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting.A standard approach for solving this kind of problem, which has already been mentioned, is multiple hypotheses testing (Singer, Sea, &amp; Housewright 1974); this algorithm exhaustively searches all logical combinations of subsets and models and faces combinatorial complexity.In the current example, we are looking for 'smile' and 'frown' patterns in noise shown in Fig. 1a without noise, and in Fig. 1b with noise, as actually measured (object signals are about 2-3 times below noise and cannot be seen).</p>
<p>To apply DL to this problem, we used DL equations given in the Appendix.Specifics of this example are contained in models.Several types of models are used: parabolic models describing 'smiles' and 'frown' patterns (unknown size, position, curvature, signal strength, and number of models), circular-blob models describing approximate patterns (unknown size, position, signal strength, and number of models), and noise model (unknown strength).Exact mathematical description of these models is given in the reference cited above.</p>
<p>The image size in this example is 100x100 points (N = 10,000 bottom-up signals, corresponding to the number of receptors in an eye retina), and the true number of models is 4 (3+noise), which is not known.Therefore, at least M = 5 models should be fit to the data, to decide that 4 fits best.This yields complexity of logical combinatorial search, M N = 10 5000 ; this combinatorially large number is much larger than the size of the Universe and the problem was considered unsolvable.Of course, DL does not guarantee finding any pattern in noise of any strength.For example, if the amount and strength of noise would increase ten-fold, most likely the patterns would not be found (this would provide an example of "falsifiability" of DL; however more accurate mathematical description of potential failures of DL algorithms is considered later).DL reduced the required number of computations from combinatorial 10 5000 to about 10 9 .By solving the CC problem DL was able to find patterns under the strong noise.In terms of signal-to-noise ratio this example gives 10,000% improvement over the previous state-of-the-art.(In this example DL actually works better than human visual system; the reason is that human brain is not optimized for recognizing these types of patterns in noise).3) are solved in 22 steps).Between stages (d) and (e) the algorithm tried to fit the data with more than one model and decided, that it needs three blob-models to 'understand' the content of the data.There are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob-models and parabolic models, which number, location, and curvature are estimated from the data.Until about stage (g) the algorithm 'thought' in terms of simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data.Iterations stopped at (h), when similarity (1) stopped increasing.</p>
<p>The main point of this example is that DL simulator-perception is a process "from vague-tocrisp," similar to visual system processes demonstrated in (Bar et al. 2006; in that publication authors refer as "low spatial frequency" to what we call "vague" in Fig. 1).</p>
<p>We would also like to take this moment to continue the arguments from sections 2.1, 2.2, and to emphasize that DL is a fundamental and revolutionary improvement in mathematics (Perlovsky 2009a;2010c); it was recognized as such in mathematical and engineering communities; it is the only mathematical theory that have suggested vague initial states; it has been developed for over 20 years; yet it might not be well known in cognitive science community.Those interested in a large number of mathematical and engineering applications of DL could consult given references and references therein.Here we would like to address two specific related concerns, first, if the DL algorithms are falsifiable, second, a possibility that Fig. 1 example could be "lucky" or "erroneous."We appreciate that some readers could be skeptical about 10,000% improvement over the state of the art.In mathematics there is a standard procedure for establishing average performance of detection (perception) and similar algorithms.It is called 'operating curves' and it takes not one example, but tens of thousands examples, randomly varying in parameters, initial conditions, etc.The results are expressed in terms of probabilities of correct and incorrect algorithm performance (this is an exact mathematical formulation of the idea of "falsifiability" of an algorithm).These careful procedures demonstrated that Fig. 1 represents an average performance of the DL algorithm (Perlovsky 2001).
A B C D F E H G</p>
<p>DL of PSS: perceptual cognition and simulators</p>
<p>Section 2.4 illustrated DL for recognition of simple objects in noise, a case complex and unsolvable for prior state-of-the-art algorithms, still too simple to be directly relevant for PSS.Here we consider a problem of situation learning, assuming that object recognition has been solved.In computational image recognition this is called "situational awareness" and it is a long-standing unsolved problem.The principled difficulty is that every situation includes many objects that are not essential for recognition of this specific situation; in fact there are many more "irrelevant" or "clutter" objects than relevant ones.Let us dwell on this for a bit.Objects are spatially-limited material things perceptible by senses.A situation is a collection of contextually related objects that tend to appear together and are perceived as meaningful, e.g., an office, a dining room.The requirement for contextual relations and meanings makes the problem mathematically difficult.Learning contexts comes along with learning situations; it reminds of the problem of a chicken and egg.We subliminally perceive many objects, most of which are irrelevant, e.g. a tiny scratch on a wall, which we learn to ignore.Combinations of even a limited number of objects exceed what is possible to learn in a single lifetime as meaningful situations and contexts (e.g.books on a shelf) from random sets of irrelevant objects (e.g. a scratch on a wall, a book, and a pattern of tree branches in a window).Presence of hundreds (or even dozens) irrelevant objects makes learning by a child of mundane situations a mathematical mystery.In addition, we constantly perceive large numbers of different objects and their combinations, which do not correspond to anything worth learning and we successfully learn to ignore them.</p>
<p>An essential part of learning-cognition is to learn which sets of objects are important for which situations (contexts).The key mathematical property of DL that made this solution possible, same as in the previous section, is a process "from vague-to-crisp."Concrete crisp models-representations of situations are formed from vague models in the process of learning (or cognition-perception).We illustrate below how complex symbols, situations, are formed by situation-simulators from simpler perceptions, objects, which are simpler perceptual symbols, being formed by simulators at "lower" levels of the mind, comparative to "higher" situation-simulators. Situation-simulators operate on PSS representations of situations, which are dynamic and vague assemblages of situations from imagery (and other modalities), bits and pieces along with some relations among them perceived at lower levels.These pieces and relations may come from different past perceptions, not necessarily from a single perceptual mode, and not necessarily stored in a contiguous parts of the brain.The dynamic process of DL-PSS-simulation, which assembles these bits into situations attempting to match those before the eyes, is mostly unconscious.We will discuss in details in section 6 that these are perceptual symbols as described in (Barsalou 1999).DL mathematically models PSS simulators (Barsalou 1999), processes that match bottom-up perceptions with top-down signals, assemble symbols in cognition-perception, and assemble conceptual representations by recreating patterns of activation in sensorimotor brain areas (as discussed later in the paper).An essential mechanism of DL cognition-perception is a process of simulation of perceptual imagination-cognitions; these situation-symbols are simulated from simpler perceptions-objects (we repeat that these simulations-imaginations are not limited to imagery, and are mostly unconscious).And the same mechanism can simulate plans and more complex abstract thoughts, as discussed in later sections.Thus, in the following sections we demonstrate that DL mathematically models PSS simulators, in this case simulators of situations.</p>
<p>DL formulation</p>
<p>In a simplified problem considered here, the task is for an intelligent agent (a child) to learn to recognize certain situations in the environment; while it is assumed that a child has learned to recognize objects.In real life a child learns to recognize situations, to some extent, in parallel with recognizing objects.But for simplicity of the illustration examples and discussions below, we consider a simplified case of objects being already known.For example, situation "office" is characterized by the presence of a chair, a desk, a computer, a book, a book shelf.Situation "playground" is characterized by the presence of a slide, a sandbox, etc.The principal difficulty is that many irrelevant objects are present in every situation.(This child learning is no different mathematically from an adult recognition.)</p>
<p>In the example below, D o is the total number of objects that the child can recognize in the world (it is a large number).In every situation he or she perceives D p objects.This is a much smaller number compared to D o .Each situation is also characterized by the presence of D s objects essential for this situation (D s &lt; D p ). Normally nonessential objects are present and D s is therefore less than D p .The sets of essential objects for different situations may overlap, with some objects being essential to more than one situation.We assume that each object is encountered in the scene only once.This is a minor and nonessential simplification, e.g.we may consider a set of similar objects as a new object.For example, "book" is an object and "books" is another object referring to more than one book.</p>
<p>The real life learning is sequential as a child is exposed to situations one at a time.Again, DL can handle this, but in this paper we consider the data about all the situations available at the time of learning.</p>
<p>Following (Ilin &amp; Perlovsky 2009) a situation can be mathematically represented as a vector in the space of all objects, X n = (x n1 , … x ni ,… x nDo ).If the value of x ni is one the object i is present in the situation n and if x ni is zero, the corresponding object is not present.Since D o is a large number, X n is a large binary vector with most of its elements equal to zero.A situation model is characterized by parameters, a vector of probabilities, p m = (p m1 ,.. p mi ,... p mDo ).Here p mi is the probability of object i being part of the situation m.Thus a situation model contains D o unknown parameters.Estimating these parameters constitutes learning.We would like to emphasize that although notations like x ni , p mi might look like amodal symbols, such an interpretation would be erroneous.Correct interpretation of notations in a mathematical model depends on what actual physical entities are referred to by the notations.These notations refer to neural signals, elements from which simulator-processes assemble symbols of a higher level.As discussed, for simplicity of presentation of the results, we assume that lower-level simulator-processes responsible for object recognition have already run their course, and objects have been recognized at a lower level, therefore x ni are 0 or 1.Given mathematical formulation can use dynamic signals x ni , parts of object-recognition simulators.We remind that simulators of interest in this example are situations; in addition to x ni these simulators involve dynamic neural signals referred by p mi .</p>
<p>These are constituent signals of the ongoing simulator processes at the considered level of situations, which learn to recognize situations, symbols at a higher level (relative to objects).</p>
<p>(We continue this discussion below in this section and also in section 6).</p>
<p>We model the elements of vector p m as independent (this is not essential for learning, if presence of various objects in a situation actually is correlated, this would simplify learning, e.g.perfect correlation would make it trivial).Correspondingly, conditional probability of observing vector X n in a situation m is then given by the standard formula (Jaynes 2003).
l(X(n) | M m (n)) = ∏ = Do i 1 p mi x ni (1 -p mi ) (1-x ni ) (1)
Consider N perceptions a child was exposed to (N includes real "situations" and "irrelevant" random ones); among them most perceptions were "irrelevant" corresponding to observing random sets of objects, and M-1 "real" situations, in which D s objects were repeatedly present.</p>
<p>All random observations we model by 1 model ("noise"); assuming that every object has an equal chance of being randomly observed in noise (which again is not essential) the probabilities for this noise model, m=1, are p 1i =0.5 for all i.Thus we define M possible sources for each of the N observed situations.</p>
<p>The total likelihood-similarity for our M models (M-1 "real" and 1 noise) is given by the same equation as similarity in the previous example (Perlovsky 2006b, also Appendix 1).And the same DL equations maximize it over the parameters, which in this case are the probabilities of objects constituting various situations.Specifics of this case of situations are given by models, which exact form is given by eq.( 1).The general DL equations given in the Appendix can be significantly simplified in this case (Ilin &amp; Perlovsky 2009) and we will describe them now in details.The DL is an iterative process, it involves a sequence of interactions between bottom-up and top-down signals; this sequence involves first, association variables connecting these signals, and second, parameter update equations, which improve parameter values in this interactionlearning.We use shorthand notations l(n|m) for conditional probabilities (1).The association variables connecting bottom-up signals n with top-down projections-simulators m are given by the general equation, as in Appendinx 1,
f(m|n) = r(m) l(n|m) / ∑ ∈M m' r(m') l(n|m').
(2)</p>
<p>For intuitive understanding we point out that these association variables are different from eq.( 1) in that they are normalized by the denominator, the sum of l(m|n) for a given bottom-up signal over all active simulators m.Whereas l(m|n) could vary greatly in their values, f(m|n) vary between 0 and 1.Also, eq.( 2) contain parameters r(m), which are needed for the following reason: it is convenient to define conditional probabilities (1) assuming the simulator m actually is present and active; therefore r(m) are needed to define the actual probability of the simulator m in the current process.These association variables (2) are defined similarly to the a posteriori Bayesian probabilities that a bottom-up signal n comes from a situation-simulator m.Yet they are not probabilities as long as parameters p mi have wrong values.In the process of learning, these parameters attain their correct values and, at the end of the DL-simulator processes, the association variables can be interpreted as probabilities.</p>
<p>These association variables are used to update parameter values, which is the second part of the DL process.In this case parameter update equations are simple,
p mi = ∑ ∈N n f(m|n) x ni / ∑ ∈N n' f(m|n')(3)
These equations have very simple interpretations: they estimate parameters p mi of the m-th simulator as weighted averages of the bottom-up signals, x ni .Note, the bottom-up signals "do not know" which situation they came from.The weights (f/Σf) are normalized association variables, associating data n with simulator m.These equations are easy to understand: if the object i never appears in a situation m, at the end of the DL-simulator learning process, f(m|n) = 0, and p mi = 0, as it should be, even if x ni are not 0 because object i appears in other situations.The role of the normalizing denominator (Σf) is easy to understand; for example, if object i is actually present in situation m, then x ni = 1 for each set of bottom up signal n, whenever situation m is observed.In this case, at the end of DL-simulator process, f(m|n) = 1 for all n, and eq.( 3) yields p mi = 1, as it should be.</p>
<p>For shortness, we did not discuss relations among objects.Spatial, temporal, or structural connections, such as "to the left," "on top," or "connected" can be easily added to the above DL formalism.Relations and corresponding markers (indicating which objects are related) are no different mathematically than objects, and can be considered as included in the above formulation.This mechanism is "flat" in the hierarchical structure of the brain, meaning that relations "reside" at the same level as entities they relate.Alternatively, some relations are realized in the brain hierarchically: relations could "reside" at a higher level, with markers being implemented similar to parametric models.Experimental data might help to find out, which relations are "flat" and which are "hierarchical."Other types of relations are principally hierarchical, e.g.objects-features, situations-objects, etc.We would also add that some relations are not "directly observable", as objects; say to differentiate between "glued to" or "stack to" might require knowledge of human actions or how the world is organized.Prerequisites to some of this knowledge might be inborn (Pinker 2008).We suggest that directly observable relations are learned as parts of a situation, similar to objects, and this learning is modeled by the DL formalism described above.Relations that require human cultural knowledge may be learned with the help of language, as discussed later, and inborn mechanisms should be further elucidated experimentally.This discussion implies several predictions that could be experimentally tested: existence of two types of relation mechanisms, flat and hierarchical; suggestions of which types of mechanisms are likely to be used in the brain for which types of relations; and suggestions of mechanisms conditioned by culture and language.</p>
<p>We repeat that the formulation here assumes that all the objects have already been recognized, but the above formulation can be applied without any change to real, continuously working brain with multiplicity of concurrently running simulators at many levels, feeding each other.</p>
<p>Example of symbol-situation learning using DL</p>
<p>In this example we set the total number of recognizable objects equal to 1000 (D o =1000).The total number of objects perceived in a situation is set to 50 (D p =50).The number of essential objects is set to 10 (D s =10).The number of situations to learn (M-1) is set to 10.Note that the true identities of the objects are not important in this simulation so we simply use object indexes varying from 1 to 1000 (this index points to neural signals corresponding to a specific objectsimulators).The situation names are also not important and we use situation indexes (this index points to neural signals corresponding to a specific situation-simulators).We would emphasize that the use of numbers for objects and situation, while may seem consistent with amodal symbols, in fact is nothing but notations.We repeat that the principled differences between PSS and amodal systems are mechanisms in the brain and their modeling, not mathematical notations.Among these mechanisms are simulators, mathematically described by DL.Let us repeat, amodal symbols are governed by classical logic, which is static and faces CC.DL is a process and overcomes CC.DL operates on PSS representations (models p m ), which are vague collections of objects (some of these objects could also be vague, not completely assembled yet representations).Another principled difference is interaction between perceptual-based bottomup and top-down neural fields X n and M m ; indexes n and m are just mathematical shorthand for corresponding neural connections.In this paper we consider object perception and situation perception in different sections, but of course the real mind-brain operates continuously, "objects" in this section are neural signals sent to situation-recognition brain area (and corresponding simulators) by excited neuron fields corresponding to models of recognizedobjects (partially, as described in section 2; and as discussed, these signals are being sent before objects are fully recognized, while object simulators are still running).The data for this example are generated by first randomly selecting D s =10 specific objects for each of the 10 groups of objects, allowing some overlap between the groups (in terms of specific objects).This selection is accomplished by setting the corresponding probabilities p mi = 1.Next we add 40 more randomly selected objects to each group (corresponding to D p =50).We also generate 10 more random groups of 50 objects to model situations without specific objects (noise); this is of course equivalent to 1 group of 500 random objects.We generate N'=800 perceptions for each situation resulting in N=16,000 perceptions (data samples, n = 1… 16,000) each represented by 1,000-dimensional vector X n .These data are shown in Fig. 2 sorted by situations.Then the samples are randomly permuted, according to randomness of real life perceptual situations, in Fig. 3.The horizontal lines disappear; the identification of repeated objects becomes nontrivial.An attempt to learn groups-situations (the horizontal lines) by inspecting various horizontal sortings (until horizontal lines would become detectable) would require M N = 10 16000 inspections, which is of course impossible.This CC is the reason why the problem of learning situations has been standing unsolved for decades.By overcoming CC, DL can solve this problem as illustrated below.The DL algorithm is initiated similarly to section 2 by defining 20 situational models (an arbitrary selection, given actual 10 situations) and one random noise model to give a total of M=21 models (in section 2.4, Fig. 1 models were automatically added by DL as required; here we have not done this (because it would be too cumbersome to present results).The models are initialized by assigning random probability values to the elements of the models.These are the initial vague perceptual models, which assign all objects to all situations.This fast and accurate convergence can be seen from Figs. 5 and 6.We measure the fitness of the models to the data by computing the sum-squared error, using the following equation.
E = ∑ ∑ ∈ = } { 1 B m Do i ( p mi -p mi True ) 2
In this equation the first summation is over the subset {B} containing top 10 models that provide the lowest error (and correspondingly, the best fit to the 10 true models).In the real brain, of course, the best models would be added as needed, and the random samples would accumulate in the noise model automatically; as mentioned, DL can model this process and the reason we did not model it, is that it would be too cumbersome to present results.Fig. 5 shows how the sum squared error changes over the iterations of the DL algorithm.It takes only a few iterations for the DL algorithm to converge.Each of the best models contains 10 large and 990 low probabilities.Iterations stop, when average error of probabilities reached a low value of 0.05 resulting in the final error E(10) =1000<em>(0.05^2)</em>10 = 25.Fig. 6 shows average associations, A(m,m') among true (m) and computed models (m'); this is an 11x11 matrix according to the true number of different models (it is computed using association variables between models and data, f(m|n))
A(m,m') = (1/N') ∑ = N n 1 f(m|n)<em> f(m'|n), m'∈{B}, A(m,11) = (1/10</em>N') ∑ ∑ = ∉ N n B m 1 } { ' f(m|n)* f(m'|n), m'∉{B}
Here, f(m|n) for true 10 models m is either 1 (for N' data samples from this model) or 0 (for others), f(m'|n) are computed associations, in the second line all 10 computed noise models are averaged together, corresponding to one true (random) noise model.The correct associations on the main diagonal in Fig. 6 are 1 (except noise model, which is spread among 10 computed noise models, and therefore equals 0.1) and off-diagonal elements are near 0 (incorrect associations, corresponding to small errors shown in Fig 5 .).</p>
<p>We would like to address briefly, why errors in Fig. 5 do not converge exactly to 0. The reason is numerical, p and x variables in eq. ( 1) cannot be allowed to take exactly 0 value, since expressions 0 0 are numerically non-defined, therefore all values have been bounded from below by 0.05 (a somewhat arbitrary limit).Fig. 6 demonstrates that nevertheless, convergence to the global maximum was achieved (the exactly known solution in terms of learning the correct situations).Again, as in section 2, learning of perceptual situation-symbols has been accomplished due to the DL process-simulator, which simulated internal model-representations of situations, M, to match patterns in bottom-up signals X (sets of lower-level perceptual object-symbols).Fig. 6.Correct associations are near 1 (diagonal, except noise) and incorrect associations are near 0 (off-diagonal).</p>
<p>Simulators, concepts, grounding, binding, and DL</p>
<p>As described previously, PSS grounds perception, cognition, and high-level symbol operation in modal symbols, which are ultimately grounded in the corresponding brain systems.Previous section provides an initial development of formal mathematical description suitable for PSS: the DL process "from vague-to-crisp" models PSS simulators.We have considered just one subsystem of PSS, a mechanism of learning, formation, and recognition of situations from objects making up the situations.(More generally, the formalized mechanism of simulators includes recognition of situations by recreating patterns of activations in sensorimotor brain areas, from objects, relations, and actions making up the situations).The mind's representations of situations are symbol-concepts of a higher level of abstractness than symbol-objects making them up.The proposed mathematical formalism can be advanced straightforwardly to "higher" levels of more and more abstract concepts.We would add a word of caution: such application to more abstract ideas may require an additional grounding in language (Perlovsky 2007b;2009a) as we briefly consider in the next section.</p>
<p>The proposed mathematical formalism can be similarly applied at a lower level of recognizing objects as constructed from their parts; mathematical techniques of sections 2 and 3 can be combined to implement this PSS object recognition idea as described in (Barsalou 1999).Specific models considered in section 2 are likely to be based on inborn mechanisms specific to certain aspects of various sensor and motor modalities; general models of section 3 can learn to represent and recognize objects as collections of multi-modal perceptual features and relations among them.In both cases principal mechanisms of object perception such as discussed in (Spelke 1990) can be modeled, either as properties of object models, or as relations between perceptual features.Since relations specific to object recognition, according to this reference are learned in infancy, the mechanism of section 3 seems appropriate (it models learning of relations, whereas models in section 2 do not readily contain mechanisms of learning of all their structural aspects and are more appropriate to modeling inborn mechanisms).Object representations, as described by Barsalou are not similar to photographs of specific objects, but similar to models in Fig. 4 are more or less loose and distributed (among modalities) collections of features (determined in part by inborn properties of sensor and motor organs) and relations.</p>
<p>We note that the described theory, by modeling the simulators, also mathematically models productivity of the mind concept-simulator system.The simulated situations and other concepts are used not only in the process of matching bottom-up and top-down signals for learning and recognizing representations, but also in the motor actions, and in the processes of imagination and planning.</p>
<p>Modeling situations in PSS as a step toward general solution of the binding problem is discussed in (Edelman &amp; Breen 1999).DL provides a general approach to the binding problem similar to the "corkboard" approach described in (Edelman &amp; Intrator 2001).That publication also discusses the role of context similar to the DL scene modeling.Here we would emphasize two mechanisms of binding modeled in the developed theory.First, binding is accomplished hierarchically: e.g.object representations-simulators bind features into objects, similarly situation representations-simulators bind objects into situations, etc.Second, binding is accomplished by relations that are learned similarly to objects and "reside" at the same level in the hierarchy of the mind with the bound entities.These two types of binding mechanisms is another novel prediction of the DL theory that could be tested experimentally.</p>
<p>Below we discuss other relationships between the mathematical DL procedures of previous sections and the fundamental ideas of PSS.Section 2 concentrated on the principal mathematical difficulty experienced by all previous attempts to solve the problem of complex symbol formation from less complex symbols, the combinatorial complexity (CC).CC was resolved by using DL, a mathematical theory, in which learning begins with vague (non-specific) symbolconcepts, and in the process of learning symbol-concepts become concrete and specific.Learning could refer to a child's learning, which might take days or months or an everyday perception and cognition, taking approximately 1/6 th of a second (in the later case learning refers to the fact that every specific realization of a concept in the world is different in some respects from any previous occurrences, therefore learning-adaptation is always required; in terms of PSS, a simulator always have to re-assemble the concept).In the case of learning situations as compositions of objects, the initial vague state of each situation-symbol is a nearly random and vague collection of objects, while the final learned situation consists of a crisp collection of few objects specific to this situation.This specific of the DL process "from vague-to-crisp" is a prediction that can be experimentally tested, and we return to this later.In the learning process random irrelevant objects are "filtered out," their probability of belonging to a concept-situation is reduced to zero, while probabilities of relevant objects, making up a specific situation is increased to a value characteristic of this object being actually present in this situation.</p>
<p>Relation of this DL process to PSS is now considered.First we address concepts and their development in the brain.According to (Barsalou 2007), "The central innovation of PSS theory is its ability to implement concepts and their interpretative functions using image content as basic building blocks."</p>
<p>This aspect of PSS theory is implemented in DL in a most straightforward way.Conceptsituations in DL are collections of objects (symbol-models at lower levels, which are neurally connected to neural fields of object-images).As objects are perceptual entities-symbols in the brain, concept-situations are collections of perceptual symbols.In this way situations are perceptual symbols of a higher order complexity than object-symbols, they are grounded in perceptual object-symbols (images), and in addition, their learning is grounded in perception of images of situations.A PSS mathematical formalization of abstract concepts (Barsalou 2003b), not grounded in direct perceptions, is considered in the next section.Here we just mention that the proposed model is applicable to higher levels, "beyond" object-situations; it is applicable to modeling interactions between bottom-up and top-down signals at every level.Barsalou (2008) has described development of concepts in the brain as forming collections of correlated features.This is explicitly implemented in the DL process described in section 3. The developed mathematical representation corresponds to multimodal and distributed representation in the brain.It has been suggested that a mathematical set or collection is implemented in the brain by a population of conjunctive neurons (Simmons &amp; Barsalou 2003).</p>
<p>DL learning and perception-cognition processes are mathematical models of PSS simulators.DL symbol-situations are not static collections of objects but dynamic processes.In the process of learning they "interpret individuals as tokens of the type" (Barsalou 2008).They model multimodal distributed representations (including motor programs) as described in the reference.</p>
<p>The same DL mathematical procedure can apply to perception of a real situation in the world as well as an imagined situation in the mind.This is the essence of imagination.Models of situations (probabilities of various objects belonging to a situation, and objects attributes, such as their locations) can depend on time, in this way they are parts of simulators accomplishing cognition of situations evolving in time.If "situations" and "time" pertain to the mind's imaginations, the simulators implement imagination-thinking process, or planning.</p>
<p>Usually we perceive-understand a surrounding situation, while at the same time thinking and planning future actions and imagine consequences.This corresponds to running multiple simulators in parallel.Some simulators support perception-cognition of the surrounding situations as well as ongoing actions, they are mathematically modeled by DL processes that converged to matching internal representations (types) to specific subsets in external sensor signals (tokens).Other simulators simulate imagined situations and actions related to perceptions, cognitions, and actions, produce plans, etc.</p>
<p>Developed here DL modeling of PSS models mathematically what Barsalou (2003b) called dynamic interpretation of PSS (DIPSS).DIPSS is fundamental to modeling abstraction processes in PSS.Three central properties of these abstractions are type-token interpretation; structured representation; and dynamic realization.Traditional theories of representation based on logic model interpretation and structure well but are not sufficiently dynamical.Conversely, connectionist theories are dynamic but are inadequate at modeling structure.PSS addresses all three properties.Similarly, the DL mathematical process developed here addresses all three properties.In type-token relations "propositions are abstractions for properties, objects, events, relations and so forth.After a concept has been abstracted from experience, its summary representation supports the later interpretation of experience."Correspondingly in the developed mathematical approach, DL models a situation as a loose collection of objects and relations.Its summary representation (the initial model) is a vague and loose collection of property and relation simulators, which evolves-simulates representation of a concrete situation in the process of perception of this concrete situation according to DL.This DL process involves structure (initial vague models) and dynamics (the DL process). .</p>
<p>DL and PSS: abstract concepts, language, the mind hierarchy, and emotions</p>
<p>Here we discuss DL as a general model of interacting bottom-up and top-down signals throughout the hierarchy-heterarchy of the mind-brain, including abstract concepts.The DL mathematical analysis suggests that modeling the process of learning abstract concepts has to go beyond PSS analysis in (Barsalou 1999).In particular, we discuss the role of language in learning abstract concepts (Perlovsky 2007b;2009a) and connect it to the PSS mechanisms.The mind-brain is not a strict hierarchy, interactions across layers are present and this is some times addressed as heterarchy (Grossberg 1988).To simplify discussion we would use a term hierarchy.</p>
<p>Section 3 discussed assembling situation representations from object representations.This addresses interaction between top-down and bottom-up signals in two adjacent layers of the mind hierarchy.The mathematical description presented in section 3 addresses top-down and bottom-up signals and representations without explicit emphasis on their referring to objects or situations.Accordingly, we would emphasize here that the mathematical formulation in section 3 equally addresses interaction between any two adjacent layers in the entire hierarchy of the mind-brain, including high-level abstract concepts.DL overcomes the ubiquitous problem of CC, the presented DL mathematics is practically computable in a machine or mind.However, another fundamental aspect, grounding, remains questionable.</p>
<p>In the PSS formulation Barsalou assumed that higher level abstract concepts remain grounded since they are based on lower level grounded concepts, and down the hierarchy to perceptions directly grounded in sensory-motor signals.The DL modeling suggests that this aspect of the PSS theory has to be revisited for the following two reasons.First, each higher level is vaguer than a lower level.Several levels on top of each other would result in representations too vaguely related to sensory-motor signals to be grounded in them with any reliability.Second, the section 3 example is impressive in its numerical complexity, which significantly exceeds anything that has been computationally demonstrated previously.We would like to emphasize again that this is mostly due to overcoming difficulty of CC.Still statistically, learning of situations was based on these situations being present among the data with statistically sufficient information to distinguish them among each other and from noise.In real life however, human learn complex abstract concepts, such as "state," "law," "rationality," and many other abstract concepts, without statistically sufficient information been experienced (we return to this statement later and discuss its various aspects and a need for experimental verification).Now we would like to emphasize the role of language in learning abstract concepts.</p>
<p>Language is learned at all levels of the hierarchy of the mind-brain and cultural knowledge from surrounding language.Experience of talking with other people operates in significant way with "ready made" language concepts.This makes it possible for kids to talk about much of cultural contents by the age of five or seven.At this age kids can talk about many abstract ideas, which they cannot yet adequately use in real life.This suggests that language concepts and cognitive concepts are different.Language concepts are grounded in surrounding language at all hierarchical levels.But learning corresponding cognitive concepts grounded in life experience takes entire life.Learning language, like learning cognition can be modeled by DL.Linguists consider words to be learned by memorizing them (Pinker 1994).Learning meaningful phrases and syntax is similar to learning situations and relations among objects in section 3. Morphology is not unlike object composition.</p>
<p>As discussed above and in the given references a fundamental difference of language from cognition is grounding.Let us repeat, language is grounded in direct experience (of talking, reading) at all levels of the hierarchy, whereas cognition is grounded in direct perceptions only at the bottom of the hierarchy.It seems to be an inescapable conclusion that higher abstract levels of cognition are grounded in language.The detailed theory of interaction between cognition and language is considered in (Perlovsky 2002;2004;2006b,d;2009a;Fontanari &amp; Perlovsky 2007;2008a;b;Fontanari, Tikhanoff, Cangelosi, Ilin, &amp; Perlovsky 2009;Perlovsky &amp; Fontanari 2006;Perlovsky &amp; Ilin 2010).It leads to a number of verifiable experimental predictions, which we summarize in section 8.The main suggested mechanism of interaction between language and cognition is the dual model, which hypothesize that every mental model-representation has two neurally connected parts, language model and cognitive model.Language models are learned by simulator processes, similar to PSS simulators, however, "perception" in case of language refers to perception of language facts.Through neural connections between the two parts of each model, the early acquired abstract language models guide the development of abstract cognitive models in correspondence with cultural experience stored in language.The dual model leads to the dual hierarchy illustrated in Fig. 7. Fig. 7.The dual-model architecture, modeling interaction of language and cognition.Learning of cognition is grounded in experience and guided by language.Learning of language is grounded in the surrounding language at all hierarchical levels.</p>
<p>Perceptual vs. amodal symbols</p>
<p>Since any mathematical notation may look like an amodal symbol, in this section we discuss the roles of amodal vs. perceptual symbols in DL.This would require clarification of the word symbol.We touch on related philosophical and semiotic discussions and relate them to mathematics of DL and to PSS.For the sake of brevity within this paper we limit discussions to the general interest, emphasizing connections between DL, perceptual, and amodal symbols; extended discussions of symbols can be found in (Perlovsky 2006b;d).We also summarize here related discussions scattered throughout the paper.</p>
<p>"Symbol is the most misused word in our culture" (Deacon, 1998).Why the word "symbol" is used in such a different way: to denote trivial objects, like traffic signs or mathematical notations, and also to denote objects affecting entire cultures over millennia, like Magen David, Swastika, Cross, or Crescent?Let us compare in this regard opinions of two founders of contemporary semiotics, Charles Peirce (Peirce 1897;1903) andFerdinand De Saussure (1916).Peirce classified signs into symbols, indexes, and icons.Icons have meanings due to resemblance to the signified (objects, situations, etc.), indexes have meanings by direct connection to the signified, and symbols have meaning due to arbitrary conventional agreements.Saussure used different terminology, he emphasized that signs receive meanings due to arbitrary conventions, whereas symbol implies motivation.It was important for him that motivation contradicted arbitrariness.Peirce concentrated on the process of sign interpretation, which he conceived as a triadic relationship of sign, object, and interpretant.Interpretant is similar to what we call today a representation of the object in the mind.However, this emphasis on interpretation was lost in the following generation of scientists.This process of "interpretation" is close to the DL processes and PSS simulators.We therefore follow Saussurean designation of symbol as a motivated process.Motivationally loaded interpretation of symbols was also proposed by Jung (1921).He considered symbols as processes bringing unconscious contents to consciousness.Similar are roles of PSS simulators and DL processes.(Motivated in DL means in particular related to drives, emotions).</p>
<p>In the development of scientific understanding of symbols and semiotics, the two functions, understanding the language and understanding the world, have often been perceived as identical.This tendency was strengthened by considering logical rules to be the mechanism of both, language and cognition.According to Russell (1919), language is equivalent to axiomatic logic, "[a word-name] merely to indicate what we are speaking about; [it] is no part of the fact asserted… it is merely part of the symbolism by which we express our thought."Hilbert (1928) was sure that his logical theory also describes mechanisms of the mind, "The fundamental idea of my proof theory is none other than to describe the activity of our understanding, to make a protocol of the rules according to which our thinking actually proceeds."Similarly, logical positivism centered on "the elimination of metaphysics through the logical analysis of language" -according to Carnap (1959) logic was sufficient for the analysis of language.As discussed in section 2.2, this belief in logic is related to functioning of human mind, which is conscious about the final states of DL processes and PSS simulators; these final states are perceived by our minds as approximately logical amodal symbols.Therefore we identify amodal symbols with these final static logical states, signs.</p>
<p>DL and PSS explain how the mind constructs symbols, which have psychological values and are not reducible to arbitrary logical amodal signs, yet are intimately related to them.In section 3 we have considered objects as learned and fixed.This way of modeling objects indeed is amenable to interpreting them as amodal symbols-signs.Yet, we have to remember that these are but final states of previous simulator processes, perceptual symbols.Every perceptual symbol-simulator has a finite dynamic life, and then it becomes a static symbol-sign.It could be stored in memory, or participate in initiating new dynamical perceptual symbols-simulators.This infinite ongoing dynamics of the mind-brain ties together static signs and dynamic symbols.It grounds symbol processes in perceptual signals that originate them; in turn, when symbol-processes reach their finite static states-signs, these become perceptually grounded in symbols that created them.We become consciously aware of static sign-states, express them in language and operate with them logically.Then, outside of the mind-brain dynamics, they could be transformed into amodal logical signs, like marks on a paper.Dynamic processes -symbols-simulators are usually not available to consciousness.These PSS processes involving static and dynamic states are mathematically modeled by DL in section 3 and further discussed in section 4.</p>
<p>To summarize, in this paper we follow a tradition using a word sign for an arbitrary, amodal, static, unmotivated notation (unmotivated means unemotional, in particular).We use a word symbol for the PSS and DL processes-simulators, these are dynamic processes, connecting unconscious to conscious; they are motivationally loaded with emotions.As discussed in section 2, DL processes are motivated toward increasing knowledge, and they are loaded with knowledge-related emotions, even in absence of any other motivation and emotion.These knowledge-related emotions are called aesthetic emotions since Kant.They are foundations of higher cognitive abilities, including abilities for the beautiful, sublime, and they are related to musical emotions.More detailed discussions can be found in (Levine &amp; Perlovsky 2008, Perlovsky 1996b, 1997, 2000, 2001, 2002a,c, 2004, 2006a,b,d, 2007a,b,c, 2009a,b, 2010a,b, Perlovsky, Bonniot-Cabanac, &amp; Cabanac 2010, Perlovsky &amp; Ilin 2010).</p>
<p>DL mathematical models (in section 3) use mathematical notations, which could be taken for amodal symbols.Such an interpretation would be erroneous.Meanings and interpretations of mathematical notations in a model depends not on the appearance, but on what is modeled.Let us repeat, any mathematical notation taken out of the modeling context, is a notation, a static sign.In DL model-processes these signs are used to designate neuronal signals, dynamic entities evolving 'from vague to crisp' and mathematically modeling processes of PSS simulatorssymbols.Upon convergence of DL-PSS simulator processes, the results are approximately static entities, approximately logical, less grounded and more amodal.DL models both, grounded, dynamic symbol-processes, overcoming combinatorial complexity and amodal static symbols, which are governed by classical logic and in the past have led to combinatorial complexity.DL operates on a non-logical type of PSS representations, which are vague combinations of lower-level representations.These lower-level representations are not necessarily complete images or events in their entirety, but could include bits and pieces of various sensor-motor modalities, memory states, as well as vague dynamic states from concurrently running simulators -DL processes of the on-going perception-cognition. (In section 3, for simplicity of presentation, we assumed that the lower-level object-simulators have already run their course and reached static states; however, the same mathematical formalism can model simulators running in parallel on multiple hierarchical levels.)The mind-brain is not a strict hierarchy, the same-level and higher-level representations could be involved along with lower levels.DL models processes-simulators, which operate on PSS representations.These representations are vague and incomplete, and DL processes are assembling and concretizing these representations.As described in several references by Barsalou, bits and pieces from which these representations are assembled could include mental imagery as well as other components, including multiple sensor, motor, and emotional modalities; these bits and pieces are mostly inaccessible to consciousness during the process dynamics.</p>
<p>DL also explains how logic and ability to operate amodal symbols originate in the mind from illogical operations of PSS: mental states approximating amodal symbols and classical logic appear as the end of the DL process-simulators.At this moment they become conscious static representations and loose that component of their emotional-motivational modality, which is associated with the need for knowledge (to qualify as amodal, these mental states should have no sources of modality, including emotional modality).The developed DL formalization of PSS, therefore suggests using a word signs for amodal mental states as well as for amodal static logical constructs outside of the mind, including mathematical notations; and to reserve symbols for perceptually grounded motivational cognitive processes in the mind-brain.Memory states, to the extent they are static entities, are signs in this terminology.Logical statements and mathematical signs are perceived and cognized due to PSS simulator symbol-processes and become signs after being understood.Perceptual symbols, through simulator processes, tie together static and dynamic states in the mind.Dynamic states are mostly outside of consciousness, while static states might be available to consciousness.Bar et al. (2006) demonstrated in neuroimaging experiments that visual perception proceeds according to DL simulating crisp perceptions from initial vague representations (In this reference authors use terminology of "low spatial frequency" for what we call vague).Experimental procedures in this reference used functional Magnetic Resonance Imaging (fMRI) to obtain high-spatial resolution of processes in the brain, which they combined with magneto-encephalography (MEG), measurements of the magnetic field next to the head, which provided high temporal resolution of the brain activity.Combining these two techniques the experimenters were able to receive high resolution of cognitive processes in space and time.Bar et al. concentrated on three brain areas: early visual cortex, object recognition area (fusiform gyrus), and object-information semantic processing area (OFC).They demonstrated that OFC is activated 130 ms after the visual cortex, but 50 ms before object recognition area.Their conclusion has been that OFC represents the cortical source of top-down facilitation in visual object recognition.This top-down facilitation is unconscious.They demonstrated that the imagined image generated by top-down signals facilitated from OFC to cortex is vague (the authors in this publication refer to low spatial-frequency content images), confirming the essential mechanism of DL.Conscious perception of an object occurs when vague projections become crisp and match the crisp and clear image from the retina, and an object recognition area is activated.</p>
<p>Experimental evidence</p>
<p>The brain continuously extracts rudimentary information from early sensory data and simulates predictions, which facilitate perception and cognition in the relevant context by pre-sensitizing relevant representations.This includes predictions of complex information, such as situations and social interactions (Bar 2007;2009).Predictions are initiated by gist information rapidly extracted from sensory data.At the "lower"-object level this gist information is a vague image of an object (low spatial frequency, Bar et al. 2006).At higher levels "the representation of gist information" is yet to be defined."The model developed here defines this higher-level gist information as vague collections of vague objects, with relevant objects for a specific situation having just slightly higher probabilities than irrelevant ones.The developed model is also consistent with the hypothesis in (Bar 2007) that perception and cognition at higher levels relies on mental simulations.Mathematical predictions in this paper suggest specific properties of these higher-level simulators (initial top-down representations are vague in terms of their associations with bottom-up signals), which could be verified experimentally.</p>
<p>Future research</p>
<p>Future research will address the DL mathematical description of PSS throughout the mind hierarchy; from features and objects "below situations" in the hierarchy to abstract models and simulators at higher levels "above situations."Modeling across the mind modalities will be addressed including diverse modalities, symbolic functions, conceptual combinations, predication.Modeling features and objects would have to account for suggestions that perception of features are partly inborn (Barsalou 1999); this development therefore might require new experimental data on which feature aspects are inborn (Edelman &amp; Newell, 1998).The developed DL formalization of PSS corresponds to observations in (Wu &amp; Barsalou 2009) and it will be used for generating more detailed experimentally verifiable predictions.A number of predictions have been made in this paper, including influence of perception on cognition (van Danzig et al, 2008).</p>
<p>The proposed theory provides solutions to classical problems of conceptual relations, binding, and recursion.Binding is a mechanism connecting events into meaningful "whole" (or largerscale events).The DL model developed here specifies two types of binding mechanisms "flat" and "hierarchical," and suggests which mechanisms are likely to be used for various relations.Our model also suggests existence of binding mechanisms conditioned by culture and language.Recursion has been postulated to be a fundamental mechanism in cognition and language (Hauser, Chomsky, and Fitch, 2002), however, that reference has not proposed specific mechanisms how recursion creates representations, nor how it maps representations into the sensory-motor or conceptual-intentional interfaces.In our opinion this is an erroneous assumption, and the error is similar to mistaking logical signs for symbol-processes (recursion is an important logical operation).According to the developed theory recursion is not a fundamental mechanism, instead the hierarchy is proposed as a mechanism of recursion.Successive hierarchical levels accomplish recursive cognitive and linguistic functions.Again, these proposals can be experimentally tested.</p>
<p>Experimental research (Bar et al. 2006;Bar 2007) can address specific properties of higher-level simulators predicted here.Among these is a prediction that early predictive stages of situation simulations are vague.Whereas vague predictions of objects resemble low-spatial frequency of object imagery (Bar et al. 2006), "the representation of gist information on higher levels of analysis is yet to be defined" (Bar 2007).According to the developed model, vague predictions of situations should contain many less-relevant (and likely vague) objects with lower probabilities.Since the mathematical model proposed here is applicable to higher levels ("above" object-situations), this hypothesis should be relevant to the nature of information of higher-level gists: initial representation of abstract concepts are vague in terms of associations with constituent bottom-up signals (these associations are not exact, but vague; probabilistically, they are not close to zeroes and ones).</p>
<p>The present model can be expanded to address another topic discussed in (Bar 2007), "how the brain integrates and holds simultaneously information from multiple points in time."Two different mechanisms should be explored: first, explicit incorporation of time into models (so that model parameters-probabilities depend on time), and second, categorized temporal relations, such as "before," "after" can be included as any other relations-objects into models.A joint mathematical-experimental approach might be fruitful in this area.Future research will address interaction between language and cognition.Language is acquired from surrounding language, rather than from direct experience in the world; language therefore is closer aligned with amodal symbols than with perceptual symbols.Kids at 5 years of can talk about much of cultural content of the surrounding language, including highly abstract contents; yet, clearly kids do not have necessary experience to understand highly abstract concepts, as perceptual symbols, and to relate them to the world.According to the developed theory, higher abstract concepts could be stronger grounded in language than in perception; not only kids, but also adults may operate with abstract concepts as with amodal symbols, and therefore have limited understanding grounded in experience of how abstract concepts relate to the world.It follows that higher-level concepts may be less grounded in perception and experience than in language.The developed theory suggests several testable hypotheses: (i) the dual model, postulating separate cognitive and language mental representations, (ii) neural connections between cognitive and language mental representations, (iii) language mental representations guiding acquisition of cognitive representations in ontological development, (iv) abstract concepts being more grounded in language than in experience; and (v) this shift from grounding in perception and experience to grounding in language progresses up the hierarchy of abstractness; while grounding in perception and experience increases with age.These make a fruitful field for future experimental research.</p>
<p>The suggested model of connections between language and cognition bears on language evolution, and future research should address theoretical and experimental tests of this connection between evolution of languages, cognition, and cultures (Brighton, Smith, &amp; Kirby 2005;Perlovsky &amp; Fontanari 2006;Fontanari &amp; Perlovsky 2007;2008a;b;Fontanari, Tikhanoff, Cangelosi, Ilin, &amp; Perlovsky 2009;Perlovsky 2009b).</p>
<p>The role of emotions in perception was addressed in (Barrett &amp; Bar 2009).There are several mechanisms of emotions and future research should extend this paper formalism to more detailed modeling of emotions and their role in cognition.Also future research would explore roles of emotions (i) in language-cognition interaction (Perlovsky 2009b), (ii) in symbol grounding, and (iii) the role of aesthetic and musical emotions in cognition (Perlovsky 2000(Perlovsky , 2000c(Perlovsky , 2006c(Perlovsky , 2008(Perlovsky , 2010)).</p>
<p>Fig. 1 illustrates DL operations: (a) true 'smile' and 'frown' patterns without noise; (b) actual image available for recognition; (c) through (h) illustrates the DL process, they show improved models at various steps of solving DL eqs.(A3), total of 22 steps (noise model is not shown; figures (c) through (h) show association variables, f, for blob and parabolic models).By comparing (h) to (a) one can see that the final states of the models match patterns in the signal.</p>
<p>Fig. 1 .
1
Fig.1.Finding 'smile' and 'frown' patterns in noise, an example of dynamic logic operation: (a) true 'smile' and 'frown' patterns are shown without noise; (b) actual image available for recognition (signals are below noise, signal-to-noise ratio is between ½ and ¼, 100 times lower than usually considered necessary); (c) an initial fuzzy blob-model, the vagueness corresponds to uncertainty of knowledge; (d) through (h) show improved models at various steps of DL (eq.(3) are solved in 22 steps).Between stages (d) and (e) the algorithm tried to fit the data with more than one model and decided, that it needs three blob-models to 'understand' the content of the data.There are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob-models and parabolic models, which number, location, and curvature are estimated from the data.Until about stage (g) the algorithm 'thought' in terms of simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data.Iterations stopped at (h), when similarity (1) stopped increasing.</p>
<p>Fig. 2 .
2
Fig. 2. Generated data; object index is along vertical axes and situation index is horizontal.The perceptions (data samples) are sorted by situation index (horizontal axis); this makes visible the horizontal lines for repeated objects.</p>
<p>Fig. 3 .
3
Fig. 3. Generated data, same as Fig. 2, randomly sorted by situation index (horizontal axis), as available to the DL algorithm for learning.</p>
<p>Fig. 4
4
Fig.4illustrates the initialization and the iterations of the DL algorithm (the first 3 steps of solving DL equations.Each subfigure displays the probability vector p m for each of the 20 models.The vectors have 1000 elements corresponding to objects (vertical axes).The values of each vector element are shown in gray scale.The initial models assign nearly uniformly distributed probabilities to all objects.The horizontal axes are the model index changing from 1 to 20.The noise model is not shown.As the algorithm progresses, situation grouping improves, and only the elements corresponding to repeating objects in "real" situations keep their high values, the other elements take low values.By the third iteration the 10 situations are identified by their corresponding models.The other 10 models converge to more or less random lowprobability vectors.</p>
<p>Fig. 4 .
4
Fig. 4. DL situation learning.Situation-model parameters converge close to true values in 3 steps.</p>
<p>Fig. 5 .
5
Fig. 5. Errors of DL learning are quickly reduced in 3-4 steps, iterations continue until average error reached low value of 0.05 (10 steps).</p>
<p>signals and their combinations as separate logical statements.A large number of combinations of these variations cause CC.This general statement manifests in various types of algorithms in different ways.Rule systems are logical in a straightforward way, and the number of rules grows combinatorially.Pattern recognition algorithms and neural networks are related to logic in learning procedures: every training sample is treated as a logical statement ("this is a chair") resulting in CC of learning.</p>
<p>Algorithms matching bottom-up and top-down signals based on formal logic have to evaluate 6 every variation in</p>
<p>Also modality of objects (various sensorial or motor mechanisms) requires no modifications (emotions can be included as well, some emotions are reducible to representations and learned to be a part of a situation similar to objects; other involve entirely different mechanisms discussed later).The bottom up signals do not have to be definitely recognized objects, these signals can be sent before objects are fully recognized, while object simulators are still running and object representations are vague; this would be represented by x ni values between 0 and 1.The bottom up signals do not have to correspond to complete objects, but could recreate patterns of activations in sensorimotor brain areas associated with perception of objects; similarly, top-down signals corresponding to situations, p m , correspond to patterns of activations recreating experience of this situation.The presented formalization therefore is a general mechanism of simulators.A fundamental experimentally testable prediction of the developed theory is that topdown signals originate from vague representations, and the vagueness is determined by degrees of uncertainty of association between bottom-up signals and various higher-level representations.</p>
<p>Appendix 1.Bottom-up signals {X(n)} in this simplified discussion, is a field of neuronal synapse activations in visual cortex.Here and below curve brackets {…} denote multiple signals, a field.Index n = 1,… N, enumerates neurons and X(n) are the activation levels.Sources of top-down signals are representations or concept-models {M m (n)} indexed by m = 1,… M. Each model m M m (n) projects a set of priming, top-down signals, representing the bottom-up signals X(n) expected from a particular object, m.Models depend on parameters {S m }, M m (S m ,n).Parameters characterize object position, angles, lightings, etc.(In case of learning situations considered in section 3, parameters characterize objects and relations making up a situation.)To summarize this highly simplified description of a visual system, n enumerates the visual cortex neurons, X(n) are the "bottom-up" activation levels of these neurons coming from the retina, and M m (n) are the "top-down" activation levels (priming) of the visual cortex neurons.The learningperception process "matches" these top-down and bottom-up activations by selecting "best" models and their parameters and the corresponding sets of bottom-up signals.Let us concentrate on defining a mathematical measure of the "best" fit between bottom-up and top-down signals.It is constructed in such a way that any of object-models can be recognized.Correspondingly, a similarity measure is designed so that it treats each object-model as a potential alternative for each subset of signals(Perlovsky 2000(Perlovsky , 2006)),Here, l(X(n)|M m (n)) (or simply l(n|m)) is called a conditional similarity between one signal X(n) and one model M m (n).Parameters r(m) are proportional to the number of objects described by the model m.Expression (1) accounts for all combinations of signals and models in the following way.Sum ∑ ensures that any of the object-models can be considered (by the mind) as a source of signal X(n).Product ∏ ensures that all signals have a chance to be considered (even if one signal is not considered, the entire product is zero, and similarity L is 0; so for good similarity all signals have to be accounted for.This does not assume exorbitant amount of attention to each minute detail: among models there is a vague simple model for "everything else").In a simple case, when all objects are perfectly recognized and separated from each other, there is just one object-model corresponding to each signal (other l(n|m) = 0).In this simple case expression (1) contains just 1 item, a product of all non-zero l(n|m).In the general case, before objects are recognized, L contains a large number of combinations of models and signals; a product over N signals is taken of the sums over M models; this results in a total of M N items; this huge number is the cause for the combinatorial complexity discussed previously.The DL learning process consists in estimating model parameters S m and associating subsets of signals with concepts by maximizing the similarity (1).Although (1) contains combinatorially many items, DL maximizes it without combinatorial complexity(Perlovsky, 1996b;1997;2000).First, fuzzy association variables f(m|n) are defined, f(m|n) = r(m) l(n|m) / ∑ ∈M m' r(m') l(n|m').(A2)These variables give a measure of correspondence between signal X(n) and model M m relative to all other models, m'.They are defined similarly to the a posteriori Bayes probabilities, they range between 0 and 1, and as a result of learning they converge to the probabilities under certain conditions.DL process is defined by the following set of differential equations,
The complete works. The revised Oxford translation. Aristotle , J. Barnes1995Princeton Univ. Press. (Original work IV BCEPrinceton, NJ</p>
<p>Predictions: A universal principle in the operation of the human brain (Introduction). M Bar, Philosophical Transactions R. Soc. B. 3642009Theme issue: Predictions in the brain: Using our past to generate a future</p>
<p>The proactive brain: using analogies and associations to generate predictions. M Bar, Trends in Cognitive Sciences. 1172007</p>
<p>Top-down facilitation of visual recognition. M Bar, K S Kassam, A S Ghuman, J Boshyan, A M Schmid, Dale, 2006Proceedings of the National Academy of Sciences103USA</p>
<p>See it with feeling: Affective predictions in the human brain. L F Barrett, M Bar, Royal Society Phil Trans B. 3642009</p>
<p>Perceptual Symbol Systems. L W Barsalou, Behavioral and Brain Sciences. 221999</p>
<p>Situated simulation in the human conceptual system. L W Barsalou, Language and Cognitive Processes. 2003a18</p>
<p>Abstraction in perceptual symbol systems. L W Barsalou, Philosophical Transactions of the Royal Society of London: Biological Sciences. 3582003b</p>
<p>Abstraction as dynamic interpretation in perceptual symbol systems. L W Barsalou, D Gershkoff-Stowe, Rakison, Embodied Grounding: Social, Cognitive, Affective, and Neuroscientific approaches. Carnegie Sympos. Ser.. Erlbaum Barsalou, L W , Mahwah, NJ; Semin; New YorkCambridge Univ. Press2005. 2007Building Object Categories</p>
<p>Grounded Cognition. L W Barsalou, Annu. Rev. Psychol. 592008</p>
<p>Cognition as coordinated noncognition. L W Barsalou, C Breazeal, L B Smith, Cogn. Process. 82007</p>
<p>Components of conceptual representation: From feature lists to recursive frames. L W Barsalou, C R Hale, Categories and concepts: Theoretical views and inductive data analysis. I Van Mechelen, J Hampton, R Michalski, P Theuns, Academic Press1993</p>
<p>R E Bellman, Adaptive Control Processes. Princeton, NJPrinceton University Press1961</p>
<p>. P P Bonnisone, M Henrion, L N Kanal, J F Lemmer, Uncertainty in Artificial Intelligence. 61991</p>
<p>The roles of body and mind in abstract thought. L Boroditsky, M ; Ramscar, K Smith, S Kirby, Physics of Life Reviews. 1332002. 2002. 2005MIT PressPsychol. Sci.</p>
<p>The need for cognition. J T Cacioppo, R E Petty, Journal of Personality and Social Psychology. 421982</p>
<p>From robotic toil to symbolic theft: grounding transfer from entrylevel to higher-level categories. A Cangelosi, A Greco, S Harnad, Connect. Sci. 122000</p>
<p>An embodied model for sensorimotor grounding and grounding transfer: experiments with epigenetic robots. A Cangelosi, T Riga, Cogn. Sci. 302006</p>
<p>The Logical Syntax of Language. R Carnap, Syntactic Structures. Adams &amp; Littlefield, Co, N Chomsky, Totowa, NJ; Haag, NetherlandsMouton1959. 1957</p>
<p>Principles and Parameters in Syntactic Theory. N ; Chomsky, N Chomsky, Explanation in Linguistics. The Logical Problem of Language Acquisition. N Hornstein, D Lightfoot, New York, NY; Longman, London1972. 1981Language and Mind. Harcourt Brace Javanovich</p>
<p>Saying, seeing and acting: The Psychological Semantics of Spatial Prepositions. A Clark, Coventry, K.R. &amp; Garrod, S.C.1997. 2004Psychology PressCambridge, MA; HoveBeing there: Putting brain, body, and world together again</p>
<p>Perceptual processing affects conceptual processing. S Van Danzig, D Pecher, R Zeelenberg, L W Barsalou, Cognitive Science. 322008</p>
<p>The Symbolic Species: The Co-Evolution of Language and the Brain. T W Deacon, 1998NortonNew York</p>
<p>On the representation of object structure in human vision: evidence from differential priming of shape and location. S Edelman, F Newell, 1998Research paper University of Sussex</p>
<p>On the virtues of going all the way. A commentary on Barsalou's BBS article, Perceptual Symbol Systems. S Edelman, E M Breen, Behavioral and Brain Sciences. 226141999</p>
<p>Towards structural systematicity in distributed, statically bound visual representations. S Edelman, N Intrator, Cognitive Science. 272001</p>
<p>But will it scale up? Not without representation. S Edelman, Adaptive Behavior. 1142003</p>
<p>Mental spaces: Aspects of meaning construction in natural language. G Fauconnier, L Festinger, 1985. 1957PetersonCambridge, MA; Evanston, ILA Theory of Cognitive Dissonance</p>
<p>J Fiske, Film, TV and the Popular. Hanet Bell, 198712</p>
<p>Evolving Compositionality in Evolutionary Language Games. J F Fontanari, L I Perlovsky, 10.1109/TEVC.2007.892763IEEE Transactions on Evolutionary Computations. 1162007</p>
<p>How language can help discrimination in the Neural Modeling Fields framework. J F Fontanari, L I Perlovsky, Neural Networks. 212-32008a</p>
<p>A game theoretical approach to the evolution of structured communication codes. J F Fontanari, L I Perlovsky, Theory in Biosciences. 1272008b</p>
<p>Cross-situational learning of objectword mapping using Neural Modeling Fields. J F Fontanari, V Tikhanoff, A Cangelosi, R Ilin, L I Perlovsky, 10.1007/s12064-008-0024-1Neural Networks. 222009in press</p>
<p>The ecological approach to visual perception. J J Gibson, Behavioral &amp; Brain Sciences. Houghton Mifflin. Glenberg, A. M.201979. 1997What memory is for</p>
<p>Grounding language in action. A M Glenberg, M Kaschak, Psychonomic Bulletin &amp; Review. 932002</p>
<p>Publications 1929-1936. Kurt Gödel, Collected Works, ; S Feferman, J W DawsonJr, S C Kleene, Shadows of the Mind. R Penrose, New York, NY; Oxford, EnglandOxford University Press1994I</p>
<p>Studies of Mind and Brain. S Grossberg, 1982D.Reidel Publishing CoDordrecht, Holland</p>
<p>Neural dynamics of attentionally modulated Pavlovian conditioning: blocking, inter-stimulus interval, and secondary reinforcement. S Grossberg, D S Levine, Psychobiology. 1531987</p>
<p>Mice, Monkeys, Men, and Motives. Harry Harlow, Psychological Review. 601953</p>
<p>The faculty of language: what is it, who has it, and how did it evolve?. M D Hauser, N Chomsky, W T Fitch, Science. 2982002</p>
<p>The foundations of mathematics. David Hilbert, From Frege to Gödel. J Van Heijenoort, Cambridge, MAHarvard University Press1928/1967475</p>
<p>Cognitively inspired neural network for recognition of situations. R Ilin, L I Perlovsky, International Journal of Natural Computing Research. 12009in print</p>
<p>On the foundations of perceptual symbol systems: Specifying embodied representations via connectionism. D Joyce, L Richards, A Cangelosi, K R Coventry, Proceedings of the Fifth International Conference on Cognitive Modeling. F Detje, D Dörner, H Schaub, the Fifth International Conference on Cognitive Modeling2003Universitätsverlag BambergThe Logic of Cognitive Systems</p>
<p>Psychological Types. C G Jung, the Collected Works. Bollingen Series XX; Princeton, NJPrinceton University Press1921. 19716</p>
<p>Learning and Soft Computing: Support Vector Machines, Neural Networks, and Fuzzy Logic Models (Complex Adaptive Systems). V Kecman, 2001The MIT PressCambridge, MA</p>
<p>Image and Brain. S M Kosslyn, Kosslyn, S. M.1980. 1994MIT PressCambridge, MA; Cambridge, MAImage and mind</p>
<p>What Categories Reveal about the Mind. G Lakoff, 1987University of Chicago PressChicagoWomen, Fire, and Dangerous Things</p>
<p>G Lakoff, M Johnson, Metaphors We Live By. ChicagoUniversity of Chicago Press1980</p>
<p>Philosophy in the flesh: The embodied mind and its challenge to western thought. G Lakoff, M Johnson, 1999HarperCollins PublishersNew York</p>
<p>R W Langacker, Foundations of Cognitive Grammar. StanfordStanford University Press19871</p>
<p>Language, Image and Symbol. R W Langacker, 1991Mouton de GruyterThe Hague</p>
<p>Simplifying heuristics versus careful thinking: scientific analysis of millennial spiritual issues. D S Levine, L I Perlovsky, Zygon. 432008</p>
<p>Human Language and our Reptilian Brain: The Subcortical Bases of Speech, Syntax, and Thought. P Lieberman, 2000Harvard University PressCambridge, MA</p>
<p>R Linnehan, Mutz, L I Perlovsky, C Weijers, B Schindler, J Brockett, R , Detection of Patterns Below Clutter in Images. Int. Conf. Integration of Knowledge Intensive Multi-Agent Systems. B Marchal, Cambridge, MA2003. Oct.1-3, 2003. 20052Theoretical Computer Science &amp; the Natural Sciences</p>
<p>A Framework for Representing Knowlege. M L Minsky, The Psychology of Computer Vision. P H Whinston, New YorkMcGraw-Hill Book1975</p>
<p>Description and recognition of curved objects. R Nevatia, T O Binford, Artificial Intelligence. 811977</p>
<p>Grounding cognition: The role of perception and action in memory, language, and thinking. D Pecher, R A Zwaan, 2005Cambridge University PressCambridge, UK</p>
<p>Logic as Semiotic: The Theory of Signs. C S Peirce, The Philosophical Writing of Peirce. Hartshorne Logic, Weiss, Dover, NY; Belknap, Cambridge, MA1897, 1903. 1955Buchler ed.</p>
<p>Cramer-Rao Bounds for the Estimation of Means in a Clustering Problem. R ; Penrose, L I Perlovsky, Shadows of the Mind. Oxford, EnglandPerlovsky, L.I1994. 1987. 1987. 1988a8DARPA Neural Network Study</p>
<p>Cramer-Rao Bounds for the Estimation of Normal Mixtures. L I Perlovsky, Pattern Recognition Letters. 101989b</p>
<p>Maximum Likelihood Neural Networks for Sensor Fusion and Adaptive Classification. L I Perlovsky, M M Mcmanus, Neural Networks. 411991</p>
<p>Estimating a Covariance Matrix from Incomplete Independent Realizations of a Random Vector. L I Perlovsky, T L Marzetta, IEEE Trans. on SP. 4081992</p>
<p>Computational Concepts in Classification: Neural Networks, Statistical Pattern Recognition, and Model Based Vision. L I Perlovsky, Journal of Mathematical Imaging and Vision. 411994a</p>
<p>L I Perlovsky, A Model Based Neural Network for Transient Signal Processing. 1994b7</p>
<p>Application of MLANS to Signal Classification. L I Perlovsky, R P Coons, R L Streit, T E Luginbuhl, S Greineder, Journal of Underwater Acoustics. 4421994</p>
<p>Maximum Likelihood Adaptive Neural Controller. L I Perlovsky, J V Jaskolski, Neural Networks. 741994</p>
<p>Multi-Sensor ATR and Identification Friend or Foe Using MLANS. L I Perlovsky, J A Chernick, W H Schoendorf, Neural Networks. 87/81995</p>
<p>Statistical Quantum Theory and Statistical Pattern Recognition. Current Topics in Pattern Recognition Research. L Garvin, L I Perlovsky, Pandalai. Research Trends. 1995</p>
<p>Concurrent Classification and Tracking Using Maximum Likelihood Adaptive Neural System. L I Perlovsky, W H Schoendorf, D M Tye, W Chang, Journal of Underwater Acoustics. 4521995</p>
<p>Cramer-Rao Bound for Tracking in Clutter and Tracking Multiple Objects. L I Perlovsky, Pattern Recognition Letters. 1831997</p>
<p>Physical Concepts of Intellect. L I Perlovsky, Proceedings of Russian Academy of Sciences. Russian Academy of Sciences1997354</p>
<p>Einsteinian Neural Network for Spectrum Estimation. L I Perlovsky, C P Plum, P R Franchi, E J Tichovolsky, D S Choi, B Weijers, Neural Networks. 1091997a</p>
<p>Development of Concurrent Classification and Tracking for Active Sonar. L I Perlovsky, W H Schoendorf, L C Garvin, Chang, Journal of Underwater Acoustics. 4721997b</p>
<p>Model-Based Neural Network for Target Detection in SAR Images. L I Perlovsky, W H Schoendorf, B J Burdick, D M Tye, IEEE Trans. on Image Processing. 611997c</p>
<p>Cramer-Rao Bound for Tracking in Clutter. L I Perlovsky, Probabilistic Multi-Hypothesis Tracking. R L Streit, Newport, RINUWC Press1998a</p>
<p>Improved ROTHR Detection and Tracking using MLANS. L I Perlovsky, V H Webb, S A Bradley, C Hansen, Probabilistic Multi-Hypothesis Tracking. R L Streit, Newport, RINUWC Press1998a</p>
<p>Conundrum of Combinatorial Complexity. L I Perlovsky, IEEE Trans. PAMI. 2061998b</p>
<p>Improved ROTHR Detection and Tracking Using MLANS. L I Perlovsky, V H Webb, S R Bradley, C A Hansen, AGU Radio Science. 3341998b</p>
<p>Beauty and mathematical Intellect. L I Perlovsky, Zvezda, 2000. 2000Russian</p>
<p>L I Perlovsky, Neural Networks and Intellect: using model-based concepts. New York, NYOxford University Press20013rd printing</p>
<p>Physical Theory of Information Processing in the Mind: concepts and emotions. L I Perlovsky, SEED On Line Journal. 222002a. 2002</p>
<p>Statistical Limitations on Molecular Evolution. L I Perlovsky, Journal of Biomolecular Structure &amp; Dynamics. 1962002b</p>
<p>Aesthetics and mathematical theories of intellect. L I Perlovsky, Iskusstvoznanie. 2/022002cRussian</p>
<p>Integrating Language and Cognition. L I Perlovsky, IEEE Connections, Feature Article. 222004</p>
<p>Solvable null model for the distribution of word frequencies. J F Fontanari, L I Perlovsky, Physical Review E. 70429012004. 2004</p>
<p>Evolving Agents: Communication and Cognition. L I Perlovsky, Autonomous Intelligent Systems: Agents and Data Mining. V Gorodetsky, J Liu, V A Skormin, Springer-Verlag GmbH2005</p>
<p>A Mathematical Theory for Learning, and its Application to Time-varying Computed Tomography. R Deming, L I Perlovsky, New Math. and Natural Computation. 112005</p>
<p>Toward Physics of the Mind: Concepts, Emotions, Consciousness, and Symbols. L I Perlovsky, Phys. Life Rev. 312006a</p>
<p>Fuzzy Dynamic Logic. L I Perlovsky, New Math. and Natural Computation. 212006b</p>
<p>Music -The First Priciple. L I Perlovsky, Musical Theatre. 2006c</p>
<p>Language and cognition integration through modeling field theory: category formation for symbol grounding. V Tikhanoff, J F Fontanari, A Cangelosi, L I Perlovsky, Book Series in Computer Science. HeidelbergSpringer20064131</p>
<p>Concurrent multi-target localization, data association, and navigation for a swarm of flying sensors. R W Deming, L I Perlovsky, Information Fusion. 82007</p>
<p>Neural Networks for Improved Tracking. L I Perlovsky, R W Deming, IEEE Trans. Neural Networks. 1862007</p>
<p>Cognitive high level information fusion. L I Perlovsky, Information Sciences. 1772007a</p>
<p>Evolving Compositionality in Evolutionary Language Games. J F Fontanari, L I Perlovsky, 10.1109/TEVC.2007.892763IEEE Transactions on Evolutionary Computations. 1162007</p>
<p>On the Design of SAR Apertures Using the Cram´er-Rao Bound. Robert Linnehan, David Brady, John Schindler, Leonid Perlovsky, IEEE Transactions on Aerospace and Electronic Systems. 4312007Muralidhar Rangaswamy</p>
<p>Evolution of Languages, Consciousness, and Cultures. L I Perlovsky, IEEE Computational Intelligence Magazine. 232007b</p>
<p>L I Perlovsky, The Mind vs. Logic: Aristotle and Zadeh. Society for Mathematics of Uncertainty. 2007c1</p>
<p>Neurodynamics of Higher-Level Cognition and Consciousness. L I Perlovsky, R Kozma, Eds, 2007aSpringer-VerlagHeidelberg, Germany</p>
<p>Modeling Field Theory of Higher Cognitive Functions. L I Perlovsky, Artificial Cognition Systems. A Loula, R Gudwin, Hershey, PA2007d</p>
<p>Symbols: Integrated Cognition and Language. L I Perlovsky, J. Queiroz. Idea Group. R. Gudwin2007e</p>
<p>Neural Networks, Fuzzy Models and Dynamic Logic. L I Perlovsky, Aspects of Automatic Text Analysis (Festschrift in Honor of Burghard Rieger). R Köhler, A Mehler, GermanySpringer2007f</p>
<p>Editorial -Neurodynamics of Cognition and Consciousness. L Perlovsky, R Kozma, Neurodynamics of Cognition and Consciousness. L Perlovsky, R Kozma, Heidelberg, GermanySpringer Verlag2007b</p>
<p>Neural Dynamic Logic of Consciousness: the Knowledge Instinct. L I Perlovsky, 978-3-540-73266-2Neurodynamics of Higher-Level Cognition and Consciousness. L I Perlovsky, R Kozma, Isbn, Heidelberg, GermanySpringer Verlag2007g</p>
<p>Sapience, Consciousness, and the Knowledge Instinct. (Prolegomena to a Physical Theory. L I Perlovsky, Sapient Systems. R Mayorga, L I Perlovsky, LondonSpringer2008a</p>
<p>. L I Perlovsky, R Mayorga, Mayorga, R., Perlovsky, L.I.2008SpringerLondon</p>
<p>How language can help discrimination in the Neural Modeling Fields framework. J F Fontanari, L I Perlovsky, Neural Networks. 212-32008a</p>
<p>A game theoretical approach to the evolution of structured communication codes. J F Fontanari, L I Perlovsky, 10.1007/s12064-008-0024-1Theory in Biosciences. 1272008b</p>
<p>Neuroscientific Insights on Biblical Myths: Simplifying Heuristics versus Careful Thinking: Scientific Analysis of Millennial Spiritual Issues. D S Levine, L I Perlovsky, Journal of Science and Religion. 4342008Zygon</p>
<p>R Mayorga, L I Perlovsky, Sapient Systems. London, UK.Springer2008</p>
<p>Music and Consciousness. L I Perlovsky, Sciences and Technology. 4142008bJournal of Arts</p>
<p>Dynamic Logic of Phenomena and Cognition. IJCNN. B Kovalerchuk, L I Perlovsky, 2008. 2008Hong Kong</p>
<p>Multitarget/Multisensor Tracking using only Range and Doppler Measurements. R Deming, J Schindler, L Perlovsky, IEEE Transactions on Aerospace and Electronic Systems. 4522009</p>
<p>Language and Cognition. L I Perlovsky, 10.1016/j.neunet.2009.03.007Neural Networks. 2232009a</p>
<p>Language and Emotions: Emotional Sapir-Whorf Hypothesis. L I Perlovsky, 10.1016/j.neunet.2009.06.034Neural Networks. 225-62009b</p>
<p>Modeling goal-oriented decision making through cognitive phase transitions. R Kozma, M Puljic, L Perlovsky, New Mathematics and Natural Computation. 512009</p>
<p>Dynamic Logic of Phenomena and Cognition. IJCNN. B Kovalerchuk, L I Perlovsky, 2009. 2009AtlantaSubmitted for journal publication</p>
<p>Cross-situational learning of object-word mapping using Neural Modeling Fields. F J Fontanari, V Tikhanoff, A Cangelosi, R Ilin, L I Perlovsky, Neural Networks. 225-62009</p>
<p>Vague-to-Crisp' Neural Mechanism of Perception. L I Perlovsky, IEEE Trans. Neural Networks. 2082009c</p>
<p>Musical emotions: Functions, origin, evolution. L I Perlovsky, 10.1016/j.plrev.2009.11.001Physics of Life Reviews. 712010a</p>
<p>L I Perlovsky, 10.1037/a0018147Intersections of Mathematical, Cognitive, and Aesthetic Theories of Mind, Psychology of Aesthetics, Creativity, and the Arts. 2010b4</p>
<p>L I Perlovsky, Aristotle, Zadeh, Trans, Neural Mechanisms of the Mind. 2010c21</p>
<p>The Mind is not a Kludge. L I Perlovsky, Skeptic. 1532010d</p>
<p>Cognitively Inspired Neural Network for Recognition of Situations. R Ilin, L I Perlovsky, International Journal of Natural Computing Research. 112010</p>
<p>Neurally and Mathematically Motivated Architecture for Language and Thought. Special Issue "Brain and Language Architectures: Where We are Now?. L I Perlovsky, R Ilin, The Open Neuroimaging Journal. 42010a</p>
<p>Computational Foundations for Perceptual Symbol System. L I Perlovsky, R Ilin, 2010b2010Barcelona, arXive</p>
<p>Science and Religion: Scientific Understanding of Emotions of Religiously Sublime. L I Perlovsky, M.-C Bonniot-Cabanac, M Cabanac, L I Perlovsky, Wall Street Journal. Perlovsky, L.I.30092010. 2010e. 2010f. June 27Jihadism and Grammars. Comment to "Lost in Translation</p>
<p>Joint Acquisition of Language and Cognition. L I Perlovsky, WebmedCentral BRAIN. 110C009942010</p>
<p>Dynamic logic, computational complexity, engineering and mathematical breakthroughs. L I Perlovsky, L I Perlovsky, Dynamic Logic. Cognitive Breakthroughs. 2010. 2010. 2010Dynamic logic</p>
<p>Mind mechanisms: concepts, emotions, instincts, imagination, intuition, beautiful, spiritually sublime. L I Perlovsky, 2010</p>
<p>Language and Cognition. L I Perlovsky, Interaction Mechanisms. 2010</p>
<p>Languages and Cultures: Emotional Sapir-Whorf Hypothesis. L I Perlovsky, 2010</p>
<p>Jihad and Arabic Language. L I Perlovsky, How Cognitive Science Can Help Us Understand the War on Terror. 2010</p>
<p>Music and Emotions. Functions. L I Perlovsky, Origins. 2010</p>
<p>Beauty and art. Cognitive function and evolution. L I Perlovsky, 2010</p>
<p>L I Perlovsky, New "Arrow of Time": Evolution increases knowledge. 2010</p>
<p>L I Perlovsky, Science and Religion: New cognitive findings bridge the fundamental gap. 2010</p>
<p>The Stuff of Thought: Language as a Window into Human Nature. S Pinker, 1994. 2008PenguinNew York; New YorkThe language instinct: How the mind creates language</p>
<p>D Richardson, M Spivey, L Barsalou, K Mcrae, Spatial representations activated during real-time comprehension of verbs, Cogntive Science. 2010in press</p>
<p>The similarity-in-topography principle: reconciling theories of conceptual deficits. B Russell, D L Schacter, D R Addis, Introduction to Mathematical Philosophy. W K Simmons, L W Barsalou, London; New York, NYMcGraw-Hill1919. 1916/1965. 2007. 2003445Nature</p>
<p>Derivation and Evaluation of Improved Tracking Filters for Use in Dense Multitarget Environments. R A Singer, R G Sea, R B Housewright, IEEE Transactions on Information Theory. 201974</p>
<p>Principles of object perception. E Spelke, Cognitive Science. 141990</p>
<p>The relation of grammar to cognition. L Talmy, L.Topics in Cognitive Linguistics. Amsterdam: Benjamins. Rudzka-Ostyn, New YorkPlenum Press Talmy1983. 1988How language structures space</p>
<p>Constructing a Language, A Usage-Based Theory of Language Acquisition. M Tomasello, 2003Harvard University PressCambridge, MA</p>
<p>Perceptual simulation in conceptual combination: Evidence from property generation. P H Winston, M A Reading, L Wu, L W Barsalou, Acta Psychologica. 1984. 2009Addison-WesleyArtificial Intelligence. in print</p>
<p>The situated nature of concepts. W Yeh, L W Barsalou, Am. J. Psychol. 1192006</p>
<p>. L A Zadeh, Fuzzy Sets. Information and Control. 81965</p>            </div>
        </div>

    </div>
</body>
</html>
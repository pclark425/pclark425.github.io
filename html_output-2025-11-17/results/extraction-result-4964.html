<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4964 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4964</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4964</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-7d97c17a75beb89f938eaac1d3ca60ac2245fb2e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7d97c17a75beb89f938eaac1d3ca60ac2245fb2e" target="_blank">Faith and Fate: Limits of Transformers on Compositionality</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.</p>
                <p><strong>Paper Abstract:</strong> Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\,increased\,task\,complexity.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4964.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4964.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑4 (zero-shot / off-the-shelf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer large language model evaluated off-the-shelf (zero-shot) on compositional reasoning tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLM from OpenAI (referred to as gpt-4 in the paper); evaluated in zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>zero-shot generation (implicit reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Model is given the problem without chain-of-thought/scratchpad examples and must produce an answer in a single generation (no explicit diversity or ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-digit multiplication (and other compositional tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compositional tasks requiring multi-step reasoning, e.g., long-form multi-digit multiplication, logic grid puzzles (Einstein puzzles), and a dynamic programming (DP) problem.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On 3-digit-by-3-digit multiplication: reported ≈59% accuracy (off-the-shelf); more generally, zero-shot accuracy drops toward near-zero as task complexity (depth/parallelism) increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Few-shot or scratchpad prompting improves performance relative to zero-shot in low-complexity cases but still collapses to near-zero as complexity increases; finetuning yields near-perfect in-distribution but fails OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Off-the-shelf GPT-4 obtains moderate accuracy on some small instances (e.g., ~59% on 3x3 multiplication) but exhibits rapid degradation as compositional complexity increases; providing no explicit reasoning steps leads the model to rely on surface patterns and shallow pattern matching.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although zero-shot can achieve nontrivial accuracy on some medium-size instances, it fails systematically on higher-depth/width instances; no evidence that zero-shot alone robustly generalizes to OOD compositional complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faith and Fate: Limits of Transformers on Compositionality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4964.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4964.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT_zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT‑3.5‑turbo (off-the-shelf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive dialogue-optimized transformer LLM (GPT‑3.5‑turbo) evaluated off-the-shelf on compositional reasoning tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-3.5-turbo / ChatGPT family; autoregressive transformer evaluated in zero-shot and few-shot contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>zero-shot generation (implicit reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single-pass answer generation without explicit chain-of-thought or multi-path ensembles; relies on learned mapping from input to output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-digit multiplication (and other compositional tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of compositional tasks (long-form multiplication, logic puzzles, DP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On 3-digit-by-3-digit multiplication: reported ≈55% accuracy (off-the-shelf); overall zero-shot accuracy falls towards near-zero as task complexity increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Few-shot, scratchpad prompting, and finetuning can improve in-distribution performance but similarly fail to generalize to higher-complexity OOD examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ChatGPT achieves moderate correctness on some moderate-size arithmetic examples but shares the same failure mode: performance rapidly degrades as compositional depth/parallelism increases; the model tends to exploit surface correlations rather than systematic multi-step algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite some nontrivial accuracy on moderate instances, ChatGPT frequently produces incorrect intermediate computations and its final accuracy collapses for larger compositional problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faith and Fate: Limits of Transformers on Compositionality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4964.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4964.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_zero/few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑3 (text-davinci-003) in zero-shot / few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's text-davinci-003 model evaluated in zero-shot and in-context few-shot prompting on compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (text-davinci-003) used in zero-shot and few-shot settings; baseline for many experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>few-shot prompting and zero-shot prompting (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Standard few-shot prompting (a small set of demonstration QA pairs, sometimes without scratchpads) or zero-shot; single-chain responses without ensemble/diverse sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiplication, Einstein puzzles, Dynamic Programming (DP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compositional reasoning tasks instantiated as computation graphs, varying depth and parallelism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot and few-shot: accuracy decreases from near-perfect on very small / trivial instances to near-zero as problem size, depth, or average parallelism increases (qualitative trend shown across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Prompting with explicit scratchpads (chain-of-thought style) provides a boost over zero-shot but still fails to generalize to high-depth/width OOD cases; exhaustive finetuning gives near-perfect in-distribution but poor OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Zero- and few-shot prompting cause models to exploit surface heuristics; few-shot provides some benefit but does not remedy the collapse in performance for higher compositional complexity. No evidence that simple in-context demonstrations alone produce systematic algorithmic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Few-shot prompting can temporarily boost performance on low-complexity examples, but performance still decays sharply with increased depth/parallelism — i.e., few-shot is not sufficient for robust compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faith and Fate: Limits of Transformers on Compositionality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4964.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4964.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_finetune_QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑3 finetuned on question-answer pairs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT‑3 models exhaustively finetuned on task-specific (question→answer) data for each compositional task to measure in-distribution and OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003) finetuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 finetuned separately on large enumerations of task-specific input→output pairs (e.g., ~1.8M multiplication pairs), trained to map problem text to final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>supervised finetuning on question→answer pairs (implicit reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Supervised training to predict the final answer directly from the problem text (no explicit intermediate steps); the model can implicitly memorize mappings and surface patterns present in training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiplication, DP, Puzzles (task-specific datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Exhaustive enumerations of task instances up to chosen sizes (e.g., multiplication enumerations limited by budget).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In-distribution (examples/splits seen during training): near-perfect accuracy. Out-of-distribution (splits unseen during training, greater depth/width): performance sharply declines toward zero.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Finetuning with explicit scratchpads achieves similar in-distribution performance but also fails to generalize OOD; few-shot and zero-shot are much weaker in-domain and OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Finetuning yields near-perfect performance on training-distribution instances but does not produce algorithmic mastery: models fail to generalize to unseen computation-graph depths/widths, indicating learning is driven by pattern matching / memorization rather than systematic composition.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Extensive finetuning beyond overfitting ('grokking' style extended training) did not produce improved OOD generalization for multiplication in the experiments (no emergence of grokking observed in this setting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faith and Fate: Limits of Transformers on Compositionality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4964.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4964.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_finetune_scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑3 finetuned on question→scratchpad pairs (explicit chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT‑3 finetuned to generate explicit intermediate computation steps (scratchpads) before or alongside final answers, to teach the model step-by-step procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003) finetuned (scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 finetuned on (problem, verbalized computation graph / scratchpad, final answer) pairs to explicitly supervise stepwise reasoning generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>scratchpad / chain-of-thought fine-tuning (single-chain explicit reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Training the model to produce a linearized topological ordering of the computation graph (explicit intermediate steps) before the final answer; a single reasoning path per example (no ensembling/diverse sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiplication, Einstein puzzles, DP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same compositional tasks represented as computation graphs; scratchpads correspond to linearized step sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In-distribution: near-perfect. Out-of-distribution (wider/deeper graphs): model fails to generalize and accuracy drops to near-zero; scratchpad training improves in-domain behavior but does not prevent OOD collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Question→answer finetuning yields similar in-distribution performance; scratchpad training improves interpretability and occasionally accuracy in few-shot contexts but does not fix compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit supervision of intermediate steps (scratchpads) helps on in-distribution examples and can temporarily boost few-shot performance, but does not endow models with systematic compositional reasoning; models still rely on pattern matching of subgraphs seen during training and fail on unseen computation-graph structures.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with explicit scratchpad supervision, many final correct answers are produced despite incorrect intermediate steps (restoration errors), indicating memorization of input→output mappings rather than correct multi-step computation. Extended training beyond overfitting did not produce OOD improvements ('grokking' not observed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faith and Fate: Limits of Transformers on Compositionality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4964.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4964.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_few-shot_scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT‑4 few-shot with scratchpad prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT‑4 evaluated with few-shot prompts that include explicit scratchpads / step-by-step demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 evaluated in few-shot settings with scratchpad (chain-of-thought) examples included in the prompt to elicit stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>few-shot + scratchpad prompting (single-chain explicit reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>In-context demonstrations that include linearized computation graphs / scratchpads; the model generates a stepwise solution conditioned on those examples (single reasoning path per prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiplication, DP, Puzzles (error analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compositional tasks represented as computation graphs, used in node-level error analysis (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Scratchpad few-shot increases instance-level correctness relative to pure zero-shot for low-complexity examples, but ratio of fully-correct internal nodes declines sharply with layer depth and overall OOD performance still collapses as complexity increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Finetuned GPT-3 with scratchpad achieves near-perfect in-distribution but similarly fails OOD; few-shot GPT-4 has better scratchpad elicitation but still suffers error propagation at increasing depths.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Few-shot scratchpads help GPT-4 perform single-step operations correctly but do not prevent propagation errors across layers; models show high rates of propagation (and restoration) errors as graph depth increases, indicating inability to systematically compose steps even when shown examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although few-shot scratchpads improve surface performance, they do not produce robust multi-step generalization; many nodes are correct only locally, and overall correctness decays with depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faith and Fate: Limits of Transformers on Compositionality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4964.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4964.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse_demonstrations_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diverse demonstrations / diverse in‑prompt examples (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) showing that using diverse demonstrations in-context can improve compositional generalization; mentioned in related work but not empirically tested here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diverse demonstrations improve in-context compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>diverse in-context demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Using varied and diverse exemplars in the prompt to expose multiple patterns and reduce shortcut/memorization behavior, thereby improving in-context compositional generalization (as reported in prior work cited by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>In-context compositional generalization (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Approach aimed at improving model generalization by diversifying the distribution of in-prompt examples to cover more local patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper cites prior work that suggests diversity of demonstrations can improve in-context compositional generalization, but this study did not empirically evaluate diverse multi-path or ensembling methods (e.g., multiple reasoning paths, self-consistency) for the compositional tasks here.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Not evaluated in this paper; authors note that approaches like diversifying prompts or planning modules are promising directions but were not part of their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faith and Fate: Limits of Transformers on Compositionality', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Diverse demonstrations improve in-context compositional generalization <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Grokking: Generalization beyond overfitting on small algorithmic datasets <em>(Rating: 1)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4964",
    "paper_id": "paper-7d97c17a75beb89f938eaac1d3ca60ac2245fb2e",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "GPT-4_zero-shot",
            "name_full": "GPT‑4 (zero-shot / off-the-shelf)",
            "brief_description": "An autoregressive transformer large language model evaluated off-the-shelf (zero-shot) on compositional reasoning tasks in this study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Autoregressive transformer LLM from OpenAI (referred to as gpt-4 in the paper); evaluated in zero-shot and few-shot settings.",
            "reasoning_method_name": "zero-shot generation (implicit reasoning)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Model is given the problem without chain-of-thought/scratchpad examples and must produce an answer in a single generation (no explicit diversity or ensembling).",
            "task_name": "Multi-digit multiplication (and other compositional tasks)",
            "task_description": "Compositional tasks requiring multi-step reasoning, e.g., long-form multi-digit multiplication, logic grid puzzles (Einstein puzzles), and a dynamic programming (DP) problem.",
            "performance": "On 3-digit-by-3-digit multiplication: reported ≈59% accuracy (off-the-shelf); more generally, zero-shot accuracy drops toward near-zero as task complexity (depth/parallelism) increases.",
            "comparison_with_other_method": true,
            "performance_other_method": "Few-shot or scratchpad prompting improves performance relative to zero-shot in low-complexity cases but still collapses to near-zero as complexity increases; finetuning yields near-perfect in-distribution but fails OOD.",
            "key_findings": "Off-the-shelf GPT-4 obtains moderate accuracy on some small instances (e.g., ~59% on 3x3 multiplication) but exhibits rapid degradation as compositional complexity increases; providing no explicit reasoning steps leads the model to rely on surface patterns and shallow pattern matching.",
            "counter_examples_or_negative_results": "Although zero-shot can achieve nontrivial accuracy on some medium-size instances, it fails systematically on higher-depth/width instances; no evidence that zero-shot alone robustly generalizes to OOD compositional complexity.",
            "uuid": "e4964.0",
            "source_info": {
                "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT_zero-shot",
            "name_full": "ChatGPT / GPT‑3.5‑turbo (off-the-shelf)",
            "brief_description": "An autoregressive dialogue-optimized transformer LLM (GPT‑3.5‑turbo) evaluated off-the-shelf on compositional reasoning tasks in this study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-turbo)",
            "model_description": "OpenAI's GPT-3.5-turbo / ChatGPT family; autoregressive transformer evaluated in zero-shot and few-shot contexts.",
            "reasoning_method_name": "zero-shot generation (implicit reasoning)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single-pass answer generation without explicit chain-of-thought or multi-path ensembles; relies on learned mapping from input to output.",
            "task_name": "Multi-digit multiplication (and other compositional tasks)",
            "task_description": "Same set of compositional tasks (long-form multiplication, logic puzzles, DP).",
            "performance": "On 3-digit-by-3-digit multiplication: reported ≈55% accuracy (off-the-shelf); overall zero-shot accuracy falls towards near-zero as task complexity increases.",
            "comparison_with_other_method": true,
            "performance_other_method": "Few-shot, scratchpad prompting, and finetuning can improve in-distribution performance but similarly fail to generalize to higher-complexity OOD examples.",
            "key_findings": "ChatGPT achieves moderate correctness on some moderate-size arithmetic examples but shares the same failure mode: performance rapidly degrades as compositional depth/parallelism increases; the model tends to exploit surface correlations rather than systematic multi-step algorithms.",
            "counter_examples_or_negative_results": "Despite some nontrivial accuracy on moderate instances, ChatGPT frequently produces incorrect intermediate computations and its final accuracy collapses for larger compositional problems.",
            "uuid": "e4964.1",
            "source_info": {
                "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3_zero/few-shot",
            "name_full": "GPT‑3 (text-davinci-003) in zero-shot / few-shot prompting",
            "brief_description": "OpenAI's text-davinci-003 model evaluated in zero-shot and in-context few-shot prompting on compositional tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003)",
            "model_description": "Autoregressive transformer (text-davinci-003) used in zero-shot and few-shot settings; baseline for many experiments in the paper.",
            "reasoning_method_name": "few-shot prompting and zero-shot prompting (implicit)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Standard few-shot prompting (a small set of demonstration QA pairs, sometimes without scratchpads) or zero-shot; single-chain responses without ensemble/diverse sampling.",
            "task_name": "Multiplication, Einstein puzzles, Dynamic Programming (DP)",
            "task_description": "Compositional reasoning tasks instantiated as computation graphs, varying depth and parallelism.",
            "performance": "Zero-shot and few-shot: accuracy decreases from near-perfect on very small / trivial instances to near-zero as problem size, depth, or average parallelism increases (qualitative trend shown across tasks).",
            "comparison_with_other_method": true,
            "performance_other_method": "Prompting with explicit scratchpads (chain-of-thought style) provides a boost over zero-shot but still fails to generalize to high-depth/width OOD cases; exhaustive finetuning gives near-perfect in-distribution but poor OOD generalization.",
            "key_findings": "Zero- and few-shot prompting cause models to exploit surface heuristics; few-shot provides some benefit but does not remedy the collapse in performance for higher compositional complexity. No evidence that simple in-context demonstrations alone produce systematic algorithmic generalization.",
            "counter_examples_or_negative_results": "Few-shot prompting can temporarily boost performance on low-complexity examples, but performance still decays sharply with increased depth/parallelism — i.e., few-shot is not sufficient for robust compositional generalization.",
            "uuid": "e4964.2",
            "source_info": {
                "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3_finetune_QA",
            "name_full": "GPT‑3 finetuned on question-answer pairs",
            "brief_description": "GPT‑3 models exhaustively finetuned on task-specific (question→answer) data for each compositional task to measure in-distribution and OOD generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003) finetuned",
            "model_description": "GPT-3 finetuned separately on large enumerations of task-specific input→output pairs (e.g., ~1.8M multiplication pairs), trained to map problem text to final answer.",
            "reasoning_method_name": "supervised finetuning on question→answer pairs (implicit reasoning)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Supervised training to predict the final answer directly from the problem text (no explicit intermediate steps); the model can implicitly memorize mappings and surface patterns present in training.",
            "task_name": "Multiplication, DP, Puzzles (task-specific datasets)",
            "task_description": "Exhaustive enumerations of task instances up to chosen sizes (e.g., multiplication enumerations limited by budget).",
            "performance": "In-distribution (examples/splits seen during training): near-perfect accuracy. Out-of-distribution (splits unseen during training, greater depth/width): performance sharply declines toward zero.",
            "comparison_with_other_method": true,
            "performance_other_method": "Finetuning with explicit scratchpads achieves similar in-distribution performance but also fails to generalize OOD; few-shot and zero-shot are much weaker in-domain and OOD.",
            "key_findings": "Finetuning yields near-perfect performance on training-distribution instances but does not produce algorithmic mastery: models fail to generalize to unseen computation-graph depths/widths, indicating learning is driven by pattern matching / memorization rather than systematic composition.",
            "counter_examples_or_negative_results": "Extensive finetuning beyond overfitting ('grokking' style extended training) did not produce improved OOD generalization for multiplication in the experiments (no emergence of grokking observed in this setting).",
            "uuid": "e4964.3",
            "source_info": {
                "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3_finetune_scratchpad",
            "name_full": "GPT‑3 finetuned on question→scratchpad pairs (explicit chain-of-thought)",
            "brief_description": "GPT‑3 finetuned to generate explicit intermediate computation steps (scratchpads) before or alongside final answers, to teach the model step-by-step procedures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003) finetuned (scratchpad)",
            "model_description": "GPT-3 finetuned on (problem, verbalized computation graph / scratchpad, final answer) pairs to explicitly supervise stepwise reasoning generation.",
            "reasoning_method_name": "scratchpad / chain-of-thought fine-tuning (single-chain explicit reasoning)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Training the model to produce a linearized topological ordering of the computation graph (explicit intermediate steps) before the final answer; a single reasoning path per example (no ensembling/diverse sampling).",
            "task_name": "Multiplication, Einstein puzzles, DP",
            "task_description": "Same compositional tasks represented as computation graphs; scratchpads correspond to linearized step sequences.",
            "performance": "In-distribution: near-perfect. Out-of-distribution (wider/deeper graphs): model fails to generalize and accuracy drops to near-zero; scratchpad training improves in-domain behavior but does not prevent OOD collapse.",
            "comparison_with_other_method": true,
            "performance_other_method": "Question→answer finetuning yields similar in-distribution performance; scratchpad training improves interpretability and occasionally accuracy in few-shot contexts but does not fix compositional generalization.",
            "key_findings": "Explicit supervision of intermediate steps (scratchpads) helps on in-distribution examples and can temporarily boost few-shot performance, but does not endow models with systematic compositional reasoning; models still rely on pattern matching of subgraphs seen during training and fail on unseen computation-graph structures.",
            "counter_examples_or_negative_results": "Even with explicit scratchpad supervision, many final correct answers are produced despite incorrect intermediate steps (restoration errors), indicating memorization of input→output mappings rather than correct multi-step computation. Extended training beyond overfitting did not produce OOD improvements ('grokking' not observed).",
            "uuid": "e4964.4",
            "source_info": {
                "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4_few-shot_scratchpad",
            "name_full": "GPT‑4 few-shot with scratchpad prompting",
            "brief_description": "GPT‑4 evaluated with few-shot prompts that include explicit scratchpads / step-by-step demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 evaluated in few-shot settings with scratchpad (chain-of-thought) examples included in the prompt to elicit stepwise reasoning.",
            "reasoning_method_name": "few-shot + scratchpad prompting (single-chain explicit reasoning)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "In-context demonstrations that include linearized computation graphs / scratchpads; the model generates a stepwise solution conditioned on those examples (single reasoning path per prompt).",
            "task_name": "Multiplication, DP, Puzzles (error analysis)",
            "task_description": "Compositional tasks represented as computation graphs, used in node-level error analysis (Figure 7).",
            "performance": "Scratchpad few-shot increases instance-level correctness relative to pure zero-shot for low-complexity examples, but ratio of fully-correct internal nodes declines sharply with layer depth and overall OOD performance still collapses as complexity increases.",
            "comparison_with_other_method": true,
            "performance_other_method": "Finetuned GPT-3 with scratchpad achieves near-perfect in-distribution but similarly fails OOD; few-shot GPT-4 has better scratchpad elicitation but still suffers error propagation at increasing depths.",
            "key_findings": "Few-shot scratchpads help GPT-4 perform single-step operations correctly but do not prevent propagation errors across layers; models show high rates of propagation (and restoration) errors as graph depth increases, indicating inability to systematically compose steps even when shown examples.",
            "counter_examples_or_negative_results": "Although few-shot scratchpads improve surface performance, they do not produce robust multi-step generalization; many nodes are correct only locally, and overall correctness decays with depth.",
            "uuid": "e4964.5",
            "source_info": {
                "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Diverse_demonstrations_mention",
            "name_full": "Diverse demonstrations / diverse in‑prompt examples (related work)",
            "brief_description": "Prior work (cited) showing that using diverse demonstrations in-context can improve compositional generalization; mentioned in related work but not empirically tested here.",
            "citation_title": "Diverse demonstrations improve in-context compositional generalization",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reasoning_method_name": "diverse in-context demonstrations",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Using varied and diverse exemplars in the prompt to expose multiple patterns and reduce shortcut/memorization behavior, thereby improving in-context compositional generalization (as reported in prior work cited by this paper).",
            "task_name": "In-context compositional generalization (general)",
            "task_description": "Approach aimed at improving model generalization by diversifying the distribution of in-prompt examples to cover more local patterns.",
            "performance": null,
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "The paper cites prior work that suggests diversity of demonstrations can improve in-context compositional generalization, but this study did not empirically evaluate diverse multi-path or ensembling methods (e.g., multiple reasoning paths, self-consistency) for the compositional tasks here.",
            "counter_examples_or_negative_results": "Not evaluated in this paper; authors note that approaches like diversifying prompts or planning modules are promising directions but were not part of their experiments.",
            "uuid": "e4964.6",
            "source_info": {
                "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Diverse demonstrations improve in-context compositional generalization",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Grokking: Generalization beyond overfitting on small algorithmic datasets",
            "rating": 1
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 1
        }
    ],
    "cost": 0.016295999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Faith and Fate: <br> Limits of Transformers on Compositionality</h1>
<p>Nouha Dzir ${ }^{1 <em>}$, Ximing Lu ${ }^{1,2 </em>}$, Melanie Sclar ${ }^{2 *}$,<br>Xiang Lorraine $\mathbf{L i}^{1 \dagger}$, Liwei Jiang ${ }^{1,2 \dagger}$, Bill Yuchen $\mathbf{L i n}^{1 \dagger}$, Peter West ${ }^{1,2}$, Chandra Bhagavatula ${ }^{1}$, Ronan Le Bras ${ }^{1}$, Jena D. Hwang ${ }^{1}$, Soumya Sanyal ${ }^{3}$, Sean Welleck ${ }^{1,2}$, Xiang Ren ${ }^{1,3}$, Allyson Ettinger ${ }^{1,4}$, Zaid Harchaoui ${ }^{1,2}$, Yejin Choi ${ }^{1,2}$<br>${ }^{1}$ Allen Institute for Artificial Intelligence ${ }^{2}$ University of Washington<br>${ }^{3}$ University of Southern California ${ }^{4}$ University of Chicago<br>nouhad@allenai.org, ximinglu@allenai.org, msclar@cs.washington.edu</p>
<h4>Abstract</h4>
<p>Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks-multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with increased task complexity.</p>
<h2>1 Introduction</h2>
<p>"It was the epoch of belief, it was the epoch of incredulity." - Charles Dickens, A Tale of Two Cities
Large-scale transformers such as ChatGPT [57] and GPT4 [58] demonstrate unprecedented capabilities $[57,74,11,15,85]$, even noted as "sparks of AGI" [12]. In stark contrast, the same models sometimes struggle with simple, intuitive tasks [9, 62, 40]. For instance, humans can solve 3-digit by 3-digit multiplication arithmetic after learning basic calculation rules [22, 34]. Yet, off-the-shelf ChatGPT and GPT4 achieve only $55 \%$ and $59 \%$ accuracies on this task, respectively (§3).
The striking discrepancy between the impressive successes of transformer LLMs on seemingly complex tasks and the astonishing failures on seemingly trivial tasks spark critical open questions about how to faithfully interpret their mixed capabilities. Under what conditions do transformers succeed, fail, and why? What types of errors do they make? Can transformers uncover implicit problem-solving rules or be taught to follow reasoning paths?
Seeking thorough answers to these questions remains an open research challenge. However, we offer novel insights into the fundamental limits of transformers ${ }^{2}$, centered around compositional</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>problems that require strict multi-hop reasoning to derive correct predictions. Applying step-by-step reasoning is fundamental to human intelligence [69, 68]. These compositional problems present compelling challenges for AI systems as they require combining basic reasoning operations to follow computational paths that arrive at unique correct solutions. In particular, we study three straightforward and flexible representative compositional tasks: long-form multiplication, logic grid puzzles (i.e., Einstein's puzzle [61]), and a classic dynamic programming problem.</p>
<p>We propose two hypotheses. First, transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized path matching. This contrasts with the systematic multi-step reasoning approach that learns to apply underlying computational rules required for building correct answers [71, 37, 27]. Shortcut learning [29] via pattern-matching may yield fast correct answers when similar compositional patterns are available during training but does not allow for robust generalization to uncommon or complex examples. Second, due to error propagation, transformers may have inherent limitations on solving high-complexity compositional tasks that exhibit novel patterns. Errors in the early stages of the computational process can lead to substantial compounding errors in subsequent steps, preventing models from finding correct solutions.</p>
<p>To investigate our hypotheses, we formulate compositional tasks as computation graphs. These graphs break down problem-solving into submodular functional steps, enabling structured measurements of complexity and verbalization of computational steps as input sequences to language models. Moreover, we leverage information gain to predict patterns that models are likely to learn based on the underlying task distribution without the need to perform full computations within the graph.</p>
<p>Empirical results show that training on task-specific data leads to near-perfect performance on indomain instances and under low compositional complexity, but fails drastically on instances outside of this region. This substantial gap suggests that systematic problem-solving capabilities do not emerge from maximum likelihood training [5] on input-output sequences, even when prompted or trained with human-like reasoning steps (i.e., a linearization of computation graphs; §3.1). Models' success can be attributed, in part, to their exposure to training examples sub-graphs that involve the same computations required for solving test examples (see Section 3.2.2) In order to gain a deeper understanding of models' failures, we conduct a comprehensive analysis by decomposing their computation graphs and examining different error types. We find that while models can memorize single-step operations, they fail to compose them into correct reasoning paths, suggesting that they mostly make predictions based on shallow, rote learning rather than a deep, holistic task understanding (§3.2.3). Importantly, we provide theoretical evidence of exponential error accumulation using abstract compositional tasks. All tasks analyzed empirically in this paper are instantiations of these abstractions (§4). We argue that transformers could be inherently limited in solving compositionally complex tasks out-of-the-box ${ }^{3}$.</p>
<p>As transformers continue to make tangible real-world impacts, it is pressing to interpret their remarkable performance critically. Our work takes a realistic look at the limitations of transformers in the context of compositional tasks. To shed light on practical future steps, we identify directions for addressing these limitations, such as using transformers for tasks that could be decomposed into few reasoning steps, tasks where evaluation may afford some leniency, and using transformers in combination with planning modules or refinement methods to improve their generations. To advance language AI, fundamental innovations are required to address or complement these limitations.</p>
<h1>2 Measuring Limitations of Transformers in Compositional Tasks</h1>
<p>Human problem-solving skills can be conceptualized as a graph structure, where each vertex represents a partial solution and the edges represent operators that can be applied to modify these solutions. As we will outline next and illustrate in Figure 1, we use computation graphs and corresponding metrics to methodically evaluate transformers' reasoning abilities.</p>
<h3>2.1 Computation Graph Definition</h3>
<p>Let $A$ be a deterministic algorithm (function), and let $\mathcal{F}<em A_mathbf_x="A(\mathbf{x">{A}$ be a set of primitives (functions) the algorithm uses in its execution. Assuming the inputs $\mathbf{x}$ to algorithm $A$ are given, we define $A(\mathbf{x})$ 's static computation graph $G</em>=(V, E, s, o p)$ is a directed acyclic graph. Nodes $V$})} . G_{A(\mathbf{x})</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Transformation of an algorithm $A$ to its computational graph $G_{A(\mathbf{x})}$. The depicted example is of long-form multiplication algorithm $A$, for inputs $\mathbf{x}=[7,49]$ (i.e. computing $7 \times 49$ ).</p>
<p>represent all variables' values during $A$ 's execution: each node $v \in V$ has a value $s(v) \in \mathbb{R}$ associated. Edges $E$ represent the function arguments involved in some computation: for each nonsource node $v \in V$, let $U=\left{u_{1}, \ldots, u_{j}\right} \subset V^{j}$ be its parent nodes. Then, $s(v)=f\left(u_{1}, \ldots, u_{j}\right)$ for some $f \in \mathcal{F}<em A="A">{A}$. Since each node $v$ is uniquely defined by the computation of a single primitive $f$, we define $o p: V \rightarrow \mathcal{F}</em>$ as $o p(v)=f$.
Let $S \subset V$ be the source nodes of $G_{A(\mathbf{x})}$ and without loss of generality, let $o \in V$ be its sole leaf node. By definition, $S \equiv \mathbf{x}$ and $A(\mathbf{x})=s(o)$, representing the input and output of $A$ respectively.
To be able to train and evaluate a language model's ability to follow algorithm $A$ we must linearize $G_{A(\mathbf{x})}$. Since we only consider autoregressive models, this linearization must also be a topological ordering.</p>
<h1>2.2 Quantifying Compositional Complexity using Graph Metrics</h1>
<p>$A$ 's representation as a computation graph $G_{A(\mathbf{x})}$ enables measuring task complexity from many angles.
We define a node $v \in V$ 's layer number as the length of the longest path from a source node to $v$ in the directed acyclic graph $G_{A(\mathbf{c})}$. We then define the reasoning depth as the largest layer number in the graph. In computation graphs, reasoning depth is a proxy for the maximum level of multi-hop reasoning required to solve the task.
Let $d_{S}: V \rightarrow \mathbb{N}_{0}$ be the shortest distance to any of $G$ 's source nodes $S \subset V$. We define the reasoning width of a graph as the mode of ${d(v): v \in V}$. This metric aims to measure the maximum number of variables required to maintain in parallel during the computation. Relatedly, we also define the average parallelism of a graph as the ratio between $|V|$ and its reasoning depth. This aims to compute the average width in computation through the graph, and not just in its mode.</p>
<h3>2.3 Predicting Surface Patterns through Relative Information Gain</h3>
<p>When evaluating model performance, we may observe partially correct answers even in an overall incorrect response. To understand model strategies in these partial successes, we use Relative Information Gain to predict surface patterns that models are likely to recognize. We represent task $T$ as a distribution $\left(X_{1}, \ldots, X_{n}, Y_{1}, \ldots, Y_{m}\right)$ and measure the amount of (normalized) information gained about an output element $Y_{j}$ by observing a subset of input random variables $X \subset\left{X_{1}, \ldots, X_{n}\right}$ :</p>
<p>$$
\operatorname{RelativeIG}\left(Y_{j}, X\right)=\frac{H\left(Y_{j}\right)-H\left(Y_{j} \mid X\right)}{H\left(Y_{j}\right)} \in[0,1]
$$</p>
<p>RelativeIG may be used to analyze the influence of any node in the computation graph (as defined in $\S 2.1$ ) with respect to a set of its ancestors; in particular, output nodes with respect to input nodes.</p>
<h3>2.4 Exploring Three Representative Compositional Tasks: Definitions</h3>
<p>Multiplication Multi-digit multiplication requires executing operations with numerical symbols based on procedural rules [34]. This task has multiple algorithmic solutions; in constructing computation graphs, we use the well-known $O\left(k_{1} k_{2}\right)$ long-form multiplication algorithm for computing $x \cdot y$, where $x$ has $k_{1} \leq 5$ digits and $y$ has $k_{2} \leq 5$ digits in base 10 . See $\S$ A. 1 for data construction details.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Zero-shot accuracy. Axes refer to problem sizes (number of digits in multiplication, number of houses and attributes in puzzle, and sequence length in the DP task). Transformers' accuracy decreases to near zero as task complexity increases, measuring task complexity by the problem size. (b) Average parallelism negatively correlates with accuracy.</p>
<p>To instantiate $G_{A(\mathbf{x})}$, let $\mathcal{F}_{A}={$ one-digit multiplication, sum, mod 10, carry over, concatenation $}$. Source nodes $S$ are digits of input numbers, leaf node $o$ is the final output, and intermediate nodes $v$ are partial results generated during execution of the long-form multiplication algorithm (see Figure 1).</p>
<p>Einstein's Puzzle Einstein's puzzle is a well-known logic puzzle often used as a benchmark for solving constraint satisfaction problems [61]. It involves a list of houses with different attributes (e.g., owner's name, pets), and the goal is to determine which attributes belong to each house by combining a set of pre-defined natural language clues or constraints. The solution to the puzzle is a matrix of size $K \times M$, where $K$ represents the number of houses and $M$ the number of attributes. As $K$ and $M$ increase, synthesizing different partial solutions that satisfy individual constraints becomes highly compositionally complex. To construct the computation graph, we consider a greedy algorithm that iteratively eliminates possible solutions by filling at least one cell each time. It deterministically fills the cell(s) that requires the minimum number of clues among all current unfilled cells. We refer to this as the elimination function. See §A.2 for examples, data construction, and algorithm details.</p>
<p>To instantiate $G_{A(\mathbf{x})}$, let $\mathcal{F}_{A}={$ elimination function $}$. The source nodes are the clues, all intermediate nodes are partially-filled matrices, and the output node is a fully-filled solution matrix.</p>
<p>Dynamic Programming Problem Dynamic programming (DP) recursively breaks down complex problems into simpler sub-problems, so problems solved using this technique are compositional. We analyze a classic relaxation of the NP-complete Maximum Weighted Independent Set problem [39]: Given a sequence of integers, find a subsequence with the highest sum, such that no two numbers in the subsequence are adjacent in the original sequence. This relaxation may be solved in $O(n)$ time using DP. See the solution in §A.3. In the experiments, we restrict each integer to the $[-5,5]$ range.</p>
<p>To instantiate $G_{A(\mathbf{x})}$, let $\mathcal{F}<em A_mathbf_x="A(\mathbf{x">{A}={$ equals, and, not, indicator function, sum, max $}$. Source nodes are elements of the input list, and the output node is a list that for each element indicates whether it should be selected. We select an $O(n)$ algorithm since $G</em>$ 's size is proportional to $A$ 's complexity.})</p>
<h1>3 Testing the Limits of Transformers: Empirical Evidence</h1>
<p>Experimental Setup To understand the capabilities of LLMs, we evaluate GPT3 (text-davinci-003) [11], ChatGPT (GPT-3.5-turbo) [57] and GPT4 (gpt-4) [58] using zeroshot, few-shot, and finetuning techniques. To enable the generation of computation graphs beyond the final answers, we use the concept of scratchpads [56]. Scratchpads are a verbalization of the computation graphs (i.e., a linearized representation of a topological ordering of $G_{A(\mathbf{x})}$ ). Overall, we consider question-answer and question-scratchpad formats for few-shot and finetuning settings to gauge models' capabilities for learning with and without explicit reasoning. See details of additional models and experimental configurations in §B and examples of scratchpad in §A.</p>
<h3>3.1 Testing the Limits of Transformers with Zero-shot, Few-shot and Finetuning</h3>
<p>Limits of transformers in zero- and few-shot settings To investigate the inherent problem-solving capabilities of LLMs, we begin by analyzing models' zero-shot and few-shot performances on our compositional tasks. As shown in Figure 2, task performances deteriorate significantly from near perfection to zero with increasing complexity when measured by either problem size (Figure 2(a))or</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: GPT3 finetuned exhaustively on taskspecific data up to a certain problem size. The blue region represents the in-distribution examples and the red region refers to OOD examples. The same trend is observed for the puzzle task (See §B.2)
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of training beyond the overfitting point for the multiplication task with the goal of exploring whether OOD generalization capabilities (i.e., grokking) arise.
average parallelism (Figure 2(b)).The trend remains the same for few-shot prompting (see §B.2). These results indicate that pre-training is in fact not sufficient to teach models how to combine basic operations to solve compositional problems, especially as problems grow more complex.</p>
<p>Limits of transformers with question-answer training The limited performance of models may be attributed to the lack of task-specific data during pre-training. To fully bring out models' potentials in solving these tasks, we next exhaustively finetune GPT3 with question-answer pairs. In multiplication and DP, we finetune models with all enumerations of questions up to the maximum problem size ${ }^{4}$ within reasonable training budget, leaving out $10 \%$ for validation and $10 \%$ for testing. In puzzles, we train on a subset of all instances up to $(K, M) \leq(4,4)$ due to combinatorial explosion. We separately finetune GPT3 models on $\sim 1.8 \mathrm{M}$ multiplication pairs, $\sim 142 \mathrm{~K}$ DP pairs, and $\sim 41 \mathrm{~K}$ puzzle pairs (see details in §B.3). Additionally, to examine problems of different complexity, we consider different training splits based on the depth and width of computation graphs.</p>
<p>Figure 3 and Figure 5a show high accuracy for examples with splits seen during training, i.e., indomain. However, the performance sharply declines when evaluating unseen splits during training, i.e., out-of-domain (OOD). Similar trends hold in all tasks (see § B.3), suggesting that systematic problem-solving capabilities do not emerge via exhaustive training on task-specific data.</p>
<p>Limits of transformers with explicit scratchpad training Next, we test whether we can explicitly teach models the required computational operations via scratchpads. To do so, we finetune GPT3 with question-scratchpad pairs for all tasks. We consider the same distribution splits as before. The results, presented in Figure 5b, show that once again GPT3 achieves near-perfect performance on in-distribution, but fails entirely in generalizing to OOD cases-in particular, wider or deeper computation graphs. These results indicate that even when training directly with guidance on the computation steps, models still fail to learn component operations in a generalizable manner. This observation holds for all tasks (See details in § B.4). Similarly, prompting transformers with question-scratchpad pairs enhances the performance compared to the zero-shot setting (refer to § B.5). However, this performance boost diminishes to zero as complexity increases. These findings suggest that the autoregressive characteristic of transformers, which forces them to tackle problems sequentially, presents a fundamental challenge that cannot be resolved by instructing the model to generate a step-by-step solution. Instead, models depend on a greedy process of producing the next word to make predictions without a rigorous global understanding of the task.</p>
<p>Limits of transformers with grokking We explore whether extended training beyond overfitting leads to improved generalization abilities, a phenomenon known as grokking [59, 53]. Due to budget constraints, we only experiment on the multiplication task. Following [53], we fine-tune GPT3 with question-answer pairs for 420 K steps and separately finetune GPT3 with question-scratchpad pairs for 30 K steps. Both models' training far exceeded the point at which in-domain accuracy plateaus ${ }^{5}$. Figure 4 shows no improvement in generalization for OOD cases beyond the overfitting point, even</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: GPT3 finetuning and prompting accuracy on different data splits. Although the in-distribution performance is almost perfect, GPT3 exhibits poor generalization with increasing graph depth and width. Refer to $\S$ B. 3 and $\S$ B. 4 for results on the puzzle and DP tasks.
after extensive training periods. We hypothesize that the absence of grokking may be due to the level of difficulty of the task. We speculate that increased task difficulty significantly impedes learning a well-structured representation, which, according to [47], aligns with achieving grokking. Even if grokking were to emerge through more prolonged training, such an approach would prove inefficient and unscalable. Future work is required to accurately explain when and how grokking occurs.</p>
<h1>3.2 Breaking Down Successes and Failures of Transformers</h1>
<h3>3.2.1 Information Gain Explains Where Transformers Partially Excel</h3>
<p>At times transformers predict partially correct answers even when the overall response is incorrect. We speculate that this may be due to particularities in the task distribution that allow for guessing partial answers without performing the full multi-step reasoning that the task requires.
Using relative information gain (defined in §2.3), we can predict surface patterns that a model is likely to learn and contrast them empirically. For multiplication, relative information gain shows that the first digit (two digits) of the output highly correlates with the first digit (two digits) of each input number (see §C.1). Hence, this spurious pattern is likely to be learned by a model. Similarly, the prediction of the last digit (or two digits) of the output is observed to solely rely on the last digit (or two digits) of each input number. This pattern holds true due to the principles of modulo arithmetic, which ensures the validity of this relationship in all cases. Empirically, we verify that models indeed learn the patterns we predicted and other patterns as well (e.g., order of magnitude of the answer, number of trailing zeros for multiplication) in all the settings with and without scratchpad. See details for multiplication, plus dynamic programming task analysis in §C.
These experiments suggest that if an output element heavily relies on a single or a small set of input features, transformers are likely to recognize such correlation during training and directly map these input features to predict the output element in testing, without going through the rigorous multi-hop reasoning and giving a false illusion of performing compositional reasoning.</p>
<h3>3.2.2 Transformers Reduce Multi-Step Compositional Reasoning into Linearized Subgraph Matching</h3>
<p>We now explore whether models' correct predictions on unseen test data are due to learning the underlying algorithm or, instead, explainable by exposure to similar training examples. We hypothesize that, beyond simple memorization, transformers largely rely on pattern matching for solving these tasks. To test this, we calculate the average frequency with which partial computations needed to solve an instance appear in the training data, for both correctly and wrongly predicted examples.
Given a model-generated computation graph $\widehat{G}<em _widehat_G="\widehat{G">{A(\mathbf{x})}$ we analyze how often the full computation of each node $v \in \widehat{V}$ is seen in training. We define $v$ 's full computation as the subgraph induced by all ancestors of $v$ including $v$, denoted $F C</em><em _widehat_G="\widehat{G">{A(\mathbf{x})}}(v)$. We say that $F C</em><em _widehat_G="\widehat{G">{A(\mathbf{x})}}(v)$ is seen during training if $F C</em><em G__A_left_mathbf_x="G_{A\left(\mathbf{x">{A(\mathbf{x})}}(v) \cong F C</em>$ in training, and for some $w \in V$. We characterize complexity of a full computation subgraph by its depth, as defined in $\S 2.1$.
Figure 6 shows that full computation subgraphs appear significantly more frequently in the training data for correctly predicted test examples than for incorrectly predicted ones, for both the multi-}^{\prime}\right)}}(w)$ for some computation graph $G_{A\left(\mathbf{x}^{\prime}\right)</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Average frequency in which test examples' full computations subgraph appear in the training data w.r.t. the subgraph depth, grouped by final answer.
plication and DP task (both frequencies tend to zero for large depths since we ensured a disjoint train/test split). This high correlation suggests that pattern matching-and not general reasoning capabilities-may be the cause behind correct model outputs. This type of learning could be largely effective when the compositional complexity of tasks is low but it becomes less efficient when tasks are increasingly complex. This may elucidate the observed performance gain in low-complexity and in-domain cases and the striking performance drop in OOD and highly complex cases.</p>
<h1>3.2.3 What Types of Errors do Transformers Make at Different Reasoning Depths?</h1>
<p>For clearer understanding of where transformers fall short, we analyze the types of errors that transformers make for nodes at different layers in the computation graph. For every input $\mathbf{x}$, we compare the ground truth computation graph $G_{A(\mathbf{x})}$ with the (possibly incorrect) model-generated computation graph $\widehat{G}<em 1="1">{A(\mathbf{x})}$. We consider a node $v$ as having a correct value if and only if $s(v)=\widehat{s}(v) .{ }^{6}$. We consider a node $v$ to be derived from a correct computation if given that $U=\left{u</em>}, \ldots, u_{k}\right}$ are the immediate predecessors of $v$ in $\widehat{G<em 1="1">{A(\mathbf{x})}$ and that $\widehat{\sigma p}(v)=f$, we have that $f\left(u</em>(v)$. Note that the notion of correct computation is independent of $G$, and that a node $v$ derived from a correct computation may not have the correct value if an error occurred in some of its ancestors.}, \ldots, u_{k}\right)=\widehat{s</p>
<p>We classify each node $v \in \widehat{V}$ into one of four categories. Node $v$ is fully correct if $v$ and its ancestors have correct values and are derived from correct computations. If a node $v$ is not fully correct, its error can be of the following types: $v$ has a local error if its parent nodes have correct values but $v$ is derived from an incorrect computation (i.e., a one-hop reasoning error); $v$ has a propagation error if $v$ is derived from a correct computation but some of its parent nodes have incorrect values; $v$ has a restoration error if it has a correct value but is derived from an incorrect computation.</p>
<p>Figure 7 shows results for few-shot GPT4 and fine-tuned GPT3 with scratchpad, with respect to graph layer number for each node. In all settings, the ratio of fully correct nodes is almost perfect but sharply decreases toward zero with increasing graph layers. Moreover, the ratio of propagation errors is usually higher than the ratio of local errors. Both phenomena suggest that models are able to correctly perform single-step reasoning, potentially due to memorizing such single-step operations during training, but fail to plan and compose several of these steps for an overall correct reasoning.
Both the DP and the puzzle tasks have a high ratio of restoration errors, suggesting memorization since correct outputs are produced despite incorrect computations. There are signs of memorization even when restoration errors are near zero: $82.3 \%$ of the final correct answers for 4-digit by 2-digit multiplications (a setting unseen during training) had at least one error in the computation graph, but still produced correct answers. These patterns are possibly due to high frequency of (input, output) multiplication pairs in the pretraining data, in contrast to intermediate reasoning steps.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 Error Propagations: The Theoretical Limits</h1>
<p>Experiments (§3) highlight the limitations of current transformers in handling complex, multi-step reasoning tasks. Concretely, we show that errors rapidly escalate as the problem size grows (§3.2.3). Here, we aim to provide theoretical insights into why autoregressive transformer LLMs can perform significantly worse in compositional tasks as the problem size increases, making explicit the different ways in which compounding stochastic errors affect final performance. We argue using stylized examples that transformers may be too limited to solve compositionally complex tasks. Formal statements and full proofs are provided in §D.</p>
<p>Algorithms designed to solve compositional tasks typically involve multiple independent applications of a function and/or iterated applications of the same function. A transformer executing such an algorithm acts as an estimator of these functions. In this context, we examine the probability of such an estimator reaching the correct answer as the problem size increases. We first consider a scenario where a transformer estimates an algorithm requiring $n$ independent applications of a function:</p>
<p>Proposition 4.1 (informal). Let $f_{n}$ involve the combination $h_{n}$ of $n$ independent applications of a function $g$. Let $\widehat{f}, \widehat{g}, \widehat{h}<em n="n">{n}$ be their estimators. Assume that $\widehat{h}</em>}$ is a perfect estimator of $h_{n}$ and that $h_{n}$ has low collision, with $c_{n}$ being an upper bound of $h_{n}$ 's collision rate ( $c_{n}<c \forall n$, with $c \ll 1$ ). If $\mathbb{P}(g \neq \widehat{g})=$ $\epsilon>0$ where $\widehat{g}$ 's errors are independent, then $\mathbb{P}\left(f_{n} \neq \widehat{f<em n="n">{n}\right)&gt;1-c</em>}-(1-\epsilon)^{n} \cdot\left(1-c_{n}\right)$. This implies that $\mathbb{P}\left(f_{n} \neq \widehat{f<em _rightarrow_infty="\rightarrow+\infty" n="n">{n}\right)$ decreases exponentially as $n$ increases, with $\liminf </em>} \mathbb{P}\left(f_{n} \neq \widehat{f<em n="n">{n}\right) \geq 1-c$. Moreover, if $c</em>\right)$ tends exponentially to 1 as $n$ increases.} \leq \beta \alpha^{n}$ for some $\alpha \in(0,1), \beta&gt;0, \mathbb{P}\left(f_{n} \neq \widehat{f}_{n</p>
<p>Prop. 4.1's proof (§D.1) shows the rate of convergence is exponential, thus concluding that transformers will rapidly fail with increasing $n$. Let's now analyze the iterated application function scenario.</p>
<p>Proposition 4.2 (informal). Let $f_{n}(\mathbf{x})=g^{n}(\mathbf{x})$ involve the repeated application of $g$. Assume that the probability of recovering from a mistake due to the randomness of applying the estimator on an incorrect input has probability at most $c$. If $\mathbb{P}(g \neq \widehat{g})=\epsilon&gt;0$, then $\mathbb{P}\left(f_{n} \neq \widehat{f}<em n="n">{n}\right)$ decreases exponentially with $n$. Precisely, $\mathbb{P}\left(f</em>} \neq \widehat{f<em _rightarrow_infty="\rightarrow+\infty" n="n">{n}\right) \geq 1-(1-\epsilon-c)^{n-1}(1-\epsilon-c /(c+\epsilon))$, implying $\liminf </em>} \mathbb{P}\left(f_{n} \neq \widehat{f<em n="n">{n}\right) \geq 1-c /(c+\epsilon)$.
The argument is as follows. Let $s</em>}:=\mathbb{P}\left(f_{n}=\widehat{f<em 1="1">{n}\right)$, where $s</em>$ when $n \rightarrow+\infty$. See formal statement and derivation in §D.2.
Prop. 4.2's proof also shows an exponential rate of convergence. Note that if $c \ll \epsilon$ then $\liminf }=1-\epsilon$ by definition. Derive $s_{n} \leq(1-\epsilon-c) \cdot s_{n-1}+c$ using law of total probability. Then, prove by induction a non-recursive upper bound for $s_{n}$ with limit $\frac{c}{c+\epsilon<em n="n">{n \rightarrow+\infty} \mathbb{P}\left(f</em>(y)$ arriving at the correct output $g(x)$ by chance when given the wrong input $y \neq x$. More details in §D.3.
Moreover, repeated applications of a function often imply unbounded errors: if $g(x)$ can be expressed as an affine transformation $F x+c$, then it may be viewed as a first-order vector autoregression, which are known to be unstable when $|\lambda| \geq 1$ for at least one $\lambda$ eigenvalue of $F$ [31, Prop. 10.1]. While we make these arguments with affine maps, similar behaviors, possibly even more acute, could occur with nonlinear maps [25]-but their study is beyond the scope of this paper.
In Prop. 4.2's current form, we implicitly assume that there is a single valid reasoning for each input since $g$ is a function. We can potentially generalize this assumption with a state-transition framing, where the probability of transitioning from a valid state to an invalid one is $\epsilon$, and the probability of recovering from an invalid state is at most $c$. See formal statement in D.2.
All tasks evaluated in the present work can be seen as instances of the results just proven. Prop. 4.1 directly applies to multiplication, since $m$-by- $n$ digit multiplication can be seen as $n$ independent instances of $m$-by-1 digit multiplication (see Cor. D.1). Prop. 4.2 directly applies to the recursive function of the dynamic programming task, as well as to $m$-by- 1 digit multiplication, and to the puzzle through its elimination function (details in D.3). They are also all low collision settings.
Note that Prop 4.1 and 4.2 apply to any high-performant estimator of reasoning tasks. We focus on out-of-the-box transformers to align with the scope of our experiments and with the goal of framing empirical results. In $\S 5$, we discuss how these propositions may inform future research directions.} \neq \widehat{f}_{n}\right) \approx 1$. It is reasonable to assume $c \ll \epsilon$ when $g$ has low collision, since $c$ represents the probability of the estimator $\widehat{g</p>
<h1>5 Discussion</h1>
<p>Collapsed Compositionality and Robustness Implications Transformers today demonstrate undeniably powerful empirical results. Yet, our study suggests that they may have fundamental weaknesses in certain intellectual tasks that require true multi-step compositional operations such as multiplications and logic puzzles. Our careful study based on the computation graph and analyses demonstrates that transformers can often solve multi-step compositional problems by collapsing the depth of the compositional operations via analogical pattern matching. More broadly, our findings suggest that the strong performance of transformers should be taken with a certain grain of salt: Despite initially appearing challenging, certain tasks may not possess the inherent compositionality they seem to have. This is due to the fact that desired solutions could be readily derived from input-output sequences present in the training data, allowing for shortcut pattern matching to produce acceptable solutions. However, such an approach can ultimately result in poor generalization as shown in our study. For example, fine-tuning GPT3 on our tasks both with and without explicit reasoning graphs shows that models' learning fails to generalize beyond levels of complexity seen in training.</p>
<p>Theoretical Findings and their Empirical Implications The proofs presented in $\S 4$ show that, under reasonable assumptions, the probability of incorrect predictions converges exponentially to $\approx 1$ for abstract compositional tasks. Importantly, these proofs apply to autoregressive LMs in general. Our insights indicate that the current configuration of transformers, with their reliance on a greedy process for predicting the next word, constrains their error recovery capability and impedes the development of a comprehensive global understanding of the task. Building on these findings, we suggest several empirical strategies for harnessing the potential of transformers. Firstly, transformers may be employed in ways that require chaining only a few compositional steps to reach a solution rather than lengthy reasoning steps (e.g., [35]). Secondly, transformers may be best suited for compositional tasks where evaluation metrics can afford some leniency; for example, finding approximate solutions that do not require executing the whole graph, such as identifying the most significant digit in a multiplication. Finally, we suggest augmenting transformers with planning modules as well as using refinement methods, that can iteratively improve their generations [82, 48].</p>
<p>Call for Broad Participation to Investigate Limitations Identification of limitations is an important step towards achieving greater robustness. Our study suggests fundamental limitations that impede transformers from fully mastering certain compositional operations. However, we acknowledge that due to our compute budget constraints as well as limited access to the largest language models such as GPT4, we are unable to push the empirical limits of transformers even further in terms of training data size and number of epochs. We invite the broader research community, particularly those with more extensive resources at their disposal, to investigate these possibilities further.</p>
<h2>6 Related Work</h2>
<p>Reasoning abilities in transformer LLMs Recently, transformers [11, 58, 57, 17, 16, 63, 73, 74] have demonstrated impressive reasoning abilities across a wide range of tasks, even outperforming humans in certain cases [79, 28, 15, 85]. This success has been largely attributed to the scaling effect, where larger models and training datasets result in improved performance [38, 33, 1]. However, these models have also been shown to struggle across multiple domains [32], including algorithmic reasoning [78], commonsense reasoning [62, 40], theory of mind [65], planning [76], logical reasoning [70], and ethical reasoning [36]. These difficulties have motivated us to take a step back and thoroughly examine both the successes and failures of transformers from empirical and theoretical perspectives on compositional reasoning tasks.</p>
<p>Challenges of transformers in compositional tasks Transformers perform fairly well in singlestep reasoning tasks [70], but face challenges when it comes to effectively combining multiple steps to solve compositionally complex problems [84, 55, 66, 81]. Recent research has focused on overcoming these limitations through various approaches. First, fine-tuning transformers to directly generate the final answer while keeping the reasoning implicit [7, 18]. Second, encouraging transformers to generate reasoning steps explicitly within a single generation [55, 80, 44, 42]. For example, Nye et al. [55] and Zhou et al. [86] used scratchpads to teach transformers how to perform algorithmic reasoning tasks such as addition by splitting the task into intermediate steps [44, 80]. Further, leveraging LLMs to generate each reasoning step iteratively via a selection and inference mechanism [20, 19, 72].</p>
<p>Lastly, choosing a training split that maximizes the number of observed patterns between the train and test data [10], or diversifying in-prompt examples to cover the maximum of patterns [41], ultimately enhancing generalization. The primary focus of these studies is to enhance model performance on compositional problems without striving for complete mastery. In contrast, our work explores the fundamental limits of vanilla transformers in achieving full mastery, striving for $100 \%$ performance in both in-domain and OOD settings. Our findings show that reaching full mastery is inherently challenging, providing insights into the complexities involved.</p>
<p>Challenges of transformers in generalization Extensive research has been done to investigate the generalization capabilities of transformers [3, 54, 26, 64, 8, 46]. This encompasses various facets of generalization, including easy-to-hard generalization [67, 4], length generalization [2, 60, 13, 54, 8], and generalization on symbolic mathematical integration [83]. Schwarzschild et al. [67] and Bansal et al. [4] employ weight-tied neural networks to generalize from easy to hard examples. Liu et al., [45] found that shallow transformers learn shortcuts during training, leading to poor OOD generalization. Razeghi et al. [64] revealed a positive correlation between the frequency of training terms and their test performance. Building upon this line of inquiry, we present a more rigorous examination of sub-graph matching between training and test instances for complex compositional tasks where we demonstrate how pattern matching can hinder generalization. We complement our empirical results with theoretical insights on transformers' limits.</p>
<p>Grokking The phenomena of models' gaining generalization capabilities when training significantly beyond overfitting, known as grokking was recently introduced in [59]. Subsequent works focus on characterizing when and why grokking arises: [47] show that perfect generalization in an arithmetic addition task happens when there is sufficient data to determine the appropriate structured representation, later extended to sparse parity in [52] where a sparse subnetwork of neurons is shown responsible for generalization behavior. Recently, [77] propose that grokking occurs when a task admits a generalizing and a memorizing solution, and the former is slower to learn. In this present work, our aim is not to explain grokking but rather to observe its emergence. We do not observe grokking arising in the context of multiplication, and we leave it to future work to explore whether this may be due to task difficulty hindering the learning of well-structured representations.</p>
<p>Transformers' theoretical expressiveness Lin et al. [43] study autoregressive models' limitations from a computational complexity theory perspective. Transformer-specific work has focused on quantifying the class of problems that (not necessarily autoregressive) transformers can express assuming perfect parameters [51, 50, 14, 49, inter alia]. All tasks analyzed in our work belong to a class expressible by transformers, suggesting that known upper bound might not be tight. Importantly, Hahn [30] shows that transformers cannot robustly model noncounter-free regular languages even when allowing infinite precision. In contrast, our focus is on error accumulation, which enables to investigate if reasoning tasks theoretically solvable by transformers are likely to be solved by them.</p>
<p>Additional literature and societal impact discussion can be found in §E.</p>
<h1>7 Conclusions</h1>
<p>On a broader scope, as transformers continue to gain widespread deployment with significant realworld impacts, it is ever more urgent to understand their successes and failures. Our study critically investigates transformers' limitations and emphasizes the need to develop models capable of robust generalization and systematic problem-solving. By examining the compositional capabilities of these models, we aspire to work towards more reliable AI systems that excel not only in tasks where abundant training examples are sufficient, but also in cases requiring precise compositional reasoning.</p>
<h2>8 Limitations</h2>
<p>We focus on analyzing compositional reasoning capabilities through the lens of computation graphs. Although they are a useful way to systematically represent rigorous reasoning processes, it is important to note that for the scratchpad approach, we are limited to only establishing a correlation between the model generation and its preceding context, as we cannot inspect the exact tokens model attends to when making the prediction. This limitation arises from our lack of access to the activations of the studied models. Furthermore, we posit that alternative approaches to linearizing reasoning processes may yield different performances and provide opportunities for further exploration.</p>
<h1>Acknowledgements</h1>
<p>We thank members of the Mosaic team at AI2 for valuable feedback on this project, as well as Agustín Santiago Gutiérrez and Kawin Ethayarajh for valuable discussions. This research was supported by the NSF DMS-2134012, DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI.</p>
<h1>References</h1>
<p>[1] Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. CoRR, abs/2301.03728, 2023.
[2] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 38546-38556. Curran Associates, Inc., 2022.
[3] Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 38546-38556. Curran Associates, Inc., 2022.
[4] Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 20232-20242. Curran Associates, Inc., 2022.
[5] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. In Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000.
[6] D. Bertsekas. Abstract Dynamic Programming: 3rd Edition. Athena scientific optimization and computation series. Athena Scientific., 2022.
[7] Gregor Betz, Christian Voigt, and Kyle Richardson. Critical thinking for language models. In Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages $63-75,2021$.
[8] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the practical ability of recurrent neural networks to recognize hierarchical languages. In Donia Scott, Núria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 1481-1494. International Committee on Computational Linguistics, 2020.
[9] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He. ChatGPT is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. CoRR, abs/2303.16421, 2023.
[10] Ben Bogin, Shivanshu Gupta, and Jonathan Berant. Unobserved local structures make compositional generalization hard. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2731-2747, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901, 2020.
[12] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712, 2023.</p>
<p>[13] Mirelle Bueno, Carlos Gemmel, Jeffrey Dalton, Roberto de Alencar Lotufo, and Rodrigo Frassetto Nogueira. Induced natural language rationales and interleaved markup tokens enable extrapolation in large language models. CoRR, abs/2208.11445, 2022.
[14] David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer encoders. Proceedings of the 40th International Conference on Machine Learning, 202:55445562, 2023.
[15] Jonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. ChatGPT goes to law school. Available at SSRN, 2023.
[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022.
[17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022.
[18] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3882-3890, 2021.
[19] Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. CoRR, abs/2208.14271, 2022.
[20] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In International Conference on Learning Representations, 2023.
[21] Leonardo De Moura and Nikolaj Bjørner. Z3: An efficient smt solver. In Tools and Algorithms for the Construction and Analysis of Systems: 14th International Conference, TACAS 2008, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2008, Budapest, Hungary, March 29-April 6, 2008. Proceedings 14, pages 337-340. Springer, 2008.
[22] Stanislas Dehaene, Nicolas Molko, Laurent Cohen, and Anna J Wilson. Arithmetic and the brain. Current opinion in neurobiology, 14(2):218-224, 2004.
[23] B. Delyon and A. Juditsky. On small perturbations of stable markov operators: Unbounded case. Theory of Probability \&amp; Its Applications, 43(4):577-587, 1999.
[24] Persi Diaconis and David Freedman. Iterated random functions. SIAM review, 41(1):45-76, 1999 .
[25] R. Douc, E. Moulines, and D. Stoffer. Nonlinear Time Series: Theory, Methods and Applications with R Examples. Chapman \&amp; Hall/CRC Texts in Statistical Science. Taylor \&amp; Francis, 2014.
[26] Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 403-413, 2020.</p>
<p>[27] Jonathan St BT Evans. Bias in human reasoning: Causes and consequences. Lawrence Erlbaum Associates, Inc, 1989.
[28] Hao Fu, Yao; Peng and Tushar Khot. How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu's Notion, Dec 2022.
[29] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665-673, 2020.
[30] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020.
[31] James Douglas Hamilton. Time series analysis. Princeton university press, 1994.
[32] Chadi Helwe, Chloé Clavel, and Fabian M. Suchanek. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In Danqi Chen, Jonathan Berant, Andrew McCallum, and Sameer Singh, editors, 3rd Conference on Automated Knowledge Base Construction, AKBC 2021, Virtual, October 4-8, 2021, 2021.
[33] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. CoRR, abs/2010.14701, 2020.
[34] James Hiebert. Conceptual and procedural knowledge: The case of mathematics. Routledge, 2013.
[35] Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. In The Eleventh International Conference on Learning Representations, 2023.
[36] Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. Delphi: Towards machine ethics and norms. CoRR, abs/2110.07574, 2021.
[37] Philip N Johnson-Laird, Sangeet S Khemlani, and Geoffrey P Goodwin. Logic, probability, and human reasoning. Trends in cognitive sciences, 19(4):201-214, 2015.
[38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020.
[39] Jon Kleinberg and Eva Tardos. Algorithm Design. Addison-Wesley Longman Publishing Co., Inc., USA, 2005.
[40] Philipp E. Koralus and Vincent Wang-Mascianica. Humans in humans out: On GPT converging toward common sense in both success and failure. CoRR, abs/2303.17276, 2023.
[41] Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1401-1422, Toronto, Canada, July 2023. Association for Computational Linguistics.
[42] Zhengzhong Liang, Zeyu Zhang, Steven Bethard, and Mihai Surdeanu. Explainable verbal reasoner plus (evr+): A natural language reasoning framework that supports diverse compositional reasoning. arXiv preprint arXiv:2305.00061, 2023.
[43] Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5147-5173, Online, June 2021. Association for Computational Linguistics.</p>
<p>[44] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, 2017.
[45] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In The Eleventh International Conference on Learning Representations, 2022.
[46] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In International Conference on Learning Representations, 2023.
[47] Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams. Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems, 35:34651-34663, 2022.
[48] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. CoRR, abs/2303.17651, 2023.
[49] William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.
[50] William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. Advances in Neural Information Processing Systems, 2023.
[51] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531-545, 2023.
[52] William Merrill, Nikolaos Tsilivis, and Aman Shukla. A tale of two circuits: Grokking as competition of sparse and dense subnetworks. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.
[53] Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher Manning. Grokking of hierarchical structure in vanilla transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 439-448, Toronto, Canada, July 2023. Association for Computational Linguistics.
[54] Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision and length extrapolation. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 276-291, 2020.
[55] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021.
[56] Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. CoRR, abs/2112.00114, 2021.
[57] OpenAI. ChatGPT: Optimizing language models for dialogue, 2022.
[58] OpenAI. GPT-4 technical report, 2023.
[59] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
[60] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022.</p>
<p>[61] Patrick Prosser. Hybrid algorithms for the constraint satisfaction problem. Computational intelligence, 9(3):268-299, 1993.
[62] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is ChatGPT a general-purpose natural language processing task solver? CoRR, abs/2302.06476, 2023.
[63] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training gopher. ArXiv, abs/2112.11446, 2021.
[64] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840-854, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
[65] Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762-3780, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
[66] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In International Conference on Learning Representations, 2023.
[67] Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. In Advances in Neural Information Processing Systems, volume 34, pages 6695-6706. Curran Associates, Inc., 2021.
[68] Herbert A. Simon. The architecture of complexity. Proceedings of the American Philosophical Society, 106(6):467-482, 1962.
[69] Herbert A Simon and Allen Newell. Human problem solving: The state of the theory in 1970. American psychologist, 26(2):145, 1971.
[70] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615, 2022.
[71] Keith Stenning and Michiel Van Lambalgen. Human reasoning and cognitive science. MIT Press, 2012.</p>
<p>[72] Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, 2021.
[73] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. CoRR, abs/2211.09085, 2022.
[74] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022.
[75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023.
[76] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.
[77] Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, and Ramana Kumar. Explaining grokking through circuit efficiency. arXiv preprint arXiv:2309.02390, 2023.
[78] Petar Veličković and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, 2021.
[79] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. Survey Certification.
[80] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc., 2022.
[81] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. NaturalProver: Grounded mathematical proof generation with language models. In Advances in Neural Information Processing Systems, volume 35, pages 4913-4927. Curran Associates, Inc., 2022.
[82] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In International Conference on Learning Representations, 2023.
[83] Sean Welleck, Peter West, Jize Cao, and Yejin Choi. Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8629-8637, 2022.
[84] Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with LEGO: a synthetic reasoning task. CoRR, abs/2206.04301, 2022.
[85] Haoyi Zheng and Huichun Zhan. ChatGPT in scientific writing: a cautionary tale. The American Journal of Medicine, 2023.
[86] Hattie Zhou, Azade Nova, Aaron Courville, Hugo Larochelle, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning, 2023.</p>
<h1>Appendices</h1>
<p>A Compositional Tasks ..... 19
A. 1 Multiplication ..... 19
A. 2 Einstein's Puzzle ..... 20
A. 3 Dynamic Programming Problem ..... 22
B Experimental Setups \&amp; Empirical Results ..... 24
B. 1 Models ..... 24
B. 2 Limits of Transformers in Zero- and Few-shot Settings ..... 24
B. 3 Limits of Transformers with question-answer Training ..... 24
B. 4 Limits of Transformers with Explicit Scratchpad Training ..... 26
B. 5 Limits of Transformers with Explicit Scratchpad Prompting ..... 26
C Surface Patterns ..... 29
C. 1 Relative Information Gain Predictions for Multiplication ..... 29
C. 2 Empirical Surface Pattern Analysis for Multiplication with GPT4, ChatGPT and GPT3 ..... 30
C. 3 Relative Information Gain Predictions for Dynamic Programming Task ..... 31
C. 4 Empirical Surface Pattern Results for Dynamic Programming Task ..... 33
D Theoretical Results: Derivations ..... 35
D. 1 Error accumulates with larger parallel applications of an estimated function (width) ..... 35
D. 2 Error accumulates with larger iterative applications of an estimated function (depth) ..... 36
D. 3 Discussing $c \ll \epsilon$ in the context of Proposition 4.2 ..... 38
E Additional Literature and Societal Impact ..... 40
E. 1 Additional Literature ..... 40
E. 2 Societal Impact Discussion ..... 40</p>
<h1>A Compositional Tasks</h1>
<h2>A. 1 Multiplication</h2>
<p>Data Construction We exhaustively generate multiplication problems as question-answer pairs (e.g., Q: "What is 4 times 32?" A: "128"). We focus on multiplications of two numbers $x=$ $\left(x_{1}, x_{2}, \ldots, x_{k}\right)$ and $y=\left(y_{1}, y_{2}, \ldots, y_{k}\right)$ where each number can have up to $k$ digits, amounting to $9 \times 10^{(k-1)}$ combinations per each number. We set $k$ to 5 in our experiments. Figure 8 showcases an example prompt for performing few-shot learning without the inclusion of a scratchpad, while Figure 9 demonstrates an example prompt using a scratchpad. Throughout our experimentation, we explored various versions of the scratchpad, ranging from verbose and detailed to more concise alternatives. Among these variations, the scratchpad version depicted in Figure 9 ultimately produced the most favorable outcomes. Listing 1 shows the Python code for solving the task.</p>
<p>To multiply two numbers, start by multiplying the rightmost digit of the multiplicand by each digit of the multiplier, writing down the products and carrying over any remainders. Repeat this process for each digit of the multiplicand, and then add up all the partial products to obtain the final result.</p>
<p>Questions: what's 22 times 2? Answer 44.</p>
<p>Figure 8: Example prompt for the multiplication task used for the few-shot setting.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">35</span><span class="w"> </span><span class="n">times</span><span class="w"> </span><span class="mi">90</span><span class="o">?</span>
</code></pre></div>

<p>Scratchpad: Let's perform the multiplication step by step:
Let's multiply 35 by the digit in the ones place of 90 , which is 0 .</p>
<ol>
<li>Multiply 0 by the digit in the ones place of 35 , which is 5 . This gives $5 \times 0$
$=0$. Write down the result 0 .</li>
<li>Multiply 0 by the digit in the tens place of 35 , which is 3 . This gives $3 \times 0$
$=0$. Write down the result 0 .</li>
<li>The partial product for this step is $A=0$ which is the concatenation of the digits we found in each step.</li>
</ol>
<p>Now, let's multiply 35 by the digit in the tens place of 90 , which is 9 .
4. Multiply 9 by the digit in the ones place of 35 , which is 5 . This gives $5 \times 9$
$=45$. Write down the result 5 and carry over the 4 to the next step.
5. Multiply 9 by the digit in the tens place of 35 , which is 3 . Add the carryover
from the previous step to account for this. This gives $(3 \times 9)+4=31$. Write
down the result 31 .
6. The partial product for this step is $B=315$ which is the concatenation of the digits we found in each step.</p>
<p>Now, let's sum the 2 partial products $A$ and $B$, and take into account the position of each digit: $A=0$ (from multiplication by 0 ) and $B=315$ (from multiplication by 9 but shifted one place to the left, so it becomes 3150). The final answer is $0 \times 1$ $+315 \times 10=0+3150=3150$</p>
<p>Figure 9: A sample scratchpad for the multiplication task.</p>
<div class="codehilite"><pre><span></span><code>def multiply(x, y):
    summands = [0] * len(y)
    for i in range(len(y) - 1, -1, -1):
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">digits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">carry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="o">[</span><span class="n">i</span><span class="o">]</span>
<span class="w">    </span><span class="n">t</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">carry</span>
<span class="w">    </span><span class="n">carry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">10</span>
<span class="w">    </span><span class="n">digits</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">%</span>
<span class="n">digits</span><span class="p">.</span><span class="k">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">carry</span><span class="p">)</span>
<span class="n">summands</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">digits</span><span class="o">[</span><span class="n">-k</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">10</span><span class="w"> </span><span class="o">**</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span>
<span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">digits</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span>
<span class="n">product</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">summands</span><span class="o">[</span><span class="n">-i</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">10</span><span class="w"> </span><span class="o">**</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nf">len</span>
<span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span>
<span class="k">return</span><span class="w"> </span><span class="n">product</span>
</code></pre></div>

<p>Listing 1: Example Python code for solving the multiplication task.</p>
<h1>A. 2 Einstein's Puzzle</h1>
<p>Data Construction In our experiments, we initially establish a set of properties, such as Color, PhoneModel, Pet, and so forth, along with their corresponding values expressed in natural language templates (e.g., "The house has a red color."). We then devise a fundamental and straightforward set of clue types: 1) 'found_at', e.g., "Alice lives in House 2", 2) 'same_house', e.g., "The person who is a cat lover lives in the house that has a red color.", 3) 'direct_left', e.g., "The person who has a dog as a pet lives to the left of the person who lives in a red house.", and 4) 'besides', e.g., "The person who has a dog as a pet and the person who has a red house live next to each other." In addition, we also set up harder clue types such as 'not_at', 'left_of' (not necessarily directly left of), 'two_house_between', etc. which are only used in auxiliary experiments.
The solution to the puzzle is a matrix of size $K \times M$, where $K$ represents the number of houses and $M$ the number of attributes. During the puzzle generation, the $M$ properties are randomly selected from the candidate pool, followed by the random sampling of $K$ values for each property. The sampled values are then randomly permuted and assigned within the table to create the solution. It is important to note that we ensure one of the sampled properties is 'Name' to enhance the readability and comprehensibility of the puzzles. To construct the clues, we initially over-generate all valid clues based on the solution and subsequently remove redundant clues at random until we obtain a set with a</p>
<h2>General Unique Rules</h2>
<p>There are 3 houses (numbered 1 on the left, 3 on the right). Each has a different person in them. They have different characteristics:</p>
<ul>
<li>Each person has a unique name: peter, eric, arnold</li>
<li>People have different favorite sports: soccer, tennis, basketball</li>
<li>People own different car models: tesla, ford, camry
Clues</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">House</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Sports</th>
<th style="text-align: left;">Car</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Eric</td>
<td style="text-align: left;">Basketball</td>
<td style="text-align: left;">Camry</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Peter</td>
<td style="text-align: left;">Tennis</td>
<td style="text-align: left;">Ford</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">Arnold</td>
<td style="text-align: left;">Soccer</td>
<td style="text-align: left;">Tesla</td>
</tr>
</tbody>
</table>
<ol>
<li>The person who owns a Ford is the person who loves tennis.</li>
<li>Arnold is in the third house.</li>
<li>The person who owns a Camry is directly left of the person who owns a Ford.</li>
<li>Eric is the person who owns a Camry.</li>
<li>The person who loves basketball is Eric.</li>
<li>The person who loves tennis and the person who loves soccer are next to each other.</li>
</ol>
<p>Reasoning Path Generation
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: A sample of the puzzle task and the reasoning path to reach a solution.</p>
<div class="codehilite"><pre><span></span><code>This is a logic puzzle. There are 3 houses (numbered 1 on the left, 3 on the
right). Each has a different person in them. They have different characteristics:
- Each person has a unique name: peter, eric, arnold
- People have different favorite sports: soccer, tennis, basketball
- People own different car models: tesla model 3, ford f150, toyota camry
</code></pre></div>

<ol>
<li>The person who owns a Ford F-150 is the person who loves tennis.</li>
<li>Arnold is in the third house.</li>
<li>The person who owns a Toyota Camry is directly left of the person who owns a Ford F-150.</li>
<li>Eric is the person who owns a Toyota Camry.</li>
<li>The person who loves basketball is Eric.</li>
<li>The person who loves tennis and the person who loves soccer are next to each other.</li>
</ol>
<p>Let's think step by step. Please first briefly talk about your reasoning and show your final solution by filling the blanks in the below table.
\$ House: ___ \$ Name: ___ \$ Sports: ___ \$ Car: ___
\$ House: ___ \$ Name: ___ \$ Sports: ___ \$ Car: ___
\$ House: ___ \$ Name: ___ \$ Sports: ___ \$ Car: ___
Reasoning:
Step 1: First apply clue <Arnold is in the third house.> We know that The Name in house 3 is arnold.
Step 2: Then combine clues: <The person who loves tennis and the person who loves soccer are next to each other.> <The person who loves basketball is Eric.>
Unique Values Rules and the fixed table structure. We know that The Name in house 1 is eric. The FavoriteSport in house 1 is basketball. The Name in house 2 is peter.
Step 3: Then apply clue <Eric is the person who owns a Toyota Camry.> We know that The CarModel in house 1 is toyota camry.
Step 4: Then apply clue <The person who owns a Toyota Camry is directly left of the person who owns a Ford F-150.> and Unique Values We know that The CarModel in house 2 is ford f150. The CarModel in house 3 is tesla model 3.
Step 5: Then apply clue <The person who owns a Ford F-150 is the person who loves tennis.> and Unique Values We know that The FavoriteSport in house 2 is tennis. The FavoriteSport in house 3 is soccer.
The puzzle is solved.
Final solution:
\$ House: 1 \$ Name: Eric \$ Sports: Basketball \$ Car: Camry
\$ House: 2 \$ Name: Peter \$ Sports: Tennis \$ Car: Ford
\$ House: 3 \$ Name: Arnold \$ Sports: Soccer \$ Car: Tesla</p>
<p>Figure 11: A sample scratchpad for the puzzle task.
unique solution, as previously sampled. This process ensures a coherent and engaging puzzle-solving experience. Refer to Figure 10 for an example.</p>
<p>Graph Construction Algorithm To solve the complex compositional reasoning process for a logical grid puzzle, we use existing puzzle solvers [21] to generate the computation graph. It follows the basic greedy principle of applying the minimum number of rules to solve any cell, i.e., if using only one rule to solve any given cell, then apply this rule. This algorithm iterates through all clues in the clue set until one or a set of clue combinations can solve any cell in the table. While it may not be the most efficient way to solve the puzzle, it provides models with explicit scratchpad verbalization through an intuitive computation graph. Refer to Figure 10 for the pseudo-code of the process, and Figure 11 for a scratchpad example.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ If a node $v$ does not appear in the ground truth graph $G$, we consider it to have an incorrect value.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
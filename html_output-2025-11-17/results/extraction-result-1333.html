<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1333 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1333</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1333</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-251018675</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2207.10821v2.pdf" target="_blank">Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation</a></p>
                <p><strong>Paper Abstract:</strong> If we want to train robots in simulation before deploying them in reality, it seems natural and almost self-evident to presume that reducing the sim2real gap involves creating simulators of increasing fidelity (since reality is what it is). We challenge this assumption and present a contrary hypothesis -- sim2real transfer of robots may be improved with lower (not higher) fidelity simulation. We conduct a systematic large-scale evaluation of this hypothesis on the problem of visual navigation -- in the real world, and on 2 different simulators (Habitat and iGibson) using 3 different robots (A1, AlienGo, Spot). Our results show that, contrary to expectation, adding fidelity does not help with learning; performance is poor due to slow simulation speed (preventing large-scale learning) and overfitting to inaccuracies in simulation physics. Instead, building simple models of the robot motion using real-world data can improve learning and generalization.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1333.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1333.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Habitat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat (AI Habitat)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-throughput embodied AI simulator for photorealistic indoor environments that integrates with a C++ Bullet physics backend; used here for training and evaluation of visual navigation policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Habitat</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Photorealistic indoor environment simulator for embodied agents; provides rendering, scene datasets (HM3D) and integrates with Bullet physics via a C++ binding for dynamic simulation or a kinematic stepping mode for faster abstracted simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied AI (robot navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Operable at multiple fidelity levels: can run kinematic (teleport/Euler integration) low-fidelity stepping or dynamic (Bullet rigid-body/contact) high-fidelity simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports photorealistic rendering and collision checking; dynamic mode uses Bullet physics at high frequency; kinematic mode teleports agent using Euler integration and skips low-level joint dynamics; noted to run ~1200% faster than iGibson in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>High-level visual navigation policy (ResNet-18 + 2-layer LSTM trained with DD-PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agent: ResNet-18 visual encoder, 2-layer LSTM policy head parameterizing Gaussian action distribution; trained with distributed DD-PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal visual navigation (egocentric depth + egomotion to command center-of-mass velocities to reach specified relative goal coordinates).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Kinematic training: converged in ~3 days, learned from ~500M steps of experience; Dynamic training: converged in ~7 days, learned from ~50M steps (approx.). Exact per-simulator SRs reported in paper (see transfer/perf).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Other simulator (iGibson) and real-world Spot robot (zero-shot sim2real evaluation in LAB environment).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Kinematic-trained policies transferred best: achieved 100% success rate and 82-83% SPL in real-world Spot experiments; dynamic-trained policies transferred worse (real-world SR 40-67% and lower SPL). Also kinematic policies often outperformed dynamic ones when evaluated across simulators (e.g., A1: 62.1% SR kinematic vs 24.2% dynamic in iGibson eval in one reported case).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Across robots and simulators, low-fidelity kinematic training produced better generalization and sim2real transfer than high-fidelity dynamic training. Kinematic policies learned from ~10× more steps (500M vs 50M) because kinematic simulation is much faster, and they outperformed dynamic policies both in-sim (often) and on real Spot robot (100% SR vs 40-67% SR for dynamic). Dynamic policies tended to overfit to simulator-specific low-level dynamics and performed poorly when evaluated in different simulators or on hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors argue that for high-level tasks that can be abstracted (like navigation), low-level dynamics fidelity is unnecessary and can be harmful; they recommend abstracting away low-level controllers and prioritizing simulation speed, while optionally adding low-dimensional actuation noise learned from hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Dynamic (high-fidelity) simulations failed to transfer well: overfitting to specific low-level controllers (Raibert-style vs MPC) led to poor sim2sim and sim2real transfer; dynamic training was slower (fewer environment steps) causing impoverished experience and more collisions on hardware. Also, for tasks requiring low-level interactions (e.g., dexterous manipulation) high fidelity remains necessary.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1333.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1333.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iGibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iGibson</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A photorealistic simulator for interactive tasks in large realistic indoor scenes that integrates with PyBullet; used here for training and cross-simulator evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>iGibson</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Photorealistic indoor environment simulator built for interactive tasks, leveraging PyBullet for physics; used here for both kinematic and dynamic simulations and cross-evaluation against Habitat.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied AI (robot navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Operable at multiple fidelity levels in experiments: kinematic (abstracted teleport/Euler integration) and dynamic (PyBullet rigid-body/contact dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Renders photorealistic scenes; dynamic mode uses PyBullet (Python binding to Bullet) with physics integration and contact dynamics; noted to be substantially slower than Habitat in the authors' setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>High-level visual navigation policy (ResNet-18 + 2-layer LSTM trained with DD-PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agent identical to the one used with Habitat: convolutional encoder + recurrent policy, trained with DD-PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal visual navigation in cluttered indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>When trained dynamically in iGibson, policies underperformed compared to kinematic training (examples: A1 dynamic 24.2% SR vs kinematic 62.1% SR when evaluated in iGibson in a reported case). Dynamic training had less experience due to slower simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Other simulator (Habitat) and real-world Spot robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Dynamic-trained policies showed large drops when evaluated across simulators (e.g., Habitat->iGibson dynamic gap reported), indicating poor generalization; kinematic-trained policies generalized better across simulators and to real robot (100% SR on Spot).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Same pattern as with Habitat: kinematic-trained agents outperformed dynamic-trained agents in cross-simulator evaluation and in sim2real transfer; dynamic agents overfit to simulator/controller specifics and were sensitive to simulator changes.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors indicate high-fidelity dynamic simulation in iGibson is not necessary for training high-level navigation policies and can reduce effective training scale due to lower simulation speed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Slower simulation speed reduced amount of experience available for learning; dynamic policies trained in iGibson showed poor transfer to Habitat and hardware.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1333.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1333.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bullet / PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bullet Physics Engine (and PyBullet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rigid-body physics engine used for dynamic simulation of contact and joint dynamics; Bullet is used by Habitat (C++ integration) and PyBullet is used by iGibson (Python binding) in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Bullet / PyBullet (physics engine)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Rigid-body physics engine that simulates contacts, collisions, and joint dynamics; in the dynamic experiments it is used at high frequency (240 Hz) to produce realistic low-level dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / contact dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity relative to kinematic stepping: simulates rigid-body dynamics, contact events, joint torques and controllers, with small physics timestep (1/240 s in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simulates contact dynamics, joint torques, and high-frequency controller loops (240 Hz); uses a physics step-size of 1/240 s for dynamic simulation. However, real-world controllers (closed-source) may still differ causing sim2real mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Dynamic low-level controllers (Raibert-style controller used for training; MPC used for evaluation) + high-level RL policy</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two low-level controllers are used with Bullet: an expert Raibert-style controller (footstep generator + IK + joint PD) and an MPC controller (direct torque commands). High-level agent is ResNet-18 + 2-layer LSTM trained with DD-PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>High-level navigation decision-making while low-level dynamics are simulated by Bullet (agency: mapping CoM velocity commands to joint torques and contact dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Dynamic training produced fewer environment steps due to computational cost (approx. 50M steps vs 500M for kinematic) and underperformed on transfer despite sometimes achieving higher in-domain metrics early; specific in-sim SR examples: Habitat-Dynamic better within-domain but poor cross-eval (e.g., Habitat Dynamic 49.8% vs iGibson Dynamic 22.3% for A1 in one comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Other simulators (iGibson/Habitat) and the real Spot robot (hardware) with manufacturer low-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Dynamic policies showed poor sim2sim and sim2real transfer: lower real-world SR (40-67%) and higher collision counts relative to kinematic-trained policies; also sensitive to switching low-level controllers (Raibert->MPC) during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>High-fidelity (Bullet-based) dynamic simulation led to overfitting to simulator/controller specifics and worse transfer; despite modeling contacts and joint dynamics, the mismatch to hardware controllers reduced real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper suggests that full low-level physics fidelity (as in Bullet) is not necessary and can be harmful for training high-level navigation policies that will be executed with hardware controllers; instead, abstracted simulation plus low-dimensional noise is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Dynamic simulations failed to transfer robustly when training used one dynamic controller (Raibert) and evaluation used another (MPC) or hardware closed-loop controller; dynamic policies also suffered from limited training scale due to slow simulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1333.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1333.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kinematic Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kinematic (abstracted) simulation / kinematic controller</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-fidelity simulation mode that teleports the robot's center-of-mass to the integrated position using Euler integration at the high-level command rate, abstracting away low-level joint and contact dynamics to speed up training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Kinematic simulation (implemented within Habitat and iGibson)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Abstracted stepping mode that integrates commanded CoM velocities at 1 Hz and teleports the robot to the resulting pose, with collision checks to prevent illegal placements; avoids simulating joint torques, foot contacts, and high-frequency dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied AI (abstracted agent simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Low-fidelity: intentionally omits low-level rigid-body dynamics and joint-level control; models only discrete CoM motion using Euler integration and collision checking.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>No high-frequency physics integration; teleportation/Euler integration of CoM at 1 Hz; collision checks using coarse URDF-based checks; can optionally inject low-dimensional actuation noise (Gaussian) fitted from hardware data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>High-level visual navigation policy (ResNet-18 + 2-layer LSTM trained with DD-PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learning-based RL agent producing CoM velocity commands; trained in the kinematic simulator to reason about navigation without learning low-level dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal navigation requiring high-level spatial reasoning and obstacle avoidance based on depth observations and egomotion (but not low-level contact control).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Kinematic-trained agents: converged faster (3 days) and had access to ~500M steps of experience, learning policies that achieved higher evaluation success rates in-sim and excellent sim2real transfer (see transfer_performance).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real Spot robot (zero-shot), dynamic simulation evaluations, and other simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Zero-shot sim2real: kinematic-trained agents achieved 100% success rate and SPL ~82-83% on Spot in the LAB environment; adding learned actuation noise (6,000 samples fitted Gaussian) improved SPL by up to ~5.6% and reduced collisions/action counts.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Kinematic (low-fidelity) training produced better transfer and generalization than dynamic (high-fidelity) training for high-level navigation tasks; authors attribute this to reduced overfitting and the ability to train on ~10× more steps due to faster simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors explicitly state that for tasks representable with abstract action spaces (navigation), low-fidelity kinematic simulation is sufficient and often preferable; essential low-level characteristics can be modeled with low-dimensional noise derived from small real-world datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Kinematic simulation can omit necessary low-level dynamics if the task fundamentally requires contact reasoning or fine manipulation; authors note such tasks (dexterous manipulation, low-level walking controllers) still require high-fidelity simulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1333.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1333.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic (rigid-body/contact) simulation with low-level controllers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-fidelity simulation mode modeling rigid-body mechanics, contact dynamics and low-level controllers (Raibert-style or MPC) at high control frequency (240 Hz); used for dynamic policy training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Dynamic simulation (Bullet physics via Habitat/iGibson)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>High-fidelity simulation that converts CoM velocity commands into joint torques via a low-level controller and simulates full rigid-body/contact dynamics at 240 Hz.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / contact dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity relative to kinematic mode: simulates joint torques, foot placement, inverse kinematics, contact forces and high-frequency closed-loop controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Physics step-size of 1/240 s; models footstep generation, IK, joint PD (Raibert) or full MPC torque control; heavy computational cost leading to slower wall-clock training speed and fewer environment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Dynamic-policy high-level visual navigation agent + low-level Raibert/MPC controllers</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>High-level RL policy (ResNet-18 + LSTM) outputs desired CoM velocities; low-level controllers (Raibert-style for training; MPC used for evaluation) convert velocities to joint torques applied at 240 Hz.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal navigation that must be executed through simulated low-level locomotion controllers and contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Dynamic-trained policies required more wall-clock time to converge (7 days) but produced less total environment steps (~50M) and often underperformed compared to kinematic-trained policies in cross-simulator and sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Other simulators/controllers (e.g., Raibert->MPC) and real hardware (Spot with manufacturer black-box controller).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Dynamic policies showed poor transfer: sim2sim gaps large (e.g., Habitat-Dynamic to iGibson-Dynamic drops reported) and real-world SRs of 40-67% (lower SPL and more collisions than kinematic-trained agents).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Dynamic (high-fidelity) training led to overfitting to precise simulated dynamics and low-level controller behavior; dynamic policies were brittle to controller/simulator changes and had worse real-world success than kinematic agents.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>While dynamic fidelity models low-level interactions, the paper concludes that such fidelity is not necessary (and can be detrimental) for learning high-level navigation policies unless the task explicitly requires low-level contact reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Dynamic training fails when the simulated low-level controller differs from hardware (closed-source controller) or when training scale is limited by slow simulation; dynamic policies tend to command slower velocities and get stuck, increasing collisions on hardware.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1333.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1333.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Actuation Noise Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-dimensional Gaussian actuation noise model (Spot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gaussian noise model of actuation error (difference between commanded and actual CoM velocities) collected from real Spot hardware and injected into kinematic simulation to improve robustness and sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Actuation noise injection into kinematic simulation (learned from Spot hardware)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A simple statistical model (bivariate Gaussian per motion dimension, diagonal covariance) fitted to 6,000 real-world velocity tracking samples (decoupled and coupled) used to perturb commanded velocities during kinematic training.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied AI (actuation modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Low-dimensional fidelity addition to otherwise low-fidelity simulation: does not model full joint/contact dynamics but captures coarse actuation noise (mean and variance) in linear/lateral/angular velocity tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Gaussian noise parameters per dimension derived from 6,000 samples (2,000 per direction for decoupled dataset); supports decoupled and coupled noise models; sampling applied to policy-predicted velocities before integration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>High-level navigation policy (ResNet-18 + 2-layer LSTM) trained in kinematic sim with noise</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same high-level RL agent as other experiments; during training, sampled Gaussian noise added to predicted CoM velocities to emulate hardware actuation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>PointGoal navigation made robust to actuation uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Policies trained with injected actuation noise still converged in the kinematic regime and achieved high performance; training specifics similar to other kinematic experiments (fast, many steps).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real Spot robot (zero-shot sim2real).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Kinematic policies trained with decoupled and coupled actuation noise both achieved 100% success on Spot; SPL improved by ~4.6% (decoupled) and ~5.6% (coupled) compared to kinematic training without noise; collisions and commanded actions decreased.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Adding low-dimensional, hardware-derived actuation noise to kinematic simulation improved real-world path efficiency and safety while retaining the benefits of fast low-fidelity training; coupled noise performed slightly better than decoupled.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors conclude that minimal low-dimensional modeling (actuation noise) is sufficient to capture key hardware mismatches for high-level navigation policies, avoiding the need for full dynamic fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure reported for the noise model itself; authors caution that more accurate or stateful noise models might be needed for different robots or more precise controllers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Habitat: A Platform for Embodied AI Research <em>(Rating: 2)</em></li>
                <li>igibson 1.0: a simulation environment for interactive tasks in large realistic scenes <em>(Rating: 2)</em></li>
                <li>DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames <em>(Rating: 2)</em></li>
                <li>Pybullet, a python module for physics simulation for games, robotics and machine learning <em>(Rating: 1)</em></li>
                <li>Sim-to-Real: Learning Agile Locomotion for Quadruped Robots <em>(Rating: 1)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1333",
    "paper_id": "paper-251018675",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Habitat",
            "name_full": "Habitat (AI Habitat)",
            "brief_description": "A high-throughput embodied AI simulator for photorealistic indoor environments that integrates with a C++ Bullet physics backend; used here for training and evaluation of visual navigation policies.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Habitat",
            "simulator_description": "Photorealistic indoor environment simulator for embodied agents; provides rendering, scene datasets (HM3D) and integrates with Bullet physics via a C++ binding for dynamic simulation or a kinematic stepping mode for faster abstracted simulation.",
            "scientific_domain": "mechanics / embodied AI (robot navigation)",
            "fidelity_level": "Operable at multiple fidelity levels: can run kinematic (teleport/Euler integration) low-fidelity stepping or dynamic (Bullet rigid-body/contact) high-fidelity simulation.",
            "fidelity_characteristics": "Supports photorealistic rendering and collision checking; dynamic mode uses Bullet physics at high frequency; kinematic mode teleports agent using Euler integration and skips low-level joint dynamics; noted to run ~1200% faster than iGibson in this work.",
            "model_or_agent_name": "High-level visual navigation policy (ResNet-18 + 2-layer LSTM trained with DD-PPO)",
            "model_description": "Reinforcement learning agent: ResNet-18 visual encoder, 2-layer LSTM policy head parameterizing Gaussian action distribution; trained with distributed DD-PPO.",
            "reasoning_task": "PointGoal visual navigation (egocentric depth + egomotion to command center-of-mass velocities to reach specified relative goal coordinates).",
            "training_performance": "Kinematic training: converged in ~3 days, learned from ~500M steps of experience; Dynamic training: converged in ~7 days, learned from ~50M steps (approx.). Exact per-simulator SRs reported in paper (see transfer/perf).",
            "transfer_target": "Other simulator (iGibson) and real-world Spot robot (zero-shot sim2real evaluation in LAB environment).",
            "transfer_performance": "Kinematic-trained policies transferred best: achieved 100% success rate and 82-83% SPL in real-world Spot experiments; dynamic-trained policies transferred worse (real-world SR 40-67% and lower SPL). Also kinematic policies often outperformed dynamic ones when evaluated across simulators (e.g., A1: 62.1% SR kinematic vs 24.2% dynamic in iGibson eval in one reported case).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Across robots and simulators, low-fidelity kinematic training produced better generalization and sim2real transfer than high-fidelity dynamic training. Kinematic policies learned from ~10× more steps (500M vs 50M) because kinematic simulation is much faster, and they outperformed dynamic policies both in-sim (often) and on real Spot robot (100% SR vs 40-67% SR for dynamic). Dynamic policies tended to overfit to simulator-specific low-level dynamics and performed poorly when evaluated in different simulators or on hardware.",
            "minimal_fidelity_discussion": "Authors argue that for high-level tasks that can be abstracted (like navigation), low-level dynamics fidelity is unnecessary and can be harmful; they recommend abstracting away low-level controllers and prioritizing simulation speed, while optionally adding low-dimensional actuation noise learned from hardware.",
            "failure_cases": "Dynamic (high-fidelity) simulations failed to transfer well: overfitting to specific low-level controllers (Raibert-style vs MPC) led to poor sim2sim and sim2real transfer; dynamic training was slower (fewer environment steps) causing impoverished experience and more collisions on hardware. Also, for tasks requiring low-level interactions (e.g., dexterous manipulation) high fidelity remains necessary.",
            "uuid": "e1333.0"
        },
        {
            "name_short": "iGibson",
            "name_full": "iGibson",
            "brief_description": "A photorealistic simulator for interactive tasks in large realistic indoor scenes that integrates with PyBullet; used here for training and cross-simulator evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "iGibson",
            "simulator_description": "Photorealistic indoor environment simulator built for interactive tasks, leveraging PyBullet for physics; used here for both kinematic and dynamic simulations and cross-evaluation against Habitat.",
            "scientific_domain": "mechanics / embodied AI (robot navigation)",
            "fidelity_level": "Operable at multiple fidelity levels in experiments: kinematic (abstracted teleport/Euler integration) and dynamic (PyBullet rigid-body/contact dynamics).",
            "fidelity_characteristics": "Renders photorealistic scenes; dynamic mode uses PyBullet (Python binding to Bullet) with physics integration and contact dynamics; noted to be substantially slower than Habitat in the authors' setup.",
            "model_or_agent_name": "High-level visual navigation policy (ResNet-18 + 2-layer LSTM trained with DD-PPO)",
            "model_description": "Reinforcement learning agent identical to the one used with Habitat: convolutional encoder + recurrent policy, trained with DD-PPO.",
            "reasoning_task": "PointGoal visual navigation in cluttered indoor environments.",
            "training_performance": "When trained dynamically in iGibson, policies underperformed compared to kinematic training (examples: A1 dynamic 24.2% SR vs kinematic 62.1% SR when evaluated in iGibson in a reported case). Dynamic training had less experience due to slower simulation.",
            "transfer_target": "Other simulator (Habitat) and real-world Spot robot.",
            "transfer_performance": "Dynamic-trained policies showed large drops when evaluated across simulators (e.g., Habitat-&gt;iGibson dynamic gap reported), indicating poor generalization; kinematic-trained policies generalized better across simulators and to real robot (100% SR on Spot).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Same pattern as with Habitat: kinematic-trained agents outperformed dynamic-trained agents in cross-simulator evaluation and in sim2real transfer; dynamic agents overfit to simulator/controller specifics and were sensitive to simulator changes.",
            "minimal_fidelity_discussion": "Authors indicate high-fidelity dynamic simulation in iGibson is not necessary for training high-level navigation policies and can reduce effective training scale due to lower simulation speed.",
            "failure_cases": "Slower simulation speed reduced amount of experience available for learning; dynamic policies trained in iGibson showed poor transfer to Habitat and hardware.",
            "uuid": "e1333.1"
        },
        {
            "name_short": "Bullet / PyBullet",
            "name_full": "Bullet Physics Engine (and PyBullet)",
            "brief_description": "A rigid-body physics engine used for dynamic simulation of contact and joint dynamics; Bullet is used by Habitat (C++ integration) and PyBullet is used by iGibson (Python binding) in the experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Bullet / PyBullet (physics engine)",
            "simulator_description": "Rigid-body physics engine that simulates contacts, collisions, and joint dynamics; in the dynamic experiments it is used at high frequency (240 Hz) to produce realistic low-level dynamics.",
            "scientific_domain": "mechanics / contact dynamics",
            "fidelity_level": "High-fidelity relative to kinematic stepping: simulates rigid-body dynamics, contact events, joint torques and controllers, with small physics timestep (1/240 s in experiments).",
            "fidelity_characteristics": "Simulates contact dynamics, joint torques, and high-frequency controller loops (240 Hz); uses a physics step-size of 1/240 s for dynamic simulation. However, real-world controllers (closed-source) may still differ causing sim2real mismatch.",
            "model_or_agent_name": "Dynamic low-level controllers (Raibert-style controller used for training; MPC used for evaluation) + high-level RL policy",
            "model_description": "Two low-level controllers are used with Bullet: an expert Raibert-style controller (footstep generator + IK + joint PD) and an MPC controller (direct torque commands). High-level agent is ResNet-18 + 2-layer LSTM trained with DD-PPO.",
            "reasoning_task": "High-level navigation decision-making while low-level dynamics are simulated by Bullet (agency: mapping CoM velocity commands to joint torques and contact dynamics).",
            "training_performance": "Dynamic training produced fewer environment steps due to computational cost (approx. 50M steps vs 500M for kinematic) and underperformed on transfer despite sometimes achieving higher in-domain metrics early; specific in-sim SR examples: Habitat-Dynamic better within-domain but poor cross-eval (e.g., Habitat Dynamic 49.8% vs iGibson Dynamic 22.3% for A1 in one comparison).",
            "transfer_target": "Other simulators (iGibson/Habitat) and the real Spot robot (hardware) with manufacturer low-level controllers.",
            "transfer_performance": "Dynamic policies showed poor sim2sim and sim2real transfer: lower real-world SR (40-67%) and higher collision counts relative to kinematic-trained policies; also sensitive to switching low-level controllers (Raibert-&gt;MPC) during evaluation.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "High-fidelity (Bullet-based) dynamic simulation led to overfitting to simulator/controller specifics and worse transfer; despite modeling contacts and joint dynamics, the mismatch to hardware controllers reduced real-world performance.",
            "minimal_fidelity_discussion": "Paper suggests that full low-level physics fidelity (as in Bullet) is not necessary and can be harmful for training high-level navigation policies that will be executed with hardware controllers; instead, abstracted simulation plus low-dimensional noise is recommended.",
            "failure_cases": "Dynamic simulations failed to transfer robustly when training used one dynamic controller (Raibert) and evaluation used another (MPC) or hardware closed-loop controller; dynamic policies also suffered from limited training scale due to slow simulation.",
            "uuid": "e1333.2"
        },
        {
            "name_short": "Kinematic Simulation",
            "name_full": "Kinematic (abstracted) simulation / kinematic controller",
            "brief_description": "A low-fidelity simulation mode that teleports the robot's center-of-mass to the integrated position using Euler integration at the high-level command rate, abstracting away low-level joint and contact dynamics to speed up training.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Kinematic simulation (implemented within Habitat and iGibson)",
            "simulator_description": "Abstracted stepping mode that integrates commanded CoM velocities at 1 Hz and teleports the robot to the resulting pose, with collision checks to prevent illegal placements; avoids simulating joint torques, foot contacts, and high-frequency dynamics.",
            "scientific_domain": "mechanics / embodied AI (abstracted agent simulation)",
            "fidelity_level": "Low-fidelity: intentionally omits low-level rigid-body dynamics and joint-level control; models only discrete CoM motion using Euler integration and collision checking.",
            "fidelity_characteristics": "No high-frequency physics integration; teleportation/Euler integration of CoM at 1 Hz; collision checks using coarse URDF-based checks; can optionally inject low-dimensional actuation noise (Gaussian) fitted from hardware data.",
            "model_or_agent_name": "High-level visual navigation policy (ResNet-18 + 2-layer LSTM trained with DD-PPO)",
            "model_description": "Learning-based RL agent producing CoM velocity commands; trained in the kinematic simulator to reason about navigation without learning low-level dynamics.",
            "reasoning_task": "PointGoal navigation requiring high-level spatial reasoning and obstacle avoidance based on depth observations and egomotion (but not low-level contact control).",
            "training_performance": "Kinematic-trained agents: converged faster (3 days) and had access to ~500M steps of experience, learning policies that achieved higher evaluation success rates in-sim and excellent sim2real transfer (see transfer_performance).",
            "transfer_target": "Real Spot robot (zero-shot), dynamic simulation evaluations, and other simulators.",
            "transfer_performance": "Zero-shot sim2real: kinematic-trained agents achieved 100% success rate and SPL ~82-83% on Spot in the LAB environment; adding learned actuation noise (6,000 samples fitted Gaussian) improved SPL by up to ~5.6% and reduced collisions/action counts.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Kinematic (low-fidelity) training produced better transfer and generalization than dynamic (high-fidelity) training for high-level navigation tasks; authors attribute this to reduced overfitting and the ability to train on ~10× more steps due to faster simulation.",
            "minimal_fidelity_discussion": "Authors explicitly state that for tasks representable with abstract action spaces (navigation), low-fidelity kinematic simulation is sufficient and often preferable; essential low-level characteristics can be modeled with low-dimensional noise derived from small real-world datasets.",
            "failure_cases": "Kinematic simulation can omit necessary low-level dynamics if the task fundamentally requires contact reasoning or fine manipulation; authors note such tasks (dexterous manipulation, low-level walking controllers) still require high-fidelity simulation.",
            "uuid": "e1333.3"
        },
        {
            "name_short": "Dynamic Simulation",
            "name_full": "Dynamic (rigid-body/contact) simulation with low-level controllers",
            "brief_description": "High-fidelity simulation mode modeling rigid-body mechanics, contact dynamics and low-level controllers (Raibert-style or MPC) at high control frequency (240 Hz); used for dynamic policy training and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Dynamic simulation (Bullet physics via Habitat/iGibson)",
            "simulator_description": "High-fidelity simulation that converts CoM velocity commands into joint torques via a low-level controller and simulates full rigid-body/contact dynamics at 240 Hz.",
            "scientific_domain": "mechanics / contact dynamics",
            "fidelity_level": "High-fidelity relative to kinematic mode: simulates joint torques, foot placement, inverse kinematics, contact forces and high-frequency closed-loop controllers.",
            "fidelity_characteristics": "Physics step-size of 1/240 s; models footstep generation, IK, joint PD (Raibert) or full MPC torque control; heavy computational cost leading to slower wall-clock training speed and fewer environment steps.",
            "model_or_agent_name": "Dynamic-policy high-level visual navigation agent + low-level Raibert/MPC controllers",
            "model_description": "High-level RL policy (ResNet-18 + LSTM) outputs desired CoM velocities; low-level controllers (Raibert-style for training; MPC used for evaluation) convert velocities to joint torques applied at 240 Hz.",
            "reasoning_task": "PointGoal navigation that must be executed through simulated low-level locomotion controllers and contact dynamics.",
            "training_performance": "Dynamic-trained policies required more wall-clock time to converge (7 days) but produced less total environment steps (~50M) and often underperformed compared to kinematic-trained policies in cross-simulator and sim2real transfer.",
            "transfer_target": "Other simulators/controllers (e.g., Raibert-&gt;MPC) and real hardware (Spot with manufacturer black-box controller).",
            "transfer_performance": "Dynamic policies showed poor transfer: sim2sim gaps large (e.g., Habitat-Dynamic to iGibson-Dynamic drops reported) and real-world SRs of 40-67% (lower SPL and more collisions than kinematic-trained agents).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Dynamic (high-fidelity) training led to overfitting to precise simulated dynamics and low-level controller behavior; dynamic policies were brittle to controller/simulator changes and had worse real-world success than kinematic agents.",
            "minimal_fidelity_discussion": "While dynamic fidelity models low-level interactions, the paper concludes that such fidelity is not necessary (and can be detrimental) for learning high-level navigation policies unless the task explicitly requires low-level contact reasoning.",
            "failure_cases": "Dynamic training fails when the simulated low-level controller differs from hardware (closed-source controller) or when training scale is limited by slow simulation; dynamic policies tend to command slower velocities and get stuck, increasing collisions on hardware.",
            "uuid": "e1333.4"
        },
        {
            "name_short": "Actuation Noise Model",
            "name_full": "Low-dimensional Gaussian actuation noise model (Spot)",
            "brief_description": "A Gaussian noise model of actuation error (difference between commanded and actual CoM velocities) collected from real Spot hardware and injected into kinematic simulation to improve robustness and sim2real transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Actuation noise injection into kinematic simulation (learned from Spot hardware)",
            "simulator_description": "A simple statistical model (bivariate Gaussian per motion dimension, diagonal covariance) fitted to 6,000 real-world velocity tracking samples (decoupled and coupled) used to perturb commanded velocities during kinematic training.",
            "scientific_domain": "mechanics / embodied AI (actuation modeling)",
            "fidelity_level": "Low-dimensional fidelity addition to otherwise low-fidelity simulation: does not model full joint/contact dynamics but captures coarse actuation noise (mean and variance) in linear/lateral/angular velocity tracking.",
            "fidelity_characteristics": "Gaussian noise parameters per dimension derived from 6,000 samples (2,000 per direction for decoupled dataset); supports decoupled and coupled noise models; sampling applied to policy-predicted velocities before integration.",
            "model_or_agent_name": "High-level navigation policy (ResNet-18 + 2-layer LSTM) trained in kinematic sim with noise",
            "model_description": "Same high-level RL agent as other experiments; during training, sampled Gaussian noise added to predicted CoM velocities to emulate hardware actuation errors.",
            "reasoning_task": "PointGoal navigation made robust to actuation uncertainty.",
            "training_performance": "Policies trained with injected actuation noise still converged in the kinematic regime and achieved high performance; training specifics similar to other kinematic experiments (fast, many steps).",
            "transfer_target": "Real Spot robot (zero-shot sim2real).",
            "transfer_performance": "Kinematic policies trained with decoupled and coupled actuation noise both achieved 100% success on Spot; SPL improved by ~4.6% (decoupled) and ~5.6% (coupled) compared to kinematic training without noise; collisions and commanded actions decreased.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Adding low-dimensional, hardware-derived actuation noise to kinematic simulation improved real-world path efficiency and safety while retaining the benefits of fast low-fidelity training; coupled noise performed slightly better than decoupled.",
            "minimal_fidelity_discussion": "Authors conclude that minimal low-dimensional modeling (actuation noise) is sufficient to capture key hardware mismatches for high-level navigation policies, avoiding the need for full dynamic fidelity.",
            "failure_cases": "No specific failure reported for the noise model itself; authors caution that more accurate or stateful noise models might be needed for different robots or more precise controllers.",
            "uuid": "e1333.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Habitat: A Platform for Embodied AI Research",
            "rating": 2,
            "sanitized_title": "habitat_a_platform_for_embodied_ai_research"
        },
        {
            "paper_title": "igibson 1.0: a simulation environment for interactive tasks in large realistic scenes",
            "rating": 2,
            "sanitized_title": "igibson_10_a_simulation_environment_for_interactive_tasks_in_large_realistic_scenes"
        },
        {
            "paper_title": "DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "rating": 2,
            "sanitized_title": "ddppo_learning_nearperfect_pointgoal_navigators_from_25_billion_frames"
        },
        {
            "paper_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning",
            "rating": 1,
            "sanitized_title": "pybullet_a_python_module_for_physics_simulation_for_games_robotics_and_machine_learning"
        },
        {
            "paper_title": "Sim-to-Real: Learning Agile Locomotion for Quadruped Robots",
            "rating": 1,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 1,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        }
    ],
    "cost": 0.01580175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation</p>
<p>Joanne Truong truong.j@gatech.edu 
Georgia Institute of Technology</p>
<p>Max Rudolph maxrudolph@gatech.edu 
Georgia Institute of Technology</p>
<p>Naoki Yokoyama nyokoyama@gatech.edu 
Georgia Institute of Technology</p>
<p>Sonia Chernova chernova@gatech.edu 
Georgia Institute of Technology</p>
<p>Dhruv Batra dbatra@gatech.edu 
Georgia Institute of Technology</p>
<p>MetaAI</p>
<p>Akshara Rai akshararai@fb.com 
MetaAI</p>
<p>Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation
Sim2RealDeep Reinforcement LearningVisual-Based Navigation
If we want to train robots in simulation before deploying them in reality, it seems natural and almost self-evident to presume that reducing the sim2real gap involves creating simulators of increasing fidelity (since reality is what it is). We challenge this assumption and present a contrary hypothesis -sim2real transfer of robots may be improved with lower (not higher) fidelity simulation. We conduct a systematic large-scale evaluation of this hypothesis on the problem of visual navigation -in the real world, and on 2 different simulators (Habitat and iGibson) using 3 different robots (A1, AlienGo, Spot). Our results show that, contrary to expectation, adding fidelity does not help with learning; performance is poor due to slow simulation speed (preventing large-scale learning) and overfitting to inaccuracies in simulation physics. Instead, building simple models of the robot motion using real-world data can improve learning and generalization.</p>
<p>Introduction</p>
<p>The sim2real paradigm consists of training robots in simulation (potentially for billions of simulation steps corresponding to decades of experience [1]) before deploying them in reality. The last few years have seen significant investments -the development of new simulators [2][3][4][5][6][7][8][9][10][11][12], curation and annotation of 3D scans and assets [13][14][15], and development of techniques for overcoming the sim2real gap [16][17][18][19] -resulting in a number of successful demonstrations of sim2real transfer [20][21][22][23][24][25]. However, no simulator is a perfect replica of reality and the main challenge in this paradigm is overcoming the sim2real gap, defined as the drop in a robot's performance in the real-world (compared to simulation). It seems natural and almost self-evident to presume that reducing this sim2real gap involves creating simulators of increasing physics fidelity, and this sometimes forms the default operating hypothesis of the field.</p>
<p>We challenge this convention and present a counter-intuitive idea -sim2real transfer of robots may be improved not by increasing but by decreasing simulation fidelity. Specifically, we propose that instead of training robots entirely in simulation, we use classical ideas from hierarchical robot control [26] to decompose the policy into a 'high-level policy' (that is trained solely in simulation) and a 'low-level controller' (that is designed entirely on hardware and may even be a black-box controller shipped by a manufacturer). This decomposition means that the simulator does not need to model low-level dynamics, which can save both simulation time (since there is no need to simulate expensive low-level controllers), and developer time spent building and designing these controllers.</p>
<p>We conduct a systematic large-scale evaluation of our hypothesis on the task of PointGoal (visual) Navigation [27] in unknown environments -using 2 simulators (Habitat and iGibson) and 3 different robots (A1, AlienGo, Spot). We train policies using two physics fidelities -kinematic and dynamic. Kinematic simulation uses abstracted physics and 'teleports' the robot to the next state using Euler integration; kinematic policies command robot center-of-mass (CoM) linear and angular velocities. Dynamic simulation consists of rigid-body mechanics and simulates contact dynamics (via Bullet [12]); dynamic policies command CoM linear and angular velocities, which are converted to robot joint-torques by a low-level controller operating at 240 Hz. We find that across all robots, a kinematically trained policy outperforms dynamic policies, even when evaluated using dynamic simulation and control. Additionally, we show that the trained kinematic policy can be transferred to a real  Figure 1: Left: We train visual navigation policies at two levels of fidelity -kinematic and dynamic. In kinematic control (top), the robot is 'teleported' to the next state using Euler integration. In dynamic control (bottom), the robot's velocity commands are converted to leg joint-torques and rigid-body physics is simulated at 240Hz. Right: We evaluate the kinematic and dynamic trained policies in simulation (top) and the real-world (bottom) across 5 identical episodes. The kinematic policy achieves a 100% success rate in all 5 episodes, and the robot takes similar paths in both simulation and the real-world. On the other hand, the dynamic policy achieves a 20-60% success rate, and the trajectories taken in simulation and the real-world do not correlate, pointing towards a larger sim2real gap.</p>
<p>Spot robot, which ships with manufacturer-provided 'black-box' low-level controllers that cannot be accurately simulated. In contrast, dynamic policies fail to achieve efficient navigation behavior on Spot, due to the sim2real gap and less simulation experience.</p>
<p>The reasons for these improvements are perhaps unsurprising in hindsight -learning-based methods overfit to simulators, and present-day physics simulators have approximations and imperfections that do not transfer to the real-world. A second equally significant mechanism is also in playlower fidelity simulation is typically faster, enabling policies to be trained with more experience under a fixed wall-clock budget. Even when the kinematic policies were trained for 2.3× less wallclock time than the dynamic policies (with the same compute), the kinematic policies were able to learn from 10× the amount of data. While our results are presented on legged locomotion and visual navigation, the underlying principle -of architecting hierarchical policies and only training the highlevel policy in an abstracted simulation -is broadly applicable. We hope that our work leads to a rethink in how the research community pursues sim2real and in how we develop the simulators of tomorrow. Specifically, our findings suggest that instead of investing in higher-fidelity physics, the field should prioritize simulation speed for tasks that can be represented with abstract action spaces.</p>
<p>Related Work</p>
<p>Visual Navigation. Recent works have shown that large-scale indoor environments and simulators like Habitat [2,3] and iGibson [4] can enable end-to-end learning of navigation policies from large amounts of agent-or expert-generated data [28][29][30] on simple, wheeled systems. This is in contrast to the typical mapping and planning paradigm used in classical robotics, which can suffer when the quality of maps is low [31] or requires expensive equipment like LiDAR [32]. In this work, we show that such end-to-end learning is also possible for complex, legged robots.</p>
<p>Sim2real for Legged Robots. Sim2real quadrupedal locomotion has been widely studied in the past several decades [22,[33][34][35][36], with most learning low-level skills in simulation and transferring them to hardware [37], or adapting them online to reduce the sim2real gap [38,39]. However, these policies are typically blind, and use only proprioceptive sensors on the robot to determine actions [23,25]. In contrast, an autonomous robot needs to respond to its environment, and take visual input into account. Some works have proposed learning visual policies in simulation and applying them to the real-world [22,24,40], and other works leverage expensive LiDAR sensors for external sensing [41]. These works use learned or hand-designed physically simulated low-level controllers; we show that physics simulations can be detrimental to learning high-performing sim2real policies, even for complex legged robots. Work from [42] also utilize similar simulator simplifications to increase simulation speed in training navigation policies for 1 robot in a single room, but does not discuss how this formulation affects sim2real transfer. In our work we present a rigorous (multirobot, multi-simulator) study of the effect of different simulation fidelities on visual navigation.</p>
<p>Abstracted Task-space Learning. Abstracted (hierarchical, high-level) action spaces are common in robotics literature. Examples include task and motion planning for manipulation [43][44][45], legged locomotion [46,47], navigation [20], etc. Several works reason over symbolic actions like pick and place, or hierarchical policies with discrete/continuous attributes [33-35, 48, 49], or even abstracted dynamics models [36]. While the ideas of abstracted/hierarchical policies are fairly common, typically both the high-and low-level policies are learned in simulation and transferred to reality [33,34,36], often augmented with techniques like domain randomization [37] and real-word adaption [18]. Instead, we use an abstracted simulator, which does not model low-level physics, and learn high-level policies that are transferred to the real-world in a zero-shot manner.</p>
<p>Experimental Setup</p>
<p>Task: PointGoal Navigation. In the task of PointGoal Navigation [27], a robot is initialized in an unknown environment and is tasked with navigating to a goal coordinate without access to a pre-built map of the environment. The goal is specified relative to the robot's starting location for the episode (i.e., "go to ∆x, ∆y"). The robot has access to an egocentric depth sensor and an egomotion sensor (sometimes referred to as GPS+Compass in this literature) from which the robot derives the goal location relative to its current pose. An episode is considered successful when the robot reaches the goal position within a success radius (typically half of the robot's body length). The robot operates within constraints of maximum number of steps per episode (150 for Spot) and velocity limits (± 0.5 m/s for linear and ± 0.3 rad/s for angular velocities on Spot). We linearly scale the linear and angular velocity limits for A1 and Aliengo to be proportional to the length of each robot's leg, and inversely scale the maximum number of steps allowed. In effect, smaller robots have a smaller maximum allowed velocity to improve stability during execution, but are allowed more steps to reach the goal. The exact parameters used for each robot is described in the appendix. For evaluation, we report the success rate (SR), and Success inversely weighted by Path Length (SPL) [27], which measures the efficiency of the trajectory taken with respect to the ground-truth shortest path.</p>
<p>Robot Platforms. We study visual navigation for 3 quadrupedal robots -A1 and Aliengo from Unitree [50], and Spot from Boston Dynamics (BD) [51] in simulation. In the real-world, we show sim2real transfer of the learned navigation policies to Spot. To have a consistent camera setup across all the robots, we attach an Intel RealSense D435 camera to Spot in the real-world, and use this camera for visual inputs to the policy. In our hardware experiments, we want to measure how often our sim2real policies lead to collisions without jeopardizing safety. We achieve this balance as follows: the BD collision-avoidance capability is kept turned on, set to trigger at a tight threshold of 0.10m. Next, we track the number of times the robot comes within 0.20m of any obstacle (as measured by any of the 5 onboard depth cameras). This gap (between 0.20m and 0.10m) allows us to record possible collisions while preventing actual ones. While the BD API allows for highlevel navigation without access to a map, it cannot navigate around obstacles autonomously, without a map. In our work, we consider complex, long-range navigation paths (up to 30m) in cluttered environments with many obstacles; the goals are unreachable with the just BD navigation API.</p>
<p>Simulation Environments. We use two simulation platforms -Habitat [2,3] and iGibson [4] for training and evaluation. Both simulators support rendering of photorealistic environments; Habitat uses a low-level (C++) integration with the Bullet physics engine [12], while iGibson leverages Py-Bullet, the Python-based integration of Bullet. Thus, while the underlying physics engines between A1 Aliengo Spot Spot-Real the two are the same, Habitat runs ∼ 1200% faster than iGibson [3]. This allows us to train policies faster with Habitat than with iGibson even when using identical policies and compute.</p>
<p>Dataset. For training and evaluation, we use a combination of the Habitat-Matterport (HM3D) [13] and Gibson [52] 3D datasets. The two datasets combined consist of over 1000 high-resolution 3D scans of real-world indoors environments, and consists of realistic clutter. We generate training and evaluation episodes compatible with our robots for the HM3D and Gibson scenes following the procedure described in [2]. Specifically, we restrict the geodesic distance from the start and positions to be between 1 and 30m, and increase navigation complexity by rejecting paths that consist of nearstraight lines, with few obstacles. As described in [2], both of these heuristics result in complex, but navigable paths. Additionally, we check for collisions along the sampled paths using the URDF of the largest robot (Spot) to ensure that all paths are navigable. Real-World Test Environment. The realworld evaluation environment, LAB, is a 325m 2 lobby in a commercial office building. The lobby contains furniture such a couches, cushions, bookshelves and tables. We specify a set of 5 waypoints as the start and end locations for the navigation episodes in LAB with an average episode length of 10m. We match the furniture layout to the position captured in the 3D scan ( Figure 3) to run identical evaluation experiments in both simulation and the realworld. The scan of LAB is not part of training.</p>
<p>Kinematic and Dynamic Control for Visual Navigation</p>
<p>As illustrated in Figure 4, our proposed approach is hierarchical, with (1) a high-level visual navigation policy that commands desired center of mass (CoM) motion at 1Hz, and (2) a low-level controller that follows this desired motion. We consider controllers at two levels of abstraction -'kinematic' and 'dynamic'. The kinematic controller simply integrates the desired velocity and outputs a CoM position at 1Hz; kinematic simulation then teleports the robot to the desired state. The dynamic controller uses a low-level controller that commands joint torques at 240Hz; dynamic simulation models rigid-body and contact dynamics via Bullet (with a physics step-size of 1 /240 sec). We provide details of all three of these pieces (high-level policy, kinematic and dynamic controllers) next.</p>
<p>High-level Visual Navigation Policies. The high-level policy takes as input an egocentric depth image, and the goal location relative to the robot's current pose. The output of the policy is a 3-dimensional vector, representing the desired CoM forward, lateral, and angular velocities (V x , V y , ω). The neural network architecture consists of a ResNet-18 visual encoder and a 2-layer LSTM policy. Using a recurrent policy allows the policy to learn temporal dependencies through the hidden state. The final layer of the policy parameterizes a Gaussian action distribution from which the action is sampled. The policy is trained using DD-PPO [1], a distributed reinforcement learning method, in both the Habitat and iGibson simulators. Our reward function is derived from [22], with an added penalty for backward velocities, which can lead to collisions and hurts performance.</p>
<p>Kinematic Control and Simulation. In kinematic control, the final state of the robot is calculated by integrating the desired CoM velocity commanded by the high-level navigation policy at 1Hz. The robot is directly moved to the desired pose, without running a physics simulation. In both Habitat and iGibson, the robot is kept in place if being at the new desired state would result in a collision. The objective of the kinematic control is to abstract away the low-level physics interactions between the robot and its environment. This has two advantages: (1) it avoids the need to accurately model low-level controllers, especially for closed-source robots like Spot; (2) it enables faster simulation speed by avoiding high-frequency physics integration, conducive to model-free RL that requires large amounts of experience. On the other hand, teleporting the robot to the desired state might remove necessary dynamics, such as poor tracking of low-level controllers. In Section 5, we propose how to incorporate such low-level characteristics into a kinematic simulation using real-world data.</p>
<p>Dynamic Control in Simulation and Hardware. We experiment with two different low-level dynamic controllers for quadruped robots. The first is an expert-designed Raibert-style controller from [22], which consists of a footstep generator and an inverse kinematic solver that commands desired joint angles from CoM velocities. The joint angles are converted to joint torques using a linear feedback controller, and applied to the simulation. This controller was shown to achieve sim2real transfer for A1 [22]. However, on other robots in our experiments, it shows relatively poor tracking of highlevel commands. Thus, we also experiment with another model-predictive control (MPC) dynamic controller from [38], which commands joint torques directly. This controller has been applied to real-world A1 robot [53,54] and shows better tracking of desired velocities for our test robots, as compared to the Raibert controller from [22]. However, MPC is prohibitively slow and cannot be used for training RL policies. Thus, we use Raibert for training dynamic policies, but evaluate using MPC. 1 This difference in train and evaluation dynamics controllers has multiple purposes: (1) the evaluation using MPC improves performance of most policies, including dynamic policies, due to its better ability to track high-level commands; (2) the difference between the two dynamic controllers in simulation is also a proxy for the difference between our low-level controllers and closed-source controllers from Spot. If a dynamic policy cannot transfer from Raibert to MPC, it has a low chance of transfer to Spot which has black box BD controllers, or even other robots in the real-world.</p>
<p>Both dynamic controllers model the low-level physics interactions between the robot and the environment. This makes them considerably slower than the kinematic controller, making training RL policies challenging. Moreover, for Spot, the low-level controller implementation is not openly available, making it hard to reproduce the low-level controller in simulation. For commercial legged robots that come with black-box controllers, kinematic simulations are the ideal fidelity for learning navigation policies. Our experiments in Section 5 show that the added fidelity of dynamic controllers does not benefit policy learning, or sim2real transfer.</p>
<p>Impact of Low-level Controllers on Policy Learning. The low-level controller used in dynamic simulations can have a significant impact on the learned policy. Low-level controllers both in sim and real are biased, and the status quo is to make them biased in the same way, as the policy learns to compensate for the bias error. For example, [55] add learned actuation noise to their simulation, while [37] measure hardware characteristics and add them to the simulation. However, given the high-dimensional nature of low-level physics, it is very difficult to ensure that the biases incorporated in simulation actually hold for a larger range of motions that the data was collected on. Thus, there are several iterations of data-collection, bias updates, training and deployment needed for good performance. Instead, kinematic controllers are unbiased by design, and can easily incorporate hardware bias through low-dimensional CoM motion noise models created from a small amount of real-world data, as shown in our experiments in Section 5.</p>
<p>Results and Analysis</p>
<p>In this section, we first study generalization of visual navigation policies across simulators (trained in one sim, tested in another) and across controllers (trained with one controller, tested with another). This shows the importance of fast simulation for learning high-level policies by comparing performance of kinematic and dynamic policies trained for the same wall-clock time. Next, we examine the performance of the different policies at zero-shot sim2real transfer on the Spot robot.</p>
<p>How large is the sim2sim gap? High for dynamic, and low for kinematic policies. We exhaustively study the combinatorial space of experiments -policies trained under 2 training conditions (with kinematic and dynamic simulation) × 2 evaluation conditions (kinematic and dynamic simulation) × 2 simulators (Habitat and iGibson) × 3 robots (A1, Aliengo, Spot). For each condition, we train and report results with 3 random seeds. Each policy is trained using 8 GPUs for 3 days, resulting in a cumulative training budget of 6,912 GPU-hours (288 GPU-days). The average success rates are presented in Figure 5. Rows represent the evaluation conditions as tuples (simulator, fidelity), while columns represent the training conditions. We evaluate all policies across 1,100 episodes from 110 unique scenes in the HM3D + Gibson validation split. Figure 5: Average success rates for sim2sim and kinematic2dynamic transfer for A1, Aliengo and Spot. We see that the kinematic trained policies perform the best overall (red quadrants), and also often outperform the dynamic trained policies, even when evaluated using dynamic control (green quadrants vs. orange quadrants).</p>
<p>We make two key observations here:</p>
<ol>
<li>
<p>Kinematic-trained policies perform best overall, for all robots. In all cases, kinematic policies outperform the dynamic policies, even when evaluated using dynamic control, e.g. 62.1% SR for A1 in (iGibson, Kinematic) vs. 24.2% SR in (iGibson, Dynamic), Fig. 5, left. This is a surprising result because the kinematic policies are being evaluated in an out-of-distribution setting, which was never seen or accounted for during training. On the other hand, the dynamic policies are being evaluated in the domain that they were trained in, hence do not require control-related generalization.</p>
</li>
<li>
<p>Dynamic policies are not robust to different dynamic simulations. The dynamic policies from the two simulations observe significant performance drops when evaluated in the other dynamic simulation. This points to the dynamic policies overfitting to the simulator dynamics during training, failing to generalize to a new setting, see e.g. column 3, rows 3 and 4; 49.8% SR for A1 in (Habitat, Dynamic) vs. 22.3% SR in (iGibson, Dynamic). (iGibson, Dynamic) shows poor performance in both iGibson and Habitat, with slightly poorer performance in Habitat. This sensitivity to simulation makes training dynamics policies difficult, especially when the controller for the real-world robot is unknown. Even if the real-world controller is known, simulation physics and real-world are different, and sim2real transfer of the learned policy can suffer (as evidenced by low sim2sim transfer). On the other hand, kinematic policies, that have been trained with no physics, can generalize to the different dynamic controllers. Both of these results go to show that not only kinematic trained policies are able to learn the task well, they have learned to reason without overfitting to simulation physics, making their chances of successful sim2real transfer high.</p>
</li>
</ol>
<p>Why do kinematic-trained policies outperform dynamic ones? Scale. We plot the evaluation performance of both policies in Habitat kinematic and dynamic simulation in Figure 6. We train both policies to convergence-3 days for the kinematic policies, and 7 days for the dynamic policies. While the kinematic policies are trained for 2.3× less wall-clock time, they still outperform the longer trained dynamic policies, even when evaluated out-of-distribution using dynamic control (+12% SR). Kinematic training is much faster than training dynamically (right, Fig. 6); with kinematic training, the robot is able to learn from approximately 10× more steps of experience (500M steps vs. 50M steps). This increased experience allows the kinematic policies to learn intelligent high-level reasoning. We contend that for any computational budget, there will always be more complex tasks that are bottlenecked by that budget. Wall-clock time is the true limitation for learning-based sim2real approaches (not experience, as different simulators have different speeds). How large is the sim2real gap for kinematic and dynamic trained policies? We evaluate the kinematic and dynamic policies on a Spot robot in the novel LAB environment described in Section 3. Note that scans of LAB were not part of training. We evaluate 3 seeds of each policy over 5 episodes in the real-world and report the average success rate (SR) and Success weighted by Path Length (SPL) [27] in Table 1 (reported as a percentage for readability). Each control type is tested in 15 real-world episodes; one run of the Spot robot navigating LAB is shown in Figure 7. Success in the real-world is measured by computing final distance from the goal position using egomotion estimates provided by the Boston Dynamics SDK.  Table 1: Zero-shot sim2real transfer performance for the visual navigation policies. Success rate (SR) and path efficiency (SPL) are high for kinematic policies, while dynamic policies have lower performance due to the dynamics gap between the low-level control in training and the controller on the robot in the real-world.</p>
<p>As reported in Table 1, all kinematic policies achieve a high success rate of 100% and SPL of 82-83% (rows 3 and 4). On the other hand, the success rate drops to 40-67% for the dynamic policies (rows 1 and 2). We notice that the dynamic policies typically commanded lower velocities, and often get stuck around obstacles (Figure 8). This is shown in the higher number of actions commanded and higher collision count for both dynamic policies; on average, a dynamic policy trained in Habitat took 107.9 actions, and collided 41.2 times (row 1, columns 8 and 9), whereas a kinematic policy also trained in Habitat took 26.4 actions, and collided 3.1 times (row 3, columns 8 and 9). We attribute this to the impoverished experience of the dynamic policies; the policies did not learn robust navigation policies that could avoid obstacles during navigation. Additionally, they overfit to the low-level behavior, which can be unstable at high velocities in sim, but not on hardware. Figure  8 (left) shows that the kinematic policy commands higher forward velocities, while the dynamic Figure 7: One run of the Spot robot navigating the real-world LAB environment using a kinematically trained policy from AI Habitat. The robot successfully navigates a hallway, moves around furniture and turns into the next hallway before stopping. In contrast, the native BD controllers without a map can only reach visible goals.</p>
<p>policy commands slower velocities (right), which are often not achieved by the robot likely due to an obstacle. 2 Successfully executed commands appear on the diagonal.</p>
<p>To improve kinematic simulation fidelity, we model actuation noise (difference between commanded and true velocity) on Spot and use it during kinematic training, similar to [16]. We collect 6,000 samples of decoupled (linear and angular velocities are actuated separately) and coupled (linear and angular velocities are actuated together) actuation noise. The parameters for noise in each dimension, and details about data collection and modeling can be found in the appendix. During training, we sample from the Gaussian distribution for each dimension, and add it to the policy's predicted velocity. We see that policies trained with noise also achieve 100% success in the real-world (rows 5 and 6), and are able to increase path efficiency (SPL) (4.6% using decoupled (row 4 vs. 5), and 5.6% using coupled actuation noise (row 4 vs. 6)). The number of collisions and commanded actions are also lower for these policies, compared to kinematic policies trained with no noise (22.7 actions and 2.8 collisions vs. 26.4 actions and 3.1 collisions). These improvements are due to the added robustness that training with noise provides -uncertainty during training forces the policy to take less risky actions resulting in fewer collisions in the real-world.</p>
<p>Conclusion, Limitations, and Future Work</p>
<p>In this work, we study the role of simulation fidelity for sim2real of visual navigation policies on three simulated and one real legged robot. Contrary to expectations, we find that higher simulation fidelity does not enable learning better high-level visual navigation policies. Dynamic policies tend to overfit to low-level simulation details, resulting in poor transfer to the real-world. On the other hand, kinematic policies are able to generalize well. These results raise important questions about the need for simulation fidelity for sim2real, especially in abstracted action spaces.</p>
<p>One limitation of this work is that we assume access to a robust 'black box' controller on hardware. While most robots come shipped with manufacturer-provided controllers, the level of accuracy may differ between robots, and more robust noise modeling may be needed to better characterize the actuation noise. In the future, we plan to improve the modeling of real-world actuation noise by using a neural network conditioned on previous states and actions of the robot. Another limitation of this work is that we specifically address only tasks that can be abstracted with high-level actions, like navigation or object rearrangement (pick, place, open, close). We agree that there are many tasks that cannot be learned in a kinematic simulation -e.g. tasks that require reasoning about low-level interactions with the environment, like dexterous manipulation, or low-level walking controllers. It is essential to create high-fidelity simulators for sim2real on such tasks. </p>
<p>Robot Details</p>
<p>Additional Evaluation Results</p>
<p>We present additional results using the Raibert controller for evaluation in Figure 1 (row 4). The policies are evaluated across 3 seeds, using the HM3D + Gibson validation split which consists of 1,100 episodes from 110 unique scenes. Our results are consistent with evaluation using the MPC controller-kinematic trained policies still outperform the dynamic trained policies, even when evaluated using dynamic control 1 (68.9 % SR for Aliengo in Habitat, Kinematic vs. 45.4 % SR in Habitat, Dynamic, Fig. 1, middle). Figure 1: Average success rates for sim2sim and kinematic2dynamic transfer for A1, Aliengo and Spot. Dynamic evaluation in iGibson is performed using the Raibert controller [1]. We see that the kinematic trained policies still perform the best overall (red quadrants), and also often outperforms the dynamic trained policies, even when evaluated using dynamic control (green quadrants vs. orange quadrants).</p>
<p>Actuation Noise Modeling Details</p>
<p>We collect actuation noise (difference between the commanded and true velocity of the robot) on the Boston Dynamics Spot robot by commanding the robot at a random velocity for 1Hz in an empty room and measuring the final velocity. Noise is collected in a decoupled and coupled manner described below:</p>
<ol>
<li>Decoupled: Random velocities ( ∼ U(−0.5, 0.5)) are commanded in the forward, lateral, and angular directions separately. When collecting data for the forward direction, the side-ways direction velocity is commanded zero velocity; the opposite is true when collecting data for the sideways direction. We collect 2,000 datapoints for each direction. 2. Coupled: Random velocities ( ∼ U(−0.5, 0.5)) are commanded in the forward, lateral, and angular directions at the same time.</li>
</ol>
<p>Each dataset contains 6,000 data points, with decoupled data containing 2000 data points for each direction. We choose to model the uncertainty in the robot's actuation with a standard bivariate Gaussian with a diagonal variance similar to [2]. The collected data is used to generate mean and variance parameters for a Gaussian distribution describing the noise in each dimension, as shown in Table 2. The Gaussian models are then used to inject noise into the the kinematic simulation during training time through the following method: 1) the policy predicts a velocity, 2) the Gaussian distributions for each direction are sampled, 3) the sampled noise is added to the policy's predicted velocity, and 4) the robot's state is updated according to the noisy velocity.</p>
<p>The two different noise collection approaches aim to study the effects data collection has on the resulting noise model. Our experiments show that both noise models perform better in the realworld than no noise modeling, and coupled noise performs slightly better than decoupled.  It is also important to note that while Spot (and other legged robots) can move in all directions, these robots are not necessarily omnidirectional platforms, since they cannot move in all directions equally well. To illustrate this, we collect displacement errors on Spot in forward and lateral direc- tions while commanding random desired CoM movements ( Figure 2). If the robot were perfectly omnidirectional, we would expect the means and variances for the forward and lateral direction to be small and the same. While the mean error in both directions is close to 0, the standard deviations in the forward and lateral directions are significantly larger and asymmetric. In the forward direction, the standard deviation is 0.097 meters, and in the lateral direction it is 0.139 meters. This tracking error, which increases with commanded velocity, motivated the choice of saturating commanded desired velocity at 0.5 m/s. This behavior is observed on the Spot robot using Boston Dynamics walking controllers, which is a very good, highly tuned controller for the robot. We would expect any open-sourced controller which is not tuned for a particular robot to only be worse.</p>
<p>Additional Low-level Controller Details</p>
<p>We use two different kinds of low-level controllers in our work-an expert-designed Raibert controller from [1] (modified to allow for lateral movement), and a model-predictive control (MPC) controller from [3]. The Raibert controller takes in desired CoM velocities (v x des , v y des , ω des ) from the high-level policy to calculate the desired foot placement location, following equations 1-3 from [1]. The footstep trajectory is followed using inverse kinematics. The MPC controller uses a contact schedule to determine each leg's contact state and compute the optimal joint torque for each leg.</p>
<p>5 Additional Policy Details 5.1 High-level policy parameters.</p>
<p>We use PPO with Generalized Advantage Estimation (GAE). We use a discount factor of 0.99, and GAE parameter of 0.95. We use the Adam optimizer, with a learning rate of 2.5e-4. We run 8 agents in parallel (in different environments) per GPU, and each agent collects a rollout of 128 frames of experience. We use 8 GPUs, for a total of 64 parallel workers.</p>
<p>Reward function.</p>
<p>Our reward function is derived from [1], with an added penalty for backward velocities, which can lead to collisions and hurts performance. Specifically, our reward function is defined as:</p>
<p>r t (a t , s t ) = R geo + R coll + R f all + R success + R slack + R backward (1)</p>
<p>R geo is a shaped reward, denoting the change in geodesic distance to the goal between two timesteps. R coll is a penalty for collisions. We set the collision penalty to -0.03. R f all is a penalty if the robot falls over. We set the falling penalty to -5.0, and terminate the episode. R success is the terminal reward for completing the episode. We set the terminal reward to 10.0. R slack is a slack penalty used to encourage the robot to reach the goal as fast as possible. We set the slack penalty to -0.002. R backward is a penalty for moving backwards, as moving backwards can lead to collisions. We set the backwards penalty to -0.03.</p>
<p>Dynamic Simulation Overfitting Details.</p>
<p>We define overfitting as the drop in performance when testing on a different controller and/or simulator than training. This is a natural generalization of the standard definition of overfitting in supervised learning (accuracy on IID training dataset -accuracy on IID testing dataset). We train dynamic policies for all three robots to congergence (Figure ??) In Table 3, we show the success rate on Habitat-Dynamic (training scenario) -success rate on iGibson-Dynamic (testing scenario) for all 3 robots. Note that these are all evaluations on the same houses/scenes/environments (from a held-out evaluation set) and the only factor changing is the simulator. We can clearly see that the gap is always positive, indicating that policies trained on Habitat-Dynamic perform worse when evaluated on iGibson-Dynamic compared to evaluation on Habitat-Dynamic. As can be expected, in all but one case, the performance gaps are increasing with more RL training, though this is not strictly necessary. A well-trained high-level policy can learn to reason intelligently about navigation (even with dynamic controllers), and then perform well across simulators.  Table 3: We measure the performance gap for dynamic policies between evaluations in Habiat-Dynamic and iGibson-Dynamic. The gap is always postive, and in all cases but one, the performance gaps increase with more RL training, demonstrating that the dynamic policies overfit to the simulator and controller it was trained on.</p>
<p>Figure 2 :
2Robots used for training and evaluation.</p>
<p>Figure 3 :
3The real-world testing environment is a part of a large commercial building and contains clutter from furniture such as tables, bookshelves, and couches.</p>
<p>Figure 4 :
4Our architecture for PointGoal Navigation on a legged robot. A high-level visual navigation policy predicts CoM linear and angular velocities. The velocities are passed into either a kinematic or dynamic lowlevel controller to step the robot in simulation. In the real-world, we directly send the velocity commands from the high-level policy to the robot, and uses the low-level controller from Boston Dynamics for movement.</p>
<p>Figure 6 :
6Success rate of PointNav policies with A1 trained and evaluated in Habitat with kinematic or dynamic control. Left: Kinematic policies outperform the dynamic trained policies (+12% SR), even when evaluated using dynamic control. Right: Using kinematic control, we can train our robot for 10× more steps of experience than with dynamic control under identical compute budgets, despite training for 2.3× less wall-clock time.</p>
<p>Figure 8 :
8Commanded vs. resultant velocities during real-world trajectory rollouts for dynamically and kinematically trained policies.</p>
<p>Figure 2 :
2We collect displacement errors on Spot in forward and lateral directions. The standard deviation for the displacement errors between the forward and lateral directions are large and asymmetric, demonstrating that the robot is not perfectly omnidirectional.</p>
<p>Table 1 :
1Robot specific parameters used for training and evaluation. The maximum number of steps and velocity limits for each robots are set in proportion to the robot's leg length.Parameter 
A1 
Aliengo 
Spot </p>
<p>1 
Success radius (m) 
0.24 
0.32 
0.425 
2 
Maximum number of steps 
326 
268 
150 
3 
Linear velocity limits (m/s) 
± 
0.23 
± 
0.28 
± 
0.50 
4 
Angular velocity limits (rad/s) 
± 
0.14 
± 
0.17 
± 
0.3 
5 
Leg length (m) 
0.2 
0.25 
0.44 </p>
<p>Table 2 :
2We fit a bivariate Gaussian to the actuation noise collected on a real Spot robot. During kinematic training, we sample from the noise models and inject the realistic actuation noise to the robot's desired final state.
Evaluation using Raibert[22] can be found in the appendix.
Actual velocity is measured using the Boston Dynamics SDK.
For all robots and training sim/controller except A1, iGibson-Kinematic 6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand.
AcknowledgmentsThe Georgia Tech effort was supported in part by NSF, ONR YIP, ARO PECASE. JT was supported by an Apple Scholars in AI/ML PhD Fellowship. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.License for dataset used Gibson Database of Spaces. License at http://svl.stanford.edu/ gibson2/assets/GDS_agreement.pdfAppendix
DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, International Conference on Learning Representations (ICLR. 2020E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra. DD- PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Habitat: A Platform for Embodied AI Research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, International Conference on Computer Vision (ICCV). M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A Platform for Embodied AI Research. In Interna- tional Conference on Computer Vision (ICCV), 2019.</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, A Chang, Z Kira, V Koltun, J Malik, M Savva, D Batra, Advances in Neural Information Processing Systems (NeurIPS). 2021A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre, M. Mukadam, D. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. Chang, Z. Kira, V. Koltun, J. Malik, M. Savva, and D. Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</p>
<p>igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Martín-Martín, L Fan, G Wang, C Pérez-D&apos;arpino, S Buch, S Srivastava, L P Tchapmi, M E Tchapmi, K Vainio, J Wong, L Fei-Fei, S Savarese, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEpage acceptedB. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, C. Pérez-D'Arpino, S. Buch, S. Srivastava, L. P. Tchapmi, M. E. Tchapmi, K. Vainio, J. Wong, L. Fei-Fei, and S. Savarese. igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), page accepted. IEEE, 2021.</p>
<p>. Isaac Nvidia, Sim, Nvidia. Isaac Sim. https://developer.nvidia.com/isaac-sim, 2020.</p>
<p>Robothor: An open simulation-to-real embodied AI platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionM. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador, D. Schwenk, E. VanderBilt, M. Wallingford, et al. Robothor: An open simulation-to-real embodied AI plat- form. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, pages 3164-3174, 2020.</p>
<p>C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, J Freitas, J Kubilius, A Bhandwaldar, N Haber, M Sano, arXiv:2007.04954A platform for interactive multi-modal physical simulation. arXiv preprintC. Gan, J. Schwartz, S. Alter, M. Schrimpf, J. Traer, J. De Freitas, J. Kubilius, A. Bhandwaldar, N. Haber, M. Sano, et al. ThreeDWorld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, IEEE Robotics and Automation Letters. 52S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark &amp; learning environment. IEEE Robotics and Automation Letters, 5(2):3019-3026, 2020.</p>
<p>SAPIEN: A simulated part-based interactive environment. F Xiang, Y Qin, K Mo, Y Xia, H Zhu, F Liu, M Liu, H Jiang, Y Yuan, H Wang, L Yi, A X Chang, L J Guibas, H Su, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su. SAPIEN: A simulated part-based interactive environ- ment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEE. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026- 5033. IEEE, 2012.</p>
<p>C D Freeman, E Frey, A Raichuk, S Girgin, I Mordatch, O Bachem, arXiv:2106.13281Brax-a differentiable physics engine for large scale rigid body simulation. arXiv preprintC. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax-a differen- tiable physics engine for large scale rigid body simulation. arXiv preprint arXiv:2106.13281, 2021.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.</p>
<p>Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J Turner, E Undersander, W Galuba, A Westbury, A X Chang, arXiv:2109.08238arXiv preprintS. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Un- dersander, W. Galuba, A. Westbury, A. X. Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238, 2021.</p>
<p>A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprintA. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.</p>
<p>A X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012An information-rich 3D model repository. arXiv preprintA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. ShapeNet: An information-rich 3D model repository. arXiv preprint arXiv:1512.03012, 2015.</p>
<p>Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. J Truong, S Chernova, D Batra, IEEE Robotics and Automation Letters (RA-L). 62J. Truong, S. Chernova, and D. Batra. Bi-directional domain adaptation for sim2real transfer of embodied navigation agents. IEEE Robotics and Automation Letters (RA-L), 6(2):2634-2641, 2021.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEEY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA), pages 8973-8979. IEEE, 2019.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE international conference on robotics and automation (ICRA), 2018.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017.</p>
<p>Sim2real predictivity: Does evaluation in simulation predict real-world performance?. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, IEEE Robotics and Automation Letters. 2020RA-LA. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee, M. Savva, S. Chernova, and D. Batra. Sim2real predictivity: Does evaluation in simulation predict real-world performance? IEEE Robotics and Automation Letters (RA-L), 2020.</p>
<p>Success weighted by completion time: A dynamics-aware evaluation criteria for embodied navigation. N Yokoyama, S Ha, D Batra, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021N. Yokoyama, S. Ha, and D. Batra. Success weighted by completion time: A dynamics-aware evaluation criteria for embodied navigation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.</p>
<p>Learning navigation skills for legged robots with learned robot embeddings. J Truong, D Yarats, T Li, F Meier, S Chernova, D Batra, A Rai, International Conference on Intelligent Robots and Systems (IROS). 2020J. Truong, D. Yarats, T. Li, F. Meier, S. Chernova, D. Batra, and A. Rai. Learning naviga- tion skills for legged robots with learned robot embeddings. In International Conference on Intelligent Robots and Systems (IROS), 2020.</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, 10.1126/scirobotics.abc5986Science Robotics. 5475986J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning quadrupedal locomotion over challenging terrain. Science Robotics, 5(47):eabc5986, 2020. doi: 10.1126/scirobotics.abc5986. URL https://www.science.org/doi/abs/10.1126/ scirobotics.abc5986.</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, 10.1126/scirobotics.abk2822Science Robotics. 762T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning robust per- ceptive locomotion for quadrupedal robots in the wild. Science Robotics, 7(62):eabk2822, 2022. doi:10.1126/scirobotics.abk2822. URL https://www.science.org/doi/abs/10. 1126/scirobotics.abk2822.</p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, RSSZ Fu, RSSD Pathak, RSSJ Malik, RSSRobotics: Science and Systems. 2021A. Kumar, Z. Fu, D. Pathak, and J. Malik. Rma: Rapid motor adaptation for legged robots. Robotics: Science and Systems (RSS), 2021.</p>
<p>C R Garrett, R Chitnis, R Holladay, B Kim, T Silver, L P Kaelbling, T Lozano-Pérez, arXiv:2010.01083Integrated task and motion planning. arXiv preprintC. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-Pérez. Integrated task and motion planning. arXiv preprint arXiv:2010.01083, 2020.</p>
<p>P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.06757On Evaluation of Embodied Navigation Agents. arXiv preprintP. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Ma- lik, R. Mottaghi, M. Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprint arXiv:1807.06757, 2018.</p>
<p>Habitat-web: Learning embodied objectsearch strategies from human demonstrations at scale. R Ramrakhya, E Undersander, D Batra, A Das, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionR. Ramrakhya, E. Undersander, D. Batra, and A. Das. Habitat-web: Learning embodied object- search strategies from human demonstrations at scale. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 5173-5183, 2022.</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, arXiv:1903.01959arXiv preprintT. Chen, S. Gupta, and A. Gupta. Learning exploration policies for navigation. arXiv preprint arXiv:1903.01959, 2019.</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, arXiv:2004.05155arXiv preprintD. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov. Learning to explore using active neural slam. arXiv preprint arXiv:2004.05155, 2020.</p>
<p>Combining optimal control and learning for visual navigation in novel environments. S Bansal, V Tolani, S Gupta, J Malik, C Tomlin, Conference on Robot Learning. PMLRS. Bansal, V. Tolani, S. Gupta, J. Malik, and C. Tomlin. Combining optimal control and learning for visual navigation in novel environments. In Conference on Robot Learning, pages 420-429. PMLR, 2020.</p>
<p>Real-time loop closure in 2d lidar slam. W Hess, D Kohler, H Rapp, D Andor, ICRA. W. Hess, D. Kohler, H. Rapp, and D. Andor. Real-time loop closure in 2d lidar slam. In ICRA, 2016.</p>
<p>Multi-agent manipulation via locomotion using hierarchical sim2real. O Nachum, M Ahn, H Ponte, S Gu, V Kumar, arXiv:1908.05224arXiv preprintO. Nachum, M. Ahn, H. Ponte, S. Gu, and V. Kumar. Multi-agent manipulation via locomotion using hierarchical sim2real. arXiv preprint arXiv:1908.05224, 2019.</p>
<p>Learning fast adaptation with meta strategy optimization. W Yu, J Tan, Y Bai, E Coumans, S Ha, arXiv:1909.12995arXiv preprintW. Yu, J. Tan, Y. Bai, E. Coumans, and S. Ha. Learning fast adaptation with meta strategy optimization. arXiv preprint arXiv:1909.12995, 2019.</p>
<p>Learning generalizable locomotion skills with hierarchical reinforcement learning. T Li, N Lambert, R Calandra, F Meier, A Rai, arXiv:1909.12324arXiv preprintT. Li, N. Lambert, R. Calandra, F. Meier, and A. Rai. Learning generalizable locomotion skills with hierarchical reinforcement learning. arXiv preprint arXiv:1909.12324, 2019.</p>
<p>Planning in learned latent action spaces for generalizable legged locomotion. T Li, R Calandra, D Pathak, Y Tian, F Meier, A Rai, IEEE Robotics and Automation Letters. 62T. Li, R. Calandra, D. Pathak, Y. Tian, F. Meier, and A. Rai. Planning in learned latent action spaces for generalizable legged locomotion. IEEE Robotics and Automation Letters, 6(2): 2682-2689, 2021.</p>
<p>J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.10332Simto-real: Learning agile locomotion for quadruped robots. arXiv preprintJ. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim- to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, RSSE Coumans, RSST Zhang, RSST.-W Lee, RSSJ Tan, RSSS Levine, RSSRobotics: Science and Systems. 2020X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine. Learning agile robotic locomotion skills by imitating animals. Robotics: Science and Systems (RSS), 2020.</p>
<p>Bayesian optimization using domain knowledge on the atrias biped. A Rai, R Antonova, S Song, W Martin, H Geyer, C Atkeson, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEA. Rai, R. Antonova, S. Song, W. Martin, H. Geyer, and C. Atkeson. Bayesian optimiza- tion using domain knowledge on the atrias biped. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1771-1778. IEEE, 2018.</p>
<p>Coupling vision and proprioception for navigation of legged robots. Z Fu, A Kumar, A Agarwal, H Qi, J Malik, D Pathak, CVPRZ. Fu, A. Kumar, A. Agarwal, H. Qi, J. Malik, and D. Pathak. Coupling vision and proprio- ception for navigation of legged robots. CVPR, 2022.</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Conference on Robot Learning. PMLRN. Rudin, D. Hoeller, P. Reist, and M. Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In Conference on Robot Learning, pages 91-100. PMLR, 2022.</p>
<p>Learning a state representation and navigation in cluttered and dynamic environments. D Hoeller, L Wellhausen, F Farshidian, M Hutter, IEEE Robotics and Automation Letters. 63D. Hoeller, L. Wellhausen, F. Farshidian, and M. Hutter. Learning a state representation and navigation in cluttered and dynamic environments. IEEE Robotics and Automation Letters, 6 (3):5081-5088, 2021.</p>
<p>C R Garrett, R Chitnis, R Holladay, B Kim, T Silver, L P Kaelbling, T Lozano-Pérez, Integrated task and motion planning. Annual review of control, robotics, and autonomous systems. 4C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-Pérez. Integrated task and motion planning. Annual review of control, robotics, and autonomous systems, 4:265-293, 2021.</p>
<p>Hierarchical task and motion planning in the now. L P Kaelbling, T Lozano-Pérez, 2011 IEEE International Conference on Robotics and Automation. IEEEL. P. Kaelbling and T. Lozano-Pérez. Hierarchical task and motion planning in the now. In 2011 IEEE International Conference on Robotics and Automation, pages 1470-1477. IEEE, 2011.</p>
<p>Efficient and interpretable robot manipulation with graph neural networks. Y Lin, A S Wang, E Undersander, A Rai, IEEE Robotics and Automation Letters. Y. Lin, A. S. Wang, E. Undersander, and A. Rai. Efficient and interpretable robot manipulation with graph neural networks. IEEE Robotics and Automation Letters, 2022.</p>
<p>Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot. S Kuindersma, R Deits, M Fallon, A Valenzuela, H Dai, F Permenter, T Koolen, P Marion, R Tedrake, Autonomous robots. 403S. Kuindersma, R. Deits, M. Fallon, A. Valenzuela, H. Dai, F. Permenter, T. Koolen, P. Marion, and R. Tedrake. Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot. Autonomous robots, 40(3):429-455, 2016.</p>
<p>Using deep reinforcement learning to learn highlevel policies on the atrias biped. T Li, H Geyer, C G Atkeson, A Rai, ICRA. IEEET. Li, H. Geyer, C. G. Atkeson, and A. Rai. Using deep reinforcement learning to learn high- level policies on the atrias biped. In ICRA, pages 263-269. IEEE, 2019.</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, J Lee, Conference on Robot Learning (CoRL). 2020A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, and J. Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.</p>
<p>SORNet: Spatial object-centric representations for sequential manipulation. W Yuan, C Paxton, K Desingh, D Fox, 5th Annual Conference on Robot Learning. W. Yuan, C. Paxton, K. Desingh, and D. Fox. SORNet: Spatial object-centric representations for sequential manipulation. In 5th Annual Conference on Robot Learning, 2021. URL https: //openreview.net/forum?id=mOLu2rODIJF.</p>
<p>Unitree robotics. Unitree robotics. https://www.unitree.com/.</p>
<p>. Boston dynamics. Boston dynamics. https://www.bostondynamics.com/spot.</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, CVPR. F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese. Gibson env: Real-world percep- tion for embodied agents. In CVPR, 2018.</p>
<p>Fast and efficient locomotion via learned gait transitions. Y Yang, T Zhang, E Coumans, J Tan, B Boots, Conference on Robot Learning. PMLRY. Yang, T. Zhang, E. Coumans, J. Tan, and B. Boots. Fast and efficient locomotion via learned gait transitions. In Conference on Robot Learning, pages 773-783. PMLR, 2022.</p>
<p>Model-based motion imitation for agile, diverse and generalizable quadupedal locomotion. T Li, J Won, S Ha, A Rai, arXiv:2109.13362arXiv preprintT. Li, J. Won, S. Ha, and A. Rai. Model-based motion imitation for agile, diverse and general- izable quadupedal locomotion. arXiv preprint arXiv:2109.13362, 2021.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, arXiv:1901.08652arXiv preprintReferencesJ. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learn- ing agile and dynamic motor skills for legged robots. arXiv preprint arXiv:1901.08652, 2019. References</p>
<p>Learning navigation skills for legged robots with learned robot embeddings. J Truong, D Yarats, T Li, F Meier, S Chernova, D Batra, A Rai, International Conference on Intelligent Robots and Systems (IROS). 2020J. Truong, D. Yarats, T. Li, F. Meier, S. Chernova, D. Batra, and A. Rai. Learning naviga- tion skills for legged robots with learned robot embeddings. In International Conference on Intelligent Robots and Systems (IROS), 2020.</p>
<p>A Murali, T Chen, K V Alwala, D Gandhi, L Pinto, S Gupta, A Gupta, arXiv:1906.08236Pyrobot: An open-source robotics framework for research and benchmarking. arXiv preprintA. Murali, T. Chen, K. V. Alwala, D. Gandhi, L. Pinto, S. Gupta, and A. Gupta. Py- robot: An open-source robotics framework for research and benchmarking. arXiv preprint arXiv:1906.08236, 2019.</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, RSSE Coumans, RSST Zhang, RSST.-W Lee, RSSJ Tan, RSSS Levine, RSSRobotics: Science and Systems. 2020X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine. Learning agile robotic locomotion skills by imitating animals. Robotics: Science and Systems (RSS), 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-427 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-427</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-427</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-253629098</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.12195v1.pdf" target="_blank">Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture</a></p>
                <p><strong>Paper Abstract:</strong> While deep neural networks have led to major advances in image recognition, language translation, data mining, and game playing, there are well-known limits to the paradigm such as lack of explainability, difficulty of incorporating prior knowledge, and modularity. Neuro symbolic hybrid systems have recently emerged as a straightforward way to extend deep neural networks by incorporating ideas from symbolic reasoning such as computational logic. In this paper, we propose a list desirable criteria for neuro symbolic systems and examine how some of the existing approaches address these criteria. We then propose an extension to generalized annotated logic that allows for the creation of an equivalent neural architecture comprising an alternate neuro symbolic hybrid. However, unlike previous approaches that rely on continuous optimization for the training process, our framework is designed as a binarized neural network that uses discrete optimization. We provide proofs of correctness and discuss several of the challenges that must be overcome to realize this framework in an implemented system.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e427.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e427.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LGAP-BNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parametrized Lower-semilattice Generalized Annotated Program with Equivalent Binarized Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic hybrid that extends Generalized Annotated Programs (GAPs) by using a lower semi-lattice for literal annotations, parametrized GAP rules whose annotation functions act as neural activations, and an equivalent binarized neural architecture (BNN/RNN) that implements the GAP fixpoint operator via unrolled recurrent cells.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Parametrized LGAP with Equivalent Binarized Neural Architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Declarative GAP rules are extended to parametrized GAP rules where each rule head's lower-bound annotation is computed by an annotation/activation function f_{a}^{(i),θ} that takes as input the lower-bound annotations of body literals. These functions are implemented as binarized-neural activations (weights and activations in {−1,1}) so that trainable binary parameters θ_{a}^{(i),j} can toggle body literals on/off. The fixed-point operator T_Π of GAPs is implemented as an RNN unrolled for K steps (K set to the theoretical bound height(T)*|L|), where each recurrent cell computes candidate head annotations (via the annotation functions) and a supremum (max-pooling) over rules for each literal; the final state corresponds to lfp(T_Π). An explicit 'incon' atom and associated rules detect inconsistencies during fixpoint computation. The system is intended for learnable rule induction, symbolic inference, and provable consistency checks using discrete optimization (binarized neural training with pseudo-gradients).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Generalized Annotated Logic Programs (GAPs) extended: interpretations map ground literals (not just atoms) to annotations drawn from a finite lower semi-lattice of intervals (e.g., [l,u] subsets of [0,1]); parametrized GAP rules whose heads annotate lower bounds; fixpoint semantics T_Π and proofs for lfp convergence and inconsistency detection.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Binarized Neural Networks (BNNs) and an RNN unrolling of the GAP fixpoint operator: annotation functions implemented using binarized weights/activations (values in {−1,1}), Sign/ReLU-based pseudo-gradient training (Larq-style frameworks), discrete optimization via gradient-descent with pseudo-gradients, and max-pooling for supremum aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight syntactic-semantic mapping: each parametrized GAP annotation function is directly implemented as a binarized NN activation (one-to-one mapping between rule structure and neural submodules); the logical fixpoint operator T_Π is implemented by unrolling the recurrent cell K times (RNN) where each cell computes rule activations and pools (supremum) to produce the next interpretation vector. Integration supports forward use of the fixpoint operator (logic-oriented) or its neural equivalent for training. Consistency is integrated via an 'incon' atom whose truth is produced by logical rules and can be used as a training-time constraint/loss signal.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines symbolic explainability (1-to-1 mapping from logical rules to neural structure) with learnable discrete rule structure: learned binary θ parameters allow pruning/selection of body literals, enabling induction of classical (non-fuzzy) rules; provable consistency detection via fixpoint semantics and explicit inconsistency atom; avoids combinatorial rule-template generation by turning off atoms rather than entire rules; discrete/binarized training mitigates vanishing-gradient issues using pseudo-gradients; a unification of formal logic guarantees (lfp, convergence bounds) with neural training enables a hybrid capable of both symbolic inference and data-driven rule induction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>No empirical benchmark evaluated in this paper; proposed for symbolic inference, rule induction/learning, and consistency checking in neuro-symbolic reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>The paper claims theoretical support for learning classical rules and scalable antecedent construction (avoiding template blow-up) which should aid generalization; no empirical evaluation of out-of-distribution or compositional generalization is provided. Grounding/first-order scaling and the need for domain knowledge (knowledge-graph constraints) are identified as potential limits to generalization in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability: 1-1 mapping between symbolic syntax and neural modules (similar to LNN) means learned binary parameters directly indicate which body literals are used in a rule; the fixpoint iteration and the 'incon' atom provide localized, symbolic diagnostics for inconsistencies and provenance for derived annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No empirical validation provided; challenges identified include the grounding problem for first-order logic (scalability when grounding), managing inconsistency during training (ensuring training does not produce inconsistent programs), tuning best practices for gradient descent with binarized weights and pseudo-gradients, integration with perceptual (lower-level) neural networks, and implementation/engineering hurdles. Theoretical guarantees require program consistency; use of a lower semi-lattice removes some trivial consistency guarantees present in UGAPs.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>A formal mapping of GAP semantics (extended to lower semi-lattice annotations and interpretations over literals) to an equivalent RNN-based neural architecture implementing T_Π; proofs are provided for monotonicity and lfp(T_Π) under consistency (Theorems IV.2–IV.4), a convergence bound (lfp reached in at most height(T)*|L| iterations), and inconsistency detection; training relies on binarized neural network discrete optimization with pseudo-gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e427.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e427.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Tensor Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework where symbols have vector embeddings and logical sentences are interpreted with fuzzy truth-values, enabling incorporation of prior knowledge during neural training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic tensor networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Tensor Networks (LTN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Represent symbols and predicates as continuous vectors and interpret logical formulas under fuzzy semantics; training assigns weights/truth-values to known logical sentences via gradient-based optimization over embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Fuzzy logical formulas / first-order logic sentences provided a priori; logical constraints are integrated as soft constraints in the loss.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Continuous neural embeddings and differentiable loss optimization (gradient descent) over vector representations.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Soft integration: logical sentences are encoded as differentiable constraints over neural embeddings and incorporated into end-to-end gradient-based training; no hard logic-to-network structural mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables symbolic inference and incorporation of prior knowledge at scale; smooth truth-values allow graded reasoning and compatibility with continuous optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General symbolic inference and knowledge integration tasks (no specific benchmark reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Scalable to large problems via embeddings; does not learn logical rule structure and treats consistency as a component of the loss, so formal guarantees about generalization in the presence of logical constraints are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Limited explainability due to embeddings and fuzzy-valued outputs; this paper states LTNs do not provide an explanation of how a symbolic result was obtained.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Cannot learn classical rule structure; consistency is not guaranteed (handled as loss), and explanations for derivations are lacking due to embedding-based representations.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Fuzzy-logic over vector embeddings, integrated into differentiable learning; no guarantee of hard logical consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e427.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e427.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic architecture that maps logical syntax one-to-one to neural structures, enabling more explainable reasoning than embedding-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logical neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logical Neural Networks (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A neural architecture whose structure mirrors logical formulas (operators and connectives mapped to network modules), so that neural activations correspond directly to logical expressions and facilitate explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Classical/fuzzy logical syntax mapped directly to neural modules; logical formulas provided as part of the structure.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Differentiable neural modules implementing logical connectives and combining into networks that reflect logical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Structural mapping (1-to-1) from logical syntax to neural network architecture; integration is achieved by constructing neural circuits that correspond to logical formulas and training via continuous optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved explainability due to direct mapping; supports symbolic inference and incorporation of prior knowledge while retaining neural training capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Symbolic reasoning/inference tasks (no specific empirical benchmarks provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Provides an explainable mapping that could aid interpretability-driven generalization; does not provide hard consistency guarantees (consistency handled as loss) and cannot learn rule structure without prior template specification.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability because of the 1-1 relationship between logical syntax and neural structure; results are symbolic and amenable to analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No hard guarantees of consistency; cannot learn rules' structure from scratch (requires a-priori rule templates); consistency treated as loss component rather than enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Syntactic mapping of logic to neural modules; differentiable training with soft consistency constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e427.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e427.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>dILP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Inductive Logic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that learns logical rules using differentiable techniques (gradient descent) by generating many rule templates and assigning weights, effectively learning weighted/fuzzy rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Differentiable inductive logic programming for structured examples</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Differentiable Inductive Logic Programming (dILP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates large sets of rule templates and uses continuous parameter optimization to assign weights/truth-values to templates, learning rules from data under a fuzzy/weighted semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Inductive logic programming-style rule templates (logical clauses) enumerated a priori; symbolic rule templates form the search space.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Differentiable optimization (gradient descent) over weights assigned to templates, producing fuzzy rule weights.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>End-to-end differentiable training where rule-template activations/weights are continuous and optimized; requires template enumeration prior to learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Capable of learning rule structure (in a weighted/fuzzy sense) from data and producing symbolic explanations mapping to rule templates, but learns fuzzy rather than classical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Rule induction tasks; structured examples (no empirical numbers provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Learns structure but scalability suffers due to combinatorial template generation; no strong formal consistency guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides symbolic rule templates as explanations, but learned rules are weighted/fuzzy which complicates classical explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Poor scalability from the need to generate many rule templates; learns only weighted/fuzzy rules rather than classical binary rules; consistency not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Differentiable relaxation of ILP with template enumeration and continuous optimization of template weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e427.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e427.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeurASP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Answer Set Programming (NeurASP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid that places a logical (ASP) layer on top of neural perception modules to enforce logical constraints and strong consistency properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neurasp: Embracing neural networks into answer set programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Answer Set Programming (NeurASP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines neural networks for perception with an Answer Set Programming (ASP) symbolic layer that enforces logical constraints and consistency; typically the symbolic solver is used to post-process or constrain neural outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Answer Set Programming (ASP) logical constraints and rules providing hard consistency conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception modules (e.g., CNNs) that produce candidates or distributions that are then constrained by ASP.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular pipeline: neural outputs are combined with ASP constraints (often as a separate nondifferentiable or weakly integrated layer) to enforce consistency; the primary role is constraint enforcement rather than learning new symbolic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Provides strong notions of logical consistency by leveraging ASP semantics; allows enforcement of hard constraints absent in many other neuro-symbolic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Tasks requiring hard logical constraints e.g., structured perception with logical consistency (no specific benchmarks reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Strong consistency guarantees help in domains with hard logical rules but approach does not scale well and does not natively learn new rules.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability in the symbolic layer (ASP) and clear enforcement of logical constraints; less focused on rule induction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability issues; does not support learning of new rules (primarily enforces given constraints); integration often non-differentiable or expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Modular combination of neural modules with ASP; leverages ASP semantics for consistency enforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e427.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e427.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Soft Logic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework (hinge-loss Markov random fields) that represents logical rules as soft constraints with continuous truth-values and explicitly quantifies and minimizes inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hinge-loss markov random fields and probabilistic soft logic</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic Soft Logic (PSL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Represents logical rules as continuous (soft) constraints in a probabilistic graphical-model-like formulation, enabling quantification of inconsistency and optimization to minimize violation of rules.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logical rules encoded as soft constraints (hinge-loss potentials) over continuous truth-values.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Convex/continuous optimization (MAP inference) to find assignments minimizing hinge-loss; not necessarily neural but used as a soft inference engine.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Soft-constraint optimization where inconsistencies are assigned costs and learning optimizes to reduce them; often used as post-processing or combined with learning systems.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Explicit measurement and minimization of inconsistency; flexible handling of uncertain or noisy rules/data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Probabilistic reasoning, statistical relational learning tasks (no explicit benchmarks in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Handles noisy data and partial inconsistency; generalization properties depend on formulation and available supervision; not directly compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides interpretable soft-rule violations and a measure of inconsistency; less direct mapping to neural modules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not inherently neural; inconsistency is tolerated and minimized rather than forbidden; integration with differentiable neural training requires additional engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Hinge-loss MRF formalism; continuous relaxation of logical constraints into an optimization objective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e427.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e427.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepOntoNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Ontological Networks / Ontology Reasoning with Deep Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Embedding-based approaches that use neural networks to perform ontology reasoning; similar to LTN but relying more on embeddings, with associated explainability challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ontology reasoning with deep neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Ontological Networks (Ontology reasoning with deep neural networks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Neural approaches that perform ontology-based reasoning by learning embeddings and neural inference mechanisms over ontology elements; aim to combine ontological knowledge with deep learning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Ontologies and logical ontological axioms represented via embedding-friendly encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Deep neural networks that operate on learned embeddings to perform reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Embedding-based integration: ontological constraints and relations encoded into vector representations and learned via neural training; prior knowledge can be incorporated but mapping is indirect.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Scalability and the ability to combine large knowledge sources with neural reasoning; however, emergent explainability is reduced due to embedding representations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Ontology reasoning and related knowledge inference tasks (no numerical benchmarks in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Scalable but limited interpretability; embedding-based approaches can generalize on distributional patterns but lack symbolic guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Lower interpretability due to embeddings; harder to explain derivations symbolically.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Explainability is more challenging compared to structured neuro-symbolic mappings; consistency guarantees are weak or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Embedding-based representations for ontological axioms combined with neural reasoning modules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic tensor networks <em>(Rating: 2)</em></li>
                <li>Logical neural networks <em>(Rating: 2)</em></li>
                <li>Differentiable inductive logic programming for structured examples <em>(Rating: 2)</em></li>
                <li>Neurasp: Embracing neural networks into answer set programming <em>(Rating: 2)</em></li>
                <li>Ontology reasoning with deep neural networks <em>(Rating: 1)</em></li>
                <li>Hinge-loss markov random fields and probabilistic soft logic <em>(Rating: 2)</em></li>
                <li>Binarized neural networks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-427",
    "paper_id": "paper-253629098",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "LGAP-BNN",
            "name_full": "Parametrized Lower-semilattice Generalized Annotated Program with Equivalent Binarized Neural Network",
            "brief_description": "A neuro-symbolic hybrid that extends Generalized Annotated Programs (GAPs) by using a lower semi-lattice for literal annotations, parametrized GAP rules whose annotation functions act as neural activations, and an equivalent binarized neural architecture (BNN/RNN) that implements the GAP fixpoint operator via unrolled recurrent cells.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Parametrized LGAP with Equivalent Binarized Neural Architecture",
            "system_description": "Declarative GAP rules are extended to parametrized GAP rules where each rule head's lower-bound annotation is computed by an annotation/activation function f_{a}^{(i),θ} that takes as input the lower-bound annotations of body literals. These functions are implemented as binarized-neural activations (weights and activations in {−1,1}) so that trainable binary parameters θ_{a}^{(i),j} can toggle body literals on/off. The fixed-point operator T_Π of GAPs is implemented as an RNN unrolled for K steps (K set to the theoretical bound height(T)*|L|), where each recurrent cell computes candidate head annotations (via the annotation functions) and a supremum (max-pooling) over rules for each literal; the final state corresponds to lfp(T_Π). An explicit 'incon' atom and associated rules detect inconsistencies during fixpoint computation. The system is intended for learnable rule induction, symbolic inference, and provable consistency checks using discrete optimization (binarized neural training with pseudo-gradients).",
            "declarative_component": "Generalized Annotated Logic Programs (GAPs) extended: interpretations map ground literals (not just atoms) to annotations drawn from a finite lower semi-lattice of intervals (e.g., [l,u] subsets of [0,1]); parametrized GAP rules whose heads annotate lower bounds; fixpoint semantics T_Π and proofs for lfp convergence and inconsistency detection.",
            "imperative_component": "Binarized Neural Networks (BNNs) and an RNN unrolling of the GAP fixpoint operator: annotation functions implemented using binarized weights/activations (values in {−1,1}), Sign/ReLU-based pseudo-gradient training (Larq-style frameworks), discrete optimization via gradient-descent with pseudo-gradients, and max-pooling for supremum aggregation.",
            "integration_method": "Tight syntactic-semantic mapping: each parametrized GAP annotation function is directly implemented as a binarized NN activation (one-to-one mapping between rule structure and neural submodules); the logical fixpoint operator T_Π is implemented by unrolling the recurrent cell K times (RNN) where each cell computes rule activations and pools (supremum) to produce the next interpretation vector. Integration supports forward use of the fixpoint operator (logic-oriented) or its neural equivalent for training. Consistency is integrated via an 'incon' atom whose truth is produced by logical rules and can be used as a training-time constraint/loss signal.",
            "emergent_properties": "Combines symbolic explainability (1-to-1 mapping from logical rules to neural structure) with learnable discrete rule structure: learned binary θ parameters allow pruning/selection of body literals, enabling induction of classical (non-fuzzy) rules; provable consistency detection via fixpoint semantics and explicit inconsistency atom; avoids combinatorial rule-template generation by turning off atoms rather than entire rules; discrete/binarized training mitigates vanishing-gradient issues using pseudo-gradients; a unification of formal logic guarantees (lfp, convergence bounds) with neural training enables a hybrid capable of both symbolic inference and data-driven rule induction.",
            "task_or_benchmark": "No empirical benchmark evaluated in this paper; proposed for symbolic inference, rule induction/learning, and consistency checking in neuro-symbolic reasoning tasks.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "The paper claims theoretical support for learning classical rules and scalable antecedent construction (avoiding template blow-up) which should aid generalization; no empirical evaluation of out-of-distribution or compositional generalization is provided. Grounding/first-order scaling and the need for domain knowledge (knowledge-graph constraints) are identified as potential limits to generalization in practice.",
            "interpretability_properties": "High interpretability: 1-1 mapping between symbolic syntax and neural modules (similar to LNN) means learned binary parameters directly indicate which body literals are used in a rule; the fixpoint iteration and the 'incon' atom provide localized, symbolic diagnostics for inconsistencies and provenance for derived annotations.",
            "limitations_or_failures": "No empirical validation provided; challenges identified include the grounding problem for first-order logic (scalability when grounding), managing inconsistency during training (ensuring training does not produce inconsistent programs), tuning best practices for gradient descent with binarized weights and pseudo-gradients, integration with perceptual (lower-level) neural networks, and implementation/engineering hurdles. Theoretical guarantees require program consistency; use of a lower semi-lattice removes some trivial consistency guarantees present in UGAPs.",
            "theoretical_framework": "A formal mapping of GAP semantics (extended to lower semi-lattice annotations and interpretations over literals) to an equivalent RNN-based neural architecture implementing T_Π; proofs are provided for monotonicity and lfp(T_Π) under consistency (Theorems IV.2–IV.4), a convergence bound (lfp reached in at most height(T)*|L| iterations), and inconsistency detection; training relies on binarized neural network discrete optimization with pseudo-gradients.",
            "uuid": "e427.0",
            "source_info": {
                "paper_title": "Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LTN",
            "name_full": "Logic Tensor Networks",
            "brief_description": "A neuro-symbolic framework where symbols have vector embeddings and logical sentences are interpreted with fuzzy truth-values, enabling incorporation of prior knowledge during neural training.",
            "citation_title": "Logic tensor networks",
            "mention_or_use": "mention",
            "system_name": "Logic Tensor Networks (LTN)",
            "system_description": "Represent symbols and predicates as continuous vectors and interpret logical formulas under fuzzy semantics; training assigns weights/truth-values to known logical sentences via gradient-based optimization over embeddings.",
            "declarative_component": "Fuzzy logical formulas / first-order logic sentences provided a priori; logical constraints are integrated as soft constraints in the loss.",
            "imperative_component": "Continuous neural embeddings and differentiable loss optimization (gradient descent) over vector representations.",
            "integration_method": "Soft integration: logical sentences are encoded as differentiable constraints over neural embeddings and incorporated into end-to-end gradient-based training; no hard logic-to-network structural mapping.",
            "emergent_properties": "Enables symbolic inference and incorporation of prior knowledge at scale; smooth truth-values allow graded reasoning and compatibility with continuous optimization.",
            "task_or_benchmark": "General symbolic inference and knowledge integration tasks (no specific benchmark reported in this paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Scalable to large problems via embeddings; does not learn logical rule structure and treats consistency as a component of the loss, so formal guarantees about generalization in the presence of logical constraints are not provided.",
            "interpretability_properties": "Limited explainability due to embeddings and fuzzy-valued outputs; this paper states LTNs do not provide an explanation of how a symbolic result was obtained.",
            "limitations_or_failures": "Cannot learn classical rule structure; consistency is not guaranteed (handled as loss), and explanations for derivations are lacking due to embedding-based representations.",
            "theoretical_framework": "Fuzzy-logic over vector embeddings, integrated into differentiable learning; no guarantee of hard logical consistency.",
            "uuid": "e427.1",
            "source_info": {
                "paper_title": "Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LNN",
            "name_full": "Logical Neural Networks",
            "brief_description": "A neuro-symbolic architecture that maps logical syntax one-to-one to neural structures, enabling more explainable reasoning than embedding-based approaches.",
            "citation_title": "Logical neural networks",
            "mention_or_use": "mention",
            "system_name": "Logical Neural Networks (LNN)",
            "system_description": "A neural architecture whose structure mirrors logical formulas (operators and connectives mapped to network modules), so that neural activations correspond directly to logical expressions and facilitate explainability.",
            "declarative_component": "Classical/fuzzy logical syntax mapped directly to neural modules; logical formulas provided as part of the structure.",
            "imperative_component": "Differentiable neural modules implementing logical connectives and combining into networks that reflect logical structure.",
            "integration_method": "Structural mapping (1-to-1) from logical syntax to neural network architecture; integration is achieved by constructing neural circuits that correspond to logical formulas and training via continuous optimization.",
            "emergent_properties": "Improved explainability due to direct mapping; supports symbolic inference and incorporation of prior knowledge while retaining neural training capabilities.",
            "task_or_benchmark": "Symbolic reasoning/inference tasks (no specific empirical benchmarks provided in this paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Provides an explainable mapping that could aid interpretability-driven generalization; does not provide hard consistency guarantees (consistency handled as loss) and cannot learn rule structure without prior template specification.",
            "interpretability_properties": "High interpretability because of the 1-1 relationship between logical syntax and neural structure; results are symbolic and amenable to analysis.",
            "limitations_or_failures": "No hard guarantees of consistency; cannot learn rules' structure from scratch (requires a-priori rule templates); consistency treated as loss component rather than enforced.",
            "theoretical_framework": "Syntactic mapping of logic to neural modules; differentiable training with soft consistency constraints.",
            "uuid": "e427.2",
            "source_info": {
                "paper_title": "Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "dILP",
            "name_full": "Differentiable Inductive Logic Programming",
            "brief_description": "An approach that learns logical rules using differentiable techniques (gradient descent) by generating many rule templates and assigning weights, effectively learning weighted/fuzzy rules.",
            "citation_title": "Differentiable inductive logic programming for structured examples",
            "mention_or_use": "mention",
            "system_name": "Differentiable Inductive Logic Programming (dILP)",
            "system_description": "Generates large sets of rule templates and uses continuous parameter optimization to assign weights/truth-values to templates, learning rules from data under a fuzzy/weighted semantics.",
            "declarative_component": "Inductive logic programming-style rule templates (logical clauses) enumerated a priori; symbolic rule templates form the search space.",
            "imperative_component": "Differentiable optimization (gradient descent) over weights assigned to templates, producing fuzzy rule weights.",
            "integration_method": "End-to-end differentiable training where rule-template activations/weights are continuous and optimized; requires template enumeration prior to learning.",
            "emergent_properties": "Capable of learning rule structure (in a weighted/fuzzy sense) from data and producing symbolic explanations mapping to rule templates, but learns fuzzy rather than classical rules.",
            "task_or_benchmark": "Rule induction tasks; structured examples (no empirical numbers provided in this paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Learns structure but scalability suffers due to combinatorial template generation; no strong formal consistency guarantees.",
            "interpretability_properties": "Provides symbolic rule templates as explanations, but learned rules are weighted/fuzzy which complicates classical explanation.",
            "limitations_or_failures": "Poor scalability from the need to generate many rule templates; learns only weighted/fuzzy rules rather than classical binary rules; consistency not guaranteed.",
            "theoretical_framework": "Differentiable relaxation of ILP with template enumeration and continuous optimization of template weights.",
            "uuid": "e427.3",
            "source_info": {
                "paper_title": "Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "NeurASP",
            "name_full": "Neural Answer Set Programming (NeurASP)",
            "brief_description": "A hybrid that places a logical (ASP) layer on top of neural perception modules to enforce logical constraints and strong consistency properties.",
            "citation_title": "Neurasp: Embracing neural networks into answer set programming",
            "mention_or_use": "mention",
            "system_name": "Neural Answer Set Programming (NeurASP)",
            "system_description": "Combines neural networks for perception with an Answer Set Programming (ASP) symbolic layer that enforces logical constraints and consistency; typically the symbolic solver is used to post-process or constrain neural outputs.",
            "declarative_component": "Answer Set Programming (ASP) logical constraints and rules providing hard consistency conditions.",
            "imperative_component": "Neural perception modules (e.g., CNNs) that produce candidates or distributions that are then constrained by ASP.",
            "integration_method": "Modular pipeline: neural outputs are combined with ASP constraints (often as a separate nondifferentiable or weakly integrated layer) to enforce consistency; the primary role is constraint enforcement rather than learning new symbolic rules.",
            "emergent_properties": "Provides strong notions of logical consistency by leveraging ASP semantics; allows enforcement of hard constraints absent in many other neuro-symbolic systems.",
            "task_or_benchmark": "Tasks requiring hard logical constraints e.g., structured perception with logical consistency (no specific benchmarks reported in this paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Strong consistency guarantees help in domains with hard logical rules but approach does not scale well and does not natively learn new rules.",
            "interpretability_properties": "High interpretability in the symbolic layer (ASP) and clear enforcement of logical constraints; less focused on rule induction.",
            "limitations_or_failures": "Scalability issues; does not support learning of new rules (primarily enforces given constraints); integration often non-differentiable or expensive.",
            "theoretical_framework": "Modular combination of neural modules with ASP; leverages ASP semantics for consistency enforcement.",
            "uuid": "e427.4",
            "source_info": {
                "paper_title": "Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "PSL",
            "name_full": "Probabilistic Soft Logic",
            "brief_description": "A framework (hinge-loss Markov random fields) that represents logical rules as soft constraints with continuous truth-values and explicitly quantifies and minimizes inconsistency.",
            "citation_title": "Hinge-loss markov random fields and probabilistic soft logic",
            "mention_or_use": "mention",
            "system_name": "Probabilistic Soft Logic (PSL)",
            "system_description": "Represents logical rules as continuous (soft) constraints in a probabilistic graphical-model-like formulation, enabling quantification of inconsistency and optimization to minimize violation of rules.",
            "declarative_component": "Logical rules encoded as soft constraints (hinge-loss potentials) over continuous truth-values.",
            "imperative_component": "Convex/continuous optimization (MAP inference) to find assignments minimizing hinge-loss; not necessarily neural but used as a soft inference engine.",
            "integration_method": "Soft-constraint optimization where inconsistencies are assigned costs and learning optimizes to reduce them; often used as post-processing or combined with learning systems.",
            "emergent_properties": "Explicit measurement and minimization of inconsistency; flexible handling of uncertain or noisy rules/data.",
            "task_or_benchmark": "Probabilistic reasoning, statistical relational learning tasks (no explicit benchmarks in this paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Handles noisy data and partial inconsistency; generalization properties depend on formulation and available supervision; not directly compared here.",
            "interpretability_properties": "Provides interpretable soft-rule violations and a measure of inconsistency; less direct mapping to neural modules.",
            "limitations_or_failures": "Not inherently neural; inconsistency is tolerated and minimized rather than forbidden; integration with differentiable neural training requires additional engineering.",
            "theoretical_framework": "Hinge-loss MRF formalism; continuous relaxation of logical constraints into an optimization objective.",
            "uuid": "e427.5",
            "source_info": {
                "paper_title": "Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "DeepOntoNet",
            "name_full": "Deep Ontological Networks / Ontology Reasoning with Deep Neural Networks",
            "brief_description": "Embedding-based approaches that use neural networks to perform ontology reasoning; similar to LTN but relying more on embeddings, with associated explainability challenges.",
            "citation_title": "Ontology reasoning with deep neural networks",
            "mention_or_use": "mention",
            "system_name": "Deep Ontological Networks (Ontology reasoning with deep neural networks)",
            "system_description": "Neural approaches that perform ontology-based reasoning by learning embeddings and neural inference mechanisms over ontology elements; aim to combine ontological knowledge with deep learning.",
            "declarative_component": "Ontologies and logical ontological axioms represented via embedding-friendly encodings.",
            "imperative_component": "Deep neural networks that operate on learned embeddings to perform reasoning tasks.",
            "integration_method": "Embedding-based integration: ontological constraints and relations encoded into vector representations and learned via neural training; prior knowledge can be incorporated but mapping is indirect.",
            "emergent_properties": "Scalability and the ability to combine large knowledge sources with neural reasoning; however, emergent explainability is reduced due to embedding representations.",
            "task_or_benchmark": "Ontology reasoning and related knowledge inference tasks (no numerical benchmarks in this paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Scalable but limited interpretability; embedding-based approaches can generalize on distributional patterns but lack symbolic guarantees.",
            "interpretability_properties": "Lower interpretability due to embeddings; harder to explain derivations symbolically.",
            "limitations_or_failures": "Explainability is more challenging compared to structured neuro-symbolic mappings; consistency guarantees are weak or absent.",
            "theoretical_framework": "Embedding-based representations for ontological axioms combined with neural reasoning modules.",
            "uuid": "e427.6",
            "source_info": {
                "paper_title": "Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic tensor networks",
            "rating": 2,
            "sanitized_title": "logic_tensor_networks"
        },
        {
            "paper_title": "Logical neural networks",
            "rating": 2,
            "sanitized_title": "logical_neural_networks"
        },
        {
            "paper_title": "Differentiable inductive logic programming for structured examples",
            "rating": 2,
            "sanitized_title": "differentiable_inductive_logic_programming_for_structured_examples"
        },
        {
            "paper_title": "Neurasp: Embracing neural networks into answer set programming",
            "rating": 2,
            "sanitized_title": "neurasp_embracing_neural_networks_into_answer_set_programming"
        },
        {
            "paper_title": "Ontology reasoning with deep neural networks",
            "rating": 1,
            "sanitized_title": "ontology_reasoning_with_deep_neural_networks"
        },
        {
            "paper_title": "Hinge-loss markov random fields and probabilistic soft logic",
            "rating": 2,
            "sanitized_title": "hingeloss_markov_random_fields_and_probabilistic_soft_logic"
        },
        {
            "paper_title": "Binarized neural networks",
            "rating": 2,
            "sanitized_title": "binarized_neural_networks"
        }
    ],
    "cost": 0.014761499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture
23 Feb 2023</p>
<p>Paulo Shakarian 
Arizona State University Tempe
AZUSA</p>
<p>Gerardo I Simari 
Department of Computer Science and Engineering
Universidad Nacional del Sur (UNS) Institute for Comp. Sci. and Eng. (UNS-CONICET
Bahia BlancaArgentina</p>
<p>Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture
23 Feb 2023A833EA2FAC28D5500F226F58351F1E6BarXiv:2302.12195v1[cs.AI]Logic programmingNeural networksMachine learning
While deep neural networks have led to major advances in image recognition, language translation, data mining, and game playing, there are well-known limits to the paradigm such as lack of explainability, difficulty of incorporating prior knowledge, and modularity.Neuro symbolic hybrid systems have recently emerged as a straightforward way to extend deep neural networks by incorporating ideas from symbolic reasoning such as computational logic.In this paper, we propose a list desirable criteria for neuro symbolic systems and examine how some of the existing approaches address these criteria.We then propose an extension to generalized annotated logic that allows for the creation of an equivalent neural architecture comprising an alternate neuro symbolic hybrid.However, unlike previous approaches that rely on continuous optimization for the training process, our framework is designed as a binarized neural network that uses discrete optimization.We provide proofs of correctness and discuss several of the challenges that must be overcome to realize this framework in an implemented system.</p>
<p>I. INTRODUCTION</p>
<p>While deep neural networks have led to major advances in image recognition, language translation, data mining, and game playing, there are well-known limits to the paradigm such as lack of explainability, difficulty of incorporating prior knowledge, and modularity [1].Neuro symbolic hybrid systems have recently emerged as a straightforward way to extend deep neural networks by incorporating ideas from symbolic reasoning such as computational logic [2]- [8].In this paper, we first propose a list of desirable criteria for a neuro symbolic system and examine how some of the existing approaches each address these criteria.We then propose an extension to generalized annotated logic [9] that allows for the creation of an equivalent neural architecture comprising an alternate neuro symbolic hybrid.However, unlike previous approaches that rely on continuous optimization for the training process, our framework is designed as a binarized neural network [10] that uses discrete optimization.We provide proofs of correctness and discuss several of the challenges that must be overcome to realize this framework in an implemented system.</p>
<p>The rest of this paper is organized as follows.In Section II we propose a list of desirable criteria and briefly discuss which of the current approaches meet the criteria.In Section III we review generalized annotated logic along with our extensions, mainly related to the use of interpretations for literals (as opposed to atoms) and the use of a lower semi-lattice (instead of an upper semi-lattice).This is followed by a section containing proofs related to our extensions to annotated logic (Section IV).This allows us to then introduce our parametrized rule structure (Section V) and associated neural architecture (Section VI).We discuss how this framework can identify inconsistencies in Section VII, briefly discuss in Section VIII how our framework meets the criteria, and conclude with a discussion of challenges (Section IX).</p>
<p>II. CRITERIA TO CONSIDER IN NEURO SYMBOLIC REASONING AND RELATED WORK</p>
<p>Inspired on the critique of deep learning in [1], we have derived the following criteria for a neuro symbolic system.</p>
<p>1) Support for symbolic inference for arbitrary queries 2) Symbolic explanation of results amenable to both further analysis and human interpretation 3) Ability to integrate prior knowledge and/or constraints 4) Strong assurances of consistency 5) Ability to learn rule structure, including classical (i.e., non-fuzzy) rules 6) Scalability The framework of logic tensor networks (LTN) [7], [8] provides a neural symbolic approach where symbols have an associated vector representation that in turn allows, via the training process, for logical sentences (known a-priori) to be assigned weights where the intuition is related to a level of truth.This follows from fuzzy logic, similar to prior approaches combining fuzzy logic and machine learning [11].The framework excels in its ability to conduct symbolic inference on queries and provide symbolic results in a scalable manner as well as the ability to incorporate prior knowledge.However, the framework does not allow for the learning of logical rules and it treats consistency as a component of the loss function, hence not guaranteeing consistency.We note that the lack of consistency assurances means that we do not have guarantees that prior knowledge is indeed integrated.Further, the system does not provide an explanation of how it obtained a symbolic result.In many ways, the capabilities of LTN's are similar to deep ontological networks [12] (although the two differ significantly in how they handle training data).However, the explainability problem is more pronounced in ontological neural networks due to its use of embeddings.In summary, LTNs and deep ontological networks meet criteria 1, 3, and 6-they allow for symbolic reasoning and incorporation of prior knowledge in a scalable manner but do not make consistency guarantees, learn rule structure, nor provide explanatory results.</p>
<p>The framework of logical neural networks (LNN) [6] overcomes some of the explainability issues of LTN and deep ontological networks.However, like LTN, it still does not have hard consistency guarantees, as consistency is handled as a component of the loss function.Further, it is not capable of learning rules without a-prior knowledge of their structure.In short, LNNs meet criteria 1, 2, 3, and 6-they provide much of the same capabilities of LTN and deep ontological networks, but also provide an explainable result due to the 1-1 relationship between logical syntax and neural structure.However, they cannot guarantee consistency or learn rule structure.</p>
<p>Differentiable inductive logic programming (ILP) [4], [5] is designed to learn logical rules using gradient descent.This approach suffers from scalability issues as it involves generating a large number of "rule templates" and assigning them weights using gradient descent.Further, this approach only learns weighted (fuzzy) rules as opposed to classical ones and also does not provide strong consistency guarantees.In short, differentiable ILP excels at learning structure, though not in the classical sense (criterion 5) and provides a symbolic framework and explainability (criteria 1 and 2).The work as presented does not incorporate prior knowledge or ensure consistency (criteria 3 and 4), but future work could possilby provide those extensions.However, a key drawback of this approach is scalability (criterion 6) due to the requirement to generate large amounts of rule templates.</p>
<p>A key insight into this work is the combined use of annotated logic and binarized neural networks to avoid rule template generation.The idea is that instead of generating a combinatorial number of rule templates for a given consequent, we can generate a constant number of rules and use binarized neural learning to prune the elements of the body.Annotated logic, which has a semantics that associated atoms with elements of a lattice structure allows us to have a "don't care" element for a given atom in the body.With binarized weight learning, we can then use discrete weights to assign the "don't care" element to weights associated with the atoms we desire to prune from the body.We avoid the vanishing gradient problem that is associated with discrete weight learning by using mature software developed for binarized neural networks [10] that is the result of a line of research on the use of pseudo-gradients for binary weights during gradient descent [13]- [15].</p>
<p>Neural answer set programming (NeurASP) [2] does provide strong notions of consistency, but to date has only been used to provide a logical layer on top of a neural network to enforce logical constraints, essentially introducing a capability absent in the other approaches discussed previously.However, this approach does not scale and does not support the learning of new rules.In short, NeurASP satisfies criteria 1-4, but does not meet criteria 5-6 (rule structure learning and scalability).</p>
<p>In what follows, we review generalized annotated programs [9] with some key extensions (the use of a lower semilattice for annotations and a semantic structure that maps literals instead of atoms to annotations).We posit that this provides the key to a framework that ensures that all criteria can be adhered to simultaneously.</p>
<p>III. GENERALIZED ANNOTATED PROGRAMS</p>
<p>We now recapitulate the definition of Generalized Annotated Logic programs (from now on referred to as "GAPs", for short) from [9], but with limited syntax and certain modifications.There are two reasons why it is advantageous to use GAPs:</p>
<p>1) It is a framework that easily allows atomic propositions to be associated with values from a lattice structure, which generalizes other real-valued logic paradigms previously introduced in the neuro symbolic reasoning literature [5]- [8].2) We can set the annotations based on a lattice structure that can support describing an atomic proposition not only as true, but as false or uncertain (i.e., no knowledge).</p>
<p>Extension: Use of Lower Semi Lattice.In [9], the authors assumed the existence of an upper semi-lattice, T (not necessarily complete) with ordering .However, in this work, we propose to instead utilize a lower semi-lattice structure.Therefore, we have a single element ⊥ and multiple top elements 0 , . . .i . . .max .The notation height(T ) is the maximum number of elements in the lattice in a path between ⊥ and a top element (including ⊥ and the top element) 1 .Note that we provide rigorous proofs of certain results from [9] on a lower semi-lattice in Section IV.</p>
<p>The employment of a lower semi-lattice structure enables two desirable characteristics.First, we desire to annotate atoms with intervals of reals in [0, 1] as done in previous work [6], [16], [17].Second, it allows for reasoning about such intervals whereby the amount of uncertainty (i.e., for interval [l, u] the quantity u − l) decreases monotonically as an operator proceeds up the lattice structure.Therefore, we define the bottom element ⊥ = [0, 1] and a set of top elements
{[x, x] | [x, x] ⊆ [0, 1]} (see note 2 ). Specifically, we set 0 = [0, 0] and max = [1, 1].
An example of such a semilattice structure is shown in Figure 1.</p>
<p>Syntax of GAPs (Review of prior work).</p>
<p>We assume the existence of a set AVar of variable symbols ranging over T and a set F of function symbols, each of which has an associated arity.We start by defining annotations.</p>
<p>Definition III.1 (Annotation).(i) Any member of T ∪ AVar is an annotation.(ii) If f is an n-ary function symbol over T and t 1 , . . ., t n are annotations, then f (t 1 , . . ., t n ) is an annotation.</p>
<p>One specific function we define is "¬", which is used in semantics of [9].For a given
[l, u], ¬([l, u]) = [1 − u, 1 − l].
Note that we also use the symbol ¬ in our first-order language (following the formalism of [9]).</p>
<p>We define a separate logical language whose constants are members of set C and whose predicate symbols are specified by set P. We also assume the existence of a set V of variable symbols ranging over the constants, that no function symbols are present, and terms and atoms are defined in the usual way (cf.[18]).We shall assume that C, P, V are discrete and finite.In general, we shall use capital letters for variable symbols and lowercase letters for constants.Similar to previous work [3], [5], we assume that all elements of P have an arity of either 1 or 2-we use P una to denote the set of unary predicates and P rel the set of binary predicates.We shall also denote a subsets of P to include "target predicates" written P tgt that can consist of either binary or unary predicates (P tgt rel , P tgt una ) provided that they are not reserved words.We shall use the symbol L to denote the set of all ground literals and A for the set of all ground atoms.We now define the syntactical structure of GAPs that will be used in this work.</p>
<p>Definition III.2 (Annotated atoms, negations, literals).The core syntactic structures are defined as follows:</p>
<p>• Annotated atom.If a is an atom and µ is an annotation, then a : µ is an annotated atom.• Annotated Negation.If a is an atom and µ is an annotation, then ¬a : µ is an annotated negation.• Annotated Literal.Collectively, atoms and negations are referred to as annotated literals.</p>
<p>Definition III.3 (GAP Rule).If 0 : µ 0 , 1 : µ 1 , . . ., m : µ m are annotated literals (such that for all i, j ∈ 1, m, i ≡ j ), then
r ≡ 0 : µ 0 ← 1 : µ 1 ∧ . . . ∧ m : µ m
is called a GAP rule.We will use the notation head(r) and body(r) to denote 0 and { 1 , . . ., m } respectively.When m = 0 (body(r) = ∅), the above GAP-rule is called a fact.A GAP-rule is ground iff there are no occurrences of variables from either AVar or V in it.For ground rule r and ground literal , bodyAnno( , r) = µ such that : µ appears in the body of r.A generalized annotated program Π is a finite set of GAP rules.</p>
<p>Semantics of GAPs (Extended in this work).The formal semantics of GAPs are defined as follows.Note that we extend the notion of an interpretation to allow for a mapping of literals to annotations (as opposed to atoms).However, we add a requirement on the annotation between each atom and negation that ensures equivalence to the semantic structure of [9].The intuition behind this extension is that we can more easily detect inconsistencies using the fixpoint operator, as we can just compare the annotations of each pair of literals (the atom and its negation).</p>
<p>Definition III.4 (Interpretation).An interpretation I is any mapping from the set of all grounds literals to T such that for literals a, ¬a, we have I(a) = ¬(I(¬a)).The set I of all interpretations can be partially ordered via the ordering: I 1 I 2 iff for all ground literals a, I 1 ( ) I 2 ( ).I forms a complete lattice under the ordering.</p>
<p>Satisfaction Relationship (Review of prior work).We now present the concept of satisfaction:</p>
<p>Definition III.5 (Satisfaction).An interpretation I satisfies a ground literal : µ, denoted I |= : µ, iff µ I( ).I satisfies the ground GAP-rule
0 : µ 0 ← 1 : µ 1 ∧ . . . ∧ m : µ m (denoted I |= 0 : µ 0 ← 1 : µ 1 ∧ . . . ∧ m : µ m ) iff either
1) I satisfies 0 : µ 0 or 2) There exists an 1 ≤ i ≤ m such that I does not satisfy i : µ i .I satisfies a non-ground literal or rule iff I satisfies all ground instances of it.</p>
<p>We say that an interpretation I is a model of program Π if it satisfies all rules in Π.Likewise, program Π is consistent if there exists some I that is a model of Π.We say Π entails : µ, denoted Π |= ent : µ, iff for every interpretation I s.t.I |= Π, we have that I |= : µ.As shown by [9], we can associate a fixpoint operator with any GAP Π that maps interpretations to interpretations.</p>
<p>Definition III.6.Suppose Π is any GAP and I an interpretation.The mapping T Π that maps interpretations to interpretations is defined as
T Π (I)( 0 ) = sup(annoSet Π,I ( 0 )), where annoSet Π,I ( 0 ) = {I( 0 )} ∪ {µ 0 | 0 : µ 0 ← 1 : µ 1 ∧ . . . ∧ m : µ m is
a ground instance of a rule in Π, and for all 1 ≤ i ≤ m, we have
I |= i : µ i }.
The key result of [9] (Theorem 2 in that paper, discussed in the next section) tells us that lfp(T Π ) precisely captures the ground atomic logical consequences of Π.We show this is also true (under the condition that Π is consistent) even if the annotations are based on a lower lattice (see Theorem IV.2).In [9], the authors also define the iteration of T Π as follows:</p>
<p>• T Π ↑ 0 is the interpretation that assigns ⊥ to all ground literals.
• T Π ↑ (i + 1) = T Π (T Π ↑ i).
For each ground ∈ L, the set Π( ) is the subset of ground rules (to include facts) in Π where is in the head.We will use the notation m to denote the number of rules in Π( ).For a given ground rule, we will use the symbol r ,i to denote that it is the i-th rule with atom in the head.</p>
<p>IV. NEW THEORETICAL RESULTS FOR ANNOTATED LOGIC ON A LOWER SEMI-LATTICE</p>
<p>The use of the lower semi-lattice structure for annotations in GAPs leads us to revisit some of the results of [9] that depend upon an upper semi-lattice structure.In this section, we shall review results from [9] that apply to an upper semi-lattice (and shall refer to results specific to the upper semi-lattice as "UGAPs") and also prove analogous results for GAPs where the annotation uses a lower semi-lattice ("LGAPs").Further, note that in UGAPs we shall refer to interpretations defined only as mappings of atoms to annotations.This makes it relatively straightforward to have consistency; consider the following proposition.</p>
<p>Proposition IV.1.Any UGAP Π consisting of UGAP-rules where the atom bodies and heads are atoms is consistent.</p>
<p>Proof.Consider the interpretation I such that ∀a ∈ A, I(a) =</p>
<p>. By the semantics of GAPs, this will satisfy all rules.</p>
<p>To provide a specific example of an LGAP that is not consistent, consider the following two rules (here a is a ground literal):
a : [0, 0] ← a : [1, 1] ←
So, we consider the key result of [9] below.</p>
<p>Theorem IV.1.(Theorem 2 in [9]) For UGAP Π, T Π is monotonic and has a least fixpoint lfp(T Π ).Moreover, for this case Π entails a : µ iff µ lfp(T Π )(a).</p>
<p>However, under the condition that Π is consistent, we can show a similar result.</p>
<p>Theorem IV.2.If LGAP Π is consistent, then:</p>
<p>1) T Π is monotonic, 2) T Π has a least fixpoint lfp(T Π ), and 3) Π entails a : µ iff µ ≤ lfp(T Π )(a).</p>
<p>Proof.(1 and 2) By creating an interpretation that maps literals to annotations, instead of atoms, the monotonicity of T Π is trivial even in the case where Π in inconsistent and has a least fixed point.</p>
<p>(3) Suppose, BWOC that Π entails a : µ and µ &gt; lfp(T Π )(a).However, this would imply there is a series of logical constructs that allow us to derive a : µ, and this would trivially be reflected in the iterative applications of the T operator.Going the other way, BWOC if µ ≤ lfp(T Π )(a) but Π does not entail a : µ would imply that there is no application of the constructs in Π that lead to the deductive conclusion of a : µ; however this is again contradicted by the fact that T directly leverages the elements of Π.</p>
<p>We can also show that for both LGAPs and UGAPs, we can bound the number of applications of T until convergence.
Theorem IV.3. If (LGAP or UGAP) Π is consistent, then lfp(T Π ) ≡ T Π ↑ x where x = height(T ) * |L|.
Proof.We know, by the definition of T, for any i ≤ x, that for all a ∈ A, T Π ↑ i(a) T Π ↑ x(a).Hence, we just need to consider the case where i &gt; x and lfp(T Π ) ≡ T Π ↑ i and lfp(T Π ) ≡ T Π ↑ x.However, at each iteration the annotation of at least one atom must change.The bound on the number of changes in annotation is height(T ) (as the annotations must stay the same or increase monotonically, as Π is consistent by the statement).Hence, we have a contradiction.</p>
<p>We can also leverage the T operator to identify inconsistencies.</p>
<p>Theorem IV.4.(LGAP or UGAP) Π is inconsistent if and only if for value i, and ground atom a, if there exist µ, µ ∈ annoSet Π,TΠ↑i (a) where µ µ and µ µ.</p>
<p>Proof.Claim 1: If there exist i, a such that the statement holds, then Π is inconsistent.Suppose, BWOC, that such an i, a pair exist and Π is consistent.We know, by the definition of T that T(T ↑ i) must be an interpretation.However, as there is no element above both µ, µ , T that T(T ↑ i) cannot be a valid interpretation.Claim 2: If Π is inconsistent, then there exist i, a such that the statement holds.Suppose, BWOC, Π is inconsistent and there does not exist such an i, a pair.Then, this implies that for all a ∈ A that there exists some i where T(T ↑ i ) = T ↑ i which means for any i &gt; i we have T ↑ i = T ↑ i .Therefore, by the definition of satisfaction, T ↑ i must satisfy Π, which is a contradiction.</p>
<p>V. PARAMETRIZED GAP RULES</p>
<p>In this section, we present a rule structure and annotation function (that later is also used as an activation function in the associated neural architecture) that can allow for us to learn GAP rules from data by using a process like gradient descent.One of the key intuitions behind the use of a lower semi-lattice for annotations is that we can easily separate the concepts of negation and "no information."Hence, in the classical case, the lattice structure would consist of three elements: a lower "uncertain" element and two upper elements, one for false and one for true (see Figure 2).Through the use of a semantic structure that assigns literals to annotations, we can restrict our rules to activation functions that only modify the lower bound (i.e., if we wish to adjust the upper bound on a literal, we can instead have a rule that adjusts the lower bound on its negation).Also note the use of the interval of [−1, 1] as opposed to [0, 1]-this is due to the common values used for binarized neural networks [10] that we will use to prune unneeded atoms from the body.</p>
<p>Rule Structure.In this paper, we shall consider the propositional case (this is the case where all predicates are unary and we have a single constant).Hence, there is no grounding (see Section IX for discussions on grounding).So, for a given literal a in the rule head and for a potential set of literals L in the body, we have the following rule r:
r ≡ a : [f (i) a,θ i a (X (i) a ), 1] ← j ∈L j : [x j , x j ].(1)
In rule 1, we have rule r that has literal a in the head that is assigned a lower bound on the lattice element based on function f
(i)
a , which utilizes learned parameters θ i a .Several observations: first, note the i index is used in the case where multiple rules are used with literal a in the head.Next, X (i) a is the vector of the lower bound of the annotations of each literal in the body (x j is the jth component of X (i) a ).Finally, also note that x j is unused by the annotation function for this particular rule (again, if we desire to use the upper bound, we can instead include the negation of literal j in the set L and use its upper bound).</p>
<p>Annotation/Activation Functions.In defining the annotation function f (i) a,θ i a , it must have the capability to use parameters to "turn off" a given literal in the body.We note that realvalued weights in traditional neural networks as well as the prior work described earlier cannot accomplish this task, hence the extensive use of fuzzy logic in other neural symbolic approaches.We seek to employ binarized neural networks [10] in which both weights and activations have values in the set {−1, 1}.The work of [10] has led to successful use of a "gradient descent" style approach to discrete optimization for model training.This avoids the well-known problem of vanishing gradients by substituting the partial derivative for a "pseudo gradient" of the activation function.There are current implementations for binarized neural networks such as Larq3 that have successfully employed this methodology.</p>
<p>For our purposes, the annotation function shall also be used as an activation function in the equivalent neural architecture.First, we present the meaning of the parameters.Let θ i a,j be the jth component of θ i a .If this value is 1, that means that literal j should be considered in the body of the rule-likewise, if it is −1, then it should not.Hence, after training, we should be able to simply erase any j where θ i a,j = −1 to no effect.Second, we must consider the meaning of the activations.For f (i) a,θ i a to return 1, then for every θ i a,j = 1, x j must also equal 1.If there is any j where θ i a,j = 1 and
x j = −1, then f (i) a,θ i a must return −1.
The following relu function accomplishes this requirement (note there are available pseudo gradients for binarized neural networks for relu).</p>
<p>We must therefore consider the meaning of the lower bound of the annotation assigned by function f (i) a,θ i a .In the below function, Sign assigns values greater than 0 to 1, and −1 otherwise (as per [10]).
f (i) a,θ i a (X (i) a ) = Sign(relu(1 + j 0.5(1 + θ i a,j )(x j − 1)))(2)
So, consider that θ i a,j = −1 does not affect the sum with respect to literal j .The same is true for any x j = 1.Therefore, if for all j such that θ i a,j = 1 and x j = 1 then the function returns a 1.Now, if for a single j we have θ i a,j = 1 and x j = −1, then the function returns −1.</p>
<p>We point out that the number of rules used for a given ground atom can be considered as a hyper-parameter.Unlike in [4], [5], where the parameters turn entire rules on or off, in this approach we are turning on or off atoms in the body.This avoids the need to generate large numbers of rule templates.However, it does rely on domain knowledge (e.g., in the form of a knowledge graph) to limit the number of literals considered in a given rule (in other words, we want to keep set L small).</p>
<p>VI. NEURAL ARCHITECTURE FOR LEARNING GAPS</p>
<p>In this section, we look at how a GAP can be embedded in a neural framework for use in training by gradient descent.The concept is similar to that of the differentiable inductive logic programming (ILP) literature [4], [5] in that it involves an unrolling of the fixed point operator.However, we use the rule structure and annotation/activation function of the previous section to avoid generating numerous rule templates.</p>
<p>We shall assume a RNN neural architecture consisting of K recurrent cells.In order to assure correctness, K must be set to the maximum number of applications of the fixpoint operator (see Theorem IV.3).We use A to denote a vector of elements in F of size n-intuitively, we want each position of A to correspond to the annotation of a single literal in L. So, we shall assume a numbering of literals such that literal a j ∈ A would correspond with the j-th position of A. The initial input to the first recurrent cell will be denoted A 0 , where all positions are set to ⊥.For recurrent cell t, A t−1 is the input and A t is the output.</p>
<p>For each rule r aj ,i there is an associated vector of body annotation lower bounds X (i) aj of size n aj ,i ≤ n; this is of a smaller size than A t as a body of a given rule may not include all atoms.However, for a given annotation function f
(i)
a in the head of a rule, when we say it evaluates an n-sized vector A t , it is actually evaluating the positions in A t corresponding to the atoms for which there are positions in
X (i) aj .
In each recurrent cell t, vector Ã(j)</p>
<p>t is created and is of length m aj (the number of rules, including facts, with atom a j in the head).Position i of Ã(j) t is equal to f
(i) aj (θ (i)
aj , A t−1 ).For facts, this position will simply be the annotation in the head of the fact.</p>
<p>Finally, the result of recurrent cell t is vector A t in which the j-th position of this vector corresponds with the supremum of the corresponding Ã(j) t .Note that as this is a supremum of annotations in which only the lower bound changes, this is equivalent to a max pooling layer that only applies the pooling function to a subset of neurons.The intuition is that each cell corresponds with one application of the T operator.Figure 3 shows a depiction of this architecture.</p>
<p>VII. IDENTIFYING INCONSISTENCIES</p>
<p>The use of the lower lattice for annotations does not allow for easy guarantees of consistency (i.e., see Proposition IV.1).While it is possible to guarantee consistency by determining rules with restrictive structures (e.g., disallowing negation in learned rules as in [5]), this becomes difficult if rules are created as a result of the training process (e.g., setting parameters that configure the rules through gradient descent).</p>
<p>Other approaches to dealing with consistency in the settings integrating symbolic and machine learning paradigms have included notions of quantifying a level of inconsistency and then attempting to minimize such inconsistency.This is commonly used in the (non-neural) probabilistic soft logic (PSL) [11].This concept has also appeared in neuro symbolic approaches as well [6].</p>
<p>However, an advantage with the approach of this paper is that an inconsistency will be detected by some application of the fixpoint operator (Theorem IV.4).This is why we changed the domain of an interpretation to be literals instead of atoms, as we can more easily identify a case where an atom and its negation are assigned annotations in such a way to cause an inconsistency.We create a special atomic symbol incon for this purpose.The negation of this symbol is never used; however, if this symbol is annotated with [1,1] ("true") then it means we have an inconsistency.In the logic program, we add a single rule of the form 3 below, and for each ground atom a we add an instance of rule 4:
incon : [−1, 1] ← (3) incon : [Sign(x + y), 1] ← a : [x, x ] ∧ ¬a : <a href="4">y, y </a>
Hence, for any application of the fixpoint operator, we know there is an inconsistency if incon is true and we need not carry out further applications of the operator.Our intuition is that this can be used during the training process to either establish a loss function that disallows any inconsistency, or to conduct a check at each iteration of gradient descent.It is noteworthy that the fixpoint operator, which also is likely to bring further efficiencies to the training process, can be used in the forward pass as opposed to the equivalent neural architecture.Further, the fixpoint operator will also provide information on where the inconsistency occurs, which can also be useful in guiding the training process.Note that some domains may accept inconsistency if it is localized and not associated with certain hard constraints (e.g., specified by rules with no parameters, which this framework also supports).</p>
<p>VIII. HOW THE FRAMEWORK MEETS THE CRITERIA</p>
<p>We now briefly review the criteria introduced in Section II and discuss how the proposed approach can meet them:</p>
<p>• As it is an inherently neuro symbolic approach, it meets criterion 1. • Due to the direct mapping to an equivalent neural architecture, which is similar to LNN [6], it meets criterion 2 in that the results are directly explainable, symbolic, and amenable to further analysis.• The framework can both allow parametrized rules (for induction) as well as directly represent logical statements in the equivalent neural architecture, which satisfies criterion 3-incorporation of prior knowledge-(as does [3], [6], [7]).• We can provably show it can guarantee consistency, even when using negation, meeting criterion 4 (as does [2]).• Through the use of parametrized rules, annotated logic, and binarized neural networks, it enables rule learning (criteria 5).</p>
<p>• Unlike [5], it avoids (non-scalable) template generation, instead building up antecedents by using the lattice structure to "turn off" certain atoms (meeting criterion 6).Though, as we have argued, our proposed formalism meets all six criteria, there remain several hurdles to overcome, which we discuss in the following section.</p>
<p>IX. CHALLENGES AND CONCLUSION</p>
<p>While the use of annotated logic in a neural symbolic framework is promising, there are several challenges to be addressed.First, it is expected that any neuro symbolic approach should support first order logic (i.e., the non-ground case).While there is nothing presented here that would not support such a capability, practical considerations around grounding can limit scalability in practice.We are exploring the use of knowledge graphs [19] as an ontology to limit relationships among constants to reduce the grounding problem.A second, but equally important, concern is dealing with inconsistency.It is challenging to specifically avoid inconsistency and ensure that the training process continues.Ultimately, this will require a determination of best practices around gradient descent and a vigilant measure of the boundaries of inconsistency.Third, and in the same vain, we also will need to explore other issues such as establishing best practices for connecting a neural network associated with a logic program with lowerlevel neural networks used for perception (e.g., CNN's).Finally, implementation issues will clearly be very important in successfully developing tools based on this framework.</p>
<p>Fig. 1 .
1
Fig. 1.Example of a lower semi-lattice structure where the elements are intervals in [0, 1].</p>
<p>Fig. 2 .
2
Fig. 2. Lower semi-lattice structure for the classical logic case.</p>
<p>Fig. 3 .
3
Fig. 3. Recurrent cells described in this section.</p>
<p>In general, we shall assume that the lattice consists of finite, discrete elements.
N.B. that when using a semi-lattice of bounds, the notation " " loses its "subset intuition", as [0, 1] [1, 1] in this case, for example.
See https://docs.larq.dev/larq/
ACKNOWLEDGMENT P.S. is supported by internal funding from the ASU Fulton Schools of Engineering.G.S. is supported by Universidad Nacional del Sur (UNS) under grant PGI 24/ZN34 and Agencia Nacional de Promoción Científica y Tecnológica under grant PICT-2018-0475 (PRH-2014-0007).
Deep learning: A critical appraisal. G Marcus, abs/1801.00631CoRR. 2018</p>
<p>Neurasp: Embracing neural networks into answer set programming. Z Yang, A Ishay, J Lee, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. C Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence20main track</p>
<p>Ontology reasoning with deep neural networks (extended abstract). P Hohenecker, T Lukasiewicz, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, ser. IJCAI'20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, ser. IJCAI'202021</p>
<p>Differentiable inductive logic programming for structured examples. H Shindo, M Nishino, A Yamamoto, Thirty-Fifth AAAI Conference on Artificial Intelligence. AAAI Press2021</p>
<p>Learning explanatory rules from noisy data. R Evans, E Grefenstette, J. Artif. Int. Res. 611jan 2018</p>
<p>Logical neural networks. R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, S Ikbal, H Karanam, S Neelam, A Likhyani, S Srivastava, 2020</p>
<p>Logic tensor networks. S Badreddine, A Avila Garcez, L Serafini, M Spranger, Artificial Intelligence. 3031036492022</p>
<p>Analyzing differentiable fuzzy logic operators. E Van Krieken, E Acar, F Van Harmelen, Artificial Intelligence. 3021036022022</p>
<p>Theory of generalized annotated logic programming and its applications. M Kifer, V Subrahmanian, J. Log. Program. 123&amp;41992</p>
<p>Binarized neural networks. I Hubara, M Courbariaux, D Soudry, R El-Yaniv, Y Bengio, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. D D Lee, M Sugiyama, U Luxburg, I Guyon, R Garnett, Barcelona, Spain2016. December 5-10, 2016. 2016</p>
<p>Hinge-loss markov random fields and probabilistic soft logic. S H Bach, M Broecheler, B Huang, L Getoor, Journal of Machine Learning Research (JMLR). 182017</p>
<p>Ontology reasoning with deep neural networks. P Hohenecker, T Lukasiewicz, Journal of Artificial Intelligence Research. 682020</p>
<p>Training binary node feedforward neural networks by back propagation of error. D Toms, Electronics Letters. 261October 1990</p>
<p>A Training Method for Discrete Multilayer Neural Networks. G D Magoulas, M N Vrahatis, T N Grapsa, G S Androulakis, 1997Springer USBoston, MA</p>
<p>Evolutionary training of hardware realizable multilayer perceptrons. V P Plagianakos, G D Magoulas, M N Vrahatis, Neural Comput. Appl. 1512006</p>
<p>Mancalog: a logic for multi-attribute network cascades. P Shakarian, G I Simari, R Schroeder, International conference on Autonomous Agents and Multi-Agent Systems, AAMAS '13. M L Gini, O Shehory, T Ito, C M Jonker, Saint Paul, MN, USAMay 6-10, 2013. 2013</p>
<p>Annotated probabilistic temporal logic. P Shakarian, A Parker, G Simari, V V S Subrahmanian, 10.1145/1877714.1877720ACM Trans. Comput. Logic. 122jan 2011</p>
<p>Foundations of logic programming. J W Lloyd, 1987Springer-Verlag New York, Inc</p>
<p>Reasoning about complex networks: A logic programming approach. P Shakarian, G I Simari, D Callahan, Theory Pract. Log. Program. 134-52013Online-Supplement</p>            </div>
        </div>

    </div>
</body>
</html>
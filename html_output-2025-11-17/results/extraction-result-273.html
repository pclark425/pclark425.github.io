<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-273 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-273</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-273</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-274822370</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.13536v1.pdf" target="_blank">MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules</a></p>
                <p><strong>Paper Abstract:</strong> Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic. Inspired by meta-learning, we propose that models should acquire not only task-specific knowledge but also transferable problem-solving skills. We introduce MetaRuleGPT, a novel Transformer-based architecture that performs precise numerical calculations and complex logical operations by learning and combining different rules. In contrast with traditional training sets, which are heavily composed of massive raw instance data, MetaRuleGPT is pre-trained on much less abstract datasets containing basic, compound, and iterative rules for mathematical reasoning. Extensive experimental results demonstrate MetaRuleGPT can mimic human's rule-following capabilities, break down complexity, and iteratively derive accurate results for complex mathematical problems. These findings prove the potential of rule learning to enhance the numerical reasoning abilities of language models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e273.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e273.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaRuleGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaRuleGPT (Transformer-based rule-learning language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 30M-parameter custom Transformer language model pre-trained on explicit arithmetic and composite rule datasets; it decomposes arithmetic tasks into mapping, alignment, compute, and carry/borrow subrules and uses iterative self-refinement (VeriGate + RefeedFormatter) to produce exact numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaRuleGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Custom Transformer-based language model (byte-based tokenization)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>High-digit addition (10+ digits), high-digit subtraction, single-digit addition/subtraction, multi-step arithmetic (2-10 operational steps), vector cross-product (3D)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Single-digit rules up to high-digit (10+ digits) random numbers; datasets include low-digit and high-digit cases; test set contains 8,000 examples including perfect-decimal addition, reverse-magnitude subtraction, misplaced/interleaved subtraction, and random numbers sampled across orders of magnitude</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Pre-training on a compact rule dataset (~20k records) containing explicit mapping, align, compute, carry and borrow rules; iterative stepwise decoding with VeriGate verification and RefeedFormatter for realignment; byte-based tokenization; rule-invocation (mapping rule, compute rule, align rule, carry/borrow rule) rather than standard chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported 100% accuracy on high-digit random addition tasks and 100% accuracy on 3D vector cross-product dataset; best-performing model on high-digit subtraction among compared models (but not 100% for some high-digit subtraction cases). Training dataset ~20k, evaluation dataset 8k.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Performs arithmetic by explicitly learning and invoking small algorithmic subrules (mapping digits to symbols, aligning digits, single-digit compute subrules, carry/borrow rules) and composing them iteratively; uses VeriGate to verify intermediate outputs and RefeedFormatter to realign/iterate when expectations are unmet; emphasizes rule execution over memorization, enabling exact carry propagation and multi-step derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Achieves strong performance despite small parameter count (30M); authors claim other large models degrade as digit count increases while MetaRuleGPT maintains accuracy (esp. for addition); suggests rule-centric pretraining can compensate for fewer parameters. Paper notes performance of many baseline models degrades with increasing digits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Noted challenges on some high-digit random subtraction tasks (less than perfect accuracy); limited generalization beyond trained rule distributions (cannot automatically handle untrained generalization forms or novel concepts outside meta-learning distribution), and unpredictable results on tasks like function integration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against ChatGPT-3.5, ChatGPT-4.0, QWen, Google PaLM, Llama2 (7B/13B/70B) and Goat on arithmetic and vector cross-product tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Learning and explicitly invoking small arithmetic rules with iterative verification enables near-perfect high-digit addition and exact vector cross-product computation with a compact (30M) Transformer model, outperforming larger off-the-shelf LLMs on these arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e273.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model by OpenAI used as a baseline; reported to perform well on some arithmetic benchmarks but to degrade on high-digit arithmetic and complex vector tasks without tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition, subtraction, and vector cross-product (evaluated as part of benchmark comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit and high-digit tests included; also tested on simplified GSM8K subset</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard invocation as a baseline (no external tools reported in these experiments); comparisons done against MetaRuleGPT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported to maintain capability better than many models as digit count increases; on the paper's simplified GSM8K subset GPT-4 reported 100% accuracy (table in paper). On vector cross-product tasks, ChatGPT achieved below 50% accuracy without external tools (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper suggests GPT-4 (and other large models) often fail because they do not deeply learn underlying computational rules and may lack a unified representation for text and numbers; no detailed mechanistic analysis provided for GPT-4 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Authors report GPT-4 degrades less than many other models as digits increase, but still struggles on some complex vector tasks; no detailed scaling law provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Degrades on high-digit arithmetic and complex operations without tool use; sub-50% accuracy on vector cross-product without external tools (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to MetaRuleGPT and other LLMs on arithmetic and vector tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Even top-of-line LLMs like GPT-4 can fail on some arithmetic and vector-computation tasks without specific rule-based training or tools, performing worse than a small rule-trained model on targeted high-digit arithmetic and cross-product problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e273.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's earlier ChatGPT model used as a baseline; performs well on low-digit arithmetic but deteriorates with larger digit counts compared to rule-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition and subtraction (benchmarked), vector cross-product included in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit up to high-digit tests; evaluated on simplified GSM8K subset</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard querying as baseline; no special prompting described in paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported near 99% on the paper's simplified GSM8K subset (per Table VIII); generally exceeds 70% accuracy on low-digit addition/subtraction but performance declines with digit count.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No specific mechanistic explanation provided; authors attribute errors to lack of deep rule understanding and difficulty learning unified representations for text and numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance declines with increasing number of digits; no parameter-scaling analysis provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Makes errors on high-digit arithmetic and carries; degrades with more digits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MetaRuleGPT, GPT-4, Llama2 series, QWen, PaLM, Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Performs well on lower-digit benchmarks and simplified GSM8K but exhibits degraded performance on high-digit arithmetic compared to a model explicitly trained on arithmetic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e273.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter open-source LLM from the Llama2 family included in benchmarks; reported unable to perform vector cross-product calculations in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition, subtraction, vector cross-product (benchmarked)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit and high-digit tests included</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard usage as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported as incapable of performing vector cross-product calculations (explicitly stated in paper). Achieves >70% on low-digit arithmetic in some experiments but performance drops with digit count.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanistic analysis provided; failure attributed to not learning computational rules deeply.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Smaller Llama2 variants (7B,13B) fail on some tasks that larger or differently trained models can attempt; paper reports degradation with increasing digits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Incapable of vector cross-product tasks; errors on high-digit arithmetic and carry/borrow-handling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MetaRuleGPT and other LLM baselines (GPT-3.5/4, QWen, PaLM, Goat).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Small/medium open models without explicit rule training can fail completely on algorithmic-like vector computations and degrade on high-digit arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e273.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter Llama2 variant used in benchmarks; reported similarly limited arithmetic/vector capabilities as the 7B variant in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition, subtraction, vector cross-product (benchmarked)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit to high-digit tests</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard baseline invocation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported as incapable of performing vector cross-product calculations (paper states 7B and 13B were incapable). Shows >70% accuracy on low-digit arithmetic but degrades with digit count.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No specific internal mechanistic detail provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Larger than 7B but still failed on vector cross-product; performance degrades with number of digits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails on vector cross-product; makes arithmetic errors on high-digit cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared versus MetaRuleGPT, GPT variants, QWen, PaLM, Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Despite larger parameter count than 7B, still lacks capability on some algorithmic arithmetic tasks without targeted rule training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e273.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The largest Llama2 variant tested; could produce outputs for vector cross-product tasks but achieved 0% accuracy on them in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition, subtraction, vector cross-product (benchmarked)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit to high-digit tests</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard baseline invocation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported to be able to attempt vector cross-product but achieved 0% accuracy on the vector cross-product test set; performance on high-digit arithmetic degraded compared to MetaRuleGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanistic account provided; demonstrates that larger parameter count alone did not yield correct cross-product computation without rule training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>In this case, scaling to 70B did not produce correct vector cross-product outputs (0% accuracy), indicating parameter scaling alone insufficient for some algorithmic arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Produces incorrect vector cross-product calculations (0% accuracy) and makes errors on high-digit arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MetaRuleGPT and other LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Large parameter count (70B) does not guarantee correct execution of algorithmic arithmetic tasks without rule-specific training; model produced outputs but none were correct on the cross-product benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e273.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen (72B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alibaba's Qwen model (72B) included in benchmarks; used as a baseline for arithmetic and simplified GSM8K subset evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition and subtraction benchmarks; evaluated on simplified GSM8K subset</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit and some higher-digit tests; simplified GSM8K subset</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard baseline invocation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On the paper's simplified GSM8K table Qwen-72B is reported at 100% on that subset (table VIII). On general arithmetic benchmarks authors report that many models lose accuracy as digit count increases.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No internal mechanistic details provided in paper; errors attributed broadly to lack of rule learning in baseline models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>No detailed scaling curves provided; like other baselines, expected to degrade with digit count unless explicitly trained on rules.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Degrades on high-digit arithmetic and struggles with algorithmic vector computations unless trained with rules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with MetaRuleGPT, GPT variants, PaLM, Llama2, Goat.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Large off-the-shelf models (e.g., Qwen-72B) can perform well on some curated/simplified benchmarks but still generally suffer on algorithmic high-digit arithmetic unless trained with explicit arithmetic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e273.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's large language model included as a baseline in comparisons; showed strong performance on some simplified benchmarks but authors report degraded performance on complex arithmetic/vector tasks for many baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition, subtraction, vector cross-product (benchmarked)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit and high-digit tests; simplified GSM8K subset included</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard baseline invocation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported 100% on the paper's simplified GSM8K subset (table VIII); authors note PaLM (like other baselines) degrades on large-digit arithmetic and complex vector tasks without targeted rule learning or tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanistic detail given in the paper for PaLM; cited general limitation: difficulty learning unified representation for text and numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Not analyzed in detail in this paper; baseline models generally degrade as digit complexity increases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails more on high-digit arithmetic and vector cross-product tasks without specialized training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MetaRuleGPT and other LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Strong on some curated benchmarks but like other large models can fail on algorithmic arithmetic and vector computations unless trained or augmented for rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e273.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e273.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (Mathematical Mastery Model / fine-tuned Llama variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model reported in related work (and included in comparisons) that focuses on arithmetic skill improvement via fine-tuning; used as a baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Arithmetic tasks (addition/subtraction, other math benchmarks); included in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Low-digit and high-digit tests (as part of benchmark suite)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Baseline fine-tuning approaches referenced in related work; in this paper Goat was invoked as an evaluated baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Included in comparison plots/tables; paper's Figure 6 compares addition/subtraction accuracy of GPT-4, Goat and MetaRuleGPT across digits (MetaRuleGPT claimed 100% as digits increase while others decline). Exact numeric results for Goat not fully enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No mechanistic analysis provided here for Goat; referenced as a fine-tuned arithmetic-focused baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Paper's figures imply Goat's accuracy declines with increasing digit count (similar trend to other large models), but explicit scaling numbers not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Degrades on high-digit arithmetic; lacks explicit rule-based iterative verification mechanism described for MetaRuleGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in plots/tables against MetaRuleGPT and GPT-4 for digit-wise arithmetic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Even fine-tuned arithmetic-specialist models (like Goat) showed degraded accuracy as digit count rose compared to a small rule-trained model that performs iterative rule application.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Recursion of thought: A divide-and-conquer approach to multi-context reasoning with language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-273",
    "paper_id": "paper-274822370",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "MetaRuleGPT",
            "name_full": "MetaRuleGPT (Transformer-based rule-learning language model)",
            "brief_description": "A 30M-parameter custom Transformer language model pre-trained on explicit arithmetic and composite rule datasets; it decomposes arithmetic tasks into mapping, alignment, compute, and carry/borrow subrules and uses iterative self-refinement (VeriGate + RefeedFormatter) to produce exact numerical results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MetaRuleGPT",
            "model_size": "30M",
            "model_architecture": "Custom Transformer-based language model (byte-based tokenization)",
            "arithmetic_operation_type": "High-digit addition (10+ digits), high-digit subtraction, single-digit addition/subtraction, multi-step arithmetic (2-10 operational steps), vector cross-product (3D)",
            "number_range_or_complexity": "Single-digit rules up to high-digit (10+ digits) random numbers; datasets include low-digit and high-digit cases; test set contains 8,000 examples including perfect-decimal addition, reverse-magnitude subtraction, misplaced/interleaved subtraction, and random numbers sampled across orders of magnitude",
            "method_or_intervention": "Pre-training on a compact rule dataset (~20k records) containing explicit mapping, align, compute, carry and borrow rules; iterative stepwise decoding with VeriGate verification and RefeedFormatter for realignment; byte-based tokenization; rule-invocation (mapping rule, compute rule, align rule, carry/borrow rule) rather than standard chain-of-thought",
            "performance_result": "Reported 100% accuracy on high-digit random addition tasks and 100% accuracy on 3D vector cross-product dataset; best-performing model on high-digit subtraction among compared models (but not 100% for some high-digit subtraction cases). Training dataset ~20k, evaluation dataset 8k.",
            "mechanistic_insight": "Performs arithmetic by explicitly learning and invoking small algorithmic subrules (mapping digits to symbols, aligning digits, single-digit compute subrules, carry/borrow rules) and composing them iteratively; uses VeriGate to verify intermediate outputs and RefeedFormatter to realign/iterate when expectations are unmet; emphasizes rule execution over memorization, enabling exact carry propagation and multi-step derivations.",
            "performance_scaling": "Achieves strong performance despite small parameter count (30M); authors claim other large models degrade as digit count increases while MetaRuleGPT maintains accuracy (esp. for addition); suggests rule-centric pretraining can compensate for fewer parameters. Paper notes performance of many baseline models degrades with increasing digits.",
            "failure_modes": "Noted challenges on some high-digit random subtraction tasks (less than perfect accuracy); limited generalization beyond trained rule distributions (cannot automatically handle untrained generalization forms or novel concepts outside meta-learning distribution), and unpredictable results on tasks like function integration.",
            "comparison_baseline": "Benchmarked against ChatGPT-3.5, ChatGPT-4.0, QWen, Google PaLM, Llama2 (7B/13B/70B) and Goat on arithmetic and vector cross-product tasks.",
            "key_finding": "Learning and explicitly invoking small arithmetic rules with iterative verification enables near-perfect high-digit addition and exact vector cross-product computation with a compact (30M) Transformer model, outperforming larger off-the-shelf LLMs on these arithmetic tasks.",
            "uuid": "e273.0",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "ChatGPT / GPT-4",
            "brief_description": "A state-of-the-art large language model by OpenAI used as a baseline; reported to perform well on some arithmetic benchmarks but to degrade on high-digit arithmetic and complex vector tasks without tools.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4.0",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Addition, subtraction, and vector cross-product (evaluated as part of benchmark comparisons)",
            "number_range_or_complexity": "Low-digit and high-digit tests included; also tested on simplified GSM8K subset",
            "method_or_intervention": "Standard invocation as a baseline (no external tools reported in these experiments); comparisons done against MetaRuleGPT",
            "performance_result": "Reported to maintain capability better than many models as digit count increases; on the paper's simplified GSM8K subset GPT-4 reported 100% accuracy (table in paper). On vector cross-product tasks, ChatGPT achieved below 50% accuracy without external tools (reported in paper).",
            "mechanistic_insight": "Paper suggests GPT-4 (and other large models) often fail because they do not deeply learn underlying computational rules and may lack a unified representation for text and numbers; no detailed mechanistic analysis provided for GPT-4 specifically.",
            "performance_scaling": "Authors report GPT-4 degrades less than many other models as digits increase, but still struggles on some complex vector tasks; no detailed scaling law provided.",
            "failure_modes": "Degrades on high-digit arithmetic and complex operations without tool use; sub-50% accuracy on vector cross-product without external tools (per paper).",
            "comparison_baseline": "Compared directly to MetaRuleGPT and other LLMs on arithmetic and vector tasks.",
            "key_finding": "Even top-of-line LLMs like GPT-4 can fail on some arithmetic and vector-computation tasks without specific rule-based training or tools, performing worse than a small rule-trained model on targeted high-digit arithmetic and cross-product problems.",
            "uuid": "e273.1",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "ChatGPT / GPT-3.5",
            "brief_description": "OpenAI's earlier ChatGPT model used as a baseline; performs well on low-digit arithmetic but deteriorates with larger digit counts compared to rule-trained model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Addition and subtraction (benchmarked), vector cross-product included in comparisons",
            "number_range_or_complexity": "Low-digit up to high-digit tests; evaluated on simplified GSM8K subset",
            "method_or_intervention": "Standard querying as baseline; no special prompting described in paper",
            "performance_result": "Reported near 99% on the paper's simplified GSM8K subset (per Table VIII); generally exceeds 70% accuracy on low-digit addition/subtraction but performance declines with digit count.",
            "mechanistic_insight": "No specific mechanistic explanation provided; authors attribute errors to lack of deep rule understanding and difficulty learning unified representations for text and numbers.",
            "performance_scaling": "Performance declines with increasing number of digits; no parameter-scaling analysis provided in this paper.",
            "failure_modes": "Makes errors on high-digit arithmetic and carries; degrades with more digits.",
            "comparison_baseline": "Compared to MetaRuleGPT, GPT-4, Llama2 series, QWen, PaLM, Goat.",
            "key_finding": "Performs well on lower-digit benchmarks and simplified GSM8K but exhibits degraded performance on high-digit arithmetic compared to a model explicitly trained on arithmetic rules.",
            "uuid": "e273.2",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama2-7B",
            "name_full": "Llama 2 (7B)",
            "brief_description": "A 7-billion-parameter open-source LLM from the Llama2 family included in benchmarks; reported unable to perform vector cross-product calculations in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "Addition, subtraction, vector cross-product (benchmarked)",
            "number_range_or_complexity": "Low-digit and high-digit tests included",
            "method_or_intervention": "Standard usage as baseline",
            "performance_result": "Reported as incapable of performing vector cross-product calculations (explicitly stated in paper). Achieves &gt;70% on low-digit arithmetic in some experiments but performance drops with digit count.",
            "mechanistic_insight": "No mechanistic analysis provided; failure attributed to not learning computational rules deeply.",
            "performance_scaling": "Smaller Llama2 variants (7B,13B) fail on some tasks that larger or differently trained models can attempt; paper reports degradation with increasing digits.",
            "failure_modes": "Incapable of vector cross-product tasks; errors on high-digit arithmetic and carry/borrow-handling.",
            "comparison_baseline": "Compared to MetaRuleGPT and other LLM baselines (GPT-3.5/4, QWen, PaLM, Goat).",
            "key_finding": "Small/medium open models without explicit rule training can fail completely on algorithmic-like vector computations and degrade on high-digit arithmetic.",
            "uuid": "e273.3",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama2-13B",
            "name_full": "Llama 2 (13B)",
            "brief_description": "A 13-billion-parameter Llama2 variant used in benchmarks; reported similarly limited arithmetic/vector capabilities as the 7B variant in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "Addition, subtraction, vector cross-product (benchmarked)",
            "number_range_or_complexity": "Low-digit to high-digit tests",
            "method_or_intervention": "Standard baseline invocation",
            "performance_result": "Reported as incapable of performing vector cross-product calculations (paper states 7B and 13B were incapable). Shows &gt;70% accuracy on low-digit arithmetic but degrades with digit count.",
            "mechanistic_insight": "No specific internal mechanistic detail provided in paper.",
            "performance_scaling": "Larger than 7B but still failed on vector cross-product; performance degrades with number of digits.",
            "failure_modes": "Fails on vector cross-product; makes arithmetic errors on high-digit cases.",
            "comparison_baseline": "Compared versus MetaRuleGPT, GPT variants, QWen, PaLM, Goat.",
            "key_finding": "Despite larger parameter count than 7B, still lacks capability on some algorithmic arithmetic tasks without targeted rule training.",
            "uuid": "e273.4",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama2-70B",
            "name_full": "Llama 2 (70B)",
            "brief_description": "The largest Llama2 variant tested; could produce outputs for vector cross-product tasks but achieved 0% accuracy on them in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2",
            "model_size": "70B",
            "model_architecture": null,
            "arithmetic_operation_type": "Addition, subtraction, vector cross-product (benchmarked)",
            "number_range_or_complexity": "Low-digit to high-digit tests",
            "method_or_intervention": "Standard baseline invocation",
            "performance_result": "Reported to be able to attempt vector cross-product but achieved 0% accuracy on the vector cross-product test set; performance on high-digit arithmetic degraded compared to MetaRuleGPT.",
            "mechanistic_insight": "No mechanistic account provided; demonstrates that larger parameter count alone did not yield correct cross-product computation without rule training.",
            "performance_scaling": "In this case, scaling to 70B did not produce correct vector cross-product outputs (0% accuracy), indicating parameter scaling alone insufficient for some algorithmic arithmetic tasks.",
            "failure_modes": "Produces incorrect vector cross-product calculations (0% accuracy) and makes errors on high-digit arithmetic.",
            "comparison_baseline": "Compared to MetaRuleGPT and other LLM baselines.",
            "key_finding": "Large parameter count (70B) does not guarantee correct execution of algorithmic arithmetic tasks without rule-specific training; model produced outputs but none were correct on the cross-product benchmark.",
            "uuid": "e273.5",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Qwen-72B",
            "name_full": "Qwen (72B)",
            "brief_description": "Alibaba's Qwen model (72B) included in benchmarks; used as a baseline for arithmetic and simplified GSM8K subset evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen",
            "model_size": "72B",
            "model_architecture": null,
            "arithmetic_operation_type": "Addition and subtraction benchmarks; evaluated on simplified GSM8K subset",
            "number_range_or_complexity": "Low-digit and some higher-digit tests; simplified GSM8K subset",
            "method_or_intervention": "Standard baseline invocation",
            "performance_result": "On the paper's simplified GSM8K table Qwen-72B is reported at 100% on that subset (table VIII). On general arithmetic benchmarks authors report that many models lose accuracy as digit count increases.",
            "mechanistic_insight": "No internal mechanistic details provided in paper; errors attributed broadly to lack of rule learning in baseline models.",
            "performance_scaling": "No detailed scaling curves provided; like other baselines, expected to degrade with digit count unless explicitly trained on rules.",
            "failure_modes": "Degrades on high-digit arithmetic and struggles with algorithmic vector computations unless trained with rules.",
            "comparison_baseline": "Compared with MetaRuleGPT, GPT variants, PaLM, Llama2, Goat.",
            "key_finding": "Large off-the-shelf models (e.g., Qwen-72B) can perform well on some curated/simplified benchmarks but still generally suffer on algorithmic high-digit arithmetic unless trained with explicit arithmetic rules.",
            "uuid": "e273.6",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "PaLM",
            "name_full": "Google PaLM",
            "brief_description": "Google's large language model included as a baseline in comparisons; showed strong performance on some simplified benchmarks but authors report degraded performance on complex arithmetic/vector tasks for many baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Addition, subtraction, vector cross-product (benchmarked)",
            "number_range_or_complexity": "Low-digit and high-digit tests; simplified GSM8K subset included",
            "method_or_intervention": "Standard baseline invocation",
            "performance_result": "Reported 100% on the paper's simplified GSM8K subset (table VIII); authors note PaLM (like other baselines) degrades on large-digit arithmetic and complex vector tasks without targeted rule learning or tools.",
            "mechanistic_insight": "No mechanistic detail given in the paper for PaLM; cited general limitation: difficulty learning unified representation for text and numbers.",
            "performance_scaling": "Not analyzed in detail in this paper; baseline models generally degrade as digit complexity increases.",
            "failure_modes": "Fails more on high-digit arithmetic and vector cross-product tasks without specialized training.",
            "comparison_baseline": "Compared against MetaRuleGPT and other LLM baselines.",
            "key_finding": "Strong on some curated benchmarks but like other large models can fail on algorithmic arithmetic and vector computations unless trained or augmented for rules.",
            "uuid": "e273.7",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Goat",
            "name_full": "Goat (Mathematical Mastery Model / fine-tuned Llama variant)",
            "brief_description": "A model reported in related work (and included in comparisons) that focuses on arithmetic skill improvement via fine-tuning; used as a baseline in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Goat",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Arithmetic tasks (addition/subtraction, other math benchmarks); included in comparisons",
            "number_range_or_complexity": "Low-digit and high-digit tests (as part of benchmark suite)",
            "method_or_intervention": "Baseline fine-tuning approaches referenced in related work; in this paper Goat was invoked as an evaluated baseline",
            "performance_result": "Included in comparison plots/tables; paper's Figure 6 compares addition/subtraction accuracy of GPT-4, Goat and MetaRuleGPT across digits (MetaRuleGPT claimed 100% as digits increase while others decline). Exact numeric results for Goat not fully enumerated in text.",
            "mechanistic_insight": "No mechanistic analysis provided here for Goat; referenced as a fine-tuned arithmetic-focused baseline.",
            "performance_scaling": "Paper's figures imply Goat's accuracy declines with increasing digit count (similar trend to other large models), but explicit scaling numbers not provided in text.",
            "failure_modes": "Degrades on high-digit arithmetic; lacks explicit rule-based iterative verification mechanism described for MetaRuleGPT.",
            "comparison_baseline": "Compared in plots/tables against MetaRuleGPT and GPT-4 for digit-wise arithmetic accuracy.",
            "key_finding": "Even fine-tuned arithmetic-specialist models (like Goat) showed degraded accuracy as digit count rose compared to a small rule-trained model that performs iterative rule application.",
            "uuid": "e273.8",
            "source_info": {
                "paper_title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
            "rating": 2,
            "sanitized_title": "goat_finetuned_llama_outperforms_gpt4_on_arithmetic_tasks"
        },
        {
            "paper_title": "Recursion of thought: A divide-and-conquer approach to multi-context reasoning with language models",
            "rating": 2,
            "sanitized_title": "recursion_of_thought_a_divideandconquer_approach_to_multicontext_reasoning_with_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01406625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules
18 Dec 2024</p>
<p>Kejie Chen 
Lin Wang 
Qinghai Zhang qinghai@zju.edu.cn 
Renjun Xu </p>
<p>Zhejiang University Hangzhou
China</p>
<p>Zhejiang University Hangzhou
China</p>
<p>Zhejiang University Hangzhou
China</p>
<p>Zhejiang University Hangzhou
China</p>
<p>MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules
18 Dec 202470B19A886CADA2287D239DFD26863D1EarXiv:2412.13536v1[cs.CL]LLMsruleshigh-digit calculations
Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic.Inspired by meta-learning, we propose that models should acquire not only task-specific knowledge but also transferable problem-solving skills.We introduce MetaRuleGPT, a novel Transformer-based architecture that performs precise numerical calculations and complex logical operations by learning and combining different rules.In contrast with traditional training sets, which are heavily composed of massive raw instance data, MetaRuleGPT is pre-trained on much less abstract datasets containing basic, compound, and iterative rules for mathematical reasoning.Extensive experimental results demonstrate MetaRuleGPT can mimic human's rule-following capabilities, break down complexity, and iteratively derive accurate results for complex mathematical problems.These findings prove the potential of rule learning to enhance the numerical reasoning abilities of language models.</p>
<p>I. INTRODUCTION</p>
<p>In the field of Natural Language Processing, large language models such as GPT-4 [1] have made remarkable progress, demonstrating impressive understanding and generation capabilities across a wide range of tasks such as text summarization [2] [3] [4] question answering [5] [6] and mathematical reasoning [7] [8] [9].However, these models still face considerable challenges when encountering mathematical tasks in a broad spectrum, including but not limited to basic arithmetic operations, calculus, and equation solving.Despite their powerful language understanding capabilities, these models often struggle with even simple numerical calculations.For example, given a straightforward addition problem:</p>
<p>241257284 + 758742716 = 1000000000, most current mainstream language models have difficulty in correctly completing such basic mathematical operations.</p>
<p>Inspired by the success of chain-of-thought(CoT) [10] reasoning, we can see that, similar to the human brain's System II, a model can achieve more reliable results on complex input problems through repeated and recursive thinking.In mathematics, this approach-recursive reasoning with a simple set of rules-serves as an effective paradigm for precisely solving complex issues.</p>
<p>In this context, we introduce MetaRuleGPT, a cuttingedge Transformer-based [11] architecture that excels CoT in precise numerical calculations and intricate logical operations through the learning and integration of diverse rules as shown in Fig 1 .MetaRuleGPT is adept at emulating human-like rule-following behaviors, enabling it to simplify complexities and iteratively arrive at accurate solutions for challenging mathematical problems.This advancement underscores the promise of rule learning in enhancing the numerical reasoning capabilities of language models.</p>
<p>II. RELATED WORK</p>
<p>A body of research indicates that LLMs often fail to capture the underlying logic required for mathematical problemsolving, particularly in tasks [6] [12] that necessitate precise numerical calculations and complex logical operations.Brown et al. [13], have examined the limitations of LLMs in mathematical tasks, revealing that even basic operations can lead to inaccuracies.A few methods [14] [15] [16] are proposed to improve the quality of reasoning paths.Complexity-based CoT [15] selects examples with more steps as in-context demonstrations and shows that prompting with more reasoning steps leads to better performance.Wei et al [17] explored the role of fine-tuning and prompt engineering in improving LLM performance on specific tasks, yet these methods often fall short when applied to higher-order mathematical reasoning.</p>
<p>Meta-learning [18] has emerged as a promising approach to enhance model capabilities, emphasizing the need for transferable problem-solving skills rather than mere task-specific knowledge.Research by Finn et al. [19] and subsequent metalearning frameworks illustrate how models can be designed to generalize across tasks by learning from previous experiences.This paradigm has inspired our development of MetaRuleGPT, which not only seeks to improve numerical reasoning but also aims to equip models with the ability to learn and apply a variety of mathematical rules through structured pre-training.</p>
<p>Rule learning [20] demonstrated its efficacy in enhancing reasoning abilities, showing that integrating rule-based reasoning into model architectures can yield significant performance gains, especially in complex logical reasoning tasks.Building on these insights, MetaRuleGPT employs a Transformer-based architecture that systematically incorporates basic, compound, Fig. 1.This image illustrates the differences between MetaRuleGPT and the traditional Chain-of-Thought (CoT) reasoning method in handling mathematical problems.MetaRuleGPT employs a rule-based reasoning approach, breaking down and solving problems step-by-step through predefined rules, such as alignment rules, carry rules, and borrow rules.This structured method ensures accuracy and generalization ability in reasoning, allowing the model to systematically handle various computational tasks while avoiding the common hallucination errors found in CoT methods.and iterative rules for mathematical reasoning, thereby addressing the shortcomings observed in existing models.</p>
<p>III. RESEARCH METHODOLOGY</p>
<p>To enhance the accuracy and generalization of language models [21] in solving complex logical reasoning and numerical calculation tasks, we introduce the MetaRuleGPT model.This model aims to bolster the reasoning capabilities and generalization potential of language models [22], inspired by the concept of meta-learning.MetaRuleGPT focuses on mastering general learning strategies to precisely complete complex logical deduction tasks by applying learned rules.The model dynamically integrates basic mathematical computation rules with higher-order operation rules, enhancing its ability to process rule combinations.This design allows MetaRuleGPT to exhibit superior accuracy and generalization when faced with complex logical reasoning challenges, such as mathematical problems.</p>
<p>Furthermore, by adopting this strategy, MetaRuleGPT is not limited to handling a single task but is capable of learning and executing various different tasks.When dealing with multitasking, the rules across different tasks might intersect; our model can flexibly learn these intersecting rules and dynamically apply them to complete multiple tasks simultaneously, while keeping the tasks independent of each other without interference.</p>
<p>A. Specific Rule Learning for Arithmetic Tasks</p>
<p>In our research, the model exhibited exceptional logical reasoning and generalization capabilities while tackling three complex tasks: high-digit(10 or more digits) addition and subtraction, as well as vector cross-product computations.To facilitate these tasks, we created specialized rule datasets for training.For instance, during the training for addition calculations, the model was guided to acquire essential knowledge, including single-digit addition rules, carry rules, digit mapping rules(between numbers and strings), and fundamental computation rules in Fig. 2. By mastering these foundational concepts, and following meticulous pre-training [21], our model was able to flexibly apply and integrate these rules, enabling it to accurately perform complex mathematical operations, including high-digit addition and subtraction.</p>
<p>We extended our model's capabilities to perform vector cross-product computations since it had mastered the rules of addition and subtraction.To achieve this goal, we introduced rules for vector representation and cross-product computation into the model to realize vector cross-product calculations.This means that once the model learns these new rules, it could combine the newly acquired rules with existing numerical computation rules to perform vector cross-product calculations.During the process of vector cross-product computation, the model needs to handle a large amount of complex derivation.Through gradual derivation, combining the right-  hand rule for cross-products with basic numerical computation rules, the model gains the ability to compute vector crossproducts.This strategy showcases the model's deep logical reasoning and strong generalization ability to solve more complex mathematical operations by learning and integrating various rules in TABLE.I.
- -  Nine Addition Table  -  Nine Subtraction Table -   Nine Multiplication Table - -  Mapping Rule    Carrying Rule  -  Borrowing Rule -   Vector Product Rule - -  Compute Rule   </p>
<p>B. Arithmetic Dataset</p>
<p>We designed the overall computation rule dataset for training, covering a wide range of arithmetic operations from the most basic single-digit arithmetic tasks to various complex arithmetic rules.This dataset, carefully planned, only contains the most basic single-digit operations, including alignment rules, carry rules, borrow rules, basic computation rules, and composite rules.Each arithmetic expression involves 2 to 10 operational steps, involving a series of mathematical computation operations, such as addition (+), subtraction (), and vector cross-product operations ().Our constructed dataset contains approximately 20, 000 records.</p>
<p>C. MetaRuleGPT Model Structure</p>
<p>To closely mimic the natural process of humans solving mathematical problems, we did not directly solve each complex arithmetic expression but adopted an iterative and stepwise strategy.Through this method, our model breaks down complex expressions into a series of simpler and basic computational steps, reasoning out the final answer step by step.This approach enables the language model to have a deeper understanding and more effective application of specific rules during the learning process, allowing for flexible combination and application of these rules in problem-solving.MetaRuleGPT's proficiency in mathematical tasks stems primarily from its mastery of core computation rules rather than mere memorization of specific cases.</p>
<p>Focusing on arithmetic tasks, we developed a language model based on the Transformer architecture, aimed at solving mathematical problems, which we refer to as the MetaRuleGPT language model.The model architecture, as shown in the Fig. 3, includes several key components: the MetaRuleGPT pre-trained model, the RefeedFormatter (formatting tool), and VeriGate (verification gate).We designed a self-iteration method that enables the model to simplify complex problems through continuous iteration, and finally obtain the correct answer within a limited number of steps.</p>
<p>D. MetaRuleGPT Pre-trained Model</p>
<p>We have trained a language model based on the Transformer [11] architecture.To flexibly adjust the model's parameter size and internal structure, we designed and implemented a custom Transformer model.Given the limited vocabulary involved in the problems we address, we adopted a single-byte-based training method, which offers clear advantages over traditional word-based or character-based methods.</p>
<p>Byte-based language models provide a flexible and effective means for handling multilingual text and unknown characters.Fig. 4 is an example of using the Transformer model to train vector cross product calculation rules.By processing each character individually, the model can ensure more accurate learning of the rules, laying a solid foundation for solving complex logical tasks.</p>
<p>E. MetaRuleGPT Calculation Example</p>
<p>To demonstrate how the model operates, we use a simple addition example.When the input "Input: 78 + 263" is provided, it is processed sequentially through the Mapping Rule, Compute Rule, Align Rule, and Carry/Borrow Rule to derive the computation result.Fig. 5 illustrates the internal structure of our model and explains how the initial input is transformed into the final result.</p>
<p>1) First, the model structurally processes our input question, where "78 + 263" under the Mapping Rule becomes:
a 1 = 7, b 1 = 8, c 1 = 2, d 1 = 6, e 1 = 3.
The expression "a 1 b 1 + c 1 d 1 e 1 " through the Align Rule becomes:
c 1 |(a 1 + d 1 )|(b 1 + e 1 ).
Through alignment, a combination of mapping rules produces the intermediate output:"2|(7 + 6)|(8 + 3)".Finally, the result is formatted using VeriGate to output: "Output : 341".In summary, MetaRuleGPT utilizes a combination of rules to align, carry, and output the final result during the computation process.</p>
<p>IV. EXPERIMENTS</p>
<p>A. Experimental Setup</p>
<p>To demonstrate the exceptional accuracy and generalization capability of our MetaRuleGPT model in reasoning tasks, we designed two experiments: numerical arithmetic tasks and vector cross-product computation tasks.These experiments not only tested the model's basic computational ability but also its ability to solve complex problems, providing a solid foundation for comprehensively evaluating the model's performance in logical reasoning.Furthermore, to further prove the advantages of MetaRuleGPT, we compared it with several well-known large language models, including ChatGPT-3.5,ChatGPT-4.0[23], Alibaba's QWen [24], Google's Palm [25], [26], Llama2 [27] and Mathematical Mastery Model Goat [28], demonstrating the superior performance of MetaRuleGPT.</p>
<p>B. Test Dataset</p>
<p>Current large language models exhibit certain limitations in handling mathematically rigorous problems, partly due to a lack of deep understanding of mathematical logic.In contrast, our model, built from the ground up on the fundamental principles of mathematics, demonstrates higher precision in solving math challenges.To validate this advantage, we designed diverse test datasets and prepared a comprehensive computation dataset to highlight the significant advantage of our model in mathematical reasoning.Through these validations, we demonstrate that MetaRuleGPT not only precisely grasps and applies the basic logic of mathematics but also exhibits powerful generalization in problem-solving.</p>
<p>In the domain of arithmetic tasks, we constructed a diverse training dataset containing a wide range of arithmetic operations.To comprehensively evaluate our model's computational accuracy and generalization ability, we designed an evaluation dataset containing 8, 000 test cases shown in Table .II, entirely non-overlapping with the training set.This dataset covers various types of numerical operations, including but not limited to perfect decimal addition, reverse magnitude subtraction, misplaced subtraction, and addition and subtraction operations based on randomly generated numbers.</p>
<p>To compare with other models focusing on mathematical calculations, we used an additional substantial number of randomly generated dataset sampled from a logarithmic space1 .This sampling approach ensures that the numbers are equally likely to originate from different orders of magnitude, similar to the method employed by [29].</p>
<p>C. Evaluation Metrics</p>
<p>In evaluating the model's performance, we consider not only the accuracy of the computed results but also the difference ratio between the computed and correct answers.A smaller absolute difference ratio indicates that the model's output is closer to the true value, suggesting potential for improvement through parameter tuning.Conversely, a difference ratio greater than 1 implies that the model struggles with such problems or faces significant challenges.Assuming the number of correctly predicted quantities is TP and the total number of predictions is N, then accuracy can be defined as: Accuracy = T P N  100% Suppose our model's computation result is y, the actual  computation result N numbers in total, then our final overall difference ratio can be defined as:
DifferenceRatio = 1 N N i=0 yi i max(yi, i)</p>
<p>D. Deep Numerical Optimization Experiments on Language Models</p>
<p>To test our model's mathematical reasoning and generalization capabilities, we conducted comparisons using well-known language models such as the currently leading ChatGPT-4.0,ChatGPT-3.5,Alibaba's QWen, Google's Palm, Llama2 and Goat.Through such comparisons, we can comprehensively assess the performance differences between models and evaluate MetaRuleGPT's capabilities in mathematical reasoning tasks.A comprehensive comparison is shown in Fig. 6 and Table .III.</p>
<p>Using the specific test datasets, we previously organized to invoke and test with the aforementioned large language models, preserving and comparing the computational results of each model.We conducted a series of detailed experiments and evaluations, and the results are shown in Table .IV, V, VI, IX and VII.</p>
<p>Finally, in order to compare with the public dataset, we selected some subsets of gsm8k [30] and simplified the natural language part into mathematical formulas for comparison.The results are shown in VIII.</p>
<p>E. Language Model-Driven Vector Cross Product Calculation Experiment</p>
<p>To demonstrate our model's capability in handling complex logical problems, we have selected vector cross product calculation, a challenging mathematical task, as a test case.This test not only verifies the model's computational accuracy but  also compares its performance with those of leading large language models.Table .X presents the detailed accuracy comparison results of various models on the vector cross product calculation dataset.</p>
<p>V. RESULTS AND DISCUSSION</p>
<p>A. Test Data Results Analysis 1) Numerical calculation results: To assess our model's performance in solving general numerical problems, we generated a large amount of experimental data with random numbers using Python.The preliminary results show that MetaRuleGPT and other tested language models achieved accuracy rates exceeding 70% in low-digit addition and subtraction operations, demonstrating high computational precision.However, as the number of digits increased, the performance of most language models significantly declined.Except for ChatGPT, other models often made mistakes in handling high-digit calculations due to their inability to deeply grasp computational rules, almost losing their computational capability.Remarkably, MetaRuleGPT maintained 100% accuracy even when facing high-digit random addition tasks.Although it faced certain challenges in high-digit random subtraction tasks, our model still showed the highest accuracy among all tested language models.This achievement not only highlights MetaRuleGPT's strong performance in solving complex numerical problems but also proves its generalization ability.</p>
<p>Table IV  2) Vector Cross Product Results: From the data in Table III, it is evident that the Llama2 models with 7B and 13B parameter sizes were incapable of performing vector cross product calculations, while the Llama2-70B, the largest parameter model of the Llama2 series, could perform cross product calculations but with 0% accuracy.Even the state-of-the-art ChatGPT achieved an accuracy rate below 50% without the aid of external tools.In contrast, MetaRuleGPT accurately calculated vector cross products in three-dimensional space with 100% accuracy, confirming the effectiveness of enhancing model capabilities by combining different rules.By comprehensively learning basic operational rules such as addition, subtraction, multiplication, and cross product, MetaRuleGPT achieved precise invocation of these rules and successfully outputted accurate calculation results.within the same pre-trained model, MetaRuleGPT demonstrated multi-task generalization ability.This indicates that our model is not only adaptable to various task scenarios but can also identify and apply common rules among these tasks, significantly enhancing learning efficiency.These results further showcase MetaRuleGPT's strong performance and flexibility.</p>
<p>B. Discussion</p>
<p>Although existing language models have demonstrated powerful capabilities, they still face challenges in terms of controllability.In particular, most models struggle to precisely answer questions within a controlled range, often resulting in significant deviations, which is a crucial issue that current language models need to address.Conversely, MetaRuleGPT's rule-based execution ensures relative reliability and better controllability.</p>
<p>Experiments show that existing language models struggle with high-digit calculations and complex computational tasks due to limitations in understanding and the difficulty of learning a unified representation for text and numbers.MetaRuleGPT addresses these challenges by precisely completing complex tasks and demonstrating the generalization potential of language models in numerical calculations through rule learning.</p>
<p>VI. CONCLUSION</p>
<p>This study explores the rule-following capabilities of language models, focusing on the combinatorial skills and generalization abilities that humans display in problem-solving.We introduce MetaRuleGPT, a Transformer-based language model that utilizes an iterative strategy and learns from a series of compound rules and sub-rules.With only 30 million parameters, MetaRuleGPT demonstrates high accuracy in handling high-digit calculations and complex vector cross-product operations, surpassing current mainstream large language models.Our findings highlight the importance of incorporating rulebased learning in language models to enhance their numerical reasoning abilities and generalization skills.MetaRuleGPT's success in solving complex mathematical problems with relatively few parameters showcases the effectiveness of this approach and paves the way for future research in this direction.</p>
<p>LIMITATIONS</p>
<p>This research raises several issues worthy of further exploration, including:</p>
<p>1) Although our model has shown certain generalization and understanding abilities after rule learning, it is limited by computational resources, and the variety of problems it can handle is relatively limited.Expanding the model's parameter size and training with more diverse rule datasets could enable MetaRuleGPT to handle a broader range of logical tasks.</p>
<p>2) The controllability of MetaRuleGPT may vary with the increase in required tasks, and we plan to add more task rules in future model training to further evaluate and improve the model's controllability.Moreover, we aim to enhance MetaRuleGPT's controllability when handling tasks beyond its current capabilities.For instance, while MetaRuleGPT excels in numerical computation tasks, it may produce unpredictable results when attempting function integration problems, leading to significant errors.Addressing this challenge is one of our current focuses, and we plan to introduce more rule data in future optimizations to improve the model's overall controllability, enabling greater stability and accuracy across a wider range of applications.3) MetaRuleGPT currently cannot automatically handle untrained generalization forms or novel concepts beyond the meta-learning distribution, which limits its ability to tackle entirely new problems.Achieving humanlike systematic generalization by leveraging real-world training experiences remains an open question and a direction for future research.</p>
<p>Fig. 2 .
2
Fig. 2.This picture shows some training rule examples, where * and !represents 10 and 9 in decimal respectively.Once our training data covers the logic required for the operation, the LLMs can follow this underlying logic to complete the arithmetic operation.</p>
<p>Fig. 3 .
3
Fig. 3.This figure shows Architecture Diagram of MetaRuleGPT.MetaRuleGPT Pre-trained model is a model that has learned basic rules.VeriGate is used to identify whether the current decoding meets the expected transformation of the recent decodings.If it does not meet the expectations, it will enter RefeedFormatter to realign and adjust the structure and then use the basic rules of the model again.According to the operation rules, after a limited number of calls to the basic rule model, the final output is obtained.</p>
<p>Fig. 4 .
4
Fig. 4. MetaRuleGPT Pre-trained Model.SAN represents Self-Attention Network.</p>
<p>2 )
2
Similarly, for "2|(7 + 6)|(8 + 3)", a combination of the Mapping Rule and single-digit addition rule (Add Subrule) produces the intermediate output: "2|13|11".3) When "2|13|11" is input, the model invokes the Carry Rule and the Mapping Rule to perform digit carry operations, producing an intermediate output: "(2 + 1)|(3 + 1)|1".</p>
<p>4) "( 2
2
+ 1)|(3 + 1)|1" as a new input, again applying the Mapping Rule and Compute Rule, leads to the final computation result: "3|4|1".5) "3|4|1" as the final input stage, our model invokes the formatting rules and uses special symbols for marking.</p>
<p>Fig. 5 .
5
Fig. 5. MetaRuleGPT language model calculation example diagram</p>
<p>( 6 , 5 , 7 )Fig. 6 .
6576
Fig. 6.Comparison of the accuracy of addition and subtraction of GPT4, Goat and MetaRuleGPT on different digits.When the number of digits increases, the accuracy of other models begins to decline, indicating that they do not understand the principles of arithmetic.Our model can guarantee 100% accuracy.</p>
<p>TABLE I SUMMARY
I
TABLE OF LEARNING RULES FOR VARIOUS TASKS Rule Type Numerical Addition Numerical Subtraction Vector Cross Product Vector Table</p>
<p>TABLE II PARTIAL
II
TEST DATA DISPLAY TABLE Data Type Test Dataset Examples Randomized Procedure 6729132856 + 1854307391, . . ., 1554887316  817095695 Perfect Decadic Addition 6659891948 + 340108052, . . ., 4376628072 + 623371928 Reverse Magnitude Subtraction 62103  2386797965, . . ., 53006  7764286617 Interleaved Subtraction 1824453209  482835016, . . ., 8858241744  261714262 Vector Cross Product</p>
<p>TABLE VIII SIMPLIFIED GSM8K
VIIIGSM8K</p>
<p>TABLE Model
ModelAccuracyGPT-4100%GPT-3.599%llama2-7b-llama2-13b-llama2-70b98%Google-PaLM100%Qwen-72b-Chat100%MetaRuleGPT100%</p>
<p>presents the test results of various models on misaligned subtraction.Table V shows the test results of various models on reverse amplitude subtraction.Tables VI and VII respectively demonstrate the test results of various models on randomly generated numbers.Table IX displays the test results of various models on perfect decimal addition.</p>
<p>Table X provides detailed accuracy comparison results of different models on the dataset for vector cross-product computation.Moreover, by training rules for two different types of tasks</p>
<p>Our dataset is available at https://www.scidb.cn/en/detail?dataSetId= 04575028fb8d4bfabeeba5825c8f57fc</p>
<p>Benchmarking large language models' performances for myopia care: a comparative analysis of chatgpt-3.5, chatgpt-4.0, and google bard. Z W Lim, K Pushpanathan, S M E Yew, Y Lai, C.-H Sun, J S H Lam, D Z Chen, J H L Goh, M C J Tan, B Sheng, EBioMedicine. 952023</p>
<p>TL;DR: Mining Reddit to learn automatic summarization. M Vlske, M Potthast, S Syed, B Stein, Proceedings of the Workshop on New Frontiers in. L Summarization, J C K Wang, G Cheung, F Carenini, Liu, the Workshop on New Frontiers inCopenhagen, DenmarkAssociation for Computational LinguisticsSep. 2017</p>
<p>Teaching machines to read and comprehend. K M Hermann, T Koisk, E Grefenstette, L Espeholt, W Kay, M Suleyman, P Blunsom, 2015</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. S Narayan, S B Cohen, M Lapata, 2018</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Y Bisk, R Zellers, R L Bras, J Gao, Y Choi, 2019</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, 2024</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, 2023</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. P Wang, L Li, Z Shao, R X Xu, D Dai, Y Li, D Chen, Y Wu, Z Sui, 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez,  Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Least-tomost prompting enables complex reasoning in large language models. D Zhou, N Schrli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, E Chi, 2023</p>
<p>Complexitybased prompting for multi-step reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, 2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, 2023</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, 2022</p>
<p>Meta-learning: A survey. J Vanschoren, 2018</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, 2017</p>
<p>Functional Network Construction in Arabidopsis Using Rule-Based Machine Learning on Large-Scale Data Sets. G W Bassel, E Glaab, J Marquez, M J Holdsworth, J Bacardit, 10.1105/tpc.111.088153The Plant Cell. 23909 2011</p>
<p>Ernie: Enhanced language representation with informative entities. Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, arXiv:1905.071292019arXiv preprint</p>
<p>Human-like systematic generalization through a meta-learning neural network. B M Lake, M Baroni, Nature. 62379852023</p>
<p>OpenAI's ChatGPT: A Revolution in Language AI. Openai, Sep. 2021</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. T Liu, B K H Low, 2023</p>
<p>Recursion of thought: A divide-and-conquer approach to multi-context reasoning with language models. S Lee, G Kim, 2023</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
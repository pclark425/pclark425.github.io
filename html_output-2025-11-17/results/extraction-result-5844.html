<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5844 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5844</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5844</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-259187647</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.09928v1.pdf" target="_blank">Friend or Foe? Exploring the Implications of Large Language Models on the Science System</a></p>
                <p><strong>Paper Abstract:</strong> The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 experts specialising in research and AI. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and helps identify areas for future action.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5844",
    "paper_id": "paper-259187647",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00508375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Friend or Foe? Exploring the Implications of Large Language Models on the Science System
2023</p>
<p>B Fecher 
M Hebing 
M Laufer 
J Pohle 
F Sofsky 
Friend or Foe? Exploring the Implications of Large Language Models on the Science System
202310.5281/zenodo.8009429This work is distributed under the terms of the Creative Commons Attribution 4.0 Licence (International) which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited (https://creativecommons.org/licenses/by/4.0/). 1 FRIEND OR FOE? EXPLORING THE IMPLICATIONS OF LARGE LANGUAGE MODELS ON THE SCIENCE SYSTEM AUTHOR INFO, AFFILIATION &amp; FUNDING Benedikt Fecher (corresponding author), Alexander von Humboldt Institute for Internet and Society (Berlin) &amp; Wissenschaft im Dialog (Berlin), fecher@hiig.de This study was funded and primarily conducted by the Alexander von Humboldt Institute for Internet and Society. The authors and affiliated institutions declare no conflicting interests. HIIG Preprint · 2023-01 2 FRIEND OR FOE? EXPLORING THE IMPLICATIONS OF LARGE LANGUAGE MODELS ON THE SCIENCE SYSTEM CONTENTSlarge language modelsdelphi studyscience transformationgenerative AIscientific practise CITATION
The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 experts specialising in research and AI. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and helps identify areas for future action.</p>
<p>INTRODUCTION</p>
<p>The release of ChatGPT by OpenAI in November 2022 has sparked a plethora of editorials, position papers and essays, or interviews with experts, as well as some articles and preprints on the potential impacts on science and higher education. While many concerns raised relate to how ChatGPT will change education (e.g., Perkins, 2023), there is much less-especially empirical research-on the implications of large language models (LLMs) as well as LLM-based chatbots or prompts on the science system and scholarly practices (Ribeiro et al., 2023). One can however draw inspiration from fields that are also characterized by largely text-based or -focused, creative and knowledge work. For instance, the editorial by Dwivedi et al. (2023) provides a viewpoint on the potential impact of generative AI technologies such as ChatGPT in the domains of education, business, and society, based on 43 contributions by AI experts from various disciplines. However, the literature on knowledge work and the transformative effects of AI cannot account for the complexities of specific practices (Jiang et al., 2022).</p>
<p>In light of the limited research conducted on large language models and their impact on the science system and scientific practice, we initiated a Delphi study involving experts who specialize in the intersection of research and AI technology. The purpose of this study was to investigate the following areas: a) the potential applications and limitations in using LLMs, b) the positive and negative effects of LLMs on the science system, c) the regulatory and ethical considerations associated with the use of LLMs in science, and d) the necessary competencies and capacities for effectively utilizing LLMs. Our objective in this study was to gather and structure expert opinions in an initial phase, focusing on the aforementioned categories, and subsequently evaluate and assess them in a second phase. As generative AI continues to advance, it is crucial to gather expert knowledge and informed assessments regarding its potential impact on science. This knowledge will contribute to an informed scholarly debate and help anticipate potential fields of action.</p>
<p>Our findings indicate that experts anticipate that the utilization of LLMs will have a transformative and largely positive impact on science and scientific practice. In LLMs, they recognize significant potential for administrative, creative and analytical tasks. The main risks associated with LLMs pertain to issues of bias, misinformation, and overburdening of the scientific quality assurance system. Despite the perceived advantages of LLMs for science, it is imperative to acknowledge and address the associated risks. This necessitates proactive measures in regulation and science education.</p>
<p>LITERATURE REVIEW</p>
<p>In the following, we provide an overview of the current state of the scholarly discourse along the aforementioned areas. While our aim was to present a comprehensive and contemporary overview of this discourse. However, it is important to acknowledge that new and pertinent studies may have emerged by the time of the publication of this article.</p>
<p>Applications and limitations of LLMs in science</p>
<p>LLMs and LLM-based tools are widely expected to have a wide range of applications in scientific practice. Possible uses for researchers identified in the literature range from generating plausible research ideas , brainstorming (Staiman, 2023), transforming notes into text (Buruk, 2023), creating a first draft of a paper (Dwivedi et al., 2023), assisting with grammar and language (Flanagin et al., 2023), e.g. to improve clarity (Lund et al., 2023), especially for non-native speakers (Perkins, 2023), but also stylistic issues, from formatting references to complying with editing standards (Flanagin et al., 2023;Lund et al., 2023). LLM-based tools like ChatGPT may be used to generate literature reviews , data crunching (Staiman, 2023), data summaries , even proposing new experiments (Grimaldi &amp; Ehrler, 2023). They may support the dissemination of publications and the diffusion of knowledge by helping to create better metadata, indexing, and summaries of research findings (Lund et al., 2023). They are expected to assist editors in screening submission for issues such as plagiarism or image manipulation, triaging, validating references, editing and formatting (Flanagin et al., 2023;Hosseini &amp; Horbach, 2023). Beyond scholarly writing, LLM-based tools are expected to assist with code writing, automating simple tasks and error management (Dwivedi et al., 2023), but also in writing reports, strategy documents, emails as well as cover and rejection letters (Corless, 2023). Scientists may also use LLM-based tools for non-scholarly tasks, as a recent Nature poll has shown: while eighty per cent of respondents have used AI chatbots, more than half say they use them for 'creative fun' (Owens, 2023).</p>
<p>While the fields of application appear diverse, it is widely accepted that LLMs and LLM-based tools have limitations in scholarly use. Several editorials and Op Eds have been published that point to glaring mistakes of ChatGPT, including referencing scientific studies that do not exist (Perkins, 2023). The company behind ChatGPT, OpenAI, admits openly in its blog: "ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers" (OpenAI, 2022). At the time of writing this article, all existing LLM-based chatbots have been trained on outdated data. As a result, they do not possess the capability to incorporate real-time data automatically, leading to a lack of updated information (Dwivedi et al., 2023). Other limitations that have been identified include flawed logical argumentation, lack of critical elaboration, and unoriginal generated content (Dwivedi et al., 2023). Errors may also occur in interpreting meaning, in particular if terms are ambiguous, have multiple meanings or consist of compound words (Lund et al., 2023). In addition, generated texts may lack semantic coherence and lexical diversity (Perkins, 2023). Teubner et al. (2023, p. 96) state that the produced texts often "read somewhat bland, generic, and vague with a noticeable tendency to seek balance", and that a very common ChatGPT phrase is: "However, it is important to note…". Like ML-based systems in general, LLM-based chatbots are considered to lack transparency and explainability (Dwivedi et al., 2023), and reproduce or even amplify biases inherent in the information that was used to train them (Corless, 2023;, reproducing an "of the same old trivialities and stereotypes" (Teubner et al., 2023, p. 99). This is considered a structural issue of how these systems are trained and cannot be resolved by simply creating bigger models as size does not guarantee diversity (Bender et al., 2021).</p>
<p>Opportunities and risks for the science system</p>
<p>A prevailing viewpoint in the literature anticipates positive effects of LLMs on the science system. Potentially opportunities of LLMs on science include positive effects on scholarly pro ductivity, quicker access to available scholarly resources via enhanced search engines to the automation of mundane, repetitive or tedious work such as correcting grammatical errors, allowing people to focus on creative and non-repetitive activities (Dwivedi et al., 2023;Lund et al., 2023). Foremost among these anticipated benefits is the enhancement of research productivity and the elevation of publication quality. There is an expectation that by using these tools to improve their texts, researchers "can focus more on what to communicate to others, rather than on how to write it" (Pividori &amp; Greene, 2023, p. 15). Staiman (2023 n.p.), for instance, notes that the writing process should be considered less an end in itself but rather "a means to an end of conveying important findings in a manner that is clear and coherent". Along these lines, Lund et al. (2023) suggest that the capability of ChatGPT and the like might lead to questioning the strong belief that 'publish or perish' is an important and valuable principle in academia and possibly change the criteria for evaluating tenure. Some scholars expect a revolution of "the whole scientific endeavor" and refer to these tools' fundamental disregard of the boundaries of scientific disciplines, which may help "bringing multidisciplinary science to new heights" (Grimaldi &amp; Ehrler, 2023, p. 879). Furthermore, these tools may also lead to the democratization of science: First, the research process might be democratized as LLM-based tools may compensate for the lack of financial resources, e.g. for "traditional (human) research assistance" (Lucey &amp; Dowling, 2023 n.p.). Second, the dissemination of knowledge might be democratized as these tools can easily polish the language of a text or even translate research output to multiple languages, both of which would level the field for researchers who speak English as a foreign language (Corless, 2023;Liebrenz et al., 2023).</p>
<p>Among the risks for the science system identified in the literature are the adverse effects on the academic quality assurance mechanisms and, subsequently, on scientific integrity. The avalanche of AI-generated "scientific-looking papers devoid of scientific content" (Grimaldi &amp; Ehrler, 2023, p. 879) is widely expected to overburden the academic review process and foster plagiarism (Dwivedi et al., 2023). Biases are expected to be reinforced and errors introduced into the scholarly debate that might be difficult to identify and correct (Lund et al., 2023). A recent study by Liang et al. (2023) evaluating the performance of several widely-used GPT detectors found that they consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Several scholars expect that LLMs may lead to an increase in misinformation and disinformation and more "junk science" (Corless, 2023 n.p.). In this regard, Lund et al. (2023) worry that the use of LLM-based tools in academia not only raises concerns about the reproducibility and transparency of research but may undermine trust in the scientific process (see also Van Noorden, 2022).</p>
<p>Competencies and capacities in scientific practice</p>
<p>It is assumed that LLMs and LLM-based tools will mark a shift in the academic skill set. Prompt engineering, developing and producing prompts for conversational AI systems like ChatGPT or is the most discussed new competence that is required from researchers (Teubner et al., 2023). This is believed to pose a particular challenge for individuals who already struggle with basic IT, as they will not derive much benefit from advances in AI, and this may lead to a widening productivity gap. As LLM-based tools may have better English writing skills than some people, especially non-native speakers, the focus in academic work is expected to shift from text writing to conducting research, which requires researchers to formulate interesting research questions and carry out research to find answers (Dwivedi et al., 2023). More generally, as Teubner et al. (2023, p. 98) observe, "the ability to read and interpret different text options becomes more important than the ability to write them." That means that researchers must be able to check the generated text for factual and citation accuracy, bias, mathematical, logical, and commonsense reasoning, relevance, and originality . That also means that researchers are expected to have the competencies to collate and combine the results that LLM-based tools generate (Floridi &amp; Chiriatti, 2020). Not surprisingly,  find that adding domain expertise greatly improves the quality of the generated results. Thus, among the key skills that researchers have to develop are critical thinking, problem solving, ethical decision-making, and creativity (Dwivedi et al., 2023).</p>
<p>Ethical and regulatory issues concerning ChatGPT</p>
<p>The existing literature often frames negative implications, i.e. risks, for the science system as ethical issues, and also mixes ethical and legal aspects. Issues are raised on how we understand 'authorship' in the research context, be it as accountability, as a substantial contribution to a text, as ownership in contrast to plagiarism, and with respect to text and language improvement (Staiman, 2023). Critics argue that chatbots cannot take responsibility for the content they produce and cannot be held accountable (Corless, 2023;Liebrenz et al., 2023). In addition, their ability to generate quality academic research ideas "raises fundamental questions around the meaning of creativity and ownership of creative ideas" (Lucey &amp; Dowling, 2023 n.p.), which in turn sparks questions about originality, scholarly citation practices and the boundary to plagiarism (Lund et al., 2023;Tomlinson et al., 2023). It thus comes as no surprise that publishers like Springer Nature have banned ChatGPT and similar software from being given authorship on papers: and Science editors have also prohibited the use of any text generated by those tools. Many commentators have raised concerns about the implications of the LLMs producing inaccurate or misleading output and the potential spread of misinformation (Dwivedi et al., 2023;Liebrenz et al., 2023). Similar ethical concerns are raised regarding the potential of these tools to reproduce and amplify bias, both in the training data and the development process, and the implications of this for the integrity of science (Lund et al., 2023). There have been techno-solutionist claims that potential harms of such systems can be mitigated by watermarking their output (Kirchenbauer et al., 2023). Additional ethical considerations include the potential to replace humans in the scholarly work process (Lund et al., 2023). This includes positions that were thought to be less likely to be automated until a few years ago (Dwivedi et al., 2023). Furthermore, the commercialization of these tools would exclude scholars and institutions in low-income and middle-income countries, thus entrenching existing inequalities in knowledge dissemination and scholarly publishing (Liebrenz et al., 2023).</p>
<p>There is a broadly perceived lack of regulation, or at least clear regulatory guidance for LLMs and related tools, on issues such as privacy, se curity, accountability, copyright violations, disinformation, misinfor mation and other forms of abuses and misuses of LLMs and LLM-based tools (Dwivedi et al., 2023;Khowaja et al., 2023;Lund et al., 2023). After the Italian Data Protection Authority imposed an immediate temporary limitation on the processing of Italian users' data by OpenAI in late March 2023 in order to enforce demands on the protection of data subjects' rights (GPDP, 2023), other national data protection authorities in Europe have followed suit and opened proceedings against OpenAI (Sokolov, 2023). European data protection authorities have even set up a task force to cooperate and exchange information on enforcing EU laws on OpenAI (Goujard, 2023). At the same time, the European Parliament called for expanding the potential reach of the proposed EU AI Act by including ChatGPT-like systems to the list of high-risk categories of AI systems (Helberger &amp; Diakopoulos, 2023). Furthermore, Hacker, Engel &amp; Mauer (2023) call for specific regulation of LLM-based tools, "large generative AI models", under the EU Digital Services Act and provide four concrete, workable suggestions that include transparency obligations, mandatory yet limited risk management, non-discrimination data audits, and expanded content moderation.</p>
<p>METHODOLOGY</p>
<p>To address our research objective, we employed the Delphi method. First developed in the 1960s, the Delphi method is a technique used to establish consensus among a group of experts on complex issues (Landeta, 2006) and in some cases used to forecast future developments (Linstone &amp; Turoff, 1975). In its basic form, this method can be described as a communication process that involves engaging experts at various stages, such as through surveys and qualitative interviews. The initial stage is open and exploratory, with the information gathered analyzed and used to inform subsequent data collections. This process continues until consensus is reached among experts, for example, in defining concepts and/or trends or weighing different viewpoints. In this light, the Delphi method is a fitting technique to investigate our objective of exploring the impact of ChatGPT and LLMs on scientific practices and the science system. The responses of the first survey were primarily coded by two authors. They first examined 25% of the responses to generate a codebook through a combination of inductive and deductive coding (Bazeley, 2009). The codebook was then evaluated by all authors and adjustments made when needed and the rest of the material was coded (for codebook see appendix table 7 and 8). Based on this analysis, the second survey was created consisting of 11 questions, the majority of which were ranking questions featuring the identified codes for applications and limitations, risks and opportunities for the science system and the competencies needed for using LLMs, as well as general opinion questions on LLMs impact on science and scientific practice. Furthermore, the survey instrument contained two open questions on future scenarios that we analyzed for the discussion part of the paper.</p>
<p>The survey was sent to the same experts, yielding 52 responses (72 % of the participants from the first round). A statistical analysis was conducted on the opinion and ranking questions. In the result tables (see tables 2 to 6 in the appendix), we provide the individual frequencies for each item and rank, as well as two scores. The first score (sum) is a simple sum of the preceding frequencies, the rank is a weighted sum, where the first rank is weighted by factor four and the second rank by factor two. The rank questions are followed by a set of statements, which the participants could evaluate on a five-point likert scale (strongly disagree, disagree, neither agree nor disagree, agree, strongly agree). We combined agree and strongly agree to sort the items and will also refer to the combination of both, when reporting it in the text. The open questions were analyzed with a combination of inductive and deductive coding, carried out jointly by the authors. Our delphi approach allowed us to identify and refine various implications of LLMs on the science system, however it was not without its limitations. For example, we were unable to track long-term implications as the interval between the data collections were relatively short.</p>
<p>We sought consent prior to each survey phase to publish the responses, aiming to enhance the transparency of our results and enable future research and educational use. The data (including the survey instruments) is published under a CC-BY-license and can be accessed via the following link.</p>
<p>RESULTS</p>
<p>Below, we present the Delphi study results based on the defined aspects, i.e. applications and limitations, risks and opportunities for the science system, competencies as well as legal and ethical implications. In each section, we begin by presenting the coded findings from phase one and use the results of the ranking and opinion questions to contextualize and weigh these results, when applicable. Figure 2 displays the results of the opinion questions, which we will refer to in the subsequent result sections. The results of the ranking questions analysis can be found in the appendix (tables 2 to 6).</p>
<p>Figure 2. Statements on LLMs, formulated based on the results from the first round of our delphi study and quantified in the second round.</p>
<p>Applications and limitations in use: LLMs as enhancement tools</p>
<p>The first phase yielded six distinct applications that can be effectively addressed by LLMs and LLM-based applications. These include (1) text improvement, which involves the rephrasing and optimization of textual content, (2) text summary, which involves the summarization of information, (3) text analysis, such as the use of sentiment analysis or qualitative coding, (4) code writing, which involves assistance in programming tasks, (5) idea generation, which involves generating new ideas through the combination of concepts, and (6) text translation, which includes the translation of a text entered into the LLM in different languages. Notably, the identified applications of LLMs extend beyond conventional text-based tasks in scientific publishing, although such tasks remain a dominant practice in the responses.</p>
<p>In the second round of the Delphi survey, we asked the experts to prioritize the identified applications. Our results show that text improvement is considered the most important application, followed by text summary as the second most important, and code writing as the third most important application. Most (59.6%) of the experts either already use or express their intention to use LLMs in their own work (figure 2). A significant portion (86.5%) of the experts perceive LLMs as valuable for administrative tasks, confirming the assumption that time savings are expected for researchers through LLM utilization (figure 2).</p>
<p>Asked about the limitations of LLMs in scientific work, five distinct types of limitations were mentioned. We observed (1) lack of transparency, as it is unclear on which data the model's outputs are based on, (2) incorrectness, especially regarding literature references and biographical information, which may affect the reliability of the generated text, (3) lack of creativity, as ChatGPT relies heavily on existing patterns and may struggle to generate entirely new content, (4) outdatedness, particularly as the version of ChatGPT used in this study relies on a database that only goes up to 2021, and (5) unspecificity, i.e. LLMs produce superficial texts that do not address topics in depth or detail. There are already approaches to address some of these limitations, even if not completely.</p>
<p>In the second round of the study, participants were requested to rank the limitations. The highest-ranked limitation was incorrectness, followed by non-transparency and unspecificity in the responses. The incorrectness of LLMs was a dominant and recurring issue mentioned by the experts. As one expert stated, "The largest problem I see are the factual mistakes, often given with confidence, which make it hard to trust ChatGPT and similar technology outputs without further research or prior knowledge".</p>
<p>The results indicate that the potential benefits of LLMs lie not only but primarily in text-based work, which is significant because scientific value creation in most disciplines is text-based. There is also evidence to suggest that LLMs are relevant for ideation, conception, and programming, the latter of which is an increasingly important scientific practice. Taken together, it is not surprising that a majority of the respondents assume that ChatGPT and other LLMs will transform scientific practice, although this mightat this stage -relate primarily to the textuality of academic work. The limitations mentioned can be essentially explained by the databases that existing LLMs were trained on, and it can be assumed that many of these limitations can be addressed in newer models, as some respondents pointed out. However, the non-transparency in the training data remains problematic and was viewed by some as inconsistent with scientific principles of quality.</p>
<p>Risks and opportunities for the science system: advantages trump disadvantages</p>
<p>According to the experts, the use of LLMs provides the science system with four opportunities: (1) LLMs can promote efficiency by automating and supporting text work, (2) LLMs may promote reflection by identifying biases and new research areas, (3) LLMs may reduce administrative workload, (4) LLMs can promote inclusiveness by leveling the playing field between researchers from different backgrounds and institutions, such as those who lack resources for grant writing or those who are non-native English speakers, and (5) LLMs promote productivity by freeing up time for researchers to conduct more analyses or produce more scientific articles. In the second phase of the Delphi study, the experts ranked these, with the reduction of administrative tasks ranked first, followed by more efficiency and inclusiveness (see table 4). These results indicate that researchers see LLMs primarily as a tool to relieve and simplify their workload. Hence, a large majority of the experts disagrees that LLMs could replace researchers (82.7%, figure 2).</p>
<p>The analysis of the first phase of the Delphi study reveals the existence of seven distinct risks associated with the use of LLMs in scientific work. These risks include (1) reinforce bias / dominant voices, because statistical systems favor mainstream opinions, (2) overburden academic quality assurance mechanisms with semi-automated papers, (3) reinforce inequalities between researchers who have access to LLMs and those who do not, (4) increase dependence on commercial providers, (5) encourage academic misconduct, either intentional or unintentional by researchers, (6) lead to a decrease in originality due to the generic nature of LLM-generated text, and (7) the possibility to an increase in disinformation, which could potentially challenge scientific truths in the public domain. In the second phase, the experts ranked these, indicating that bias is seen as the biggest threat, followed by disinformation and overburdening academic quality assurance mechanisms (see table 5).</p>
<p>These risks are significant as they touch on fundamental pillars of scientific ethics and good practice, such as scientific freedom regarding the dependence on commercial publishers, scientific quality assurance concerning the handling of highly generic publications, as well as the public legitimation of science, which could be put into question by plausible and seemingly scientific nonsense produced by LLMs -large majority of the experts (75.0%) regard LLMs as a catalyst for disinformation (figure 2). Notwithstanding the gravity of the aforementioned risks, the majority of experts perceive the benefits of LLMs to outweigh the drawbacks (figure 2), which explains why most of them already use or intend to use LLMs in their work. This, however, can also be attributed to the sampling strategy employed in this study, possibly involving technology-proficient experts. This result is noteworthy nonetheless and supports the hypothesis that generative AI will change scientific work in the long run.</p>
<p>Competencies in usage: scientists need to learn to (re)think</p>
<p>In the inquiry regarding the competencies required for researchers to utilize ChatGPT and other LLMs, the respondents pointed out four distinct competencies, namely (1) technical know-how to comprehend the inner workings of LLMs, (2) the ability to contextualize results utilizing the outcomes generated by LLMs in practical scenarios, (3) a reflective mindset to consider the feedback effects on scientific practice, and (4) ethical understanding to responsibly employ LLMs. In the second phase, they ranked a reflective mindset first, followed by the ability to contextualize results and ethical understanding. The results indicate that the experts anticipate feedback effects on science, while also suggesting that the responsible application of knowledge will become even more paramount in the future.</p>
<p>It can be argued that reflexivity highlights the ethical implications of AI on scientific practices and ways to proactively address them, while contextuality focuses on the practical use of AI-supported findings and strategies for maximizing their utility. Our findings suggest that generative AI should be incorporated in scientific training and science education, specifically in relation to scientific ethics and effective communication of AI-driven results in their appropriate context.</p>
<p>Ethical and legal implications: clear need for regulation</p>
<p>The answers in the first phase allow to discern five ethical implications, namely (1) the need for accountability in relation to the outcomes produced by LLMs, (2) the question of originality with regards to human creativity (e.g., concerns of plagiarism arise), (3) the sustainability issue regarding the environmental effects of LLMs, (4) the potential exclusion of researchers who lack access to LLMs, raising concerns about universalism, and (5) the issue of autonomy, in which researchers may become overly dependent on (commercial) AI tools. The comments show clearly that the majority deem ChatGPT unfit for authorship due to its inability to assume responsibility for the results.</p>
<p>The experts perceive legal implications regarding (1) copyright, due to the unclear infringement of intellectual property by LLMs, (2) data protection, due to the ambiguity of the data used and how OpenAI utilizes input data, and (3) liability, due to the uncertainty of the extent to which LLMs can be held responsible for criminal errors. A large majority of the experts (63.5%) believe that LLMs should be subject to stronger regulations (figure 2). 1</p>
<p>The initial round of the Delphi survey revealed that the ethical implications discussed frequently underscore the significance of the human element in scientific endeavors. This includes the responsibility and accountability of individuals for their contributions, the value of creativity and generating novel ideas, ensuring equitable access to science and the scientific community, and addressing the potential risk of dependency on LLM-based tools that may hinder individual skills and capabilities in scientific work. The amount of energy that is necessary both for training models and running inference and the CO2 footprint are mentioned as primary examples for the ecological sustainability issues ChatGPT and the like present. Taking into account that LLMs are trained on works produced by others and produce (or co-produce) works, both of which almost certainly fall under copyright law, it is not surprising that a large majority of the experts identify issues with copyright law as a pressing legal implication. The lack of transparency regarding the personal data on which the LLMs were trained, but also the further possible uses of personal data generated by the use of the tools, certainly explains why many respondents identify privacy and data protection law issues as considerable legal challenges. Whereas accountability is identified by many experts as a key ethical challenge, this does not carry over to the legal principle of liability that builds on it, which is mentioned by relatively few respondents.</p>
<p>Transformative and deformative scenario</p>
<p>In the first phase of the Delphi, we consulted with experts to ascertain the potential impact of LLMs on scientific practice within the next 5-10 years. In the subsequent phase, we investigated the potential influence of generative AI on the relationship between science and society. Based on the answers to these questions, our study reveals two possible scenarios, namely (1) a utopian transformative scenario and a (2) dystopian deformative scenario. It is noteworthy that the negative scenario is almost a negation of the positive scenario and vice versa. However, overall, there are significantly more indications (in terms of the number of codes) for a positive scenario, which was also confirmed by the opinion battery in Phase 2.</p>
<p>In the utopian scenario, integrating generative AI into scientific practices offers transformative potential, overcoming path dependencies in scientific practice and accelerating scientific and societal progress. Our analysis identifies three key aspects of its impact on science: (1) streamlining repetitive tasks, (2) promoting inclusivity, and (3) facilitating interdisciplinary research. The experts propose that generative AI could automate administrative and generic tasks, freeing up time for critical reflection, analysis and innovation. It may democratize access to scientific resources, foster diversity of voices and collaboration, and aid in discovering connections across different schools of thought. The integration of generative AI tools aligns research with societal challenges, driving technological development and supporting evidence-based decision-making. Effective science communication and education are enabled through AI-driven tools. This collaborative approach propels scientific advancements towards innovative solutions.</p>
<p>In a dystopian scenario, the anticipated positive impacts of generative AI are largely negated, as our analysis reveals three crucial aspects: (1) a decline in research quality due to plausible yet flawed results, compromising reliability and validity; (2) a loss of research diversity through amplifying mainstream voices, resulting in missed opportunities for novel perspectives; and (3) a decrease in scientific integrity, as the ease of producing AI-generated content raises risks of reinforcing predatory publishing practices and disseminating false information, leading to confusion and distrust. The perpetuation of plausible nonsense could further have negative consequences for society when policy decisions or public opinions rely on unreliable information. Additionally, dependence on commercial providers for generative AI tools raises concerns among experts about the lack of independence and control over scientific research, potentially leading to conflicts of interest and biases in research results. Furthermore, the loss of diversity in research and a decrease in scientific integrity perpetuate biases, eroding credibility and leading to conflicts of interest, ultimately distorting the pursuit of knowledge.</p>
<p>DISCUSSION &amp; CONCLUSION</p>
<p>The aim of this study was to investigate the impact of ChatGPT and other LLMs on the science system and scientific practices by examining their potential applications, limitations, effects, ethical and legal considerations and the necessary competencies needed by users. To date, scholars have primarily focused on the implications of LLMs on education (e.g. Perkins, 2023) with limited attention being paid to their impact on science and scientific practices (for exception, see Ribeiro et al., (2023). The overnight popularity ChatGPT experienced since its debut in November 2022 stressed even more the necessity to evaluate the implications of LLMs for science and scientific practice. To examine these implications, we employed a two-stage Delphi method, which included inviting experts, researchers working in the fields of science, technology and society to participate in two surveys as means to identify and refine the impact of LLMs on the science system and scientific practices.</p>
<p>At the time of the second round of our Delphi method, less than half a year had passed since the first preview of ChatGPT. Accordingly, it is difficult to make concrete predictions about the potential capabilities of future versions of LLMs like ChatGPT. Nevertheless, our study presents a consistent picture from experts which furthers our understanding of future expectations of LLMs. We were also able to identify patterns emerging regarding potential opportunities and risks. It is important to note the majority of the experts saw no danger that LLMs will replace the traditional scientist in the foreseeable future.</p>
<p>Overall, the experts in our study were optimistic and in agreement that the advantages of this technology outweigh their disadvantages. This optimism was paired with thoughtful concerns, which allow us to paint a nuanced picture of the potential positive and negative implications of LLMs. In general, ChatGPT and other LLMs were collectively understood as potential 'time-savers' to be used to improve and streamline the writing process, especially academic writing. For example, text improvement as in the rephrasing and optimization of textual content was considered the most important application. This outcome resonates with the scholarly discourse which highlights how generative AI can be used to enhance texts, such as with brainstorming (Staiman, 2023), crafting literature reviews , and improving text clarity (Lund et al., 2023). At the same time, experts in our study were aware of the limitations of LLMs and cited similar apprehensions to those raised in the literature (Dwivedi et al., 2023;OpenAI, 2022;Perkins, 2023). The experts highlighted key shortcomings such as AI produced texts may have incorrect information, their origin and referencing is non-transparent and that they lack specificity, shortcomings which are at odds with the principles of good scientific practice. It is not surprising that our study reinforced text-based applications and limitations for LLMs identified in the scholarly discourse, as text production is a key scientific practice. However, this focus may shift in the future as more usages of LLMs are explored.</p>
<p>In addition, our study indicates that LLMs have the potential to reshape the science system. The experts anticipate that they will lead to more efficient workflows, with the reduction of administrative tasks being ranked the highest anticipated change. This forecast supports claims made by other scholars, who argue that LLMs will help automate mundane tasks and free up space for creative thinking (Lund et al., 2023;Dwivedi et al., 2023). Other changes LLMs bring to the science system are however more complex. For example, our findings point to a double-edged sword embedded within the LLM constellation: this technology could serve to both promote inclusion and reinforce biases. On the one hand, LLMs can level the playing field for non-English speakers as they can provide editorial support, but on the other hand, they can also increase inequalities by drawing on mainstream opinions and widening the gap between those who have access to these technologies and those who do not. This multifaceted concern was also echoed by other scholars (Corless, 2023;Liebrenz et al., 2023).</p>
<p>The most pressing fear we identified is that LLMs perpetuate disinformation and will overburden quality assurance mechanisms in academia. In other words, LLMs will increase the sheer quantity of potentially incorrect papers and the peer-review process will simply be unable to keep up with the volume leading to a drop in quality. Similar thoughts are discussed by other scholars (Grimaldi &amp; Ehrler, 2023;Lund et al., 2023), with these changes being described in revolutionary terms in which LLMs are positioned as the great 'game-changers' of academia. In contrast, experts in our study were more cautious with such claims seeing these changes as more incremental and pragmatic.</p>
<p>Moreover, our study provided insights into the competencies researchers need to be able to utilize LLMs. In line with scholars such as Teubner et al. (2023), experts in our study voiced concerns that ChatGPT and other LLMs have the potential to widen the digital divide between researchers who possess technical know-how and researchers who do not. Furthermore, the experts pointed out that the researcher's role in the writing process will shift from being the originator of ideas and texts to being required to contextualize and reflect on AI generated results. This change will entail a new way of thinking about key scientific practices and the role the individual academic plays in them. Our experts also expressed the importance of researchers having an ethical understanding, e.g. using AI in a responsible manner. A point that was only marginally addressed in the literature (Dwivedi et al., 2023). Underlying these findings is the understanding that it is up to the individual academic to ensure that they have the skills and knowledge needed to navigate these technological changes. Such a stance can contribute to furthering digital divides due to preexisting uneven digital literacy between academics, institutions and higher education systems.</p>
<p>Our study aided in disentangling the ethical and legal implications of ChatGPT and other LLMs. The findings further articulate the issue of authorship when it comes to using AI, an issue discussed by other scholars Tomlinson et al., 2023). The majority of the experts deem that ChatGPT cannot claim authorship due to its inability to assume responsibility for its actions. In this light, the experts centred on distilling the role humans play in being accountable for their usage of LLMs, taking into consideration issues such as plagiarism, copyright and data protection. Thus, they underlined that human responsibility in AI usage is both a legal and ethical challenge, a sentiment that echoes the arguments of critics who postulate that chatbots cannot take responsibility for their actions (Corless, 2023;Liebrenz et al., 2023). In addition, the issue of access was highlighted as an ethical dilemma, that is, not all researchers will have equal access to such technologies, potentially furthering inequalities. Furthermore, the CO2 emissions generated by these use of AI technologies poses environmental risks (Hao, 2019). The complexities of these ethical and legal implications show the need to take diverse issues into account when it comes to regulating the usage of LLMs in academia.</p>
<p>Lastly, our study presents potential future pathways for AI and its impact on the science system and society in the form of future scenarios constructed from our data. In the positive transformative scenario, the integration of LLMs in scientific practice holds great potential for improving scientific productivity, efficiency, education, communication, creativity, and discovery. In other words, LLMs can automate repetitive tasks, allowing researchers to allocate more time and resources to analytical and innovative work. It is the prevailing perception of the experts that suggests that this scenario is more likely to occur. However, it is crucial to acknowledge the potential negative deformative scenario. Experts raised concerns about the impact of generative AI on scientific quality, integrity, and the scientific ecosystem. Issues such as decreased scientific rigor, reproducibility, and a potential homogenization of science were highlighted. In addition, the reliance on generative AI models without proper validation may lead to a decrease in critical thinking and creativity.</p>
<p>We can strive to ensure the positive scenario by addressing the concerns highlighted in our study. In this regard, striking a balance between embracing the benefits of LLMs and upholding scientific principles is crucial. Accordingly, we should remember our scientific tools, the good practices of scientific work, and create appropriate frameworks and conditions that enable us to make use of the diverse opportunities these technologies might have to offer. At the same time, we must withstand any attempt to compromise the quality standards that we as a science community have established and which distinguishes the scientific discourse. Researchers and policymakers should invest in transparency, accountability, and comprehensive validation processes to maintain the credibility of scientific research. Efforts should be made to address biases, ensure diversity, and guard against the potential misuse of generative AI. Additionally, ongoing training and education on the responsible use of LLMs are essential for scientists to adapt to the evolving scientific landscape.</p>
<p>In conclusion, while the transformative scenario holds great promise for the positive impact of LLMs on the science system and society, it is imperative to proactively address the potential risks and challenges to ensure that the integration of generative AI in science is guided by ethical considerations, scientific integrity, and a commitment to societal benefit.</p>
<p>ACKNOWLEDGEMENTS</p>
<p>We express our gratitude to the participating experts, the majority of whom have given their consent to be named in the second phase of the Delphi. These experts are Alexander Terenin, Alison Kennedy, Anaëlle Gonzalez, André Vellino, Andrea Klein, Benjamin Tan We also extend our thanks to the 9 experts who preferred to remain anonymous. We furthermore would like to extend our appreciation to our colleagues from the Global Network of Internet and Society Research Centers (https://networkofcenters.net/) for their invaluable assistance in the recruitment of experts and their insightful contributions in shaping initial ideas.</p>
<p>Technology Review. https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-asfive-cars-in-their-lifetimes/.</p>
<p>Writing with AI (arXiv:2305.03722). arXiv. http://arxiv.org/abs/2305.0372. Van Noorden, R. (2022). How language-generation AIs could transform science. Nature, 605(7908), 21-21.</p>
<p>https://doi.org/10.1038/d41586-022-01191-3. text translation statements about the application of a LLM as a translating device as well as their integration in existing applications for translation.</p>
<p>APPENDIX</p>
<p>""ChatGPT might be able to provide useful draft texts for non-essential writing, such as emails or invitation letters.</p>
<p>An additional option might be to let ChatGPT edit texts to improve readability. This could be helpful in various ways. For instance, it can easily put parts of texts in different tenses or voices or even different styles."</p>
<p>limitations: use in scientific work lack of transparency statements on the perpetuation or amplification of biases present in the training data and lack of transparency of LLMs as well as how they arrive at their outputs /. Relates to the generation of the output "The biggest danger is in using them to summarise informationdifficulties in their ability to attribute answers, risks of harm of incorrect information disseminating. Biases in training data probably strictly limit their perspectives. The cost in terms of dollars, and to the environment in terms of energy consumption also likely prohibit broader use (when not subsidised by tech companies eager for growth)" incorrectness statements about the outputs that are false or misleading based on data or language patterns that are outside of an LLM's training or understanding / Relates to the output.</p>
<p>"ChatGPT seems to produce plausible results, but at the moment in my experience it can mess up quite spectacularly while looking plausible"</p>
<p>non-creativity statements about the reliance on existing patterns and language structures in an LLM's training data and the difficulties to generate entirely new or creative content without significant human input or manipulation "Answers often depend significantly on asking the question in the "right" way; Since it is based on existing knowledge, it can only ever create something that has already been thought of" main codes definition example quote outdatedness statements about the inaccuracy of data over time as language patterns and cultural contexts evolve as well as (un)relevance and usefulness of outdated data.</p>
<p>"Could help in polishing an idea? However, this is limited as well as ChatGPT is not continuously updated as in Google search. Data for it is only until 2021!" unspecificity statements about LLMs producing vague or ambiguous outputs that lack specificity or clarity reflect the concern over instances where the generated content fails to provide precise or unambiguous information as well as statements about generic or generalized responses without addressing the specific nuances of a given query or context. reflection statements about the reflection of research methodologies and assumptions due to potential biases or gaps in existing data and analysis, identification of new research areas and directions based on the analysis of existing research and data as well as rethinking and renewing science due to the opportunities as well as the improvement of productivity and efficiency. productivity statements about the productivity of scientists due to automation of language-based tasks including literature reviews, data extraction, and scientific writing, enable time for critical thinking, analysis, and experimentation.</p>
<p>"Increase in productivity. But we need to use them correctly -their generated contents shouldn't be considered as truths. We shouldn't rely on these contents." risks for the science system overburden academic quality assurance mechanisms statements about the production of an overwhelming amount of scientific papers, data sets, and findings.</p>
<p>""Steigerung der produzierten Texte nicht wünschenswert, da bereits ohne ChatGPT die Menge und Breite der publizierten Texte in vielen Bereichen unüberschaubar ist." inequalities statements about the disproportionately benefit of scientists with better access to technology and resources, further widening the digital divide and exacerbating existing inequalities in the scientific community. ethical understanding statements about the ethical justifiability of using LLMs such as the perpetuation of biases, lack of transparency in decision-making processes, and the potential risk of unintended consequences, including amplifying existing inequalities or creating new ones.</p>
<p>"A legal problem with ChatGPT is the way that authorship and copyright are attributed. This holds for both, the texts that ChatGPT is trained on and the texts that are produced with the help of ChatGPT. A moral problem has to do with the ethics of research. Even before AI, there were many instances where researchers could choose to omit certain actions that are morally obligatory in science. Now that the production of text can be (partially) exported to an AI, it is easy to give in to the temptation of not caring too much whether the wording provided by some AI is accurate or not. Of course, the problem intensifies in a context where researchers are pressured to publish." reflective mindset statements of the need to understand the implications of the use of LLM for scientific work and the broader science system, including issues related to bias, transparency, accuracy, originality, productivity, and ethics, as well as consideration of how to mitigate potential risks and maximize the benefits of LLMs: What is the tool doing with us?  generate dependencies statements about the potential risk of LLMs creating a reliance on proprietary software or commercial (third-party) providers for access to LLM resources, tools, and expertise, which may limit academic freedom, restrict innovation, and raise ethical concerns related to ownership, control, and transparency of research outputs. decrease of quality statements about the potential negative impact on the quality of research or outputs resulting from overreliance on LLMs without proper scrutiny, which can lead to errors, biases, or other limitations in the data or analysis.</p>
<p>"There is a risk on lowering quality through ran increased redundant replication of existing work and ideas though."</p>
<p>homogenization of science statements about the potential risk of reducing diversity and creativity in scientific research or outputs, as reliance on LLMs may lead to the replication of existing knowledge and biases, mainstream voices, limiting the exploration of new and diverse research questions or approaches.</p>
<p>"They also draw on existing corpuses of knowledge which are biased towards certain perspectives. Bias is inevitably something scientists deal with on a regular basis, but without knowing the algorithms underlying AI models, it is much more difficult to assess bias." loss of intellectual ability to think statements about the potential risk of decreasing the intellectual independence and critical thinking skills of researchers or users who over-rely on LLMs, leading to a loss of creativity and originality in research or outputs "There could be the risk that researchers lose the ability to think about their results and claims made thoroughly and to construct new ways of argumentation and classifcation." negative impact on scientific integrity undermine academic training statements about the potential risk of replacing traditional academic training and critical thinking skills with an over-reliance on LLMs for tasks such as data analysis or interpretation, which can lead to a lack of understanding of underlying principles and limitations and undermine the development of essential academic and professional skills "Problematisch ist dabei, dass (die meisten) jungen Wissenschaftler:innen als Teil ihrer Ausbildung diesen repititiven [sic] Prozess brauchen, um Basiswissen parat zu haben und eben auch die Leistung der KI-generierten Texte einschätzen zu können ("repetitio est mater studiorum" -ist was dran)." loss in science credibility statements about the potential risk of reducing the credibility of scientific research or outputs, as over-reliance on LLMs without proper scrutiny or critical evaluation can lead to errors, biases, or inaccuracies, thereby diminishing the trust and confidence in scientific findings "A main risk that I see, however, is the potential of an increased generation and proliferation of misinformation. If AI tools are, e.g., abused to "mass-produce" papers of low quality or containing misinformation this could, ultimately, lead to a decreased trust in science."</p>
<p>spreading disinformation statements about the potential risk of unintentionally or intentionally spreading false or misleading information through the use of LLMs that may be trained on biased or inaccurate data, leading to the spread of disinformation and misinformation "On the other hand, I think AI can encourage scientists to be lazy, both in producing new ideas and in being critical of the ideas of others. It increases the risk of disinformation, since AI models can invent sources, as we have seen." transformation positive impact on scientific creativity and discovery:</p>
<p>augment science statements about the potential for LLMs to enhance and advance scientific research and outputs by enabling the processing and analysis of vast amounts of complex data, supporting the exploration of new research questions, and promoting greater efficiency and accuracy in scientific workflows.</p>
<p>"The most exciting change will be the ability to link ideas and research on a much greater scale than is currently possible. Ideally it will also speed up the time between experiment and publication and also reduce administration, allowing researchers to focus on science." positive impact on science education and communication:</p>
<p>Enhance science statements about the potential for LLMs to improve and strengthen scientific research and outputs by providing new insights and perspectives, facilitating interdisciplinary collaborations, supporting the development of innovative research methods, and advancing the overall understanding of complex phenomena in science and beyond "The positive potential of AI in academia is that it can be used to increase the efficiency of research and, ideally, also contribute to the quality of its products (publications, data, code/software). In an ideal scenario, this could also contribute to increasing the appreciation of as well as the trust in science."</p>
<p>innovate education statements about the potential for LLMs to transform and improve education by providing new opportunities for personalized learning, enabling the development of intelligent tutoring systems and educational chatbots, enhancing the accessibility of educational resources, and promoting greater engagement and motivation among students "I believe consequences will be majorly positive as it will make the process of learning much easier than before." positive impact on scientific productivity and efficiency streamline tasks statements about the potential benefit of LLMs in automating and optimizing certain tasks related to language processing, such as language translation, text summarization, or sentiment analysis, which may save time and resources, improve accuracy and consistency, and enhance the efficiency and effectiveness of various domains, such as healthcare, education, or customer service.</p>
<p>"In my opinion generative AI will have an effect on administrative and formal tasks by writing applications, assesments proposals, abstracts or status reports, basically everything which is highly standardized and therefore easy to replicate" assistance statements about the potential benefit of LLMs in providing intelligent and personalized assistance to users in various domains, such as language learning, writing, or content creation, by offering suggestions, corrections, or feedback, which may improve the quality and accessibility of education and communication, as well as enhance the user experience and satisfaction but also in other areas such as public health etc.</p>
<p>"Ich denke, dass man die Sprachmodelle weniger zur Generierung von Inhalten nutzen wird, sondern mehr als Suchmaschine, mit der man in Interaktion treten kann."</p>
<p>Figure 1 .
1Initial sample (round one) of our delphi study. Overview of participants by discipline and professional status (n=72).</p>
<p>, Brigitte Mathiak, Christian Gagné, Christian Vater, Daniel Guagnin, Debora Weber-Wulff, Ekaterina Hertog, Eva Seidlmayer, Evgeny Bobrov, Fabro Steibel, Fiona Kinniburgh, Florian Hoffmann, Georg von Richthofen, Graham Taylor, Hadi Asghari, Hendrik Send, Ingrid Richardson, Johannes Breuer, Katharina Mosene, Klaus Gasteier, Marina Gavrilova, Mark Spektor, Martin Schmidt, Maximlian Heimstädt, Mike Thelwall, Naireet Gosh, Natalie Sontopski, Philipp Mehl, Richard Boire, Robert Lepenies, Ronny Röwert, Sebastian Moraga Scheuermann, Thorsten Thiel, Tony Ross-Hellauer, Vince I. Madai, Vincent Traag, Wojciech Hardy, Zining Zhu.</p>
<p>Table 1 .
1OpinionsStrongly 
disagree </p>
<p>Disagree Neither 
agree 
nor 
disagree </p>
<p>Agree 
Strongly 
agree </p>
<p>"Motivation for reflecting on what researchers actually do" ability to contextualize results statements about the scientist's ability to apply and embed the results of LLMs into the specific context of their research or scientific work, ensuring that the generated insights are relevant and meaningful for their particular field or domain: What are we doing with the tool and output? "Eingabekompetenz, d.h. die Fähigkeit "prompts" so zu formulieren, dass die Antwort möglichst genau/differenziert ist. Bewertungskompetenz, d.h. Kontextualisierung der Ergebnisse." "Copyright might become an issue if the model does not alter its source material in a sufficient manner; There is an incentive for basically everyone to try and find out how far they can take the technology, increasing the likelihood of fraudulent behaviour; Referencing is usually not happening within ChatGPT, so it does not cite the sources it makes use of, which is at least problematic and at worst could result in plagiarism;" "I see huge problem with copyright and data protection because OpenAI has not made it transparent where the data is coming from, how much data was used and if the rightful owners of the data agreed to its use in training ChatGPT. This is especially a problem for already marginalzed groups. Also there is the risk that bias that already exists in the data used to train ChatGPT is reproduced in the system and goes unchecked."statements about the accountability for the results obtained from using LLMs including to ensure critical analysis or evaluation of outputs regarding ethical implications."-Accountability is a big issue if ChatGPT is accepted as a coauthor. Are machine can hardly be held responsible.-Chatbots may spread stereotypes and biases.""The main problems that I see are the provision of incorrect answers by LLMs, and the risks associated with relying on commercial products and intransparent algorithms. Other issues include the question of costs and the consequences of unnecessary use of ChatGPT or other similar tools with regard to energy consumption."Fairness: who has access (small universities in the global south having a different acces compare to Google researchers?). Also: will it be available in many languages or will it just enforce English? Autorship, copyright: these are legal questions. My opinion is that these two point require a human, with moral and legal agency (i.e. legal responsibility).legal implications 
for scientists 
when using LLMs 
main codes 
definition 
example quote </p>
<p>copyright </p>
<p>statements about outputs that infringe 
copyright laws or intellectual property 
rights, usage of copyrighted or 
proprietary data without proper 
authorization or reference. </p>
<p>data protection </p>
<p>statements about complying to privacy 
laws, such as ensuring the 
confidentiality and protection of 
personal data used to feed models, and 
compliance with regulations when it 
comes to sensitive personal data. </p>
<p>liability </p>
<p>statements about a Scientist's potential 
liability for any unintended 
consequences or errors that may arise 
from the use of LLM's. </p>
<p>"[Verbreitung] (Rassistische[r] und sexistische[r] Ergebnisse)" </p>
<p>ethical 
implication for 
scientists when 
using LLMs </p>
<p>need for 
accountability in 
relation to the 
outcomes 
produced by LLMs </p>
<p>originality with 
regards to human 
creativity </p>
<p>statements about authorship, credit, 
plagiarism regarding the output 
generated by LLMs. </p>
<p>"-plagiarism 
-definition of authorship 
-legal: author(ity)" </p>
<p>sustainability </p>
<p>Statement about an LLM's 
environmental impact from their 
development to usage, such as high 
computational resources that result in 
carbon footprint and energy 
consumption. </p>
<p>potential exclusion 
of researchers </p>
<p>statements about the implications of 
potential for unequal access and high 
costs limiting the ability of some 
scientists to use these tools and 
contribute to scientific knowledge. </p>
<p>Table 8 .
8Codebook Phase2 statements about the potential risk of LLMs replacing or reducing the need for certain support posts or administrative jobs that involve tasks such as data entry, transcription, or document analysis "Daraus folgend werden ggf. weitere Berufsgruppen (Sekretariat, Verwaltungen, Werbetexter, Callcenter/Kundenservice, etc. ) weiter automatisiert und entsprechende Jobs verschwinden. Diese Freisetzung von Menschen wird (hoffentlich) zu einer Einführung des Grundeinkommens führen :)"reinforce inequalities statements about the potential risk of LLMs perpetuating or amplifying limiting access to LLM resources or knowledge, thereby exacerbating the digital divide and marginalizing disadvantaged or underrepresented groups "On the negative side, society will generally become more competitive and probably add a massive gap between those who know how to use Ai and those who don't."scenario 
main Codes 
definition 
example quote </p>
<p>statements about the potential risk of LLMs being used to generate low-quality or fraudulent content, which may be published in predatory journals for financial gain, thereby compromising the integrity and credibility of scientific research and contributing to the spread of misinformation."Abhängigkeiten von kommerziellen 
Anbietern" </p>
<p>negative impact 
on scientific 
quality </p>
<p>reinforce predatory 
publishing </p>
<p>"The biggest potential problem I see is 
that if there's a greater deluge of 
predatory publishing practices being 
driven by LLM" </p>
<p>We did not ask the participants to rank the ethical and legal implications in the second round of the Delphi survey.
main codes definition example quoteData protection is interesting. Many people will not think twice and upload all their data to an LLM, asking for "write me a paper", despide any confidentiality or ethical constraints linked to the data.issue of autonomy statements about dependencies of scientists. no significant change statements about the science system remaining unchanged "ChatGPT is like another programming language. I don't think the scientific practices will be fundamentally changed by another programming language."
Analysing qualitative data: More than 'identifying themes. P Bazeley, Malaysian Journal of Qualitative Research. 22Bazeley, P. (2009). Analysing qualitative data: More than 'identifying themes.' Malaysian Journal of Qualitative Research, 2(2), 6-22.</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and TransparencyBender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? . Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610-623. https://doi.org/10.1145/3442188.3445922.</p>
<p>Academic Writing with GPT-3. O Buruk, Oz, arXiv:2304.11079Reflections on Practices, Efficacy and Transparency. 5Buruk, O. "Oz." (2023). Academic Writing with GPT-3.5: Reflections on Practices, Efficacy and Transparency (arXiv:2304.11079). arXiv. http://arxiv.org/abs/2304.11079.</p>
<p>ChatGPT is making waves in the scientific literature. V Corless, Advanced Science News. Corless, V. (2023, February 21). ChatGPT is making waves in the scientific literature. Advanced Science News. https://www.advancedsciencenews.com/where-and-how-should-chatgpt-be-used-in-the-scientific-literature/.</p>
<p>ChatGPT for (Finance) research: The Bananarama Conjecture. M Dowling, B Lucey, 10.1016/j.frl.2023.103662Finance Research Letters. 53103662Dowling, M., &amp; Lucey, B. (2023). ChatGPT for (Finance) research: The Bananarama Conjecture. Finance Research Letters, 53, 103662. https://doi.org/10.1016/j.frl.2023.103662.</p>
<p>So what if ChatGPT wrote it?" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. Y K Dwivedi, N Kshetri, L Hughes, E L Slade, A Jeyaraj, A K Kar, A M Baabdullah, A Koohang, V Raghavan, M Ahuja, H Albanna, M A Albashrawi, A S Al-Busaidi, J Balakrishnan, Y Barlette, S Basu, I Bose, L Brooks, D Buhalis, R Wright, 10.1016/j.ijinfomgt.2023.102642International Journal of Information Management. 71102642Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Baabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Albashrawi, M. A., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., Buhalis, D., … Wright, R. (2023). "So what if ChatGPT wrote it?" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, 102642. https://doi.org/10.1016/j.ijinfomgt.2023.102642.</p>
<p>Nonhuman "Authors" and Implications for the Integrity of Scientific Publication and Medical Knowledge. A Flanagin, K Bibbins-Domingo, M Berkwits, S L Christiansen, 10.1001/jama.2023.1344JAMA. 3298Flanagin, A., Bibbins-Domingo, K., Berkwits, M., &amp; Christiansen, S. L. (2023). Nonhuman "Authors" and Implications for the Integrity of Scientific Publication and Medical Knowledge. JAMA, 329(8), 637. https://doi.org/10.1001/jama.2023.1344.</p>
<p>GPT-3: Its Nature, Scope, Limits, and Consequences. Minds and Machines. L Floridi, M Chiriatti, 10.1007/s11023-020-09548-130Floridi, L., &amp; Chiriatti, M. (2020). GPT-3: Its Nature, Scope, Limits, and Consequences. Minds and Machines, 30(4), 681-694. https://doi.org/10.1007/s11023-020-09548-1.</p>
<p>European data regulators set up ChatGPT task force -POLITICO. C Goujard, Goujard, C. (2023, April 13). European data regulators set up ChatGPT task force -POLITICO. Politico. https://www.politico.eu/article/european-data-regulators-set-up-chatgpt-taskforce/.</p>
<p>Intelligenza artificiale: Il Garante blocca ChatGPT. Raccolta illecita di dati personali. Assenza di sistemi per la verifica dell' età dei minori. Gpdp, GPDP. (2023, March 31). Intelligenza artificiale: Il Garante blocca ChatGPT. Raccolta illecita di dati personali. Assenza di sistemi per la verifica dell' età dei minori. https://www.garanteprivacy.it:443/home/docweb/-/docweb-display/docweb/9870847.</p>
<p>Machines Are About to Change Scientific Publishing Forever. G Grimaldi, B Ehrler, Ai, 10.1021/acsenergylett.2c02828ACS Energy Letters. 81Grimaldi, G., &amp; Ehrler, B. (2023). AI et al.: Machines Are About to Change Scientific Publishing Forever. ACS Energy Letters, 8(1), 878-880. https://doi.org/10.1021/acsenergylett.2c02828.</p>
<p>Regulating ChatGPT and other Large Generative AI Models. P Hacker, A Engel, M Mauer, 10.48550/ARXIV.2302.02337Hacker, P., Engel, A., &amp; Mauer, M. (2023). Regulating ChatGPT and other Large Generative AI Models. https://doi.org/10.48550/ARXIV.2302.02337.</p>
<p>Training a single AI model can emit as much carbon as five cars in their lifetimes. K Hao, N Helberger, N Diakopoulos, 10.14763/2023.1.1682Internet Policy Review. 112ChatGPT and the AI ActHao, K. (2019, June 6). Training a single AI model can emit as much carbon as five cars in their lifetimes. MIT Helberger, N., &amp; Diakopoulos, N. (2023). ChatGPT and the AI Act. Internet Policy Review, 12(1). https://doi.org/10.14763/2023.1.1682.</p>
<p>Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other Large Language Models in scholarly peer review. M Hosseini, S P J M Horbach, 10.21203/rs.3.rs-2587766/v1Review. PreprintHosseini, M., &amp; Horbach, S. P. J. M. (2023). Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other Large Language Models in scholarly peer review [Preprint]. In Review. https://doi.org/10.21203/rs.3.rs-2587766/v1.</p>
<p>Using AI to write scholarly publications. M Hosseini, L M Rasmussen, D B Resnik, 10.1080/08989621.2023.2168535Accountability in Research. Hosseini, M., Rasmussen, L. M., &amp; Resnik, D. B. (2023). Using AI to write scholarly publications. Accountability in Research, 1-9. https://doi.org/10.1080/08989621.2023.2168535.</p>
<p>How does Artificial Intelligence Transform Knowledge Work?. M Jiang, C Breidbach, S Karanasios, PACIS 2022 Proceedings. 312Jiang, M., Breidbach, C., &amp; Karanasios, S. (2022). How does Artificial Intelligence Transform Knowledge Work? PACIS 2022 Proceedings, 312. https://aisel.aisnet.org/pacis2022/312.</p>
<p>S A Khowaja, P Khuwaja, K Dev, arXiv:2305.03123ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. Khowaja, S. A., Khuwaja, P., &amp; Dev, K. (2023). ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review (arXiv:2305.03123). arXiv. http://arxiv.org/abs/2305.03123.</p>
<p>A Watermark for Large Language Models. J Kirchenbauer, J Geiping, Y Wen, J Katz, I Miers, T Goldstein, 10.48550/ARXIV.2301.10226Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., &amp; Goldstein, T. (2023). A Watermark for Large Language Models. https://doi.org/10.48550/ARXIV.2301.10226.</p>
<p>Current validity of the Delphi method in social sciences. J Landeta, 10.1016/j.techfore.2005.09.002Technological Forecasting and Social Change. 735Landeta, J. (2006). Current validity of the Delphi method in social sciences. Technological Forecasting and Social Change, 73(5), 467-482. https://doi.org/10.1016/j.techfore.2005.09.002.</p>
<p>W Liang, M Yuksekgonul, Y Mao, E Wu, J Zou, arXiv:2304.02819GPT detectors are biased against non-native English writers. Liang, W., Yuksekgonul, M., Mao, Y., Wu, E., &amp; Zou, J. (2023). GPT detectors are biased against non-native English writers (arXiv:2304.02819). arXiv. http://arxiv.org/abs/2304.02819.</p>
<p>Generating scholarly content with ChatGPT: Ethical challenges for medical publishing. M Liebrenz, R Schleifer, A Buadze, D Bhugra, A Smith, 10.1016/S2589-7500(23)00019-5The Lancet Digital Health. 53Liebrenz, M., Schleifer, R., Buadze, A., Bhugra, D., &amp; Smith, A. (2023). Generating scholarly content with ChatGPT: Ethical challenges for medical publishing. The Lancet Digital Health, 5(3), e105-e106. https://doi.org/10.1016/S2589-7500(23)00019-5.</p>
<p>The delphi method. H A Linstone, M Turoff, Addison-Wesley Reading, MALinstone, H. A., &amp; Turoff, M. (1975). The delphi method. Addison-Wesley Reading, MA.</p>
<p>ChatGPT: our study shows AI can produce academic papers good enough for journals -just as some ban it. The Conversation. B Lucey, M Dowling, Lucey, B., &amp; Dowling, M. (2023, January 26). ChatGPT: our study shows AI can produce academic papers good enough for journals -just as some ban it. The Conversation. https://theconversation.com/chatgpt-our-study-shows-ai-can-produce-academic-papers-good-enough-for-journals -just-as-some-ban-it-197762.</p>
<p>ChatGPT and a new academic reality: Artificial Intelligence-written research papers and the ethics of the large language models in scholarly publishing. B D Lund, T Wang, N R Mannuru, B Nie, S Shimray, Z Wang, 10.1002/asi.24750asi.24750Journal of the Association for Information Science and Technology. Lund, B. D., Wang, T., Mannuru, N. R., Nie, B., Shimray, S., &amp; Wang, Z. (2023). ChatGPT and a new academic reality: Artificial Intelligence-written research papers and the ethics of the large language models in scholarly publishing. Journal of the Association for Information Science and Technology, asi.24750. https://doi.org/10.1002/asi.24750.</p>
<p>Introducing ChatGPT. Openai, OpenAI. (2022, November 30). Introducing ChatGPT. https://openai.com/blog/chatgpt.</p>
<p>How Nature readers are using ChatGPT. B Owens, 10.1038/d41586-023-00500-8Nature. 7950Owens, B. (2023). How Nature readers are using ChatGPT. Nature, 615(7950), 20-20. https://doi.org/10.1038/d41586-023-00500-8.</p>
<p>Academic integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond. M Perkins, 10.53761/1.20.02.07Journal of University Teaching and Learning Practice. 202Perkins, M. (2023). Academic integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond. Journal of University Teaching and Learning Practice, 20(2). https://doi.org/10.53761/1.20.02.07.</p>
<p>A publishing infrastructure for AI-assisted academic authoring. M Pividori, C S Greene, 10.1101/2023.01.21.525030Pividori, M., &amp; Greene, C. S. (2023). A publishing infrastructure for AI-assisted academic authoring [Preprint]. Scientific Communication and Education. https://doi.org/10.1101/2023.01.21.525030.</p>
<p>The digitalisation paradox of everyday scientific labour: How mundane knowledge work is amplified and diversified in the biosciences. B Ribeiro, R Meckin, A Balmer, P Shapira, 10.1016/j.respol.2022.104607Research Policy. 521Ribeiro, B., Meckin, R., Balmer, A., &amp; Shapira, P. (2023). The digitalisation paradox of everyday scientific labour: How mundane knowledge work is amplified and diversified in the biosciences. Research Policy, 52(1), 104607. https://doi.org/10.1016/j.respol.2022.104607.</p>
<p>ChatGPT: Deutschlands Datenschützer eröffnen Verfahren gegen OpenAI | heise online. D A Sokolov, Sokolov, D. A. (2023, April 20). ChatGPT: Deutschlands Datenschützer eröffnen Verfahren gegen OpenAI | heise online. Heise Online. https://heise.de/-8974708.</p>
<p>Guest Post-Academic Publishers Are Missing the Point on ChatGPT. A Staiman, Staiman, A. (2023, March 31). Guest Post-Academic Publishers Are Missing the Point on ChatGPT. The Scholarly Kitchen. https://scholarlykitchen.sspnet.org/2023/03/31/guest-post-academic-publishers-are-missing-the-point-on-chatgp t/.</p>
<p>Welcome to the Era of ChatGPT et al.: The Prospects of Large Language Models. T Teubner, C M Flath, C Weinhardt, W Van Der Aalst, O Hinz, 10.1007/s12599-023-00795-xBusiness &amp; Information Systems Engineering. 652Teubner, T., Flath, C. M., Weinhardt, C., Van Der Aalst, W., &amp; Hinz, O. (2023). Welcome to the Era of ChatGPT et al.: The Prospects of Large Language Models. Business &amp; Information Systems Engineering, 65(2), 95-101. https://doi.org/10.1007/s12599-023-00795-x.</p>
<p>B Tomlinson, A W Torrance, R W Black, ChatGPT and Works Scholarly: Best Practices and Legal Pitfalls. Tomlinson, B., Torrance, A. W., &amp; Black, R. W. (2023). ChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in</p>            </div>
        </div>

    </div>
</body>
</html>
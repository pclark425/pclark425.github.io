<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-195 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-195</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-195</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-5b70e69b65b29d231d37bea354b25c05daec07e2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5b70e69b65b29d231d37bea354b25c05daec07e2" target="_blank">Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A new calibration study is presented, where models are discouraged from presenting any single answer when presented with multiple conflicting answer candidates in retrieved evidences, and it is discovered that contradictions among knowledge sources affect model confidence only marginally.</p>
                <p><strong>Paper Abstract:</strong> Question answering models can use rich knowledge sources — up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend information stored in their LM parameters with that from retrieved evidence documents. In this paper, we simulate knowledge conflicts (i.e., where parametric knowledge suggests one answer and different passages suggest different answers) and examine model behaviors. We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledgein their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally. To address this issue, we present a new calibration study, where models are discouraged from presenting any single answer when presented with multiple conflicting answer candidates in retrieved evidences.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e195.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e195.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD_high_recall_evidence_dominance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fusion-in-Decoder grounding with high-recall retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finding that the FiD retrieval-based generative reader (T5-large) predominantly copies answers from retrieved passages when the retriever has high answer recall, showing little memorization in parametric weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fusion-in-Decoder (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-retrieval extractive/generative question answering (short entity-span answers).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Multiple retrieved passages (up to 100); non-parametric retrieved documents</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia passages retrieved by DPR (dense retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>aligned/mixed (when retrieved passages contain the gold answer the evidence aligns; otherwise mixed/contradictory)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book (CBQA) T5-large often gives different answers: over 70% of examples have different answers from CBQA, indicating heavy change when evidence is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>When retriever success is high (e.g., answer recall ~88%), FiD copies answer spans from retrieved passages in ≈96.6% of instances overall (extractive behavior); FiD generates novel spans only ~3.4% (NQ) / 6.2% (TQA). (See Tables 2, 4, 5.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive (adding correct retrieved evidence overwhelmingly causes FiD to ground answers in evidence, greatly reducing reliance on parametric memorized answers).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Multi-pass cross-attention reader architecture enables FiD to attend to and copy from retrieved passages; high-recall retrieval ensures the correct span appears in evidence so the model relies on non-parametric information rather than parametric memory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When paired with a high-recall retriever, FiD behaves largely as an extractive model: it almost always produces answers present in the retrieved passages (very low novel-span generation); memorization from parametric weights is rare (<~3-9% depending on split) when multiple passages are available during training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e195.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e195.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Partial_Entity_Substitution</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Partial entity-substitution (mixed-bag) experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments where a subset of retrieved passages have the answer entity substituted (simulating conflicting evidence); measures how often the model still outputs the original answer and how confidence changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fusion-in-Decoder (FiD); comparisons with RAG</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M (FiD reader)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-retrieval QA under mixed/conflicting retrieved evidence (partial substitution of answer entities in some passages).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Contradictory retrieved documents (partial entity substitutions), sampling strategies: random / top-retrieval / top-attention</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Synthetic perturbations applied to retrieved passages after retrieval (random substitutes drawn from training entities or realistic alternatives from AmbigQA / SituatedQA)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (retrieved passages can contradict the model's parametric knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>When no substitution (original evidence), model predicts original gold answer nearly 100% for the filtered set (by construction in this evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With 50% of answer-containing passages randomly substituted: FiD still predicts the original answer ~52% of the time vs substitute ~25% on NQ; on TriviaQA it predicts original ~59% vs substitute ~15% (text reports these empirical rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed — adding conflicting evidence sometimes shifts answers away from the original (positive effect) but frequently does not; overall, adding substituted (contradictory) evidence only partially reduces selection of the original answer and does not reliably reduce model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Model tends to focus on a small set of high-scoring passages and uses parametric knowledge to break ties when multiple plausible answers appear; lower-scoring supporting passages are often effectively ignored.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Partial substitution reveals that FiD often still prefers the original (pre-substitution) answer when only a portion of passages are contradicted — e.g., at 50% random substitution the original answer remains the mode on many examples; perturbing top-ranked (by retrieval or attention) passages is much more effective at changing predictions than perturbing random passages.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e195.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e195.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top_passage_dependency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dependency on a small number of top passages (attention-weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finding that FiD disproportionately relies on a few top-ranked passages (by attention or retrieval score); keeping a small number of unperturbed top passages preserves original-answer prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fusion-in-Decoder (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-retrieval QA; ablation where only top-k passages retain the original answer and all other passages are substituted</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Retrieved passages with selective perturbation (all except top-k changed)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Retrieved Wikipedia passages; top-k selected by reader cross-attention score</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed/contradictory in partial sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>If only low-ranked passages contain the original answer (top-k removed), model more often switches away from original.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>When keeping only top-1 unperturbed passage: original predicted ≈41.33% vs substitute ≈33.11%; top-3 kept: original ≈69.15% vs substitute ≈12.38%; top-5 kept: original ≈78.50% vs substitute ≈5.97% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>neutral-to-negative for diversified evidence: adding many additional passages that support an alternative answer has little effect if top-k passages still support the original — the model behaves as though only the top few passages matter.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Decoder cross-attention (and learned attention weights) concentrates influence on a handful of passages, producing effective 'winner-take-most' behavior; passages with low attention are largely ignored.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>With up to 3 unperturbed top passages, FiD often continues to output the original answer (≈70% for k=3), showing strong dependence on a small set of high-attention passages rather than aggregating across all retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e195.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e195.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic_Perturbation_Robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial semantic perturbation (negation, modality, future, infilling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rule-based semantic edits to answer-containing sentences (negation, future tense, modality, text infilling) frequently fail to dissuade models from returning the original answer; confidence/calibration scores are only partially sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fusion-in-Decoder (FiD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-retrieval QA robustness to adversarial semantic changes in evidence passages (should invalidate the gold answer)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Semantically-contradictory retrieved passages (negation, future-tense, modal insertion, blank infilling)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Synthetic rule-based edits applied to retrieved evidence sentences (dependency-based transformations and infilling)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory (perturbations are intended to invalidate evidence relative to original parametric answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Before perturbation, FiD correctly predicts the original answer (examples filtered to those FiD initially answered correctly).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>After semantic perturbations FiD often still outputs the original answer; large fraction of examples still return original answer (Table 8 and Appendix). Calibration/probability scores decreased only for a minority of examples (paper: 'only for 30–40% of examples we see a decrease in calibration score'); in Appendix reported per-perturbation percentages of examples where confidence dropped (e.g., Table 14 shows per-perturbation Gen.Prob drops: negation 65.94%, modality 62.75%, future 58.87%, text-infilling 60.56% — counts depend on subset and coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed/negative — semantic contradictions in supporting passages often fail to prevent the model from returning the original (now unsupported) answer; confidence measures do not reliably reflect the invalidation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Model behaves like extractive systems: it tends to return answers that match the answer-type and previously attended spans, and its token-generation probability / calibrator score is insufficiently sensitive to semantic edits; attention and answer-type priors dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic perturbations that should invalidate an answer frequently do not cause FiD to change its predicted span, and calibration/generation-probability metrics are only partially sensitive (decrease on a minority or an inconsistent fraction of examples); the model returns apparently 'confident' but unsupported answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e195.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e195.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration_and_Abstention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Separately trained calibrator for abstention and conflict detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An XGBoost-based calibrator trained on generation probability plus encoder features can learn to abstain when evidence contains conflicting answer candidates, but generalization across different conflict-collection methods is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Separate calibrator (XGBoost) over FiD features</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selective question answering / abstention: detect instances where model should refrain because evidence is conflicting</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Mixed/conflicting retrieved passages (simulated partial substitution, AmbigQA-derived alternatives, SituatedQA temporal alternatives)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Augmented training sets constructed by perturbation methods and by retrieving alternative-answer passages from AmbigQA / SituatedQA</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (conflicting evidence vs parametric priors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Vanilla model generation-probability (no calibrator) yields baseline binary calibration accuracy = 66.32% on original NQ dev (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Calibrators trained on matched augmented datasets can substantially improve binary abstention/accuracy on that evaluation set (e.g., Partial-Substitutions trained calibrator achieves 98.60% on Partial Sub evaluation; combined training improves macro average to 72.46%), but these calibrators do not generalize reliably to other conflict types (see Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive (a trained calibrator can lower the model's tendency to present a single answer when it sees conflicting evidence), but limited generalization means effect is dataset/method dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Train a separate classifier on features (generation probability + averaged encoder representations) to predict correctness/need-to-abstain; data augmentation with realistic conflict types is required to teach the calibrator to detect specific conflict patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A standalone calibrator can learn to abstain on simulated or matched conflict sets, improving selective-answering accuracy on those sets, but calibration trained on one conflict-collection method often fails to generalize to other real-world conflict types (AmbigQA, SituatedQA), so recalibration is a partial solution requiring diverse augmented training data.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entity-based knowledge conflicts in question answering <em>(Rating: 2)</em></li>
                <li>Knowing more about questions can help: Improving calibration in question answering <em>(Rating: 2)</em></li>
                <li>Beyond accuracy: Behavioral testing of NLP models with checklist <em>(Rating: 1)</em></li>
                <li>ContraQA: Question answering under contradicting contexts <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-195",
    "paper_id": "paper-5b70e69b65b29d231d37bea354b25c05daec07e2",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "FiD_high_recall_evidence_dominance",
            "name_full": "Fusion-in-Decoder grounding with high-recall retriever",
            "brief_description": "Finding that the FiD retrieval-based generative reader (T5-large) predominantly copies answers from retrieved passages when the retriever has high answer recall, showing little memorization in parametric weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fusion-in-Decoder (FiD)",
            "model_size": "770M",
            "task_description": "Open-retrieval extractive/generative question answering (short entity-span answers).",
            "evidence_type": "Multiple retrieved passages (up to 100); non-parametric retrieved documents",
            "evidence_source": "Wikipedia passages retrieved by DPR (dense retriever)",
            "parametric_knowledge_alignment": "aligned/mixed (when retrieved passages contain the gold answer the evidence aligns; otherwise mixed/contradictory)",
            "performance_without_evidence": "Closed-book (CBQA) T5-large often gives different answers: over 70% of examples have different answers from CBQA, indicating heavy change when evidence is provided.",
            "performance_with_evidence": "When retriever success is high (e.g., answer recall ~88%), FiD copies answer spans from retrieved passages in ≈96.6% of instances overall (extractive behavior); FiD generates novel spans only ~3.4% (NQ) / 6.2% (TQA). (See Tables 2, 4, 5.)",
            "evidence_effect": "positive (adding correct retrieved evidence overwhelmingly causes FiD to ground answers in evidence, greatly reducing reliance on parametric memorized answers).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Multi-pass cross-attention reader architecture enables FiD to attend to and copy from retrieved passages; high-recall retrieval ensures the correct span appears in evidence so the model relies on non-parametric information rather than parametric memory.",
            "key_findings": "When paired with a high-recall retriever, FiD behaves largely as an extractive model: it almost always produces answers present in the retrieved passages (very low novel-span generation); memorization from parametric weights is rare (&lt;~3-9% depending on split) when multiple passages are available during training and inference.",
            "counterintuitive_behavior": false,
            "uuid": "e195.0",
            "source_info": {
                "paper_title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Partial_Entity_Substitution",
            "name_full": "Partial entity-substitution (mixed-bag) experiments",
            "brief_description": "Experiments where a subset of retrieved passages have the answer entity substituted (simulating conflicting evidence); measures how often the model still outputs the original answer and how confidence changes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fusion-in-Decoder (FiD); comparisons with RAG",
            "model_size": "770M (FiD reader)",
            "task_description": "Open-retrieval QA under mixed/conflicting retrieved evidence (partial substitution of answer entities in some passages).",
            "evidence_type": "Contradictory retrieved documents (partial entity substitutions), sampling strategies: random / top-retrieval / top-attention",
            "evidence_source": "Synthetic perturbations applied to retrieved passages after retrieval (random substitutes drawn from training entities or realistic alternatives from AmbigQA / SituatedQA)",
            "parametric_knowledge_alignment": "mixed (retrieved passages can contradict the model's parametric knowledge)",
            "performance_without_evidence": "When no substitution (original evidence), model predicts original gold answer nearly 100% for the filtered set (by construction in this evaluation).",
            "performance_with_evidence": "With 50% of answer-containing passages randomly substituted: FiD still predicts the original answer ~52% of the time vs substitute ~25% on NQ; on TriviaQA it predicts original ~59% vs substitute ~15% (text reports these empirical rates).",
            "evidence_effect": "mixed — adding conflicting evidence sometimes shifts answers away from the original (positive effect) but frequently does not; overall, adding substituted (contradictory) evidence only partially reduces selection of the original answer and does not reliably reduce model confidence.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Model tends to focus on a small set of high-scoring passages and uses parametric knowledge to break ties when multiple plausible answers appear; lower-scoring supporting passages are often effectively ignored.",
            "key_findings": "Partial substitution reveals that FiD often still prefers the original (pre-substitution) answer when only a portion of passages are contradicted — e.g., at 50% random substitution the original answer remains the mode on many examples; perturbing top-ranked (by retrieval or attention) passages is much more effective at changing predictions than perturbing random passages.",
            "counterintuitive_behavior": true,
            "uuid": "e195.1",
            "source_info": {
                "paper_title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Top_passage_dependency",
            "name_full": "Dependency on a small number of top passages (attention-weighted)",
            "brief_description": "Finding that FiD disproportionately relies on a few top-ranked passages (by attention or retrieval score); keeping a small number of unperturbed top passages preserves original-answer prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fusion-in-Decoder (FiD)",
            "model_size": "770M",
            "task_description": "Open-retrieval QA; ablation where only top-k passages retain the original answer and all other passages are substituted",
            "evidence_type": "Retrieved passages with selective perturbation (all except top-k changed)",
            "evidence_source": "Retrieved Wikipedia passages; top-k selected by reader cross-attention score",
            "parametric_knowledge_alignment": "mixed/contradictory in partial sets",
            "performance_without_evidence": "If only low-ranked passages contain the original answer (top-k removed), model more often switches away from original.",
            "performance_with_evidence": "When keeping only top-1 unperturbed passage: original predicted ≈41.33% vs substitute ≈33.11%; top-3 kept: original ≈69.15% vs substitute ≈12.38%; top-5 kept: original ≈78.50% vs substitute ≈5.97% (Table 7).",
            "evidence_effect": "neutral-to-negative for diversified evidence: adding many additional passages that support an alternative answer has little effect if top-k passages still support the original — the model behaves as though only the top few passages matter.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Decoder cross-attention (and learned attention weights) concentrates influence on a handful of passages, producing effective 'winner-take-most' behavior; passages with low attention are largely ignored.",
            "key_findings": "With up to 3 unperturbed top passages, FiD often continues to output the original answer (≈70% for k=3), showing strong dependence on a small set of high-attention passages rather than aggregating across all retrieved documents.",
            "counterintuitive_behavior": true,
            "uuid": "e195.2",
            "source_info": {
                "paper_title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Semantic_Perturbation_Robustness",
            "name_full": "Adversarial semantic perturbation (negation, modality, future, infilling)",
            "brief_description": "Rule-based semantic edits to answer-containing sentences (negation, future tense, modality, text infilling) frequently fail to dissuade models from returning the original answer; confidence/calibration scores are only partially sensitive.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fusion-in-Decoder (FiD)",
            "model_size": "770M",
            "task_description": "Open-retrieval QA robustness to adversarial semantic changes in evidence passages (should invalidate the gold answer)",
            "evidence_type": "Semantically-contradictory retrieved passages (negation, future-tense, modal insertion, blank infilling)",
            "evidence_source": "Synthetic rule-based edits applied to retrieved evidence sentences (dependency-based transformations and infilling)",
            "parametric_knowledge_alignment": "contradictory (perturbations are intended to invalidate evidence relative to original parametric answer)",
            "performance_without_evidence": "Before perturbation, FiD correctly predicts the original answer (examples filtered to those FiD initially answered correctly).",
            "performance_with_evidence": "After semantic perturbations FiD often still outputs the original answer; large fraction of examples still return original answer (Table 8 and Appendix). Calibration/probability scores decreased only for a minority of examples (paper: 'only for 30–40% of examples we see a decrease in calibration score'); in Appendix reported per-perturbation percentages of examples where confidence dropped (e.g., Table 14 shows per-perturbation Gen.Prob drops: negation 65.94%, modality 62.75%, future 58.87%, text-infilling 60.56% — counts depend on subset and coverage).",
            "evidence_effect": "mixed/negative — semantic contradictions in supporting passages often fail to prevent the model from returning the original (now unsupported) answer; confidence measures do not reliably reflect the invalidation.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Model behaves like extractive systems: it tends to return answers that match the answer-type and previously attended spans, and its token-generation probability / calibrator score is insufficiently sensitive to semantic edits; attention and answer-type priors dominate.",
            "key_findings": "Semantic perturbations that should invalidate an answer frequently do not cause FiD to change its predicted span, and calibration/generation-probability metrics are only partially sensitive (decrease on a minority or an inconsistent fraction of examples); the model returns apparently 'confident' but unsupported answers.",
            "counterintuitive_behavior": true,
            "uuid": "e195.3",
            "source_info": {
                "paper_title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Calibration_and_Abstention",
            "name_full": "Separately trained calibrator for abstention and conflict detection",
            "brief_description": "An XGBoost-based calibrator trained on generation probability plus encoder features can learn to abstain when evidence contains conflicting answer candidates, but generalization across different conflict-collection methods is limited.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Separate calibrator (XGBoost) over FiD features",
            "model_size": null,
            "task_description": "Selective question answering / abstention: detect instances where model should refrain because evidence is conflicting",
            "evidence_type": "Mixed/conflicting retrieved passages (simulated partial substitution, AmbigQA-derived alternatives, SituatedQA temporal alternatives)",
            "evidence_source": "Augmented training sets constructed by perturbation methods and by retrieving alternative-answer passages from AmbigQA / SituatedQA",
            "parametric_knowledge_alignment": "mixed (conflicting evidence vs parametric priors)",
            "performance_without_evidence": "Vanilla model generation-probability (no calibrator) yields baseline binary calibration accuracy = 66.32% on original NQ dev (Table 9).",
            "performance_with_evidence": "Calibrators trained on matched augmented datasets can substantially improve binary abstention/accuracy on that evaluation set (e.g., Partial-Substitutions trained calibrator achieves 98.60% on Partial Sub evaluation; combined training improves macro average to 72.46%), but these calibrators do not generalize reliably to other conflict types (see Table 9).",
            "evidence_effect": "positive (a trained calibrator can lower the model's tendency to present a single answer when it sees conflicting evidence), but limited generalization means effect is dataset/method dependent.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Train a separate classifier on features (generation probability + averaged encoder representations) to predict correctness/need-to-abstain; data augmentation with realistic conflict types is required to teach the calibrator to detect specific conflict patterns.",
            "key_findings": "A standalone calibrator can learn to abstain on simulated or matched conflict sets, improving selective-answering accuracy on those sets, but calibration trained on one conflict-collection method often fails to generalize to other real-world conflict types (AmbigQA, SituatedQA), so recalibration is a partial solution requiring diverse augmented training data.",
            "counterintuitive_behavior": true,
            "uuid": "e195.4",
            "source_info": {
                "paper_title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entity-based knowledge conflicts in question answering",
            "rating": 2,
            "sanitized_title": "entitybased_knowledge_conflicts_in_question_answering"
        },
        {
            "paper_title": "Knowing more about questions can help: Improving calibration in question answering",
            "rating": 2,
            "sanitized_title": "knowing_more_about_questions_can_help_improving_calibration_in_question_answering"
        },
        {
            "paper_title": "Beyond accuracy: Behavioral testing of NLP models with checklist",
            "rating": 1,
            "sanitized_title": "beyond_accuracy_behavioral_testing_of_nlp_models_with_checklist"
        },
        {
            "paper_title": "ContraQA: Question answering under contradicting contexts",
            "rating": 2,
            "sanitized_title": "contraqa_question_answering_under_contradicting_contexts"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        }
    ],
    "cost": 0.0156875,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence</h1>
<p>Hung-Ting Chen Michael J.Q. Zhang Eunsol Choi<br>Department of Computer Science<br>The University of Texas at Austin<br>{hungtingchen, mjqzhang, eunsol}@utexas.edu</p>
<h4>Abstract</h4>
<p>Question answering models can use rich knowledge sources - up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend information stored in their LM parameters with that from retrieved evidence documents. In this paper, we simulate knowledge conflicts (i.e., where parametric knowledge suggests one answer and different passages suggest different answers) and examine model behaviors. We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledge in their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally. To address this issue, we present a new calibration study, where models are discouraged from presenting any single answer when presented with multiple conflicting answer candidates in retrieved evidences.</p>
<h2>1 Introduction</h2>
<p>Traditionally, QA models have relied on retrieved documents to provide provenance for their answers (Chen et al., 2017). Recent studies (Petroni et al., 2019) have shown that large language models are able to retain vast amounts of factual knowledge seen during pretraining, and closed-book QA systems (Roberts et al., 2020) build upon this foundation by memorizing facts from QA finetuning. Retrieval-based generation approaches (Izacard and Grave, 2021; Lewis et al., 2020) emerge as the best of both worlds - generating free-form answers from the question paired with retrieved evidence documents. They further combine these parametric knowledge sources with a large number of retrieved evidence documents, achieving state-of-the-art performances on open retrieval QA datasets (Joshi et al., 2017; Kwiatkowski et al., 2019).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Models can use both parametric and nonparametric knowledge sources. In this example, the answer could be the U.S./Norway/Germany. We investigate for a given question which knowledge source was the most influential to output an answer. The model should be able to abstain from answering for these examples, as it is difficult for the model to decide which answer candidate is correct.</p>
<p>Understanding how retrieval-based generation models combine information from parametric and non-parametric knowledge sources is crucial for interpreting and debugging such complex systems, particularly in adversarial and complex real world scenarios where these sources may conflict with each other (see an example in Figure 1). This can aid both developers to debug such models and for users to estimate how much they should trust an answer (Ribeiro et al., 2016). Thus, we focus on the following core question: when provided with numerous evidence passages and a pretrained and finetuned language model, which knowledge source do models ground their answers in?</p>
<p>A recent study (Longpre et al., 2021) investigated this in a limited single evidence document setting. We expand this study to consider a more realistic scenario, where models consider multiple evidence passages (up to 100 passages), and observe results diverging from their reported heavy reliance on parametric knowledge. We further sim-</p>
<p>ulate a setting where a subset of evidence passages are perturbed to suggest a different answer to reflect the realistic scenario where retrieval returns a mixed bag of information. Such scenarios are common in settings where some passages are updated with new information, while other passages remain outdated (Shah et al., 2020; Zhang and Choi, 2021). Such conflicts can also occur when passages are adversarially edited to contain false information ( Du et al., 2022), or when passages are authored by multiple people who have differing opinions about an answer (Chen et al., 2019).</p>
<p>Our extensive studies on two datasets (Joshi et al., 2017; Kwiatkowski et al., 2019) and two models (Izacard and Grave, 2020; Lewis et al., 2020) exhibit that retrieval-based generation models are primarily extractive and are heavily influenced by a few most relevant documents instead of aggregating information over a large set of documents. Learning that models mostly rely on evidence passages rather than parametric knowledge, we evaluate how sensitive models are toward semantic perturbation to the evidence documents (e.g., adding negation). We find retrieval-based generation models behave similarly to extractive models, sharing their weakness of returning answer candidates with high confidence, even after the context is modified to no longer support the answer (Ribeiro et al., 2020).</p>
<p>What should models do when confronted with conflicting knowledge sources? We propose a new calibration setting (Section 5), where a model is encouraged to abstain from proposing a single answer in such scenarios. We find that teaching models to abstain when there are more than one plausible answers is challenging, and training a separate calibrator with augmented data helps moderately.</p>
<p>To summarize, we empirically test how QA models (Izacard and Grave, 2021; Lewis et al., 2020) use diverse knowledge sources. We present the first analysis of knowledge conflicts where (1) the model uses multiple passages, (2) knowledge conflicts arise from ambiguous and context-dependent user queries, and (3) there are knowledge conflicts between different passages. Our findings are as follows: when provided with a high recall retriever, models rely almost exclusively on the evidence passages without hallucinating answers from parametric knowledge. When different passages suggest multiple conflicting answers, models prefer the answer that matches their parametric knowledge.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Generative</th>
<th style="text-align: center;">Retrieval-Based</th>
<th style="text-align: center;">Multi-Pass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">REALM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">T5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RAG</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Overview of recent open retrieval QA approaches. Generative indicates whether the model generates the answer and, therefore, can produce answers not found in the retrieved documents. Retrieval-Based indicates whether the model uses retrieval to find relevant passages to help produce an answer. Multi-Passage indicates whether the system is able to model interactions between separate evidence passages.</p>
<p>Lastly, we identify various weaknesses of retrievalbased generation models, including its confidence score not reflecting the existence of conflicting answers between knowledge sources. Our initial calibration study suggests that dissuading models from presenting a single answer in the presence of rich, potentially conflicting, knowledge sources is challenging, and demands future study.</p>
<h2>2 Background</h2>
<p>We first describe the task setting, QA models, and calibrator used in our study.</p>
<p>We study open retrieval QA, where the goal is to find an appropriate answer $y^{*}$ for a given question $q$. Systems for open retrieval QA may also be provided with access to a knowledge corpus consisting of a large number of passages, $p$, which is used to help answer the question. We use the open retrieval split (Lee et al., 2019) of the NaturalQuestions dataset (NQ-Open) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017), and use Wikipedia as our knowledge corpus. ${ }^{1}$</p>
<h3>2.1 Model</h3>
<p>We investigate two retrieval-based generation QA models: Fusion-in-Decoder (Izacard and Grave, 2021) and Retrieval Augmented Generation model (Lewis et al., 2020). Both architectures have reader and retriever components, using the same dense phrase retriever (Karpukhin et al., 2020) which learns an embedding of question and passage, and retrieves a fixed number $(N)$ of passages that are most similar to the query embedding. They mainly differ in their reader architecture and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>learning objective, which we describe below.</p>
<p>Fusion-in-Decoder (FiD) The reader model is based on pretrained language model (specifically, T5-large (Raffel et al., 2020)). Each retrieved passage, $p_{i}(i=[1, N])$, is concatenated with the question, $q$, before being encoded by T5 to generate representations, $\left[h_{1}^{i}, \ldots, h_{m}^{i}\right]$, where $m$ is the length of the $i$ th passage prepended with the question. All $N$ passages are then concatenated to form a single sequence, $\left[h_{1}^{1}, \ldots, h_{m}^{1}, \ldots, h_{1}^{N}, \ldots, h_{m}^{N}\right]$, which the decoder interacts with using cross-attention to generate the answer. ${ }^{2}$</p>
<p>We use trained FiD (large) checkpoint provided by the authors for most analysis. ${ }^{3}$ When evaluating models with access to different number of passages, we re-train FiD model (pretrained weights loaded from T5-large) using 1, 5, 20 and 50 passages retrieved by DPR. Refer to Appendix A. 2 for full model and training details.</p>
<p>Retrieval Augmented Generation (RAG) RAG conditions on each retrieved evidence document individually to produce an answer, marginalizing the probability of producing an answer over all retrieved evidence documents. ${ }^{4}$ By applying this constraint, RAG is able to jointly train the reader and retriever, at the cost of ignoring interactions between evidence documents. FiD, in contrast, is able to model such interactions during decoding while the reader and retriever is completely disjoint.</p>
<p>Recent work explored jointly training the reader and retriever in FiD (Izacard and Grave, 2020; Sachan et al., 2021; Yang and Seo, 2020), showing small gains. Table 1 summarizes different architectures, including two open book approaches (Karpukhin et al., 2020; Guu et al., 2020), one closed book approach (Roberts et al., 2020) and two retrieval-based generation approaches. As FiD is efficient and effective, we focus most of our analysis (Section 4, B) on it. We only report RAG results on a few of our main analyses to verify that general trends of the FID model hold for RAG (which they typically do).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.2 Model Confidence Study</h3>
<p>We analyze the model confidence score, asking a more nuanced question: is model's confidence on the gold answer decreased after we perturb knowledge sources? We compare the model confidence on the same example before and after perturbation. We determine the confidence of the model using either (1) the generation probability of the answer (i.e., the product of the probability of generating each token conditioned on all the previously generated tokens) or (2) the confidence score of separately trained answer calibrator, which provides a score indicating the probability of the model correctly predicting the answer for each example. We train a binary calibrator following prior work (Kamath et al., 2020; Zhang et al., 2021), using gradient boosting library XGBoost (Chen and Guestrin, 2016). The goal of the calibrator is to enable selective question answering - equipping models to decide when to abstain from answering. Given an input question $q$ and learned model $M_{\theta}$, the calibrator predicts whether the predicted answer $\hat{y}=M_{\theta}(q)$ will match the annotated answer $y^{*}$. We follow the settings of calibrator from prior work (Zhang et al., 2021), and details can be found in Appendix A.1.</p>
<h2>3 When do retrieval-based generation models rely on parametric knowledge?</h2>
<p>As an initial step investigating whether retrievalbased generation models ground their answers in the retrieval corpus or in the pretrained language model's parametric knowledge, we evaluate whether models generate a novel answer that is not present in a set of evidence documents. Unlike extractive QA models (Seo et al., 2017), generation based approaches (Roberts et al., 2020; Izacard and Grave, 2021) do not require the evidence documents to contain the gold answer span. Thus, we first analyze whether they actually generate novel answer spans not found in the retrieved passages.</p>
<p>Table 2 reports how often models generate a span not found in the evidence passages, split by the retrieval performance on the NQ-Open (Kwiatkowski et al., 2019; Lee et al., 2019) and TriviaQA (Joshi et al., 2017) development set. We observe that models typically copy a span from the evidence passages, only generating novel spans for $3.4 \% / 6.2 \%$ of examples in NQ/TriviaQA for FiD and 20.2\% for RAG in NQ. Even for the small subset of examples where the retrieved documents do not contain the answer string, FiD remains extractive</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model <br> (Data)</th>
<th style="text-align: center;">Retrival <br> suc.</th>
<th style="text-align: center;">CBQA <br> Diff \%</th>
<th style="text-align: center;">Extractive</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Abstractive</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">\%</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">\%</td>
<td style="text-align: center;">EM</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{FiD} \ &amp; (\mathrm{NQ}) \end{aligned}$</td>
<td style="text-align: center;">Y (89\%)</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N (11\%)</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">21.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">12.4</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{gathered} \text { RAG } \ (\mathrm{NQ}) \end{gathered}$</td>
<td style="text-align: center;">Y (63\%)</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N (37\%)</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">9.6</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{gathered} \text { FiD } \ (\mathrm{TQA}) \end{gathered}$</td>
<td style="text-align: center;">Y (88\%)</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">N (12\%)</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">25.6</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of hybrid models on the NQOpen (NQ) and TriviaQA (TQA) development set broken down by their retrieval performance. Results are split based on whether the retrieval was successful (i.e., gold answer string is within the top $\mathrm{K}(\mathrm{K}=100$ for FID; $\mathrm{K}=5$ for RAG) retrieved documents ( Y ), or not $(\mathrm{N})$, and the percentage in parentheses refers to the percentage of examples belonging to each set. We report the proportion of predictions that are not matching the CBQA model prediction. ' - ' means cell's value is zero by definition.
for $82.9 \% / 69.6 \%$ of examples in NQ/TriviaQA. In contrast, for RAG, where retrieved documents frequently miss the gold answer ( $37 \%$ ), such copying behavior was less common, generating unseen text for $42.1 \%$ of examples. The results suggest reliance on retrieved documents increased as retriever performance increases. We also report the percentage of examples where the model prediction is different from that of a T5 closed-book question answering (CBQA) model trained on the same data. ${ }^{5}$ Over $70 \%$ of examples have different answers from the CBQA model, even when the answer is abstractive, suggesting hybrid models use passages even when there is no exact string match.</p>
<p>Revisiting knowledge conflict study in Longpre et al. (2021) This observation stands at odds with an earlier study on knowledge conflict (Longpre et al., 2021) which simulates knowledge conflict by substituting the existing answer with a new answer candidate in the evidence passage (see Table 3 for an example), creating a mismatch between knowledge from parametric knowledge and the evidence document. They showed that models frequently rely on parametric knowledge, generating answers not present in the evidence passage. The original passage is minimally changed, yet now suggests an alternative, incorrect answer candidate that likely</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Question: When was the last time the Bills won their division?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Passage</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">None</td>
<td style="text-align: center;">Original <br> Entity</td>
<td style="text-align: center;">...the 1995 Bills won the AFC East</td>
<td style="text-align: center;">1995</td>
</tr>
<tr>
<td style="text-align: center;">Entity <br> Sub.</td>
<td style="text-align: center;">Random <br> (Same <br> Type)</td>
<td style="text-align: center;">...the 1936 Bills won the AFC East</td>
<td style="text-align: center;">1936</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Negation</td>
<td style="text-align: center;">...the 1995 Bills did not win the AFC East ...</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Semantic <br> Pert.</td>
<td style="text-align: center;">Modality</td>
<td style="text-align: center;">...the 1995 Bills might win the AFC East ...</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Future</td>
<td style="text-align: center;">...the 1995 Bills will win the AFC East ...</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Infilling</td>
<td style="text-align: center;">...the 1995 Bills lost the AFC East</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Example perturbations. Entity substitutions modify the passage by replacing the answer entity mention with another answer candidate of the same entity type. Given the modified passage, the new answer is the substitute entity. Semantic perturbation invalidates the previous answer without introducing a new answer.
contradicts with knowledge from LM. The model produced the original answer $17 \%$ of the time, even when the answer no longer appears in the passage.</p>
<p>We identify that the main difference in their experimental setup is in using a single evidence passage rather than multiple evidence passages. We re-visit their study, as single document setting is impractical. Most open-retrieval QA models (Lewis et al., 2020; Karpukhin et al., 2020; Izacard and Grave, 2021) are trained with multiple passage to make up for imperfect passage retrieval. According to the answer recall in Table 4 and 5, when the model is provided with 100 passages, the correct span is available nearly $90 \%$ of the time (compared up to $50 \%$ when provided one passage), thus the model remains extractive.</p>
<p>Following their setup, we only evaluate on examples that the model has correctly answered (as perturbing examples where models are already confused is unnecessary) and where the answer is an entity. ${ }^{6}$ We then substitute every answer entity mention in all evidence passages with a random entity of same type sampled from the training data. ${ }^{7}$ All manipulation was done only at inference, and after the passages are retrieved.</p>
<p>We report the exact match score to the original answer. Prior to perturbation, the exact match score against the original answer is $100 \%$. We also report the exact match score to the substituted answer and</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"># Pass. <br> train / inf.</th>
<th style="text-align: right;">Ans. <br> Rec.</th>
<th style="text-align: right;">Exact Match <br> Orig.</th>
<th style="text-align: right;">Sub.</th>
<th style="text-align: right;">$M_{R}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$1 / 1$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">27</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$1 / 1$</td>
<td style="text-align: right;">48.5</td>
<td style="text-align: right;">10.1</td>
<td style="text-align: right;">61.1</td>
<td style="text-align: right;">14.1</td>
</tr>
<tr>
<td style="text-align: left;">RAG</td>
<td style="text-align: left;">$5 / 1$</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">10.3</td>
<td style="text-align: right;">65.9</td>
<td style="text-align: right;">13.5</td>
</tr>
<tr>
<td style="text-align: left;">RAG</td>
<td style="text-align: left;">$5 / 5$</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">11.6</td>
<td style="text-align: right;">63.7</td>
<td style="text-align: right;">15.3</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$5 / 1$</td>
<td style="text-align: right;">72.9</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">69.5</td>
<td style="text-align: right;">4.2</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$5 / 5$</td>
<td style="text-align: right;">72.9</td>
<td style="text-align: right;">2.7</td>
<td style="text-align: right;">53.1</td>
<td style="text-align: right;">4.8</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$20 / 1$</td>
<td style="text-align: right;">83.1</td>
<td style="text-align: right;">1.2</td>
<td style="text-align: right;">70.6</td>
<td style="text-align: right;">1.6</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$20 / 20$</td>
<td style="text-align: right;">83.1</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">2.0</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$50 / 1$</td>
<td style="text-align: right;">86.8</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">82.0</td>
<td style="text-align: right;">0.4</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$50 / 50$</td>
<td style="text-align: right;">86.8</td>
<td style="text-align: right;">1.1</td>
<td style="text-align: right;">50.4</td>
<td style="text-align: right;">2.1</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$100 / 1$</td>
<td style="text-align: right;">88.7</td>
<td style="text-align: right;">1.1</td>
<td style="text-align: right;">71.3</td>
<td style="text-align: right;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$100 / 100$</td>
<td style="text-align: right;">88.7</td>
<td style="text-align: right;">2.4</td>
<td style="text-align: right;">64.5</td>
<td style="text-align: right;">3.6</td>
</tr>
</tbody>
</table>
<p>Table 4: Answer Exact Match / Memorization Ratio with different amount of passages in NQ. The results in the first row are reported in Longpre et al. (2021), which uses MRQA version of NQ (Fisch et al., 2019) dataset. All other rows use NQ-Open split. The second column reports the number of passages used during training and inference time, respectively. Ans Rec. refers to \% of examples where retrieved passage set contains the answer string.
memorization ratio $\left(M_{R}=\frac{p_{o}}{p_{o}+p_{s}}\right)$ where $p_{o}$ is the fraction of examples where the model predicts the original answer, and $p_{s}$ is the fraction of examples predicting the substitute answer.</p>
<p>Table 4 and 5 reports how models respond to entity-substituted contexts with a differing number of passages available at training and inference time. In congruence with our prior experiments, we observe higher reliance on parametric knowledge as answer recall in the retrieved evidence decreases. Departing from Longpre et al. (2021), we find that memorization in FiD is uncommon (less than $3.6 \% / 8.5 \%$ for NQ/TriviaQA) when reader is provided with multiple passages at training time, and FiD grounds its answers mostly in evidence passages instead of its parametric knowledge when answer recall is reliably high. Furthermore, when provided with multiple evidence passages with comparable answer recall, FiD exhibits far less memorization than RAG, suggesting that using a multi-passage reader that doesn't marginalize over passages inhibits memorization. We study domain transfer setting in Appendix A.9, showing that the memorization is still rare when the reader models are evaluated on out-of-domain datasets, as long as retriever performance was high during its training.</p>
<p>Takeaway Retrieval-based reader models exhibit little memorization when the retriever has a high</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># Pass. <br> train / inf.</th>
<th style="text-align: left;">Ans. <br> R</th>
<th style="text-align: left;">Exact Match <br> Orig.</th>
<th style="text-align: left;">Sub.</th>
<th style="text-align: right;">$M_{R}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$1 / 1$</td>
<td style="text-align: left;">67.1</td>
<td style="text-align: left;">20.6</td>
<td style="text-align: left;">38.6</td>
<td style="text-align: right;">34.8</td>
</tr>
<tr>
<td style="text-align: left;">$5 / 1$</td>
<td style="text-align: left;">81.7</td>
<td style="text-align: left;">10.4</td>
<td style="text-align: left;">52.7</td>
<td style="text-align: right;">16.5</td>
</tr>
<tr>
<td style="text-align: left;">$5 / 5$</td>
<td style="text-align: left;">81.7</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">52.4</td>
<td style="text-align: right;">16.9</td>
</tr>
<tr>
<td style="text-align: left;">$20 / 1$</td>
<td style="text-align: left;">85.7</td>
<td style="text-align: left;">8.5</td>
<td style="text-align: left;">53.9</td>
<td style="text-align: right;">13.6</td>
</tr>
<tr>
<td style="text-align: left;">$20 / 20$</td>
<td style="text-align: left;">85.7</td>
<td style="text-align: left;">8.8</td>
<td style="text-align: left;">52.1</td>
<td style="text-align: right;">14.5</td>
</tr>
<tr>
<td style="text-align: left;">$50 / 1$</td>
<td style="text-align: left;">87.2</td>
<td style="text-align: left;">6.0</td>
<td style="text-align: left;">59.3</td>
<td style="text-align: right;">9.1</td>
</tr>
<tr>
<td style="text-align: left;">$50 / 50$</td>
<td style="text-align: left;">87.2</td>
<td style="text-align: left;">6.8</td>
<td style="text-align: left;">57.9</td>
<td style="text-align: right;">10.6</td>
</tr>
<tr>
<td style="text-align: left;">$100 / 1$</td>
<td style="text-align: left;">87.9</td>
<td style="text-align: left;">8.64</td>
<td style="text-align: left;">56.2</td>
<td style="text-align: right;">13.3</td>
</tr>
<tr>
<td style="text-align: left;">$100 / 100$</td>
<td style="text-align: left;">87.9</td>
<td style="text-align: left;">4.9</td>
<td style="text-align: left;">52.6</td>
<td style="text-align: right;">8.5</td>
</tr>
</tbody>
</table>
<p>Table 5: Exact Match / Memorization Ratio for FiD model with different amount of passages on TriviaQA. The memorization ratio decreases as we increase the number of evidence passages.
recall during its training.</p>
<h2>4 Simulating Mixed Bag of Evidence Passages</h2>
<p>Having identified that retrieval-based generation models rely heavily on evidence passages, especially when paired with a high-performance retriever, we study how models make use of multiple evidence passages when different passages suggest different answers. This happens frequently in real life, as questions can be ambiguous based on different, valid interpretations of the question (Min et al., 2020) or different extra-linguistic contexts (Zhang and Choi, 2021).</p>
<p>We introduce two perturbations - an entity substitution perturbation (Longpre et al., 2021) (Section 4.1) and adversarial semantic perturbation (Jia and Liang, 2017) (Section 4.2) - both will dissuade model from returning the original answer in the evidence passage (see Table 3 for examples). We analyze the best performing FiD model trained with 100 passages.</p>
<h3>4.1 Entity Substitution</h3>
<p>Setting. To simulate a mixed bag of evidence passages, we perform partial entity substitution, changing answers to a subset of passages mentioning the answer entity. On average, the answer entity is mentioned in 16.7 out of 100 retrieved evidence passages for NQ-open and 21.5 for TriviaQA dataset. We substituted the answer entity mentions in $25 \%$, $50 \%, 75 \%$ and $100 \%$ of evidence passages that contain the original gold answer span with a new entity. We sample passages to substitute answer entity in one of three ways.</p>
<ul>
<li>random: randomly sample passages.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Substituting different proportion of retrieved passages containing gold answer spans on filtered NQ-Open (top) and Trivia QA (bottom) development set.</p>
<ul>
<li><strong>top-retrieval</strong>: select top passages ranked by retrieval score.</li>
<li><strong>top-attention</strong>: select top passages ranked by attention score. Attention score for each passage is computed as the cross-attention score on the first decoded token averaged across layers, heads, and the tokens in the passage, as defined in Izacard and Grave (2020).</li>
</ul>
<p><strong>Results.</strong> Figure 2 reports our results with different amounts of perturbation (i.e., how many evidence passages are perturbed) and different methods of sampling passages to substitute entities in. After perturbing all of the passages, so that the original answer is no longer within any of the passages, the model successfully refrains from predicting the original answer 98% of the time. However, after randomly selecting 50% of the passages to perturb, we find that the model still favors the original answer almost twice as frequently on NQ (52% vs. 25%) and almost four times on TriviaQA (59% vs. 15%). This indicates that parametric knowledge still plays a significant role when more than one potential answer exists in the retrieval results.</p>
<p>When we perturb the top scoring passages, as measured by either retrieval or attention score, the model changes its answer much more frequently. Using either scoring metric, perturbing the top 25% of passages successfully changes the predicted answers in about 30% of examples compared to the 8% of examples whose answers are successfully changed by perturbing randomly sampled passages. This suggests <strong>that the model may be ignoring lower-scoring retrieved passages that are less relevant to the query, despite containing the answer entity.</strong></p>
<p><strong>Confidence Study.</strong> Table 6 reports the change in model confidence after performing random entity substitution in NQ-Open and TriviaQA development set.</p>
<table>
<thead>
<tr>
<th>%</th>
<th>NQ-Open</th>
<th>TriviaQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pert.</td>
<td>Gen. Prob.</td>
<td>Calib.</td>
</tr>
<tr>
<td>25</td>
<td>48.15%</td>
<td>57.07%</td>
</tr>
<tr>
<td>50</td>
<td>49.67%</td>
<td>56.30%</td>
</tr>
<tr>
<td>75</td>
<td>49.86%</td>
<td>56.84%</td>
</tr>
<tr>
<td>100</td>
<td>52.22%</td>
<td>56.95%</td>
</tr>
</tbody>
</table>
<p>Table 6: The % of examples in which model confidence on the correct answer dropped after partial answer substitution in NQ-Open and TriviaQA development set.</p>
<table>
<thead>
<tr>
<th>k</th>
<th>NQ-Open</th>
<th>TriviaQA</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Original</td>
<td>Substitute</td>
</tr>
<tr>
<td>1</td>
<td>41.33</td>
<td>33.11</td>
</tr>
<tr>
<td>3</td>
<td>69.15</td>
<td>12.38</td>
</tr>
<tr>
<td>5</td>
<td>78.50</td>
<td>5.97</td>
</tr>
</tbody>
</table>
<p>Table 7: Substituting all the passages except top k passages (k=[1,3,5]), which are selected based on passage attention scores. On average, 16.7 (NQ-Open) and 21.5 (TriviaQA) passages out of 100 passages contained gold answer entity. Yet, with access of up to 3 passages containing the gold answer span, the FiD model can still generate the original answer nearly 70% of the time.</p>
<p>Substitution in the evidence passages. Consistent with the results from Zhang et al. (2021), we find that a separately trained calibrator consistently outperforms the model's inherent confidence score. <strong>Surprisingly, there is no clear connection between the percentage of perturbed passages and model confidence.</strong> Ideally, when given a mixed bag of evidence, a model's confidence should decrease to reflect the uncertainty from seeing multiple, conflicting answers. We revisit this in Section 5 where we pilot a calibrator whose confidence drops when presented with conflicting evidence.</p>
<p><strong>Additional Analysis.</strong> Our confidence study suggests model might not consider all provided passages. To further investigate this, we substitute answers in all passages except top K passages, ranked by the attention score from the reader. Table 7 presents the results. If you change the answer to all passages except for the top scoring article, the model already outputs the original answer more frequently than the substitute answer. This again suggests that the model might focus on a handful of most relevant passages and ignore other passages.</p>
<p>In Appendix B, we include two further studies. First, we study whether the choice of alternative answer impacts its behaviors. When we provide more realistic alternative answer (either drawn from outdated corpus or answers to the slightly different interpretation of the question), unsurprisingly, model</p>
<p>is less biased to choose the original answer. Second, we study whether model's parametric knowledge is learned during pre-training phase or fine-tuning phase, concluding most of its parametric knowledge is learned during the fine-tuning stage.</p>
<h2>Takeaway.</h2>
<ul>
<li>The models resort to the parametric knowledge to resolve conflicts between different retrieved passages.</li>
<li>Model confidence itself cannot be used to identify knowledge conflicts.</li>
<li>The model rely on a few most relevant passages, ignoring others.</li>
</ul>
<h3>4.2 Adversarial Semantic Perturbation</h3>
<p>Semantic perturbation follows earlier work on counterfactual example generation with heuristics (Ribeiro et al., 2020) which perturbs the sentence containing the answer. We simulate four perturbations, and after each perturbation, the model should refrain from returning the original answer. We aim to test model's understanding of the passage with such perturbation.</p>
<p>Setting. We design the four perturbations applicable to question answering: negation, changing to future tense, adding modal verb and text infilling. Examples of each perturbation are in Table 3. To generate these, we run a dependency parser on the sentence containing the gold answer span. ${ }^{8}$ We then filter examples where the root token of answer sentence is not a verb (about $40 \%$ of sentences, see Appendix A. 3 for full statistics). Finally, we apply simple rules (see Appendix A.4) to modify the verb. For text infilling, the only difference is that we convert the root token into "[blank]" and fill in the blank using language modeling (Donahue et al., 2020). For passages containing multiple gold answer spans, we apply these perturbations to all sentences as long as their root tokens are verbs.</p>
<p>Results. In Table 8, we report the exact match to the original answer after applying semantic perturbations. Since our perturbation rules only cover 67$86 \%$ of all sentences containing an answer string, we further subreport our results based on whether there are any remaining unperturbed answer sentences in the evidence. The "partial coverage" subset is the set we created based on the perturbation rules. The "full coverage" subset is created by</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 8: Exact match score with the original answer after perturbation of each type: models largely disregard the perturbation and outputs the original answer.
removing the examples where not all answer sentences have been perturbed.</p>
<p>Since our perturbation rules only cover 67-86\% of all sentences containing an answer string, we further subreport our results based on whether there are any remaining unperturbed answer sentences in the evidence (partial coverage) or if all answer sentences are perturbed (full coverage). Examples with partial coverage simulate a mixed bag of evidence which may induce the model to return the original answer. In all instances, we expect the exact match to drop significantly after perturbation, as all edits invalidate the original answer; however, we observe that models still return the original answer after perturbation, mirroring what Ribeiro et al. (2020) finds with extractive models. ${ }^{9}$</p>
<p>Confidence Study. We repeat the calibration study with semantic perturbation. We find that calibration scores remain mostly steady after the perturbation for all four perturbation types, only for $30-40 \%$ of examples we see a decrease in calibration score after the perturbation. The model is particularly less sensitive to temporal perturbation (future). The exact numbers and the ratio of calibration scores before and after the perturbation can be found in the Appendix A.8. We observe that model behaves similarly to extractive model (Ribeiro et al., 2020), returning an answer matching the answer type with high confidence even when the passage no longer supports it.</p>
<h2>5 Re-Calibrating Models Given a Mixed Bag of Evidence</h2>
<p>When presented with a mixed bag of evidence, systems should inform users of the multiple, conflicting answers. While there are many of approaches for relaying this information to users (e.g., composing a paragraph aggregating answer candidates,</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Perturbation Method</th>
<th style="text-align: center;">Original <br> (NQ Dev) <br> $(N=8.7 \mathrm{k})$</th>
<th style="text-align: center;">Partial <br> Sub. <br> $(N=2.5 \mathrm{k} * 2)$</th>
<th style="text-align: center;">AmbigQA <br> (Disambiguated Q.) <br> $(N=448 * 2)$</th>
<th style="text-align: center;">SituatedQA <br> (Retrieval 2021) <br> $(N=55 * 2)$</th>
<th style="text-align: center;">Macro <br> Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model Confidence</td>
<td style="text-align: center;">$\mathbf{6 6 . 3 2}$</td>
<td style="text-align: center;">28.21</td>
<td style="text-align: center;">13.17</td>
<td style="text-align: center;">20.24</td>
<td style="text-align: center;">31.99</td>
</tr>
<tr>
<td style="text-align: left;">Org. Calibrator $\left(N_{t r}=80 \mathrm{k}\right)$</td>
<td style="text-align: center;">62.92</td>
<td style="text-align: center;">12.63</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">8.33</td>
<td style="text-align: center;">22.09</td>
</tr>
<tr>
<td style="text-align: left;">Partial Sub. $\left(N_{t r}=36 \mathrm{k}^{*} 2\right)$</td>
<td style="text-align: center;">63.61</td>
<td style="text-align: center;">$\mathbf{9 8 . 6 0}$</td>
<td style="text-align: center;">13.62</td>
<td style="text-align: center;">28.57</td>
<td style="text-align: center;">51.1</td>
</tr>
<tr>
<td style="text-align: left;">AmbigQA $\left(N_{t r}=5 \mathrm{k}^{*} 2\right)$</td>
<td style="text-align: center;">59.84</td>
<td style="text-align: center;">42.15</td>
<td style="text-align: center;">$\mathbf{8 7 . 9 3}$</td>
<td style="text-align: center;">79.76</td>
<td style="text-align: center;">67.42</td>
</tr>
<tr>
<td style="text-align: left;">SituatedQA $\left(N_{t r}=1.5 \mathrm{k}^{*} 2\right)$</td>
<td style="text-align: center;">59.09</td>
<td style="text-align: center;">37.15</td>
<td style="text-align: center;">40.40</td>
<td style="text-align: center;">$\mathbf{9 2 . 8 6}$</td>
<td style="text-align: center;">57.38</td>
</tr>
<tr>
<td style="text-align: left;">Part.+Amb.+Sit. $\left(N_{t r}=42.5 \mathrm{k}^{*} 2\right)$</td>
<td style="text-align: center;">64.55</td>
<td style="text-align: center;">98.22</td>
<td style="text-align: center;">54.46</td>
<td style="text-align: center;">72.62</td>
<td style="text-align: center;">$\mathbf{7 2 . 4 6}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Binary accuracy (\%) of the calibrator trained on different augmented datasets tested on various evaluation sets. Each column represents evaluation set. Baseline calibrators are in the top row block, and calibrators are trained with augmented training data matching evaluation dataset in the second row block. $N_{t r}$ stands for the training dataset size, and $N$ denotes the size of each evaluation set.
or providing set of answers mapped to documents supporting them), a necessary prerequisite to all such systems is the ability to detect when there are conflicting answers in the evidence. Thus, we explore creating systems that can detect and abstain from predicting on instances with conflicting evidence. Questions should only be answered if (1) there is no knowledge conflict in its evidence set and (2) model's predicted answer matches the annotated answer. We report calibrator's binary calibration accuracy following prior work (Kamath et al., 2020). We explain four evaluation settings here.
Original We use the original NQ development set as is to provide a reference for the performances of calibrators.</p>
<p>In the following settings, we only look at examples where the original FID model correclty answers. Thus, the calibrator should only abstain for knowledge conflict. We construct three different types of knowledge conflict set where calibrator should abstain on half of the examples because of the knowledge conflict. To construct these set, we use the original question, 100 evidence passage set (where model should present its answer), and augment one perturbed example, where 100 evidence passage set is perturbed to have multiple answer candidates to the same question. We discuss three ways to introduce perturbed evidence set, with more than one valid answer candidate now.
Partial Substitution We use the sets of conflicting evidence passages constructed in Section 4.1 (randomly sampling $50 \%$ of the retrieved passages to substitute a new entity in).
AmbigQA Instead of random new entity, we sample valid alternative answer to the question taken from a different interpretation of the same question from AmbigQA (Min et al., 2020) dataset. Instead
of simply replacing answer in existing passage, we retrieve new passage for each rewritten, disambiguated version of the question.
SituatedQA We sample valid alternative answer from either corpus taken from a different time period from SituatedQA (Zhang and Choi, 2021) dataset. We use the same query, but retrieve over two different snapshots of the same corpus (the Wikipedia dump from 2018 and from 2021).</p>
<p>We evenly combine retrieved passages from conflicting answer sets, using the top retrieved passages that contain the respective answer and backing off to the passages with high retrieval scores if not enough passages contain the answer string.</p>
<p>Calibrator As a baseline, we use same calibration model from our prior study in Section 2.2. We also retrain separator calibrators for each of our three substituted answer types, which are trained by applying the same data augmentation process that was applied to the evaluation set (described above) to training portion of filtered NQ-Open dataset.</p>
<p>Results We report the results in Table 9. We observe vanilla model confidence outperforms trained calibrator, showing robustness towards out of domain setting. This could be caused by a large gap in accuracy of FiD model for training ( $80 \%$ ) and testing data ( $52 \%$ ). Base calibrators, without data augmentation, struggles substantially, particularly on real world knowledge conflict scenario where it is presented with multiple valid answer candidates (AmbigQA, SituatedQA). Training with data augmentation improves the calibrator's performance; however, this fix does not easily generalize over different methods of collecting conflicting answers and evidence sets. Interestingly, training with more realistic conflicting evidence sets (AmbigQA, Situated QA), while being substantially smaller, gen-</p>
<p>eralizes better than simulated conflicting evidence set (Partial Substitution). Training over all types of conflicting evidence sets jointly improves performance over the baseline calibrators only modestly compared to the gains from training on data from each method separately. Future work can explore improving calibrator generalization across different knowledge conflict types.</p>
<h2>6 Related Work</h2>
<p>Recent analysis (Lewis et al., 2021; Krishna et al., 2021) pointed the overlap in training and evaluation dataset inflates question answering performances. Longpre et al. (2021) showed that the reader model tend to memorize entity answers despite the answer mentions are substituted by another entity. We showed that memorization do occur when the model can only have access to one passage, but can be reduced significantly if the model is trained with multiple passages. Concurrent work (Pan et al., 2021) investigates QA models' robustness to misinformation by providing contradicting contexts. They focus on generating conflicting passages, while we focus on understanding how models behave under such settings, including in-depth study of their confidence score.</p>
<p>Recent works evaluated robustness by minimally perturbing input examples (Kaushik et al., 2020; Gardner et al., 2020) to identify models that are invariant under distributional shift. Prior work explored automatically generating such perturbed input (counterfactual data) with heuristics (Ribeiro et al., 2020) or learned models (Wu et al., 2021; Bartolo et al., 2020; Paranjape et al., 2021). Recent work (Du et al., 2022) studies knowledge poisoning for a related task, fact checking. Our perturbation methods are rule-based similar to Ribeiro et al. (2020), but designed specifically for QA task.</p>
<h2>7 Conclusion</h2>
<p>We summarize our findings: Do models ground their answers from retrieved document or parametric knowledge? (Section 3) Current SoTA models ground their answers mostly from retrieved passages, when paired with a high recall retriever (Table 2, 4).</p>
<p>How do models use multiple passages when different passages suggest different answers? (Section 4.1) Models rely on a few, most relevant passages (Table 7), and use parametric knowledge to break ties (Figure 2, Table 18).</p>
<p>How do models behave if some passages are perturbed not to support an answer? (Section 4.2) Models largely ignore semantic perturbations and outputs potential answer entity in the retrieved passages (Table 8).</p>
<p>How is the model's confidence score affected by knowledge conflicts? Confidence score is not sensitive to knowledge conflicts (Table 6, Figure 3), and a separately trained calibrator offers some improvements.</p>
<p>Can we train a model to refrain from returning a single answer when there is conflicting evidence? If we train a calibrator on the conflicting evidence set, calibrator can learn to refrain, but does not generalize to different types of conflicting evidence sets (Table 9).</p>
<p>What should the model do when there is conflicting evidence? We present a partial solution of training a calibrator which learns to abstain from answering when provided conflicting evidence. Future work can explore summarizing and comparing different answers suggested by diverse passages.</p>
<p>Overall, models' limited ability to aggregate conflicting information among its rich knowledge sources encourage future work in this domain.</p>
<h2>Limitations</h2>
<p>Our study is based on current state-of-the-art model on popular benchmark datasets. For other datasets (e.g., datasets where retrieval quality is substantially worse) or different models (Brown et al., 2020; Chowdhery et al., 2022; Rae et al., 2021; Thoppilan et al., 2022) of substantially richer parametric knowledge, our observation that memorization is relatively rare will not hold.</p>
<p>We focus on extractive question answering task, where the answer consists of short entity span. Studying knowledge conflicts in complex question answering tasks where answer is multisentence (Fan et al., 2019) or conditional (Sun et al., 2022) requires future work.</p>
<p>Lastly, most of our knowledge conflicts study (except the settings where we retrieve passages with AmbigQA and SituatedQA) are simulated, and we leave identifying and evaluating model on real-world knowledge conflicts as future work.</p>
<h2>Acknowledgements</h2>
<p>We thank members of UT Austin NLP community for providing feedback. The work is partially supported by a grant from Open Philanthrophy and</p>
<h2>Geogic Research Awar</h2>
<h2>References</h2>
<p>Max Bartolo, A Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662-678.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. ArXiv, abs/1906.03538.</p>
<p>Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pages 785-794, New York, NY, USA. ACM.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to fill in the blanks. arXiv preprint arXiv:2005.05339.
Y. Du, Antoine Bosselut, and Christopher D. Manning. 2022. Synthetic disinformation attacks on automated fact verification systems. ArXiv, abs/2202.09381.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558-3567, Florence, Italy. Association for Computational Linguistics.</p>
<p>Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 1-13, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307-1323, Online. Association for Computational Linguistics.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrievalaugmented language model pre-training. Proceedings of the International Conference on Machine Learning (ICML).</p>
<p>Gautier Izacard and Edouard Grave. 2020. Distilling knowledge from reader to retriever for question answering. arXiv preprint arXiv:2012.04584.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In EACL.</p>
<p>Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), abs/1707.07328.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Amita Kamath, Robin Jia, and Percy Liang. 2020. Selective question answering under domain shift. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Divyansh Kaushik, Eduard Hovy, and Zachary C Lipton. 2020. Learning the difference that makes a difference with counterfactually augmented data. International Conference on Learning Representations (ICLR).</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. ArXiv, abs/2103.06332.
T. Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, C. Alberti, D. Epstein, Illia Polosukhin, J. Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey,</p>
<p>Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics (TACL), 7:453-466.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 6086-6096, Florence, Italy. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, F. Petroni, V. Karpukhin, Naman Goyal, Heinrich Kuttler, M. Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrievaluagmented generation for knowledge-intensive nlp tasks. Proceedings of Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Question and answer test-train overlap in opendomain question answering datasets. In EACL.</p>
<p>Shayne Longpre, Kartik Kumar Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. ArXiv, abs/2109.05052.</p>
<p>Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.</p>
<p>Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. Ambigqa: Answering ambiguous open-domain questions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang Wang. 2021. Contraqa: Question answering under contradicting contexts. arXiv preprint arXiv:2110.07803.</p>
<p>Bhargavi Paranjape, Matthew Lamm, and Ian Tenney. 2021. Retrieval-guided counterfactual generation for qa. ArXiv, abs/2110.07596.</p>
<p>Fabio Petroni, Tim Rocktäschel, A. H. Miller, P. Lewis, A. Bakhtin, Y. Wu, and S. Riedel. 2019. Language models as knowledge bases? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Peng Qi, Timothy Dozat, Yuhao Zhang, and Christopher D. Manning. 2018. Universal dependency parsing from scratch. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 160-170, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:</p>
<p>Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "why should i trust you?": Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</p>
<p>Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of nlp models with checklist.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-toend training of multi-document reader and retriever for open-domain question answering. NeurIPS, abs/2106.05346.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. International Conference on Learning Representations (ICLR), abs/1611.01603.</p>
<p>Darsh J. Shah, Tal Schuster, and Regina Barzilay. 2020. Automatic fact-guided sentence modification. In AAAI.</p>
<p>Haitian Sun, William W. Cohen, and Ruslan Salakhutdinov. 2022. Reasoning over logically interacted conditions for question answering. ArXiv, abs/2205.12898.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. 2021. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6707-6723, Online. Association for Computational Linguistics.</p>
<p>Sohee Yang and Minjoon Seo. 2020. Is retriever merely an approximator of reader? ArXiv, abs/2010.10999.</p>
<p>Michael J.Q. Zhang and Eunsol Choi. 2021. Situatedqa: Incorporating extra-linguistic contexts into qa. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Shujian Zhang, Chengyue Gong, and Eunsol Choi. 2021. Knowing more about questions can help: Improving calibration in question answering. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 1958-1970, Online. Association for Computational Linguistics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#Passages</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">neg. /modal. <br> / fut.</th>
<th style="text-align: center;">negation- <br> polyjuice</th>
<th style="text-align: center;">text- <br> infilling</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">\%Ex.</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$61.14 \%$</td>
<td style="text-align: center;">$57.38 \%$</td>
<td style="text-align: center;">$62.43 \%$</td>
</tr>
<tr>
<td style="text-align: center;">\%Cov.</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$89.55 \%$</td>
<td style="text-align: center;">$88.18 \%$</td>
<td style="text-align: center;">$89.68 \%$</td>
</tr>
<tr>
<td style="text-align: center;">\%Ex. <br> (100\% Cov.)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$85.77 \%$</td>
<td style="text-align: center;">$82.90 \%$</td>
<td style="text-align: center;">$86.07 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$66.93 \%$</td>
<td style="text-align: center;">$61.12 \%$</td>
<td style="text-align: center;">$68.25 \%$</td>
</tr>
</tbody>
</table>
<p>Table 10: Data statistics for different perturbations schemes. The first two rows are the numbers of examples, shown in percentage out of the examples that FiD can answer correctly. The third and fourth rows shows the percentage of gold answer span covered (valid for perturbation) in the chosen examples. The last two rows shows the percentage of valid examples we could get if all the gold answer spans are perturbed.</p>
<h2>A Appendix</h2>
<h2>A. 1 Calibrator Hyperparameter</h2>
<p>The input to the calibrator is the concatenation of the generation probability and the encoder feature representation averaged across length, and the output is a score indicating the probability of the model correctly predicting the answer. For each dataset, we reserve 4 K examples of the training set for validation, and trained our calibrator on the remaining data. Hyperparameters are selected based on AUROC on validation set.</p>
<p>We use 100 boosting rounds, subsample ratio of 0.5 and learning rate of 0.5 . The same subsample ratio is applied for constructing each tree, for each level and for each split.</p>
<h2>A. 2 Model and Training Details</h2>
<p>The Fusion-in-Decoder (FiD) model consist of a retriever and a reader module. The retriever (Karpukhin et al., 2020) is a BERT biencoder model, which calculate the similarity between the question $q$ and each of the passages $\left{p_{i}\right}$ in the knowledge source and output the most similar ones. The similarity is computed as the dot product of the encoded vectors</p>
<p>$$
E_{Q}(q)^{T} E_{P}\left(p_{i}\right)
$$</p>
<p>where $E_{Q}$ is the question encoder and $E_{P}$ is the passage encoder.</p>
<p>The reader module is a pretrained T5-large (Raffel et al., 2020), an encoder-decoder model containing 770M parameters. Each passage is concatenated with the question and truncated to 250 word pieces. For our experiments finetuning FiD,</p>
<table>
<thead>
<tr>
<th>$\%$</th>
<th>Random Entity</th>
<th></th>
<th>AmbigQA Entity</th>
<th></th>
<th>Random Entity</th>
<th></th>
<th>SituatedQA Entity</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Perturbed</td>
<td>Original</td>
<td>Substitute</td>
<td>Original</td>
<td>Substitute</td>
<td>(on SituatedQA set)</td>
<td></td>
<td>Original</td>
<td>Substitute</td>
</tr>
<tr>
<td>25</td>
<td>74.90</td>
<td>6.37</td>
<td>75.45</td>
<td>9.15</td>
<td>76.74</td>
<td>4.65</td>
<td>80.00</td>
<td>7.27</td>
</tr>
<tr>
<td>50</td>
<td>51.79</td>
<td>24.70</td>
<td>51.11</td>
<td>27.68</td>
<td>55.81</td>
<td>16.28</td>
<td>56.36</td>
<td>25.45</td>
</tr>
<tr>
<td>75</td>
<td>27.88</td>
<td>43.03</td>
<td>25.22</td>
<td>46.21</td>
<td>46.51</td>
<td>13.95</td>
<td>38.18</td>
<td>43.64</td>
</tr>
<tr>
<td>100</td>
<td>2.39</td>
<td>65.34</td>
<td>5.80</td>
<td>63.17</td>
<td>4.65</td>
<td>39.53</td>
<td>14.55</td>
<td>58.18</td>
</tr>
</tbody>
</table>
<p>Table 11: Entity substitution results on subsets of NQ-Open. We perform random entity substitution on the AmbigQA and SituatedQA sets for fair comparisons between different sources of substitute answers.
we train the reader module with 1,20 , and 50 evidence passages. To train the reader, we use the AdamW optimizer (Loshchilov and Hutter, 2018) and a learning rate of $5 \cdot 10^{-5}$ with linear warmup of 8000 steps followed by linear decay to zero. The total training steps is 300 k , and the final model checkpoint is selected based on exact match score on NQ Open development set. We only use batch size of 1 due to memory constraints. The models take roughly 7 GPU days to train on a Quadro RTX 8000 machine.</p>
<p>The closed-book question answering (CBQA) model is trained using a T5-large pretrained model, with a batch size of 32, 500k total training steps, and all the other hyperparameters the same as FiD reader models. It roughly take 2 GPU days to train on a Quadro RTX 8000 machine.</p>
<h2>A. 3 Perturbation Coverage</h2>
<p>As mentioned in Section 4, if the root token of the answer sentence is not a verb, then we ignore that sentence, and thus some examples would be excluded. The first row shows the percentage of valid examples after applying the rules mentioned in Section 4. We consider it valid example if one of the gold answer span can be perturbed. The corresponding percentage of perturbed gold answer spans is shown in the third row. A small portion of gold answer spans remain unchanged after performing the perturbation. For the second and fourth row it shows the same except the model has access to 100 passages. The percentage of valid examples are much higher since we consider the example valid if one of the gold answer spans in any of the passages can be perturbed. The last two rows show the percentage of examples where all gold answer spans in all the retrieved passages can be perturbed.</p>
<h2>A. 4 Technical Details on Semantic Perturbations</h2>
<p>For perturbation schemes except text infilling, we first identify the root token's part-of-speech tag. If it is in one of [VB, VBP, VBZ], then we treat it as the present tense, and modify the verb accordingly. (e.g. $\mathrm{V} \rightarrow$ "does not V"/"do not V" for negation, $\mathrm{V} \rightarrow$ "may V" for modality, $\mathrm{V} \rightarrow$ "will V" for future tense) The lemmatized verb forms after "will" and "may" are obtained by the "WordNetLemmatizer" class in nltk ${ }^{10}$. We also identify ["is", "am", "are"] and modify the verbs into their corresponding forms. If the part-of-speech tag is VBD, then it is in past tense and the root token is modified similarly to present tense. Lastly, if the part-of-speech tag is VBN or VBG, then it is present/past participle or gerund. We then identify the be-verbs and/or ["had", "have", "has"], and perform modifications accordingly.</p>
<h2>A. 5 Model Tested on NQ Open Subset</h2>
<p>Both AmbigQA and SituatedQA annotate subsets of NQ Open. To ensure identical data distribution and isolate the effect of different substitute answers, we report results of random entity substitution on AmbigQA set and SitutatedQA set respectively. We present the results in Table 11. For AmbigQA subset, different substitute entity types (random or alternative valid entity) do not seem to affect the results too much. However, the model seems to bias toward the substitute answer more with valid alternative entity substitutions on SituatedQA subset, indicating the parametric knowledge of model do know which answers are more likely to be correct. One possible explanation is that AmbigQA answers do not always take the same form as the original ones (e.g. 76th season and 1995 in Table 3).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>$\%$</th>
<th>NQ Open</th>
<th></th>
<th>AmbigQA</th>
<th></th>
<th>SituatedQA</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>perturbed</td>
<td>Original</td>
<td>Substitute</td>
<td>Original</td>
<td>Substitute</td>
<td>Original</td>
<td>Substitute</td>
</tr>
<tr>
<td>25</td>
<td>67.35</td>
<td>9.51</td>
<td>72.16</td>
<td>7.21</td>
<td>66.67</td>
<td>0.00</td>
</tr>
<tr>
<td>50</td>
<td>45.50</td>
<td>27.51</td>
<td>40.20</td>
<td>34.02</td>
<td>33.33</td>
<td>33.33</td>
</tr>
<tr>
<td>75</td>
<td>21.85</td>
<td>48.84</td>
<td>22.68</td>
<td>41.23</td>
<td>0.00</td>
<td>66.67</td>
</tr>
<tr>
<td>100</td>
<td>0.00</td>
<td>68.12</td>
<td>1.03</td>
<td>63.92</td>
<td>0.00</td>
<td>66.67</td>
</tr>
</tbody>
</table>
<p>Table 12: Exact match score of substituting different number of passages on NAO sets.</p>
<table>
<thead>
<tr>
<th>$\%$</th>
<th>Exact Match</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>perturbed</td>
<td>Original</td>
<td>Substitute</td>
</tr>
<tr>
<td>25</td>
<td>80.00</td>
<td>7.27</td>
</tr>
<tr>
<td>50</td>
<td>60.00</td>
<td>25.45</td>
</tr>
<tr>
<td>75</td>
<td>41.82</td>
<td>43.64</td>
</tr>
<tr>
<td>100</td>
<td>18.18</td>
<td>60.00</td>
</tr>
</tbody>
</table>
<p>Table 13: Results of substituting different number of passages on SituatedQA. The substitute answer is randomly selected from the SituateQA answer set and is not in the original ansewr set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Change Type</th>
<th style="text-align: left;">Gen. Prob.</th>
<th style="text-align: left;">Calibration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">negation</td>
<td style="text-align: left;">$65.94 \%$</td>
<td style="text-align: left;">$70.28 \%$</td>
</tr>
<tr>
<td style="text-align: left;">modality</td>
<td style="text-align: left;">$62.75 \%$</td>
<td style="text-align: left;">$66.34 \%$</td>
</tr>
<tr>
<td style="text-align: left;">future</td>
<td style="text-align: left;">$58.87 \%$</td>
<td style="text-align: left;">$62.92 \%$</td>
</tr>
<tr>
<td style="text-align: left;">text-infilling</td>
<td style="text-align: left;">$60.56 \%$</td>
<td style="text-align: left;">$64.36 \%$</td>
</tr>
</tbody>
</table>
<p>Table 14: The percentage of examples in which model confidence dropped after perturbation; i.e., the model confidence when predicting the original example is higher than the perturbed example. Model confidence is measured with generation probability/calibration.</p>
<h2>A. 6 Answer Entity Sampling Details</h2>
<p>When substituting with AmbigQA answers, we consider only the examples with multiple valid answers. For each example, we randomly sample one answer not in the original answer set of NQ as the substitute answer. For substitution with SituatedQA answers, we select the most recent answer as substitute answer. We also include the result of randomly sample an answer from SituatedQA answer set in Table 13.</p>
<h2>A. 7 Full Results on No Answer Overlap Set</h2>
<p>Table 12 contain the full results on NAO set for NQ Open, AmbigQA, and SituateQA.</p>
<h2>A. 8 Confidence Study Full Results</h2>
<p>Table 14 contains the full results for confidence study on adversarial semantic perturbation.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The ratio of calibration score after perturbation to that before perturbation, in log scale. The occurrences of examples of different ratio are plotted in terms of probability density (the area under curve is sum to 1). The distributions are bell-shaped, but shift slightly towards negative x-axis.</p>
<h2>A. 9 Domain Adaptation Results for Entity Substitution</h2>
<p>We would like to study the memorization issue when the model is tested on out-of-domain datasets. Following the setting in Section 3, we substitute the answer entity mentions in the retrieved passages with random entities of the same type after the retrieval step. The only difference is that the reader model is trained on a different domain. We evaluate FiD reader model which is trained with NQ-Open on TriviaQA dataset, and vice versa. The results are presented in Table 15 and 16. The memorization ratio is still low with high-recall retrievers for both settings, indicating that the model actually relies on the retrieved passages under the distribution shift.</p>
<h2>B Further Analysis</h2>
<p>We further examine our results, focusing on the quality of substitute answer in entity substitution study and which parametric knowledge (pretraining vs. fine-tuning) was used.</p>
<p>Improving Substitute Entities Prior work (Longpre et al., 2021) substitutes answer entity with another entity with same coarse</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;"># Pass. <br> train / inf.</th>
<th style="text-align: center;">$\%$ <br> ex.</th>
<th style="text-align: center;">Ans. <br> R</th>
<th style="text-align: center;">Exact Match <br> Orig.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$M_{R}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$1 / 1$</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">22.9</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$5 / 1$</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">8.7</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$5 / 5$</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">9.2</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$20 / 1$</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">5.8</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$20 / 20$</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">6.7</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$50 / 1$</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">5.6</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$50 / 50$</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">7.0</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$100 / 1$</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$100 / 100$</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">8.6</td>
</tr>
</tbody>
</table>
<p>Table 15: Exact Match / Memorization Ratio for FiD model trained on NQ-Open with different amount of passages and evaluated on TriviaQA. The memorization is still low for domain adapted models, when provided with multiple retrieved passages.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;"># Pass. <br> train / inf.</th>
<th style="text-align: center;">$\%$ <br> ex.</th>
<th style="text-align: center;">Ans. <br> R</th>
<th style="text-align: center;">Exact Match <br> Orig.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$M_{R}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$1 / 1$</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$5 / 1$</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">9.5</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$5 / 5$</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">9.0</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$20 / 1$</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">7.6</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$20 / 20$</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">6.6</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$50 / 1$</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">5.2</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$50 / 50$</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">5.9</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$100 / 1$</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: left;">$100 / 100$</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">5.6</td>
</tr>
</tbody>
</table>
<p>Table 16: Exact Match / Memorization Ratio for FiD model trained on TriviaQA with different amount of passages and evaluated on NQ-Open.
entity type. This makes substitute entities sometimes unreasonable, despite better than randomly sampling entities without type constraint. For example, "Heartbreak Hotel" was substituted as an answer to the following question "who did the lions play on thanksgiving last year".</p>
<p>We make perturbation more realistic by substituting with alternative answer from two datasets, AmbigQA (Min et al., 2020) and SituatedQA (Zhang and Choi, 2021), which augmented existing NQ open dataset. Both datasets annotated valid alternative answers for different interpretation of the same question (AmbigQA) and answers belonging to different temporal contexts (SituatedQA) for NQOpen dataset. We sample these additional answers as a new answer to inject (details in Appendix A.6).</p>
<p>Table 17 presents perturbation results with valid entities sourced from AmbigQA and SituatedQA. We identify a surprising trend - that model outputs original answers more frequently when substituted with better alternatives. This contradicts our intuition as model should be less hesitant to choose new substitute answer as they are also valid answer</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity source</th>
<th style="text-align: center;">AmbigQA (N=448)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SituatedQA (N=55)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\%$ per.</td>
<td style="text-align: center;">Ori.</td>
<td style="text-align: center;">Sub.</td>
<td style="text-align: center;">Ori.</td>
<td style="text-align: center;">Sub.</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">74.11</td>
<td style="text-align: center;">10.40</td>
<td style="text-align: center;">77.45</td>
<td style="text-align: center;">8.36</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50.71</td>
<td style="text-align: center;">26.65</td>
<td style="text-align: center;">56.73</td>
<td style="text-align: center;">25.09</td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">25.40</td>
<td style="text-align: center;">47.05</td>
<td style="text-align: center;">33.09</td>
<td style="text-align: center;">44.73</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">5.80</td>
<td style="text-align: center;">63.17</td>
<td style="text-align: center;">14.55</td>
<td style="text-align: center;">58.18</td>
</tr>
</tbody>
</table>
<p>Table 17: Results of substituting different proportion of 100-retrieved passages on NQ-Open where entities are derived from AmbigQA and SituatedQA dataset. The number next to the entity refers to the number of examples in this evaluation set after filtering.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">\% per.</th>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">NAO</th>
<th style="text-align: right;">AO</th>
<th style="text-align: right;">AO\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: left;">TQA (Random Entity)</td>
<td style="text-align: right;">75.25</td>
<td style="text-align: right;">81.40</td>
<td style="text-align: right;">86.66</td>
</tr>
<tr>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: left;">NQ (Random Entity)</td>
<td style="text-align: right;">61.83</td>
<td style="text-align: right;">70.92</td>
<td style="text-align: right;">85.93</td>
</tr>
<tr>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: left;">w/ AmbigQA Entity</td>
<td style="text-align: right;">55.08</td>
<td style="text-align: right;">66.40</td>
<td style="text-align: right;">78.35</td>
</tr>
<tr>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: left;">w/ SituatedQA Entity</td>
<td style="text-align: right;">50.00</td>
<td style="text-align: right;">71.36</td>
<td style="text-align: right;">94.55</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: left;">TQA (Random Entity)</td>
<td style="text-align: right;">2.75</td>
<td style="text-align: right;">9.37</td>
<td style="text-align: right;">86.66</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: left;">NQ (Random Entity)</td>
<td style="text-align: right;">0.45</td>
<td style="text-align: right;">4.14</td>
<td style="text-align: right;">85.93</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: left;">w/ AmbigQA Entity</td>
<td style="text-align: right;">1.59</td>
<td style="text-align: right;">10.16</td>
<td style="text-align: right;">78.35</td>
</tr>
<tr>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: left;">w/ SituatedQA Entity</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">21.05</td>
<td style="text-align: right;">94.55</td>
</tr>
</tbody>
</table>
<p>Table 18: Memorization ratio ( $M_{R}$ of substituting different number of passages on NQ-Open No Answer Overlap (NAO) / Answer overlap (AO) set of NQ-Open and TriviaQA. AO\% signifies the percentage of examples that belong to AO set for each subset.
to the question, for different contexts. We further investigate this issue below.</p>
<p>Does parametric knowledge come from pretraining or fine-tuning? Some memorization (2$15 \%$ ) remains even after all the evidence documents are perturbed, and model is biased toward the original answer under partial substitution. We aim to identify whether it comes from pretraining or finetuning of the reader model by using the evaluation data splits from prior work (Lewis et al., 2021): questions where answers were seen (Answer Overlap (AO)) and questions where answers were unseen (No Answer Overlap (NAO)). If memorization ratio is higher on AO set compared to NAO set, we can hypothesize that memorization mostly happens during fine-tuning compared to pre-training. ${ }^{11}$</p>
<p>Table 18 presents results for $50 \%$ and $100 \%$ substitution setting. ${ }^{12}$ This study shed lights on mysterious trend: there were more examples with answer overlap in AmbigQA/SituatedQA subset. If we perturb all the evidence documents, the model exhibit little to no memorization on NAO portion. We can</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>thus infer that memorization effect comes almost exclusively from fine-tuning. When accounting for different proportion of answer overlap examples in the subsets, memorization ratio is lower in AmbigQA/SituatedQA NAO set. This suggests that model uses parametric knowledge - which answer candidate is more reasonable - in a subtle way, even when behaving as a copying model.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ Earlier study (Longpre et al., 2021) in a single document setting also reports memorization is more severe in AO set.
${ }^{12}$ See Appendix A. 7 for $25 \%$ and $75 \%$ substitution setting.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ Semantic perturbation details (e.g., statistics of $\%$ of valid examples after perturbation) in the Appendix A.3.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
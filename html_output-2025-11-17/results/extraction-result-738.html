<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-738 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-738</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-738</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-227254549</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.02055v1.pdf" target="_blank">Intervention Design for Effective Sim2Real Transfer</a></p>
                <p><strong>Paper Abstract:</strong> The goal of this work is to address the recent success of domain randomization and data augmentation for the sim2real setting. We explain this success through the lens of causal inference, positioning domain randomization and data augmentation as interventions on the environment which encourage invariance to irrelevant features. Such interventions include visual perturbations that have no effect on reward and dynamics. This encourages the learning algorithm to be robust to these types of variations and learn to attend to the true causal mechanisms for solving the task. This connection leads to two key findings: (1) perturbations to the environment do not have to be realistic, but merely show variation along dimensions that also vary in the real world, and (2) use of an explicit invariance-inducing objective improves generalization in sim2sim and sim2real transfer settings over just data augmentation or domain randomization alone. We demonstrate the capability of our method by performing zero-shot transfer of a robot arm reach task on a 7DoF Jaco arm learning from pixel observations.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e738.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e738.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal discovery method that finds sets of variables whose conditional distribution for a target is invariant across environments, using hypothesis tests to identify causal parents under interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference using invariant prediction: Identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ICP searches for subsets S of observed variables such that the conditional distribution P(target | S) is invariant across multiple environments (interventions). It uses statistical hypothesis tests to check invariance and returns the intersection of all subsets that pass the invariance tests as the set of causal parents; identification relies on interventions across environments and the method provides confidence intervals for effects. Computational complexity is super-exponential in the number of variables, and it requires access to the true variables (not raw pixels).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General multi-environment / intervention datasets (mentioned abstractly)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Originally formulated for settings with multiple observational/interventional environments where variables are available and environments differ by known interventions; not explicitly tied to an interactive simulator in this paper but applicable when multiple domains (interventions) exist.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection via invariance testing: finds variable subsets whose predictive relationship to the target remains invariant across interventions, thereby excluding variables that are spuriously correlated (distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations / non-causal predictors that vary across environments; confounding to the extent that invariance tests can detect noninvariant predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical hypothesis tests for invariance of conditional distributions across environments (test whether P(Y|S) changes across domains).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation via rejecting non-invariant subsets; only subsets that pass invariance tests are retained as causal parents; confidence intervals derived from tests provide formal uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as the canonical algorithm that justifies using interventions to identify causal parents via invariance; paper notes ICP requires access to true variables and is computationally infeasible (super-exponential) in realistic high-dimensional / pixel settings, motivating representation-based alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e738.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e738.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation learning approach that seeks representations for which there exists a classifier that is simultaneously optimal across multiple environments, implemented via a gradient-based invariant penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Risk Minimization (IRM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IRM formulates causal discovery as finding a data representation φ and classifier w such that w ∘ φ is simultaneously optimal (minimizes risk) in all training environments. Practically, the constrained optimization is approximated by adding a gradient-based penalty that enforces the classifier's optimality condition (e.g., gradient of risk wrt classifier parameters is zero) across domains, encouraging learned features to be invariant and causal.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-environment supervised / multi-task settings (abstract)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Settings with multiple labeled domains/environments; in this paper IRM is cited as a gradient-based method for invariant representation learning but not directly applied to the pixel-based sim2real tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization/constraint enforcing invariance of predictor (representation+classifier) across environments, which suppresses features that are predictive only in some environments (spurious features).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant / unstable predictors whose relation to target changes across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit: non-invariant features are detected because they cause violations of the invariance constraint (manifested via non-zero gradient penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalty-based (gradient penalty) that discourages reliance on features not supporting invariant optimal classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a gradient-based method for learning invariant representations across tasks; the paper references IRM as conceptual background for invariance objectives but notes different practical approximations (e.g., REx) are used in their RL setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e738.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e738.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>V-REx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Risk Extrapolation (V-REx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that enforces invariance by penalizing variance of per-environment risks, encouraging models with equal risk across training environments to improve out-of-distribution generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Out-of-distribution generalization via risk extrapolation (rex)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>V-REx (Variance Risk Extrapolation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>V-REx computes a per-environment risk (here: mean-square error for reward and transition models in latent space) and adds to the objective a penalty proportional to the variance of these risks across environments, weighted by β; the objective is sum of mean risks plus β·Var({R_e}). As β increases, the optimizer is driven to find representations and predictors whose performance is equalized across domains, thereby preferring invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-task simulated environments (rendering and post-rendering interventions) used in sim2sim and sim2real experiments (Meta-World, Sawyer tasks, Jaco reach)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated MDPs where rendering parameters are randomized to produce multiple training domains; environments are episodic control tasks (robot manipulation) with access to simulator to create rendering interventions; allows active creation of multiple domains but not active interventional selection in V-REx usage here.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization via variance penalty on per-domain risks; features that cause variable risk across domains (likely spurious) are discouraged.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant visual features / spurious correlations that cause variable predictive performance across rendering environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection by measuring variance of per-domain risk: high variance indicates features/predictors not invariant and thus potentially spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalize variance of risks across tasks (β·Var), causing optimization to prefer features with equalized risk and downweight reliance on domain-specific signals.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: When used (IBIT-REx) the paper reports improved generalization to unseen visual domains and higher sim2real success compared to ERM/baselines; exact numeric metrics are reported in the paper's tables/figures but not enumerated in the text body.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Qualitative baseline (β=0 / ERM) overfits to training environments and fails to generalize to unseen test domains; DrQ and SAC baselines perform worse than IBIT-REx on unseen environments according to reported plots and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adding the V-REx penalty (IBIT-REx) improves zero-shot generalization to unseen rendering variants compared to ERM/data-augmentation-alone; however, V-REx (REx) can suffer from training instability and hyperparameter sensitivity (penalty magnitude, annealing).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e738.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e738.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bisimulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bisimulation metrics (behavioral equivalence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-abstraction / causal-representation approach that groups states by behavioral equivalence using a pseudometric combining immediate reward differences and a Wasserstein distance over next-state distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Bisimulation metrics (and DBC approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Bisimulation metric d(s_i,s_j)=max_a (1-c)|R(si,a)-R(sj,a)| + c·W1(P(.|si,a),P(.|sj,a); d). The distance measures behavioral (causal) similarity; in high-dimensional/pixel settings the metric is approximated via learned encoders and neural critics. In this paper, the authors use a learned bisimulation objective in latent space (Siamese encoding) and use DBC (a neural approximation cited) to find the coarsest bisimulation partition under interventions, enforcing that visually different but behaviorally equivalent observations map close in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated MDPs with rendering/post-rendering interventions (Meta-World Sawyer tasks, Jaco 7DoF reacher)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive control environments (robot manipulation) with full MDP dynamics where the emission mapping q is changed across domains (camera, colors, textures) but underlying dynamics and rewards remain the same; simulator access allows generation of many rendering interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Representation learning that enforces behavioral equivalence: encoder training objective collapses states that are bisimilar (same reward and transition behavior) regardless of visual distractors, effectively discarding irrelevant visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant visual variation (background textures, colors, camera), measurement-level confounders in observations that do not affect reward/dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection via computing/approximating bisimulation distance: large reward/transition discrepancies indicate behaviorally relevant differences; small distances despite visual differences indicate distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Encoder loss (Siamese-style) that minimizes latent distance subject to reward/transition consistency; downweights visual features that increase latent distances without corresponding behavioral differences.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Theoretical link: Zhang et al. result and Theorem 2 in paper assert that the causal ancestors of reward correspond to the coarsest bisimulation partition; thus if states are collapsed by bisimulation, spurious visual correlations that are not causal will be excluded from causal features.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: Bisimulation-based IBIT improves latent clustering of behaviorally equivalent states (t-SNE comparison) and yields better sim2sim and sim2real generalization than non-bisimulation baselines; ablations show bisimulation helps under incomplete interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without bisimulation objective, encoders (e.g., DrQ baseline) learn representations correlated with critic value and fail to cluster equivalent states across domains, leading to poorer generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning bisimulation-based invariances in latent space causes visually different but behaviorally equivalent observations to map together, which weakens spurious correlations from visual distractors and improves zero-shot transfer to unseen rendering variants including sim2real.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e738.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e738.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DBC (neural approximation for bisimulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned deep approximation (cited as DBC) used in this work to compute/approximate the coarsest bisimulation partition via neural encoders, enabling bisimulation-based invariance learning in high-dimensional observation spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DBC (neural bisimulation approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>DBC is used as a practical neural method to approximate bisimulation distances and produce a coarsest bisimulation partition in pixel-based control tasks. It trains an encoder (Siamese architecture) with objectives combining latent-distance, reward differences, and predicted latent-next-state errors, often using Wasserstein-like penalties or learned transition models to approximate the bisimulation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pixel-based simulated control tasks (Meta-World Sawyer tasks, Jaco reach) with rendering randomizations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated robotics environments with rendering interventions; DBC operates on encoded observations (pixels) and uses environment rollouts collected under varied renderings to learn behaviorally-relevant latent metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Representation collapse of visually variant but behaviorally equivalent observations via a bisimulation-inspired encoder loss; uses learned dynamics/reward predictors in latent space to enforce behavioral equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Visual distractors (background, texture, color) that do not change underlying reward/dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit via learned bisimulation objective: if visual differences do not alter reward/transition predictions in latent space, they are treated as distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Loss terms that minimize latent distance for behaviorally equivalent states while penalizing reward/transition mismatches; this reduces the encoder's sensitivity to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Reported qualitatively as improving generalization and enabling IBIT to outperform baselines on unseen visual domains; exact numeric values are in the paper's results (figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines without DBC/bisimulation produce latent spaces that reflect value estimates and do not align equivalent states, yielding worse transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DBC (neural bisimulation) enables practical enforcement of behavioral invariance in pixel-based RL and is a core component of IBIT's ability to ignore visual distractors in sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e738.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e738.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intervention-based Invariant Transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed method that combines rendering/post-rendering interventions (domain randomization and data augmentation) with a bisimulation-inspired invariance objective to learn latent representations robust to visual distractors for sim2sim and sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IBIT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IBIT trains a pixel encoder φ with a bisimulation-metric-inspired objective in a multi-task setting: draw pairs of states under the same policy across randomized renderings, minimize J(φ)=||φ(x_i)-φ(x_j)||_1 - |R(φ(x_i))-R(φ(x_j))| - γ·W2(P(·|φ(x_i)),P(·|φ(x_j)))^2, combined with standard actor-critic (SAC/DrQ) training on the latent states. It uses both rendering interventions (simulator changes) and post-rendering augmentations, and implements a Siamese network to collapse behaviorally equivalent observations without requiring environment ids.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated robotics tasks (Meta-World Sawyer manipulation tasks; Jaco 7DoF reacher) with visual domain randomization and data augmentation; evaluated in sim2sim and zero-shot sim2real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic MDPs for robot control where the emission mapping q is varied across training domains (background, textures, camera), allowing generation of many interventions; agent acts and gathers rollouts (active control) but IBIT does not actively select interventions—it trains across sampled randomized domains.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Bisimulation-based representation learning: Siamese encoder loss that collapses visually different but behaviorally equivalent observations; domain randomization/data augmentation provide interventions that enable the bisimulation objective to distinguish causal features from distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Visual distractors (background color/texture, camera views) and other observation-level spurious correlations that do not affect reward/dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via bisimulation loss: if differences in observation do not lead to reward or transition differences in latent predictions, they are treated as distractors and collapsed.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Encoder training minimizes latent distances for behaviorally equivalent observations and penalizes representations that reflect spurious visual differences; this downweights spurious visual features in the learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Empirical refutation via improved zero-shot performance on unseen domains and sim2real success: interventions that do not change bisimulation are provably valid (Theorem 2), and collapse in latent space provides evidence that visual signals were non-causal.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Uses simulator-sampled rendering interventions (multi-task batching of randomized environments) rather than actively chosen experiments; sampling rate and training env batch size control exposure to interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: IBIT exhibits substantially better generalization to unseen visual domains and improved sim2real transfer success compared to baselines (DrQ, SAC) in the paper's experiments; t-SNE and unseen-env performance plots show clear gains but exact numeric metrics are presented in tables/figures in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline methods without the bisimulation invariance (DrQ, SAC) overfit to training environments and perform poorly on unseen test domains; policies trained without IBIT fail more often in sim2real trials according to the reported comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining interventions (rendering + post-rendering) with an explicit bisimulation invariance objective yields better sim2sim and sim2real generalization than data-augmentation or domain randomization alone; interventions need not be realistic, only to vary along dimensions present in real world, and bisimulation-based IBIT does not require environment ids.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e738.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e738.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBIT-REx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IBIT with Risk Extrapolation (IBIT-REx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of IBIT that adds a V-REx penalty to explicitly equalize per-environment risks (reward and transition prediction MSE) across randomized domains, enforcing stronger invariance and improving generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IBIT-REx</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IBIT-REx augments the IBIT bisimulation encoder objective with the V-REx penalty: define per-environment risks R_e(φ) = MSE(reward)+MSE(transition) in latent space, and optimize sum_e R_e + β·Var({R_e}). This requires environment ids (multi-task setting) and forces the learned latent dynamics/reward models to have equalized risk across domains, thereby discovering stable features useful for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated robotics tasks as IBIT (Meta-World Sawyer tasks, Jaco 7DoF reach) with multi-domain training and evaluation on unseen test domains and on a real robot for sim2real.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated MDPs where multiple training domains (renderings) are sampled; IBIT-REx uses these environment ids to compute per-domain risks and apply the variance penalty; evaluated zero-shot on unseen rendered domains and on a real Jaco arm.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Combined bisimulation representation collapse and risk-variance penalization: bisimulation removes behaviorally irrelevant variance, REx equalizes per-domain prediction risks to suppress reliance on domain-specific (spurious) signals.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Visual distractors and domain-specific features that cause varying prediction risk across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>High variance in per-environment latent reward/transition MSE signals presence of domain-specific/spurious predictors; variance measurement serves as detection signal.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalize variance of per-domain risks (β·Var) so the optimizer reduces reliance on features that increase risk variance across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Requires environment ids and explicit per-domain risk estimation; sampling of domain batches is used rather than actively selecting interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: IBIT-REx generally improves over IBIT and baselines in generalization to unseen visual domains and shows higher sim2real transfer success in reported experiments; the paper notes improved performance but also reports training volatility and hyperparameter sensitivity for REx penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>IBIT (without REx) performs well but IBIT-REx often further improves unseen-domain performance; baselines that lack invariance objectives perform worse.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adding a V-REx penalty to bisimulation-based representation learning strengthens invariance and generalization, but REx can cause empirical instability and requires careful hyperparameter tuning; method requires environment ids.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e738.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e738.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang-BMDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant causal prediction for block MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent approach connecting invariant prediction to block MDPs: learns invariant representations when only emission functions change across tasks by disentangling task-agnostic and task-specific factors, often using reconstruction or invariance losses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant causal prediction for block mdps</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant causal prediction for Block MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>This line of work (Zhang et al.) extends invariant prediction ideas to Block MDPs where underlying latent state transitions are shared but emission functions vary; methods aim to learn a representation that recovers the latent causal state by enforcing invariance across tasks, e.g., via reconstruction-based losses or invariance objectives. The paper references the result that causal ancestors of reward correspond to the coarsest bisimulation partition.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Block MDP family (rendering varies, latent dynamics/rewards fixed) — conceptual environment category used as theory for sim2real</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive MDPs where only the emission mapping q_d changes across domains (rendering/camera/background), matching the sim2real setup; allows many domains but assumes shared latent structure and dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Learning task-agnostic latent states by enforcing invariance across emission changes and disentangling task-specific rendering factors, often via reconstruction and invariance losses.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Rendering-level distractors and observation-level spurious variations that do not alter latent dynamics or reward.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Invariance across emission changes: variables that vary with emission but not with reward/dynamics are treated as task-specific and excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Training objectives that separate and ignore task-specific emission features while retaining task-agnostic causal features.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as theoretical and practical background: Block MDP invariance results justify using bisimulation/invariance objectives in sim2real; Zhang et al.'s connection between bisimulation and invariant prediction is used to motivate IBIT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intervention Design for Effective Sim2Real Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal inference using invariant prediction: Identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Invariant Risk Minimization <em>(Rating: 2)</em></li>
                <li>Out-of-distribution generalization via risk extrapolation (rex) <em>(Rating: 2)</em></li>
                <li>Invariant causal prediction for block mdps <em>(Rating: 2)</em></li>
                <li>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels <em>(Rating: 1)</em></li>
                <li>Designing data augmentation for simulating interventions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-738",
    "paper_id": "paper-227254549",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "ICP",
            "name_full": "Invariant Causal Prediction",
            "brief_description": "A causal discovery method that finds sets of variables whose conditional distribution for a target is invariant across environments, using hypothesis tests to identify causal parents under interventions.",
            "citation_title": "Causal inference using invariant prediction: Identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant Causal Prediction (ICP)",
            "method_description": "ICP searches for subsets S of observed variables such that the conditional distribution P(target | S) is invariant across multiple environments (interventions). It uses statistical hypothesis tests to check invariance and returns the intersection of all subsets that pass the invariance tests as the set of causal parents; identification relies on interventions across environments and the method provides confidence intervals for effects. Computational complexity is super-exponential in the number of variables, and it requires access to the true variables (not raw pixels).",
            "environment_name": "General multi-environment / intervention datasets (mentioned abstractly)",
            "environment_description": "Originally formulated for settings with multiple observational/interventional environments where variables are available and environments differ by known interventions; not explicitly tied to an interactive simulator in this paper but applicable when multiple domains (interventions) exist.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection via invariance testing: finds variable subsets whose predictive relationship to the target remains invariant across interventions, thereby excluding variables that are spuriously correlated (distractors).",
            "spurious_signal_types": "Spurious correlations / non-causal predictors that vary across environments; confounding to the extent that invariance tests can detect noninvariant predictors.",
            "detection_method": "Statistical hypothesis tests for invariance of conditional distributions across environments (test whether P(Y|S) changes across domains).",
            "downweighting_method": null,
            "refutation_method": "Refutation via rejecting non-invariant subsets; only subsets that pass invariance tests are retained as causal parents; confidence intervals derived from tests provide formal uncertainty quantification.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as the canonical algorithm that justifies using interventions to identify causal parents via invariance; paper notes ICP requires access to true variables and is computationally infeasible (super-exponential) in realistic high-dimensional / pixel settings, motivating representation-based alternatives.",
            "uuid": "e738.0",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "IRM",
            "name_full": "Invariant Risk Minimization",
            "brief_description": "A representation learning approach that seeks representations for which there exists a classifier that is simultaneously optimal across multiple environments, implemented via a gradient-based invariant penalty.",
            "citation_title": "Invariant Risk Minimization",
            "mention_or_use": "mention",
            "method_name": "Invariant Risk Minimization (IRM)",
            "method_description": "IRM formulates causal discovery as finding a data representation φ and classifier w such that w ∘ φ is simultaneously optimal (minimizes risk) in all training environments. Practically, the constrained optimization is approximated by adding a gradient-based penalty that enforces the classifier's optimality condition (e.g., gradient of risk wrt classifier parameters is zero) across domains, encouraging learned features to be invariant and causal.",
            "environment_name": "Multi-environment supervised / multi-task settings (abstract)",
            "environment_description": "Settings with multiple labeled domains/environments; in this paper IRM is cited as a gradient-based method for invariant representation learning but not directly applied to the pixel-based sim2real tasks.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization/constraint enforcing invariance of predictor (representation+classifier) across environments, which suppresses features that are predictive only in some environments (spurious features).",
            "spurious_signal_types": "Irrelevant / unstable predictors whose relation to target changes across environments.",
            "detection_method": "Implicit: non-invariant features are detected because they cause violations of the invariance constraint (manifested via non-zero gradient penalty).",
            "downweighting_method": "Penalty-based (gradient penalty) that discourages reliance on features not supporting invariant optimal classifiers.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as a gradient-based method for learning invariant representations across tasks; the paper references IRM as conceptual background for invariance objectives but notes different practical approximations (e.g., REx) are used in their RL setting.",
            "uuid": "e738.1",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "V-REx",
            "name_full": "Risk Extrapolation (V-REx)",
            "brief_description": "A method that enforces invariance by penalizing variance of per-environment risks, encouraging models with equal risk across training environments to improve out-of-distribution generalization.",
            "citation_title": "Out-of-distribution generalization via risk extrapolation (rex)",
            "mention_or_use": "use",
            "method_name": "V-REx (Variance Risk Extrapolation)",
            "method_description": "V-REx computes a per-environment risk (here: mean-square error for reward and transition models in latent space) and adds to the objective a penalty proportional to the variance of these risks across environments, weighted by β; the objective is sum of mean risks plus β·Var({R_e}). As β increases, the optimizer is driven to find representations and predictors whose performance is equalized across domains, thereby preferring invariant features.",
            "environment_name": "Multi-task simulated environments (rendering and post-rendering interventions) used in sim2sim and sim2real experiments (Meta-World, Sawyer tasks, Jaco reach)",
            "environment_description": "Interactive simulated MDPs where rendering parameters are randomized to produce multiple training domains; environments are episodic control tasks (robot manipulation) with access to simulator to create rendering interventions; allows active creation of multiple domains but not active interventional selection in V-REx usage here.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization via variance penalty on per-domain risks; features that cause variable risk across domains (likely spurious) are discouraged.",
            "spurious_signal_types": "Irrelevant visual features / spurious correlations that cause variable predictive performance across rendering environments.",
            "detection_method": "Implicit detection by measuring variance of per-domain risk: high variance indicates features/predictors not invariant and thus potentially spurious.",
            "downweighting_method": "Penalize variance of risks across tasks (β·Var), causing optimization to prefer features with equalized risk and downweight reliance on domain-specific signals.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: When used (IBIT-REx) the paper reports improved generalization to unseen visual domains and higher sim2real success compared to ERM/baselines; exact numeric metrics are reported in the paper's tables/figures but not enumerated in the text body.",
            "performance_without_robustness": "Qualitative baseline (β=0 / ERM) overfits to training environments and fails to generalize to unseen test domains; DrQ and SAC baselines perform worse than IBIT-REx on unseen environments according to reported plots and tables.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Adding the V-REx penalty (IBIT-REx) improves zero-shot generalization to unseen rendering variants compared to ERM/data-augmentation-alone; however, V-REx (REx) can suffer from training instability and hyperparameter sensitivity (penalty magnitude, annealing).",
            "uuid": "e738.2",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Bisimulation",
            "name_full": "Bisimulation metrics (behavioral equivalence)",
            "brief_description": "A state-abstraction / causal-representation approach that groups states by behavioral equivalence using a pseudometric combining immediate reward differences and a Wasserstein distance over next-state distributions.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Bisimulation metrics (and DBC approximation)",
            "method_description": "Bisimulation metric d(s_i,s_j)=max_a (1-c)|R(si,a)-R(sj,a)| + c·W1(P(.|si,a),P(.|sj,a); d). The distance measures behavioral (causal) similarity; in high-dimensional/pixel settings the metric is approximated via learned encoders and neural critics. In this paper, the authors use a learned bisimulation objective in latent space (Siamese encoding) and use DBC (a neural approximation cited) to find the coarsest bisimulation partition under interventions, enforcing that visually different but behaviorally equivalent observations map close in latent space.",
            "environment_name": "Simulated MDPs with rendering/post-rendering interventions (Meta-World Sawyer tasks, Jaco 7DoF reacher)",
            "environment_description": "Interactive control environments (robot manipulation) with full MDP dynamics where the emission mapping q is changed across domains (camera, colors, textures) but underlying dynamics and rewards remain the same; simulator access allows generation of many rendering interventions.",
            "handles_distractors": true,
            "distractor_handling_technique": "Representation learning that enforces behavioral equivalence: encoder training objective collapses states that are bisimilar (same reward and transition behavior) regardless of visual distractors, effectively discarding irrelevant visual features.",
            "spurious_signal_types": "Irrelevant visual variation (background textures, colors, camera), measurement-level confounders in observations that do not affect reward/dynamics.",
            "detection_method": "Detection via computing/approximating bisimulation distance: large reward/transition discrepancies indicate behaviorally relevant differences; small distances despite visual differences indicate distractors.",
            "downweighting_method": "Encoder loss (Siamese-style) that minimizes latent distance subject to reward/transition consistency; downweights visual features that increase latent distances without corresponding behavioral differences.",
            "refutation_method": "Theoretical link: Zhang et al. result and Theorem 2 in paper assert that the causal ancestors of reward correspond to the coarsest bisimulation partition; thus if states are collapsed by bisimulation, spurious visual correlations that are not causal will be excluded from causal features.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitative: Bisimulation-based IBIT improves latent clustering of behaviorally equivalent states (t-SNE comparison) and yields better sim2sim and sim2real generalization than non-bisimulation baselines; ablations show bisimulation helps under incomplete interventions.",
            "performance_without_robustness": "Without bisimulation objective, encoders (e.g., DrQ baseline) learn representations correlated with critic value and fail to cluster equivalent states across domains, leading to poorer generalization.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Learning bisimulation-based invariances in latent space causes visually different but behaviorally equivalent observations to map together, which weakens spurious correlations from visual distractors and improves zero-shot transfer to unseen rendering variants including sim2real.",
            "uuid": "e738.3",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "DBC",
            "name_full": "DBC (neural approximation for bisimulation)",
            "brief_description": "A learned deep approximation (cited as DBC) used in this work to compute/approximate the coarsest bisimulation partition via neural encoders, enabling bisimulation-based invariance learning in high-dimensional observation spaces.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "DBC (neural bisimulation approximation)",
            "method_description": "DBC is used as a practical neural method to approximate bisimulation distances and produce a coarsest bisimulation partition in pixel-based control tasks. It trains an encoder (Siamese architecture) with objectives combining latent-distance, reward differences, and predicted latent-next-state errors, often using Wasserstein-like penalties or learned transition models to approximate the bisimulation metric.",
            "environment_name": "Pixel-based simulated control tasks (Meta-World Sawyer tasks, Jaco reach) with rendering randomizations",
            "environment_description": "Interactive simulated robotics environments with rendering interventions; DBC operates on encoded observations (pixels) and uses environment rollouts collected under varied renderings to learn behaviorally-relevant latent metrics.",
            "handles_distractors": true,
            "distractor_handling_technique": "Representation collapse of visually variant but behaviorally equivalent observations via a bisimulation-inspired encoder loss; uses learned dynamics/reward predictors in latent space to enforce behavioral equivalence.",
            "spurious_signal_types": "Visual distractors (background, texture, color) that do not change underlying reward/dynamics.",
            "detection_method": "Implicit via learned bisimulation objective: if visual differences do not alter reward/transition predictions in latent space, they are treated as distractors.",
            "downweighting_method": "Loss terms that minimize latent distance for behaviorally equivalent states while penalizing reward/transition mismatches; this reduces the encoder's sensitivity to distractors.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Reported qualitatively as improving generalization and enabling IBIT to outperform baselines on unseen visual domains; exact numeric values are in the paper's results (figures/tables).",
            "performance_without_robustness": "Baselines without DBC/bisimulation produce latent spaces that reflect value estimates and do not align equivalent states, yielding worse transfer performance.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "DBC (neural bisimulation) enables practical enforcement of behavioral invariance in pixel-based RL and is a core component of IBIT's ability to ignore visual distractors in sim2real transfer.",
            "uuid": "e738.4",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "IBIT",
            "name_full": "Intervention-based Invariant Transfer learning",
            "brief_description": "The paper's proposed method that combines rendering/post-rendering interventions (domain randomization and data augmentation) with a bisimulation-inspired invariance objective to learn latent representations robust to visual distractors for sim2sim and sim2real transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "IBIT",
            "method_description": "IBIT trains a pixel encoder φ with a bisimulation-metric-inspired objective in a multi-task setting: draw pairs of states under the same policy across randomized renderings, minimize J(φ)=||φ(x_i)-φ(x_j)||_1 - |R(φ(x_i))-R(φ(x_j))| - γ·W2(P(·|φ(x_i)),P(·|φ(x_j)))^2, combined with standard actor-critic (SAC/DrQ) training on the latent states. It uses both rendering interventions (simulator changes) and post-rendering augmentations, and implements a Siamese network to collapse behaviorally equivalent observations without requiring environment ids.",
            "environment_name": "Simulated robotics tasks (Meta-World Sawyer manipulation tasks; Jaco 7DoF reacher) with visual domain randomization and data augmentation; evaluated in sim2sim and zero-shot sim2real transfer.",
            "environment_description": "Interactive episodic MDPs for robot control where the emission mapping q is varied across training domains (background, textures, camera), allowing generation of many interventions; agent acts and gathers rollouts (active control) but IBIT does not actively select interventions—it trains across sampled randomized domains.",
            "handles_distractors": true,
            "distractor_handling_technique": "Bisimulation-based representation learning: Siamese encoder loss that collapses visually different but behaviorally equivalent observations; domain randomization/data augmentation provide interventions that enable the bisimulation objective to distinguish causal features from distractors.",
            "spurious_signal_types": "Visual distractors (background color/texture, camera views) and other observation-level spurious correlations that do not affect reward/dynamics.",
            "detection_method": "Implicit detection via bisimulation loss: if differences in observation do not lead to reward or transition differences in latent predictions, they are treated as distractors and collapsed.",
            "downweighting_method": "Encoder training minimizes latent distances for behaviorally equivalent observations and penalizes representations that reflect spurious visual differences; this downweights spurious visual features in the learned policy.",
            "refutation_method": "Empirical refutation via improved zero-shot performance on unseen domains and sim2real success: interventions that do not change bisimulation are provably valid (Theorem 2), and collapse in latent space provides evidence that visual signals were non-causal.",
            "uses_active_learning": false,
            "inquiry_strategy": "Uses simulator-sampled rendering interventions (multi-task batching of randomized environments) rather than actively chosen experiments; sampling rate and training env batch size control exposure to interventions.",
            "performance_with_robustness": "Qualitative: IBIT exhibits substantially better generalization to unseen visual domains and improved sim2real transfer success compared to baselines (DrQ, SAC) in the paper's experiments; t-SNE and unseen-env performance plots show clear gains but exact numeric metrics are presented in tables/figures in the paper.",
            "performance_without_robustness": "Baseline methods without the bisimulation invariance (DrQ, SAC) overfit to training environments and perform poorly on unseen test domains; policies trained without IBIT fail more often in sim2real trials according to the reported comparison.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Combining interventions (rendering + post-rendering) with an explicit bisimulation invariance objective yields better sim2sim and sim2real generalization than data-augmentation or domain randomization alone; interventions need not be realistic, only to vary along dimensions present in real world, and bisimulation-based IBIT does not require environment ids.",
            "uuid": "e738.5",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "IBIT-REx",
            "name_full": "IBIT with Risk Extrapolation (IBIT-REx)",
            "brief_description": "An extension of IBIT that adds a V-REx penalty to explicitly equalize per-environment risks (reward and transition prediction MSE) across randomized domains, enforcing stronger invariance and improving generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "IBIT-REx",
            "method_description": "IBIT-REx augments the IBIT bisimulation encoder objective with the V-REx penalty: define per-environment risks R_e(φ) = MSE(reward)+MSE(transition) in latent space, and optimize sum_e R_e + β·Var({R_e}). This requires environment ids (multi-task setting) and forces the learned latent dynamics/reward models to have equalized risk across domains, thereby discovering stable features useful for transfer.",
            "environment_name": "Same simulated robotics tasks as IBIT (Meta-World Sawyer tasks, Jaco 7DoF reach) with multi-domain training and evaluation on unseen test domains and on a real robot for sim2real.",
            "environment_description": "Interactive simulated MDPs where multiple training domains (renderings) are sampled; IBIT-REx uses these environment ids to compute per-domain risks and apply the variance penalty; evaluated zero-shot on unseen rendered domains and on a real Jaco arm.",
            "handles_distractors": true,
            "distractor_handling_technique": "Combined bisimulation representation collapse and risk-variance penalization: bisimulation removes behaviorally irrelevant variance, REx equalizes per-domain prediction risks to suppress reliance on domain-specific (spurious) signals.",
            "spurious_signal_types": "Visual distractors and domain-specific features that cause varying prediction risk across environments.",
            "detection_method": "High variance in per-environment latent reward/transition MSE signals presence of domain-specific/spurious predictors; variance measurement serves as detection signal.",
            "downweighting_method": "Penalize variance of per-domain risks (β·Var) so the optimizer reduces reliance on features that increase risk variance across domains.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": "Requires environment ids and explicit per-domain risk estimation; sampling of domain batches is used rather than actively selecting interventions.",
            "performance_with_robustness": "Qualitative: IBIT-REx generally improves over IBIT and baselines in generalization to unseen visual domains and shows higher sim2real transfer success in reported experiments; the paper notes improved performance but also reports training volatility and hyperparameter sensitivity for REx penalties.",
            "performance_without_robustness": "IBIT (without REx) performs well but IBIT-REx often further improves unseen-domain performance; baselines that lack invariance objectives perform worse.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Adding a V-REx penalty to bisimulation-based representation learning strengthens invariance and generalization, but REx can cause empirical instability and requires careful hyperparameter tuning; method requires environment ids.",
            "uuid": "e738.6",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Zhang-BMDP",
            "name_full": "Invariant causal prediction for block MDPs",
            "brief_description": "A recent approach connecting invariant prediction to block MDPs: learns invariant representations when only emission functions change across tasks by disentangling task-agnostic and task-specific factors, often using reconstruction or invariance losses.",
            "citation_title": "Invariant causal prediction for block mdps",
            "mention_or_use": "mention",
            "method_name": "Invariant causal prediction for Block MDPs",
            "method_description": "This line of work (Zhang et al.) extends invariant prediction ideas to Block MDPs where underlying latent state transitions are shared but emission functions vary; methods aim to learn a representation that recovers the latent causal state by enforcing invariance across tasks, e.g., via reconstruction-based losses or invariance objectives. The paper references the result that causal ancestors of reward correspond to the coarsest bisimulation partition.",
            "environment_name": "Block MDP family (rendering varies, latent dynamics/rewards fixed) — conceptual environment category used as theory for sim2real",
            "environment_description": "Interactive MDPs where only the emission mapping q_d changes across domains (rendering/camera/background), matching the sim2real setup; allows many domains but assumes shared latent structure and dynamics.",
            "handles_distractors": true,
            "distractor_handling_technique": "Learning task-agnostic latent states by enforcing invariance across emission changes and disentangling task-specific rendering factors, often via reconstruction and invariance losses.",
            "spurious_signal_types": "Rendering-level distractors and observation-level spurious variations that do not alter latent dynamics or reward.",
            "detection_method": "Invariance across emission changes: variables that vary with emission but not with reward/dynamics are treated as task-specific and excluded.",
            "downweighting_method": "Training objectives that separate and ignore task-specific emission features while retaining task-agnostic causal features.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as theoretical and practical background: Block MDP invariance results justify using bisimulation/invariance objectives in sim2real; Zhang et al.'s connection between bisimulation and invariant prediction is used to motivate IBIT.",
            "uuid": "e738.7",
            "source_info": {
                "paper_title": "Intervention Design for Effective Sim2Real Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal inference using invariant prediction: Identification and confidence intervals",
            "rating": 2,
            "sanitized_title": "causal_inference_using_invariant_prediction_identification_and_confidence_intervals"
        },
        {
            "paper_title": "Invariant Risk Minimization",
            "rating": 2,
            "sanitized_title": "invariant_risk_minimization"
        },
        {
            "paper_title": "Out-of-distribution generalization via risk extrapolation (rex)",
            "rating": 2,
            "sanitized_title": "outofdistribution_generalization_via_risk_extrapolation_rex"
        },
        {
            "paper_title": "Invariant causal prediction for block mdps",
            "rating": 2,
            "sanitized_title": "invariant_causal_prediction_for_block_mdps"
        },
        {
            "paper_title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
            "rating": 1,
            "sanitized_title": "image_augmentation_is_all_you_need_regularizing_deep_reinforcement_learning_from_pixels"
        },
        {
            "paper_title": "Designing data augmentation for simulating interventions",
            "rating": 1,
            "sanitized_title": "designing_data_augmentation_for_simulating_interventions"
        }
    ],
    "cost": 0.0191245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Intervention Design for Effective Sim2Real Transfer</p>
<p>Melissa Mozifian 
Amy Zhang 
Joelle Pineau 
David Meger 
Intervention Design for Effective Sim2Real Transfer</p>
<p>The goal of this work is to address the recent success of domain randomization and data augmentation for the sim2real setting. We explain this success through the lens of causal inference, positioning domain randomization and data augmentation as interventions on the environment which encourage invariance to irrelevant features. Such interventions include visual perturbations that have no effect on reward and dynamics. This encourages the learning algorithm to be robust to these types of variations and learn to attend to the true causal mechanisms for solving the task. This connection leads to two key findings: (1) perturbations to the environment do not have to be realistic, but merely show variation along dimensions that also vary in the real world, and (2) use of an explicit invariance-inducing objective improves generalization in sim2sim and sim2real transfer settings over just data augmentation or domain randomization alone. We demonstrate the capability of our method by performing zero-shot transfer of a robot arm reach task on a 7DoF Jaco arm learning from pixel observations.</p>
<p>I. INTRODUCTION</p>
<p>The use of simulation for robot training has many advantages, in terms of speed and cost. But simulation-based training can be brittle, and lead to poor results when tranferred to the physical robot. This can be alleviated substantially by sampling the domain randomization parameters to sufficiently cover a range of parameters. Selecting which aspects of the simulation to vary is often left to human design. However, negative transfer (poor performance in the real world) can arise perturbing irrelevant factors, or even varying the right factors by too much or by too little.</p>
<p>This paper provides a deeper understanding of what types of interventions are effective for good generalization. We introduce the use of causal inference (CI) as both an analytical method to determine what randomizations (interventions in the CI terminology) are useful, and to provide powerful learning tools that automatically pay attention to relevant variations, while ignoring spurious correlations among the random factors. Our approach further shows that the types of data augmentation and domain randomization used do not have to be physically realistic, but must be used on components of the environment that truly vary without affecting the dynamics or reward function. This enables us to make as few assumptions as possible about the features of the real world, as long as training in simulation captures the fundamental visual features required in order to solve the task at hand.</p>
<p>Recent connections between state abstractions, specifically bisimulation [1] [Section II-D], and causal inference [2], help determine the difference between modifications to the 1 Montreal Institute of Learning Algorithms (MILA) and McGill University. 2 Facebook AI Research. Correspondence to: Melissa Mozifian melissa.mozifian@mail.mcgill.ca environment that permit positive, as opposed to negative, transfer. If the true underlying causal model of the generated data is known, this knowledge can be leveraged to design data augmentation techniques appropriate to the task [3].</p>
<p>Recent results in the model-free regime to solve robotics tasks have shown significant gains through the use of data augmentation [4], [5] in the single-task setting. However, current deep RL models trained with observational data and the principle of empirical risk minimization [6] fail to generalize to unseen domains. Domain randomization and data augmentation methods do not explicitly take spurious correlations into account. In more practical applications such as robotics, data augmentation techniques are merely heuristics to enable learning invariant features.</p>
<p>In this work, we show how domain randomization combined with an invariance objective can weaken the spurious correlations in observed domains, enabling RL agents to generalize to unseen domains. Furthermore, we show what assumptions are needed to learn a minimal, causal representation of an environment and connect these findings back to explain the success of domain randomization and data augmentation. To bridge the gap between theory and practice, we explain why this method works well for sim2real transfer and show that the incorporation of an additional invariance objective further improves generalization performance. We introduce a practical method, Intervention-based Invariant Transfer learning (IBIT), that uses both data augmentation and domain randomization in a multi-task setting, additionally enforcing invariance across domains to obtain sampleefficient transfer in sim2sim [7] and sim2real settings. This real-world transfer experiment for a robot manipulation task is, to our knowledge, the first demonstration of generalization capability of current state-of-the-art pixel-based control methods in deep RL. For sample videos and link to the code base see https://sites.google.com/view/ibit. In this section, we introduce assumptions about the environment, defined as latent structure in Markov Decision Processes (MDPs), the transfer learning setting, and concepts from causal inference and state abstraction theory that are leveraged in this work.</p>
<p>A. Structured MDPs</p>
<p>We assume the domains can be described as Markov Decision Processes, defined by tuple S, A, P, R with state space S and action space A. P denotes the latent transition distribution P (s |s, a) for s, s ∈ S, a ∈ A, and R(s, a) the reward function. To address visual differences, we assume the true state space is latent, and the agent instead receives information from an additional observation space, X . We additionally define an emission mapping q, which denotes the mapping from state to observation. Data augmentation and domain randomization can be framed as interventions on this mapping q. These changes across domains can be denoted by a domain-specific mapping q d .</p>
<p>B. Domain Randomization and Data Augmentation</p>
<p>Domain randomization is an approach where one tries to find a representation that generalizes across different environments, called domains. Tobin et al. [8] introduced domain randomization which randomizes the rendering configuration in the simulator and with enough variability in the simulator, the real world could appear, as just another variation. In this setting, we call an environment that we have full access to a source domain and the environment that we want to transfer the model to, a target domain. Training proceeds in the source domain and a set of N randomization parameters are used to expose the policy to a variety of environments which help it to generalize. In this work we only consider domain randomizations that affect the mapping q.</p>
<p>Data augmentation is a very similar framework, but limited to changes to the observation that can be made after the mapping q. Data augmentation has been found to improve generalization performance in supervised learning settings [9] and RL settings where access to the simulator engine is not assumed [4], [5], [10].</p>
<p>C. Causal Discovery &amp; Invariant Prediction</p>
<p>Causal discovery aims to find causal relations from data. We assume a Structural Causal Model [11], or a latent DAG (Directed Acyclic Graph) structure underlying the data generation, which differs from conditional observational dependence between variables. Such a model will reflect on the underlying 'true' causal structure of domain generalization problem. However, causal structure cannot be found from purely observational data [3]. Peters et al. [3] details the types of interventions necessary to find the correct causal graph structure. Under these interventions, invariance can be used as a proxy for causal relationships, as those causal relationships will stay constant under interventions. Based on this insight, Peters et al. [3] introduce an algorithm, Invariant Causal Prediction (ICP), to discover causal graph structure. However, it relies on access to the variables and is super-exponential in the number of variables. More recently, Arjovsky et al. [12] and Krueger et al. [13] propose gradientbased methods for learning invariant representations. A form for learning invariant representations in MDPs was introduced by [2]. We extend this work to the sim2real setting, and provide guidelines as to the types of interventions necessary to achieve the necessary generalization. The types of interventions relevant to the sim2real setting can be found in Section III-A.</p>
<p>D. State Abstractions</p>
<p>Bisimulation is a form of state abstraction that groups states s i and s j that are "behaviorally equivalent" [14]. For any action sequence a 0:∞ , the probabilistic sequence of rewards from s i and s j are identical. A more compact definition has a recursive form: two states are bisimilar if they share both the same immediate reward and equivalent distributions over the next bisimilar states [15], [16].</p>
<p>Definition 1 (Bisimulation Relations [16]): Given an MDP M, an equivalence relation B between states is a bisimulation relation if, for all states s i , s j ∈ S that are equivalent under B (denoted s i ≡ B s j ) the following conditions hold:
R(s i , a) = R(s j , a) ∀a ∈ A,(1)P(G|s i , a) = P(G|s j , a) ∀a ∈ A, ∀G ∈ S B , (2)
where S B is the partition of S under the relation B (the set of all groups G of equivalent states), and P(G|s, a) = s ∈G P(s |s, a). Exact partitioning with bisimulation relations is generally impractical in continuous state spaces, as the relation is highly sensitive to infinitesimal changes in the reward function or dynamics. For this reason, Bisimulation Metrics [1], [17] instead defines a pseudometric space (S, d), where a distance function d : S ×S → R ≥0 measures the "behavioral similarity" between two states 1 . The bisimulation metric is the reward difference added to the Wasserstein distance between transition distributions:
Definition 2 (Bisimulation Metric): From Theorem 2.6 in [1] with c ∈ [0, 1): d(s i , s j ) = max a∈A (1 − c) · |R a si − R a sj | + c · W 1 (P a si , P a sj ; d).
(3) Zhang et al. [2] first drew connections between bisimulation and invariant prediction. Here we show how to use bisimulation metrics to find the coarsest bisimulation partition as a way to enforce invariance across interventions.</p>
<p>III. PROBLEM SETTING</p>
<p>We assume access to a simulator where we can modify the rendering function. These modifications give rise to different environments, which we treat as a multi-task setting. This requires access to an environment id that allows the agent to determine when it has changed environments, but this assumption can also be relaxed, as discussed in Section V. </p>
<p>A. Forms of Interventions</p>
<p>We posit two forms of possible interventions in the simulation setting that are relevant for sim2real. The first is postrendering interventions, one form of which is data augmentation, or an intervention staged on the observation x after it is rendered from the latent state s. This has been shown to be useful in RL settings [4], [5], and is the most versatile in that it doesn't require access to the simulation engine or emission function. Examples of this type of intervention include crops, flips, or rotations. Specifically, it refers to any reasonable transformations that can be done to the observation, after it is rendered. Such transformations should be selected in a way that does not hinder the RL reward function. For instance, consider a reacher task where the goal of the agent is to reach a red goal. If the transformation alters the goal colour in any way, this can cause inconsistency to the observation and the reward function.</p>
<p>The second type of intervention we consider is rendering interventions, that is, modifying the environment or simulator to intervene directly on the emission function q d : S → X . This would also modify only the observation and not the underlying dynamics or reward function of the MDP, but generally requires access to the rendering engine in practice. Examples of this type of intervention include different camera angles or colour or texture changes of the background or objects in the scene. Some forms of this don't require access to the rendering engine with inductive bias and image processing techniques, like interventions described in [18] of injecting natural video into backgrounds of simulators.</p>
<p>B. Multi-Task Setting</p>
<p>We treat interventions as separate tasks, and use the goal of learning an invariant representation across tasks to generalize to unseen, new tasks in a zero-shot manner. We assume access to N different domains at training time. For each domain, an intervention tag is assigned which indicates the type of intervention, i.e. augmentation to the domain. In order to test the ability of the model to generalize, we evaluate in a previously unseen test domain d = N + 1.</p>
<p>IV. CONNECTIONS TO CAUSAL INFERENCE</p>
<p>In this section we present results on the range of interventions needed to guarantee generalization, as well as the restrictions on those interventions to guarantee they will always help transfer rather than harm it.</p>
<p>Theorem 1 (Identifiability): We assume underlying variables {X 1 , X 2 , ..., X n } make up the state space S of MDP M, with a linear Gaussian relationship with the reward. If at training time we have access to an intervention on each variable X i , i ∈ {1, ..., n}, the causal parents of the reward, P A(R), are identifiable. The proof follows directly from Theorem 3 in Peters et al. [3]. Theorem 1 gives us the requirements necessary to obtain desired generalization in sim2real settings. We can define and intervene on specific variables we know can vary across simulation and real settings, e.g. table top color and texture, background, and/or mass of objects. Further, these interventions don't need to be "realistic". We can use a texture never seen at evaluation time, as shown in Figure 2, or an extreme friction coefficient never seen in the family of real tasks. Rather, what is important is the presence of the variation itself across training tasks.</p>
<p>Definition 3 (Valid interventions): An intervention is valid if it positively affects generalization performance. Definition 3 defines the type of intervention we want to design. The below result gives guidance on how to find valid interventions.</p>
<p>Theorem 2 (Bisimulation validity): All interventions that result in an MDP that is bisimilar to the original are valid.</p>
<p>Proof: From the causal inference literature, an intervention is always valid unless it is intervening on the target variable [3], which in the RL setting pertains to the reward and dynamics of future reward. We use Theorem 1 from Zhang et al. [2], which proves that the causal ancestors of the reward are the causal feature set of an MDP, and also correspond to the coarsest bisimulation partition. Therefore, any intervention that generates an MDP that is bisimilar to the original is valid. The benefit of domain randomization techniques where the simulator is directly modified means it is easier to design valid interventions that do not violate Definition 3, which was an issue discussed by Kostrikov et al. [4] when performing post-rendering interventions.</p>
<p>V. METHODS FOR LEARNING INVARIANCE OVER INTERVENTIONS</p>
<p>Our goal is to leverage interventions, modeled as different domains in a multi-task setting, to learn invariant representations that generalize across simulation domains, and from simulation domains to real ones. We explore two approaches to invariance: 1) using bisimulation metrics to learn the coarsest bisimulation partition, the fewest clusters of behaviorally equivalent states where similarity is measured by current and future reward, and 2) risk extrapolation, a way to encourage robustness to affine combinations of tasks.</p>
<p>A. Leveraging Bisimulation Metrics for Invariance</p>
<p>To train the encoder φ towards the desired relation 
d(x i , x j ) := ||φ(x i ) − φ(x j )|| 1 ,</p>
<p>Algorithm 1 IBIT and IBIT-REx</p>
<p>for Time t = 0 to ∞ do Sample a batch of visually randomized environments for env in envs batch do Apply data augmentation to pixel frames Encode observations z t = φ(s t ) Sample action from policy, a t ∼ π(.|s t )</p>
<p>Step in environment, s t ∼ p(.|s t , a t ) bisimulation metric and 1 distance in latent space [19]:
D ← D ∪ (s t , a t ,J(φ) = ||z i − z j || 1 − |R(z i ) −R(z j )| (4) − γ · W 2 P (·|z i ,π(z i )),P (·|z j ,π(z j )) 2 ,
This bisimulation metrics objective for learning the coarsest bisimulation partition under interventions is our base method, IBIT. Note that this invariance-inducing objective does not require access to environment ids.</p>
<p>B. Leveraging Interventions</p>
<p>We can also explicitly add an invariance regularization term to the reward and transition models using risk extrapolation [13] with the motivation of discovering stable features as means of generalization. V-REx [13] enforces the risk R to be close across all tasks. For our setting, the risk can be aggregated over the latent transition model or reward model, or both, defined in Equation (5):</p>
<p>R i (φ) = M SE(R,R) + M SE(P,P ).</p>
<p>The V-REx objective is as follows,
R V-REx (φ) . = βVar({R 1 (φ), ..., R m (φ)}) + m e=1 R e (φ).(6)
Here β ∈ [0, ∞) controls the balance between reducing average risk and enforcing equality of risks, with β = 0 recovering ERM, and β → ∞ leading V-REx to focus entirely on making the risks equal. The addition of this risk extrapolation objective to control for invariance we denote IBIT-REx. We note that this objective does require the multitask setting with access to environment ids. Algorithm 1 shows our method for learning invariances. Rendering interventions are used to generate multiple environments, which we train over in a multi-task manner. Bisimulation metrics steps are in orange, post-rendering intervention steps in green, and risk extrapolation in blue. Figure 3 (best viewed in colour) presents another view of our method, where we use a Siamese network to compare distances between randomly drawn states to train the encoder, then train soft actor-critic on the latent states.</p>
<p>VI. EXPERIMENTS &amp; RESULTS</p>
<p>In this section, we evaluate IBIT on sim2sim and sim2real generalization tasks. Our experiments are designed to answer the following questions: 1) Can we get better generalization performance by staging a thorough set of interventions in simulation using data augmentation and domain randomization techniques? 2) Do "unrealistic" modifications to the environment also help generalization to real settings? 3) Does leveraging invariances improve performance beyond just interventions?</p>
<p>Our experiment setup is as follows:</p>
<p>We use the opensource implementation of DrQ [4], which implements Soft Actor-Critic with data augmentation including random crops and random shifts which were found to be effective for locomotion-based RL tasks. We further modified the training environments to incorporate visual domain randomization of the background colour and top table texture and colour. We use DBC [19] to find the coarsest bisimulation partition via bisimulation metrics, and the REx penalty term [13]. In addition to the pixel observations, we also append low level states such as robot joint information, to the output of the observation encoder, to fully utilize all available information which would also be present at test time. Our experiments show the effect of using this regularization term in the performance of the policy. All of our evaluations are performed on unseen environments, where the agent does not observe the selection of colour and textures at training time. Our experiments demonstrate that the exact choices of unseen textures or colours do not matter, as long as the important features for the task itself are consistently present both during training and test time. All other details such as background should be ignored during training.</p>
<p>A. Sawyer Manipulation Tasks</p>
<p>We tested the following manipulation tasks with dense reward from the Metaworld Benchmark Suite [7] including Reacher v1, Button Press v1, Window Open v1 and Window Close v1. We selected these environments in particular, as they offer challenging visual manipulation tasks. Each of these tasks exhibit a single goal setting where the agent is expected to reach a target, or manipulate an object towards a goal configuration. Observations from example train and test environments can be found in Figure 2. We see that the training environments are not "realistic" in the sense that they do not attempt to be as close to the real setting as possible. For baselines, we evaluate IBIT and IBIT-REx against methods that don't explicitly induce invariance but still utilize both rendering and post-rendering interventions (DrQ), and those that only utilize rendering interventions, i.e. just soft actor-critic in our multi-task setting (SAC).</p>
<p>B. Jaco 7DoF Reacher</p>
<p>We run both sim2sim and sim2real transfer for the Jaco arm reacher task. In contrast to the Sawyer tasks, where the agent is learning to control the end-effector gripper of the arm, to make the task more challenging, we allow the agent to control all the joints, which in turn, brings more degrees of freedom in terms of controlling the arm and it can be more realistic for certain manipulation tasks. Similar to the Sawyer experiments, we evaluate the sim2sim performance on a visually unseen environment.</p>
<p>For the main Sim2Real experiments, we train a 7DoF Jaco arm [20] in simulation. The simulated Jaco arm used within these environments is part of the DeepMind control suite [21], with modifications made to visually randomize the scene. For this task, the agent learns to controls joints velocities of the 7DoF robot arm. We assume dense reward during training and only sparse reward during testing on the real arm. Although this is a single goal reach task, the arm is reset to a random initial position at the beginning of each episode, making the task of generalization slightly harder as the agent needs to find a trajectory towards the target goal from any starting joint configuration. This implies that the arm starts in a new position which visually looks different. Figure 5 shows results across all sim2sim tasks aggregated over 10 seeds for both Sawyer and Jaco -addressing questions 2) and 3) by confirming that invariance-seeking objectives do help generalization performance, and we can successfully generalize with unrealistic interventions. These results show performance on both seen (during training) and unseen environments. As expected, without the invariance objective, the policy overfits to the training environments and is unable to solve the task in the unseen case. Our results show IBIT and IBIT-REx are both effective at generalizing, with their performance dependent on the task and the choices of hyperparameters per task. This is with the exception to the Sawyer window Open task where all methods struggle to converge due to the difficulty of the task. We hypothesize that this task is visually more challenging compared to others and none of the methods are able to reliably generalize in the unseen environment case. We also plot ablations of how well DBC as an invariance method generalizes to unseen environments under incomplete forms of interventions in   Figure 6, which answers question 1) that both types of interventions are helpful. Finally, Figure 4 shows a t-SNE comparison of the latent representations learned in Sawyer Reacher v1 by IBIT and DrQ. We see that IBIT successfully learns invariances and plots equivalent states to the same area in latent space, even though they have large visual differences. DrQ learns a latent space that strongly correlates with estimated value, but does not learn to plot equivalent states under different interventions near each other.</p>
<p>C. Results</p>
<p>The results of the sim2real arm experiments are demonstrated in Table I. We ran the best policy of each method 10 times, and success was evaluated based on the arm successfully touching the target. The results demonstrate the robustness of each method in a real world setting where there are unseen variations of the visual representation that challenge the true generalization capability of each method.</p>
<p>VII. RELATED WORK</p>
<p>Prior work has been done in domain randomization for sim2real transfer, data augmentation for better generalization in RL settings, and invariance finding through the causal inference lens, which we detail below.</p>
<p>Tobin et al. [8] introduced domain randomization for sim2real transfer, which randomizes the rendering configuration in the simulator, and posited that with enough variability in the simulator, the real world could appear just as another variation. However, they did not specify what types of randomization are required to bridge the reality gap. Rusu et al. [22] explore using the progressive network architecture to adapt a model that is pre-trained on simulated pixels. Rao et al. [23] proposes to automatically bridge the simulationto-reality gap by employing generative models to translate simulated images into realistic ones. Such transformation is task-agnostic and the transformed images may not preserve all task-agnostic features. The authors propose a RL-scene consistency loss which ensures the image transformation is invariant with respect to the Q-values associated with the images. This enables learning task-aware transformations, but requires transferring simulated environments to match reality, which IBIT shows is not a necessary factor.</p>
<p>Data Augmentation has gained recent popularity in selfsupervised learning for improving generalization performance in supervised learning and reinforcement learning. Several contrastive representation learning approaches [10], [24] utilize data augmentations and perform patch-wise or instance discrimination. The contrastive objective aims to maximize agreement between augmentations of the same image and minimize it between all other images in the dataset. More recent work found that the contrastive objective is unnecessary, and even harmful, for representation learning in RL [5]. Kostrikov et al. [4] propose augmentations and   weighted Q-functions in conjunction with Soft Actor-Critic (SAC). They utilize standard image transformations, specifically random shifts, to regularize the Q-function learned by the critic so that different shifts to the same input image have similar Q-function values. Also inspired by the impact of data augmentation in computer vision Laskin et al. [5] propose RAD: Reinforcement Learning with Augmented Data, showing that data augmentations can enable simple RL algorithms to match and even outperform complex state-of-the-art methods such as [25]- [27] on the task of visual dynamics learning. Cobbe et al. [28] and Lee et al. [29] also show that simple data augmentation techniques such as cutout [28] and random convolution [29] can be useful to improve generalization of agents on the OpenAI CoinRun and ProcGen benchmarks.</p>
<p>Peters et al. [3] first introduced invariance as a proxy for causal mechanisms, using hypothesis testing methods to find the set of variables that cause a target variable. Their method unfortunately relies on access to the true variables and is super-exponential in the number of variables, and therefore is not widely applicable in realistic settings. Arjovsky et al. [12] extend this idea to learning invariant representations for the multi-task supervised learning setting with a gradient penalty, and Krueger et al. [13] impose a similar constraint on invariance to risk across tasks to extrapolate to new, unseen tasks. Ilse et al. [30] also draw parallels between domain generalization and causal inference, but only focus on data augmentation -pointing out that certain types of data augmentation can harm downstream performance. Zhang et al. [2] also use an invariance objective in a multitask setting for block MDPs, a family of tasks where only the rendering functions changes, e.g. camera angle or background. They assume these rendering functions are given and use a reconstruction-based loss to disentangle task-agnostic and task-specific representations. None of these methods utilize both rendering and post-rendering interventions or target the sim2real problem.</p>
<p>VIII. DISCUSSION</p>
<p>This paper presents Intervention-Based Invariant Transfer learning (IBIT): a method that encourages robustness to visual variations introduced at training time, which in turn, learns to attend to the true causal mechanisms of the underlying visual changes. Our experiments show that techniques such as Domain Randomization and Data Augmentation are forms of interventions -namely rendering and post-rendering interventions -that are effective for sim2real generalizability. In order to have guarantees on generalization capabilities, we propose to combine these interventions with an invariance objective that focuses on removing distractions in the background setting, enabling the agent to attend to the visual features relevant to solving the task at hand. However one difficulty we found with latent model-based methods, was the sensitivity of these methods to the choice of hyperparameters. We found that REx also suffers empirically from training volatility, likely caused by large penalties early on in training. Future work could explore avenues to improve stability in such invariance methods. Working with the Metaworld tasks, we also found the multi-goal setting too challenging for an RL agent when dealing with rich observations. Further work could also explore approaches towards more robust multigoal RL agents.</p>
<p>APPENDIX</p>
<p>A. Implementation Details a) Learning a bisimulation metric: Defining a distance d between states requires defining both a distance between rewards (to soften Equation (1)), and distance between state distributions (to soften Equation (2)). Prior works use the Wasserstein metric for the latter, originally used in the context of bisimulation metrics by Breugel et al. [31]. The p th Wasserstein metric is defined between two probability distributions P i and P j as
W p (P i , P j ; d) = ( inf γ ∈Γ(Pi,Pj ) S×S d(s i , s j ) p dγ (s i , s j )) 1/p (7) where Γ(P i , P j )
is the set of all couplings of P i and P j . This is known as the "earth mover" distance, denoting the cost of transporting mass from one distribution to another [32].</p>
<p>B. Hyperparameter Details</p>
<p>The following parameters are the best performing parameters for IBIT. The training time and episode lengths depend on the environment and the task at hand. Table VII shows the complete set of parameters and possible ranges for some parameters we experimented with, and the rest show specific parameters per task.       </p>
<p>Fig. 1 :
1Sim2real Jaco robot arm setup. arXiv:2012.02055v1 [cs.RO] 3 Dec 2020 II. TECHNICAL BACKGROUND</p>
<p>Fig. 2 :
2Training and evaluation settings. At training time, the agent is exposed to variable changes such as background and table colour while relevant visual features such as the arm itself and target are kept constant. At test time, the agent is evaluated on new unseen environments.</p>
<p>Fig. 3 :
3we draw batches of observation pairs and minimise the mean square error between the Flow diagram of our method, IBIT. Shaded in blue is the main model architecture, it is reused for both states, like a Siamese network.</p>
<p>r t , s t , env tag) UpdateCritic(D) Apply Data Augmentation Reg UpdateActor(D) Sample batch B i ∼ D Permute batch randomly: B j = permute(B i ) Train policy: E Bi [J(π)] Train encoder: E Bi,Bj [J(φ)] Train dynamics: J(P, φ) = (P(φ(s t ), a t ) −z t+1 ) 2 Train reward: J(R,P, φ) = (R(P(φ(st), at)) − rt+1) 2 if REx thenApply penalty term RV-REx from Eq.</p>
<p>Fig
. 4: t-SNE comparison of the latent representations learned by IBIT and DrQ. colours represent the predicted value by the critic.Our method successfully learns an invariant representation to domain randomization, while DrQ does not.</p>
<p>Fig. 5 :
5Comparison of methods on unseen environment (top) and seen environment (bottom).</p>
<p>Fig. 6 :
6Comparison of methods with and without PRI (post-rendering interventions), and with and without RI (rendering interventions) on unseen (top) and seen environments (bottom).</p>
<p>TABLE I :
IReal Jaco transfer success rate over 10 trials.</p>
<p>Critic Q-function soft-update rate τ Q 0.005 Critic encoder soft-update rate τ Q 0.005 Actor learning rate [1e −3 − 1e −5 ]Parameter name 
Value 
Replay buffer capacity 
100000 
Batch size 
[32-256] 
Image frame size 
84 × 84 
Image frame stack 
5 
Discount γ 
0.99 
Optimizer 
Adam 
Critic learning rate 
[1e −3 − 1e −5 ] 
Critic target update frequency 
[1-4] 
Actor target update frequency 
[2-4] 
Actor log stddev bounds 
[−5,2] 
Encoder learning rate 
10 −5 
Temperature learning rate 
10 −4 
Temperature Adam's β 1 
0.9 
Init temperature 
0.1 
Penalty weight 
[0.1 − 1.0] 
Penalty anneal iterations 
[8000 − 20000] 
Training env batch size 
5 
Training env resampling rate 
[150 − 3000] </p>
<p>TABLE II :
IIA complete overview of common hyper parameters for Sawyer and Jaco environments.Parameter name 
Value 
Batch size 
256 
Critic learning rate 
0.005 
Critic target update frequency 
2 
Actor learning rate 
0.005 
Actor target update frequency 
2 
Encoder learning rate 
0.005 
Penalty weight 
0.5 
Penalty anneal iterations 
8000 
Training env batch size 
5 
Training env resampling rate 
150 </p>
<p>TABLE III :
IIIEnvironment specific hyper parameters for Sawyer Reach environment.Parameter name 
Value 
Batch size 
32 
Critic learning rate 
0.001 
Critic target update frequency 
1 
Actor learning rate 
0.001 
Actor target update frequency 
4 
Encoder learning rate 
0.001 
Penalty weight 
0.13 
Penalty anneal iterations 
6000 
Training env batch size 
5 
Training env resampling rate 
300 </p>
<p>TABLE IV :
IVEnvironment specific hyper parameters forDMSuite Jaco Reach environment.Parameter name 
Value 
Batch size 
64 
Critic learning rate 
0.001 
Critic target update frequency 
2 
Actor learning rate 
0.001 
Actor target update frequency 
2 
Encoder learning rate 
0.001 
Penalty weight 
0.5 
Penalty anneal iterations 
10000 
Training env batch size 
5 
Training env resampling rate 
3000 </p>
<p>TABLE V :
VEnvironment specific hyper parameters for Sawyer Window Open environment.Parameter name 
Value 
Batch size 
64 
Critic learning rate 
0.002 
Critic target update frequency 
2 
Actor learning rate 
0.002 
Actor target update frequency 
2 
Encoder learning rate 
0.002 
Penalty weight 
0.8 
Penalty anneal iterations 
10000 
Training env batch size 
5 
Training env resampling rate 
3000 </p>
<p>TABLE VI :
VIEnvironment specific hyper parameters for SawyerWindow Close environment.Parameter name 
Value 
Batch size 
32 
Critic learning rate 
0.001 
Critic target update frequency 
2 
Actor learning rate 
0.001 
Actor target update frequency 
2 
Encoder learning rate 
0.001 
Penalty weight 
0.2 
Penalty anneal iterations 
3000 
Training env batch size 
5 
Training env resampling rate 
150 </p>
<p>TABLE VII :
VIIEnvironment specific hyper parameters for Sawyer Button Press environment.
Note that d is a pseudometric, meaning the distance between two different states can be zero, corresponding to behavioral equivalence.(a) Training envs with randomized visual rendering. (b) Test time.
ACKNOWLEDGMENTWe thank Johanna Hansen for the help with the real robot arm setup. This work was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada.
Bisimulation metrics for continuous Markov decision processes. N Ferns, P Panangaden, D Precup, 10.1137/10080484XDOI: 10 . 1137 / 10080484XSociety for Industrial and Applied Mathematics. 406N. Ferns, P. Panangaden, and D. Precup, "Bisimulation metrics for continuous Markov decision processes," Society for Industrial and Applied Mathematics, vol. 40, no. 6, 1662-1714, Dec. 2011, ISSN: 0097-5397. DOI: 10 . 1137 / 10080484X. [Online]. Available: https://doi.org/10.1137/10080484X.</p>
<p>Invariant causal prediction for block mdps. A Zhang, C Lyle, S Sodhani, A Filos, M Kwiatkowska, J Pineau, Y Gal, D Precup, International Conference on Machine Learning (ICML). 2020A. Zhang, C. Lyle, S. Sodhani, A. Filos, M. Kwiatkowska, J. Pineau, Y. Gal, and D. Precup, "Invariant causal prediction for block mdps," in International Conference on Machine Learning (ICML), 2020.</p>
<p>Causal inference using invariant prediction: Identification and confidence intervals. J Peters, P Bühlmann, N Meinshausen, Journal of the Royal Statistical Society, Series B (with discussion). 785J. Peters, P. Bühlmann, and N. Meinshausen, "Causal inference using invariant prediction: Identification and confidence intervals," Journal of the Royal Statistical Society, Series B (with discussion), vol. 78, no. 5, pp. 947-1012, 2016.</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. I Kostrikov, D Yarats, R Fergus, arXiv:2004.136492020cs.LGI. Kostrikov, D. Yarats, and R. Fergus, "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels," 2020. arXiv: 2004.13649 [cs.LG].</p>
<p>. M Laskin, K Lee, A Stooke, L Pinto, P Abbeel, A Srinivas, Reinforcement learning with augmented data, 2020. arXiv: 2004. 14990 [cs.LGM. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas, Reinforcement learning with augmented data, 2020. arXiv: 2004. 14990 [cs.LG].</p>
<p>Principles of risk minimization for learning theory. V Vapnik, Advances in Neural Information Processing Systems. J. E. Moody, S. J. Hanson, and R. P. LippmannMorgan-Kaufmann4V. Vapnik, "Principles of risk minimization for learning theory," in Advances in Neural Information Processing Systems 4, J. E. Moody, S. J. Hanson, and R. P. Lippmann, Eds., Morgan-Kaufmann, 1992, pp. 831-838. [Online]. Available: http://papers.nips.cc/ paper / 506 -principles -of -risk -minimization - for-learning-theory.pdf.</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, arXiv:1910.10897Conference on Robot Learning (CoRL). cs.LGT. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning," in Conference on Robot Learning (CoRL), 2019. arXiv: 1910.10897 [cs.LG]. [Online]. Available: https://arxiv.org/abs/1910.10897.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), IEEE, 2017, pp. 23-30.</p>
<p>Generalizing to unseen domains via adversarial data augmentation. R Volpi, H Namkoong, O Sener, J C Duchi, V Murino, S Savarese, Advances in Neural Information Processing Systems 31. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, IncR. Volpi, H. Namkoong, O. Sener, J. C. Duchi, V. Murino, and S. Savarese, "Generalizing to unseen domains via adversarial data augmentation," in Advances in Neural Information Processing Sys- tems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., Curran Associates, Inc., 2018, pp. 5334-5344. [Online]. Available: http : / / papers . nips . cc/paper/7779-generalizing-to-unseen-domains- via-adversarial-data-augmentation.pdf.</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. A Srinivas, M Laskin, P Abbeel, arXiv:2004.04136arXiv preprintA. Srinivas, M. Laskin, and P. Abbeel, "Curl: Contrastive unsu- pervised representations for reinforcement learning," arXiv preprint arXiv:2004.04136, 2020.</p>
<p>Causality: Models, Reasoning and Inference, 2nd. J Pearl, Cambridge University Press9780521895606New York, NY, USAJ. Pearl, Causality: Models, Reasoning and Inference, 2nd. New York, NY, USA: Cambridge University Press, 2009, ISBN: 052189560X, 9780521895606.</p>
<p>. M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, arXiv:1907.02893Invariant Risk Minimization," arXiv e-prints. stat.MLM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz, "Invariant Risk Minimization," arXiv e-prints, Jul. 2019. arXiv: 1907.02893 [stat.ML].</p>
<p>Out-of-distribution generalization via risk extrapolation (rex. D Krueger, E Caballero, J.-H Jacobsen, A Zhang, J Binas, R L Priol, A Courville, arXiv:2003.006882020cs.LGD. Krueger, E. Caballero, J.-H. Jacobsen, A. Zhang, J. Binas, R. L. Priol, and A. Courville, Out-of-distribution generalization via risk extrapolation (rex), 2020. arXiv: 2003.00688 [cs.LG].</p>
<p>Towards a unified theory of state abstraction for MDPs. L Li, T J Walsh, M L Littman, ISAIM. L. Li, T. J. Walsh, and M. L. Littman, "Towards a unified theory of state abstraction for MDPs," in ISAIM, 2006.</p>
<p>Bisimulation through probabilistic testing (preliminary report). K G Larsen, A Skou, 10.1145/75277.75307ISBN: 0897912942. DOI: 10 . 1145 / 75277 . 75307Symposium on Principles of Programming Languages. Association for Computing MachineryK. G. Larsen and A. Skou, "Bisimulation through probabilistic testing (preliminary report)," in Symposium on Principles of Pro- gramming Languages, Association for Computing Machinery, 1989, 344-352, ISBN: 0897912942. DOI: 10 . 1145 / 75277 . 75307. [Online]. Available: https://doi.org/10.1145/75277. 75307.</p>
<p>Equivalence notions and model minimization in Markov decision processes. R Givan, T L Dean, M Greig, Artificial Intelligence. 147R. Givan, T. L. Dean, and M. Greig, "Equivalence notions and model minimization in Markov decision processes," Artificial Intelligence, vol. 147, pp. 163-223, 2003.</p>
<p>Bisimulation metrics are optimal value functions. N Ferns, D Precup, Uncertainty in Artificial Intelligence (UAI). N. Ferns and D. Precup, "Bisimulation metrics are optimal value functions.," in Uncertainty in Artificial Intelligence (UAI), 2014, pp. 210-219.</p>
<p>Natural environment benchmarks for reinforcement learning. A Zhang, Y Wu, J Pineau, arXiv:1811.06032Computing Research Repository (CoRR). A. Zhang, Y. Wu, and J. Pineau, "Natural environment bench- marks for reinforcement learning," Computing Research Repository (CoRR), vol. abs/1811.06032, 2018. arXiv: 1811.06032. [Online].</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. A Zhang, R Mcallister, R Calandra, Y Gal, S Levine, arXiv:2006.10742cs.LGA. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine, Learning invariant representations for reinforcement learning with- out reconstruction, 2020. arXiv: 2006.10742 [cs.LG].</p>
<p>Kinova modular robot arms for service robotics applications. A Campeau-Lecours, H Lamontagne, S Latour, P Fauteux, V Maheu, F Boucher, C Deguire, L.-J C L&apos;ecuyer, Rapid Automation: Concepts, Methodologies, Tools, and Applications. IGI GlobalA. Campeau-Lecours, H. Lamontagne, S. Latour, P. Fauteux, V. Maheu, F. Boucher, C. Deguire, and L.-J. C. L'Ecuyer, "Kinova modular robot arms for service robotics applications," in Rapid Automation: Concepts, Methodologies, Tools, and Applications, IGI Global, 2019, pp. 693-719.</p>
<p>Dm control: Software and tasks for continuous control. S Tunyasuvunakool, A Muldal, Y Doron, S Liu, S Bohez, J Merel, T Erez, T Lillicrap, N Heess, Y Tassa, 10.1016/j.simpa.2020.100022Software Impacts. 6S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, N. Heess, and Y. Tassa, "Dm control: Software and tasks for continuous control," Software Impacts, vol. 6, p. 100 022, 2020, ISSN: 2665-9638. DOI: 10.1016/j.simpa. 2020 . 100022. [Online]. Available: http : / / dx . doi . org / 10.1016/j.simpa.2020.100022.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerik, T Rothörl, N Heess, R Pascanu, R Hadsell, arXiv:1610.04286arXiv preprintA. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell, "Sim-to-real robot learning from pixels with progressive nets," arXiv preprint arXiv:1610.04286, 2016.</p>
<p>Rl-cyclegan: Reinforcement learning aware simulation-to-real. K Rao, C Harris, A Irpan, S Levine, J Ibarz, M Khansari, arXiv:2006.09001cs.ROK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, Rl-cyclegan: Reinforcement learning aware simulation-to-real, 2020. arXiv: 2006.09001 [cs.RO].</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, arXiv:2002.05709arXiv preprintT. Chen, S. Kornblith, M. Norouzi, and G. Hinton, "A simple framework for contrastive learning of visual representations," arXiv preprint arXiv:2002.05709, 2020.</p>
<p>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. A X Lee, A Nagabandi, P Abbeel, S Levine, arXiv:1907.00953arXiv preprintA. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine, "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model," arXiv preprint arXiv:1907.00953, 2019.</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, arXiv:1811.04551arXiv preprintD. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson, "Learning latent dynamics for planning from pixels," arXiv preprint arXiv:1811.04551, 2018.</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, arXiv:1912.01603arXiv preprintD. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, "Dream to con- trol: Learning behaviors by latent imagination," arXiv preprint arXiv:1912.01603, 2019.</p>
<p>Quantifying generalization in reinforcement learning. K Cobbe, O Klimov, C Hesse, T Kim, J Schulman, arXiv:1812.02341arXiv preprintK. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, "Quan- tifying generalization in reinforcement learning," arXiv preprint arXiv:1812.02341, 2018.</p>
<p>Network randomization: A simple technique for generalization in deep reinforcement learning. K Lee, K Lee, J Shin, H Lee, International Conference on Learning Representations. 2020K. Lee, K. Lee, J. Shin, and H. Lee, "Network randomization: A simple technique for generalization in deep reinforcement learn- ing," in International Conference on Learning Representations. https://openreview. net/forum, 2020.</p>
<p>Designing data augmentation for simulating interventions. M Ilse, J M Tomczak, P Forré, arXiv:2005.01856arXiv preprintM. Ilse, J. M. Tomczak, and P. Forré, "Designing data augmenta- tion for simulating interventions," arXiv preprint arXiv:2005.01856, 2020.</p>
<p>Towards quantitative verification of probabilistic transition systems. F Van Breugel, enJ Worrell, en10.1007/3-540-48224-5_35ISBN: 978-3-540-48224-6. DOI: 10. 1007/3-540-48224-5_35Automata, Languages and Programming. F. Orejas, P. G. Spirakis, and J. van LeeuwenSpringerF. van Breugel and J. Worrell, "Towards quantitative verification of probabilistic transition systems," en, in Automata, Languages and Programming, F. Orejas, P. G. Spirakis, and J. van Leeuwen, Eds., Springer, 2001, pp. 421-432, ISBN: 978-3-540-48224-6. DOI: 10. 1007/3-540-48224-5_35.</p>
<p>Topics in optimal transportation. C Villani, American Mathematical SocietyC. Villani, Topics in optimal transportation. American Mathematical Society, Jan. 2003.</p>            </div>
        </div>

    </div>
</body>
</html>
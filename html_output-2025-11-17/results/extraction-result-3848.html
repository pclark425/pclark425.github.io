<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3848 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3848</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3848</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13" target="_blank">KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience, demonstrating how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
                <p><strong>Paper Abstract:</strong> Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3848.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3848.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KNIMEZoBot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNIMEZoBot: Zotero + KNIME + OpenAI Integration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-free literature-review assistant that integrates Zotero (user/group reference libraries), KNIME visual workflows, FAISS vector store, LangChain PDF chunking, OpenAI embeddings (text-embedding-ada-002) and GPT models via a Retrieval-Augmented Generation (RAG) pipeline to answer natural-language queries over a user's curated PDF corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>KNIMEZoBot</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A user-facing system that automates retrieval and LLM-based synthesis from personal or group Zotero libraries using a RAG architecture inside the KNIME visual programming environment. It downloads PDFs via the Zotero REST API, splits documents into overlapping chunks (LangChain/unstructuredPDFLoader), computes embeddings (OpenAI text-embedding-ada-002), stores vectors in a FAISS vector store, performs semantic retrieval of relevant chunks for a query, and sends retrieved context plus the query to OpenAI GPT models to generate synthesized natural-language answers via an agent prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>User-selected Zotero libraries or group libraries (PDFs and metadata attached to Zotero items). The paper does not specify a numeric corpus size; documents are arbitrary-length PDFs in typical academic formats and languages handled by Zotero. Corpus is local to the user's Zotero collections and can be restricted to collections or the entire library.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language questions submitted via a conversational chatbot UI inside KNIMEZoBot; the system accepts free-text prompts and maintains conversation history. The system message (system prompt) is customizable (example: instruct the agent to answer only from provided Zotero information and otherwise apologize).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) pipeline: PDFs are chunked (configurable chunk size and overlap via LangChain), chunks embedded with OpenAI embeddings, vectors stored in FAISS, semantic similarity retrieves top chunks for a query (query converted to embedding), and retrieved text fragments are provided to an OpenAI LLM (e.g., GPT-3.5-turbo or GPT-4) with a tailored system prompt to synthesize answers. The KNIME 'OpenAI Functions Agent' and 'Agent Prompter' nodes orchestrate prompt and agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language synthesized answers to user queries presented in a chatbot interface; full conversation history can be exported as a CSV. Answers are expected to be grounded in retrieved Zotero content per the system prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>No formal quantitative evaluation reported in the paper (no benchmarks, user studies, or automated metrics). The paper reports system behavior qualitatively and discusses potential utility; evaluation appears limited to illustrative use and system demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The system demonstrates a working, low-code RAG pipeline that enables users without programming skills to query and synthesize information from their Zotero libraries, with configurable chunking and model selection. The paper reports functional components (PDF retrieval, chunking, FAISS indexing, retrieval, LLM synthesis) and claims potential to expedite literature reviews, but provides no numeric performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No empirical evaluation provided; potential for hallucination inherent to LLMs is noted and mitigated only by instructing the model to use provided context. Context-window/token limits require chunking and overlap tuning. System requires users' API keys (OpenAI, Zotero). Accuracy and sophistication of automated analysis remain areas for improvement per the authors. The corpus size, retrieval freshness, and scalability limits are not empirically characterized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No direct comparisons to baseline automated methods, other RAG implementations, or human literature-review performance are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3848.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3848.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-then-generation approach that augments LLMs with externally retrieved textual context: documents are chunked, embedded, stored in a vector index, and semantically-retrieved fragments are supplied to an LLM to produce grounded responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Described as a two-stage approach: text corpora are split into overlapping fragments, fragments are converted to vector embeddings and stored in a vector database, queries are embedded and nearest-neighbor retrieval returns semantically relevant fragments, and an LLM consumes the query plus retrieved fragments to generate a synthesized answer. RAG addresses models' context-window constraints and reduces hallucination by grounding generation in retrieved sources.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>General large text corpora described in the paper as being chunked into smaller overlapping fragments; applicable to multiple documents and domains (no numeric corpus details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Queries are provided in natural language and converted to embeddings for vector-similarity retrieval; retrieved passages and the original query are forwarded to the LLM for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Semantic retrieval via vector embeddings (embedding model unspecified in the general description) followed by LLM-based synthesis conditioned on retrieved fragments (i.e., RAG). The paper cites literature (Lewis et al., Chen et al.) for the method.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Coherent, context-grounded natural-language responses or summaries that synthesize information across retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>This paper does not present an evaluation of RAG itself but cites prior work (Lewis et al., Chen et al.) for RAG methods and benchmarking; the authors note RAG's usefulness but do not provide new benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RAG is presented as an effective strategy to navigate context-window constraints and to synthesize across documents; no new quantitative results are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RAG previously required coding knowledge to implement; the paper also mentions general LLM issues like hallucination and context-window limits that RAG mitigates but does not eliminate. Implementation details (indexing, retriever quality, freshness) can affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>The paper claims RAG addresses limitations of plain chat-based LLM use (e.g., limited context window, hallucination) and enables synthesis across many documents, but it does not report formal baseline comparisons in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3848.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3848.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI GPT / ChatGPT / Claude / Bard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Chat-optimized Language Models (e.g., ChatGPT/GPT-4, Claude, Bard)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Commercial chat-optimized LLMs capable of summarizing and synthesizing text; discussed in the paper as representative LLMs used for literature summarization but with inherent context-window size and hallucination limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Chat-based LLM summarization (e.g., GPT-4, Claude, Bard)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>LLMs are used in a chat format to summarize single publications or answer questions; their native approach is to process prompt + context and generate a response, but they are constrained by model context window sizes and can hallucinate unsupported facts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Individual papers or aggregated text passed into the chat model as prompt/context. The paper cites approximate context window sizes (public ChatGPT-4 ≈ 5000 words; Claude up to 75,000 words) as relevant constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language prompts in a chat interface.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct prompt-based summarization/QA (non-retrieval), i.e., feeding text to the model and asking for summaries or answers. The authors contrast this with RAG for large multi-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language summaries and answers returned in chat form.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>No evaluation reported in this paper for these models; the paper discusses known failure modes (hallucination) and context-window limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Authors note that chat LLMs can summarize single publications but face limitations when synthesizing across many documents due to context-window constraints and hallucination risk; Claude's larger context window expands single-document summarization but still has restrictions for multi-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Context window length limits the amount of text that can be given to the model in a single prompt; hallucinations (confabulations) remain a concern; some models cannot practically summarize many long academic documents without retrieval/chunking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared qualitatively to RAG: chat-only LLM usage is less suitable for multi-document synthesis and more prone to hallucination; no empirical human comparisons given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3848.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3848.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain (document loaders and chunking utilities)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tooling library used inside KNIMEZoBot to load PDFs (unstructuredPDFLoader) and split long documents into smaller overlapping chunks to respect LLM input size limits prior to embedding and indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Introduction | Langchain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LangChain document loading + chunking</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>LangChain's PDF loader is used to read full-text PDFs and split them into smaller token-limited chunks with configurable chunk size and overlap to satisfy LLM input constraints; these chunks are then embedded and indexed for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Full-text PDF documents retrieved from Zotero; language and document formats as supported by the chosen LangChain loader and unstructured[pdf] tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Chunking is controlled by user settings (chunk size and overlap) input via the KNIME UI; queries remain natural-language prompts to the chatbot.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Preprocessing step in the RAG pipeline: chunking long documents into smaller segments to allow embedding and retrieval; not a distillation algorithm by itself but essential to RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Smaller text chunks (string segments) saved to be embedded and stored in the vector index.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>No evaluation reported for chunking choices; chunk size and overlap are user-configurable and suggested to be tuned according to model limits.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LangChain enabled processing of long PDFs by splitting them into token-sized chunks for embedding and retrieval; no performance metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Chunking introduces trade-offs: too-small chunks can lose cross-sentence context; overlap can create redundancy and increased indexing cost; optimal parameters depend on the model and corpus but are not empirically optimized in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No comparisons to alternative chunking approaches are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3848.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3848.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FAISS (vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FAISS Vector Store (Facebook AI Similarity Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vector index used in the KNIMEZoBot pipeline to store embeddings of document chunks and perform efficient nearest-neighbor semantic retrieval for RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAISS Vector Store Creator</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>FAISS-based vector retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Embeddings produced by OpenAI's embedding model are stored in a FAISS index (via KNIME's FAISS Vector Store Creator node) to enable fast similarity search and retrieval of the most semantically relevant text chunks for a given query embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Vector embeddings of text chunks derived from Zotero PDFs (chunking done with LangChain); no numeric index size reported.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Queries are embedded and used to run nearest-neighbor searches in FAISS to select candidate context chunks for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>FAISS performs approximate/efficient similarity search as the retriever in the RAG pipeline; it is an engineering component enabling retrieval rather than a distillation algorithm itself.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Ordered list of top-k retrieved text chunks (strings) returned to the generation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>No retrieval-accuracy or latency metrics provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FAISS was used effectively as the vector index in the KNIME workflow enabling semantic retrieval; no quantitative retrieval performance is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper does not report empirical scalability or accuracy tests; FAISS performance depends on embedding quality, index configuration, and corpus size which are not characterized here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No comparisons to other vector stores (e.g., Chroma) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>Benchmarking Large Language Models in RetrievalAugmented Generation <em>(Rating: 2)</em></li>
                <li>Artificial intelligence and the conduct of literature reviews <em>(Rating: 1)</em></li>
                <li>Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3848",
    "paper_id": "paper-3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "KNIMEZoBot",
            "name_full": "KNIMEZoBot: Zotero + KNIME + OpenAI Integration",
            "brief_description": "A code-free literature-review assistant that integrates Zotero (user/group reference libraries), KNIME visual workflows, FAISS vector store, LangChain PDF chunking, OpenAI embeddings (text-embedding-ada-002) and GPT models via a Retrieval-Augmented Generation (RAG) pipeline to answer natural-language queries over a user's curated PDF corpus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "KNIMEZoBot",
            "system_or_method_description": "A user-facing system that automates retrieval and LLM-based synthesis from personal or group Zotero libraries using a RAG architecture inside the KNIME visual programming environment. It downloads PDFs via the Zotero REST API, splits documents into overlapping chunks (LangChain/unstructuredPDFLoader), computes embeddings (OpenAI text-embedding-ada-002), stores vectors in a FAISS vector store, performs semantic retrieval of relevant chunks for a query, and sends retrieved context plus the query to OpenAI GPT models to generate synthesized natural-language answers via an agent prompt.",
            "input_corpus_description": "User-selected Zotero libraries or group libraries (PDFs and metadata attached to Zotero items). The paper does not specify a numeric corpus size; documents are arbitrary-length PDFs in typical academic formats and languages handled by Zotero. Corpus is local to the user's Zotero collections and can be restricted to collections or the entire library.",
            "topic_or_query_specification": "Natural-language questions submitted via a conversational chatbot UI inside KNIMEZoBot; the system accepts free-text prompts and maintains conversation history. The system message (system prompt) is customizable (example: instruct the agent to answer only from provided Zotero information and otherwise apologize).",
            "distillation_method": "Retrieval-Augmented Generation (RAG) pipeline: PDFs are chunked (configurable chunk size and overlap via LangChain), chunks embedded with OpenAI embeddings, vectors stored in FAISS, semantic similarity retrieves top chunks for a query (query converted to embedding), and retrieved text fragments are provided to an OpenAI LLM (e.g., GPT-3.5-turbo or GPT-4) with a tailored system prompt to synthesize answers. The KNIME 'OpenAI Functions Agent' and 'Agent Prompter' nodes orchestrate prompt and agent behavior.",
            "output_type_and_format": "Natural-language synthesized answers to user queries presented in a chatbot interface; full conversation history can be exported as a CSV. Answers are expected to be grounded in retrieved Zotero content per the system prompt.",
            "evaluation_or_validation_method": "No formal quantitative evaluation reported in the paper (no benchmarks, user studies, or automated metrics). The paper reports system behavior qualitatively and discusses potential utility; evaluation appears limited to illustrative use and system demonstration.",
            "results_summary": "The system demonstrates a working, low-code RAG pipeline that enables users without programming skills to query and synthesize information from their Zotero libraries, with configurable chunking and model selection. The paper reports functional components (PDF retrieval, chunking, FAISS indexing, retrieval, LLM synthesis) and claims potential to expedite literature reviews, but provides no numeric performance metrics.",
            "limitations_or_challenges": "No empirical evaluation provided; potential for hallucination inherent to LLMs is noted and mitigated only by instructing the model to use provided context. Context-window/token limits require chunking and overlap tuning. System requires users' API keys (OpenAI, Zotero). Accuracy and sophistication of automated analysis remain areas for improvement per the authors. The corpus size, retrieval freshness, and scalability limits are not empirically characterized.",
            "comparison_to_baselines_or_humans": "No direct comparisons to baseline automated methods, other RAG implementations, or human literature-review performance are reported.",
            "uuid": "e3848.0",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-then-generation approach that augments LLMs with externally retrieved textual context: documents are chunked, embedded, stored in a vector index, and semantically-retrieved fragments are supplied to an LLM to produce grounded responses.",
            "citation_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "mention_or_use": "mention",
            "system_or_method_name": "Retrieval-Augmented Generation (RAG)",
            "system_or_method_description": "Described as a two-stage approach: text corpora are split into overlapping fragments, fragments are converted to vector embeddings and stored in a vector database, queries are embedded and nearest-neighbor retrieval returns semantically relevant fragments, and an LLM consumes the query plus retrieved fragments to generate a synthesized answer. RAG addresses models' context-window constraints and reduces hallucination by grounding generation in retrieved sources.",
            "input_corpus_description": "General large text corpora described in the paper as being chunked into smaller overlapping fragments; applicable to multiple documents and domains (no numeric corpus details provided in this paper).",
            "topic_or_query_specification": "Queries are provided in natural language and converted to embeddings for vector-similarity retrieval; retrieved passages and the original query are forwarded to the LLM for generation.",
            "distillation_method": "Semantic retrieval via vector embeddings (embedding model unspecified in the general description) followed by LLM-based synthesis conditioned on retrieved fragments (i.e., RAG). The paper cites literature (Lewis et al., Chen et al.) for the method.",
            "output_type_and_format": "Coherent, context-grounded natural-language responses or summaries that synthesize information across retrieved documents.",
            "evaluation_or_validation_method": "This paper does not present an evaluation of RAG itself but cites prior work (Lewis et al., Chen et al.) for RAG methods and benchmarking; the authors note RAG's usefulness but do not provide new benchmarks.",
            "results_summary": "RAG is presented as an effective strategy to navigate context-window constraints and to synthesize across documents; no new quantitative results are provided in this paper.",
            "limitations_or_challenges": "RAG previously required coding knowledge to implement; the paper also mentions general LLM issues like hallucination and context-window limits that RAG mitigates but does not eliminate. Implementation details (indexing, retriever quality, freshness) can affect performance.",
            "comparison_to_baselines_or_humans": "The paper claims RAG addresses limitations of plain chat-based LLM use (e.g., limited context window, hallucination) and enables synthesis across many documents, but it does not report formal baseline comparisons in this work.",
            "uuid": "e3848.1",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "OpenAI GPT / ChatGPT / Claude / Bard",
            "name_full": "Large Chat-optimized Language Models (e.g., ChatGPT/GPT-4, Claude, Bard)",
            "brief_description": "Commercial chat-optimized LLMs capable of summarizing and synthesizing text; discussed in the paper as representative LLMs used for literature summarization but with inherent context-window size and hallucination limitations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Chat-based LLM summarization (e.g., GPT-4, Claude, Bard)",
            "system_or_method_description": "LLMs are used in a chat format to summarize single publications or answer questions; their native approach is to process prompt + context and generate a response, but they are constrained by model context window sizes and can hallucinate unsupported facts.",
            "input_corpus_description": "Individual papers or aggregated text passed into the chat model as prompt/context. The paper cites approximate context window sizes (public ChatGPT-4 ≈ 5000 words; Claude up to 75,000 words) as relevant constraints.",
            "topic_or_query_specification": "Natural-language prompts in a chat interface.",
            "distillation_method": "Direct prompt-based summarization/QA (non-retrieval), i.e., feeding text to the model and asking for summaries or answers. The authors contrast this with RAG for large multi-document synthesis.",
            "output_type_and_format": "Natural-language summaries and answers returned in chat form.",
            "evaluation_or_validation_method": "No evaluation reported in this paper for these models; the paper discusses known failure modes (hallucination) and context-window limitations.",
            "results_summary": "Authors note that chat LLMs can summarize single publications but face limitations when synthesizing across many documents due to context-window constraints and hallucination risk; Claude's larger context window expands single-document summarization but still has restrictions for multi-document synthesis.",
            "limitations_or_challenges": "Context window length limits the amount of text that can be given to the model in a single prompt; hallucinations (confabulations) remain a concern; some models cannot practically summarize many long academic documents without retrieval/chunking.",
            "comparison_to_baselines_or_humans": "Compared qualitatively to RAG: chat-only LLM usage is less suitable for multi-document synthesis and more prone to hallucination; no empirical human comparisons given.",
            "uuid": "e3848.2",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LangChain",
            "name_full": "LangChain (document loaders and chunking utilities)",
            "brief_description": "A tooling library used inside KNIMEZoBot to load PDFs (unstructuredPDFLoader) and split long documents into smaller overlapping chunks to respect LLM input size limits prior to embedding and indexing.",
            "citation_title": "Introduction | Langchain",
            "mention_or_use": "use",
            "system_or_method_name": "LangChain document loading + chunking",
            "system_or_method_description": "LangChain's PDF loader is used to read full-text PDFs and split them into smaller token-limited chunks with configurable chunk size and overlap to satisfy LLM input constraints; these chunks are then embedded and indexed for retrieval.",
            "input_corpus_description": "Full-text PDF documents retrieved from Zotero; language and document formats as supported by the chosen LangChain loader and unstructured[pdf] tooling.",
            "topic_or_query_specification": "Chunking is controlled by user settings (chunk size and overlap) input via the KNIME UI; queries remain natural-language prompts to the chatbot.",
            "distillation_method": "Preprocessing step in the RAG pipeline: chunking long documents into smaller segments to allow embedding and retrieval; not a distillation algorithm by itself but essential to RAG.",
            "output_type_and_format": "Smaller text chunks (string segments) saved to be embedded and stored in the vector index.",
            "evaluation_or_validation_method": "No evaluation reported for chunking choices; chunk size and overlap are user-configurable and suggested to be tuned according to model limits.",
            "results_summary": "LangChain enabled processing of long PDFs by splitting them into token-sized chunks for embedding and retrieval; no performance metrics reported.",
            "limitations_or_challenges": "Chunking introduces trade-offs: too-small chunks can lose cross-sentence context; overlap can create redundancy and increased indexing cost; optimal parameters depend on the model and corpus but are not empirically optimized in the paper.",
            "comparison_to_baselines_or_humans": "No comparisons to alternative chunking approaches are provided.",
            "uuid": "e3848.3",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "FAISS (vector store)",
            "name_full": "FAISS Vector Store (Facebook AI Similarity Search)",
            "brief_description": "A vector index used in the KNIMEZoBot pipeline to store embeddings of document chunks and perform efficient nearest-neighbor semantic retrieval for RAG.",
            "citation_title": "FAISS Vector Store Creator",
            "mention_or_use": "use",
            "system_or_method_name": "FAISS-based vector retrieval",
            "system_or_method_description": "Embeddings produced by OpenAI's embedding model are stored in a FAISS index (via KNIME's FAISS Vector Store Creator node) to enable fast similarity search and retrieval of the most semantically relevant text chunks for a given query embedding.",
            "input_corpus_description": "Vector embeddings of text chunks derived from Zotero PDFs (chunking done with LangChain); no numeric index size reported.",
            "topic_or_query_specification": "Queries are embedded and used to run nearest-neighbor searches in FAISS to select candidate context chunks for the LLM.",
            "distillation_method": "FAISS performs approximate/efficient similarity search as the retriever in the RAG pipeline; it is an engineering component enabling retrieval rather than a distillation algorithm itself.",
            "output_type_and_format": "Ordered list of top-k retrieved text chunks (strings) returned to the generation stage.",
            "evaluation_or_validation_method": "No retrieval-accuracy or latency metrics provided in the paper.",
            "results_summary": "FAISS was used effectively as the vector index in the KNIME workflow enabling semantic retrieval; no quantitative retrieval performance is reported.",
            "limitations_or_challenges": "Paper does not report empirical scalability or accuracy tests; FAISS performance depends on embedding quality, index configuration, and corpus size which are not characterized here.",
            "comparison_to_baselines_or_humans": "No comparisons to other vector stores (e.g., Chroma) are reported.",
            "uuid": "e3848.4",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2
        },
        {
            "paper_title": "Benchmarking Large Language Models in RetrievalAugmented Generation",
            "rating": 2
        },
        {
            "paper_title": "Artificial intelligence and the conduct of literature reviews",
            "rating": 1
        },
        {
            "paper_title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation",
            "rating": 1
        }
    ],
    "cost": 0.010525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation.</h1>
<p>Suad Alshammari ${ }^{1,2}$, Lama Basalelah ${ }^{1,3}$, Walaa Abu Rukbah ${ }^{1,4}$, Ali Alsuhibani ${ }^{1,5}$ and Dayanjan S. Wijesinghe ${ }^{1,6,7}$.</p>
<ol>
<li>Department of Pharmacotherapy and Outcomes Sciences, School of Pharmacy, Virginia Commonwealth University. 2. Faculty of Pharmacy, Northern Border University, Saudi Arabia. 3. Faculty of Pharmacy, Imam Abdulrahman Bin Faisal University, Saudi Arabia. 4. Faculty of Pharmacy, University of Tabuk, Saudi Arabia. 5. Department of Pharmacy Practice, Unaizah College of Pharmacy, Qassim University, Unaizah, Saudi Arabia. 6. Institute for Structural Biology, Drug Discovery and Development, Virginia Commonwealth University, Richmond, Virginia, USA. 7. Da Vinci Center, School of Pharmacy, Virginia Commonwealth University School of Medicine, Richmond, Virginia, USA.</li>
</ol>
<p>Project files to be found at: https://github.com/dayanjan-lab/KNIMEZoBot</p>
<h4>Abstract</h4>
<p>Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed Al tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
<h1>Introduction:</h1>
<p>The current era witnesses academicians, clinicians, and researchers grappling with the formidable challenge of information overload ${ }^{1,2}$. The incessant surge in published research findings over recent years has significantly outpaced the ability to stay updated. This challenge is poised to intensify with the advent of natural language optimized Al technologies, which are expected to propel the pace of discoveries and subsequent publications at an even faster rate ${ }^{3}$. The dire need for a mechanism to proficiently manage and assimilate this burgeoning knowledge is palpable. Within this complex quandary, two distinct scenarios emerge:</p>
<p>Firstly, the task of extracting precise answers from a pre-existing knowledge corpus poses a hurdle ${ }^{1}$. Researchers typically amass publications pertinent to their expertise in reference libraries. This textual corpus expands over time with the continual influx of new findings. The task of extracting specific information from this meticulously curated content escalates in complexity with the growing volume of publications, compelling users to sift through numerous documents. Consequently, the appeal for an Al-driven platform capable of synthesizing information from multiple different publications to precise queries from an ever-expanding corpus of curated scientific literature within personal and group reference libraries is burgeoning.</p>
<p>Secondly, the endeavor of encapsulating knowledge through exhaustive literature reviews unveils knowledge gaps and unveils avenues for consequential research ${ }^{4}$. This endeavor entails the conduct of meticulous literature reviews which begin with the identification and collation of relevant publications to address the posed inquiries. Post collection, a thorough analysis of the amassed information is essential to derive answers to specific queries. The conventional workflow of executing literature reviews is notably time-consuming and labor-intensive ${ }^{5}$. Given the swift pace of new discoveries, the traditional approach often yields a knowledge summary that becomes obsolete by the time of its completion.</p>
<p>The imperativeness for simplified, automated strategies enabling researchers to query curated literature libraries and routinely refresh their domain knowledge is apparent. The recent strides in artificial intelligence (AI), particularly the Large Language Models (LLMs), harbor the potential to alleviate the aforementioned challenges ${ }^{6}$. Platforms like ChatGPT, Claude, or Bard are proficient in summarizing papers and synthesizing findings across multiple documents ${ }^{7}$. However, their native "chat" formats present certain impediments for academic research. These include constraint of context window lengths ${ }^{8}$ and the propensity for confabulation (hallucination) ${ }^{9}$. For instance, publicly available chatGPT4 has a context window of about 5000 words, while Claude's window extends to 75,000 words, rendering a bulk of academic publications too lengthy for chat GPT. Claude, although capable of summarizing single publications, finds its utility curtailed across multiple documents.</p>
<p>A notable breakthrough addressing the context window limitations and hallucination and aiding in data summarization from diverse documents is the Retrieval Augmented Generation (RAG) approach ${ }^{10,11}$. The operational workflow of RAG commences with the segmentation of broad text corpora into smaller, overlapping textual fragments. Following this, these fragments</p>
<p>are transformed into vector representations and cataloged within a vector-based database. Upon query submission, it's converted into a vector form. A vectorial similarity assessment is then executed to identify all text segments within the database showcasing semantic alignment. The query, along with all relevant text fragments, is forwarded to a Large Language Model (LLM) to formulate a coherent and pertinent response. This technique adeptly navigates the context window constraints, fostering response synthesis across varied domains. The capability to repeatedly execute this process establishes a robust question-and-answer framework, invaluable for extensive literature reviews, serving scholars and medical professionals.</p>
<p>The RAG-based system, although expedient in summarizing information, hitherto necessitated substantial coding knowledge. Recognizing that a sizable faction of academicians and clinicians lacks coding expertise, we orchestrated a code-free, open-source strategy, culminating in the creation of KNIMEZoBot. This innovation amalgamates three pivotal elements: Konstanz Information Miner (KNIME) - a code-free data science platform, Zotero - an open-source reference management system, and GPT4 from Open AI - the chosen language model for knowledge synthesis. KNIMEZoBot heralds a revolutionary stride in enhancing the literature review workflow by seamlessly integrating the prowess of reference managers, scholarly databases, and AI. Through this ingenious approach, we have democratized access to AIpowered research tools, opening doors for those with non-coding backgrounds to harness natural language queries for interacting with the curated publications housed in their Zotero libraries, thereby significantly amplifying the accessibility and utility of Al in academic spheres.</p>
<h1>Materials and Methods</h1>
<p>KNIME: For the development of the KNIMEZoBot platform, KNIME played an integral role in providing the graphical modular interface for building the workflow steps, integrating the Zotero and OpenAI APIs seamlessly via dedicated nodes, processing the extracted text data, creating the FAISS vector indexing workflow, and hosting the final chatbot user interface. KNIME is an open-source platform originating from the University of Konstanz in Germany, catering to data analytics, reporting, and integration needs, with a strong footing in data science and machine learning domains ${ }^{12}$. It's a free, community-enhanced tool, widely embraced by data professionals globally owing to its user-friendly, graphical interface enabling code-free workflow creation, modification, and visualization. KNIME's core strength is its extensive node repository facilitating seamless data pipeline construction for tasks ranging from data preprocessing to advanced analytics using a non/low code approach. It boasts robust data integration, connecting effortlessly to various data sources like databases and web services, thus centralizing data for comprehensive analysis. Scalability is a hallmark of KNIME, adeptly managing small to large datasets, with ease of integration into big data frameworks like Apache Hadoop and Apache Spark. The platform supports building, training, and evaluating machine learning models utilizing popular libraries such as scikit-learn and TensorFlow, alongside offering an array of statistical and analytical techniques. Automation is seamless with KNIME, allowing scheduled workflow executions, while its server facilitates collaborative efforts and workflow sharing. Commercial versions of KNIME extend advanced features and support, enriching its open-source ecosystem. It's a versatile tool for creating insightful reports, visualizing data, and finds applications across</p>
<p>diverse fields including bioinformatics, predictive analytics, business intelligence, and industrial research.</p>
<h1>KNIME extensions used:</h1>
<p>KNIME AI Extension (Labs) ${ }^{13}$ : The KNIME labs extension enables users to leverage powerful large language models (LLMs) from OpenAI, Hugging Face Hub, and GPT4ALL for tasks like chat and text embeddings. It also provides connectivity to vector stores like Chroma and FAISS for building knowledge bases that can inform chatbot responses. The extension allows combining vector stores and LLMs into intelligent agents. These agents can dynamically select the most relevant vector store to query based on the user input, enabling more natural and knowledgeable conversations. Overall, this extension brings together state-of-the-art LLMs and vector stores within the KNIME analytics platform, unlocking new possibilities for building conversational interfaces and knowledge-powered AI assistants.</p>
<p>KNIME REST Client Extension ${ }^{14}$ : The KNIME REST Client Extension provides nodes for making REST API calls within KNIME workflows. This enables seamless integration with web services and APIs.</p>
<p>The Get Request node ${ }^{15}$ is used to send HTTP GET requests to REST endpoints. It allows specifying the URL, headers, query parameters, and authentication settings. The response from the REST API is returned as a JSON/XML document that can be further processed in the KNIME workflow ${ }^{14}$.</p>
<p>KNIME Python 2 Integration (legacy) ${ }^{16}$ : This extension encompasses the legacy version of KNIME Python integration. It facilitates the integration of Python 2 and Python 3 scripts within the KNIME platform. The extension operates by executing Python scripts in a local Python installation, which is not included in the extension package.</p>
<p>KNIME Python Integration ${ }^{17}$ : The "KNIME Python Integration" is the modern and preferred choice for Python integration within KNIME. This extension incorporates nodes that enable the execution of Python 3 scripts seamlessly in the KNIME workflow. Notably, this integration brings substantial performance improvements compared to its legacy counterpart. It also provides enhanced support for handling larger-than-memory datasets. Additionally, this extension comes equipped with a Python installation that includes a curated selection of essential Python packages.</p>
<p>Note: Throughout our workflow, we employed both the "KNIME Python 2 Integration (legacy)" and the "KNIME Python Integration" extensions interchangeably.</p>
<p>KNIME JSON-Processing ${ }^{18}$ : The KNIME JSON-Processing extension provides nodes for working with JSON data within KNIME workflows. It allows for parsing, creating, transforming, and serializing JSON documents. The JSON To Table node enables easy ingestion of JSON data into tabular form for use in KNIME workflows. It reduces the complexity of handling nested JSON structures and schemas.</p>
<p>Python (Version 3.9) ${ }^{19}$ : Integrating of Python and Anaconda with KNIME can be a powerful combination that allows data scientists and analysts to leverage the extensive libraries and capabilities of Python within the KNIME analytics platform. This integration provides a seamless way to utilize Python scripts, packages, and machine learning models within the KNIME workflows. A detailed guide for Python integration in KNIME has been published elsewhere ${ }^{20}$</p>
<p>Specifically, Python nodes were utilized to execute API calls to extract metadata and PDF files from the Zotero reference manager using its REST API bindings. The Python Requests library facilitated sending GET requests to the API and processing the responses. Another vital usage was the Langchain library ${ }^{21}$ within a Python node to load in the full text of PDF papers and segment them into smaller chunks that meet the length limits of the GPT model inputs.</p>
<p>The Python nodes accept inputs from earlier workflow components, run the defined Python logic and code using those inputs, and return any outputs to subsequent nodes in the workflow. For instance, a node might accept a list of extracted PDFs from the Zotero API calls, utilize Langchain to chunk each PDF into shorter text segments, and output these chunks to the next node for vectorization.</p>
<p>The following Python packages were installed and imported within the Python nodes in KNIME to support core functionality. The installation can be done in multiple ways; we used the following:</p>
<p>1- Open Anaconda Prompt from the Start menu.
2- Write this command: "conda activate <your_environment>." Your_environment is the environment name that is set up in the KNIME Python preference.
3- After the name is changed from base to the name of the environment, install the following libraries via pip:</p>
<ul>
<li>pip install pandas openai langchain unstructured fitz PyPDF2 PyMuPDF "unstructured[pdf]"</li>
</ul>
<h1>Zotero:</h1>
<p>Zotero, a free, open-source reference management software ${ }^{22}$, is cherished by a broad spectrum of academia and professionals for easing the collection, organization, and citation of research materials. Originating from George Mason University, it's a boon for scholarly research and writing, streamlining reference, citation, and bibliography management. Key facets include effortless reference collection from diverse sources like websites and academic journals, with automatic citation information extraction from web pages and PDFs. Its intuitive interface facilitates organizing references via folders, tags, and notes, ensuring easy retrieval. A hallmark feature is its citation and bibliography generation in numerous styles like APA and MLA, significantly reducing formatting time. Integration with prevalent word processors like Microsoft Word and Google Docs allows direct citation insertion and bibliography generation in documents, ensuring accuracy and consistency. Its PDF management capability lets users attach, organize, and annotate PDFs within the reference library. Zotero encourages collaborative research through shared library features, vital for research teams. It offers cloud synchronization for easy access across devices and data backup, enhancing data security. Browser extensions for Chrome and</p>
<p>Firefox simplify capturing references online. Being open-source, it's continually evolved by community contributions, and its cross-platform availability extends its reach. Applications are vast, aiding academic research, education, library assistance, and professionals across legal, medical, and media fields in managing and citing a vast array of references effortlessly.</p>
<h1>Results and Discussion</h1>
<h2>KNIMEZoBot:</h2>
<p>The developed application "KNIMEZoBot", represents an innovative integration of Zotero and OpenAI through the code free platform KNIME to streamline literature reviews and research. This project seamlessly combines the above-mentioned Zotero reference manager, with OpenAI's powerful natural language processing capabilities via a RAG based approach using KNIME as the interface. The primary goal is to simplify retrieving PDFs from Zotero libraries and collections and then utilize OpenAI within KNIME workflows to ask insightful questions and extract key information from academic papers.</p>
<p>KNIMEZoBot uses a Retrieval-Augmented Generation (RAG) architecture, first conducting a semantic search to identify relevant passages from retrieved PDFs. It then leverages large language models (in this case OpenAI's GPT models) to synthesize natural language answers based on the extracted information. This enables KNIMEZoBot to provide informative responses to questions by efficiently searching academic papers and distilling salient facts and main points. Overall, the integration of Zotero and OpenAI represents an innovative approach to enhance literature reviews and research by combining reference management, scholarly databases, and OpenAI.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure1: Underlying overall workflow for KNIMEZoBot.</p>
<h2>First component (Setup and configure Zotero):</h2>
<p>In order to effectively use the KNIMEZoBot, users need to follow a series of key steps. The first requirement is selecting the type of Zotero library they want to access - either a personal</p>
<p>Zotero library or a group library. Based on that choice, users will need to input their corresponding Zotero API key, which allows the system to interface with the library.</p>
<p>Additionally, users will need to provide either their personal Zotero user ID if accessing their own library, or the group ID if accessing a shared group library. To assist users in easily finding and copying their user ID or group ID, we have included hyperlinks within the system interface that direct users to Zotero guides with instructions on locating that information.</p>
<p>Furthermore, to enable more targeted searches, users have the option to filter based on Zotero collections. This allows them to refine the content being retrieved from their library down to specific collections, rather than everything in the library. The system was designed to be flexible - some users may want to search across their entire library, while others may want to narrow in on papers from select collections.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Component "Setup Zotero" when executed - User interface of KNIMEZoBot. Users are required to complete the Zotero information fields.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Step</span><span class="w"> </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="nx">Interacting</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">Collections</span><span class="p">:</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="err">&quot;</span><span class="nx">Set</span><span class="w"> </span><span class="nx">X</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">specific</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span>
<span class="w">    </span><span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collection</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">own</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="nx">Yes</span><span class="err">&#39;</span><span class="nx">X</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">these</span><span class="w"> </span><span class="nx">key</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">PDFs</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">PDF</span><span class="w"> </span><span class="nx">files</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">own</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="nx">Note</span><span class="p">:</span>
<span class="w">    </span><span class="nx">Collection</span><span class="w"> </span><span class="nx">ID</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">part</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">URL</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collections</span><span class="o">/</span><span class="nx">COREQAD</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">collection</span><span class="w"> </span><span class="nx">ID</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">1</span><span class="nx">COREQAD</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Do</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collection</span><span class="p">?</span>
<span class="w">    </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nx">Yes</span>
<span class="w">    </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nx">Yes</span>
<span class="w">    </span><span class="nx">Collection</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span>
<span class="w">    </span><span class="mi">2</span>
</code></pre></div>

<p>Figure 3: Continuation of the first component when executed. We provided options to filter by specific collections.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The "setup Zotero" component contains widget nodes ${ }^{23}$ that allow the user to input the information required by the system. It also includes text output nodes and a header node that control the appearance of the user interface.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The first metanode processes information from the "setup zotero" component. Specifically, the "python script" node ${ }^{24}$ accesses and extracts all data from the Zotero library. The "get request" node ${ }^{15}$ (a child node) retrieves attached files for each Zotero item. Subsequently, the 'binary objects to files' node ${ }^{25}$ is employed to facilitate the secure storage of PDF documents in a pre-defined temporary directory within the workflow.</p>
<h1>Second component (Setup OpenAI):</h1>
<p>The second core component of the system involves setting up the OpenAI environment according to the user's preferences. Users have the ability to adjust key settings such as chunk size and chunk overlap. Chunk size refers to the maximum number of tokens processed per API request, while overlap determines the number of duplicated tokens between chunks. Giving users control over these parameters enables them to customize the configuration based on their specific computational needs and use case.</p>
<p>After inputting their OpenAI API key, which grants access to the AI models, users can select from a variety of available models offered through the OpenAI API. Users can make a selection from a range of available OpenAI models, including but not limited to GPT-3.5 Turbo and GPT-4.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Component "Setup OpenAI" when executed- Users are required to select chunk size and overlap settings for text processing, enter their OpenAI API key, and select an AI model.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: The "setup OpenAI" component contains widget nodes enabling users to input their OpenAI API key, chunk size, chunk overlap, and select an AI model as required by the system. Additionally, text output nodes and a header node are included for the appearance of the user interface. The "Python Script" node was used to read and split the PDFs to smaller chunks. We used the Langchain package ${ }^{21}$ "unstructuredPDFLoader" to load the documents. The documents were then split into smaller chunks because of size limitation as GPT models have a maximum input size, usually 1024-2048 tokens. Breaking PDFs into smaller chunks allows to feed longer documents into the models.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: In the metanode, we used the "FAISS Vector Store Creator" node ${ }^{26}$. This node will store the numerical vector representation created by the embedding model from the "OpenAI Embeddings Connector" node ${ }^{27}$. Also, we selected "text-embedding-ada-002" as the embedding model.</p>
<p>The "OpenAI Functions Agent Creator" node ${ }^{28}$ is used to customize the system message. We rote the following message "You are KnimeZoBot, an AI assistant specifically designed to seamlessly integrate the power of the KNIME platform with the vast knowledge stored within your Zotero library. Your mission is to provide the user with a unique and efficient way to access information, answer questions, and streamline users' research tasks by tapping into your personal Zotero library. Get the answer only from the provided information and if it is not store there write "I apologize, but I do not have any information about it in my Zotero library."</p>
<h1>Last component (Chat app):</h1>
<p>The last component of the system is the Chat application, which provides an interactive interface for users to engage with their Zotero library. This chatbot-style app enables users to pose questions and queries about the content of their Zotero library in a natural conversational format. The seamless integration of the chatbot with the Zotero reference database creates a convenient and user-friendly method for users to search for information within their library.</p>
<p>In addition, users have the option to download their full conversation history with the chatbot in a .csv format. This allows users to save all of their questions and the chatbot's responses so they can refer back to the information later.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Final Component "Chat App"- Chat Interface when deployed. This component allows users to ask questions and receive answers through a conversational chatbot. Users can also download their full chat history as a CSV file.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: The final "Chat app" component utilizes an "Agent Prompter" node ${ }^{29}$ to leverage an AI agent, prompts, and the conversation history from the input table to generate responses. The conversation table requires at least two string columns to store previous exchanges. Additionally, an option is provided to save the chat history.</p>
<h1>Conclusion:</h1>
<p>In summary, the KNIMEZoBot represents a promising integration of technologies to expedite literature reviews or undertake natural language queries of existing Zotero libraries. By</p>
<p>unifying the capabilities of Zotero, OpenAI, and KNIME, this system automates laborious tasks such as combing through academic papers to identify relevant information. Researchers can save significant time while benefiting from state-of-the-art Al techniques for synthesizing knowledge in a low code manner. This innovation demonstrates the potential for Al to assume a greater role in accelerating informed research. While further enhancements to the accuracy and sophistication of the automated analysis remain desirable, KNIMEZoBot marks an important step toward streamlining access to critical information in existing literature by domain experts who are not coders by training. By facilitating more rapid and comprehensive understanding of prior work, this system could substantially benefit the research community and knowledge-building process.</p>
<h1>References:</h1>
<ol>
<li>Arnold M, Goldschmitt M, Rigotti T. Dealing with information overload: a comprehensive review. Front Psychol. 2023;14:1122200. doi:10.3389/fpsyg.2023.1122200</li>
<li>Bornmann L, Haunschild R, Mutz R. Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Humanit Soc Sci Commun. 2021;8(1):1-15. doi:10.1057/s41599-021-00903-w</li>
<li>Kousha K, Thelwall M. Artificial intelligence to support publishing and peer review: A summary and review. Learn Publ. n/a(n/a). doi:10.1002/leap. 1570</li>
<li>Paré G, Kitsiou S. Chapter 9 Methods for Literature Reviews. In: Handbook of eHealth Evaluation: An Evidence-Based Approach [Internet]. University of Victoria; 2017. Accessed November 3, 2023. https://www.ncbi.nlm.nih.gov/books/NBK481583/</li>
<li>Tay A. How to write a superb literature review. Nature. Published online December 4, 2020. doi:10.1038/d41586-020-03422-x</li>
<li>Wagner G, Lukyanenko R, Paré G. Artificial intelligence and the conduct of literature reviews. J Inf Technol. 2022;37(2):209-226. doi:10.1177/02683962211048201</li>
<li>Nashwan AJ, Jaradat JH. Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation. Cureus. 15(8):e43023. doi:10.7759/cureus. 43023</li>
<li>Stern J. GPT-4 Has the Memory of a Goldfish. The Atlantic. Published March 17, 2023. Accessed November 3, 2023. https://www.theatlantic.com/technology/archive/2023/03/gpt-4-has-memory-context-window/673426/</li>
<li>Sharun K, Banu SA, Pawde AM, et al. ChatGPT and artificial hallucinations in stem cell research: assessing the accuracy of generated references - a preliminary study. Ann Med Surg 2012. 2023;85(10):5275-5278. doi:10.1097/MS9.0000000000001228</li>
<li>Lewis P, Perez E, Piktus A, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Published online April 12, 2021. doi:10.48550/arXiv.2005.11401</li>
<li>Chen J, Lin H, Han X, Sun L. Benchmarking Large Language Models in RetrievalAugmented Generation. Published online September 4, 2023. doi:10.48550/arXiv.2309.01431</li>
<li>Berthold MR, Cebron N, Dill F, et al. KNIME - the Konstanz information miner: version 2.0 and beyond. ACM SIGKDD Explor Newsl. 2009;11(1):26-31. doi:10.1145/1656274.1656280</li>
<li>KNIME AI Extension (Labs). KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest</li>
<li>
<p>KNIME REST Client Extension. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.rest/latest</p>
</li>
<li>
<p>GET Request. KNIME Community Hub. Accessed November 7, 2023.
https://hub.knime.com/knime/extensions/org.knime.features.rest/latest/org.knime.rest.nodes .get.RestGetNodeFactory</p>
</li>
<li>KNIME Python 2 Integration (legacy). KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python2/latest</li>
<li>KNIME Python Integration. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python3.scripting/latest</li>
<li>KNIME JSON-Processing. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.json/latest</li>
<li>Python Release Python 3.9.0. Python.org. Accessed November 7, 2023. https://www.python.org/downloads/release/python-390/</li>
<li>KNIME Python Integration Guide. Accessed November 7, 2023. https://docs.knime.com/2021-12/python_installation_guide/index.html#_introduction</li>
<li>Introduction | Langchain. Accessed November 7, 2023. https://python.langchain.com/docs/get_started/introduction</li>
<li>credits_and_acknowledgments [Zotero Documentation]. Accessed November 3, 2023. https://www.zotero.org/support/credits_and_acknowledgments#about_zotero</li>
<li>Explore the Wonderful World of KNIME Widgets. KNIME. Accessed November 7, 2023. https://www.knime.com/blog/mini-guide-widget-examples</li>
<li>Python Script. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python3.scripting/latest/org.kni me.python3.scripting.nodes.script.PythonScriptNodeFactory</li>
<li>Binary Objects to Files. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.base.filehandling/latest/org.kni me.base.filehandling.binaryobjects.writer.BinaryObjectsToFilesNodeFactory</li>
<li>FAISS Vector Store Creator. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:e1168c28</li>
<li>OpenAI Embeddings Connector. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:3a4ffd4b</li>
<li>OpenAI Functions Agent Creator. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:232d61e6</li>
<li>Agent Prompter. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:378eea</li>
</ol>
<p>https://github.com/dayanjan-lab/KNIMEZoBot</p>            </div>
        </div>

    </div>
</body>
</html>
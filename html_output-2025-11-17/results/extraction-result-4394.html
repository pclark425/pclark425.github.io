<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4394 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4394</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4394</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-273374881</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12601v3.pdf" target="_blank">CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization</a></p>
                <p><strong>Paper Abstract:</strong> To broaden the dissemination of scientific knowledge to diverse audiences, it is desirable for scientific document summarization systems to simultaneously control multiple attributes such as length and empirical focus. However, existing research typically focuses on controlling single attributes, leaving the compositional control of multiple attributes underexplored. To address this gap, we introduce CCSBench, the first evaluation benchmark for compositional controllable summarization in the scientific domain. Our benchmark enables fine-grained control over both explicit attributes (e.g., length), which are objective and straightforward, and implicit attributes (e.g., conceptual or empirical focus), which are more subjective and abstract. We conduct extensive experiments using various large language models (LLMs) under various settings, including in-context learning, parameter-efficient fine-tuning, and two-stage modular methods for balancing control over different attributes. Our findings reveal significant limitations in LLMs capabilities in balancing trade-offs between control attributes, especially implicit ones that require deeper understanding and abstract reasoning.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4394.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4394.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that demonstrates using large language models to automatically compose literature surveys by synthesizing content from scientific papers; referenced in this paper as an example of LLM-driven literature-review synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in the CCSBench paper as an example of LLM-based literature-review synthesis; CCSBench does not provide implementation details but cites AutoSurvey as prior work showing LLMs can synthesize multiple papers into survey-style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based summarization/synthesis of literature (explicit technique not specified in CCSBench)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Survey-style synthesis across multiple papers (specific multi-document aggregation technique not specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>general scientific literature / literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>literature surveys / literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that LLMs can perform literature-review synthesis; CCSBench uses this to motivate LLM use for scientific-document processing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4394.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Adaptation (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient fine-tuning (PEFT) technique that inserts low-rank trainable matrices into a pretrained model to adapt it to a downstream task, used in this paper to adapt open-source LLMs for controllable scientific summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LoRA: Low-Rank Adaptation of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LoRA (PEFT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LoRA is applied to multiple model families in CCSBench to fine-tune them for compositional controllable summarization; CCSBench inserts low-rank trainable matrices into pretrained models and fine-tunes those added parameters while leaving the base model largely frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5-XL, Flan-T5-XXL, LLaMA2-7B, Mistral-7B (as PEFT targets in the reported experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Supervised fine-tuning (PEFT) on I+C sections with gold summaries to adapt models to extract salient information under control attributes</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-document controlled summarization (compositional control learned via LoRA adapters) rather than explicit multi-document theory construction</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>per-instance: single source paper (I+C sections) used to generate one summary</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>computer science / AI papers (arXiv)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>attribute-controlled scientific summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE (quality), PCC & MAD (length control), Success Rate (keywords), FKGL delta (readability), F1 (focus)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>PEFT (LoRA) results reported in paper: e.g., Flan-T5-XL (B1) ROUGE-L 22.22, PCC 0.49, MAD 0.55, SR 0.78, FKGL normal 13.11 / high 9.37 (delta 3.59), Focus F1 0.70; Flan-T5-XXL (B2) ROUGE-L 23.43, PCC 0.78, MAD 0.27, SR 0.85, FKGL normal 14.40 / high 10.78 (delta 3.62), Focus F1 0.75; decoder-only LoRA variants (LLaMA2-7B B3, Mistral-7B B4) showed smaller gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Zero-shot/few-shot closed-source LLMs (GPT-3.5, GPT-4, GPT-4o) and other baselines (hard prompt, soft prefix)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LoRA PEFT improves controllable summarization performance; encoder-decoder architectures benefit more from PEFT than decoder-only models for compositional control, particularly on implicit attributes (readability, focus).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Even with LoRA, models struggle with implicit attributes and balancing attribute trade-offs; decoder-only architectures still suffer from degraded long-range attention and weak compositional control.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Larger encoder-decoder model (Flan-T5-XXL) with LoRA shows better compositional control than smaller variants; architecture and size interact with PEFT effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4394.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRAHub</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoRAHub (dynamic LoRA composition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular two-stage approach that assigns fixed weights to LoRA modules corresponding to different attribute controls and composes them to attempt compositional controllability; implemented and evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LoRAHub (two-stage modular LoRA composition)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CCSBench implements LoRAHub as a two-stage method: stage 1 trains LoRA adapters on single-attribute tasks; stage 2 composes those adapters with fixed weights (LoRAHub) to produce summaries satisfying multiple attributes. The paper reports empirical evaluation of this composition approach.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5-XL (configured with LoRA modules and LoRAHub composition in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Adapter-based extraction where each LoRA module is trained to extract/rewrite for a single attribute; input is I+C sections and gold summaries during training</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Fixed-weight composition of attribute-specific LoRA modules to generate compositional outputs</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>per-instance: single source paper summarization (no multi-paper aggregation described)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>computer science / AI scientific summarization</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>compositional attribute-controlled summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, PCC, MAD, Success Rate (SR), FKGL delta, Focus F1 (as in CCSBench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported poor results: Flan-T5-XL-LoraHub (D1) ROUGE-L 14.31; PCC -0.05; MAD 2.59; SR 0.55; FKGL normal 13.69 / high 13.92 (delta -0.23); Focus F1 0.33 â€” indicative of low-quality, incoherent outputs and weak controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-attribute LoRA models and AdapterFusion two-stage method</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LoRAHub produced substantially worse ROUGE and controllability than single-attribute fine-tuned models and compared poorly to other composition attempts (e.g., AdapterFusion).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fixed-weight composition of LoRA modules is overly simplistic for compositional controllability; produces incoherent summaries and poor attribute balancing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Overly simple integration mechanism; cannot learn complex interactions between attributes; yields incoherent outputs and low ROUGE.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed beyond observed poor behavior on evaluated model (Flan-T5-XL); no evidence of improvement with scale in CCSBench experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4394.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdapterFusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdapterFusion: Non-Destructive Task Composition for Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention-based method that learns to fuse task-specific adapter modules; implemented in CCSBench as a two-stage approach to compose attribute-specific adapters for compositional controllable summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AdapterFusion: Non-Destructive Task Composition for Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AdapterFusion (attention-based adapter fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CCSBench implements AdapterFusion by training adapters for single-attribute tasks and then learning an attention fusion layer to weight and combine them for compositional control. The paper evaluates AdapterFusion's ability to balance multiple summary attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5-XL with adapters and fusion layer (experimental configuration in CCSBench)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Adapter-based extraction where each adapter is specialized to control one attribute; extraction is via the adapter representations conditioned on the input I+C text</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Attention-based fusion of multiple attribute adapters to synthesize a single summary output</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>per-instance: single scientific paper input (I+C), not multi-paper aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>computer science / AI scientific summarization</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>compositional controlled summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, PCC, MAD, SR, FKGL delta, Focus F1 (as in CCSBench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>AdapterFusion (D2) produced low controllability: reported metrics include ROUGE-L 17.65, PCC 0.04, MAD 1.40, SR 0.65, FKGL normal 14.31 / high 14.49 (delta 0.18), Focus F1 0.41; overall poor summarization quality relative to single-attribute models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-attribute LoRA models, LoRAHub, fully fine-tuned models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AdapterFusion failed to achieve meaningful compositional controllability and had lower ROUGE and attribute performance than single-attribute fine-tuned models; fusion exhibited bias toward the focus adapter per attention analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attention-based adapter fusion displayed an empirical bias toward the focus adapter and could not flexibly adjust emphasis across attributes, leading to interference and degraded output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Adapter fusion's aggregation is insufficiently expressive for complex attribute interactions; attention bias leads to over-prioritization of some attributes and poor compositional behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports observed adapter attention bias across test set; does not report positive scaling trends for AdapterFusion in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4394.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hard Prompt (HP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hard Prompting (structured attribute prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline controllable generation approach that uses structured, explicit natural-language instructions (e.g., 'Attribute: Value') to steer LLM outputs; implemented and evaluated in CCSBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hard Prompt (HP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CCSBench implements hard prompts by formatting control attributes explicitly in the input prompt (e.g., 'Length: Bin 0; Focus: high; Readability: normal; Keywords: ...') so that the LLM generates a summary conditioned on those written constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5-Large (evaluated as Flan-T5-Large-HP in CCSBench experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-conditioned summarization of I+C sections using explicit attribute fields</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct single-step generation from prompt; no adapter/module composition</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>per-instance: single paper</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>scientific summarization (CS/AI arXiv papers)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>attribute-controlled summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, PCC, MAD, SR, FKGL delta, Focus F1 (as used in CCSBench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported (C1) Flan-T5-Large-HP: ROUGE-L 21.36, PCC 0.56, MAD 0.53, SR 0.86, FKGL normal 13.71 / high 10.40 (delta 3.31), Focus F1 0.63.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Soft prefix tuning, PEFT LoRA, zero-shot/few-shot closed-source LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Hard prompt achieved reasonable explicit-attribute control but did not outperform the best PEFT encoder-decoder models on compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit structured prompts are a straightforward baseline for control, but struggle when multiple, interacting attributes must be balanced.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Performance degrades under compositional control compared to single-attribute settings; few-shot prompts limited by context window when inputs are long.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed beyond empirical results on evaluated model; few-shot demonstrations did not improve performance for long scientific inputs due to context window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4394.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft Prefix (SP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft Prefix Tuning (prefix embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A soft prompt baseline that prepends trainable continuous prefix embeddings to the model to exert fine-grained control over generation for each attribute; implemented and evaluated in CCSBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Soft Prefix (SP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CCSBench implements soft prefix tuning by adding attribute-specific trainable prefix embeddings to each layer; during training prefixes are learned for each attribute value and then used to guide generation at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Flan-T5-Large (evaluated as Flan-T5-Large-SP in CCSBench experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prefix-conditioned summarization where prefixes encode the desired attribute settings and influence generation; input is I+C sections</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-step generation conditioned on learned soft prefixes</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>per-instance: single paper</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>scientific summarization (CS/AI arXiv papers)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>controlled summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE, PCC, MAD, SR, FKGL delta, Focus F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported (C2) Flan-T5-Large-SP: ROUGE-L 20.58, PCC 0.63, MAD 0.52, SR 0.74, FKGL normal 13.47 / high 11.37 (delta 2.10), Focus F1 0.57.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Hard prompt, PEFT LoRA, zero-shot/few-shot closed-source LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Soft prefix tuning performed comparably to hard prompt baselines but was outperformed by the strongest PEFT encoder-decoder models on compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Soft prefixes can capture attribute signals but struggle with complex multi-attribute compositionality in scientific summarization contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited when multiple conflicting control attributes must be balanced; effectiveness constrained by the complexity of attribute interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not specifically analyzed for scale in CCSBench beyond the reported Flan-T5-Large experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4394.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CocoSciSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior toolkit that extends controllable summarization to scientific texts with support for multiple explicit attributes (e.g., length, keywords); cited by CCSBench as related work but limited to explicit attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CocoSciSum</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as prior work that extended control to length and keywords within scientific papers; CCSBench contrasts CocoSciSum's focus on explicit attributes with its own inclusion of implicit attributes (readability, empirical focus).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in CCSBench (described in the cited CocoSciSum work)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Controlled summarization across scientific documents focusing on explicit attributes (details not provided here)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>scientific paper summarization (CS/AI)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>controlled summaries with explicit attribute control</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CocoSciSum extended control to explicit attributes but did not handle subjective implicit attributes; CCSBench positions itself as advancing to implicit attributes and compositional settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited to explicit, objectively measurable attributes (length, keywords) per CCSBench discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4394.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MACSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MACSum: Controllable Summarization with Mixed Attributes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system for multi-attribute controllable summarization in the news and dialogue domains; cited as related work but not applied to scientific documents in CCSBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MACSum: Controllable Summarization with Mixed Attributes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MACSum</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as prior work that supports multi-attribute control in domains with less technical content (news, dialogue); CCSBench notes that scientific texts pose unique challenges beyond MACSum's scope.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>news and dialogue summarization (multi-attribute control)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>multi-attribute controlled summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MACSum supports mixed-attribute control in less technical domains; CCSBench highlights the need to adapt multi-attribute methods to scientific texts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not designed for the domain-specific complexity of scientific papers according to CCSBench.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4394.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LimGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that probes LLMs' ability to generate suggested limitations for research papers; referenced by CCSBench in the related materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LimGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in CCSBench's references as a project exploring LLM generation of suggestive limitations for papers; CCSBench does not detail LimGen's methods.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM generation of paper-specific limitations (details not provided in CCSBench)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>scientific papers / meta-analysis of papers</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>generated limitations / critique of papers</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work illustrating LLMs applied to generating meta-level content about scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4394.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4394.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Research-Idea-Generation Study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited human-study investigating whether LLMs can generate novel research ideas; referenced by CCSBench as an example of LLM-driven idea generation in scientific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based Research Idea Generation (study)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as prior work demonstrating LLMs' use for idea generation in scientific research; CCSBench does not report technical details of the study's system.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-driven idea generation from scientific content (specific mechanism not described in CCSBench)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>research idea generation for NLP / scientific research</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>proposed research ideas</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to illustrate LLMs' demonstrated potential to generate research ideas and the broader range of scientific-document processing tasks LLMs have been applied to.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AutoSurvey: Large Language Models Can Automatically Write Surveys <em>(Rating: 2)</em></li>
                <li>LoRA: Low-Rank Adaptation of Large Language Models <em>(Rating: 2)</em></li>
                <li>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition <em>(Rating: 2)</em></li>
                <li>AdapterFusion: Non-Destructive Task Composition for Transfer Learning <em>(Rating: 2)</em></li>
                <li>CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability <em>(Rating: 2)</em></li>
                <li>MACSum: Controllable Summarization with Mixed Attributes <em>(Rating: 1)</em></li>
                <li>LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers <em>(Rating: 1)</em></li>
                <li>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4394",
    "paper_id": "paper-273374881",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
            "brief_description": "A cited work that demonstrates using large language models to automatically compose literature surveys by synthesizing content from scientific papers; referenced in this paper as an example of LLM-driven literature-review synthesis.",
            "citation_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
            "mention_or_use": "mention",
            "system_name": "AutoSurvey",
            "system_description": "Described in the CCSBench paper as an example of LLM-based literature-review synthesis; CCSBench does not provide implementation details but cites AutoSurvey as prior work showing LLMs can synthesize multiple papers into survey-style outputs.",
            "llm_model_used": null,
            "extraction_technique": "LLM-based summarization/synthesis of literature (explicit technique not specified in CCSBench)",
            "synthesis_technique": "Survey-style synthesis across multiple papers (specific multi-document aggregation technique not specified here)",
            "number_of_papers": null,
            "domain_or_topic": "general scientific literature / literature reviews",
            "output_type": "literature surveys / literature reviews",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as evidence that LLMs can perform literature-review synthesis; CCSBench uses this to motivate LLM use for scientific-document processing.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4394.0",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LoRA",
            "name_full": "Low-Rank Adaptation (LoRA)",
            "brief_description": "A parameter-efficient fine-tuning (PEFT) technique that inserts low-rank trainable matrices into a pretrained model to adapt it to a downstream task, used in this paper to adapt open-source LLMs for controllable scientific summarization.",
            "citation_title": "LoRA: Low-Rank Adaptation of Large Language Models",
            "mention_or_use": "use",
            "system_name": "LoRA (PEFT)",
            "system_description": "LoRA is applied to multiple model families in CCSBench to fine-tune them for compositional controllable summarization; CCSBench inserts low-rank trainable matrices into pretrained models and fine-tunes those added parameters while leaving the base model largely frozen.",
            "llm_model_used": "Flan-T5-XL, Flan-T5-XXL, LLaMA2-7B, Mistral-7B (as PEFT targets in the reported experiments)",
            "extraction_technique": "Supervised fine-tuning (PEFT) on I+C sections with gold summaries to adapt models to extract salient information under control attributes",
            "synthesis_technique": "Single-document controlled summarization (compositional control learned via LoRA adapters) rather than explicit multi-document theory construction",
            "number_of_papers": "per-instance: single source paper (I+C sections) used to generate one summary",
            "domain_or_topic": "computer science / AI papers (arXiv)",
            "output_type": "attribute-controlled scientific summaries",
            "evaluation_metrics": "ROUGE (quality), PCC & MAD (length control), Success Rate (keywords), FKGL delta (readability), F1 (focus)",
            "performance_results": "PEFT (LoRA) results reported in paper: e.g., Flan-T5-XL (B1) ROUGE-L 22.22, PCC 0.49, MAD 0.55, SR 0.78, FKGL normal 13.11 / high 9.37 (delta 3.59), Focus F1 0.70; Flan-T5-XXL (B2) ROUGE-L 23.43, PCC 0.78, MAD 0.27, SR 0.85, FKGL normal 14.40 / high 10.78 (delta 3.62), Focus F1 0.75; decoder-only LoRA variants (LLaMA2-7B B3, Mistral-7B B4) showed smaller gains.",
            "comparison_baseline": "Zero-shot/few-shot closed-source LLMs (GPT-3.5, GPT-4, GPT-4o) and other baselines (hard prompt, soft prefix)",
            "performance_vs_baseline": "Encoder-decoder LoRA models (Flan-T5-XXL/B2) achieved results comparable to or surpassing GPT-4 on many metrics; decoder-only LoRA models showed only minor gains and underperformed relative to encoder-decoder LoRA models.",
            "key_findings": "LoRA PEFT improves controllable summarization performance; encoder-decoder architectures benefit more from PEFT than decoder-only models for compositional control, particularly on implicit attributes (readability, focus).",
            "limitations_challenges": "Even with LoRA, models struggle with implicit attributes and balancing attribute trade-offs; decoder-only architectures still suffer from degraded long-range attention and weak compositional control.",
            "scaling_behavior": "Larger encoder-decoder model (Flan-T5-XXL) with LoRA shows better compositional control than smaller variants; architecture and size interact with PEFT effectiveness.",
            "uuid": "e4394.1",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LoRAHub",
            "name_full": "LoRAHub (dynamic LoRA composition)",
            "brief_description": "A modular two-stage approach that assigns fixed weights to LoRA modules corresponding to different attribute controls and composes them to attempt compositional controllability; implemented and evaluated in this paper.",
            "citation_title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition",
            "mention_or_use": "use",
            "system_name": "LoRAHub (two-stage modular LoRA composition)",
            "system_description": "CCSBench implements LoRAHub as a two-stage method: stage 1 trains LoRA adapters on single-attribute tasks; stage 2 composes those adapters with fixed weights (LoRAHub) to produce summaries satisfying multiple attributes. The paper reports empirical evaluation of this composition approach.",
            "llm_model_used": "Flan-T5-XL (configured with LoRA modules and LoRAHub composition in experiments)",
            "extraction_technique": "Adapter-based extraction where each LoRA module is trained to extract/rewrite for a single attribute; input is I+C sections and gold summaries during training",
            "synthesis_technique": "Fixed-weight composition of attribute-specific LoRA modules to generate compositional outputs",
            "number_of_papers": "per-instance: single source paper summarization (no multi-paper aggregation described)",
            "domain_or_topic": "computer science / AI scientific summarization",
            "output_type": "compositional attribute-controlled summaries",
            "evaluation_metrics": "ROUGE, PCC, MAD, Success Rate (SR), FKGL delta, Focus F1 (as in CCSBench)",
            "performance_results": "Reported poor results: Flan-T5-XL-LoraHub (D1) ROUGE-L 14.31; PCC -0.05; MAD 2.59; SR 0.55; FKGL normal 13.69 / high 13.92 (delta -0.23); Focus F1 0.33 â€” indicative of low-quality, incoherent outputs and weak controllability.",
            "comparison_baseline": "Single-attribute LoRA models and AdapterFusion two-stage method",
            "performance_vs_baseline": "LoRAHub produced substantially worse ROUGE and controllability than single-attribute fine-tuned models and compared poorly to other composition attempts (e.g., AdapterFusion).",
            "key_findings": "Fixed-weight composition of LoRA modules is overly simplistic for compositional controllability; produces incoherent summaries and poor attribute balancing.",
            "limitations_challenges": "Overly simple integration mechanism; cannot learn complex interactions between attributes; yields incoherent outputs and low ROUGE.",
            "scaling_behavior": "Not discussed beyond observed poor behavior on evaluated model (Flan-T5-XL); no evidence of improvement with scale in CCSBench experiments.",
            "uuid": "e4394.2",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AdapterFusion",
            "name_full": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
            "brief_description": "An attention-based method that learns to fuse task-specific adapter modules; implemented in CCSBench as a two-stage approach to compose attribute-specific adapters for compositional controllable summarization.",
            "citation_title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
            "mention_or_use": "use",
            "system_name": "AdapterFusion (attention-based adapter fusion)",
            "system_description": "CCSBench implements AdapterFusion by training adapters for single-attribute tasks and then learning an attention fusion layer to weight and combine them for compositional control. The paper evaluates AdapterFusion's ability to balance multiple summary attributes.",
            "llm_model_used": "Flan-T5-XL with adapters and fusion layer (experimental configuration in CCSBench)",
            "extraction_technique": "Adapter-based extraction where each adapter is specialized to control one attribute; extraction is via the adapter representations conditioned on the input I+C text",
            "synthesis_technique": "Attention-based fusion of multiple attribute adapters to synthesize a single summary output",
            "number_of_papers": "per-instance: single scientific paper input (I+C), not multi-paper aggregation",
            "domain_or_topic": "computer science / AI scientific summarization",
            "output_type": "compositional controlled summaries",
            "evaluation_metrics": "ROUGE, PCC, MAD, SR, FKGL delta, Focus F1 (as in CCSBench)",
            "performance_results": "AdapterFusion (D2) produced low controllability: reported metrics include ROUGE-L 17.65, PCC 0.04, MAD 1.40, SR 0.65, FKGL normal 14.31 / high 14.49 (delta 0.18), Focus F1 0.41; overall poor summarization quality relative to single-attribute models.",
            "comparison_baseline": "Single-attribute LoRA models, LoRAHub, fully fine-tuned models",
            "performance_vs_baseline": "AdapterFusion failed to achieve meaningful compositional controllability and had lower ROUGE and attribute performance than single-attribute fine-tuned models; fusion exhibited bias toward the focus adapter per attention analysis.",
            "key_findings": "Attention-based adapter fusion displayed an empirical bias toward the focus adapter and could not flexibly adjust emphasis across attributes, leading to interference and degraded output quality.",
            "limitations_challenges": "Adapter fusion's aggregation is insufficiently expressive for complex attribute interactions; attention bias leads to over-prioritization of some attributes and poor compositional behavior.",
            "scaling_behavior": "Paper reports observed adapter attention bias across test set; does not report positive scaling trends for AdapterFusion in this task.",
            "uuid": "e4394.3",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Hard Prompt (HP)",
            "name_full": "Hard Prompting (structured attribute prompts)",
            "brief_description": "A baseline controllable generation approach that uses structured, explicit natural-language instructions (e.g., 'Attribute: Value') to steer LLM outputs; implemented and evaluated in CCSBench.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Hard Prompt (HP)",
            "system_description": "CCSBench implements hard prompts by formatting control attributes explicitly in the input prompt (e.g., 'Length: Bin 0; Focus: high; Readability: normal; Keywords: ...') so that the LLM generates a summary conditioned on those written constraints.",
            "llm_model_used": "Flan-T5-Large (evaluated as Flan-T5-Large-HP in CCSBench experiments)",
            "extraction_technique": "Prompt-conditioned summarization of I+C sections using explicit attribute fields",
            "synthesis_technique": "Direct single-step generation from prompt; no adapter/module composition",
            "number_of_papers": "per-instance: single paper",
            "domain_or_topic": "scientific summarization (CS/AI arXiv papers)",
            "output_type": "attribute-controlled summaries",
            "evaluation_metrics": "ROUGE, PCC, MAD, SR, FKGL delta, Focus F1 (as used in CCSBench)",
            "performance_results": "Reported (C1) Flan-T5-Large-HP: ROUGE-L 21.36, PCC 0.56, MAD 0.53, SR 0.86, FKGL normal 13.71 / high 10.40 (delta 3.31), Focus F1 0.63.",
            "comparison_baseline": "Soft prefix tuning, PEFT LoRA, zero-shot/few-shot closed-source LLMs",
            "performance_vs_baseline": "Hard prompt achieved reasonable explicit-attribute control but did not outperform the best PEFT encoder-decoder models on compositional tasks.",
            "key_findings": "Explicit structured prompts are a straightforward baseline for control, but struggle when multiple, interacting attributes must be balanced.",
            "limitations_challenges": "Performance degrades under compositional control compared to single-attribute settings; few-shot prompts limited by context window when inputs are long.",
            "scaling_behavior": "Not discussed beyond empirical results on evaluated model; few-shot demonstrations did not improve performance for long scientific inputs due to context window limits.",
            "uuid": "e4394.4",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Soft Prefix (SP)",
            "name_full": "Soft Prefix Tuning (prefix embeddings)",
            "brief_description": "A soft prompt baseline that prepends trainable continuous prefix embeddings to the model to exert fine-grained control over generation for each attribute; implemented and evaluated in CCSBench.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Soft Prefix (SP)",
            "system_description": "CCSBench implements soft prefix tuning by adding attribute-specific trainable prefix embeddings to each layer; during training prefixes are learned for each attribute value and then used to guide generation at inference.",
            "llm_model_used": "Flan-T5-Large (evaluated as Flan-T5-Large-SP in CCSBench experiments)",
            "extraction_technique": "Prefix-conditioned summarization where prefixes encode the desired attribute settings and influence generation; input is I+C sections",
            "synthesis_technique": "Single-step generation conditioned on learned soft prefixes",
            "number_of_papers": "per-instance: single paper",
            "domain_or_topic": "scientific summarization (CS/AI arXiv papers)",
            "output_type": "controlled summaries",
            "evaluation_metrics": "ROUGE, PCC, MAD, SR, FKGL delta, Focus F1",
            "performance_results": "Reported (C2) Flan-T5-Large-SP: ROUGE-L 20.58, PCC 0.63, MAD 0.52, SR 0.74, FKGL normal 13.47 / high 11.37 (delta 2.10), Focus F1 0.57.",
            "comparison_baseline": "Hard prompt, PEFT LoRA, zero-shot/few-shot closed-source LLMs",
            "performance_vs_baseline": "Soft prefix tuning performed comparably to hard prompt baselines but was outperformed by the strongest PEFT encoder-decoder models on compositional tasks.",
            "key_findings": "Soft prefixes can capture attribute signals but struggle with complex multi-attribute compositionality in scientific summarization contexts.",
            "limitations_challenges": "Limited when multiple conflicting control attributes must be balanced; effectiveness constrained by the complexity of attribute interactions.",
            "scaling_behavior": "Not specifically analyzed for scale in CCSBench beyond the reported Flan-T5-Large experiments.",
            "uuid": "e4394.5",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CocoSciSum",
            "name_full": "CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability",
            "brief_description": "Prior toolkit that extends controllable summarization to scientific texts with support for multiple explicit attributes (e.g., length, keywords); cited by CCSBench as related work but limited to explicit attributes.",
            "citation_title": "CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability",
            "mention_or_use": "mention",
            "system_name": "CocoSciSum",
            "system_description": "Referenced as prior work that extended control to length and keywords within scientific papers; CCSBench contrasts CocoSciSum's focus on explicit attributes with its own inclusion of implicit attributes (readability, empirical focus).",
            "llm_model_used": null,
            "extraction_technique": "Not specified in CCSBench (described in the cited CocoSciSum work)",
            "synthesis_technique": "Controlled summarization across scientific documents focusing on explicit attributes (details not provided here)",
            "number_of_papers": null,
            "domain_or_topic": "scientific paper summarization (CS/AI)",
            "output_type": "controlled summaries with explicit attribute control",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "CocoSciSum extended control to explicit attributes but did not handle subjective implicit attributes; CCSBench positions itself as advancing to implicit attributes and compositional settings.",
            "limitations_challenges": "Limited to explicit, objectively measurable attributes (length, keywords) per CCSBench discussion.",
            "scaling_behavior": null,
            "uuid": "e4394.6",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MACSum",
            "name_full": "MACSum: Controllable Summarization with Mixed Attributes",
            "brief_description": "A prior system for multi-attribute controllable summarization in the news and dialogue domains; cited as related work but not applied to scientific documents in CCSBench.",
            "citation_title": "MACSum: Controllable Summarization with Mixed Attributes",
            "mention_or_use": "mention",
            "system_name": "MACSum",
            "system_description": "Referenced as prior work that supports multi-attribute control in domains with less technical content (news, dialogue); CCSBench notes that scientific texts pose unique challenges beyond MACSum's scope.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "news and dialogue summarization (multi-attribute control)",
            "output_type": "multi-attribute controlled summaries",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "MACSum supports mixed-attribute control in less technical domains; CCSBench highlights the need to adapt multi-attribute methods to scientific texts.",
            "limitations_challenges": "Not designed for the domain-specific complexity of scientific papers according to CCSBench.",
            "scaling_behavior": null,
            "uuid": "e4394.7",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LimGen",
            "name_full": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
            "brief_description": "A cited work that probes LLMs' ability to generate suggested limitations for research papers; referenced by CCSBench in the related materials.",
            "citation_title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
            "mention_or_use": "mention",
            "system_name": "LimGen",
            "system_description": "Mentioned in CCSBench's references as a project exploring LLM generation of suggestive limitations for papers; CCSBench does not detail LimGen's methods.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "LLM generation of paper-specific limitations (details not provided in CCSBench)",
            "number_of_papers": null,
            "domain_or_topic": "scientific papers / meta-analysis of papers",
            "output_type": "generated limitations / critique of papers",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as related work illustrating LLMs applied to generating meta-level content about scientific papers.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4394.8",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Research-Idea-Generation Study",
            "name_full": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "brief_description": "A cited human-study investigating whether LLMs can generate novel research ideas; referenced by CCSBench as an example of LLM-driven idea generation in scientific contexts.",
            "citation_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "mention_or_use": "mention",
            "system_name": "LLM-based Research Idea Generation (study)",
            "system_description": "Mentioned as prior work demonstrating LLMs' use for idea generation in scientific research; CCSBench does not report technical details of the study's system.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "LLM-driven idea generation from scientific content (specific mechanism not described in CCSBench)",
            "number_of_papers": null,
            "domain_or_topic": "research idea generation for NLP / scientific research",
            "output_type": "proposed research ideas",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited to illustrate LLMs' demonstrated potential to generate research ideas and the broader range of scientific-document processing tasks LLMs have been applied to.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4394.9",
            "source_info": {
                "paper_title": "CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models",
            "rating": 2,
            "sanitized_title": "lora_lowrank_adaptation_of_large_language_models"
        },
        {
            "paper_title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition",
            "rating": 2,
            "sanitized_title": "lorahub_efficient_crosstask_generalization_via_dynamic_lora_composition"
        },
        {
            "paper_title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
            "rating": 2,
            "sanitized_title": "adapterfusion_nondestructive_task_composition_for_transfer_learning"
        },
        {
            "paper_title": "CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability",
            "rating": 2,
            "sanitized_title": "cocoscisum_a_scientific_summarization_toolkit_with_compositional_controllability"
        },
        {
            "paper_title": "MACSum: Controllable Summarization with Mixed Attributes",
            "rating": 1,
            "sanitized_title": "macsum_controllable_summarization_with_mixed_attributes"
        },
        {
            "paper_title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
            "rating": 1,
            "sanitized_title": "limgen_probing_the_llms_for_generating_suggestive_limitations_of_research_papers"
        },
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "rating": 1,
            "sanitized_title": "can_llms_generate_novel_research_ideas_a_largescale_human_study_with_100_nlp_researchers"
        }
    ],
    "cost": 0.023124,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization
4 Aug 2025</p>
<p>Yixi Ding yixi.d@comp.nus.edu.sg 
Jiaying Wu jiayingw@nus.edu.sg 
Tongyao Zhu tongyao.zhu@u.nus.edu 
Yanxia Qin yanxia_qin@sutd.edu.sg 
Qian Liu liuqian@sea.com 
Min-Yen Kan </p>
<p>National University of Singapore</p>
<p>National University of Singapore</p>
<p>National University of Singapore</p>
<p>Singapore University of Technology</p>
<p>Sea AI Lab</p>
<p>National University of Singapore</p>
<p>CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization
4 Aug 202502D4635F4832BA2DA2591F5E37DBAC27arXiv:2410.12601v3[cs.CL]Abstractive SummarizationScientific Document SummarizationLarge Language Models Quick Glance? Performance? Practical Implications? â€¦
To broaden the dissemination of scientific knowledge to diverse audiences, it is desirable for scientific document summarization systems to simultaneously control multiple attributes such as length and empirical focus.However, existing research typically focuses on controlling single attributes, leaving the compositional control of multiple attributes underexplored.To address this gap, we introduce CCSBench, the first evaluation benchmark for compositional controllable summarization in the scientific domain.Our benchmark enables fine-grained control over both explicit attributes (e.g., length), which are objective and straightforward, and implicit attributes (e.g., conceptual or empirical focus), which are more subjective and abstract.We conduct extensive experiments using various large language models (LLMs) under various settings, including in-context learning, parameter-efficient fine-tuning, and two-stage modular methods for balancing control over different attributes.Our findings reveal significant limitations in LLMs capabilities in balancing trade-offs between control attributes, especially implicit ones that require deeper understanding and abstract reasoning. 1CCS Conceptsâ€¢ Computing methodologies â†’ Natural language processing; â€¢ Information systems â†’ Summarization.</p>
<p>Introduction</p>
<p>Compositional controllability -the ability to generate tailored summaries based on multiple attributes -is essential for scientific document summarization systems to effectively communicate research to diverse audiences.Scientific papers contain complex information, but different readers have distinct needs and preferences when engaging with this content.For instance, as illustrated in Figure 1, an NLP beginner may seek a clear, concept-focused explanation, while a product manager may prioritize a concise summary highlighting the model's empirical performance.By dynamically adjusting summaries to align with varying expertise levels and interests, compositional control ensures that scientific knowledge is more accessible, relevant, and engaging for a broader audience.</p>
<p>Despite the broad demand for compositional controllability in scientific summarization, this capability remains largely underexplored in controllable text generation research.Most current efforts focus on using large language models (LLMs) to generate nonacademic texts such as movie reviews [23] and news articles [4], which are less complex than scientific documents adhering to strict academic conventions.Preliminary investigations into compositional scientific summarization [8] address only straightforward control requirements such as length and keyword inclusion, overlooking more subtle user requirements such as focus on empirical aspects.Developing methods for real-world deployment requires benchmarks that not only incorporate more sophisticated control attributes but also capture their interplay in a truly compositional manner, a challenge that remains unresolved.</p>
<p>In this paper, we introduce CCSBench,the first evaluation benchmark for compositional scientific summarization that enables finegrained control over multiple attributes.Drawing inspiration from Kahneman's cognitive theory [18], which distinguishes between the fast, intuitive "System 1" and the slow, deliberate "System 2" modes of thinking, CCSBench incorporates both explicit and implicit control attributes.Explicit attributes (e.g., length) are objective, easily quantifiable, and straightforward for both LLMs and humans to process.In contrast, implicit attributes (e.g., empirical focus) require deeper reasoning and human-like understanding, making them more challenging for LLMs to control effectively.</p>
<p>As illustrated in Figure 2, CCSBench is structured around two explicit attributes: (1) length and (2) keyword inclusion, and two implicit attributes: (3) readability and (4) focus.Length sets the target word count for the summary, while keyword inclusion ensures the presence of specific terms in the output.Readability controls language complexity and accessibility, tailoring the summary for technical or general audiences.Focus, short for "Empirical Focus Level", adjusts the emphasis on empirical evidence versus conceptual aspects, requiring deeper contextual understanding and abstract reasoning.By dynamically adjusting these attributes, we curate diverse scientific summaries -such as concise, highly readable conceptual explanations -that meet real-world needs and cater to diverse reader expectations.</p>
<p>We evaluate a diverse set of LLMs on CCSBench, including both proprietary LLMs and fine-tuned open-source variants.Results reveal significant limitations in managing trade-offs between control attributes, particularly due to inadequate abstract reasoning for implicit attributes.In parameter-efficient fine-tuning (PEFT), decoder-only models like LLaMA2 [32] and Mistral [17] struggle with long-term dependencies, while encoder-decoder models like Flan-T5 [5] adapt more effectively.Further analysis shows that standard task composition methods, such as AdapterFusion [28], are unsuitable due to oversimplified adapter aggregation.Our findings suggest the need for more targeted research on compositionality in scientific document summarization.</p>
<p>Related Work 2.1 Controllable Summarization</p>
<p>Controllable summarization has attracted significant research interest, with efforts focused on guiding summaries using attributes such as length, topic, keywords, and sentiment.Early work explored the impact of oracle-provided attributes on summary quality [10,29].More recent studies have aimed at controlling individual attributes like length or keywords [3,13], but these methods often treat attributes independently, limiting their scalability and effectiveness in multi-attribute scenarios.MacSum [36] represents a step forward by supporting multi-attribute control in the news and dialogue domains, where the content is relatively less technical.In contrast, scientific texts pose unique challenges for controllable summarization due to their high degree of linguistic precision and domainspecific complexity.CocoScisum [8] extends control to both length and keywords within scientific papers but remains constrained to explicitly defined attributes.Our work, CCSBench, advances the field by enabling fine-grained control over both explicit and implicit attributes, offering a broader and more nuanced evaluation framework tailored to scientific summarization.</p>
<p>LLMs for Scientific Document Processing</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of scientific document processing tasks, including idea generation [30], literature review synthesis [34], research critique [9], and the production of scientific news articles [22].These models also show promise in controllable text generation [33,35,37], though existing work primarily evaluates them under single-constraint settings (e.g., adjusting formality).Despite the growing need to tailor scientific content for diverse audiences, the ability of LLMs to perform compositional controllable generation -handling multiple constraints simultaneously -remains underexplored.To address this gap, we introduce the CCSBench benchmark, designed to systematically evaluate LLM performance under multi-attribute control.Our findings highlight the limitations of current models, particularly in handling implicit, conceptual attributes such as readability and aspect focus.</p>
<p>CCSBench Data Curation 3.1 Task Formulation</p>
<p>We formally introduce the task and notation used throughout the paper.Each example consists of a scientific document  accompanied by a set of control signals C = {, ,  ,  }, where  refers to length,  to keywords,   to empirical focus level, and  to readability.The objective is to generate a summary  that adheres to the constraints imposed by C while maintaining narrative coherence and preserving key information from the original document.</p>
<p>We define the four control attributes for CCSBench as follows:</p>
<p>Length specifies the desired number of words in the summary, ranging from concise to detailed, and catering to different audience needs based on their familiarity with the topic or time constraints.Length is divided into five bins, each representing a 50-word range (e.g., Bin 0: 0-50 words; Bin 1: 51-100 words).</p>
<p>Keywords ensures the inclusion of key terms from the source document, enabling readers to quickly grasp the core elements and assess the paper's relevance.Keywords can be left empty or set as pre-defined terms to be included in the summary.</p>
<p>Readability adjusts syntactic complexity and vocabulary to match audience proficiency, making summaries more accessible or technical.This enhances comprehension and engagement.It has two levels: [normal, high].</p>
<p>Empirical Focus Level (Focus) adjusts emphasis between empirical results and theoretical contributions, with two levels: [</p>
<p>Dataset</p>
<p>We construct CCSBench based on the arXiv dataset [6], a comprehensive collection of scientific documents from the widely used online preprint repository, arXiv.CCSBench focuses on papers in Computer Science and Artificial Intelligence.To ensure highquality input with minimal noise, we use the introduction and conclusion sections (I+C) as the input text, a practice validated by prior research [2,25] and our empirical analysis in Section 7. Following established practices in scientific summarization research [7], we use abstracts as reference summaries, leveraging their widely recognized factuality and comprehensiveness [31].</p>
<p>The benchmark comprises four single-attribute control datasets and one compositional control dataset, each split into training, validation, and test sets using a 60/20/20 ratio.Dataset statistics are summarized in Table 1.</p>
<p>Dataset Construction</p>
<p>The construction of CCSBench involves two main steps: (1) construction of single-attribute control datasets, followed by (2) the creation of a compositional control dataset.</p>
<p>Single Control Dataset.We begin with KWX [11], a dataset that provides keywords for each scientific document sourced from arXiv.</p>
<p>From this, we select 3,700 papers in the subjects of Computer Science and Artificial Intelligence to form the foundation of our controllable summarization dataset, using their abstracts as natural reference summaries.</p>
<p>The Keywords dataset, D  , is created by filtering out any keywords from the KWX dataset that do not appear in the abstracts, ensuring high-quality keywords.</p>
<p>To address the challenge of automatically assessing readability and focus, we use GPT-4 [27] to assist in generating summaries related to these two control attributes.We sample 2,000 papers from our data source, and assign the original abstracts with "normal readability".GPT-4 rewrites each abstract into a more layman-friendly version while maintaining the original meaning.These rewritten abstracts are labeled as "high readability".Combining these two types of abstracts, we form the Readability dataset, D  .</p>
<p>For focus control, we task GPT-4 with identifying the conceptual and empirical components of each abstract, then rewriting them separately without altering their meaning.Summaries derived from conceptual components are labeled as having "low empirical focus, " while those from empirical components are labeled as having "high empirical focus".These summaries form the Focus dataset, D   .</p>
<p>To construct the Length dataset, D  , we sample 4,000 instances from D  and    .Word counts are rounded to the nearest 50-word bin to create a length-controllable dataset.</p>
<p>Compositional Control Dataset.To manage the complexity of controlling all four attributes simultaneously, we construct a compositional control dataset using D  and D   , denoted as D  .Since readability and empirical focus are already annotated, and keywords and length can be easily derived as described earlier, each instance in this dataset is tagged with 2-3 attributes.This allows us to create both training and validation sets with compositional control over multiple attributes.</p>
<p>While it is impractical to collect instances covering every attribute combination in the training set, we ensure comprehensive evaluation by constructing a test set that includes all four attributes.We sample 400 instances from    , where the length, keywords, and empirical focus levels are pre-determined.Following the method used to generate   , we use GPT-4 to rewrite each summary for high readability, ensuring accessibility for a layman audience.</p>
<p>Data Quality Validation</p>
<p>We assess the quality of GPT-4-generated summaries in CCSBench through automated and human evaluations.First, we compute ROUGE scores to assess fluency and content accuracy by comparing generated summaries with their original counterparts.Next, we conduct a human evaluation to further verify the summaries' controllability and overall quality.We sample 120 documents each from   and    and recruit Amazon Mechanical Turk 2 annotators for binary assessments on three aspects: (1) Factual Consistency (FAC): whether the summary accurately reflects key points, (2) Fluency (FLU): whether it is natural and well-written, and (3) Controllability (CTRL): for Readability, whether the summary is easier to read than the reference; for Focus, whether empirically-focused summaries emphasize data collection, experimental setup, and results over conceptual aspects (details in Appendix B).</p>
<p>Table 2 shows high accuracy scores (&gt;0.9) across all metrics, confirming the quality of CCSBench 's summaries.Rigorous annotator selection and strong inter-annotator agreement (&gt;0.85,Section 3.5) further validate these results.</p>
<p>Reliability of Human Evaluation</p>
<p>We implement rigorous measures to ensure the reliability of human evaluations.Annotators on Amazon Mechanical Turk are selected based on strict quality criteria, including a â‰¥ 98% HIT approval rate and a minimum of 500 completed HITs, ensuring experienced and reliable contributors.</p>
<p>To validate annotation reliability, we conduct a pilot study with 20 randomly sampled instances for readability-and focus-controlled summaries.Each summary is evaluated by two independent annotators, and inter-annotator agreement is calculated.As shown in Table 3, all agreement scores exceed 0.85, confirming evaluation consistency.Based on these reliable pilot results, we scale the evaluation to two larger sets of 120 randomly sampled instances, each assessed by a single annotator, as shown in Section 3.4.We evaluate a range of closed-source and open-source models for compositional controllable summarization, including GPT-3.5 [26], GPT-4 [27], GPT-4o [16], LLaMA2 [32], Mistral [17], and Flan-T5 [5].For closed-source models, we conduct few-shot experiments by randomly selecting three demonstrations per test sample.For open-source models, we explore parameter-efficient fine-tuning (PEFT) using Low-Rank Adaptation (LoRA) [14] to adapt them to our dataset.Additionally, we revisit controllable summarization baselines from the news domain, including hard prompt (HP) [36] and soft prefix tuning (SP) [20].Further implementation details, including training setups, are provided in Appendix A.</p>
<p>Evaluation Metrics</p>
<p>We evaluate all models based on both summarization quality and attribute controllability.We employ ROUGE [21] to evaluate the overall quality.To assess attribute controllability, we employ the following metrics tailored to each attribute.</p>
<p>Length Control.We measure length controllability using the Mean Absolute Deviation (MAD) [24], and the Pearson Correlation Coefficient (PCC) [24], to evaluate the distance between the target and generated lengths.Keywords Control.Keyword control is evaluated using the Success Rate (SR) [13], which measures the fraction of specified keywords present in the generated summaries through exact matching after stemming.</p>
<p>Readability Control.For readability, we calculate the Flesch-Kincaid Grade Level (FKGL) [19], where a lower score indicates higher readability.The difference in FKGL scores (   ) between the high readability setting ( â„Žâ„Ž ) and the normal readability setting (  ) reflects the model's ability to control this attribute.A higher    is desirable.</p>
<p>Empirical Focus Control.For empirical focus, we use GPT-4 to classify summaries into high or low empirical focus levels.Validated with 93% accuracy on a manually labeled dataset, GPT-4 predictions are used to compute the F1 score for each category, assessing the model's focus control.Prompt details are provided in Appendix C.</p>
<p>Experiments</p>
<p>In this section, we benchmark and compare the compositional controllability of LLMs on CCSBench, focusing on four control attributes, which are categorized into explicit control (length, keywords) and implicit control (focus, readability).As we focus on evaluating LLM's capabilities for compositional control, all experiments are conducted on the Compositional dataset (D  ) of CCSBench, unless otherwise specified.Most models struggle with implicit tasks.For readability, the    score remains below 1 across most models, indicating poor differentiation between readability levels.While GPT-4 achieves the highest   of 2.13, its high FKGL score (âˆ¼17) suggests it generates overly complex summaries, failing to improve readability as intended.Focus control is similarly weak, with most models scoring around 0.5 F1, and even GPT-4o, the best performer, reaching only 0.72 F1, highlighting limited adaptability.</p>
<p>In contrast, closed-source models excel at explicit control.GPT-3.5, GPT-4, and GPT-4o achieve strong length and keyword control, with PCC scores up to 0.94, MAD scores as low as 0.09, and success rates consistently exceeding 95%.While open-source models underperform in length control, LLaMA2-7B and Mistral-7B still demonstrate strong keyword control.</p>
<p>Effect of Few-Shot Demonstrations.Comparing zero-shot and fewshot performance of GPT-3.5 and GPT-4, we observe that few-shot demonstrations do not improve LLM capabilities in this task.This is likely due to the challenges of processing long sequences, where the length of scientific documents limits the number of effective in-context examples.As a result, the context window becomes constrained, and overly lengthy prompts reduce model effectiveness.</p>
<p>Effect of PEFT.</p>
<p>LoRA fine-tuning improves performance across all models, though decoder-only (DO) models show only minor gains in controllability.For example,    values for settings B3 and B4 remain below 1, indicating minimal differentiation in readability.Similarly, their length control remains weak, with PCC values below 0.3, reflecting a low correlation between generated and target lengths.</p>
<p>In contrast, encoder-decoder (ED) models B1 and B2 achieve results comparable to or surpassing GPT-4, despite weaker zeroshot performance.B2, in particular, exhibits strong control over length and keywords while significantly improving readability and focus-both implicit attributes.Under high readability constraints, its FKGL score drops below 11, aligning with a high school reading level.For focus control, while its F1 score of 0.75 leaves room for improvement, it still outperforms GPT-4.</p>
<p>Limitations of Decoder-Only Models in Compositional Controllable</p>
<p>Summarization.Our comparison of encoder-decoder models (B1, B2) and decoder-only models (B3, B4) shows that LLaMA2 and Mistral underperform compared to Flan-T5 under fine-tuning.This disparity likely arises from architectural differences.Encoder-decoder models like Flan-T5 are optimized for sequence-to-sequence tasks, where the encoder processes input while the decoder generates output by attending to the encoded representations.In contrast, decoder-only models such as LLaMA2 and Mistral handle both   source and target sequences within the same unidirectional crossattention mechanism [12].As sequence length increases, attention becomes more diffused, weakening the model's ability to capture long-range dependencies-critical for controllable summarization.</p>
<p>To validate this, we analyze LLaMA2's attention patterns.Figure 3 shows a sharp decline in attention as token distance increases, indicating that the model prioritizes recent context while progressively disregarding earlier tokens.This degradation in long-range attention reduces the model's ability to maintain focus on control signals and relevant input, ultimately impairing summarization quality.</p>
<p>Discussion</p>
<p>Comparison of Single-Attribute vs. Compositional Control.The results in Section 5 reveal the limitations of LLMs in compositional control.To explore these limitations further, we compare the performance of Flan-T5-XXL fine-tuned on single-attribute tasks (i.e., fine-tuned separately on each of the four single-attribute datasets in CCSBench) with its performance on compositional control (i.e., fine-tuned on D  as outlined in Section 5).</p>
<p>As shown in Table 5, performance drops significantly when multiple attributes are controlled simultaneously, suggesting a conflict between the attributes that limits the model's compositional controllability.</p>
<p>Bias in LLMs to Prioritize Explicit Attributes.To better understand inter-attribute conflicts, we randomly select 300 samples from the test set of D  for each attribute.Keeping the other attribute settings unchanged, we modified the specified attribute (e.g., changing Readability from "normal" to "high") to create 300 new samples for each attribute accordingly.Then, we use the same model to generate the summary.For a given controllability metric, we calculate the change amplitude (CA) between the control ability before the attribute value change  and after the change   , defined as:
ð¶ð´ = | ð‘š ð‘›ð‘’ð‘¤ âˆ’ ð‘š ð‘š |.
We illustrate the dependencies between attributes in Figure 4, where each row represents the attribute being changed and each column represents the attribute being affected.We observe that implicit attributes are more susceptible to the influence of other attributes compared to explicit attributes.Specifically, in the case of Flan-T5, readability shows a stronger dependency on length than vice versa.This indicates that when controlling both attributes simultaneously, the model tends to prioritize the length constraint.For example, in Case 1 of Table 6, to shorten the length of the summary, the model omits the original simple explanation of the term "feature" and replaces it with the more abstract concept of "descriptor", making the summary more concise but harder to understand.</p>
<p>While implicit control requires more abstract reasoning, explicit control involves clearer and more concrete constraints.This bias suggests a limitation in the model's ability to deeply understand and reason across multiple attributes.</p>
<p>Conflicts Between Readability and Focus.We observe a strong interaction between focus and readability, indicating that the model struggles to balance these two attributes.For example, in Case 2 of Table 6, when the summary emphasizes empirical content, it tends to use simpler sentence structures and phrases such as "The results show... " or "something was tested.. " along with terms like "effective".However, when readability requirements are reduced, the model Table 6: Case studies on conflicts between readability and focus controllability.The left column presents unchanged attribute values, while the right column shows the impact of modifying a specific attribute.Blue text highlights content that satisfies the control conditions, while red text indicates control failures.</p>
<p>Case 1: Length Against Readability -Shorter Length Results in Unsatisfying Readability</p>
<p>Readability: high Empirical Focus Level: low Length Bin: 1 -We are interested in how the human visual system recognizes objects.Inspired by this, we created a new set of features that include information about gradients in both horizontal and vertical directions, as well as multi-scale contrast maps.These features are designed to help identify pedestrians in complex scenes.Length Bin: 0 -We introduce a new set of features inspired by the human visual system, including multidirection, multi-scale contrast and statistical multi-channel descriptors.These features are designed to enhance pedestrian detection performance in various conditions.Case 2: Readability Affects Focus -Lower Readability Results in Less Empirical Focus Length Bin: 0 Empirical Focus Level: high Readability: high -The effectiveness of the proposed human detection method was tested on several images.The results showed that the method was effective even when the human's face was not visible, when the head was partially obscured, or when the subject was far from the camera.Readability: normal -This paper presents a novel method for detecting human presence in surveillance videos by analyzing the OMEGA symbol.The proposed method involves identifying the unique signature of the human head-neck-shoulder region and classifying images as human or non-human based on this signature.</p>
<p>Case 3: Focus Affects Readability -Lower Empirical Focus Results in Lower Readability</p>
<p>Length Bin: 0 Readability: high Empirical Focus Level: high -We applied our model to a large set of product reviews and achieved an accuracy of 98.28%, which is comparable to the current state of the art in sentiment classification.Empirical Focus Level: low -This paper proposes a supervised sentiment classification model based on the Naive Bayes algorithm.It argues that a simple model can achieve comparable accuracy to state-of-the-art methods, highlighting the importance of selecting appropriate features and eliminating noise.</p>
<p>Length</p>
<p>Keywords Readability Focus</p>
<p>Figure 5: Average attention scores of the fusion layer for each adapter show a bias toward the focus adapter.</p>
<p>introduces more complex, technical vocabulary, shifting the focus toward conceptual content.In practice, when controlling both readability and focus, we expect the model to prioritize identifying content that aligns with the specified focus, rather than introducing incorrect content just to incorporate more technical language.Similarly, in Case 3, when a lower empirical focus level is set, the model uses more conceptual terminology, making the summary harder for a lay audience to understand.This suggests that the model has difficulty maintaining simple language while describing content with a low empirical focus.</p>
<p>Can Two-Stage Modular Methods Address Inter-Attribute Conflicts?</p>
<p>To address these trade-offs, we explore two-stage approaches where single-attribute control is learned in the first stage, and balancing multiple attributes is fine-tuned in the second stage.Specifically, we implement LoRAHub [15], which assigns a fixed weight to each LoRA module; and AdapterFusion [28], which utilizes an attention layer to learn how to focus on different attribute modules.However, results in Table 4 show that neither AdapterFusion nor LoRAHub achieves meaningful controllability, with both producing poor summaries, as reflected in their low ROUGE scores.We believe this failure stems from the overly simplistic integration mechanisms, which are insufficient for learning the complexities required for compositional control, resulting in incoherent outputs.</p>
<p>We further probe the limitations of AdapterFusion by analyzing the model's attention differences across various attributes in over the entire test set, as shown in Figure 5.The model exhibits a bias toward the focus module, suggesting that the attention mechanism struggles to adjust its emphasis on different attributes based on varying control requirements.This interference between modules ultimately degrades the model's overall performance.</p>
<p>Discussion on Input Choice for Compositional Controllable Scientific Document Summarization</p>
<p>Obtaining high-quality summaries is key to effective scientific document summarization.To this end, we investigate the effects of using only the introduction and conclusion sections (I+C) versus the full scientific document as input for compositional controllable scientific document summarization.CCSBench adopts the I+C sections as input, a choice supported by prior research demonstrating its effectiveness over full-document summarization [2,25].This effectiveness likely stems from the challenges language models face in maintaining coherence and With recent LLMs such as GPT-4o [16] supporting extended context windows of up to 128k tokens, an open question arises: Can these models overcome long-context challenges in processing full scientific documents for compositional controllable summarization?To explore this, we conduct experiments on the first 200 samples of CCSBench, retrieving their full documents and comparing GPT-4o's summarization performance using introduction and conclusion sections (I+C) versus full-text input (Full).</p>
<p>The results, presented in Table 7, indicate that I+C outperforms Full in keyword preservation, readability, and focus while maintaining comparable performance on other metrics.Notably, I+C attains comparable performance while consuming only 23.7% of the tokens required for Full, validating its effectiveness and practicality for compositional controllable scientific document summarization.</p>
<p>Conclusion</p>
<p>We introduce CCSBench, the first benchmark for evaluating compositional controllable scientific summarization.Integrating both explicit (length, keyword inclusion) and implicit (readability, empirical focus) attributes, CCSBench provides a structured framework for assessing LLMs' ability to generate controlled, contextually appropriate scientific summaries.Our extensive experiments establish concrete LLM baselines and reveal significant limitations in compositional controllability, emphasizing the need for further research.We identify key challenges, including trade-offs between attributes and difficulties in handling implicit control, while also outlining potential directions for improvement.By providing actionable insights into LLMs' strengths and limitations in this challenging task, we offer a foundation for advancing the field.</p>
<p>Limitations and Future Work</p>
<p>Empirical results on CCSBench highlight key challenges in LLMs' ability to handle compositional controllable scientific summarization.While our findings show that fine-tuning smaller models (e.g., Flan-T5) can achieve performance comparable to GPT-4 across multiple metrics and provide deeper insights into failure cases, further research is needed to better understand and resolve observed interactions between control attributes.</p>
<p>Currently, CCSBench focuses on scientific documents in English.Expanding this framework to support multilingual scientific summarization is a promising direction for future work.Additionally, while CCSBench is designed for effective general communication, more granular summarization -addressing the need for in-depth analysis, such as detailed experimental setups and side observations -remains an important area for exploration.Specifically, extending our framework and methodology to full scientific documents using datasets like LimGen [9] could provide valuable insights into more comprehensive and nuanced summarization strategies.</p>
<p>"""</p>
<h1>System You are a NLP expert.Please help me paraphrase some scientific summaries according my requests.</h1>
<h1>Instruction</h1>
<p>Please paraphrase this abstract for middle school students without using metaphors and without including information that cannot be obtained directly from the original abstract: {abstract} """</p>
<p>C.2 Focus Data Construction</p>
<p>""" # System You are a NLP expert.Please help me analyze some scientific abstracts.</p>
<h1>Instruction for focus identification Can you identify the empirical content and conceptual content of the following abstract?If the abstract is obviously predominated by only one type of information, the other part can be 'None'.</h1>
<p>Definition:</p>
<p>The empirical content emphasizes the experimental, data-driven aspects of a study.It highlights data collection, experimental settings, performance metrics and concrete results obtained from the research, especially when it comes to statistics and data.</p>
<p>The conceptual content emphasizes the theoretical and innovative aspects of a study.It highlights the underlying theoretical framework, the description of the proposed method or algorithm, and broader implications of the research.If neither part is 'None', please paraphrase the empirical content and the conceptual content into a separated empirical-focused abstract and a separated conceptual-focused abstract respectively.Both abstracts should be coherent and fluent without including information that can't be inferred directly in the original abstract.If either part is 'None', take the input abstract and 'None' as the two output abstracts.Note that the difference between empirical focus and conceptual focus lies on the content rather than language expression.# Output (Extracted by functions) {"Empirical Summary": output3, "Conceptual Summary": out-put4} """</p>
<p>C.3 Focus Evaluation</p>
<p>""" # System You are a NLP expert.Please help me analyze some scientific abstracts.</p>
<h1>Instruction</h1>
<p>Help me decide whether this abstract is more empirically-focused or more conceptually-focused.The output should be 'empirical' or 'conceptual'.</p>
<p>Definition:</p>
<p>The empirically-focused abstract emphasizes the experimental, data-driven aspects of a study.It highlights data collection, experimental settings, performance metrics and concrete results obtained from the research, especially when it comes to statistics and data.</p>
<p>The conceptually-focused abstract emphasizes the theoretical and innovative aspects of a study.It highlights the motivation, the challenge, the underlying theoretical framework, the description of the proposed method or algorithm, and broader implications of the research.</p>
<p>Abstract: {abstract} """</p>
<p>C.4 Zero-Shot LLM Experiments</p>
<p>""" # System You are an expert summarizer who can generate summaries with specific controls.</p>
<h1>Instruction</h1>
<p>Your task is to create a summary of the given scientific document with the following controls: Length: The summary should fit in the specified word counts.Keywords: Include the following keywords in the summary: [list of keywords] Readability: Ensure the summary is either highly readable for laymen (high) or not specifically optimized for readability (normal).</p>
<p>Empirical Focus Level: Make the summary has high empirical focus level (high) by emphasizing the experimental, data-driven aspects of a study.It highlights data collection, experimental settings, performance metrics and concrete results obtained from the research, especially when it comes to statistics and data; or low empirical focus level (low) the theoretical and innovative aspects of a study.It highlights the motivation, the challenge, the underlying theoretical framework, the description of the proposed method or algorithm, and broader implications of the research.Readability: Ensure the summary is either highly readable for laymen (high) or not specifically optimized for readability (normal).</p>
<p>Empirical Focus Level: Make the summary has high empirical focus level (high) by emphasizing the experimental, data-driven aspects of a study.It highlights data collection, experimental settings, performance metrics and concrete results obtained from the research, especially when it comes to statistics and data; or low empirical focus level (low) the theoretical and innovative aspects of a study.It highlights the motivation, the challenge, the underlying theoretical framework, the description of the proposed method or algorithm, and broader implications of the research.</p>
<p>Figure 1 :
1
Figure 1: Compositional controllability allows scientific summaries to be tailored to diverse reader needs, addressing specific reader requirements.</p>
<p>Figure 2 :
2
Figure 2: Illustration of explicit and implicit control attributes in CCSBench.</p>
<p>Figure 3 :
3
Figure 3: Average attention across all layers and heads for the last 10 tokens generated by LLaMA2 over the preceding 1,024 tokens, displayed on a log-log scale.The sharp decline in attention scores as token distance increases highlights the model's difficulty in maintaining focus on distant tokens within the sequence.</p>
<p>Figure 4 :
4
Figure 4: Attribute dependencies reflected by change amplitude (CA) on PCC (length), SR (keywords),    (readability), and F1 (focus).Each row represents the attribute being modified, and each column represents the affected attribute.Numerical values and color intensity denote the strength of the dependency.</p>
<p>Abstract: {abstract} # Output (Extracted by functions) {"Empirical Content": output1, "Conceptual Content": output2} # Instruction for paraphrasing KDD'25 SciSoc LLM Workshop, August 4, 2025, Toronto, ON, Canada Ding et al.</p>
<p>are an expert summarizer who can generate summaries with specific controls.#InstructionYour task is to create a summary of the given scientific document with the following controls:Length: The summary should fit in the specified word counts.Keywords: Include the following keywords in the summary: [list of keywords]</p>
<p>Explicit Attributes Implicit Attributes Keywords-inclusion (open-ended generation):
...Overall, the study shows that instruction fine-tuning enhances performance across various models and tasks, particularlylow,high]. A high empirical focus highlights data-driven aspects likeexperiments and results, while a low empirical focus emphasizes</p>
<p>in open-ended generation scenariosâ€¦ Non-inclusion: â€¦The
results confirm that instruction finetuning can significantly improve performance across different models, tasks, and evaluations.Non-technical: This paper focuses on improving artificial intelligence (AI) models, specifically language models, to better understand and</p>
<p>complete tasks they haven't seen beforeâ€¦ Technical: â€¦Flan</p>
<p>-PaLM also has improved usabilityfor example, it can perform</p>
<p>zero-shot reasoning without prompt engineering or few-shot exemplarsâ€¦
Short: This paper advances instruction finetuning by scaling models and tasks, integrating chain-of-thought data, and improving reasoning and multilingual abilities... (47 words)Long: â€¦In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought dataâ€¦ (157 words)Empirical Focus: â€¦Flan-PaLM 540B instruction-finetuned on 1.8K tasks</p>
<p>outperforms PALM 540B by a large margin (+9.4% on average)â€¦ Conceptual Focus: This paper advances instruction finetuning by studying its scaling effects and incorporating chain-of-thought (CoT) dataâ€¦</p>
<p>Table 1 :
1
CCSBench statistics.</p>
<h1>Samples / #DocsDatasetTrainValTestLength2,400 / 1,561 800 / 705 800 / 698Keywords2,029 / 2,029 677 / 677 677 / 677Readability2,400 / 1,687 800 / 724 800 / 728Focus2,332 / 1,659 779 / 709 768 / 703Compositional 2,400 / 1,590 800 / 655 758 / 364theories, frameworks, and broader implications. This control tai-lors summaries to readers seeking practical insights or conceptualunderstanding.</h1>
<p>Table 2 :
2
Evaluation results validate the high quality of LLM-generated summaries in CCSBench.
DatasetFAC FLU CTRL ROUGE-1 ROUGE-2 ROUGE-LReadability 0.98 0.930.9253.8821.8141.31Focus0.91 0.930.9351.2534.2341.41</p>
<p>Table 3 :
3Inter-annotator agreement scores for assessmentquestions on readability (Appendix B.1) and empirical focus(Appendix B.2).Q1Q2Q3Q4Q5Readability 1.00 0.85 0.95 0.90-Focus0.95 1.00 0.85 0.90 0.952 https://www.mturk.com/</p>
<p>Table 4 :
4
Performance comparison of representative language models on CCSBench, evaluating overall quality and four control attributes.Arch refers to model architectures, with ED for encoder-decoder and DO for decoder-only.Bold numbers indicate the best performance, while underlined numbers represent the second-best.Implicit vs. Explicit Control.As shown in Table4, implicit control proves more challenging than explicit control.
Explicit AttributesImplicit AttributesMethodArchQualityLengthKeywordsReadabilityFocusROUGE-L â†‘ PCC â†‘ MAD â†“SR â†‘F ð‘›ð‘œð‘Ÿð‘šð‘Žð‘™ F â„Žð‘–ð‘”â„Ž â†“ ð›¿ ð¹ ð¾ðºð¿ â†‘F1 â†‘Zero-Shot LLMsA1 Flan-T5-XLED13.490.001.430.4413.9413.570.370.36A2 Flan-T5-XXLED14.250.001.390.4612.6612.68-0.020.39A3 LLaMA2-7BDO17.100.231.110.7514.4014.44-0.040.51A4 Mistral-7BDO17.670.131.160.7512.0812.44-0.360.48A5 GPT-3.5DO18.050.620.390.9515.1215.060.060.56A6 GPT-4DO19.260.840.660.9919.1216.992.130.71A7 GPT-4oDO19.410.940.090.9916.2615.380.880.72Few-Shot LLMs w/ 3 DemonstrationsA8 GPT-4DO19.220.770.690.9818.7717.701.070.68A9 GPT-3.5DO19.050.540.410.7716.0716.17-0.100.54LLMs w/ Parameter-Efficient Fine-TuningB1 Flan-T5-XLED22.220.490.550.7813.119.373.590.70B2 Flan-T5-XXLED23.430.780.270.8514.4010.783.620.75B3 LLaMA2-7BDO17.750.201.440.7714.1213.800.320.61B4 Mistral-7BDO18.210.291.760.8313.1012.420.680.55Fully Fine-Tuned LLMsC1 Flan-T5-Large-HPED21.360.560.530.8613.7110.403.310.63C2 Flan-T5-Large-SPED20.580.630.520.7413.4711.372.100.57Two-Stage Modular MethodsD1 Flan-T5-XL-LoraHubED14.31-0.052.590.5513.6913.92-0.230.33D2 Flan-T5-XL-AdapterFusionED17.650.041.400.6514.3114.490.180.41</p>
<p>Table 5 :
5
Performance comparison of Flan-T5-XXL with LoRA on single and compositional control tasks.
ReadabilityLengthKeywords FocusModelð›¿ ð¹ â†‘PCC â†‘ MAD â†“SR â†‘F1 â†‘Single4.660.890.230.880.92Compositional3.620.780.270.850.75</p>
<p>Table 7 :
7
Comparison of GPT-4o's performance on the first 200 samples of CCSBench.I+C: using introduction and conclusion sections as input; Full: using the retrieved full texts as input.Detailed descriptions of metrics are provided in Section 4.2.
Explicit AttributesImplicit AttributesMethodQuality#TokensLengthKeywordsReadabilityFocusROUGE-L â†‘ PCC â†‘ MAD â†“SR â†‘F ð‘›ð‘œð‘Ÿð‘šð‘Žð‘™ F â„Žð‘–ð‘”â„Ž â†“ ð›¿ ð¹ ð¾ðºð¿ â†‘F1 â†‘Full21.150.980.060.9816.5516.100.450.676.78kI+C (ours)20.840.960.060.9915.8715.370.500.721.61krelevance over long contexts. As CCSBench aims to foster effec-tive general scientific summarization rather than produce in-depthanalyses, the I+C sections provide a concise yet comprehensiverepresentation of a paper's key elements, including task definitions,prior limitations, methodologies, and main findings [31]. Thus, ourCCSBench aligns with the established I+C practice in the field.
AcknowledgmentsWe are grateful to the reviewers for their constructive feedback.This work was supported by the Singapore Ministry of Education Academic Research Fund Tier 1 (251RES2216).Ding et al.A Experimental Setup A.1 ModelsRecent advancements in large-scale generative language models based on transformers, such as GPT[1], have significantly improved performance on various natural language processing tasks.Given their widespread use and strong baseline performance, we select a range of large language models to evaluate their ability to perform compositional controlled summarization.In our experiments, we focus on a variety of model architectures, including encoder-decoder models like the Flan-T5 series, decoder-only models such as LLaMA[32]and Mistral[17], and open-source large models like GPT-4[27].Additionally, we evaluate their parameter-efficient fine-tuning (PEFT) versions to assess whether parameter-efficient fine-tuning enhances their ability to handle controlled summarization tasks.Moreover, we also revisit previously established baselines in the news domain for controlled summarization, namely hard prompt (HP, 36) and soft prefix tuning (SP, 20) approaches on smaller models.We fine-tune each of these models using our compositional control dataset, specifically designed to test their ability to handle multiple attributes simultaneously.Across these models, we explore various objectives, yielding multiple model variants tailored for controlled summarization.These configurations allow us to comprehensively study the strengths and limitations of different architectures and tuning strategies when applied to compositional summarization tasks, as described in further detail below.Zero-shot (ZS).In our simplest setting, we evaluate the compositional controllability already learned by large language models due to pretraining on large-scale corpora.In this setting, the models are not fine-tuned on any portion of our compositional control dataset.Instead, they must generate summaries for the test set based solely on the representations learned during pretraining.At test time, the model receives the input document along with a set of control signals (, ,  , , ) and generates the summary accordingly.The prompt template we used can be found in the Appendix C.Parameter-efficient Fine-tuning (PEFT).Because the pretraining domains of large language models are broad and cover diverse textual sources, we investigate whether adapting these models to the specific data distribution of our compositional control dataset can enhance their ability to handle multi-attribute summarization for scientific documents.In this setting, we apply parameter-efficient fine-tuning (PEFT) to these models, which allows us to adapt these large models to our task without updating all of their parameters.In this setting, we leverage the widely-used PEFT technique LoRA (Low-Rank Adaptation, Hu et al.[14]), which inserts lowrank trainable matrices into the model's architecture.The document with the golden summary and four controlled attributes (, , ,  , , ) is the input to the model.Hard Prompt (HP).Following Zhang et al.[36], in the hard prompt setting, we provide the model with explicit instructions regarding the desired control attributes by using a structured prompt format.Each control attribute is formatted as "Attribute: Value", where "Attribute" refers to one of the specified control dimensions, such as "Length", "Focus", "Keywords", or "Readability", and "Value" represents the target value for that attribute (e.g., "Noramal", "High", etc.).Soft Prefix (SP).In this setting, we prepend external trainable parameters-referred to as "prefixes"-to each layer of the summarization model to exert fine-grained control over the generation process.For controlling each attribute value, we assign  prefix embeddings for the respective attribute, where  is a hyperparameter representing the length of the prefix.For instance, for controlling summary readability with a "High" specification, we assign a series of embeddings  :â„Žâ„Ž = [ 1 :â„Žâ„Ž , ...,   :â„Žâ„Ž ], where each    is a vector of the same dimensionality as the model's word embeddings.For implementation details, readers may refer to Li and Liang[20].A.2 Implementation DetailsWe use PyTorch and the Huggingface library to implement all the models.The experiments are conducted on 2 A40 GPUs.We choose 1e-4 as the learning rate for all PEFT models and the max epoch is set to 20.For hard prompt and soft prefix tuning, we largely follow the same training and inference setups as in Zhang et al.[36].In all of our experiments, we run each model once.A.3 Evaluation MetricsHere we introduce different metrics in detail.Length Control.For length controllability evaluation, we adopt (1) the Mean of Absolute Deviation (MAD, 24) of length codes of system-generated summaries and the references, measuring their length distance; and (2) the Pearson Correlation Coefficient (PCC, 24) between the generated length and the input length bin.Readability Control.For evaluating readability controllabiltiy, we calculate the Flesch-Kincaid Grade Level (FKGL,19)for each document under both high and normal readability settings.A lower FKGL score indicate higher readability.We use   to represent the difference in FKGL scores between the two categories.A larger    indicates a greater distinction in readability, which in turn reflects stronger control over readability by the model.Keywords Control.For evaluating keyword controllability, we use the Success Rate (SR, 13), namely the fraction of keywords actually occurring in the output summaries.We calculate SR employing exact matching after stemming.Empirical Focus Level Control.Since focus is difficult to distinguish using simple rules, we opt to use an LLM evaluator for identification.For the generated summaries, we instruct GPT-4o to determine whether they are more conceptual-focused or empiricalfocused.We experiment with various instruction patterns and validate the approach on the manually annotated dataset, achieving a best accuracy of 93%.Next, we use the categories predicted by GPT-4o as the labels for the generated summaries and calculate the F1 score for each category.A higher F1 score indicates stronger control over the focus attribute.Prompt details can be found in Appendix C.B Human Evaluation DetailsAs described in Section 3.3, we employed human evaluators to assess the quality of GPT-generated summaries (regarding Readability and Focus) in CCSBench.The evaluations were conducted via Amazon Mechanical Turk (MTurk) 3 , with participants compensated at a rate of USD $0.60 per Readability case and USD $0.75 per Focus case.We clearly stated during recruitment that the collected data would be used solely for research purposes.The human evaluation process has received IRB approval from the authors' institution.B.1 Instructions for Readability EvaluationInstructions to MTurk Annotators.Read the two scientific summaries and evaluate their facuality, readability and fluency.[Summary 1] [Summary 2]Questions for MTurk Annotators.Q1: Do the two summaries agree on the main points?(Yes / No) Q2: Which summary is more readable?(Summary 1 / Summary 2)Q3: Is Summary 1 written in natural and fluent language?(Yes / No) Q4: Is Summary 2 written in natural and fluent language?(Yes / No)B.2 Instructions for Focus EvaluationInstructions to MTurk Annotators.Read the two scientific summaries and the reference abstract, evaluate their factuality, focus and fluency.Definition:The empirical-focused summary emphasizes the experimental, data-driven aspects of a study.It highlights data collection, experimental settings, performance metrics and concrete results obtained from the research, especially when it comes to statistics and data.The conceptual-focused summary emphasizes the theoretical and innovative aspects of a study.It highlights the motivation, the challenge, the underlying theoretical framework, the description of the proposed method or algorithm, and broader implications of the research.Empirical-focused Example: This study evaluates the performance of a modified CRF-based POS tagging system for Manipuri, incorporating new features and the Reduplicated Multiword Expression (RMWE) feature.The experiment shows that the new CRF system achieves a Recall of 78.22%, Precision of 73.15%, and F-measure of 75.60%.With the inclusion of RMWE as a feature, the results improve to a Recall of 80.20%, Precision of 74.31%, and F-measure of 77.14%.Conceptual-focused Example: This paper provides an in-depth overview of the updated feature selection approach in CRF for Manipuri POS tagging.It highlights the significance of optimal feature selection in enhancing CRF performance and discusses the introduction of new features, including the Reduplicated Multiword Expression (RMWE), which is crucial for accurately tagging Manipuri language POS due to its rich occurrence of RMWE.
Tom B Brown, Benjamin Mann, Nick Ryder, arXiv:2005.14165[cs.CLLanguage Models are Few-Shot Learners. 2020</p>
<p>TLDR: Extreme Summarization of Scientific Documents. Isabel Cachola, Kyle Lo, Arman Cohan, Daniel Weld, 10.18653/v1/2020.findings-emnlp.428Findings of the Association for Computational Linguistics: EMNLP 2020. Trevor Cohn, Yulan He, Yang Liu, Online2020</p>
<p>Controllable Summarization with Constrained Markov Decision Process. Lu Hou Pong Chan, Irwin Wang, King, 10.1162/tacl_a_00423Transactions of the Association for Computational Linguistics. 92021. 2021</p>
<p>Can LLM-Generated Misinformation Be Detected. Canyu Chen, Kai Shu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Scaling Instruction-Finetuned Language Models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Yu, 10.48550/ARXIV.2210.11416Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Slav Petrov</p>
<p>Colin B Clement, Matthew Bierbaum, Kevin P O'keeffe, Alexander A Alemi, arXiv:1905.00075[cs.IROn the Use of ArXiv as a Dataset. 2019</p>
<p>A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents. Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang, Goharian, 10.18653/v1/N18-2097Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. Marilyn Walker, Ji Heng, Amanda Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20182</p>
<p>CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability. Yixi Ding, Yanxia Qin, Qian Liu, Min-Yen Kan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2023</p>
<p>Abdur Rahman Bin, Md Faizullah, arXiv:2403.15529[cs.CLAshok Urlana, and Rahul Mishra. 2024. LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers. </p>
<p>Controllable Abstractive Summarization. Angela Fan, David Grangier, Michael Auli, 10.18653/v1/W18-2706Proceedings of the 2nd Workshop on Neural Machine Translation and Generation. Thang Luong, Graham Neubig, Yusuke Oda, the 2nd Workshop on Neural Machine Translation and GenerationAlexandra Birch, Andrew Finch; Melbourne, AustraliaAssociation for Computational Linguistics2018</p>
<p>Xiao Fu, 10.5281/zenodo.7929439KWX: scholarly articles with keywords from arXiv. 2023</p>
<p>Zihao Fu, Wai Lam, Qian Yu, Anthony , Man-Cho So, Shengding Hu, Zhiyuan Liu, Nigel Collier, arXiv:2304.04052[cs.CLDecoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. 2023</p>
<p>Nazneen Rajani, and Caiming Xiong. Junxian He, Wojciech KryÅ›ciÅ„ski, Bryan Mccann, arXiv:2012.04281[cs.CLCTRLsum: Towards Generic Controllable Text Summarization. 2020</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, arXiv:2106.09685[cs.CL2021</p>
<p>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, Min Lin, arXiv:2307.13269[cs.CL2024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024. 2024arXiv preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, arXiv:2310.06825[cs.CLMistral 7B. 2023</p>
<p>. Daniel Kahneman, Thinking, Fast and Slow. Language. 21252013. 2013</p>
<p>Peter Kincaid, Robert P Fishburne, Richard L Rogers, Brad S Chissom, Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel. 197561131325</p>
<p>Prefix-Tuning: Optimizing Continuous Prompts for Generation. Lisa Xiang, Percy Li, Liang, 10.18653/v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>SciNews: From Scholarly Complexities to Public Narratives -a Dataset for Scientific News Report Generation. Dongqi Liu, Yifan Wang, Jia E Loy, Vera Demberg, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024Torino, Italy2024ELRA and ICCL</p>
<p>Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation. Yi Liu, Xiangyu Liu, Xiangrong Zhu, Wei Hu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Controlling Length in Abstractive Summarization Using a Convolutional Neural Network. Yizhu Liu, Zhiyi Luo, Kenny Zhu, 10.18653/v1/D18-1444Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He, 10.18653/v1/2021.acl-short.137Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Short Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212Association for Computational Linguistics</p>
<p>ChatGPT: Optimizing language models for dialogue. 2022OpenAI</p>
<p>. Josh Openai, Steven Achiam, Adler, arXiv:2303.08774[cs.CL2024GPT-4 Technical Report</p>
<p>AdapterFusion: Non-Destructive Task Composition for Transfer Learning. Jonas Pfeiffer, Aishwarya Kamath, Andreas RÃ¼cklÃ©, Kyunghyun Cho, Iryna Gurevych, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain Volume2021</p>
<p>Abigail See, Peter J Liu, Christopher D Manning, arXiv:1704.04368[cs.CLSummarization with Pointer-Generator Networks. 2017Get To The Point</p>
<p>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Tips for Writing Technical Papers. 2006Stanford University</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, arXiv:2307.09288[cs.CLLlama 2: Open Foundation and Fine-Tuned Chat Models. 2023</p>
<p>Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects -A Survey. Ashok Urlana, Pruthwik Mishra, Tathagato Roy, Rahul Mishra, 10.18653/v1/2024.findings-acl.93Findings of the Association for Computational Linguistics: ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>AutoSurvey: Large Language Models Can Automatically Write Surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>UltraGen: Extremely Finegrained Controllable Generation via Attribute Reconstruction and Global Preference Optimization. Longfei Yun, Letian Peng, Jingbo Shang, arXiv:2502.123752025. 2025arXiv preprint</p>
<p>MACSum: Controllable Summarization with Mixed Attributes. Yusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, Rui Zhang, 10.1162/tacl_a_00575Transactions of the Association for Computational Linguistics. 112023. 2023</p>
<p>Evaluating the Smooth Control of Attribute Intensity in Text Generation with LLMs. Shang Zhou, Feng Yao, Chengyu Dong, Zihan Wang, Jingbo Shang, 10.18653/v1/2024.findings-acl.258Findings of the Association for Computational Linguistics ACL 2024. Bangkok, Thailand2024and virtual meeting</p>            </div>
        </div>

    </div>
</body>
</html>
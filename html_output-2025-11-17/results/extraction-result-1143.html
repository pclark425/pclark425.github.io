<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1143 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1143</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1143</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-370345</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1511.06890v1.pdf" target="_blank">Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of epsilon-GPP with performance guarantee. We empirically demonstrate the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1143.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1143.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPP (π*)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-optimal Gaussian Process Planning policy (π*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The ideal Bayes-optimal policy for Gaussian Process Planning that maximizes the H-stage expected cumulative reward by solving the Bellman recursion over GP posterior predictive distributions; intractable in general because expectations require integrating over an uncountable measurement space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPP (π*)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Bayes-optimal sequential decision agent that (theoretically) uses a Gaussian process model of the environment, computes posterior predictive p(z_{t+1}|d_t,s_{t+1}), and selects next locations by maximizing Q_t = E[R(Z_{t+1},s_{t+1}) | d_t, s_{t+1}] + E[V*_{t+1}(...)]. Key components: GP regression (closed-form posterior mean/variance), H-stage Bellman recursion, Lipschitz-continuous reward functions used to prove regularity.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayes-optimal active planning (integrated active learning / Bayesian optimization via Bellman planning)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts actions by using the current GP posterior (mean and predictive variance) and the full expected future value (V*_{t+1}) to choose the next sampling location that maximizes expected immediate reward plus expected future cumulative reward; adaptation is conditioned on realized past observations (i.e., fully adaptive Bayes planning).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Environmental fields (simulated wind speed, simulated plankton (chl-a), real-world log-potassium (lg-K) field)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown spatial field modeled as a Gaussian process; continuous-valued, spatially correlated, noisy observations (Gaussian measurement noise), partially observable (only visited locations observed), discrete spatial action grid (robot moves to reachable locations), stochastic observations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Grid experiments: simulated fields on 20×20 grid (simulations) and real-world lg-K on 14×12 grid; action set: up to 4 neighboring moves per time step; episode length / sampling budget H up to 20; continuous measurement space (real-valued), GP hyperparameters assumed known during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Theoretically optimal in expected-reward sense, but not computable in general due to integral over continuous observation space; sample-efficiency statements are theoretical (basis for bounds derived later) rather than empirical.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly balanced via Bellman objective: immediate expected reward (exploitation) plus expected future value that depends on posterior updates (exploration); the trade-off is solved jointly by maximizing expected cumulative reward under the GP belief.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as the theoretical optimal reference; compared against -GPP (practical approx), anytime -GPP, nonmyopic UCB, greedy PI/EI/UCB in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Formulation of Bayes-optimal GPP clarifies how to jointly optimize exploration and exploitation under a GP belief and Lipschitz reward class; V*_t is proven Lipschitz in observed measurements (Theorem 1), enabling deterministic approximation schemes and performance guarantees for practical approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Intractable to compute exactly in general because expectations require integrating over an uncountable measurement space; only computable in degenerate cases (e.g., reward independent of measurement or tiny horizons).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1143.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1143.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ε-GPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonmyopic adaptive ε-optimal Gaussian Process Planning (-GPP / epsilon-GPP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical nonmyopic adaptive GP planning algorithm that approximates the Bayes-optimal GPP by deterministic sampling of the GP predictive measurement distribution and provides user-specified performance guarantees (ε-optimality) under Lipschitz-continuous reward functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>-GPP (epsilon-GPP)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that approximates the Bellman recursion by partitioning the predictive measurement distribution into n deterministic sample points (parameterized by n and a tail-width τ), uses these weighted samples to approximate expectations, and performs H-stage planning; relies on GP posterior mean/variance and Lipschitz reward structure to bound approximation error. Key components: GP model, deterministic sampling approximation, Bellman search over reachable locations, theoretical bounds (Theorems 2–3).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active experimental design via nonmyopic Bayesian planning; unifies active learning and Bayesian optimization objectives within a GP-belief Bellman framework.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by computing approximate Q-values using deterministic sample points from the GP predictive p(z_{t+1}|d_t,s_{t+1}); selects next location maximizing approximate expected immediate reward plus approximate expected future value; adaptation depends on realized past observations through the GP posterior and the approximated Bellman backup.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental environments used in paper: simulated wind-speed (energy harvesting), simulated plankton (chl-a) field (BO), and real-world log-potassium (lg-K) concentration field (BO)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown spatial fields modeled as GPs, continuous-valued, spatially correlated, Gaussian observation noise, partially observable (only sampled locations observed), discretized spatial domain (grid), robot mobility constraints (4-neighbor moves).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Simulations: 20×20 grid (simulated) or 14×12 grid (real lg-K); action branching: up to |A(s_t)| ≤ 4; planning horizon H up to 20 (experiments used H=20 for many tasks); deterministic sampling parameter n (varied per run) and τ control approximation granularity; worst-case planner complexity O(n^H) for full tree expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical: -GPP outperforms methods that assume maximum-likelihood observations during planning and greedy baselines in the reported tasks. Example: on real-world lg-K BO after 20 steps the paper reports that nonmyopic UCB (assuming ML observations) found a maximum of 3.62 which was at least 0.4 σ_y worse than the best -GPP policy; on simulated chl-a nonmyopic UCB found a max of 1.25 (≥0.26 σ_y worse) while best -GPP variants achieved higher maxima and total rewards. (Rewards normalized by subtracting prior mean.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (non-adaptive or myopic): nonmyopic UCB with ML-observation assumption and greedy policies performed worse (see above); greedy policies and ML-assumption planners often reached perceived local maxima early and had lower total rewards after 20 samples (numerical examples provided in paper as described in performance_with_adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically effective within limited sampling budget H=20: -GPP variants achieved higher cumulative rewards within 20 samples averaged over 25–30 trials. The paper also shows that increasing deterministic sampling size n and/or reducing ε improves approximation but increases computation; theoretical sample-like parameter n controls approximation quality rather than number of environment samples (environment samples were limited by H).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Automatically and jointly optimized by maximizing approximate expected cumulative reward (approximate Bellman backups); exploration arises through the expected future value term which depends on predictive uncertainty (GP variance); no explicit ad-hoc exploration weight required, but a βσ term may be added to R_3 when a stronger explicit exploration incentive is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against nonmyopic UCB (Marchant et al. 2014; ML-observation variant), greedy PI, EI, greedy UCB (Srinivas et al. 2010 / Brochu et al. 2010), and the anytime -GPP variant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>-GPP provides a mechanism to compute nonmyopic adaptive GP planning policies with provable ε-optimality guarantees (Theorems 2 and 3) under Lipschitz reward assumptions. Empirically, -GPP policies outperform greedy baselines and nonmyopic UCB when the latter assumes maximum-likelihood observations during planning; tighter user-specified loss bounds (smaller ε) and longer planning horizons improve final performance but increase computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computational scalability: full tree expansion time grows as O(n^H) and becomes prohibitive for small ε (requires large n) or long horizons H; requires known GP hyperparameters (unknown-hyperparameter handling left for future work); approximation quality sensitive to deterministic sampling parameters (n, τ) and to the choice of ε; extreme ML-observation approximation (τ=0) can lead to premature local maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1143.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1143.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>anytime -GPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Branch-and-bound anytime variant of ε-GPP (anytime -GPP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An anytime branch-and-bound algorithm that incrementally constructs the deterministic-sampling Bellman search tree, expanding nodes with largest uncertainty first and maintaining Lipschitz-informed upper/lower bounds to produce progressively better nonmyopic adaptive policies under time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>anytime -GPP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Practical anytime planner built on ε-GPP that keeps upper and lower heuristic bounds of value at nodes, selects nodes to expand by highest uncertainty (difference between bounds weighted by sample weights), constructs minimal subtrees using medians of predictive partitions, and backpropagates refined bounds via Lipschitz continuity; key components: incremental tree expansion, bound propagation, deterministic sampling, GP predictive model.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active Bayesian planning with anytime branch-and-bound approximation</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Produces an initial coarse policy quickly by expanding a small subset of the full search tree (median samples), and iteratively refines the policy by expanding subtrees rooted at nodes with highest value uncertainty; uses Lipschitz continuity to tighten sibling bounds during backpropagation so that tree expansion focuses on most informative branches.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same environments as -GPP: simulated wind (energy harvesting), simulated plankton (chl-a), real-world log-potassium (lg-K)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown GP-modeled spatial fields: continuous measurements, spatial correlation (length-scales may be small or large), Gaussian observation noise, partial observability, discrete spatial action grid; experiments used grids 20×20 and 14×12, move-to-adjacent actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Used with planning horizons up to H=20 for some tasks; experiments constrained maximum tree sizes to practical limits (examples: 3×10^4 nodes for lg-K runs, 5×10^4 for some energy-harvesting runs, 7.5×10^4 for simulated chl-a), action branching ≤4, averaged over 25–30 trials per setting.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical: anytime -GPP achieved performance comparable to the best full -GPP policies under realistic node-budget constraints; e.g., on real-world lg-K the anytime variant (max tree size 3×10^4) performed comparably to the best -GPP with ε=3 and outperformed nonmyopic UCB (ML assumption) after 20 steps. On simulated tasks it performs reasonably close to full -GPP while using far fewer tree nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Provides rapid, usable policies under tight time budgets—empirically produces good policies much faster than full -GPP tree expansion; sample-efficiency in terms of environment samples remains governed by H (e.g., H=20), but planning-time/sample trade-offs are favorable: good policies found with tree sizes orders of magnitude smaller than full expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Same Bellman-based mechanism as -GPP but the anytime algorithm focuses computation on branches with high uncertainty; exploration is encouraged implicitly by the expected-future-value term computed (or lower-bounded) at partially expanded nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared empirically to full -GPP runs (various ε and H settings), nonmyopic UCB (ML-observation), and greedy PI/EI/UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Anytime -GPP delivers near-best adaptive performance under practical compute budgets by prioritizing expansion of promising/high-uncertainty branches and using Lipschitz-based bounds to prune/unexpand others; achieves comparable empirical performance to best full -GPP in experiments while using constrained tree sizes (examples: 3×10^4–7.5×10^4 nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance depends on node budget and heuristic bound quality; if tree budget is too small relative to problem difficulty, the policy may be suboptimal; still requires careful tuning of deterministic sampling (n, τ) used at expanded nodes and assumes known GP hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1143.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1143.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nonmyopic UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonmyopic UCB-based planning (as in Marchant, Ramos, and Sanner 2014)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning approach that uses a UCB-style acquisition µ + βσ within a nonmyopic planning framework; commonly implemented in practice with an approximation that assumes maximum-likelihood (ML) observations during planning to reduce computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sequential Bayesian optimisation for spatial-temporal monitoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Nonmyopic UCB (ML-observation variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses GP posterior mean µ and variance σ to define UCB acquisition µ + βσ for candidate locations; in nonmyopic variants, this acquisition is used stagewise within a planning horizon but is often approximated by assuming ML observations (i.e., future observations equal posterior mean) to avoid integrating over measurement distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization using UCB acquisition, often approximated for nonmyopic planning by maximum-likelihood observation assumption</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by selecting next location that maximizes µ + βσ (or a multi-step plan built from that criterion); when ML-observation approximation used, the planner updates beliefs deterministically with predicted means, reducing adaptivity and neglecting measurement uncertainty in future planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental environments used in paper (simulated fields and real-world lg-K)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>GP-modeled continuous spatial fields, noisy observations, partial observability, discrete movement on grid; the method's ML-observation approximation makes future planning non-adaptive to realized stochastic measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Experiment settings: grids 20×20 and 14×12, action branching ≤4, horizon H up to 20 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical: when implemented with ML-observation approximation, performance degrades relative to -GPP; paper reports on lg-K BO that this method found a maximum lg-K of 3.62 after 20 steps, which was at least 0.4 σ_y worse than the best -GPP policy; on simulated chl-a it found max 1.25 (≥0.26 σ_y worse than best -GPP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Faster in planning (due to ML-observation assumption) but sample-inefficient in final-quality sense: can converge to local maxima prematurely and hence require more environment samples (or fail to reach global maxima) within same budget H.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit via µ + βσ acquisition (β tunes exploration strength); however, in the ML-observation nonmyopic approximation, exploration is under-represented because future uncertainty is not properly integrated. Paper notes that β must be tuned if stronger exploration is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against -GPP and anytime -GPP (which integrate expectation properly / approximate it with deterministic sampling), and greedy PI/EI/UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>When using ML-observation approximation for nonmyopic planning, nonmyopic UCB can perform substantially worse than -GPP (which properly accounts for measurement uncertainty) and can prematurely fixate on perceived local maxima; provides computational efficiency at the cost of reduced solution quality and no theoretical performance guarantee under that approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>The ML-observation approximation reduces adaptivity and can cause poor empirical performance (local maxima); nonmyopic UCB without approximation lacks practical performance guarantees in the paper's context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1143.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1143.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy BO (PI/EI/UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy Bayesian Optimization policies (PI, EI, UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard one-step-lookahead acquisition strategies from Bayesian optimization (Probability of Improvement, Expected Improvement, Upper Confidence Bound) applied greedily at each time step (H=1 planning horizon).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Greedy BO (PI/EI/UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Myopic BO agents that at each step select the next evaluation location by optimizing a one-step acquisition function (PI, EI, or UCB) computed from the current GP posterior; no explicit multi-step planning or integration of future value.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Greedy Bayesian optimization (PI/EI/UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts only myopically: uses current GP posterior mean and variance to compute acquisition function and selects the immediate best location; adaptation uses past observations to update GP but does not anticipate future information-gathering benefits beyond one step.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental environments (simulated chl-a, simulated wind, real-world lg-K) reported in paper comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous spatial fields modeled by GP, noisy measurements, partial observability, discrete movement constraints (robot must move to adjacent grid cells), limited sample budget H.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same grid sizes (20×20 or 14×12), action branching ≤4, experiments with H up to 20.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical: greedy myopic policies performed notably worse than nonmyopic -GPP in experiments; e.g., on real-world lg-K greedy UCB with β=0 found a maximum of 3.56 after 20 steps (worse than best -GPP variants), and on simulated chl-a greedy with β=0 found max 1.28 which was at least 0.22 σ_y worse than the best nonmyopic policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Myopic acquisition is computationally cheap and can yield quick local improvements but is sample-inefficient for global objectives under small sample budgets and when strong exploration is required; in experiments often converged to suboptimal regions within H=20.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled via acquisition function design (e.g., EI trades off improvement vs uncertainty, UCB uses βσ term), but lacks multi-step foresight so exploration is purely myopic and can be insufficient to escape local optima under limited samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against -GPP, anytime -GPP, and nonmyopic UCB in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Greedy BO methods perform worse under limited sampling budgets than nonmyopic planning that accounts for future information gains; the paper shows multi-step planning (even approximate) yields higher cumulative rewards and higher maxima in spatial BO tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Myopic nature leads to insufficient exploration on problems with multimodality or when spatial correlation is limited (small length-scales); cannot naturally trade off longer-term information gains without explicit modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sequential Bayesian optimisation for spatial-temporal monitoring. <em>(Rating: 2)</em></li>
                <li>Gaussian process optimization in the bandit setting: No regret and experimental design. <em>(Rating: 2)</em></li>
                <li>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Nonmyopic ε-Bayes-optimal active learning of Gaussian processes <em>(Rating: 2)</em></li>
                <li>Information-theoretic approach to efficient adaptive path planning for mobile robotic environmental sensing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1143",
    "paper_id": "paper-370345",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "GPP (π*)",
            "name_full": "Bayes-optimal Gaussian Process Planning policy (π*)",
            "brief_description": "The ideal Bayes-optimal policy for Gaussian Process Planning that maximizes the H-stage expected cumulative reward by solving the Bellman recursion over GP posterior predictive distributions; intractable in general because expectations require integrating over an uncountable measurement space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPP (π*)",
            "agent_description": "A Bayes-optimal sequential decision agent that (theoretically) uses a Gaussian process model of the environment, computes posterior predictive p(z_{t+1}|d_t,s_{t+1}), and selects next locations by maximizing Q_t = E[R(Z_{t+1},s_{t+1}) | d_t, s_{t+1}] + E[V*_{t+1}(...)]. Key components: GP regression (closed-form posterior mean/variance), H-stage Bellman recursion, Lipschitz-continuous reward functions used to prove regularity.",
            "adaptive_design_method": "Bayes-optimal active planning (integrated active learning / Bayesian optimization via Bellman planning)",
            "adaptation_strategy_description": "Adapts actions by using the current GP posterior (mean and predictive variance) and the full expected future value (V*_{t+1}) to choose the next sampling location that maximizes expected immediate reward plus expected future cumulative reward; adaptation is conditioned on realized past observations (i.e., fully adaptive Bayes planning).",
            "environment_name": "Environmental fields (simulated wind speed, simulated plankton (chl-a), real-world log-potassium (lg-K) field)",
            "environment_characteristics": "Unknown spatial field modeled as a Gaussian process; continuous-valued, spatially correlated, noisy observations (Gaussian measurement noise), partially observable (only visited locations observed), discrete spatial action grid (robot moves to reachable locations), stochastic observations.",
            "environment_complexity": "Grid experiments: simulated fields on 20×20 grid (simulations) and real-world lg-K on 14×12 grid; action set: up to 4 neighboring moves per time step; episode length / sampling budget H up to 20; continuous measurement space (real-valued), GP hyperparameters assumed known during planning.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": "Theoretically optimal in expected-reward sense, but not computable in general due to integral over continuous observation space; sample-efficiency statements are theoretical (basis for bounds derived later) rather than empirical.",
            "exploration_exploitation_tradeoff": "Explicitly balanced via Bellman objective: immediate expected reward (exploitation) plus expected future value that depends on posterior updates (exploration); the trade-off is solved jointly by maximizing expected cumulative reward under the GP belief.",
            "comparison_methods": "Used as the theoretical optimal reference; compared against -GPP (practical approx), anytime -GPP, nonmyopic UCB, greedy PI/EI/UCB in experiments.",
            "key_results": "Formulation of Bayes-optimal GPP clarifies how to jointly optimize exploration and exploitation under a GP belief and Lipschitz reward class; V*_t is proven Lipschitz in observed measurements (Theorem 1), enabling deterministic approximation schemes and performance guarantees for practical approximations.",
            "limitations_or_failures": "Intractable to compute exactly in general because expectations require integrating over an uncountable measurement space; only computable in degenerate cases (e.g., reward independent of measurement or tiny horizons).",
            "uuid": "e1143.0",
            "source_info": {
                "paper_title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "ε-GPP",
            "name_full": "Nonmyopic adaptive ε-optimal Gaussian Process Planning (-GPP / epsilon-GPP)",
            "brief_description": "A practical nonmyopic adaptive GP planning algorithm that approximates the Bayes-optimal GPP by deterministic sampling of the GP predictive measurement distribution and provides user-specified performance guarantees (ε-optimality) under Lipschitz-continuous reward functions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "-GPP (epsilon-GPP)",
            "agent_description": "Agent that approximates the Bellman recursion by partitioning the predictive measurement distribution into n deterministic sample points (parameterized by n and a tail-width τ), uses these weighted samples to approximate expectations, and performs H-stage planning; relies on GP posterior mean/variance and Lipschitz reward structure to bound approximation error. Key components: GP model, deterministic sampling approximation, Bellman search over reachable locations, theoretical bounds (Theorems 2–3).",
            "adaptive_design_method": "Active experimental design via nonmyopic Bayesian planning; unifies active learning and Bayesian optimization objectives within a GP-belief Bellman framework.",
            "adaptation_strategy_description": "Adapts by computing approximate Q-values using deterministic sample points from the GP predictive p(z_{t+1}|d_t,s_{t+1}); selects next location maximizing approximate expected immediate reward plus approximate expected future value; adaptation depends on realized past observations through the GP posterior and the approximated Bellman backup.",
            "environment_name": "Same experimental environments used in paper: simulated wind-speed (energy harvesting), simulated plankton (chl-a) field (BO), and real-world log-potassium (lg-K) concentration field (BO)",
            "environment_characteristics": "Unknown spatial fields modeled as GPs, continuous-valued, spatially correlated, Gaussian observation noise, partially observable (only sampled locations observed), discretized spatial domain (grid), robot mobility constraints (4-neighbor moves).",
            "environment_complexity": "Simulations: 20×20 grid (simulated) or 14×12 grid (real lg-K); action branching: up to |A(s_t)| ≤ 4; planning horizon H up to 20 (experiments used H=20 for many tasks); deterministic sampling parameter n (varied per run) and τ control approximation granularity; worst-case planner complexity O(n^H) for full tree expansion.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical: -GPP outperforms methods that assume maximum-likelihood observations during planning and greedy baselines in the reported tasks. Example: on real-world lg-K BO after 20 steps the paper reports that nonmyopic UCB (assuming ML observations) found a maximum of 3.62 which was at least 0.4 σ_y worse than the best -GPP policy; on simulated chl-a nonmyopic UCB found a max of 1.25 (≥0.26 σ_y worse) while best -GPP variants achieved higher maxima and total rewards. (Rewards normalized by subtracting prior mean.)",
            "performance_without_adaptation": "Baselines (non-adaptive or myopic): nonmyopic UCB with ML-observation assumption and greedy policies performed worse (see above); greedy policies and ML-assumption planners often reached perceived local maxima early and had lower total rewards after 20 samples (numerical examples provided in paper as described in performance_with_adaptation).",
            "sample_efficiency": "Empirically effective within limited sampling budget H=20: -GPP variants achieved higher cumulative rewards within 20 samples averaged over 25–30 trials. The paper also shows that increasing deterministic sampling size n and/or reducing ε improves approximation but increases computation; theoretical sample-like parameter n controls approximation quality rather than number of environment samples (environment samples were limited by H).",
            "exploration_exploitation_tradeoff": "Automatically and jointly optimized by maximizing approximate expected cumulative reward (approximate Bellman backups); exploration arises through the expected future value term which depends on predictive uncertainty (GP variance); no explicit ad-hoc exploration weight required, but a βσ term may be added to R_3 when a stronger explicit exploration incentive is desired.",
            "comparison_methods": "Compared against nonmyopic UCB (Marchant et al. 2014; ML-observation variant), greedy PI, EI, greedy UCB (Srinivas et al. 2010 / Brochu et al. 2010), and the anytime -GPP variant.",
            "key_results": "-GPP provides a mechanism to compute nonmyopic adaptive GP planning policies with provable ε-optimality guarantees (Theorems 2 and 3) under Lipschitz reward assumptions. Empirically, -GPP policies outperform greedy baselines and nonmyopic UCB when the latter assumes maximum-likelihood observations during planning; tighter user-specified loss bounds (smaller ε) and longer planning horizons improve final performance but increase computational cost.",
            "limitations_or_failures": "Computational scalability: full tree expansion time grows as O(n^H) and becomes prohibitive for small ε (requires large n) or long horizons H; requires known GP hyperparameters (unknown-hyperparameter handling left for future work); approximation quality sensitive to deterministic sampling parameters (n, τ) and to the choice of ε; extreme ML-observation approximation (τ=0) can lead to premature local maxima.",
            "uuid": "e1143.1",
            "source_info": {
                "paper_title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "anytime -GPP",
            "name_full": "Branch-and-bound anytime variant of ε-GPP (anytime -GPP)",
            "brief_description": "An anytime branch-and-bound algorithm that incrementally constructs the deterministic-sampling Bellman search tree, expanding nodes with largest uncertainty first and maintaining Lipschitz-informed upper/lower bounds to produce progressively better nonmyopic adaptive policies under time constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "anytime -GPP",
            "agent_description": "Practical anytime planner built on ε-GPP that keeps upper and lower heuristic bounds of value at nodes, selects nodes to expand by highest uncertainty (difference between bounds weighted by sample weights), constructs minimal subtrees using medians of predictive partitions, and backpropagates refined bounds via Lipschitz continuity; key components: incremental tree expansion, bound propagation, deterministic sampling, GP predictive model.",
            "adaptive_design_method": "Active Bayesian planning with anytime branch-and-bound approximation",
            "adaptation_strategy_description": "Produces an initial coarse policy quickly by expanding a small subset of the full search tree (median samples), and iteratively refines the policy by expanding subtrees rooted at nodes with highest value uncertainty; uses Lipschitz continuity to tighten sibling bounds during backpropagation so that tree expansion focuses on most informative branches.",
            "environment_name": "Same environments as -GPP: simulated wind (energy harvesting), simulated plankton (chl-a), real-world log-potassium (lg-K)",
            "environment_characteristics": "Unknown GP-modeled spatial fields: continuous measurements, spatial correlation (length-scales may be small or large), Gaussian observation noise, partial observability, discrete spatial action grid; experiments used grids 20×20 and 14×12, move-to-adjacent actions.",
            "environment_complexity": "Used with planning horizons up to H=20 for some tasks; experiments constrained maximum tree sizes to practical limits (examples: 3×10^4 nodes for lg-K runs, 5×10^4 for some energy-harvesting runs, 7.5×10^4 for simulated chl-a), action branching ≤4, averaged over 25–30 trials per setting.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical: anytime -GPP achieved performance comparable to the best full -GPP policies under realistic node-budget constraints; e.g., on real-world lg-K the anytime variant (max tree size 3×10^4) performed comparably to the best -GPP with ε=3 and outperformed nonmyopic UCB (ML assumption) after 20 steps. On simulated tasks it performs reasonably close to full -GPP while using far fewer tree nodes.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Provides rapid, usable policies under tight time budgets—empirically produces good policies much faster than full -GPP tree expansion; sample-efficiency in terms of environment samples remains governed by H (e.g., H=20), but planning-time/sample trade-offs are favorable: good policies found with tree sizes orders of magnitude smaller than full expansion.",
            "exploration_exploitation_tradeoff": "Same Bellman-based mechanism as -GPP but the anytime algorithm focuses computation on branches with high uncertainty; exploration is encouraged implicitly by the expected-future-value term computed (or lower-bounded) at partially expanded nodes.",
            "comparison_methods": "Compared empirically to full -GPP runs (various ε and H settings), nonmyopic UCB (ML-observation), and greedy PI/EI/UCB.",
            "key_results": "Anytime -GPP delivers near-best adaptive performance under practical compute budgets by prioritizing expansion of promising/high-uncertainty branches and using Lipschitz-based bounds to prune/unexpand others; achieves comparable empirical performance to best full -GPP in experiments while using constrained tree sizes (examples: 3×10^4–7.5×10^4 nodes).",
            "limitations_or_failures": "Performance depends on node budget and heuristic bound quality; if tree budget is too small relative to problem difficulty, the policy may be suboptimal; still requires careful tuning of deterministic sampling (n, τ) used at expanded nodes and assumes known GP hyperparameters.",
            "uuid": "e1143.2",
            "source_info": {
                "paper_title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Nonmyopic UCB",
            "name_full": "Nonmyopic UCB-based planning (as in Marchant, Ramos, and Sanner 2014)",
            "brief_description": "A planning approach that uses a UCB-style acquisition µ + βσ within a nonmyopic planning framework; commonly implemented in practice with an approximation that assumes maximum-likelihood (ML) observations during planning to reduce computational cost.",
            "citation_title": "Sequential Bayesian optimisation for spatial-temporal monitoring.",
            "mention_or_use": "use",
            "agent_name": "Nonmyopic UCB (ML-observation variant)",
            "agent_description": "Uses GP posterior mean µ and variance σ to define UCB acquisition µ + βσ for candidate locations; in nonmyopic variants, this acquisition is used stagewise within a planning horizon but is often approximated by assuming ML observations (i.e., future observations equal posterior mean) to avoid integrating over measurement distributions.",
            "adaptive_design_method": "Bayesian optimization using UCB acquisition, often approximated for nonmyopic planning by maximum-likelihood observation assumption",
            "adaptation_strategy_description": "Adapts by selecting next location that maximizes µ + βσ (or a multi-step plan built from that criterion); when ML-observation approximation used, the planner updates beliefs deterministically with predicted means, reducing adaptivity and neglecting measurement uncertainty in future planning.",
            "environment_name": "Same experimental environments used in paper (simulated fields and real-world lg-K)",
            "environment_characteristics": "GP-modeled continuous spatial fields, noisy observations, partial observability, discrete movement on grid; the method's ML-observation approximation makes future planning non-adaptive to realized stochastic measurements.",
            "environment_complexity": "Experiment settings: grids 20×20 and 14×12, action branching ≤4, horizon H up to 20 in experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical: when implemented with ML-observation approximation, performance degrades relative to -GPP; paper reports on lg-K BO that this method found a maximum lg-K of 3.62 after 20 steps, which was at least 0.4 σ_y worse than the best -GPP policy; on simulated chl-a it found max 1.25 (≥0.26 σ_y worse than best -GPP).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Faster in planning (due to ML-observation assumption) but sample-inefficient in final-quality sense: can converge to local maxima prematurely and hence require more environment samples (or fail to reach global maxima) within same budget H.",
            "exploration_exploitation_tradeoff": "Explicit via µ + βσ acquisition (β tunes exploration strength); however, in the ML-observation nonmyopic approximation, exploration is under-represented because future uncertainty is not properly integrated. Paper notes that β must be tuned if stronger exploration is desired.",
            "comparison_methods": "Compared against -GPP and anytime -GPP (which integrate expectation properly / approximate it with deterministic sampling), and greedy PI/EI/UCB.",
            "key_results": "When using ML-observation approximation for nonmyopic planning, nonmyopic UCB can perform substantially worse than -GPP (which properly accounts for measurement uncertainty) and can prematurely fixate on perceived local maxima; provides computational efficiency at the cost of reduced solution quality and no theoretical performance guarantee under that approximation.",
            "limitations_or_failures": "The ML-observation approximation reduces adaptivity and can cause poor empirical performance (local maxima); nonmyopic UCB without approximation lacks practical performance guarantees in the paper's context.",
            "uuid": "e1143.3",
            "source_info": {
                "paper_title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Greedy BO (PI/EI/UCB)",
            "name_full": "Greedy Bayesian Optimization policies (PI, EI, UCB)",
            "brief_description": "Standard one-step-lookahead acquisition strategies from Bayesian optimization (Probability of Improvement, Expected Improvement, Upper Confidence Bound) applied greedily at each time step (H=1 planning horizon).",
            "citation_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning.",
            "mention_or_use": "use",
            "agent_name": "Greedy BO (PI/EI/UCB)",
            "agent_description": "Myopic BO agents that at each step select the next evaluation location by optimizing a one-step acquisition function (PI, EI, or UCB) computed from the current GP posterior; no explicit multi-step planning or integration of future value.",
            "adaptive_design_method": "Greedy Bayesian optimization (PI/EI/UCB)",
            "adaptation_strategy_description": "Adapts only myopically: uses current GP posterior mean and variance to compute acquisition function and selects the immediate best location; adaptation uses past observations to update GP but does not anticipate future information-gathering benefits beyond one step.",
            "environment_name": "Same experimental environments (simulated chl-a, simulated wind, real-world lg-K) reported in paper comparisons",
            "environment_characteristics": "Unknown continuous spatial fields modeled by GP, noisy measurements, partial observability, discrete movement constraints (robot must move to adjacent grid cells), limited sample budget H.",
            "environment_complexity": "Same grid sizes (20×20 or 14×12), action branching ≤4, experiments with H up to 20.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical: greedy myopic policies performed notably worse than nonmyopic -GPP in experiments; e.g., on real-world lg-K greedy UCB with β=0 found a maximum of 3.56 after 20 steps (worse than best -GPP variants), and on simulated chl-a greedy with β=0 found max 1.28 which was at least 0.22 σ_y worse than the best nonmyopic policies.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Myopic acquisition is computationally cheap and can yield quick local improvements but is sample-inefficient for global objectives under small sample budgets and when strong exploration is required; in experiments often converged to suboptimal regions within H=20.",
            "exploration_exploitation_tradeoff": "Handled via acquisition function design (e.g., EI trades off improvement vs uncertainty, UCB uses βσ term), but lacks multi-step foresight so exploration is purely myopic and can be insufficient to escape local optima under limited samples.",
            "comparison_methods": "Compared against -GPP, anytime -GPP, and nonmyopic UCB in the experiments.",
            "key_results": "Greedy BO methods perform worse under limited sampling budgets than nonmyopic planning that accounts for future information gains; the paper shows multi-step planning (even approximate) yields higher cumulative rewards and higher maxima in spatial BO tasks.",
            "limitations_or_failures": "Myopic nature leads to insufficient exploration on problems with multimodality or when spatial correlation is limited (small length-scales); cannot naturally trade off longer-term information gains without explicit modifications.",
            "uuid": "e1143.4",
            "source_info": {
                "paper_title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond",
                "publication_date_yy_mm": "2015-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sequential Bayesian optimisation for spatial-temporal monitoring.",
            "rating": 2,
            "sanitized_title": "sequential_bayesian_optimisation_for_spatialtemporal_monitoring"
        },
        {
            "paper_title": "Gaussian process optimization in the bandit setting: No regret and experimental design.",
            "rating": 2,
            "sanitized_title": "gaussian_process_optimization_in_the_bandit_setting_no_regret_and_experimental_design"
        },
        {
            "paper_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning.",
            "rating": 1,
            "sanitized_title": "a_tutorial_on_bayesian_optimization_of_expensive_cost_functions_with_application_to_active_user_modeling_and_hierarchical_reinforcement_learning"
        },
        {
            "paper_title": "Nonmyopic ε-Bayes-optimal active learning of Gaussian processes",
            "rating": 2,
            "sanitized_title": "nonmyopic_εbayesoptimal_active_learning_of_gaussian_processes"
        },
        {
            "paper_title": "Information-theoretic approach to efficient adaptive path planning for mobile robotic environmental sensing",
            "rating": 1,
            "sanitized_title": "informationtheoretic_approach_to_efficient_adaptive_path_planning_for_mobile_robotic_environmental_sensing"
        }
    ],
    "cost": 0.0195265,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond</p>
<p>Chun Kai Ling 
Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
USA</p>
<p>Kian Hsiang Low 
Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
USA</p>
<p>Patrick Jaillet jaillet@mit.edu† </p>
<p>Department of Computer Science
National University of Singapore
RepublicSingapore</p>
<p>Gaussian Process Planning with Lipschitz Continuous Reward Functions: Towards Unifying Bayesian Optimization, Active Learning, and Beyond</p>
<p>This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive -optimal GPP ( -GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of -GPP with performance guarantee. We empirically demonstrate the effectiveness of our -GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.</p>
<p>Introduction</p>
<p>The fundamental challenge of integrated planning and learning is to design an autonomous agent that can plan its actions to maximize its expected total rewards while interacting with an unknown task environment. Recent research efforts tackling this challenge have progressed from the use of simple Markov models assuming discrete-valued, independent observations (e.g., in Bayesian reinforcement learning (BRL) (Poupart et al. 2006)) to that of a rich class of Bayesian nonparametric Gaussian process (GP) models characterizing continuous-valued, correlated observations in order to represent the latent structure of more complex, possibly noisy task environments with higher fidelity. Such a challenge is posed by the following important problems in machine learning, among others: Active learning/sensing (AL). In the context of environmental sensing (e.g., adaptive sampling in oceanography (Leonard et al. 2007), traffic sensing Chen et al. 2015)), its objective is to select the most informative (possibly noisy) observations for predicting a spatially varying environmental field (i.e., task environment) modeled by a GP subject to some sampling budget constraints (e.g., number of sensors, energy consumption). The rewards of an AL agent are defined based on some formal measure of predictive uncertainty such as the entropy or mutual information criterion. To resolve the issue of sub-optimality (i.e., local maxima) faced by greedy algorithms (Krause, Singh, and Guestrin 2008;Ouyang et al. 2014;Zhang et al. 2016), recent developments have made nonmyopic AL computationally tractable with provable performance guarantees (Cao, Low, and Dolan 2013;Hoang et al. 2014;Low, Dolan, and Khosla 2009;2011), some of which have further investigated the performance advantage of adaptivity by proposing nonmyopic adaptive observation selection policies that depend on past observations. Bayesian optimization (BO). Its objective is to select and gather the most informative (possibly noisy) observations for finding the global maximum of an unknown, highly complex (e.g., non-convex, no closed-form expression nor derivative) objective function (i.e., task environment) modeled by a GP given a sampling budget (e.g., number of costly function evaluations). The rewards of a BO agent are defined using an improvement-based (Brochu, Cora, and de Freitas 2010) (e.g., probability of improvement (PI) or expected improvement (EI) over currently found maximum), entropybased (Hennig and Schuler 2012;Hernández-Lobato, Hoffman, and Ghahramani 2014), or upper confidence bound (UCB) acquisition function (Srinivas et al. 2010). A limitation of most BO algorithms is that they are myopic. To overcome this limitation, approximation algorithms for nonmyopic adaptive BO (Marchant, Ramos, and Sanner 2014;Osborne, Garnett, and Roberts 2009) have been proposed, but their performances are not theoretically guaranteed. General tasks/problems. In practice, other types of rewards (e.g., logarithmic, unit step functions) need to be specified for an agent to plan and operate effectively in a given realworld task environment (e.g., natural phenomenon like wind or temperature) modeled by a GP, as detailed in Section 2.</p>
<p>As shall be elucidated later, similarities in the structure of the above problems motivate us to consider whether it is possible to tackle the overall challenge by devising a nonmyopic adaptive GP planning framework with a general class of reward functions unifying some AL and BO criteria and affording practitioners some flexibility to specify their desired choices for defining new tasks/problems. Such an integrated planning and learning framework has to address the exploration-exploitation trade-off common to the above problems: The agent faces a dilemma between gathering observations to maximize its expected total rewards given its current, possibly imprecise belief of the task environment (exploitation) vs. that to improve its belief to learn more about the environment (exploration). This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some AL and BO criteria (e.g., UCB) discussed earlier and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems (Section 2). In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off, consequently allowing planning and learning to be integrated seamlessly and performed simultaneously instead of separately (Deisenroth, Fox, and Rasmussen 2015). In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive -optimal GPP ( -GPP) policy given an arbitrarily userspecified loss bound (Section 3). To plan in real time, we further propose an asymptotically optimal, branch-andbound anytime variant of -GPP with performance guarantee. Finally, we empirically evaluate the performances of our -GPP policy and its anytime variant in BO and an energy harvesting task on simulated and real-world environmental fields (Section 4). To ease exposition, the rest of this paper will be described by assuming the task environment to be an environmental field and the agent to be a mobile robot, which coincide with the setup of our experiments.</p>
<p>Gaussian Process Planning (GPP)</p>
<p>Notations and Preliminaries. Let S be the domain of an environmental field corresponding to a set of sampling locations. At time step t &gt; 0, a robot can deterministically move from its previous location s t−1 to visit location s t ∈ A(s t−1 ) and observes it by taking a corresponding realized (random) field measurement z t (Z t ) where A(s t−1 ) ⊆ S denotes a finite set of sampling locations reachable from its previous location s t−1 in a single time step. The state of the robot at its initial starting location s 0 is represented by prior observations/data d 0 s 0 , z 0 available before planning where s 0 and z 0 denote, respectively, vectors comprising locations visited/observed and corresponding field measurements taken by the robot prior to planning and s 0 is the last component of s 0 . Similarly, at time step t &gt; 0, the state of the robot at its current location s t is represented by observations/data d t s t , z t where s t s 0 ⊕ (s 1 , · · · s t ) and z t z 0 ⊕ (z 1 , · · · z t ) denote, respectively, vectors comprising locations visited/observed and corresponding field measurements taken by the robot up until time step t and '⊕' denotes vector concatenation. At time step t &gt; 0, the robot also receives a reward R(z t , s t ) to be defined later. Modeling Environmental Fields with Gaussian Processes (GPs). The GP can be used to model a spatially varying environmental field as follows: The field is assumed to be a re-alization of a GP. Each location s ∈ S is associated with a latent field measurement Y s . Let Y S {Y s } s∈S denote a GP, that is, every finite subset of Y S has a multivariate Gaussian distribution (Rasmussen and Williams 2006). Then, the GP is fully specified by its prior mean µ s E[Y s ] and covariance k ss cov[Y s , Y s ] for all s, s ∈ S, the latter of which characterizes the spatial correlation structure of the environment field and can be defined using a covariance function. A common choice is the squared exponential covariance function k ss σ 2 y exp{−0.5(s−s ) M −2 (s−s )} where σ 2 y is the signal variance controlling the intensity of measurements and M is a diagonal matrix with length-scale components l 1 and l 2 governing the degree of spatial correlation or "similarity" between measurements in the respective horizontal and vertical directions of the 2D fields in our experiments.</p>
<p>The field measurements taken by the robot are assumed to be corrupted by Gaussian white noise, i.e., Z t Y st + ε where ε ∼ N (0, σ 2 n ) and σ 2 n is the noise variance. Supposing the robot has gathered observations d t = s t , z t from time steps 0 to t, the GP model can perform probabilistic regression by using d t to predict the noisy measurement at any unobserved location s t+1 ∈ A(s t ) as well as provide its predictive uncertainty using a Gaussian predictive distribution p(z t+1 |d t , s t+1 ) = N (µ st+1|dt , σ 2 st+1|st ) with the following posterior mean and variance, respectively:
µ st+1|dt µ st+1 + Σ st+1st Γ −1 stst (z t − µ st ) σ 2 st+1|st k st+1st+1 + σ 2 n − Σ st+1st Γ −1 stst Σ stst+1
where µ st is a row vector with mean components µ s for every location s of s t , Σ st+1st is a row vector with covariance components k st+1s for every location s of s t , Σ stst+1 is the transpose of Σ st+1st , and Γ stst Σ stst + σ 2 n I such that Σ stst is a covariance matrix with components k ss for every pair of locations s, s of s t . An important property of the GP model is that, unlike µ st+1|dt , σ 2 st+1|st is independent of z t . Problem Formulation. To frame nonmyopic adaptive Gaussian process planning (GPP) as a Bayesian sequential decision problem, let an adaptive policy π be defined to sequentially decide the next location π(d t ) ∈ A(s t ) to be observed at each time step t using observations d t over a finite planning horizon of H time steps/stages (i.e., sampling budget of H locations). The value V π 0 (d 0 ) under an adaptive policy π is defined to be the expected total rewards achieved by its selected observations when starting with some prior observations d 0 and following π thereafter and can be computed using the following H-stage Bellman equations:
V π t (d t ) Q π t (d t , π(d t )) Q π t (d t , s t+1 ) E[R(Z t+1 , s t+1 ) + V π t+1 ( s t+1 , z t ⊕ Z t+1 )|d t , s t+1 ] for stages t = 0, . . . , H − 1 where V π H (d H ) 0.
To solve the GPP problem, the notion of Bayes-optimality 1 is exploited for selecting observations to achieve the largest possible expected total rewards with respect to all possible induced sequences of future Gaussian posterior beliefs p(z t+1 |d t , s t+1 ) for t = 0, . . . , H − 1 to be discussed next. Formally, this involves choosing an adaptive policy π to maximize V π 0 (d 0 ), which we call the GPP policy π * . That is,
V * 0 (d 0 ) V π * 0 (d 0 ) = max π V π 0 (d 0 ). By plugging π * into V π t (d t ) and Q π t (d t , s t+1 ) above, V * t (d t ) max st+1∈A(st) Q * t (d t , s t+1 ) Q * t (d t , s t+1 ) E[R(Z t+1 , s t+1 )|d t , s t+1 ] + E<a href="1">V * t+1 ( s t+1 , z t ⊕ Z t+1 )|d t , s t+1 </a>for stages t = 0, . . . , H − 1 where V * H (d H ) 0.
To see how the GPP policy π * jointly and naturally optimizes the exploration-exploitation trade-off, its selected location π * (d t ) = arg max st+1∈A(st) Q * t (d t , s t+1 ) at each time step t affects both the immediate expected reward E[R(Z t+1 , s t ⊕ π * (d t ))|d t , π * (d t )] given current belief p(z t+1 |d t , π * (d t )) (i.e., exploitation) as well as the Gaussian posterior belief p(z t+2 | s t ⊕ π * (d t ), z t ⊕ z t+1 , π * ( s t ⊕ π * (d t ), z t ⊕ z t+1 )) at next time step t + 1 (i.e., exploration), the latter of which influences expected future re-
wards E[V * t+1 ( s t ⊕ π * (d t ), z t ⊕ Z t+1 )|d t , π * (d t )]
. In general, the GPP policy π * cannot be derived exactly because the expectation terms in (1) usually cannot be evaluated in closed form due to an uncountable set of candidate measurements (Section 1) except for degenerate cases like R(z t+1 , s t+1 ) being independent of z t+1 and H ≤ 2. To overcome this difficulty, we will show in Section 3 later how the Lipschitz continuity of the reward functions can be exploited for theoretically guaranteeing the performance of our proposed nonmyopic adaptive -optimal GPP policy, that is, the expected total rewards achieved by its selected observations closely approximates that of π * within an arbitrarily user-specified loss bound &gt; 0. Lipschitz Continuous Reward Functions. R(z t , s t ) R 1 (z t )+R 2 (z t )+R 3 (s t ) where R 1 , R 2 , and R 3 are user-defined reward functions that satisfy the conditions below:
• R 1 (z t ) is Lipschitz continuous in z t with Lipschitz con- stant 1 . So, h σ (u) (R 1 * N (0, σ 2 ))(u) is Lipschitz continuous in u with 1 where ' * ' denotes convolution; • R 2 (z t ): Define g σ (u) (R 2 * N (0, σ 2 ))(u) such that (a) g σ (u) is well-defined for all u ∈ R, (b) g σ (u)
can be evaluated in closed form or computed up to an arbitrary precision in reasonable time for all u ∈ R, and (c) g σ (u) is Lipschitz continuous 2 in u with Lipschitz constant 2 (σ); • R 3 (s t ) only depends on locations s t visited/observed by the robot up until time step t and is independent of realized measurement z t . It can be used to represent some sampling or motion costs or explicitly consider exploration by defining it as a function of σ 2 st+1|st . Using the above definition of R(z t , s t ), the immediate expected reward in (1) evaluates to E[R(Z t+1 , s t+1 )|d t , s t+1 ] the reward function to be independent of measurements and past states, and/or, when exploiting GP, maximum likelihood observations during planning with no provable performance guarantee.</p>
<p>2 Unlike R1, R2 does not need to be Lipschitz continuous (or continuous); it must only be Lipschitz continuous after convolution with any Gaussian kernel. An example of R2 is unit step function. = (h σ s t+1 |s t +g σ s t+1 |s t ) µ st+1|dt +R 3 (s t+1 ) which is Lipschitz continuous in the realized measurements z t :
Lemma 1 Let α(s t+1 ) Σ st+1st Γ −1 stst and d t s t , z t . Then,|E[R(Z t+1 ,s t+1 )|d t ,s t+1 ]−E[R(Z t+1 ,s t+1 )|d t ,s t+1 ]| ≤ α(s t+1 ) 1 + 2 (σ st+1|st ) z t − z t .
Its proof is in Appendix A. Lemma 1 will be used to prove the Lipschitz continuity of V * t in (1) later. Before doing this, let us consider how the Lipschitz continuous reward functions defined above can unify some AL and BO criteria discussed in Section 1 and be used for defining new tasks/problems. Active learning/sensing (AL). Setting R(z t+1 , s t+1 ) = R 3 (s t+1 ) = 0.5 log(2πeσ 2 st+1|st ) yields the well-known nonmyopic AL algorithm called maximum entropy sampling (MES) (Shewry and Wynn 1987) which plans/decides locations with maximum entropy to be observed that minimize the posterior entropy remaining in the unobserved areas of the field. Since R(z t+1 , s t+1 ) is independent of z t+1 , the expectations in (1) go away, thus making MES non-adaptive and hence a straightforward search algorithm not plagued by the issue of uncountable set of candidate measurements. As such, we will not focus on such a degenerate case. This degeneracy vanishes when the environment field is instead a realization of log-Gaussian process. Then, MES becomes adaptive (Low, Dolan, and Khosla 2009) and its reward function can be represented by our Lipschitz continuous reward functions: By setting R 1 (z t+1 ) = 0, R 2 and g σ s t+1 |s t as identity functions with 2 (σ st+1|st ) = 1, and
R 3 (s t+1 ) = 0.5 log(2πeσ 2 st+1|st ), E[R(Z t+1 , s t+1 )|d t , s t+1 ] = µ st+1|dt + 0.5 log(2πeσ 2 st+1|st )
. Bayesian optimization (BO). The greedy BO algorithm of Srinivas et al. (2010) utilizes the UCB selection criterion µ st+1|dt + βσ st+1|st (β ≥ 0) to approximately optimize the global BO objective of total field measurements H t=1 z t taken by the robot or, equivalently, minimize its total regret. UCB can be represented by our Lipschitz continuous reward functions: By setting R 1 (z t+1 ) = 0, R 2 and g σ s t+1 |s t as identity functions with 2 (σ st+1|st ) = 1, and R 3 (s t+1 ) = βσ st+1|st , E[R(Z t+1 , s t+1 )|d t , s t+1 ] = µ st+1|dt + βσ st+1|st . In particular, when β = 0, it can be derived that our GPP policy π * maximizes the expected total field measurements taken by the robot, hence optimizing the exact global BO objective of Srinivas et al. (2010) in the expected sense. So, unlike greedy UCB, our nonmyopic GPP framework does not have to explicitly consider an additional weighted exploration term (i.e., βσ st+1|st ) in its reward function because it can jointly and naturally optimize the exploration-exploitation trade-off, as explained earlier. Nevertheless, if a stronger exploration behavior is desired (e.g., in online planning), then β has to be fine-tuned. Different from nonmyopic BO algorithm of Marchant, Ramos, and Sanner (2014) using UCB-based rewards, our proposed nonmyopic -optimal GPP policy (Section 3) does not need to impose an extreme assumption of maximum likelihood observations during planning and, more importantly, provides a performance guarantee, including for the extreme assumption made by nonmyopic UCB. Our GPP framework differs from nonmyopic BO algorithm of Osborne, Garnett, and Roberts (2009) in that every selected observation contributes to the total field measurements taken by the robot instead of considering just the expected improvement for the last observation. So, it usually does not have to expend all the given sampling budget to find the global maximum. General tasks/problems. In practice, the necessary reward function can be more complex than the ones specified above that are formed from an identity function of the field measurement. For example, consider the problem of placing wind turbines in optimal locations to maximize the total power production. Though the average wind speed in a region can be modeled by a GP, the power output is not a linear function of the steady-state wind speed. In fact, power production requires a certain minimum speed known as the cutin speed. After this threshold is met, power output increases and eventually plateaus. Assuming the cut-in speed is 1, this effect can be modeled with a logarithmic reward function 3 :
R(z t+1 , s t+1 ) = R 1 (z t+1 ) gives a value of log(z t+1 ) if z t+1 &gt; 1, and 0 otherwise where 1 = 1.
To the best of our knowledge, h σ s t+1 |s t (u) has no closed-form expression. In Appendix B, we present other interesting reward functions like unit step function 2 and Gaussian distribution that can be represented by R(z t+1 , s t+1 ) and used in real-world tasks.</p>
<p>Theorem
1 below reveals that V * t (d t ) (1) with Lipschitz continuous reward functions is Lipschitz continuous in z t with Lipschitz constant L t (s t ) defined below: Definition 1 Let L H (s H ) 0. For t = 0, . . . , H − 1, de- fine L t (s t ) max st+1∈A(st) α(s t+1 ) 1 + 2 (σ st+1|st ) + L t+1 (s t+1 ) 1 + α(s t+1 ) 2 . Theorem 1 (Lipschitz Continuity of V * t ) For t = 0, . . . , H, |V * t (d t ) − V * t (d t )| ≤ L t (s t ) z t − z t .
Its proof uses Lemma 1 and is in Appendix C. The result below is a direct consequence of Theorem 1 and will be used to theoretically guarantee the performance of our proposed nonmyopic adaptive -optimal GPP policy in Section 3:
Corollary 1 For t = 0, . . . , H, |V * t ( s t , z t−1 ⊕ z t ) − V * t ( s t , z t−1 ⊕ z t )| ≤ L t (s t )|z t − z t |.</p>
<p>-Optimal GPP ( -GPP)</p>
<p>The key idea of constructing our proposed nonmyopic adaptive -GPP policy is to approximate the expectation terms in (1) at every stage using a form of deterministic sampling, as illustrated in the figure below. Specifically, the measurement space of p(z t+1 |d t , s t+1 ) is first partitioned into n ≥ 2 intervals ζ 0 , . . . , ζ n−1 such that intervals ζ 1 , . . . , ζ n−2 are equally spaced within the bounded gray region [µ st+1|dt − τ σ st+1|st , µ st+1|dt + τ σ st+1|st ] specified by a user-defined width parameter τ ≥ 0 while intervals ζ 0 and ζ n−1 span the two infinitely long red tails. Note that τ &gt; 0 requires n &gt; 2 for the partition to be valid. The n sample measurements z 0 . . . z n−1 are then selected by setting z 0 as upper limit of red interval ζ 0 , z n−1 as lower limit of red interval ζ n−1 , and z 1 , . . . , z n−2 as centers of the respective gray intervals
w i Φ( 2iτ n−2 −τ )−Φ( 2(i−1)τ n−2 −τ ) for i = 1, . . . , n − 2; w 0 = w n−1 Φ(−τ ). z 0 w 0 z 1 w 1 . . . . . . . . . . . . z i-1 w i-1 z i w i z i+1 w i+1 z n-1 w n-1 z n-2 w n-2 . . . . . . . . . . . . z t+1 ζ 1 ζ i-1 ζ i ζ i+1 ζ n-2 ζ 0 ζ n-1 ζ 1 , .
. . , ζ n−2 . Next, the weights w 0 . . . w n−1 for the corresponding sample measurements z 0 , . . . , z n−1 are defined as the areas under their respective intervals ζ 0 , . . . , ζ n−1 of the Gaussian predictive distribution p(z t+1 |d t , s t+1 ). So,
n−1 i=0 w i = 1.
An example of such a partition is given in Appendix D. The selected sample measurements and their corresponding weights can be exploited for approximating V * t with Lipschitz continuous reward functions (1) using the following H-stage Bellman equations:
V t (d t ) max st+1∈A(st) Q t (d t , s t+1 ) Q t (d t , s t+1 ) g σ s t+1 |s t µ st+1|dt + R 3 (s t+1 ) + n−1 i=0 w i R 1 (z i ) + V t+1 ( s t+1 , z t ⊕ z i ) (2) for stages t = 0, . . . , H − 1 where V H (d H ) 0.
The resulting induced -GPP policy π jointly and naturally optimizes the exploration-exploitation trade-off in a similar manner as that of the GPP policy π * , as explained in Section 2. It is interesting to note that setting τ = 0 yields z 0 = . . . = z n−1 = µ st+1|dt , which is equivalent to selecting a single sample measurement of µ st+1|dt with corresponding weight of 1. This is identical to the special case of maximum likelihood observations during planning which is the extreme assumption used by nonmyopic UCB (Marchant, Ramos, and Sanner 2014) for sampling to gain time efficiency. Performance Guarantee. The difficulty in theoretically guaranteeing the performance of our -GPP policy π (i.e., relative to that of GPP policy π * ) lies in analyzing how the values of the width parameter τ and deterministic sampling size n can be chosen to satisfy the user-specified loss bound , as discussed below. The first step is to prove that V t in (2) approximates V * t in (1) closely for some chosen τ and n values, which relies on the Lipschitz continuity of V * t in Corollary 1. Define Λ(n, τ ) to be equal to the value of 2/π if n ≥ 2∧τ = 0, and value of κ(τ )+η(n, τ ) if n &gt; 2∧τ &gt; 0 where κ(τ ) 2/π exp(−0.5τ 2 ) − 2τ Φ(−τ ), η(n, τ ) 2τ (0.5 − Φ(−τ ))/(n − 2), and Φ is a standard normal CDF.</p>
<p>Theorem 2 Suppose that λ &gt; 0 is given. For all d t and t = 0, . . . , H, if
λ ≥ Λ(n, τ )σ st+1|st ( 1 + L t+1 (s t+1 )) (3) for all s t+1 ∈ A(s t ), then |V t (d t ) − V * t (d t )| ≤ λ(H − t)
. Its proof uses Corollary 1 and is given in Appendix E. Remark 1. From Theorem 2, a tighter bound on the error |V t (d t ) − V * t (d t )| can be achieved by decreasing the sam-pling budget of H locations 4 and increasing the deterministic sampling size n; increasing n reduces η(n, τ ) and hence Λ(n, τ ), which allows λ to be reduced as well. The width parameter τ has a mixed effect on this error bound: Note that κ(τ ) (η(n, τ )) is proportional to some upper bound on the error incurred by the extreme sample measurements z 0 and z n−1 (z 1 , . . . , z n−2 ), as shown in Appendix E. Increasing τ reduces κ(τ ) but unfortunately raises η(n, τ ). So, in order to reduce Λ(n, τ ) further by increasing τ , it has to be complemented by raising n fast enough to keep η(n, τ ) from increasing. This allows λ to be reduced further as well.</p>
<p>Remark 2. A feasible choice of τ and n satisfying (3) can be expressed analytically in terms of the given λ and hence computed prior to planning, as shown in Appendix F. Remark 3. σ st+1|st and L t+1 (s t+1 ) for all s t+1 and t = 0, . . . , H − 1 can be computed prior to planning as they depend on s 0 and all reachable locations from s 0 but not on their measurements. Using Theorem 2, the next step is to bound the performance loss of our -GPP policy π relative to that of GPP policy π * , that is, policy π is -optimal:</p>
<p>Theorem 3 Given the user-specified loss bound &gt; 0,
V * 0 (d 0 ) − V π 0 (d 0 )
≤ by substituting λ = /(H(H + 1)) into the choice of τ and n stated in Remark 2 above.</p>
<p>Its proof is in Appendix G. It can be observed from Theorem 3 that a tighter bound on the error V * 0 (d 0 ) − V π 0 (d 0 ) can be achieved by decreasing the sampling budget of H locations 4 and increasing the deterministic sampling size n. The effect of width parameter τ on this error bound is the same as that on the error bound of |V t (d t ) − V * t (d t )|, as explained in Remark 1 above. Anytime -GPP. Unlike GPP policy π * , our -GPP policy π can be derived exactly since its incurred time is independent of the size of the uncountable set of candidate measurements. However, expanding the entire search tree of -GPP (2) incurs time containing a O(n H ) term and is not always necessary to achieve -optimality in practice. To mitigate this computational difficulty 5 , we propose an anytime variant of -GPP that can produce a good policy fast and improve its approximation quality over time, as briefly discussed here and detailed with the pseudocode in Appendix H.</p>
<p>The key intuition is to expand the sub-trees rooted at "promising" nodes with the highest weighted uncertainty of their corresponding values V * t (d t ) so as to improve their estimates. To represent such uncertainty at each encountered node, upper &amp; lower heuristic bounds (respectively, V * t (d t ) and V * t (d t )) are maintained, like in (Smith and Simmons 2006). A partial construction of the entire tree is maintained and expanded incrementally in each iteration of anytime -GPP that incurs linear time in n and comprises 3 steps: Node selection. Traverse down the partially constructed tree by repeatedly selecting nodes with largest difference between their upper and lower bounds (i.e., uncertainty) discounted by weight w i * of its preceding sample measurement z i * until an unexpanded node, denoted by d t , is reached. Expand tree. Construct a "minimal" sub-tree rooted at node d t by sampling all possible next locations and only their median sample measurements zī recursively up to full height H. Backpropagation. Backpropagate bounds from the leaves of the newly constructed sub-tree to node d t , during which the refined bounds of expanded nodes are used to inform the bounds of unexpanded siblings by exploiting the Lipschitz continuity of V * t (Corollary 1), as explained in Appendix H. Backpropagate bounds to the root of the partially constructed tree in a similar manner.</p>
<p>By using the lower heuristic bound to produce our anytime -GPP policy, its performance loss relative to that of GPP policy π * can be bounded, as proven in Appendix H.</p>
<p>Experiments and Discussion</p>
<p>This section empirically evaluates the online planning performance and time efficiency of our -GPP policy π and its anytime variant under limited sampling budget in an energy harvesting task on a simulated wind speed field and in BO on simulated plankton density (chl-a) field and real-world logpotassium (lg-K) concentration (mg l −1 ) field (Appendix I) of Broom's Barn farm (Webster and Oliver 2007). Each simulated (real-world lg-K) field is spatially distributed over a 0.95 km by 0.95 km (520 m by 440 m) region discretized into a 20 × 20 (14 × 12) grid of sampling locations. These fields are assumed to be realizations of GPs. The wind speed (chl-a) field is simulated using hyperparameters µ s = 0, 6 l 1 = l 2 = 0.2236 (0.2) km, σ 2 n = 10 −5 , and σ 2 y = 1. The hyperparameters µ s = 3.26, l 1 = 42.8 m, l 2 = 103.6 m, σ 2 n = 0.0222, and σ 2 y = 0.057 of lg-K field are learned using maximum likelihood estimation (Rasmussen and Williams 2006). The robot's initial starting location is near to the center of each simulated field and randomly selected for lg-K field. It can move to any of its 4 adjacent grid locations at each time step and is tasked to maximize its total rewards over 20 time steps (i.e., sampling budget of 20 locations).</p>
<p>In BO, the performances of our -GPP policy π and its anytime variant are compared with that of state-of-the-art nonmyopic UCB (Marchant, Ramos, and Sanner 2014) and greedy PI, EI, UCB (Brochu, Cora, and de Freitas 2010;Srinivas et al. 2010). Three performance metrics are used: (a) Total rewards achieved over the evolved time steps (i.e., higher total rewards imply less total regret in BO (Section 2)), (b) maximum reward achieved during experiment, and (c) search tree size in terms of no. of nodes (i.e., larger tree size implies higher incurred time). All experiments are run on a Linux machine with Intel Core i5 at 1.7 GHz. Energy Harvesting Task on Simulated Wind Speed Field. A robotic rover equipped with a wind turbine is tasked to harvest energy/power from the wind while exploring a polar region . It is driven by the logarithmic reward function described under 'General tasks/problems' in Section 2. Fig. 1 shows results of performances of our -GPP policy and its anytime variant averaged over 30 independent realizations of the wind speed field. It can be observed vs. no. of time steps for energy harvesting task. The plot of * = 5 uses our anytime variant with a maximum tree size of 5 × 10 4 nodes while the plot of = 250 effectively assumes maximum likelihood observations during planning like that of nonmyopic UCB (Marchant, Ramos, and Sanner 2014).</p>
<p>that the gradients of the achieved total rewards (i.e., power production) increase over time, which indicate a higher obtained reward with an increasing number of time steps as the robot can exploit the environment more effectively with the aid of exploration from previous time steps. The gradients eventually stop increasing when the robot enters a perceived high-reward region. Further exploration is deemed unnecessary as it is unlikely to find another preferable location within H time steps; so, the robot remains near-stationary for the remaining time steps. It can also be observed that the incurred time is much higher in the first few time steps. This is expected because the posterior variance σ st+1|st decreases with increasing time step t, thus requiring a decreasing deterministic sampling size n to satisfy (3).</p>
<p>Initially, all -GPP policies achieve similar total rewards as the robots begin from the same starting location. After some time, -GPP policies with lower user-specified loss bound and longer online planning horizon H achieve considerably higher total rewards at the cost of more incurred time. In particular, it can be observed that a robot assuming maximum likelihood observations during planning (i.e., = 250) like that of nonmyopic UCB or using a greedy policy (i.e., H = 1) performs poorly very quickly. In the former case (Fig. 1a), the gradient of its total rewards stops increasing quite early (i.e., from time step 9 onwards), which indicates that its perceived local maximum is reached prematurely. Interestingly, it can be observed from Fig. 1d that the -GPP policy with H = 2 and = 0.06 incurs more time than that with H = 3 and = 0.8 despite the latter achieving higher total rewards. This suggests trading off tighter loss bound for longer online planning horizon H , especially when is too small that in turn requires a very large n and consequently incurs significantly more time 5 . BO on Real-World Log-Potassium Concentration Field. An agricultural robot is tasked to find the peak lg-K measurement (i.e., possibly in an over-fertilized area) while exploring the Broom's Barn farm (Webster and Oliver 2007). It is driven by the UCB-based reward function described under 'BO' in Section 2. Fig. 2 shows results of performances of our -GPP policy and its anytime variant, nonmyopic UCB (i.e., = 25), and greedy PI, EI, UCB (i.e., H = 1) aver- (a) (b) (c) Figure 2: Graphs of total normalized 7 rewards of -GPP policies using UCB-based rewards with (a) H = 4, β = 0, and varying , (b) varying H = 1, 2, 3, 4 (respectively, = 0.002, 0.003, 0.4, 2) and β = 0, and (c) H = 4, = 1, and varying β vs. no. of time steps for BO on real-world lg-K field. The plot of * = 1 uses our anytime variant with a maximum tree size of 3×10 4 nodes while the plot of = 25 effectively assumes maximum likelihood observations during planning like that of nonmyopic UCB. aged over 25 randomly selected robot's initial starting location. It can be observed from Figs. 2a and 2b that the gradients of the achieved total normalized 7 rewards generally increase over time. In particular, from Fig. 2a, nonmyopic UCB assuming maximum likelihood observations during planning obtains much less total rewards than the other -GPP policies and the anytime variant after 20 time steps and finds a maximum lg-K measurement of 3.62 that is at least 0.4σ y worse after 20 time steps. The performance of the anytime variant is comparable to that of our best-performing -GPP policy with = 3. From Fig. 2b, the greedy policy (i.e., H = 1) with β = 0 performs much more poorly than its nonmyopic -GPP counterparts and finds a maximum lg-K measurement of 3.56 that is lower than that of greedy PI and EI due to its lack of exploration. By increasing H to 2-4, our -GPP policies with β = 0 outperform greedy PI and EI as they can naturally and jointly optimize the explorationexploitation trade-off. Interestingly, Fig. 2c shows that our -GPP policy with β = 2 achieves the highest total rewards after 20 time steps, which indicates the need of a slightly stronger exploration behavior than that with β = 0. This may be explained by a small length-scale (i.e., spatial correlation) of the lg-K field, thus requiring some exploration to find the peak measurement. By increasing H beyond 4 or with larger spatial correlation (Appendix J), we expect a diminishing role of the βσ st+1|st term. It can also be observed that aggressive exploration (i.e., β ≥ 10) hurts the performance. Results of the tree size (i.e., incurred time) of our -GPP policy and its anytime variant are in Appendix I.</p>
<p>Due to lack of space, we present additional experimental results for BO on simulated plankton density field in Appendix J that yield similar observations to the above.</p>
<p>Conclusion</p>
<p>This paper describes a novel nonmyopic adaptive -GPP framework endowed with a general class of Lipschitz continuous reward functions that can unify some AL and BO criteria and be used for defining new tasks/problems. In particular, it can jointly and naturally optimize the explorationexploitation trade-off. We theoretically guarantee the performances of our -GPP policy and its anytime variant and em-pirically demonstrate their effectiveness in BO and an energy harvesting task. For our future work, we plan to scale up -GPP and its anytime variant for big data using parallelization Low et al. 2015), online learning (Xu et al. 2014), and stochastic variational inference (Hoang, Hoang, and Low 2015) and extend them to handle unknown hyperparameters (Hoang et al. 2014).</p>
<p>A Proof of Lemma 1</p>
<p>Since h σ s t+1 |s t and g σ s t+1 |s t are Lipschitz continuous with the respective Lipschitz constants 1 and 2 (σ st+1|st ) (Section 2), E[R(Z t+1 , s t+1 )|d t , s t+1 ] is Lipschitz continuous in µ st+1|dt with Lipschitz constant 1 + 2 (σ st+1|st ). Then, by using the definition of GP posterior mean µ st+1|dt and Cauchy-Schwarz inequality, Lemma 1 results.</p>
<p>B Other Complex Reward Functions Represented by R</p>
<p>When sampling a natural phenomenon, locations featuring a specific band of field measurements may be preferred. Suppose that a robot is tasked to gather algal samples from an aquatic environment. It is well established that algal growth is faster in certain temperature range and survival is not possible at either extremes. The temperature field can be modeled as a GP. A rectangular reward function can be used to direct the robot to search among regions with the specified temperature range. The rectangular reward function can be viewed as a difference of two unit step functions, each of which is of the form: R 2 (z t+1 ) gives a value of 1 if z t+1 &gt; a, and 0 otherwise where a is some user-defined constant. For such a step function, g σ s t+1 |s t (u) = 1 − Φ((a − u)/σ st+1|st ) and 2 (σ st+1|st ) = 1/( √ 2πσ st+1|st ).</p>
<p>Another interesting reward function is the Gaussian distribution itself 8 which may be represented using R 2 (z t+1 ) = N (0, 1), g σ s t+1 |s t (u) = (2π(1 + σ 2 st+1|st )) −1/2 exp(−u 2 /(2(1 + σ 2 st+1|st ))) and 2 (σ st+1|st ) = ( √ 2π(1 + σ 2 st+1|st )) −1 exp(−1/2).</p>
<p>C Proof of Theorem 1</p>
<p>We will give a proof by induction on t that (4) holds for t + 1 (i.e., induction hypothesis), we will prove that it holds for 0 ≤ t &lt; H. Without loss of generality, assume that
|V * t (d t ) − V * t (d t )| ≤ L t (s t ) z t − z t .(4)When t = H, |V * H (d H ) − V * H (d H )| = 0 ≤ L H (s H ) z H − z H . SupposingV * t (d t ) ≥ V * t (d t ). Let s * t+1 π * (d t ) and ∆ t+1 µ s * t+1 |dt − µ s * t+1 |d t . Then, |∆ t+1 | ≤ α(s t ⊕ s * t+1 ) z t − z t . V * t (d t ) − V * t (d t ) ≤ Q * t (d t , s * t+1 ) − Q * t (d t , s * t+1 ) = E[R(Z t+1 , s t ⊕ s * t+1 )|d t , s * t+1 ] − E[R(Z t+1 , s t ⊕ s * t+1 )|d t , s * t+1 ] + ∞ −∞ p(z t+1 |d t , s * t+1 ) V * t+1 (d t+1 ) dz t+1 − ∞ −∞ p(z t+1 |d t , s * t+1 ) V * t+1 (d t+1 ) dz t+1 ≤ |∆ t+1 | 1 + 2 (σ s * t+1 |st ) + ∞ −∞ p(z t+1 |d t , s * t+1 )L t+1 (s t ⊕ s * t+1 ) (z t − z t ) ⊕ ∆ t+1 dz t+1 = |∆ t+1 | 1 + 2 (σ s * t+1 |st ) + L t+1 (s t ⊕ s * t+1 ) (z t − z t ) ⊕ ∆ t+1 ≤ α(s t ⊕ s * t+1 ) z t − z t 1 + 2 (σ s * t+1 |st ) + L t+1 (s t ⊕ s * t+1 ) 1 + α(s t ⊕ s * t+1 ) 2 z t − z t = α(s t ⊕ s * t+1 ) 1 + 2 (σ s * t+1 |st ) + L t+1 (s t ⊕ s * t+1 ) 1 + α(s t ⊕ s * t+1 ) 2 z t − z t ≤ L t (s t ) z t − z t .
The first equality is due to (1). The second inequality follows from Lemma 1, change of variable z t+1 z t+1 − ∆ t+1 , and the induction hypothesis. The last inequality is due to definition of L t (Definition 1).</p>
<p>D Illustration of Deterministic Sampling Strategy
p(z t+1 |d t , s t+1 ) −5σ −4σ −3σ −2σ 2σ 3σ 4σ 5σ −σ 0 σ z t+1 z 0 w 0 z 1 w 1 z 2 w 2 z 3 w 3 z 4 w 4 R 1 (z t+1 ) + V * t+1 ( s t+1 , z t ⊕ z t+1 ) ζ 1 ζ 2 ζ 3 ζ 0 ζ 4 τ σ z t+1
Figure 3: Example of a possible partition with µ st+1|dt = 0, τ = 3, and n = 5 where we let σ σ st+1|st for notational simplicity. At the bottom, the measurement space of p(z t+1 |d t , s t+1 ) is partitioned into 5 intervals ζ 0 , ζ 1 , ζ 2 , ζ 3 , and ζ 4 such that intervals ζ 1 , ζ 2 , and ζ 3 are equally spaced within the bounded gray region [−3σ, 3σ] while intervals ζ 0 and ζ 4 span the two infinitely long red tails. The 5 sample measurements z 0 , z 1 , z 2 , z 3 , and z 4 are then selected by setting z 0 as the upper limit of red interval ζ 0 , z 4 as the lower limit of red interval ζ 4 , and z 1 , z 2 , and z 3 as the centers of the respective gray intervals ζ 1 , ζ 2 , and ζ 3 . The weights w 0 , w 1 , w 2 , w 3 , and w 4 for the corresponding sample measurements z 0 , z 1 , z 2 , z 3 , and z 4 are defined as the areas under their respective intervals ζ 0 , ζ 1 , ζ 2 , ζ 3 , and ζ 4 of the Gaussian predictive distribution p(z t+1 |d t , s t+1 ). At the top, the brown curve (i.e., Lipschitz continuous) denotes the sum of R 1 (z t+1 ) and the expected future rewards V * t+1 ( s t+1 , z t ⊕ z t+1 ) achieved by the GPP policy. The latter is unknown in practice, as explained in Section 2. The blue curve shows the rectangular approximations using the sample measurements z 0 , z 1 , z 2 , z 3 , and z 4 . Note that, in reality, V * t+1 cannot be accurately evaluated -there will be additional errors accruing from future time steps that are not reflected in this figure but will be formally accounted for in the theoretical performance guarantee of our -GPP policy, as shown in the proof of Theorem 2 in Appendix E.</p>
<p>E Proof of Theorem 2</p>
<p>There are two sources of error arising in using V t in (2) to approximate V * t in (1): (a) In our deterministic sampling strategy (Section 3), only a finite number of sample measurements is selected for approximating the stage-wise expectation terms in (1), and (b) computing V t does not involve utilizing the values of V * t+1 but that of its approximation V t+1 instead. To facilitate capturing the error due to finite deterministic sampling described in (a), the following intermediate function is introduced:
U * t (d t , s t+1 ) g σ s t+1 |s t µ st+1|dt + R 3 (s t+1 ) + n−1 i=0 w i R 1 (z i ) + V * t+1 ( s t+1 , z t ⊕ z i )(5)
for stages t = 0, . . . , H − 1 where V * H (d H ) 0. The following lemma bounds the error between U * t and Q * t , which can then be used to bound the error between V t and V * t :
Lemma 2 For t = 0, . . . , H − 1, |U * t (d t , s t+1 ) − Q * t (d t , s t+1 )| ≤ λ.
Proof. When n &gt; 2, definez z n−1 − z 0 n − 2 . Then, ζ 0 = [−∞, z 0 ], ζ n−1 = [z n−1 , ∞], and ζ i = [z 0 + (i − 1)z, z 0 + iz] for i = 1, . . . , n − 2. For i = 0, . . . , n − 1,
w i = zt+1∈ζi p(z t+1 |d t , s t+1 ) dz t+1 .
Observe that the integration limits form a partition (at least in the sense of a Riemannian integral) of R, thus implying
n−1 i=0 w i = 1. |U * t (d t , s t+1 ) − Q * t (d t , s t+1 )| = n−1 i=0 ζi p(z t+1 |d t , s t+1 ) V * t+1 ( s t+1 , z t ⊕ z i ) + R 1 (z i ) − V * t+1 ( s t+1 , z t ⊕ z t+1 ) − R 1 (z t+1 ) dz t+1 ≤ n−1 i=0 ζi p(z t+1 |d t , s t+1 ) V * t+1 ( s t+1 , z t ⊕ z i ) + R 1 (z i ) − V * t+1 ( s t+1 , z t ⊕ z t+1 ) − R 1 (z t+1 ) dz t+1 ≤ ( 1 + L t+1 (s t+1 )) n−1 i=0 ζi p(z t+1 |d t , s t+1 ) z i − z t+1 dz t+1 .(6)
The last inequality follows from Corollary 1 and the fact that R 1 is Lipschitz continuous with Lipschitz constant 1 (Section 2). We now focus on the integral within the summation. When i = 0,
ζ0 p(z t+1 |d t , s t+1 ) z 0 − z t+1 dz t+1 = σ st+1|st √ 2π −τ −∞ exp − y 2 2 (−τ − y) dy = σ st+1|st 1 √ 2π exp − τ 2 2 − τ Φ(−τ ) = 1 2 σ st+1|st κ(τ ) .(7)
The first equality can be obtained by plugging in the density function of a Gaussian and normalizing. A similar result can be obtained when i = n − 1. When n &gt; 2,
n−2 i=1 ζi p(z t+1 |d t , s t+1 ) z i − z t+1 dz t+1 ≤z 2 n−2 i=1 ζi p(z t+1 |d t , s t+1 ) dz t+1 = τ σ st+1|st n − 2 n−2 i=1 ζi p(z t+1 |d t , s t+1 ) dz t+1 = 2τ σ st+1|st n − 2 1 2 − Φ(−τ ) = σ st+1|st η(n, τ ) .(8)
The inequality is obtained by observing that z i is the center of ζ i for i = 1, . . . , n−2 and that each interval ζ i for i = 1, . . . , n−2 is of equal lengthz. The first equality is obtained by combining the definitions ofz, z 0 , and z n−1 . Plugging (7) and (8) into (6) gives
|U * t (d t , s t+1 ) − Q * t (d t , s t+1 )| ≤ Λ(n, τ )σ st+1|st
( 1 + L t+1 (s t+1 )) A similar derivation shows that the result is also true when n = 2. Using (3), Lemma 2 results. Main Proof. We will give a proof by induction on t that
|V t (d t ) − V * t (d t )| ≤ λ(H − t) .(9)
The base case of t = H is trivially true. Supposing (9) holds for t + 1 (i.e., induction hypothesis), we will prove that it holds for 0 ≤ t &lt; H. For all s t+1 ∈ A(s t ),
|Q t (d t , s t+1 ) − Q * t (d t , s t+1 )| ≤ |Q t (d t , s t+1 )−U * t (d t , s t+1 )| + |U * t (d t , s t+1 )−Q * t (d t , s t+1 )| ≤λ ≤ λ + n−1 i=0 w i V t+1 (s t+1 , z t ⊕ z i ) − V * t+1 (s t+1 , z t ⊕ z i ) ≤ λ(H − t) .(10)
The second inequality follows from Lemma 2 and the definitions of Q t (2) and U * t . The last inequality follows from the induction hypothesis and the fact that
n−1 i=0 w i = 1. Then, |V t − V * t (d t )| ≤ max st+1∈A(st) Q t (d t , s t+1 ) − max s t+1 ∈A(st) Q * t (d t , s t+1 ) ≤ max st+1∈A(st) |Q t (d t , s t+1 ) − Q * t (d t , s t+1 )| ≤ λ(H − t) .
The final inequality is due to (10).</p>
<p>F Analytically Computing a Feasible Choice of τ and n
Set n = 2 + τ π 2 exp τ 2 2 .(11)
Then, Λ(n, τ ) = κ(τ ) + η(n, τ )
≤ 2 π exp − τ 2 2 + τ n − 2 = 2 2 π exp − τ 2 2 .
Using (3), the feasible values of τ satisfy
2 2 π exp − τ 2 2 ≤ λ σ st+1|st ( 1 + L t+1 (s t+1 ))
.</p>
<p>So, a feasible choice of τ can be obtained from
2 2 π exp − τ 2 2 = λ σ st+1|st ( 1 + L t+1 (s t+1 )) , which gives τ = −2 log π 2 λ 2σ st+1|st ( 1 + L t+1 (s t+1 ))
.</p>
<p>By substituting this value of τ into (11), the value of n can be obtained. If n is not an integer, then take the ceiling of n.</p>
<p>If the deterministic sampling size n is to be constrained (i.e., less than some user-defined constant n) to preserve the time efficiency of solving for the -GPP policy, then a numerical alternative is available: For all n &lt; n, min τ Λ(n, τ ) can be computed numerically offline. Choosing the values of τ and n to satisfy (3) simply involves finding n * &lt; n such that min τ Λ(n * , τ ) ≤ λ/(σ st+1|st ( 1 + L t+1 (s t+1 ))).  </p>
<p>J Bayesian Optimization (BO) on Simulated Plankton Density Field</p>
<p>A robotic boat equipped with a fluorometer is tasked to find the peak chl-a measurement (i.e., possibly in an algal bloom) while exploring a lake ). It is driven by the UCB-based reward function described under 'Bayesian optimization' in Section 2. Fig. 6 shows results of performances of our -GPP policy and its anytime variant, nonmyopic UCB (i.e., = 250), and greedy PI, EI, UCB (i.e., H = 1) averaged over 30 independent realizations of the chl-a field. Similar to that of the energy harvesting task, the gradients of their achieved total rewards increase over time. In particular, it can be observed from Fig. 6a that nonmyopic UCB assuming maximum likelihood observations during planning obtains much less total rewards than the other -GPP policies and the anytime variant and finds a maximum chl-a measurement of 1.25 that is at least 0.26σ y worse after 20 time steps. The anytime variant performs reasonably well, albeit slightly worse than our -GPP policy with = 25.0. From   Fig. 6b, the greedy policy (i.e., H = 1) performs much more poorly than its nonmyopic counterparts and finds a maximum chl-a measurement of 1.28 that is at least 0.22σ y worse after 20 time steps. Such a greedy policy with β = 0 is also worse than greedy PI and EI due to its lack of exploration. However, by increasing H to 3 or 4, our -GPP policies with β = 0 outperform greedy PI and EI as they can naturally and jointly optimize the exploration-exploitation trade-off. To see this, Figs. 6c to 6f reveal that the weighted exploration term βσ st+1|st in the UCB-based reward function becomes redundant when H ≥ 3; in fact, such nonmyopic -GPP policies with β = 0 achieve the highest total rewards among all -GPP policies with H = 1, 2, 3, 4 and β = 0, 1, 10, 20 and their performances degrade with stronger exploration behavior (i.e., β &gt; 0). Results of the tree size (i.e., incurred time) of our -GPP policy and its anytime variant are similar to that of the energy harvesting task and shown in Fig. 7. varying H = 1, 2, 3, 4 (respectively, = 0.002, 0.015, 0.4, 5.0) and β = 0 for BO on simulated chl-a field. The plot of * = 5 utilizes our anytime variant with a maximum tree size of 7.5 × 10 4 nodes while the plot of = 250 effectively implements the nonmyopic UCB (Marchant, Ramos, and Sanner 2014) assuming maximum likelihood observations during planning.</p>
<p>Figure 1
1Figure 1: Graphs of total rewards and tree size of -GPP policies with (a-b) online planning horizon H = 4 and varying and (c-d) varying H = 1, 2, 3, 4 (respectively, = 0.002, 0.06, 0.8, 5)</p>
<p>Figure 4 :
4Graphs of tree size of -GPP policies using UCB-based rewards with (a) H = 4, β = 0, and varying and (b) varying H = 1, 2, 3, 4 (respectively, = 0.002, 0.003, 0.4, 2) and β = 0 vs. no. of time steps for BO on real-world lg-K field. The plot of * = 1 utilizes our anytime variant with a maximum tree size of 3 × 10 4 nodes while the plot of = 25 effectively implements the nonmyopic UCB(Marchant, Ramos, and Sanner 2014) assuming maximum likelihood observations during planning.</p>
<p>Figure 5 :
5Real-world log-potassium (lg-K) concentration (mg l −1 ) field of Broom's Barn farm.</p>
<p>Figure 6 :Figure 7 :
67Graphs of total rewards of -GPP policies using UCB-based rewards with (a) H = 4, β = 0, and varying , (b) varying H = 1, 2, 3, 4 (respectively, = 0.002, 0.015, 0.4, 5) and β = 0, (c) H = 1, = 0.002, (d) H = 2, = 0.015, (e) H = 3, = 0.4, and (f) H = 4, = 5, and varying β vs. no. of time steps for BO on simulated chl-a field. The plot of * = 5 utilizes our anytime variant with a maximum tree size of 7.5 × 10 4 nodes while the plot of = 250 effectively implements the nonmyopic UCB (Marchant, Ramos, and Sanner 2014) assuming maximum likelihood observations during planning. Graphs of tree size of -GPP policies using UCB-based rewards with (a) H = 4, β = 0, and varying and (b)
Bayes-optimality has been studied in discrete BRL(Poupart et al. 2006) whose assumptions (Section 1) do not hold in GPP. Continuous BRLs(Dallaire et al. 2009;Ross, Chaib-draa, and Pineau 2008) assume a known parametric form of observation function,
In reality, the speed-power relationship is not exactly logarithmic, but this approximation suffices for the purpose of modeling. p(z t+1 |d t , s t+1 ) z 0 µ st+1|dt −τ σ st+1|st ; z n−1 µ st+1|dt +τ σ st+1|st ; z i z 0 + i−0.5 n−2 (z n−1 −z 0 ) for i = 1, . . . , n − 2.
This changes -GPP by reducing its planning horizon though.5  The value of n is a bigger computational issue than that of H when is small and in online planning.
Its actual prior mean is not zero; we have applied zero-mean GP to Ys − µs for simplicity.
To ease interpretation of the results, each reward is normalized by subtracting the prior mean from it.
Convolution between two Gaussian distributions yields another Gaussian which is Lipschitz continuous. An alternative formulation is to set R1(zt+1) = N (0, 1) since the reward function is itself Lipschitz continuous.
Acknowledgments. This work was supported by Singapore-MIT Alliance for Research and Technology Subaward Agreement No. 52 R-252-000-550-592.G Proof of Theorem 3We will give a proof by induction on t that(12) The base case of t = H is trivially true. Supposing (12) holds for t + 1 (i.e., induction hypothesis), we will prove that it holds for 0 ≤ t &lt; H. For all s t+1 ∈ A(s t ),The first inequality follows from definitions of Q * t (d t , s t+1 ) (1) and Q π t (d t , s t+1 ). The last inequality follows from the induction hypothesis.The second inequality requires Theorem 2. The last inequality is due to(10)and(13). By substituting t = 0 and λ = /(H(H + 1)), Theorem 3 results.H Anytime -GPPAlgorithm 1 gives the pseudocode of our anytime -GPP. We discuss a few important issues:• The CONSTRUCTTREE function needs to ensure that the chosen branch is not already fully expanded; we have omitted this for brevity. This implies that the "rank" of the tree should be maintained while performing expansions -this allows us to quickly determine if a node is already fully expanded. • The result below proves that V * t (d t ) and V * t (d t ) are upper and lower heuristic bounds for V * t (d t ), respectively: Theorem 4 For t = 0, . . . , H,Proof. We will give a proof by induction on t. The base case of t = H is trivially true. Supposing (14) holds for t + 1 (i.e., induction hypothesis), we will prove that it holds for 0 ≤ t &lt; H. For all s t+1 ∈ A(s t ),The first inequality is due to Lemma 2 while the second inequality follows from the definition of U * t in (5) and induction hypothesis. The equality is by definition of Q * t (d t , s t+1 ) in lines 11 and 30 of Algorithm 1. It follows thatThe first inequality is due to Lemma 2 while the second inequality follows from the definition of U * t in (5) and induction hypothesis. The equality is by definition of Q * t (d t , s t+1 ) in lines 12 and 31 of Algorithm 1. It follows thatAlgorithm 1: Anytime -GPP• The result below exploits the Lipschitz continuity of V * t (Corollary 1) for producing informed upper and lower heuristic bounds in lines 20 and 21 of Algorithm 1, respectively: Theorem 5 For t = 0, . . . , H and i, k = 0, . . . , n − 1,Proof.The first inequality is due to Corollary 1 while the second inequality follows from Theorem 4. Similarly,The first inequality is due to Corollary 1 while the second inequality follows from Theorem 4. • Finally, the performance loss of our anytime -GPP policy relative to that of GPP policy π * can be bounded:Theorem 6 Suppose that the user-specified loss bound &gt; 0 is given, our anytime -GPP is terminated at α V *, and our anytime -GPP policy is defined as π≤ αH by setting and substituting λ = /(H(H + 1)) into the choice of τ and n stated in Remark 2 in Section 3. Proof. It follows thatIn general, given that the length of planning horizon is reduced to H − t for t = 0, . . . , H − 1, this inequality is equivalent toby increasing/shifting the indices of V * 0 (d 0 ) and V * 0 above from 0 to t so that these value functions start at stage t instead. We will give a proof by induction on t thatThe base case of t = H is trivially true. Supposing (17) holds for t + 1 (i.e., induction hypothesis), we will prove that it holds for 0 ≤ t &lt; H. For all s t+1 ∈ A(s t ),The first inequality follows from definitions of Q * t (d t , s t+1 ) (1) and Q π α t (d t , s t+1 ). The last inequality follows from the induction hypothesis.The first inequality is due to (16). The second inequality follows from(15)and(18). By substituting t = 0, Theorem 6 results.
A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. E Brochu, V M Cora, N Freitas, arXiv:1012.2599Brochu, E.; Cora, V. M.; and de Freitas, N. 2010. A tuto- rial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. arXiv:1012.2599.</p>
<p>Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms. N Cao, K H Low, J M Dolan, Proc. AAMAS. AAMASCao, N.; Low, K. H.; and Dolan, J. M. 2013. Multi-robot in- formative path planning for active sensing of environmental phenomena: A tale of two algorithms. In Proc. AAMAS.</p>
<p>Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena. J Chen, K H Low, C K Tan, .-Y Oran, A Jaillet, P Dolan, J M Sukhatme, G S , Proc. UAI. UAIChen, J.; Low, K. H.; Tan, C. K.-Y.; Oran, A.; Jaillet, P.; Dolan, J. M.; and Sukhatme, G. S. 2012. Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena. In Proc. UAI, 163-173.</p>
<p>Parallel Gaussian process regression with low-rank covariance matrix approximations. J Chen, N Cao, K H Low, R Ouyang, C K Tan, .-Y Jaillet, P , Proc. UAI. UAIChen, J.; Cao, N.; Low, K. H.; Ouyang, R.; Tan, C. K.-Y.; and Jaillet, P. 2013. Parallel Gaussian process regression with low-rank covariance matrix approximations. In Proc. UAI, 152-161.</p>
<p>Design and power management of a wind-solar-powered polar rover. J Chen, J Liang, T Wang, T Zhang, Y Wu, Journal of Ocean and Wind Energy. 12Chen, J.; Liang, J.; Wang, T.; Zhang, T.; and Wu, Y. 2014. Design and power management of a wind-solar-powered po- lar rover. Journal of Ocean and Wind Energy 1(2):65-73.</p>
<p>Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobility-ondemand systems. J Chen, K H Low, P Jaillet, Y Yao, IEEE Trans. Autom. Sci. Eng. 12Chen, J.; Low, K. H.; Jaillet, P.; and Yao, Y. 2015. Gaussian process decentralized data fusion and active sensing for spa- tiotemporal traffic modeling and prediction in mobility-on- demand systems. IEEE Trans. Autom. Sci. Eng. 12:901-921.</p>
<p>Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system. J Chen, K H Low, C K Tan, .-Y , Proc. RSS. RSSChen, J.; Low, K. H.; and Tan, C. K.-Y. 2013. Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system. In Proc. RSS.</p>
<p>Bayesian reinforcement learning in continuous POMDPs with Gaussian processes. P Dallaire, C Besse, S Ross, B Chaib-Draa, Proc. IEEE/RSJ IROS. IEEE/RSJ IROSDallaire, P.; Besse, C.; Ross, S.; and Chaib-draa, B. 2009. Bayesian reinforcement learning in continuous POMDPs with Gaussian processes. In Proc. IEEE/RSJ IROS, 2604- 2609.</p>
<p>Gaussian processes for data-efficient learning in robotics and control. M P Deisenroth, D Fox, C E Rasmussen, IEEE Transactions on Pattern Analysis and Machine Intelligence. 372Deisenroth, M. P.; Fox, D.; and Rasmussen, C. E. 2015. Gaussian processes for data-efficient learning in robotics and control. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence 37(2):408-423.</p>
<p>Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet. J M Dolan, G Podnar, S Stancliff, K H Low, A Elfes, J Higinbotham, J C Hosler, T A Moisan, J Moisan, Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water Regions. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water Regions7473Dolan, J. M.; Podnar, G.; Stancliff, S.; Low, K. H.; Elfes, A.; Higinbotham, J.; Hosler, J. C.; Moisan, T. A.; and Moisan, J. 2009. Cooperative aquatic sensing using the telesuper- vised adaptive ocean sensor fleet. In Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water Regions, volume 7473.</p>
<p>Entropy search for information-efficient global optimization. P Hennig, C J Schuler, JMLR. 13Hennig, P., and Schuler, C. J. 2012. Entropy search for information-efficient global optimization. JMLR 13:1809- 1837.</p>
<p>Predictive entropy search for efficient global optimization of black-box functions. J M Hernández-Lobato, M W Hoffman, Z Ghahramani, Proc. NIPS. NIPSHernández-Lobato, J. M.; Hoffman, M. W.; and Ghahra- mani, Z. 2014. Predictive entropy search for efficient global optimization of black-box functions. In Proc. NIPS.</p>
<p>Nonmyopic -Bayes-optimal active learning of Gaussian processes. T N Hoang, K H Low, P Jaillet, M Kankanhalli, Proc. ICML. ICMLHoang, T. N.; Low, K. H.; Jaillet, P.; and Kankanhalli, M. 2014. Nonmyopic -Bayes-optimal active learning of Gaus- sian processes. In Proc. ICML, 739-747.</p>
<p>A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data. T N Hoang, Q M Hoang, K H Low, Proc. ICML. ICMLHoang, T. N.; Hoang, Q. M.; and Low, K. H. 2015. A unifying framework of anytime sparse Gaussian process re- gression models with stochastic variational inference for big data. In Proc. ICML, 569-578.</p>
<p>Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies. A Krause, A Singh, C Guestrin, JMLR. 9Krause, A.; Singh, A.; and Guestrin, C. 2008. Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies. JMLR 9:235-284.</p>
<p>Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing. N E Leonard, D A Palley, F Lekien, R Sepulchre, D M Fratantoni, R E Davis, K H Low, J Chen, J M Dolan, S Chien, D R Thompson, Proc. AAMAS. AAMAS95Collective motion, sensor networks, and ocean samplingLeonard, N. E.; Palley, D. A.; Lekien, F.; Sepulchre, R.; Fratantoni, D. M.; and Davis, R. E. 2007. Collective motion, sensor networks, and ocean sampling. Proc. IEEE 95:48-74. Low, K. H.; Chen, J.; Dolan, J. M.; Chien, S.; and Thomp- son, D. R. 2012. Decentralized active robotic exploration and mapping for probabilistic field classification in environ- mental sensing. In Proc. AAMAS, 105-112.</p>
<p>Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation. K H Low, J Yu, J Chen, P ; K H Jaillet, J M Dolan, P Khosla, Proc. AAAI. Low. AAAI. LowProc. AAMASLow, K. H.; Yu, J.; Chen, J.; and Jaillet, P. 2015. Parallel Gaussian process regression for big data: Low-rank repre- sentation meets Markov approximation. In Proc. AAAI. Low, K. H.; Dolan, J. M.; and Khosla, P. 2008. Adaptive multi-robot wide-area exploration and mapping. In Proc. AAMAS, 23-30.</p>
<p>Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing. K H Low, J M Dolan, P ; K H Khosla, J M Dolan, P Khosla, Proc. ICAPS. Low. ICAPS. LowProc. AAMASLow, K. H.; Dolan, J. M.; and Khosla, P. 2009. Information- theoretic approach to efficient adaptive path planning for mobile robotic environmental sensing. In Proc. ICAPS. Low, K. H.; Dolan, J. M.; and Khosla, P. 2011. Active Markov information-theoretic path planning for robotic en- vironmental sensing. In Proc. AAMAS, 753-760.</p>
<p>Sequential Bayesian optimisation for spatial-temporal monitoring. R Marchant, F Ramos, S Sanner, Proc. UAI. UAIMarchant, R.; Ramos, F.; and Sanner, S. 2014. Sequential Bayesian optimisation for spatial-temporal monitoring. In Proc. UAI.</p>
<p>Gaussian processes for global optimization. M A Osborne, R Garnett, S J Roberts, Proc. 3rd International Conference on Learning and Intelligent Optimization. 3rd International Conference on Learning and Intelligent OptimizationOsborne, M. A.; Garnett, R.; and Roberts, S. J. 2009. Gaus- sian processes for global optimization. In Proc. 3rd Interna- tional Conference on Learning and Intelligent Optimization.</p>
<p>Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena. R Ouyang, K H Low, J Chen, P Jaillet, Proc. AAMAS. AAMASOuyang, R.; Low, K. H.; Chen, J.; and Jaillet, P. 2014. Multi- robot active sensing of non-stationary Gaussian process- based environmental phenomena. In Proc. AAMAS.</p>
<p>An analytic solution to discrete Bayesian reinforcement learning. P Poupart, N Vlassis, J Hoey, K Regan, Proc. ICML. ICMLPoupart, P.; Vlassis, N.; Hoey, J.; and Regan, K. 2006. An analytic solution to discrete Bayesian reinforcement learn- ing. In Proc. ICML, 697-704.</p>
<p>Gaussian Processes for Machine Learning. C E Rasmussen, C K I Williams, MIT PressRasmussen, C. E., and Williams, C. K. I. 2006. Gaussian Processes for Machine Learning. MIT Press.</p>
<p>Bayesian reinforcement learning in continuous POMDPs with application to robot navigation. S Ross, B Chaib-Draa, J Pineau, Proc. IEEE ICRA. IEEE ICRARoss, S.; Chaib-draa, B.; and Pineau, J. 2008. Bayesian rein- forcement learning in continuous POMDPs with application to robot navigation. In Proc. IEEE ICRA, 2845-2851.</p>
<p>Maximum entropy sampling. M C Shewry, H P Wynn, J. Applied Statistics. 142Shewry, M. C., and Wynn, H. P. 1987. Maximum entropy sampling. J. Applied Statistics 14(2):165-170.</p>
<p>Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic. T Smith, R Simmons, Proc. AAAI. AAAISmith, T., and Simmons, R. 2006. Focused real-time dy- namic programming for MDPs: Squeezing more out of a heuristic. In Proc. AAAI, 1227-1232.</p>
<p>Gaussian process optimization in the bandit setting: No regret and experimental design. N Srinivas, A Krause, S Kakade, M Seeger, Proc. ICML. ICMLSrinivas, N.; Krause, A.; Kakade, S.; and Seeger, M. 2010. Gaussian process optimization in the bandit setting: No re- gret and experimental design. In Proc. ICML, 1015-1022.</p>
<p>Geostatistics for Environmental Scientists. R Webster, M Oliver, John Wiley &amp; Sons, IncNY2nd editionWebster, R., and Oliver, M. 2007. Geostatistics for Environ- mental Scientists. NY: John Wiley &amp; Sons, Inc., 2nd edition.</p>
<p>. N Xu, K H Low, J Chen, K K Lim, E Ozgul, Xu, N.; Low, K. H.; Chen, J.; Lim, K. K.; and Ozgul, E. B.</p>
<p>Persistent mobile robot localization using online sparse Gaussian process observation model. Gp-Localize, Proc. AAAI. AAAIGP-Localize: Persistent mobile robot localization us- ing online sparse Gaussian process observation model. In Proc. AAAI, 2585-2592.</p>
<p>Near-optimal active learning of multi-output Gaussian processes. Y Zhang, T N Hoang, K H Low, M Kankanhalli, Proc. AAAI. AAAIZhang, Y.; Hoang, T. N.; Low, K. H.; and Kankanhalli, M. 2016. Near-optimal active learning of multi-output Gaussian processes. In Proc. AAAI.</p>            </div>
        </div>

    </div>
</body>
</html>
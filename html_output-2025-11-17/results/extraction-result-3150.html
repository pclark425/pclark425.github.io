<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3150 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3150</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3150</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-5fefc28ae503c465b1801da2b457f5a2cb5bd51f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5fefc28ae503c465b1801da2b457f5a2cb5bd51f" target="_blank">PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> PASTA is introduced for table-based fact verification via pre-training with synthesized sentence–table cloze questions and achieves new state-of-the-art (SOTA) performance on two table- based fact verification datasets TabFact and SEM-TAB- FACTS.</p>
                <p><strong>Paper Abstract:</strong> Fact verification has attracted a lot of attention recently, e.g., in journalism, marketing, and policymaking, as misinformation and dis- information can sway one’s opinion and affect one’s actions. While fact-checking is a hard task in general, in many cases, false statements can be easily debunked based on analytics over tables with reliable information. Hence, table- based fact verification has recently emerged as an important and growing research area. Yet, progress has been limited due to the lack of datasets that can be used to pre-train language models (LMs) to be aware of common table operations, such as aggregating a column or comparing tuples. To bridge this gap, this paper introduces PASTA for table-based fact verification via pre-training with synthesized sentence–table cloze questions. In particular, we design six types of common sentence–table cloze tasks, including Filter, Aggregation, Superlative, Comparative, Ordinal, and Unique, based on which we synthesize a large corpus consisting of 1.2 million sentence–table pairs from WikiTables. PASTA uses a recent pre-trained LM, DeBERTaV3, and further pre- trains it on our corpus. Our experimental results show that PASTA achieves new state-of-the-art (SOTA) performance on two table-based fact verification datasets TabFact and SEM-TAB- FACTS. In particular, on the complex set of TabFact, which contains multiple operations, PASTA largely outperforms previous SOTA by 4.7% (85.6% vs. 80.9%), and the gap between PASTA and human performance on the small test set is narrowed to just 1.5% (90.6% vs. 92.1%).</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3150.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3150.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PASTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-training and fine-tuning framework that teaches a pre-trained LM (DeBERTaV3) to perform table-aware operations by synthesizing sentence-table cloze tasks (Filter, Aggregation, Superlative, Comparative, Ordinal, Unique) and masking operation-aware spans for reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTaV3 (fine-tuned with PASTA pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeBERTaV3Large transformer (disentangled attention, relative-position encodings) initialized from public DeBERTaV3 checkpoint and further pre-trained with 1.2M sentence-table cloze examples, then fine-tuned on TabFact/SEM-TAB-FACTS.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Aggregation (sum, average), Counting (COUNT DISTINCT), Comparative numeric comparisons, Ordinal (rank-based), Filter retrieving numeric cell values</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Supervised learning to predict operation-aware tokens (numeric results or comparative words) from joint sentence + linearized table input; uses SQL templates executed on the table to synthesize correct numeric targets, relying on DeBERTaV3's disentangled attention/positional encodings to align sentence tokens and table cells.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) Operation-aware cloze accuracy rises during pre-training (after 400K steps >60% overall). 2) PASTA significantly outperforms the same LM without operation-aware pretraining on operation-specific test sets (e.g., Aggregation: 84.5% vs 81.0%). 3) Ablation vs MLM-style masking and vs no row-ranking show gains attributable to the operation-aware pretraining and select-then-rank preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>1) Cloze accuracy plateaus at about ~60% after 400K steps (not near-perfect). 2) Error analysis shows failures on large tables and on statements with multiple operations; aggregation is harder and learned later, indicating limited generalization to complex chained arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Operation-aware pre-training (sentence-table cloze masking of operation-aware spans), select-then-rank fine-tuning (column selection + row-wise ranking), probing-based sentence polishing for template fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Pre-training with operation-aware cloze improved downstream fact verification accuracy (TabFact test 89.3% vs DeBERTaV3 86.2%), larger gains on complex/multi-row arithmetic-heavy cases (complex set 85.6% vs 82.9%), and improved aggregation-specific verification (example: Aggregation accuracy +3.5 points). Select-then-rank (row ranking) further increased effectiveness versus no ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PASTA on TabFact: Test accuracy 89.3% (overall), Complex set 85.6%; DeBERTaV3 baseline 86.2% overall, 82.9% complex. Aggregation-specific test (sampled 200): PASTA 84.5% vs DeBERTaV3 81.0%. Pre-training cloze tasks: >60% accuracy across operation types after 400K steps. MLM comparison (140K steps pretraining): PASTA (pretrain) -> Test 87.9% vs MLM-pretrained -> Test 84.9%; Complex: 84.9% vs 81.2%. SEM-TAB-FACTS: PASTA 84.10% vs DeBERTaV3 78.92%. Small TabFact human baseline: human 92.1% vs PASTA small test 90.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails more on large tables (error-set avg #cells higher than dataset average) and on statements containing multiple operations; aggregation and long-span answers (Filter-type) are harder; synthetic-template diversity limits generalization to unseen phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>PASTA narrows gap to human on small TabFact test (90.6% vs 92.1%). The paper uses SQL execution only to synthesize supervision (not as a runtime symbolic calculator); program-driven semantic-parsing methods are discussed as alternative symbolic approaches but have issues (spurious programs) according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3150.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3150.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTaV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTaV3 (Decoding-enhanced BERT with disentangled attention, v3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-like transformer that uses disentangled attention with separate content and relative-position projections and additional pretraining (replaced token detection) to improve encoding of positional/structural signals; used here as the base encoder for sentence-table joint inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTaV3Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-scale transformer with disentangled content/position attention and RTD-style pretraining; used in this paper starting from public DeBERTaV3Large checkpoint and further pre-trained with PASTA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used as substrate to learn Aggregation (sum/avg), Counting, Comparative numeric reasoning via fine-tuning; not an arithmetic-specialized architecture per se.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Disentangled attention and relative-position encodings enable improved sentence-table alignment and help the model attend to relevant cells for numeric reasoning; the model learns to map table cell values through supervised cloze targets rather than explicit algorithmic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>DeBERTaV3 baseline already yields strong performance on table tasks (Test 86.2% on TabFact), indicating its positional encoding helps table understanding; PASTA's improvements build on this architecture, and ablations show importance of row-wise ordering given positional sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite architectural strengths, DeBERTaV3 without operation-aware pretraining underperforms PASTA on aggregation/complex numeric tasks, indicating architecture alone does not provide full arithmetic capability.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Further pre-training (PASTA), select-then-rank preprocessing, operation-aware masking during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>When further pre-trained with PASTA tasks, DeBERTaV3's arithmetic-related verification accuracy improves (e.g., complex TabFact +2.7 to +3.9 points depending on split).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DeBERTaV3 baseline on TabFact: Test 86.2% overall, Simple 92.8%, Complex 82.9%. On SEM-TAB-FACTS: 78.92% test.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>As above: struggles on large tables and multi-operation statements even with the architecture's positional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>DeBERTaV3 is a learned approximate reasoner and falls short of human-level on small TabFact set; symbolic SQL execution approaches are discussed in related work as an alternate route for exact arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3150.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3150.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Language Modeling (random masking) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The standard MLM objective (randomly mask ~15% tokens) applied to sentence+table joint inputs as a pretraining baseline, contrasted with PASTA's operation-aware masking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTaV3 + MLM-style pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same DeBERTaV3Large backbone; pre-trained by randomly masking tokens in sentence-table pairs (15% masking), then fine-tuned on TabFact for comparison with PASTA masking.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same downstream arithmetic-type verification tasks (Aggregation, Comparative, etc.) used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>MLM relies on contextual language prediction which does not target operation-aware numeric reasoning; it may capture local lexical patterns but not explicit table-based numerical operations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>MLM gave modest improvements over vanilla DeBERTaV3 in some settings but underperformed PASTA on complex/aggregation heavy cases (Test MLM 84.9% vs PASTA-pretrained 87.9% in the controlled 140K-step comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Random masking does not target operation spans and thus is less effective for learning table-based arithmetic; authors note random masking may not predict cell contents and is not focused on operation tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Pretraining scheme (random-span MLM vs targeted operation-aware masking).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>MLM pretraining provided smaller gains than PASTA; in the 140K-step experiment MLM -> Test 84.9% vs PASTA 87.9%, Complex 81.2% vs PASTA 84.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MLM (140K-step pretrain) on TabFact: Val 84.8%, Test 84.9%, Simple 92.5%, Complex 81.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Underperforms on aggregation/complex numerical statements compared to operation-aware pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>MLM-based pretraining is an unsupervised language modeling strategy and does not approach human-level arithmetic reasoning on complex table tasks in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3150.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3150.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPEX: Table Pre-training via Learning a Neural SQL Executor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that pre-trains a seq2seq model (BART-based) to mimic execution of SQL-like queries over tables to improve table question answering and numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TAPEX: table pre-training via learning a neural SQL executor</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TAPEX (BART-based neural SQL executor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained BART-like model trained to emulate a SQL executor using synthesized SQL queries and execution results on tables (execution-centric pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>SQL-executable numerical operations (SUM, AVG, COUNT, ORDER BY, etc.) and other table-based numeric calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learns to map natural language / SQL-like programs to numeric execution results via supervised imitation of SQL execution, effectively internalizing arithmetic transformations through sequence generation and execution supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited as an execution-centric pretraining approach that helps numeric reasoning by training the model to produce results of SQL queries; referenced as a related technique to PASTA's SQL-based synthesis of targets.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not evaluated in this paper; authors contrast program-driven or execution-mimicking approaches with PASTA's cloze masking, noting program methods can suffer from issues like spurious programs under weak supervision (discussion in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Execution-centric pretraining (neural SQL executor supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>As discussed in related work, program-driven approaches can be affected by spurious programs under weak supervision; exact failure modes not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>TAPEX is closer to symbolic execution by mimicking SQL execution; PASTA instead uses SQL only to synthesize supervision and relies on the LM to predict operation tokens rather than execute programs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3150.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3150.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FORTAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FORTAP: using formulae for numerical-reasoning-aware table pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining approach (cited) that leverages spreadsheet-like formulas to train LMs for numerical reasoning, including objectives like Numerical Reference Prediction (NRP) and Numerical Calculation Prediction (NCP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FORTAP: using formulae for numerical-reasoning-aware table pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FORTAP (pretraining objectives NRP/NCP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretraining framework that incorporates numerical reasoning objectives derived from formulae and spreadsheet data to improve models' numeric capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numerical calculation prediction and numerical reference resolution (spreadsheet formula style operations, aggregation, arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Uses explicit numerical-centric pretraining tasks (NRP, NCP) to force the model to learn numeric relationships and calculation patterns from formula-like supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited as related work that focuses on numerical characteristics of tables (authors position PASTA as complementary by focusing on operation-aware cloze tasks), but no primary results reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct evidence in this paper; FORTAP is referenced as an alternative pretraining focusing on numeric formulae rather than cloze masking.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Numerical-specific pretraining objectives (NRP, NCP) derived from spreadsheet formula data.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>FORTAP is an approach that more explicitly targets numeric/formulaic patterns, closer to symbolic numeric manipulation than generic MLM, but this paper does not provide direct head-to-head comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TAPEX: table pre-training via learning a neural SQL executor <em>(Rating: 2)</em></li>
                <li>FORTAP: using formulae for numerical-reasoning-aware table pretraining <em>(Rating: 2)</em></li>
                <li>Tapas: Weakly supervised table parsing via pre-training <em>(Rating: 2)</em></li>
                <li>LOGICALFACTCHECKER: Leveraging logical operations for fact checking with graph module network <em>(Rating: 1)</em></li>
                <li>Table-BERT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3150",
    "paper_id": "paper-5fefc28ae503c465b1801da2b457f5a2cb5bd51f",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "PASTA",
            "name_full": "PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training",
            "brief_description": "A pre-training and fine-tuning framework that teaches a pre-trained LM (DeBERTaV3) to perform table-aware operations by synthesizing sentence-table cloze tasks (Filter, Aggregation, Superlative, Comparative, Ordinal, Unique) and masking operation-aware spans for reconstruction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeBERTaV3 (fine-tuned with PASTA pre-training)",
            "model_description": "DeBERTaV3Large transformer (disentangled attention, relative-position encodings) initialized from public DeBERTaV3 checkpoint and further pre-trained with 1.2M sentence-table cloze examples, then fine-tuned on TabFact/SEM-TAB-FACTS.",
            "arithmetic_task_type": "Aggregation (sum, average), Counting (COUNT DISTINCT), Comparative numeric comparisons, Ordinal (rank-based), Filter retrieving numeric cell values",
            "reported_mechanism": "Supervised learning to predict operation-aware tokens (numeric results or comparative words) from joint sentence + linearized table input; uses SQL templates executed on the table to synthesize correct numeric targets, relying on DeBERTaV3's disentangled attention/positional encodings to align sentence tokens and table cells.",
            "evidence_for_mechanism": "1) Operation-aware cloze accuracy rises during pre-training (after 400K steps &gt;60% overall). 2) PASTA significantly outperforms the same LM without operation-aware pretraining on operation-specific test sets (e.g., Aggregation: 84.5% vs 81.0%). 3) Ablation vs MLM-style masking and vs no row-ranking show gains attributable to the operation-aware pretraining and select-then-rank preprocessing.",
            "evidence_against_mechanism": "1) Cloze accuracy plateaus at about ~60% after 400K steps (not near-perfect). 2) Error analysis shows failures on large tables and on statements with multiple operations; aggregation is harder and learned later, indicating limited generalization to complex chained arithmetic.",
            "intervention_type": "Operation-aware pre-training (sentence-table cloze masking of operation-aware spans), select-then-rank fine-tuning (column selection + row-wise ranking), probing-based sentence polishing for template fluency.",
            "effect_of_intervention": "Pre-training with operation-aware cloze improved downstream fact verification accuracy (TabFact test 89.3% vs DeBERTaV3 86.2%), larger gains on complex/multi-row arithmetic-heavy cases (complex set 85.6% vs 82.9%), and improved aggregation-specific verification (example: Aggregation accuracy +3.5 points). Select-then-rank (row ranking) further increased effectiveness versus no ranking.",
            "performance_metrics": "PASTA on TabFact: Test accuracy 89.3% (overall), Complex set 85.6%; DeBERTaV3 baseline 86.2% overall, 82.9% complex. Aggregation-specific test (sampled 200): PASTA 84.5% vs DeBERTaV3 81.0%. Pre-training cloze tasks: &gt;60% accuracy across operation types after 400K steps. MLM comparison (140K steps pretraining): PASTA (pretrain) -&gt; Test 87.9% vs MLM-pretrained -&gt; Test 84.9%; Complex: 84.9% vs 81.2%. SEM-TAB-FACTS: PASTA 84.10% vs DeBERTaV3 78.92%. Small TabFact human baseline: human 92.1% vs PASTA small test 90.6%.",
            "notable_failure_modes": "Fails more on large tables (error-set avg #cells higher than dataset average) and on statements containing multiple operations; aggregation and long-span answers (Filter-type) are harder; synthetic-template diversity limits generalization to unseen phrasing.",
            "comparison_to_humans_or_symbolic": "PASTA narrows gap to human on small TabFact test (90.6% vs 92.1%). The paper uses SQL execution only to synthesize supervision (not as a runtime symbolic calculator); program-driven semantic-parsing methods are discussed as alternative symbolic approaches but have issues (spurious programs) according to the authors.",
            "uuid": "e3150.0",
            "source_info": {
                "paper_title": "PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "DeBERTaV3",
            "name_full": "DeBERTaV3 (Decoding-enhanced BERT with disentangled attention, v3)",
            "brief_description": "A BERT-like transformer that uses disentangled attention with separate content and relative-position projections and additional pretraining (replaced token detection) to improve encoding of positional/structural signals; used here as the base encoder for sentence-table joint inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTaV3Large",
            "model_description": "Large-scale transformer with disentangled content/position attention and RTD-style pretraining; used in this paper starting from public DeBERTaV3Large checkpoint and further pre-trained with PASTA tasks.",
            "arithmetic_task_type": "Used as substrate to learn Aggregation (sum/avg), Counting, Comparative numeric reasoning via fine-tuning; not an arithmetic-specialized architecture per se.",
            "reported_mechanism": "Disentangled attention and relative-position encodings enable improved sentence-table alignment and help the model attend to relevant cells for numeric reasoning; the model learns to map table cell values through supervised cloze targets rather than explicit algorithmic execution.",
            "evidence_for_mechanism": "DeBERTaV3 baseline already yields strong performance on table tasks (Test 86.2% on TabFact), indicating its positional encoding helps table understanding; PASTA's improvements build on this architecture, and ablations show importance of row-wise ordering given positional sensitivity.",
            "evidence_against_mechanism": "Despite architectural strengths, DeBERTaV3 without operation-aware pretraining underperforms PASTA on aggregation/complex numeric tasks, indicating architecture alone does not provide full arithmetic capability.",
            "intervention_type": "Further pre-training (PASTA), select-then-rank preprocessing, operation-aware masking during pretraining.",
            "effect_of_intervention": "When further pre-trained with PASTA tasks, DeBERTaV3's arithmetic-related verification accuracy improves (e.g., complex TabFact +2.7 to +3.9 points depending on split).",
            "performance_metrics": "DeBERTaV3 baseline on TabFact: Test 86.2% overall, Simple 92.8%, Complex 82.9%. On SEM-TAB-FACTS: 78.92% test.",
            "notable_failure_modes": "As above: struggles on large tables and multi-operation statements even with the architecture's positional encodings.",
            "comparison_to_humans_or_symbolic": "DeBERTaV3 is a learned approximate reasoner and falls short of human-level on small TabFact set; symbolic SQL execution approaches are discussed in related work as an alternate route for exact arithmetic.",
            "uuid": "e3150.1",
            "source_info": {
                "paper_title": "PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "MLM baseline",
            "name_full": "Masked Language Modeling (random masking) baseline",
            "brief_description": "The standard MLM objective (randomly mask ~15% tokens) applied to sentence+table joint inputs as a pretraining baseline, contrasted with PASTA's operation-aware masking.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTaV3 + MLM-style pretraining",
            "model_description": "Same DeBERTaV3Large backbone; pre-trained by randomly masking tokens in sentence-table pairs (15% masking), then fine-tuned on TabFact for comparison with PASTA masking.",
            "arithmetic_task_type": "Same downstream arithmetic-type verification tasks (Aggregation, Comparative, etc.) used for comparison.",
            "reported_mechanism": "MLM relies on contextual language prediction which does not target operation-aware numeric reasoning; it may capture local lexical patterns but not explicit table-based numerical operations.",
            "evidence_for_mechanism": "MLM gave modest improvements over vanilla DeBERTaV3 in some settings but underperformed PASTA on complex/aggregation heavy cases (Test MLM 84.9% vs PASTA-pretrained 87.9% in the controlled 140K-step comparison).",
            "evidence_against_mechanism": "Random masking does not target operation spans and thus is less effective for learning table-based arithmetic; authors note random masking may not predict cell contents and is not focused on operation tokens.",
            "intervention_type": "Pretraining scheme (random-span MLM vs targeted operation-aware masking).",
            "effect_of_intervention": "MLM pretraining provided smaller gains than PASTA; in the 140K-step experiment MLM -&gt; Test 84.9% vs PASTA 87.9%, Complex 81.2% vs PASTA 84.9%.",
            "performance_metrics": "MLM (140K-step pretrain) on TabFact: Val 84.8%, Test 84.9%, Simple 92.5%, Complex 81.2%.",
            "notable_failure_modes": "Underperforms on aggregation/complex numerical statements compared to operation-aware pretraining.",
            "comparison_to_humans_or_symbolic": "MLM-based pretraining is an unsupervised language modeling strategy and does not approach human-level arithmetic reasoning on complex table tasks in these experiments.",
            "uuid": "e3150.2",
            "source_info": {
                "paper_title": "PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "TAPEX",
            "name_full": "TAPEX: Table Pre-training via Learning a Neural SQL Executor",
            "brief_description": "A prior work that pre-trains a seq2seq model (BART-based) to mimic execution of SQL-like queries over tables to improve table question answering and numerical reasoning.",
            "citation_title": "TAPEX: table pre-training via learning a neural SQL executor",
            "mention_or_use": "mention",
            "model_name": "TAPEX (BART-based neural SQL executor)",
            "model_description": "Pre-trained BART-like model trained to emulate a SQL executor using synthesized SQL queries and execution results on tables (execution-centric pretraining).",
            "arithmetic_task_type": "SQL-executable numerical operations (SUM, AVG, COUNT, ORDER BY, etc.) and other table-based numeric calculations.",
            "reported_mechanism": "Learns to map natural language / SQL-like programs to numeric execution results via supervised imitation of SQL execution, effectively internalizing arithmetic transformations through sequence generation and execution supervision.",
            "evidence_for_mechanism": "Cited as an execution-centric pretraining approach that helps numeric reasoning by training the model to produce results of SQL queries; referenced as a related technique to PASTA's SQL-based synthesis of targets.",
            "evidence_against_mechanism": "Not evaluated in this paper; authors contrast program-driven or execution-mimicking approaches with PASTA's cloze masking, noting program methods can suffer from issues like spurious programs under weak supervision (discussion in related work).",
            "intervention_type": "Execution-centric pretraining (neural SQL executor supervision).",
            "effect_of_intervention": null,
            "performance_metrics": null,
            "notable_failure_modes": "As discussed in related work, program-driven approaches can be affected by spurious programs under weak supervision; exact failure modes not analyzed in this paper.",
            "comparison_to_humans_or_symbolic": "TAPEX is closer to symbolic execution by mimicking SQL execution; PASTA instead uses SQL only to synthesize supervision and relies on the LM to predict operation tokens rather than execute programs.",
            "uuid": "e3150.3",
            "source_info": {
                "paper_title": "PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "FORTAP",
            "name_full": "FORTAP: using formulae for numerical-reasoning-aware table pretraining",
            "brief_description": "A pretraining approach (cited) that leverages spreadsheet-like formulas to train LMs for numerical reasoning, including objectives like Numerical Reference Prediction (NRP) and Numerical Calculation Prediction (NCP).",
            "citation_title": "FORTAP: using formulae for numerical-reasoning-aware table pretraining",
            "mention_or_use": "mention",
            "model_name": "FORTAP (pretraining objectives NRP/NCP)",
            "model_description": "A pretraining framework that incorporates numerical reasoning objectives derived from formulae and spreadsheet data to improve models' numeric capabilities.",
            "arithmetic_task_type": "Numerical calculation prediction and numerical reference resolution (spreadsheet formula style operations, aggregation, arithmetic).",
            "reported_mechanism": "Uses explicit numerical-centric pretraining tasks (NRP, NCP) to force the model to learn numeric relationships and calculation patterns from formula-like supervision.",
            "evidence_for_mechanism": "Cited as related work that focuses on numerical characteristics of tables (authors position PASTA as complementary by focusing on operation-aware cloze tasks), but no primary results reported in this paper.",
            "evidence_against_mechanism": "No direct evidence in this paper; FORTAP is referenced as an alternative pretraining focusing on numeric formulae rather than cloze masking.",
            "intervention_type": "Numerical-specific pretraining objectives (NRP, NCP) derived from spreadsheet formula data.",
            "effect_of_intervention": null,
            "performance_metrics": null,
            "notable_failure_modes": null,
            "comparison_to_humans_or_symbolic": "FORTAP is an approach that more explicitly targets numeric/formulaic patterns, closer to symbolic numeric manipulation than generic MLM, but this paper does not provide direct head-to-head comparisons.",
            "uuid": "e3150.4",
            "source_info": {
                "paper_title": "PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TAPEX: table pre-training via learning a neural SQL executor",
            "rating": 2
        },
        {
            "paper_title": "FORTAP: using formulae for numerical-reasoning-aware table pretraining",
            "rating": 2
        },
        {
            "paper_title": "Tapas: Weakly supervised table parsing via pre-training",
            "rating": 2
        },
        {
            "paper_title": "LOGICALFACTCHECKER: Leveraging logical operations for fact checking with graph module network",
            "rating": 1
        },
        {
            "paper_title": "Table-BERT",
            "rating": 1
        }
    ],
    "cost": 0.015563999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PaSTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training</h1>
<p>Zihui Gu ${ }^{1}$, Ju Fan ${ }^{1}$, Nan Tang ${ }^{2}$, Preslav Nakov ${ }^{3}$, Xiaoman Zhao ${ }^{1 *}$, Xiaoyong Du ${ }^{1}$<br>${ }^{1}$ Renmin University of China, Beijing, China,<br>${ }^{2}$ Qatar Computing Research Institute, HBKU, Doha, Qatar<br>${ }^{3}$ Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE<br>${ }^{1}$ {guzh, fanj, xiaomanzhao, duyong}@ruc.edu.cn,<br>${ }^{2}$ ntang@hbku.edu.qa, ${ }^{3}$ preslav.nakov@mbzuai.ac.ae</p>
<h4>Abstract</h4>
<p>Fact verification has attracted a lot of research attention recently, e.g., in journalism, marketing, and policymaking, as misinformation and disinformation online can sway one's opinion and affect one's actions. While fact-checking is a hard task in general, in many cases, false statements can be easily debunked based on analytics over tables with reliable information. Hence, table-based fact verification has recently emerged as an important and growing research area. Yet, progress has been limited due to the lack of datasets that can be used to pre-train language models (LMs) to be aware of common table operations, such as aggregating a column or comparing tuples. To bridge this gap, in this paper we introduce PASTA, a novel state-of-the-art framework for table-based fact verification via pre-training with synthesized sentence-table cloze questions. In particular, we design six types of common sentence-table cloze tasks, including Filter, Aggregation, Superlative, Comparative, Ordinal, and Unique, based on which we synthesize a large corpus consisting of 1.2 million sentence-table pairs from WikiTables. PASTA uses a recent pre-trained LM, DeBERTaV3, and further pretrains it on our corpus. Our experimental results show that PASTA achieves new state-of-the-art performance on two table-based fact verification benchmarks: TabFact and SEMTAB-FACTS. In particular, on the complex set of TabFact, which contains multiple operations, PASTA largely outperforms the previous state of the art by $\mathbf{4 . 7}$ points ( $85.6 \%$ vs. $80.9 \%$ ), and the gap between PASTA and human performance on the small TabFact test set is narrowed to just $\mathbf{1 . 5}$ points ( $90.6 \%$ vs. $92.1 \%$ ). ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Fact verification, which checks the factuality of a statement, is crucial for journalism (Shu et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2017), and is increasingly being applied in other fields (Ott et al., 2011; Yoon et al., 2019). According to Duke Reporters' Lab, there are 300+ active certified fact-checking organizations worldwide. ${ }^{2}$</p>
<p>Automatic and explainable approaches, a.k.a. reference-based approaches, are widely used to assist fact-checkers. They verify the input statement against a trusted source, such as relevant passages from Wikipedia (Popat et al., 2017; Thorne et al., 2018; Shaar et al., 2020). Recently, table-based fact verification has been extensively studied (Chen et al., 2020a; Zhong et al., 2020; Eisenschlos et al., 2020) due to the wide availability of tabular data.</p>
<p>Evidently, performing fact verification over tables requires the ability to reason about tablebased operations, such as aggregating the values in a column or comparing tuples. For example, for the statement $S_{1}$ in Figure 1, it is desirable to reason about the operation over table $T$ that compares the viewers of Night Noves with 3.61 to determine whether $S_{1}$ is entailed or refuted by $T$.</p>
<p>Most previous work (Herzig et al., 2020; Wang et al., 2021a; Schlichtkrull et al., 2021) leverages pre-trained language models (LMs) (Devlin et al., 2019; Liu et al., 2019), which are originally designed for unstructured data, and have a key limitation of overlooking such operations. Some approaches (Zhong et al., 2020; Yang et al., 2020) attempt to explicitly capture the operations by generating a logical form (e.g., a tree) containing the operations from the statement via semantic parsing techniques. However, such approaches face the problem of "spurious programs" (Chen et al., 2020a), due to weak supervision signals in semantic parsing.</p>
<p>To address the above issues, we propose PASTA, a table-operations aware approach. Instead of relying on semantic parsing, PASTA captures tablebased operations by designing a novel sentence-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Series</th>
<th style="text-align: center;">Title</th>
<th style="text-align: center;">Written By</th>
<th style="text-align: center;">Viewers (Million)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Pilot</td>
<td style="text-align: center;">Mikael Salomon</td>
<td style="text-align: center;">3.82</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Healing Time</td>
<td style="text-align: center;">Arvin Brown</td>
<td style="text-align: center;">3.80</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Night Moves</td>
<td style="text-align: center;">Roxann Dawson</td>
<td style="text-align: center;">3.61</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Mother's Day</td>
<td style="text-align: center;">Jeff Bleckner</td>
<td style="text-align: center;">3.35</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Statement 51
"The episode Night Moves was watched by more than 3.61 million viewers"
Statement 52
"The sum of the viewers of Pilot and Healing Time is 7.62 million"</p>
<p>Figure 1: An example of table-based fact verification.
table cloze pre-training strategy that better guides LMs to reason about table-based operations.</p>
<p>We tackle two challenges for pre-training LMs towards supporting table-based fact verification:</p>
<ul>
<li>Challenge 1: What types of tasks should be designed, so as to pre-train (or teach) LMs to be aware of operations over tables?</li>
<li>Challenge 2: How to obtain a large-scale and high-quality corpus for pre-training?</li>
</ul>
<p>To address Challenge 1, PASTA automatically synthesizes sentence-table cloze questions. It first synthesizes operations from tables, and then generates cloze tasks by masking the key tokens corresponding to the table-based operations, e.g., "more than" and "sum of" in Figure 1. Then, LMs are pre-trained to predict the masked operation-aware tokens based on the tables.</p>
<p>Regarding Challenge 2, PASTA uses a large table collection, WikiTables (Bhagavatula et al., 2013), and for each table, it synthesizes a diverse set of cloze tasks with six types of table-based operations, including Filter, Aggregation, Superlative, Comparative, Ordinal, and Unique.</p>
<p>For implementation, PASTA uses a recent pretrained LM, DeBERTaV3 (He et al., 2021a,b) with better positional encoding of the input. To cope with the limited input length of DeBERTaV3, we introduce a select-then-rank strategy for large tables, which further improves the performance.</p>
<p>In sum, we make the following contributions.</p>
<ul>
<li>We propose PASTA, a table-operations aware fact verification approach without explicitly generating logical forms.</li>
<li>We propose a new benchmark for pre-training LMs to be aware of common table-based operations by automatically synthesizing sentencetable cloze questions from WikiTables. In particular, we synthesize a large corpus consisting of 1.2 million sentence-table cloze questions, which we release for future research.</li>
<li>We evaluate PASTA, which is DeBERTaV3 pre-trained with our table-operations aware pre-training approach, on two widelyadopted table-based fact verification benchmark datasets, TabFact (Chen et al., 2020a) and SEM-TAB-FACTS (Wang et al., 2021b). The experimental results show that PASTA achieves new state-of-the-art (SOTA) results on the two datasets. In particular, on the complex set of TabFact that contains multiple operations, PASTA outperforms the previous SOTA by $\mathbf{4 . 7}$ points ( $85.6 \%$ vs. $80.9 \%$ ), and the gap between PASTA and human performance on the small test set is narrowed to $\mathbf{1 . 5}$ points $(90.6 \%$ vs. $92.1 \%)$.</li>
</ul>
<h2>2 Preliminaries</h2>
<h3>2.1 Problem Formulation</h3>
<p>Let $T$ be a table with $m$ columns and $n$ rows. Let $T_{i, j}$ denote the cell in the $i$-th column and $j$-the row of $T$. Let $S$ be a natural language (NL) statement.</p>
<p>The problem of table-based fact verification is formulated as follows: Given an NL statement $S$ and a table $T$, it determines whether statement $S$ can be entailed or refuted by table $T^{3}$.</p>
<p>See Figure 1 for an example table about movies and their viewers, and two statements where $S_{1}$ contains a Comparative operation and $S_{2}$ contains an Aggregation operation.</p>
<h3>2.2 DeBERTa for Sentence-Table Encoding</h3>
<p>Inspired by the success of BERT-like models (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020) in natural language understanding (NLU) tasks, many existing studies leverage pre-trained LMs for table understanding, achieving superior results (Chen et al., 2020b; Schlichtkrull et al., 2021). In this paper, we apply DeBERTa (He et al., 2021b) for sentence-table encoding, as it can effectively capture positional information of the input with its</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>positional encoding scheme, which is useful for sentence-table encoding.</p>
<p>Given an input token at position $i$, DeBERTa represents it using two vectors, $\left{H_{i}\right}$ and $\left{P_{i \mid j}\right}$, to represent its content and relative position with respect to the token at position $j$. For a single-head self-attention layer, DeBERTa (He et al., 2021b) represents the disentangled self-attention mechanism as follows:</p>
<p>$$
\begin{aligned}
&amp; Q_{c}=H W_{q, c}, K_{c}=H W_{k, c}, V_{c}=H W_{v, c} \
&amp; Q_{r}=P W_{q, r}, K_{r}=P W_{k, r} \
&amp; \tilde{A}<em i="i">{i, j}=Q</em>
\end{aligned}
$$}^{c} K_{j}^{c \mathrm{~T}}+Q_{i}^{c} K_{\delta(i, j)}^{\mathrm{T}}+K_{j}^{c} Q_{\delta(j, i)}^{\mathrm{T}</p>
<p>where $\tilde{A}<em c="c" q_="q,">{i, j}$ represents the attention score between token $i$ and token $j$. The content vector $H$ is projected by the matrices $W</em>$, respectively.}, W_{k, c}, W_{v, c} \in R^{d \times d}$ to generate the projected content vectors $Q_{c}, K_{c}$ and $V_{c}$, respectively, $P \in R^{2 k \times d}$ is the relative position embedding vector, and $\delta(i, j)$ is the relative distance from token $i$ to token $j$. Similarly to $H$, $P$ is projected by the matrices $W_{q, r}, W_{k, r} \in R^{d \times d}$ to generate the projected position vectors $Q_{r}$ and $K_{r</p>
<p>In our implementation, we adopt the latest version DeBERTaV3 (He et al., 2021a), which improves DeBERTa by further pre-training with replaced token detection (RTD) to jointly encode a sentence and a table. Unlike NL, tables have distinct structural information that is difficult to capture by a pre-trained LMs. Therefore, we use special symbols to inject the structural information into an NL sentence. Specifically, we linearize the table $T=\left{T_{i, j} \mid i \leq m, j \leq n\right}$ into sentence $S_{T}$ $=\left[\right.$ Header] $T_{0,0} \mid T_{0,1} \ldots \mid T_{0, n}$ [Row] $T_{1,0} \mid T_{1,1}$ $\left.\ldots\right.$ [Row] $T_{i, 0} \mid T_{i, 1} \ldots \mid T_{m, n}$. We use [Header] to indicate the beginning of the headers and [Row] to indicate the beginning of each row. Inspired by Liu et al., 2021, we also use " $|$ " to separate each cell. Afterwards, we concatenate the statement $S$ and the linearized table $S_{T}$.</p>
<h2>3 Our PaSTA Model</h2>
<p>Figure 2 gives an overview of our PaSTA framework, which follows the pre-training-fine-tuning framework. For pre-training, we guide our model to understand sentences and to perform table-based operations (e.g., Aggregation) to complete synthesized cloze tasks in the sentence. For finetuning, we apply a select-then-rank strategy to trade off between sizes of large tables and the limited input length of DeBERTaV3. Next, we will
first present our sentence-table cloze task and pretraining corpus generation in Sections 3.1 and 3.2, respectively. We will then discuss our fine-tuning strategy in Section 3.3.</p>
<h3>3.1 Sentence-Table Cloze Pre-training</h3>
<p>An essential ability for our fact verification model is to be capable of reasoning about table-based operations on the entities of the table. Although it is difficult to enumerate all the expressions of real-world statements, the types of operations (e.g., Unique) expressed by statements could be limited. Therefore, it is possible and reasonable for the model to understand these operations. To this end, we propose a table-operations aware pre-training task which can guide the model to understand tableaware operations expressed by the statements.</p>
<p>Inspired by the Masked Language Modeling (MLM) (Devlin et al., 2019), we design a cloze task to pre-train the model's ability to reason about operations over tables. However, the key difference is that we do not use the random masking strategy in MLM due to the following reasons. First, masking a specific cell of the table and training the model to predict it is difficult, because, unlike the words in a sentence, the contents of a cell may not be predictable from the contents of the surrounding cells. Moreover, the content of an individual cell may be useless for determining whether statement $S$ can be entailed or refuted by table $T$. Second, not every token in the sentence needs to be predicted. For example, in the sentence "The Palazzo has more floors than Las Vegas Hilton.", tokens like has and the, which can be easily predicted from contextual information, are not worth learning for the model because this kind of ability is already captured by pre-trained LMs.</p>
<p>To pre-train the model to be aware of table operations, we propose to mask operation-aware tokens in the sentence, which need to meet two requirements: (i) to appear in the sentence and to correspond to table-based operations, and (ii) to be predictable by reasoning over tables. For example, in the above sentence, as the model needs to find the numbers of floors for "Las Vegas Hilton" and for "The Palazzo" in the table and then to compare them, "more" is the operation-aware token that the model needs to predict.</p>
<p>To cover common types of operations in pretraining, we refer to the operations list defined in LPA (Chen et al., 2020a). We design six sentence-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: An overview of the pre-training and fine-tuning procedures of PASTA.
table cloze tasks according to various operation types: Filter, Aggregation, Superlative, Comparative, Ordinal, and Unique. Figure 2 shows some example cloze tasks of operation types, Filter, Aggregation and Comparative. The answer to a cloze task, i.e., operation-aware tokens to be predicted, may be a specific table cell (e.g., "114") or the result of a series of calculations (e.g., "134.7", "more"). Note that we assume that only atomic operation types need to be learned in pre-training, and various combinations and expressions are left to fine-tuning. Thus, only one type of operation-aware token is masked in each statement.</p>
<p>We formally define the table-operations aware pre-training task as follows: Given a sentence $S=$ $\left{x_{i}\right}$ and a table $T=\left{T_{i, j} \mid i \leq m, j \leq n\right}$, we corrupt $S$ into $\tilde{S}$ by masking the operation-aware span of tokens $S_{\text {span }}=\left{\tilde{x}<em i="i">{i}\right} \subset S$ in it, and then we train an LM parameterized by $\theta$ to reconstruct $S$ by predicting the masked tokens $\left{\tilde{x}</em>\right}$, i.e., optimizing the following objective:</p>
<p>$$
\begin{aligned}
L_{\text {PASTA }} &amp; =-\log p_{\theta}(S \mid \tilde{S}, T) \
&amp; =-\sum_{\tilde{x}<em _span="{span" _text="\text">{i} \in S</em>}}} \log p_{\theta}\left(\tilde{x<em i="i">{i}=x</em>, T\right)
\end{aligned}
$$} \mid \tilde{S</p>
<h3>3.2 Pre-training Corpus Generation</h3>
<p>Next, we introduce our strategy for generating the pre-training corpus consisting of sentence-table pairs. According to the table-operations aware pretraining task described in Section 3.1, the difficulty of corpus generation is how to collect a large scale
of sentence-table pairs and how to identify the operation-aware tokens in each sentence. To solve these problems, we propose an automatic data generation method, which consists of table collection and sentence generation. In addition, we also introduce a probing-based sentence polishing method to make it more fluent and natural.</p>
<p>Table Collection. Inspired by previous work (Herzig et al., 2020; Schlichtkrull et al., 2021), we use WikiTables, ${ }^{4}$ which contains Web tables extracted from Wikipedia. Concretely, we only select well-formed relational tables that contain headers and at least one numeric column that can be used for operations. Moreover, considering the maximum input length ( 512 tokens) of our pre-trained LM, we filter out all tables with more than 500 cells. Based on the above process, we obtain a total of 580 K tables from WikiTables, and we then randomly select 20 K tables to improve the efficiency of pre-training.
Sentence Generation. Figure 3 shows the pipeline of our automatic sentence generation method. To ensure that each sentence contains an operation and the operation-aware tokens can be clearly identified, we design NL Templates for each table-aware operation type (e.g., the NL Template in Figure 3 is designed for Comparative. See more details of the manually designed templates in Appendix A). Each NL Template is pre-defined with the position of the operation-aware tokens (e.g., "higher").</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Automatic pre-training corpus generation.</p>
<p>Unlike fact verification, the sentence in the cloze task must be a correct description of the table. To achieve this, we design an SQL Template for each NL Template, and these two templates will be instantiated based on the table at the same time. During instantiation, the [Column] in both templates is replaced by a column header (e.g., [Column1] is replaced by team) and the [Value] is instantiated by a cell (e.g., [Value] is replaced by 97). Then, the SQL Instance will be automatically executed on the table, and the execution result [ANS] will be filled in the NL Instance to ensure its correctness.</p>
<p>Based on the above method, we generate up to 100 related sentences for each table, depending on the size of the table. Statistics about the pre-training corpus are given in Table 1. We can see that the proportion of each type is different; it mainly depends on how many different expressions a type contains. Taking Aggregation with the highest proportion as an example: in addition to "the average of" in Figure 2, there may also be "the sum of", "the total amount of", etc.</p>
<p>Sentence Polishing. We notice that using fixed templates for each table could generate unnatural sentences. For example, in Figure 3, if [Column2] is not populated with "score" but by "age", then the operation-aware token should use "older" instead of "higher". Therefore, we introduce a probing-based method to improve the fluency of sentences, which leverages the rich knowledge learned by the LMs implicitly during pre-training. Our main idea is that since BERT-like LMs are pre-trained on extensive textual corpora, their predictions can approximate the natural language expressions used in real-world scenarios.</p>
<p>Specifically, we identify the context sensitive word $w^{\prime}$ in each template (e.g., "higher"), and define a set of candidate values (e.g., "higher", "more", ..., "older") for $w^{\prime}$. Then, we replace the $w^{\prime}$ with [MASK] and leverage a fixed LM to de-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: right;"># Sentence-Table</th>
<th style="text-align: center;">Len (Ans)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Filter</td>
<td style="text-align: right;">$77,609(6 \%)$</td>
<td style="text-align: center;">3.2</td>
</tr>
<tr>
<td style="text-align: left;">Superlative</td>
<td style="text-align: right;">$349,241(27 \%)$</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: left;">Aggregation</td>
<td style="text-align: right;">$388,046(30 \%)$</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: left;">Comparative</td>
<td style="text-align: right;">$349,241(27 \%)$</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Ordinal</td>
<td style="text-align: right;">$103,479(8 \%)$</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: left;">Unique</td>
<td style="text-align: right;">$25,872(2 \%)$</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">$1,293,488$</td>
<td style="text-align: center;">1.8</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of our pre-training corpus, where "Len (Ans)" represents the average length of the answer.
termine the appropriate value for [MASK] based on context. For example, if [Column2] is populated as "age", the pre-trained LM calculates probabilities for all candidate values and then selects the one with the highest probability, e.g., "older". Please refer to Appendix A for the detailed definition of context sensitive words and their candidate sets.</p>
<h3>3.3 Fine-tuning with Select-then-Rank</h3>
<p>For fine-tuning, we pre-process the table based on the following two considerations: (i) As mentioned in Eisenschlos et al., 2020, the sentence-table pairs in the downstream datasets may be too long for pre-trained LMs, and (ii) considering that the disentangled attention mechanism in DeBERTa makes the model more sensitive to the positional information of the input, we assume that it would be easier for the model to capture the sentence-table relationship by putting the most relevant cells in the table closer to the sentence. To address the above two problems, we propose a select-then-rank method to reconstruct the table content.
Column-wise Selection. To make the table size to meet the input length limit of DeBERTaV3, we follow previous work (Chen et al., 2020a) to only select columns in the table containing entities linked to the statement, which results in a pre-processed table $\hat{T}$. Note that the reason for not selecting by rows is that some operations may involve an entire column of cells, e.g., Aggregation.
Row-wise Ranking. To make the sentence and its relevant cells in the table have closer positions, we reorder the table $\hat{T}$ by row. Specifically, we slice $\hat{T}$ into a set of rows $\left{r_{1}, \ldots, r_{m}\right}$, and rank these rows by their relevance scores $\left{p_{i}\right}$, as defined below. Let $\hat{r_{i}}$ and $\hat{s}$ denote the token sets of row $r_{i}$ and statement $s$ respectively. The relevance score $p_{i}$ is given by $\left|\hat{r_{i}} \cap \hat{s}\right|$. Note that we remove the stopwords (e.g., the) in $\hat{r_{i}}$ and $\hat{s}$. Finally, we reconstruct</p>
<p>the table by ordering the rows in descending order of the relevant scores, before applying the table linearization introduced in Section 2.2.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Datasets</h3>
<p>By using the method introduced in 3.2, we synthesize 1.2 million sentence-table cloze questions as the pre-training corpus. Specifically, our pretraining corpus contains 20 K well-structured tables selected from WikiTables, and sentences are automatically generated from the tables.</p>
<p>During fine-tuning, we evaluate our model on two widely-adopted table-based fact verification benchmark datasets TabFact (Chen et al., 2020a) and SEM-TAB-FACTS (Wang et al., 2021b). TabFact contains 16 K tables collected from WikiTables and 118 K human-annotated natural language statements, where each statement-table pair is labeled as either entailed or refuted. TabFact contains statements with two difficulty levels: (i) simple statements corresponding to single rows, and (ii) complex statements involving multiple rows with table-based operations like Aggregation. SEMTAB-FACTS contains 2 K tables and 4 K humanannotated natural language statements. Different from TabFact, these tables are collected from scientific articles in a variety of domains. We use the official splits in the two benchmarks for evaluation: the training, validation and test sets of TabFact respectively contain 92283, 12792 and 12779 sentence-table pairs; the training, validation and test sets of SEM-TAB-FACTS respectively contain 4506, 423 and 522 sentence-table pairs. In addition, TabFact also holds out a small test set with 2 K sentence-table pairs with human performance.</p>
<h3>4.2 Baselines</h3>
<p>We evaluate PASTA with the following ten state-of-the-art methods for table-based fact verification.
Table-BERT (Chen et al., 2020a) adopts templates to linearize a table into an NL sentence, and then directly leverages a BERT model to encode the linearized table and the statement.
LogicFactChecker (Zhong et al., 2020) leverages a sequence-to-action semantic parser to generate a "program", i.e., a tree with multiple operations, and uses a graph neural network to encode statements, tables, and the generated programs.
SAT (Zhang et al., 2020) creates a structure-aware mask matrix to encode the structural data. In par-
ticular, it considers recovering the alignment information of tabular data by masking signals of unimportant cells during self-attention.
ProgVGAT (Yang et al., 2020) integrates programs and execution into a natural language inference model. This method uses a verbalization with a program execution model to accumulate evidences and constructs a graph attention network to combine various evidences.
Tapas (Herzig et al., 2020) extends BERT with additional structure-aware positional embeddings to represent the tables. Eisenschlos et al., 2020 further pre-train Tapas on counterfactually-augmented and grammar-based synthetic statements.
Schlichtkrull et al., 2021 study table-based fact verification in an open-domain setting, and combine a TF-IDF retrieval model with a RoBERTabased joint reranking-and-verification model.
Tapex (Liu et al., 2021) guides the pre-trained BART model to mimic an SQL executor via an execution-centric table pre-training approach. The pre-training corpus of Tapex is synthesized via sampling SQL queries from the SQUALL dataset (Shi et al., 2020).
SaMoE (Zhou et al., 2022) develops a mixture-of-experts network based on the RoBERTa-large model (Liu et al., 2019). The MoE network consists of different experts, and then a management module decides the contribution of each expert network to the verification result.
Volta (Gautam et al., 2021) analyzes how transfer learning and standardizing tables to contain a single header row can boost the effectiveness of tablebased fact verification.
LKA (Zhao and Yang, 2022) studies the sentencetable's evidence correlation. It develops a dualview alignment module based on the statement and table views to identify the most important words through various interactions.</p>
<h3>4.3 Implementation Details</h3>
<p>Our model is implemented based on the transformer architecture (Wolf et al., 2020). Specifically, we start pre-training with the public DebertaV3Large checkpoint ${ }^{5}$ and optimize the learning objective with Adam (Kingma and Ba, 2015). Our pre-training process runs up to 400 K steps with a batch size of 16 and a learning rate of $1 \times 10^{-6}$. The complete pre-training procedure takes about 3</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>days on 2 RTX A6000 GPUs. For fine-tuning, the model runs up to 300 K steps with a batch size of 8 and a learning rate of $5 \times 10^{-6}$.</p>
<h2>5 Experiments and Results</h2>
<h3>5.1 Overall Performance</h3>
<p>Table 2 summarizes the overall experimental results for various fact verification methods on TabFact. We can see that PASTA achieves the new SOTA results on all splits of TabFact. In particular, on the complex set containing multiple operations, PASTA largely outperforms the previous state-of-the-art by 4.7 points ( $\mathbf{8 5 . 6 \%}$ vs. $\mathbf{8 0 . 9 \%}$ ).</p>
<p>Table 2 also reports the good performance of DeBERTaV3 on the table-based fact verification task. This result is analogous to the observation in Schlichtkrull et al., 2021 that RoBERTa can yield strong performance exceeding the previous closedsetting ( $77.6 \%$ vs. $74.4 \%$ ). Both results illustrate that the BERT-like model pre-trained on textual data can also perform well on linearized tabular data. However, PASTA surpasses DeBERTaV3 by 3.1 points on the test set ( $89.3 \%$ vs. $86.2 \%$ ), i.e., 3.9 points and 2.7 points on the simple test set and complex test set respectively. The experimental result shows that PASTA endows DeBERTaV3 with more powerful statement-table reasoning ability, which is very crucial for fact verification.</p>
<p>Table 3 shows that PASTA outperforms all baseline models by large margins on the SEM-TABFACTS dataset. In particular, PASTA significantly surpasses the DeBERTaV3 model by 5.2 points ( $84.1 \%$ vs $78.9 \%$ ) on the test set. This shows that although our pre-training corpus only contains tables from Wikipedia, it can be applied to other domains, such as tables from scientific articles included in the SEM-TAB-FACTS dataset.</p>
<h3>5.2 Impact of Operation Aware Pre-training</h3>
<p>Performance on cloze pre-training. As described in Section 3.1, PASTA is pre-trained on tableoperations aware cloze tasks to be capable of reasoning about table-aware operations over tables. To explore whether the model has learned such ability, we generate six test sets corresponding to different operation types, and evaluate the performance of PASTA in various steps during pre-training. The experimental results are reported in Figure 4. Overall, after 400 K steps, PASTA can correctly complete more than $60 \%$ sentence-table cloze questions with various types. More specifically, with increasing
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Accuracy on operation-aware cloze tasks at different training steps. For each operation, the size of its test set is 1 K where the test set does not contain any tables from the training set.
the steps, PASTA is firstly capable of reasoning about Comparative operations, and finally mastering Aggregation and Filter operations. This may be attributed to the difficulty of the operations (e.g., Aggregation is harder than other types) and the length of token span that needs to be predicted (As shown in Table 1, Filter needs to predict more tokens than other types).</p>
<p>Operation understanding on fact verification. We further analyze whether the model can utilize the reasoning ability learned from pre-training for our downstream task, i.e., table-based fact verification. To this end, we evaluate PASTA on test sets of different operation types. We split the test set of TabFact according to the trigger words in the statement, which are defined in Appendix B, e.g., "highest" and "lowest" related to the Superlative type. We control the size of each test set as 200, while ensuring that these sets have no overlap. We compare PASTA and DeBERTaV3 on these test sets, and the results are shown in the Table 4. We can see that PASTA outperforms DeBERTaV3 on every test set, especially on the Aggregation type. Note that we did not use any fine-tuning strategy on both models, and thus all the improvements of PASTA are to be attributed to our table-operations aware pre-training strategy.
Comparison with Masked Language Modeling. We compare PASTA with the random masking scheme in Masked Language Modeling (MLM). For MLM, we randomly mask $15 \%$ of the tokens in a sentence-table pair, of which $10 \%$ of the masked tokens remain unchanged, $10 \%$ are replaced with randomly picked tokens, and the remainders are replaced with the [MASK] token. For PASTA, we</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Val</th>
<th style="text-align: left;">Test</th>
<th style="text-align: left;">Simple Test</th>
<th style="text-align: left;">Complex Test</th>
<th style="text-align: left;">Small Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Table-BERT (Chen et al., 2020a)</td>
<td style="text-align: left;">66.1</td>
<td style="text-align: left;">65.1</td>
<td style="text-align: left;">79.1</td>
<td style="text-align: left;">58.2</td>
<td style="text-align: left;">68.1</td>
</tr>
<tr>
<td style="text-align: left;">LogicFactChecker (Zhong et al., 2020)</td>
<td style="text-align: left;">71.8</td>
<td style="text-align: left;">71.7</td>
<td style="text-align: left;">85.4</td>
<td style="text-align: left;">65.1</td>
<td style="text-align: left;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">SAT (Zhang et al., 2020)</td>
<td style="text-align: left;">73.3</td>
<td style="text-align: left;">73.2</td>
<td style="text-align: left;">85.5</td>
<td style="text-align: left;">67.2</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">ProgVGAT (Yang et al., 2020)</td>
<td style="text-align: left;">74.9</td>
<td style="text-align: left;">74.4</td>
<td style="text-align: left;">88.3</td>
<td style="text-align: left;">67.6</td>
<td style="text-align: left;">76.2</td>
</tr>
<tr>
<td style="text-align: left;">Schlichtkrull et al., 2021 (Oracle retrieval)</td>
<td style="text-align: left;">78.2</td>
<td style="text-align: left;">77.6</td>
<td style="text-align: left;">88.9</td>
<td style="text-align: left;">72.1</td>
<td style="text-align: left;">79.4</td>
</tr>
<tr>
<td style="text-align: left;">Tapas (Eisenschlos et al., 2020)</td>
<td style="text-align: left;">81.0</td>
<td style="text-align: left;">81.0</td>
<td style="text-align: left;">92.3</td>
<td style="text-align: left;">75.6</td>
<td style="text-align: left;">83.9</td>
</tr>
<tr>
<td style="text-align: left;">Tapex (Liu et al., 2021)</td>
<td style="text-align: left;">84.6</td>
<td style="text-align: left;">84.2</td>
<td style="text-align: left;">93.9</td>
<td style="text-align: left;">79.6</td>
<td style="text-align: left;">85.9</td>
</tr>
<tr>
<td style="text-align: left;">SaMoE (Zhou et al., 2022)</td>
<td style="text-align: left;">84.2</td>
<td style="text-align: left;">85.1</td>
<td style="text-align: left;">93.6</td>
<td style="text-align: left;">80.9</td>
<td style="text-align: left;">86.7</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTaV3</td>
<td style="text-align: left;">$86.1_{ \pm 0.2}$</td>
<td style="text-align: left;">$86.2_{ \pm 0.1}$</td>
<td style="text-align: left;">$92.8_{ \pm 0.2}$</td>
<td style="text-align: left;">$82.9_{ \pm 0.1}$</td>
<td style="text-align: left;">$86.5_{ \pm 0.3}$</td>
</tr>
<tr>
<td style="text-align: left;">PASTA</td>
<td style="text-align: left;">$\mathbf{8 9 . 2}_{ \pm 0.4}$</td>
<td style="text-align: left;">$\mathbf{8 9 . 3}_{ \pm 0.3}$</td>
<td style="text-align: left;">$\mathbf{9 6 . 7}_{ \pm 0.2}$</td>
<td style="text-align: left;">$\mathbf{8 5 . 6}_{ \pm 0.3}$</td>
<td style="text-align: left;">$\mathbf{9 0 . 6}_{ \pm 0.2}$</td>
</tr>
<tr>
<td style="text-align: left;">Human Performance</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">92.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance on TabFact in terms of binary classification accuracy (\%). The human performance on a small set is from Chen et al., 2020a. The notation "-" indicates that the corresponding values are not listed in the original paper. In addition, models are evaluated with 5 random runs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Val</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Volta (Gautam et al., 2021)</td>
<td style="text-align: left;">74.35</td>
<td style="text-align: left;">73.87</td>
</tr>
<tr>
<td style="text-align: left;">Tapas (Müller et al., 2021)</td>
<td style="text-align: left;">78.33</td>
<td style="text-align: left;">75.33</td>
</tr>
<tr>
<td style="text-align: left;">Tapex</td>
<td style="text-align: left;">77.53</td>
<td style="text-align: left;">75.47</td>
</tr>
<tr>
<td style="text-align: left;">LKA (Zhao and Yang, 2022)</td>
<td style="text-align: left;">80.34</td>
<td style="text-align: left;">78.54</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTaV3</td>
<td style="text-align: left;">81.85</td>
<td style="text-align: left;">78.92</td>
</tr>
<tr>
<td style="text-align: left;">PASTA</td>
<td style="text-align: left;">$\mathbf{8 4 . 2 3}$</td>
<td style="text-align: left;">$\mathbf{8 4 . 1 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance on SEM-TAB-FACTS in terms of micro-F1 (\%). The experimental results of Tapex is from Zhao and Yang, 2022.
use the masking strategy introduced in Section 3.1, which only masks the operation-aware span in the sentence. We pre-train both MLM and PASTA on DeBERTaV3. Considering pre-training efficiency, for both MLM and PASTA, we set the training step as 140 K . Table 5 shows the fine-tuned results of MLM and PASTA on the TabFact dataset. We can see that PASTA outperforms MLM by a large margin on the complex set. This improvement further proves that our table-operations aware pre-training task helps the high-order symbolic reasoning in the complex set. By also observing Table 2, we find that the impact of MLM decreases slightly on the basis of DeBERTaV3 ( $84.9 \%$ vs. $86.2 \%$ ). This may be because the random masking scheme in MLM does not work for sentence-table joint understanding, as we have already analyzed in Section 3.1.</p>
<h3>5.3 Impact of Select-then-Rank</h3>
<p>To verify the effectiveness of the select-then-rank method, we conduct experiments with column-wise selection and row-wise ranking on the TabFact dataset. The results of the experiment are shown in Table 6. We can see that the row-wise ranking
strategy is more effective than the column-wise selection strategy. The main reason may be that the disentangled attention mechanism in DeBERTa makes the model more sensitive to the positional information of the input. The row-wise ranking strategy can put the most relevant cells in the table closer to the sentence, and thus the model can more effectively capture the sentence-table relationship.</p>
<h3>5.4 Error Analysis</h3>
<p>To analyze the errors of PASTA for table-based fact verification, we analyze the sentence-table pairs that PASTA predicts incorrectly in the TabFact dataset. Specifically, we consider the size of the tables and the complexity of the operations in the statements. Table 7 presents some basic statistics about the subset of the test where PASTA makes mistakes. For comparison, we also list the basic statistics about DeBERTaV3's error set and the full test set. We have the following observations. (1) Our models may not perform well on large tables. Concretely, although PASTA reduces the impact of large tables on the fact verification task compared to DeBERTaV3 ( 97.5 vs. 107.4), the impact of large tables on PASTA still exists compared to the average table size in the test set ( 97.5 vs. 89.0). (2) The number of operations in the statement is also an important cause of errors: the proportion of statements with multiple operations in PASTA's error set ( $16.5 \%$ ) is larger than that of the overall test set (11.3\%). Thus, PASTA correctly verifies most of the statements that contain only a single operation, but it still encounters difficulty to verify statements that contain multiple types of operations.</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Example in TabFact</th>
<th>DeBERTaV3</th>
<th>PASTA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Filter</td>
<td>the blues and penguins game on march 20 , score was 2 - 4</td>
<td>88.0</td>
<td>$\mathbf{9 0 . 7}(+2.7)$</td>
</tr>
<tr>
<td>Superlative</td>
<td>pacific national has the highest number in class</td>
<td>85.9</td>
<td>$\mathbf{8 6 . 7}(+0.8)$</td>
</tr>
<tr>
<td>Aggregation</td>
<td>the average amount of points among all teams is 29</td>
<td>81.0</td>
<td>$\mathbf{8 4 . 5}(+3.5)$</td>
</tr>
<tr>
<td>Comparative</td>
<td>ian woosnam placed higher than craig parry</td>
<td>85.2</td>
<td>$\mathbf{8 6 . 2}(+1.0)$</td>
</tr>
<tr>
<td>Ordinal</td>
<td>the second largest number of runs was 8529</td>
<td>83.8</td>
<td>$\mathbf{8 6 . 9}(+3.1)$</td>
</tr>
<tr>
<td>Unique</td>
<td>there are 5 different nations in the tournament</td>
<td>74.2</td>
<td>$\mathbf{7 9 . 1}(+4.9)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Binary classification accuracy (\%) on sentence-table pairs containing different types of operations. The six sets are sampled from TabFact based on trigger words, and each set contains 200 sentence-table pairs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Scheme</th>
<th style="text-align: center;">Val</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Simple</th>
<th style="text-align: center;">Complex</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MLM</td>
<td style="text-align: center;">$84.8_{ \pm 0.2}$</td>
<td style="text-align: center;">$84.9_{ \pm 0.2}$</td>
<td style="text-align: center;">$92.5_{ \pm 0.1}$</td>
<td style="text-align: center;">$81.2_{ \pm 0.2}$</td>
</tr>
<tr>
<td style="text-align: left;">PASTA</td>
<td style="text-align: center;">$\mathbf{8 7 . 0}_{ \pm 0.1}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 9}_{ \pm 0.2}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 0}_{ \pm 0.2}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 9}_{ \pm 0.2}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation study for the masking scheme. MLM uses random masking and PASTA uses table-operations aware masking. To avoid co-effects, we didn't use any data pre-processing method on MLM or PASTA. Models are evaluated with 5 random runs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Val</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Simple</th>
<th style="text-align: center;">Complex</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PASTA</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: left;">w/o col</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">85.5</td>
</tr>
<tr>
<td style="text-align: left;">w/o row</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">84.7</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation study for the select-then-rank strategy. "w/o col" means that the column-wise selection strategy is not used, and "w/o row" means that the row-wise ranking strategy is not used.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># Row</th>
<th style="text-align: center;"># Col</th>
<th style="text-align: center;"># Cell</th>
<th style="text-align: center;">\% Mul-Ops</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">All Test</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTaV3</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">107.4</td>
<td style="text-align: center;">13.4</td>
</tr>
<tr>
<td style="text-align: left;">PASTA</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">16.5</td>
</tr>
</tbody>
</table>
<p>Table 7: Statistics of data collected from PASTA and DeBERTaV3's error sets. Proportion of statements that contain more than two operations is marked as "\% MulOps". "# Row", "# Col", "# Cell" respectively represent the average numbers of rows, columns, and cells.</p>
<h2>6 Related Work</h2>
<p>Sentence-Table Joint Understanding Many tasks, such as table question answering, table search, table-to-text, and table-based fact verification, require understanding tables and an NL sentence jointly. TAPAS (Herzig et al., 2020) and FORTAP (Cheng et al., 2021) design sentence-table joint pre-training tasks. TAPAS further leverages Whole Word Masking (WWM) to learn better representations of tables, while FORTAP leverages Numerical Reference Prediction (NRP) and Numerical Calculation Prediction (NCP) on a large corpus of spreadsheet formulas. These methods are closest to ours, but one focuses on the contextual information of NL and the other on the statistical characteristics of tables. Our method considers both, and improves the joint understanding of the model by means of operation-aware cloze tasks.
(Table-based) Fact Verification Program-driven methods such as LogicFactChecker (Zhong et al., 2020) and ProgVGAT (Yang et al., 2020) mainly focus on explicitly capturing logical operations in statements and representing them using a graph neural network to support fact verification. PLMdriven methods such as Table-BERT (Chen et al., 2020a), TAPAS (Eisenschlos et al., 2020), and SaMoE (Zhou et al., 2022) perform table-based fact verification tasks as an NLI task and apply a BERTlike model to encode sentence-table pairs. Our approach is more similar to SaMoE (Zhou et al., 2022). The main difference is that SaMoE uses different experts to solve different types of operations, while our method assumes that the operation combinations in statements are complex and diverse. We directly inject the atomic operations into the model through operation-aware pre-training, and then fine-tune the model to learn the various operation combinations on downstream datasets. However, PASTA is compatible with the mixture-of-experts framework, and thus we would evaluate PASTA with MoE structure on table-based fact verification datasets in the future.</p>
<h2>7 Conclusion and Future Work</h2>
<p>We introduced PASTA, a table-operations aware pretraining approach to train LMs for better performing fact verification over tables. PASTA achieved new SOTA results on two widely-adopted tablebased fact verification benchmark datasets, TabFact and SEM-TAB-FACTS. Future work should explore how to address the challenges of more complex operations and large tables in fact verification.</p>
<h2>Limitations</h2>
<p>The first limitation of our work is that our synthetic pre-training corpus may lack diversity. As explained in Section 3.2, to ensure the correctness and controllability of these sentences, we generate the pre-training corpus using human-designed natural language templates. While our insight is that only atomic operations need to be learned at pre-training, which reduces the need for diversity, generating high-quality sentences with both diversity and controllability to support self-supervised learning is still a direction worth exploring.</p>
<p>The second limitation of our work is that fact verification is only supported on a single table. Although the TabFact (Chen et al., 2020a) dataset we used assumes that each statement can be verified by a single table, a more realistic scenario would be to combine information from multiple tables. Exploring how to do this effectively and to overcome the limitation of the input length of BERT-like models is a important direction for future work.</p>
<h2>Ethics Statement</h2>
<p>Dataset Collection For the pre-training dataset, we use the publicly available WikiTables (Bhagavatula et al., 2013) dataset as the table source and select high-quality relational tables from it. Then we use these tables to generate entailed statements, instead of collecting statements from the web. For the fine-tuning dataset, we use the publicly available datasets, TabFact (Chen et al., 2020a) and SEMTAB-FACTS (Wang et al., 2021b).
Intended Use and Misuse Potential The goal of the fact verification task is to help identify misinformation. Our work focuses on verifying the statements based on analysis over tables and aims to pre-train language models to be aware of common table operations, such as aggregation over a column or comparing two tuples. It should be noted that while we treat the tables from TabFact and SEMTAB-FACTS as trustworthy sources of evidence in the experiments, we do not assume that all of the tables in the network are trustworthy and unbiased. So, this work could also be misused through fact verification on unreliable or socially biased tables.
Broader Impact First, our operation-aware pretraining approach potentially has a broad impact on sentence-table joint understanding tasks. Second, we also notice that training a large-scale pre-trained language model requires the use of GPUs/TPUs
for training, which contributes to global warming. However, we're pre-training from a public checkpoint, not from scratch. And our pre-training dataset contains only 1.2 million examples, which is very small compared to other related work (Eisenschlos et al., 2020; Liu et al., 2021).</p>
<h2>Acknowledgements</h2>
<p>This work was partly supported by the NSF of China (62122090, 62072461, and U1911203) and the fund for building world-class universities (disciplines) of Renmin University of China.</p>
<h2>References</h2>
<p>Chandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2013. Methods for exploring and mining tables on wikipedia. In Proceedings of the ACM SIGKDD Workshop on Interactive Data Exploration and Analytics, IDEA@KDD 2013, Chicago, Illinois, USA, August 11, 2013, pages 18-26. ACM.</p>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020a. Tabfact: A large-scale dataset for table-based fact verification. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian D. Davison. 2020b. Table search using a deep contextualized language model. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, pages 589-598. ACM.</p>
<p>Zhoujun Cheng, Haoyu Dong, Fan Cheng, Ran Jia, Pengfei Wu, Shi Han, and Dongmei Zhang. 2021. FORTAP: using formulae for numerical-reasoningaware table pretraining. CoRR, abs/2109.07323.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics.</p>
<p>Julian Martin Eisenschlos, Syrine Krichene, and Thomas Müller. 2020. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 281-296. Association for Computational Linguistics.</p>
<p>Devansh Gautam, Kshitij Gupta, and Manish Shrivastava. 2021. Volta at semeval-2021 task 9: Statement verification and evidence finding with tables using TAPAS and transfer learning. In Proceedings of the 15th International Workshop on Semantic Evaluation, SemEval@ACL/IJCNLP 2021, Virtual Event / Bangkok, Thailand, August 5-6, 2021, pages 12621270. Association for Computational Linguistics.</p>
<p>Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021a. Debertav3: Improving debería using electra-style pretraining with gradient-disentangled embedding sharing. CoRR, abs/2111.09543.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021b. Deberta: decoding-enhanced bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. 2020. Tapas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4320-4333. Association for Computational Linguistics.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Qian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and JianGuang Lou. 2021. TAPEX: table pre-training via learning a neural SQL executor. CoRR, abs/2107.07653.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Thomas Müller, Julian Eisenschlos, and Syrine Krichene. 2021. TAPAS at semeval-2021 task 9: Reasoning over tables with intermediate pre-training. In Proceedings of the 15th International Workshop on Semantic Evaluation, SemEval@ACL/IJCNLP 2021, Virtual Event / Bangkok, Thailand, August 5-6, 2021, pages 423-430. Association for Computational Linguistics.</p>
<p>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 309-319. The Association for Computer Linguistics.</p>
<p>Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, and Gerhard Weikum. 2017. Where the truth lies: Explaining the credibility of emerging claims on the web and social media. In WWW, pages 1003-1012. ACM.</p>
<p>Michael Sejr Schlichtkrull, Vladimir Karpukhin, Barlas Oguz, Mike Lewis, Wen-tau Yih, and Sebastian Riedel. 2021. Joint verification and reranking for open fact checking over tables. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 6787-6799. Association for Computational Linguistics.</p>
<p>Shaden Shaar, Nikolay Babulkov, Giovanni Da San Martino, and Preslav Nakov. 2020. That is a known lie: Detecting previously fact-checked claims. In ACL, pages 3607-3618. Association for Computational Linguistics.</p>
<p>Tianze Shi, Chen Zhao, Jordan L. Boyd-Graber, Hal Daumé III, and Lillian Lee. 2020. On the potential of lexico-logical alignments for semantic parsing to SQL queries. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pages 1849-1864. Association for Computational Linguistics.</p>
<p>Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017. Fake news detection on social media: A data mining perspective. SIGKDD Explor., 19(1):22-36.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and verification. In NAACL-HLT, pages 809-819. Association for Computational Linguistics.</p>
<p>Fei Wang, Kexuan Sun, Jay Pujara, Pedro A. Szekely, and Muhao Chen. 2021a. Table-based fact verification with salience-aware learning. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 4025-4036. Association for Computational Linguistics.</p>
<p>Nancy Xin Ru Wang, Diwakar Mahajan, Marina Danilevsky, and Sara Rosenthal. 2021b. Semeval2021 task 9: Fact verification and evidence finding for tabular data in scientific documents (SEM-TAB-FACTS). In Proceedings of the 15th Inter-</p>
<p>national Workshop on Semantic Evaluation, SemEval@ACL/IJCNLP 2021, Virtual Event / Bangkok, Thailand, August 5-6, 2021, pages 317-326. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, and Xiaodan Zhu. 2020. Program enhanced fact verification with verbalization and graph attention network. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 7810-7825. Association for Computational Linguistics.</p>
<p>Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. Tabert: Pretraining for joint understanding of textual and tabular data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 8413-8426. Association for Computational Linguistics.</p>
<p>Seunghyun Yoon, Kunwoo Park, Joongbo Shin, Hongjun Lim, Seungpil Won, Meeyoung Cha, and Kyomin Jung. 2019. Detecting incongruity between news headline and body text via a deep hierarchical encoder. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 791-800. AAAI Press.</p>
<p>Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi Cao, Fuzheng Zhang, and Zhongyuan Wang. 2020. Table fact verification with structure-aware transformer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 1624-1629. Association for Computational Linguistics.</p>
<p>Guangzhen Zhao and Peng Yang. 2022. Table-based fact verification with self-labeled keypoint alignment. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 1401-1411. International Committee on Computational Linguistics.</p>
<p>Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, and Jian Yin. 2020. Logicalfactchecker: Leveraging logical operations for fact checking with graph module network. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6053-6065. Association for Computational Linguistics.</p>
<p>Yuxuan Zhou, Xien Liu, Kaiyin Zhou, and Ji Wu. 2022. Table-based fact verification with self-adaptive mixture of experts. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 139-149. Association for Computational Linguistics.</p>
<h2>A Details of Pre-training Corpus</h2>
<p>For the six table-aware operation types, we design a total of 50 NL-SQL template pairs to generate sentence-table instances for pre-training. Table 8 shows two examples of each type of operations. We first populate the NL-SQL template pairs with the specific content in the table $T$, and then the context sensitive words $w^{\prime}$ in the NL template are selected by a fixed pre-trained LM in its synonym set to improve the fluency of the generated sentences. We define four context-sensitive word candidate sets, which are presented in Table 9.</p>
<h2>B Trigger Words Definition</h2>
<p>Table 10 shows the trigger words we have used to identify different operation types in Section 5.2. These trigger words are expanded from the trigger words defined in TabFact (Yin et al., 2020), and are then classified according to the six table-aware operation types introduced in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Operation</th>
<th style="text-align: center;">NL Template</th>
<th style="text-align: center;">SQL Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Filter</td>
<td style="text-align: center;">[Value2]'s [Column1] is [ANS].</td>
<td style="text-align: center;">SELECT [Column1] FROM T WHERE [Column2] = [Value1]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">the [Column1] of [Value2] is [ANS].</td>
<td style="text-align: center;">SELECT [Column1] FROM T WHERE [Column2] = [Value1]</td>
</tr>
<tr>
<td style="text-align: center;">Superlative</td>
<td style="text-align: center;">the highest [Column1] is [ANS]</td>
<td style="text-align: center;">SELECT MAX([Column1]) FROM T</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[ANS] has the highest [Column2] of all [Column1]</td>
<td style="text-align: center;">SELECT [Column1] FROM T ORDER BY [Column2] DESC LIMIT 1</td>
</tr>
<tr>
<td style="text-align: center;">Aggregation</td>
<td style="text-align: center;">the sum of [Column1] when [Column2] is [Value2] is [ANS].</td>
<td style="text-align: center;">SELECT SUM([Column1]) FROM T WHERE [Column2] $=[\text { Value } 2]$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">the average of [Column1] when [Column2] is [Value2] is [ANS].</td>
<td style="text-align: center;">SELECT AVG([Column1]) FROM T WHERE [Column2] $=[\text { Value } 2]$</td>
</tr>
<tr>
<td style="text-align: center;">Comparative</td>
<td style="text-align: center;">[Column1] [ANS]'s [Column2] is higher than [Value2]</td>
<td style="text-align: center;">SELECT [Column1] FROM t WHERE [Column2] &gt; [Value2]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[ANS] has higher [Column2] than [Value2]</td>
<td style="text-align: center;">SELECT [Column1] FROM t WHERE [Column2] &gt; [Value2]</td>
</tr>
<tr>
<td style="text-align: center;">Ordinal</td>
<td style="text-align: center;">[ANS] has the second highest [Column2]</td>
<td style="text-align: center;">SELECT [Column1] FROM T WHERE [Column2] &lt; ( SELECT MAX([Column2]) FROM T ) ORDER BY [Column2] DESC LIMIT 1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">[ANS] has the second lowest [Column2]</td>
<td style="text-align: center;">SELECT [Column1] FROM T WHERE [Column2] &gt; ( SELECT MIN([Column2]) FROM T ) ORDER BY [Column2] ASC LIMIT 1</td>
</tr>
<tr>
<td style="text-align: center;">Unique</td>
<td style="text-align: center;">there are [ANS] different [Column1] on the list.</td>
<td style="text-align: center;">SELECT COUNT( DISTINCT [Column1] ) FROM T</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">the total number of different [Column1] is [ANS].</td>
<td style="text-align: center;">SELECT COUNT( DISTINCT [Column1] ) FROM T</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of NL Templates and SQL Templates for each table-aware operation type. The words in each NL Template that need to be polished by the pre-trained LM are marked in red. The operation-aware tokens that need to be predicted during pre-training are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$w^{t}$</th>
<th style="text-align: left;">Candidate Set</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">"highest"</td>
<td style="text-align: left;">"highest", "most", "biggest", "largest", "oldest", "greatest", "heaviest", "longest", <br> "tallest"</td>
</tr>
<tr>
<td style="text-align: center;">"lowest"</td>
<td style="text-align: left;">"lowest", "least", "smallest", "youngest", "shortest"</td>
</tr>
<tr>
<td style="text-align: center;">"higher"</td>
<td style="text-align: left;">"higher", "more", "bigger", "larger", "older"</td>
</tr>
<tr>
<td style="text-align: center;">"less"</td>
<td style="text-align: left;">"less", "smaller", "lower", "younger"</td>
</tr>
</tbody>
</table>
<p>Table 9: Four context-sensitive word candidate sets. $w^{t}$ refers to the context-sensitive word in the NL templates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Operation</th>
<th style="text-align: left;">Trigger Words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Filter</td>
<td style="text-align: left;">"is", "was", "are", "were"</td>
</tr>
<tr>
<td style="text-align: left;">Superlative</td>
<td style="text-align: left;">"lowest", "least", "smallest", "youngest", "shortest", "first", "best", "newest", "latest"</td>
</tr>
<tr>
<td style="text-align: left;">Aggregation</td>
<td style="text-align: left;">"average", "count", "sum", "total"</td>
</tr>
<tr>
<td style="text-align: left;">Comparative</td>
<td style="text-align: left;">"than", "higher", "more", "bigger", "larger", "older", "less", "smaller", "lower", <br> "younger", "same", "equal"</td>
</tr>
<tr>
<td style="text-align: left;">Ordinal</td>
<td style="text-align: left;">"second", "third", "fourth"</td>
</tr>
<tr>
<td style="text-align: left;">Unique</td>
<td style="text-align: left;">"different"</td>
</tr>
</tbody>
</table>
<p>Table 10: Trigger words for each table-aware operation type.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://huggingface.co/microsoft/ debería-v3-large&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ http://reporterslab.org/
fact-checking-count-tops-300-for-the-first-time/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
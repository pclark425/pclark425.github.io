<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1787 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1787</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1787</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-3ed67ded2b4d3614b38798b3f17a8e69803d0980</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3ed67ded2b4d3614b38798b3f17a8e69803d0980" target="_blank">Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties.</p>
                <p><strong>Paper Abstract:</strong> Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn't work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1787.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1787.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Inverse Dynamics Transfer (MuJoCo locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer via Learning Deep Inverse Dynamics Model (Sim1 -> Sim2 experiments in MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-sim (used to study sim-to-real) transfer method that runs a source-domain policy in simulation to predict next observations, then uses a learned inverse dynamics neural network trained on the target domain to produce target-domain actions that achieve those next observations; evaluated on MuJoCo locomotion tasks (Reacher, Hopper, Half-Cheetah, Humanoid).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Reacher, Hopper, Half-Cheetah, Humanoid (MuJoCo robots)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Standard simulated articulated robot models from OpenAI Gym / MuJoCo: a 2-link Reacher arm, a 2D Hopper, a 2D bipedal Half-Cheetah, and a 3D Humanoid; used to evaluate locomotion/reaching policies and dynamics adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (via OpenAI Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A rigid-body physics simulator (MuJoCo) providing reduced-coordinate multibody dynamics, contact handling, joint positions and velocities; used with OpenAI Gym task wrappers.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>rigid-body dynamics, high-performance numerically-smooth simulator (MuJoCo); not a full high-fidelity FEM/fluid/soft-body simulator</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>multibody rigid dynamics, joint kinematics/velocities, approximate contact modelling between rigid bodies, actuator torques (as modeled in MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>detailed contact micro-phenomena, complex deformable-body effects, fluids, accurate hysteresis/backlash/motor temperature effects, some fine frictional details and possibly exact sensor noise models were not modeled or are approximate</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>N/A for this entry (both source and target were simulators, Sim1 -> Sim2 transfer to study capability under controlled physical-parameter variations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Locomotion and reaching policies (trajectory following / gait/ hopping / reaching) from a source simulator to a target simulator with modified physical parameters (mass, link lengths, friction, torque scale/limits, gravity).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Source expert policies: reinforcement learning (Trust Region Policy Optimization, TRPO). Inverse dynamics model: supervised learning (deep neural network regression) trained on target-domain trajectories collected by executing the current inverse model and injecting exploration noise.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Simulated tasks: normalized cumulative reward (normalized so expert policy performance = 1); success measured as fraction of expert performance (e.g., reaching 3/4 of expert performance used as a convergence threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Changes in mass, link lengths, friction coefficients, torque scale and limits, gravity, motor noise (uncorrelated and slowly-varying), contact discontinuities, stiction, backlash and other unmodeled low-level actuator/dynamics effects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Availability of a source-domain forward dynamics (simulator) to predict desired next observations; learning an inverse dynamics model on target-domain data (including history) so it can map desired next observation to a real action; collecting training data in conditions similar to test-time trajectories (interleaved execution and learning), injecting exploration noise carefully (not too much, not too often), using history windows to capture latent temporal effects, and using action-correction output when domains are similar.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No strict numerical fidelity thresholds given; authors argue that a numerically smooth rigid-body simulator (MuJoCo) suffices as long as inverse dynamics are learned on real/target data. They emphasize that contact dynamics and unmodeled low-level actuator effects matter, but their approach can compensate for many such discrepancies without requiring a much more expensive high-fidelity simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning an inverse dynamics model on the target domain that maps a simulator-predicted next observation to a real action enables robust transfer of policies across simulated domains with substantial parameter differences (gravity, mass, friction, motor noise). The method outperforms online forward-model adaptation baselines in contact-rich and unstable tasks because inverse dynamics avoid unstable forward-model-based optimization; including temporal history in the inverse model improves sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1787.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1787.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Inverse Dynamics Transfer (MuJoCo -> Fetch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer via Deep Inverse Dynamics Model: Sim-to-Real transfer to a physical Fetch robot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same inverse-dynamics-mediated transfer approach applied to transfer trajectories/policies from a MuJoCo Fetch simulation onto a real Fetch robot by predicting simulator next-observations and using a learned inverse dynamics network on the real robot to produce actions that achieve those observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Fetch robot (physical)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A service-class manipulator mobile platform (Fetch robot) controlled in position-control mode through ROS at 10 Hz; experiments focus on the arm, including an agile back-and-forth swing with a bungee cord on the arm.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (physical robot arm)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (custom Fetch model / modified Gym environment)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Rigid-body simulation of the Fetch robot (MuJoCo) modeling joint kinematics, velocities and approximate contact/actuation for trajectory generation; used as the source domain to generate desired next observations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>rigid-body actuator/joint simulator (MuJoCo) — high-performance, numerically smooth but simplified relative to physical robot details (e.g., ignores some actuator nonlinearities and fine frictional/hysteresis effects).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>joint kinematics and velocities, multibody rigid dynamics, gravity, actuator commands as in MuJoCo model, approximate contact interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>motor/backlash hysteresis, bungee elasticity interactions beyond rigid approximations, precise actuator calibration, low-level motor dynamics, time-correlated actuator noise, and sensor/firmware specific behaviors were simplified or not modeled in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical Fetch robot arm controlled at 10 Hz with stock ROS firmware; task involved swinging the arm back-and-forth with a bungee attached to the arm (introducing complex external forces). Real-world observations used were joint positions/velocities (no visual sensors reported).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Agile swinging trajectory of the arm (trajectory following under external bungee force), i.e., transferring a simulated trajectory to be tracked by the real robot under unmodeled external dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Source policy / source trajectories: generated in simulation (MuJoCo) potentially with optimization/RL methods; inverse dynamics in the real robot trained by supervised learning on collected real trajectories produced by executing the current inverse model with injected exploration noise and intermittently random actions to gather labeled (state, next-state) → action pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Normalized distance between observations achieved in simulator and observations achieved on the physical robot (trajectory tracking error), reported as percentage distance to desired trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported table entries: one reported value 3.72% ± 0.020% and another 4.49% ± 0.050% (table in paper shows these two numbers for the task; the table labeling in the paper is ambiguous as to which number maps to which method).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Unmodeled external forces (bungee), actuator calibration and nonlinearities, motor/backlash/hysteresis effects, time-correlated motor noise, mismatch in torque/position control dynamics and frequency (10 Hz in real robot), and simplifications in the simulator regarding these effects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training an inverse dynamics model on the physical robot via interleaved data collection that is similar to test trajectories (short targets, reset when deviating), injecting exploration noise at controlled frequency/amplitude, using history windows in the inverse model to capture temporal dependencies and latent factors, and relying on simulator forward predictions so the inverse model only needs to map desired next observations to real actions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The authors emphasize that exact high-fidelity simulation of all low-level effects is not required if an inverse dynamics model is trained on real data; however, they note that observation mismatch (e.g., visual sensors) would require explicit observation adaptation and that contact and actuator effects can be critical if not compensated via target-domain learning.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Yes — the inverse dynamics network φ is trained on real robot trajectories collected by executing the current φ-based policy with injected noise; data collection is interleaved with training, exploration is controlled (noise not added every step), and resets are used when execution deviates far from expected simulator trajectories. Reported sample complexity across tasks ranged from thousands to low-hundreds-of-thousands of samples to reach a performance threshold (paper lists sample counts such as on the order of 13k–157k for different tasks, and contrasts this with learning-from-scratch RL requirements of millions to tens of millions of samples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The inverse-dynamics-mediated transfer approach can enable sim-to-real transfer even in the presence of difficult-to-model external forces (bungee) and actuator nonlinearities by learning a real-world inverse dynamics mapping. Careful data collection (similar to target trajectories), inclusion of history, and controlled exploration noise are important; the method is sample-efficient relative to learning policies from scratch and outperforms forward-model adaptation baselines in contact-rich or discontinuous dynamics settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model', 'publication_date_yy_mm': '2016-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>One-shot learning of manipulation skills with online dynamics adaptation and neural network priors <em>(Rating: 2)</em></li>
                <li>Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids <em>(Rating: 2)</em></li>
                <li>Combining model-based policy search with online model learning for control of physical humanoids <em>(Rating: 2)</em></li>
                <li>Towards adapting deep visuomotor representations from simulated to real environments <em>(Rating: 1)</em></li>
                <li>Deep learning helicopter dynamics models <em>(Rating: 1)</em></li>
                <li>Pilco: A model-based and data-efficient approach to policy search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1787",
    "paper_id": "paper-3ed67ded2b4d3614b38798b3f17a8e69803d0980",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Deep Inverse Dynamics Transfer (MuJoCo locomotion)",
            "name_full": "Transfer via Learning Deep Inverse Dynamics Model (Sim1 -&gt; Sim2 experiments in MuJoCo)",
            "brief_description": "A sim-to-sim (used to study sim-to-real) transfer method that runs a source-domain policy in simulation to predict next observations, then uses a learned inverse dynamics neural network trained on the target domain to produce target-domain actions that achieve those next observations; evaluated on MuJoCo locomotion tasks (Reacher, Hopper, Half-Cheetah, Humanoid).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Reacher, Hopper, Half-Cheetah, Humanoid (MuJoCo robots)",
            "agent_system_description": "Standard simulated articulated robot models from OpenAI Gym / MuJoCo: a 2-link Reacher arm, a 2D Hopper, a 2D bipedal Half-Cheetah, and a 3D Humanoid; used to evaluate locomotion/reaching policies and dynamics adaptation.",
            "domain": "general robotics manipulation / locomotion",
            "virtual_environment_name": "MuJoCo (via OpenAI Gym)",
            "virtual_environment_description": "A rigid-body physics simulator (MuJoCo) providing reduced-coordinate multibody dynamics, contact handling, joint positions and velocities; used with OpenAI Gym task wrappers.",
            "simulation_fidelity_level": "rigid-body dynamics, high-performance numerically-smooth simulator (MuJoCo); not a full high-fidelity FEM/fluid/soft-body simulator",
            "fidelity_aspects_modeled": "multibody rigid dynamics, joint kinematics/velocities, approximate contact modelling between rigid bodies, actuator torques (as modeled in MuJoCo)",
            "fidelity_aspects_simplified": "detailed contact micro-phenomena, complex deformable-body effects, fluids, accurate hysteresis/backlash/motor temperature effects, some fine frictional details and possibly exact sensor noise models were not modeled or are approximate",
            "real_environment_description": "N/A for this entry (both source and target were simulators, Sim1 -&gt; Sim2 transfer to study capability under controlled physical-parameter variations).",
            "task_or_skill_transferred": "Locomotion and reaching policies (trajectory following / gait/ hopping / reaching) from a source simulator to a target simulator with modified physical parameters (mass, link lengths, friction, torque scale/limits, gravity).",
            "training_method": "Source expert policies: reinforcement learning (Trust Region Policy Optimization, TRPO). Inverse dynamics model: supervised learning (deep neural network regression) trained on target-domain trajectories collected by executing the current inverse model and injecting exploration noise.",
            "transfer_success_metric": "Simulated tasks: normalized cumulative reward (normalized so expert policy performance = 1); success measured as fraction of expert performance (e.g., reaching 3/4 of expert performance used as a convergence threshold).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Changes in mass, link lengths, friction coefficients, torque scale and limits, gravity, motor noise (uncorrelated and slowly-varying), contact discontinuities, stiction, backlash and other unmodeled low-level actuator/dynamics effects.",
            "transfer_enabling_conditions": "Availability of a source-domain forward dynamics (simulator) to predict desired next observations; learning an inverse dynamics model on target-domain data (including history) so it can map desired next observation to a real action; collecting training data in conditions similar to test-time trajectories (interleaved execution and learning), injecting exploration noise carefully (not too much, not too often), using history windows to capture latent temporal effects, and using action-correction output when domains are similar.",
            "fidelity_requirements_identified": "No strict numerical fidelity thresholds given; authors argue that a numerically smooth rigid-body simulator (MuJoCo) suffices as long as inverse dynamics are learned on real/target data. They emphasize that contact dynamics and unmodeled low-level actuator effects matter, but their approach can compensate for many such discrepancies without requiring a much more expensive high-fidelity simulator.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Learning an inverse dynamics model on the target domain that maps a simulator-predicted next observation to a real action enables robust transfer of policies across simulated domains with substantial parameter differences (gravity, mass, friction, motor noise). The method outperforms online forward-model adaptation baselines in contact-rich and unstable tasks because inverse dynamics avoid unstable forward-model-based optimization; including temporal history in the inverse model improves sample efficiency.",
            "uuid": "e1787.0",
            "source_info": {
                "paper_title": "Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model",
                "publication_date_yy_mm": "2016-10"
            }
        },
        {
            "name_short": "Deep Inverse Dynamics Transfer (MuJoCo -&gt; Fetch)",
            "name_full": "Transfer via Deep Inverse Dynamics Model: Sim-to-Real transfer to a physical Fetch robot",
            "brief_description": "The same inverse-dynamics-mediated transfer approach applied to transfer trajectories/policies from a MuJoCo Fetch simulation onto a real Fetch robot by predicting simulator next-observations and using a learned inverse dynamics network on the real robot to produce actions that achieve those observations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Fetch robot (physical)",
            "agent_system_description": "A service-class manipulator mobile platform (Fetch robot) controlled in position-control mode through ROS at 10 Hz; experiments focus on the arm, including an agile back-and-forth swing with a bungee cord on the arm.",
            "domain": "general robotics manipulation (physical robot arm)",
            "virtual_environment_name": "MuJoCo (custom Fetch model / modified Gym environment)",
            "virtual_environment_description": "Rigid-body simulation of the Fetch robot (MuJoCo) modeling joint kinematics, velocities and approximate contact/actuation for trajectory generation; used as the source domain to generate desired next observations.",
            "simulation_fidelity_level": "rigid-body actuator/joint simulator (MuJoCo) — high-performance, numerically smooth but simplified relative to physical robot details (e.g., ignores some actuator nonlinearities and fine frictional/hysteresis effects).",
            "fidelity_aspects_modeled": "joint kinematics and velocities, multibody rigid dynamics, gravity, actuator commands as in MuJoCo model, approximate contact interactions.",
            "fidelity_aspects_simplified": "motor/backlash hysteresis, bungee elasticity interactions beyond rigid approximations, precise actuator calibration, low-level motor dynamics, time-correlated actuator noise, and sensor/firmware specific behaviors were simplified or not modeled in simulation.",
            "real_environment_description": "Physical Fetch robot arm controlled at 10 Hz with stock ROS firmware; task involved swinging the arm back-and-forth with a bungee attached to the arm (introducing complex external forces). Real-world observations used were joint positions/velocities (no visual sensors reported).",
            "task_or_skill_transferred": "Agile swinging trajectory of the arm (trajectory following under external bungee force), i.e., transferring a simulated trajectory to be tracked by the real robot under unmodeled external dynamics.",
            "training_method": "Source policy / source trajectories: generated in simulation (MuJoCo) potentially with optimization/RL methods; inverse dynamics in the real robot trained by supervised learning on collected real trajectories produced by executing the current inverse model with injected exploration noise and intermittently random actions to gather labeled (state, next-state) → action pairs.",
            "transfer_success_metric": "Normalized distance between observations achieved in simulator and observations achieved on the physical robot (trajectory tracking error), reported as percentage distance to desired trajectory.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Reported table entries: one reported value 3.72% ± 0.020% and another 4.49% ± 0.050% (table in paper shows these two numbers for the task; the table labeling in the paper is ambiguous as to which number maps to which method).",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Unmodeled external forces (bungee), actuator calibration and nonlinearities, motor/backlash/hysteresis effects, time-correlated motor noise, mismatch in torque/position control dynamics and frequency (10 Hz in real robot), and simplifications in the simulator regarding these effects.",
            "transfer_enabling_conditions": "Training an inverse dynamics model on the physical robot via interleaved data collection that is similar to test trajectories (short targets, reset when deviating), injecting exploration noise at controlled frequency/amplitude, using history windows in the inverse model to capture temporal dependencies and latent factors, and relying on simulator forward predictions so the inverse model only needs to map desired next observations to real actions.",
            "fidelity_requirements_identified": "The authors emphasize that exact high-fidelity simulation of all low-level effects is not required if an inverse dynamics model is trained on real data; however, they note that observation mismatch (e.g., visual sensors) would require explicit observation adaptation and that contact and actuator effects can be critical if not compensated via target-domain learning.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Yes — the inverse dynamics network φ is trained on real robot trajectories collected by executing the current φ-based policy with injected noise; data collection is interleaved with training, exploration is controlled (noise not added every step), and resets are used when execution deviates far from expected simulator trajectories. Reported sample complexity across tasks ranged from thousands to low-hundreds-of-thousands of samples to reach a performance threshold (paper lists sample counts such as on the order of 13k–157k for different tasks, and contrasts this with learning-from-scratch RL requirements of millions to tens of millions of samples).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "The inverse-dynamics-mediated transfer approach can enable sim-to-real transfer even in the presence of difficult-to-model external forces (bungee) and actuator nonlinearities by learning a real-world inverse dynamics mapping. Careful data collection (similar to target trajectories), inclusion of history, and controlled exploration noise are important; the method is sample-efficient relative to learning policies from scratch and outperforms forward-model adaptation baselines in contact-rich or discontinuous dynamics settings.",
            "uuid": "e1787.1",
            "source_info": {
                "paper_title": "Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model",
                "publication_date_yy_mm": "2016-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "One-shot learning of manipulation skills with online dynamics adaptation and neural network priors",
            "rating": 2
        },
        {
            "paper_title": "Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids",
            "rating": 2
        },
        {
            "paper_title": "Combining model-based policy search with online model learning for control of physical humanoids",
            "rating": 2
        },
        {
            "paper_title": "Towards adapting deep visuomotor representations from simulated to real environments",
            "rating": 1
        },
        {
            "paper_title": "Deep learning helicopter dynamics models",
            "rating": 1
        },
        {
            "paper_title": "Pilco: A model-based and data-efficient approach to policy search",
            "rating": 1
        }
    ],
    "cost": 0.0114285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model</h1>
<p>Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba<br>OpenAI, San Francisco, CA, USA</p>
<h4>Abstract</h4>
<p>Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesnt work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.</p>
<h2>I. INTRODUCTION</h2>
<p>Many methods exist for generating control policies in simulated environments, including methods based on motion planning, optimization, control, and learning. However, an important practical challenge is that often there are discrepancies between simulation and the real world, which results in policies that work well in simulation yet perform poorly in the real world.</p>
<p>Significant bodies of work exist that strive to address this challenge. One important line of work studies how to improve simulators to better match reality, which involves improving simulation of contact, non-rigidity, friction, as well as improving identification of physical quantities needed for accurate simulation such as mass, geometry, friction coefficients, elasticity. However, despite significant progress, discrepancies continue to exist, and more accurate simulation can have the downside of being slower.</p>
<p>Another important line of work studies robustness of control policies, which could be measured through, for example, gain and phase margins, and robust control methods exist that can optimize for these. Optimizing for robustness means finding control policies that apply across a wide range of
possible real worlds, but unfortunately tends to come at the expense of performance in the one specific real world the system is faced with.</p>
<p>Adaptive methods, which is the topic of this paper, do not use the same policy for the entire family of possible environments, but rather try to learn about the specific real world the system is faced with. In principle, such methods can exploit the physics of the real world and behave in the optimal way.</p>
<p>Concretely, our work considers the following problem setting: We assume to be given a simulator and a method for generating policies that perform well in simulation. The goal is to leverage this to perform well in new real-world situations. To achieve this, a training period exists during which an adaptation mechanism can be trained to learn to adapt from simulation to real world by collecting experience on the real system, but without having access to the new real-world situations that the system will be evaluated on later.</p>
<p>We leverage the following intuition: Often policies found from simulation capture the high-level gist well (e.g., overall trajectory), but fail to accurately capture some of the lowerlevel details, such as friction, stiction, backlash, hysteresis, precise measurements, precise deformation, etc. Indeed, this is the type of situation that motivates the work in this paper and in which we will be evaluating our approach (as well as baselines).</p>
<p>Note that while we assume that a method exists for generating policies in simulation, our approach is agnostic to the details of this method, which could be based on any techniques from motion planning, optimization, control, learning, and others, which return a policy, which could be a model-predictive policy which uses the simulator in its inner loop.</p>
<p>Our approach proceeds as follows: During execution on a test trajectory, at each time step it computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. As our experiments show, when these inverse dynamics models are trained on sufficient data, this results in compelling transfer from simulation to real world, in particular with challenging dynamics involving</p>
<p>contact and collision. To collect the training data, there is a training phase which proceeds the same way, but only has access to a poor inverse dynamics model, and then uses the collected data to improve the model. Our experiments show that having the training data collection be similar to the test time conditions improves results significantly compared to data collection based on just applying random controls. To maximize data collection efficiency, target trajectories for training are initially short (or cut short once significantly deviating from the target).</p>
<p>Our experiments validate the applicability of our approach through two families of experiments: (i) Sim1 to Sim2 Transfer: To better understand the transfer capabilities, we first study transfer from one simulation (Sim1) to another simulation (Sim2). We consider several standard tasks: Reacher, Hopper, Cheetah, Humanoid from MuJoCo / OpenAI Gym [38] [3]. For each experiment Sim2 has the same type of robot as Sim1, but the physical properties are different (change in mass, link lengths, friction coefficients, torque scale and limits). (ii) Sim to Real Transfer with Fetch: In this family of experiments we study transfer of policies that work well for a simulated Fetch robot onto a real Fetch robot. To calibrate performance, we consider as a baseline a PD controller tuned for our Fetch robot.</p>
<p>We compare our approach with output error control [24] and Gaussian Dynamics Adaptation [11], two established approaches to handle mismatch between simulation and real world.</p>
<h2>II. RELATED WORK</h2>
<p>Simulation has been an invaluable tool in advancing the development of robotics and many simulation techniques have been developed over the years. Reduced coordinate rigid multibody dynamics are especially suited for simulating articulated robots [9]. Unfortunately, many significant physical effects may not be possible to model with such simulation approaches. Flexible or inflatable bodies [35] [13], area contact [12], interaction with fluids [34] [31] are just a few of such examples. More accurate simulators, such as those based on Finite Element Method [14] can be used to more closely match such real world effects, but they can be extremely computationally intensive (requiring days to compute seconds of simulation) and furthermore can be numerically ill-conditioned, which makes them difficult to use within numerical trajectory or policy optimization methods. Our method allows the use of simple, high-performance, and numerically smooth rigid body simulators (we use MuJoCo [38]) for policy or trajectory optimization, while still being able to adapt to complex effects present in the real world.</p>
<p>Even if a simulator were capable of modeling all the physical effects of interest, it would still require detailed and accurate model parameters (such as mass distributions, material properties, etc.). A significant body of research has focused on identifying these parameters from observations of robots’ behavior in the real world, but tend to require separate specialized identification approaches and models for different robot platforms, such as legged robots [23], helicopters [26], or fixed-wing UAVs [18]. Furthermore, individual physical effects also require specialized expert-designed models and parameter identification methods, such as motor backlash [17], hydraulic actuation [6], series elastic actuation [30], or pneumatic actuation [36]. Our learned deep inverse dynamics models are based on past histories of observed states and in principle have the ability to model the above effects and platforms in one simple unified method without requiring any domain-specific manual model design and identification.</p>
<p>To remove the need for explicit dynamics, learning of dynamics models has received much attention in recent years. A number of approaches learn forward dynamics models functions mapping current state and action to a next state [24] [32]. Such functions can then be used to solve for actions that lead to desired next state. Alternatively, inverse dynamics models learn a mapping from current and next state to an action that achieves the transition between the two [29], [4], [25]. Such models are appealing because their output can be directly used for control, and is the model type we use in this work. The data for model learning is typically gathered in a batch fashion, either from random trajectories, or from representative demonstrations. This can be problematic if the robot state trajectories resulting from policy execution do not match the model training trajectories. An alternative is to learn dynamics models in an on-line fashion, constantly adapting the model based on an incoming stream of observed states and actions [11] [28] [43] [22]. These approaches however are slow to adapt to rapidly-changing dynamics modes (such as those arising when making or breaking contact) and may be problematic when applied on robots performing rapid motions. Another alternative is to iteratively intertwine data collection and dynamics model learning [7] [10]. Such approaches concentrate training data in the regions of the state space that are relevant for task completion and inspire the data collection procedure in our work.</p>
<p>A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11]. Linear functions are very efficient to evaluate and solve controls for, but have limited expressive power. Gaussian Processes are able to provide model uncertainty estimates, but are problematic to scale to large dataset sizes. Deep neural networks are an expressive class of functions independent of dataset size and are what we use in this work.</p>
<p>Our approach to transfer between simulator and the real world is based on adapting actions. There is a rich body of work focusing on adapting policies, rather than actions in the context of reinforcement learning [37] [1] [5]. Another alternative is to consider robust control methods in simulation that produce policies that are robust to mismatch between simulator and the real world [44] [27]. In addition to actions, adaptation of states and observations between simulation and the real world is another challenging problem [41] [16] [40] [8]. In the current work, we choose to focus solely on adaptation of actions and leave other types of adaptation for future work.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Overview of our method applied to Fetch robot in source simulator (bottom) and target physical world (top). Given an existing source domain policy π and forward dynamics T, we learn inverse dynamics neural network φ and use it to generate actions otarget for the physical Fetch robot at any given time instant.</p>
<h2>III. METHOD</h2>
<h3>A. Setting</h3>
<p>We study transfer from a source environment to a target environment. Typically the source environment would be a simulator, and the target environment would be a physical robot. However, in order to validate our method we start by having simulator both in the source and in the target domain. This setup has merit in developing an experimental understanding of our approach, as we can control the degree of variation between source and target environments. Our final experiments are in transfer from a simulator to the physical robot.</p>
<p>For each environment we denote the state space by S, the action space by A and the observation space by O. Points s ∈ S, a ∈ A, o ∈ O are states, actions, and observations. The state is not assumed observed. Overloading notation slightly, the agent makes noisy and incomplete observations of the underlying system, o = o(s) ∈ O, which typically don't expose some latent factors (e.g., fluctuating temperature or motor backlash). The special situation where the state is observed is readily captured by having the observation function o(s) = s. The system forward dynamics are given by a function from state-action pair to a new state: T(s, a) = s'.</p>
<p>We use subscripts to explicitly distinguish between the source environment and the target environment. For example, Asource denotes the action space in the source environment, and Atarget denotes the action space in the target environment.</p>
<p>A trajectory τ is a sequence of observations and actions: (o1,a1,o2,a2,...) We write τH:H+k to refer to the subsequence (oH,aH,...)oH+k-1,oH+k-1,oH+k). We write τ-k: to refer to the most recent k observations and k-1 actions in a trajectory, and τ-1 to refer to the most recent observation.</p>
<p>A policy π is a mapping from observations to actions, that depends on the last k observations, prescribing a = π(τ-k). Our goal is to find a policy πtarget that performs well in the target environment.</p>
<p>Rather than learning a policy for the target environment from scratch, we assume that we have access to a competent policy πsource in the source environment. Such policy could be obtained through any of a variety of methods, including motion planning, model-predictive or optimization-based control, reinforcement learning, etc. Our approach is agnostic to how the policy πsource was obtained.</p>
<h3>B. Transfer to the target environment</h3>
<p>Rather than directly executing πsource in the target environment, we seek to transfer the high-level properties of πsource to be re-used in the target environment, but not its lower-level specifics. Our approach is illustrated in Figure 1. During execution, we repeat the following at every time instant: consider the recent history of observations τ-k:, compute the action asource = πsource(τ-k:) which our source policy prescribes for the source environment. Simulate what observation ∂next = o(Tsource(τ-k:, asource)) would be attained at the next time step in the source environment, and then compute atarget = φ(τ-k:, ∂next). φ is a learned inverse dynamics model for the target environment, which takes in the recent history of actions and observations, as well as the desired next observations, and produces the action in the target domain that leads as close as possible to the desired observation ∂next.</p>
<p>Putting this all together, we have:</p>
<p>πtarget(τ-k:) = φ(τ-k:, o(Tsource(τ-k:, πsource(τ-k:)))).</p>
<p>To be able to execute this approach, we assume that the simulator provides a forward dynamics model Tsource that allows us to compute a reasonable estimate of the next state s' and observation o(s').</p>
<p>If the learned inverse dynamics model is sufficiently accurate, then the next observation otarget after taking action πtarget(τ-k:) will be similar to ∂next.</p>
<p>For this approach to be meaningful, it is assumed that source and target environments have the same actuated degrees of freedom. However, the actions taken by policies</p>
<p>$\pi_{\text{source}}$ and $\pi_{\text{target}}$ may be very different from each other. For example, the actuators may be calibrated differently, or realistic actuators may have complex dynamics like fluctuating temperature or gear backlash, which are not modeled in simulation. The dimensionality of the action space may even be different, for example when the target domain actions may be over biarticular pairs of antagonistic cables or muscle tendons, as in [21]. We have such flexibility in our method because the actions generated by the policy $\pi_{\text{source}}$ are never directly used in the target space, but only through mediation of the simulator and the anticipated next observation.</p>
<h3>III-C Training of the inverse dynamics model</h3>
<p>We propose to collect trajectories in the physical environment, and to train a neural network that represents the inverse dynamics model, i.e., that can (approximately) predict the action that will lead to the next observation. For a snippet of a trajectory: $\tau_{H:H+k}$ and next observation $o_{H+k+1}$, we train a neural network $\phi$ to predict the preceding action $a_{H+k}$:</p>
<p>$\phi:\left(o_{H},a_{H},o_{H+1},\ldots,a_{k+H-1},o_{k+H},o_{k+H+1}\right) \mapsto a_{k+H}$</p>
<p>We incorporate history in our model and pick the history window parameter $H$ to be large enough that $\phi$ can (implicitly) infer any important latent factors or temporal dependencies present in the dynamics.</p>
<h3>III-D Data collection / Exploration</h3>
<p>At each point during training we have a preliminary inverse dynamics model $\phi$, which we can use to implement a preliminary policy $\pi_{\text{target}}$. In order to collect training data for our model, we execute this preliminary policy $\pi_{\text{target}}$. We add noise to the prescribed actions for exploration, i.e., in order to ensure that we have sufficiently diverse training data. Adding too much noise will result in data collected too far from the target trajectories, adding too little noise will result in insufficient exploration and the inverse dynamics model will improve very slowly. In our experiments we describe our noise settings. We found it helpful to not add noise at every time step. Adding noise too frequently steers the data collection too far away from the relevant parts of the space for the task at hand. In simulation we can collect training samples very efficiently by setting the simulator to the states that occur along a trajectory; in a physical system, the efficiency of collecting training data depends on the amount of noise that can be injected into the controls before the robot moves far enough from the target trajectories that its behavior is no longer useful for training. We also found it more efficient to reset once the target execution starts deviating very far from what would have happened in the source environment.</p>
<h3>III-E Inverse dynamics neural network architecture</h3>
<p>All of our inverse dynamics models $\phi$ take as input a sequence of $k$ previous observations, $k-1$ previous actions, and a target observation. Observations and actions are concatenated into one large input vector for the neural net. As is common in current neural net learning practice,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Images of environments in our simulation experiments: Reacher, Hopper, Half Cheetah and Humanoid.</p>
<p>the neural network inputs are normalized to have mean 0 and variance 1 [20]. We then apply a sequence of two fully-connected hidden layers with ReLU activations and 256 units each, followed by a fully-connected output layer, which gives the action $a=\phi(\tau_{-k:,o})$.</p>
<h2>IV EXPERIMENTS</h2>
<p>The purpose of our method is to adapt a policy from a source environment to a target environment, with the key application being adaptation from simulation to real world. First, we measure adaptation capability between two simulators IV-A as this allows us to quantify most directly the differences between source environment and target environment. Then, we present results for adaptation from a simulation to a physical environment.</p>
<h3>IV-A Simulated Environments – Sim1 to Sim2 transfer</h3>
<p>We test our method on several simulated models in the robotics simulator MuJoCo [38] using OpenAI Gym environment [3]. Therefore, both source and target environments are in simulation. We perform experiments on the following standard OpenAI Gym environments (Figure 2). In each case, observation space consists of positions and velocities of all degrees of freedom.</p>
<ul>
<li>Reacher. Two-link arm aiming toward a target location, with a 11-dimensional observation space and 2 actuators. Arm end effector and target are included in the observation.</li>
<li>Hopper. Two-dimensional model of a robot with a single "foot" that moves by hopping, with a 12-dimensional observation space and 3 actuators.</li>
<li>Half-Cheetah. Two-dimensional model of a bipedal robot with a 17-dimensional observation space and 6 actuators.</li>
<li>Humanoid. Three-dimensional model of a humanoid robot with a 376-dimensional observation space and 17 actuators.</li>
</ul>
<p>In each environment, we train our models to imitate an "expert policy". The expert policies are obtained from Trust Region Policy Optimization [33] (source code by Ho et. al [15]). We measure the performance of policies using</p>
<p><sup>1</sup>We modify the Gym environment by increasing the mass of the arm to be 34 kilograms, roughly in line with the physical Fetch robot. This has a minimal effect on the original task, but it becomes relevant when we try to adapt to a modified version of the task with different gravity.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Plots present robustness of expert policies and our adaptation method to differences between source environment and target environment. x-axis measures how much target environment differs from the source environment. y-axis is the normalized cumulative amount of reward averaged over ten random seeds. We observe that the expert policy performs well in small changes to environment, but is not robust to large changes. Baseline adaptation methods achieve near-zero reward on contact-rich environments. By constrast, our adaptation method performs well in both small and large environment changes - in part due to outputting action correction terms when environments are similar. We further gain minor improvement when the adaptation network that is trained with a history windows.</p>
<p>a reward given by OpenAI gym [3]. We normalize the performance measurement so that the performance of the expert policy is 1.</p>
<p>Note that our algorithm never observes the performance of the adapted policy. This is important for our intended application; evaluating the performance of adapted policies operating in the real world is typically more expensive than executing those policies, as it might, for example, require instrumentation of the physical world with ground truth sensors. We only use the performance measures to determine whether our method has successfully adapted the critical features of the expert policy.</p>
<p>To produce training data, we interleave learning with execution in the target domain, executing the previous estimate of the inverse dynamics model φ to generate trajectories to</p>
<p>be used for further training. We interrupt trajectories at a random point in order to take a random action, and train the model to predict the random action from the resulting state (as well as the history of recent states and actions). We report all of our training times in terms of the number of training samples that we collect. In the case when inverse dynamics model includes history (as described in III-C), we use a window size H = 2 for all the experiments.</p>
<p>We compare our approach to several popular methods that have been developed to deal with simulation to real world model discrepancy. The baselines we use are:</p>
<ul>
<li><strong>Expert Policy.</strong> We perform no adaptation and directly use the actions of the policy obtained from source domain in the new target domain. atarget = πsource.</li>
<li><strong>Output Error Control.</strong> We perform Model Predictive Control in the target domain using an adapted version of a dynamics model Tsource transferred from the source domain. At each timestep, we use the current observation and previous action to update the dynamics model, and</li>
</ul>
<p><sup>2</sup>These reward functions feature penalties for applying large torques; we remove these penalties, because they make it more difficult to interpret results which require gravity compensation or for which there is motor noise.</p>
<p>use the updated dynamics model to compute a policy using iterative LQR [39]. Output Error Control dynamics adaptation scheme adjusts the source dynamics model</p>
<p>$T_{\text{target}}^{t}=T_{\text{source}}+e_{t}$</p>
<p>by an error term</p>
<p>$e_{t}=(1-\gamma)e_{t-1}+\gamma(o_{t}-T_{\text{source}}(o_{t-1},a_{t-1}))$</p>
<p>representing a decayed version of the error in $T_{\text{source}}$ in the target domain.</p>
<ul>
<li>Gaussian Dynamics Adaptation. As the previous baseline, we perform Model Predictive Control using iterative LQR on an adjusted dynamics model. The adjustment scheme in this case uses the source dynamics model to form a local Gaussian prior $p(o_{t},a_{t},o_{t+1})$. We update this prior according to the empirical mean and covariance of the data observed in target domain, and condition it to form</li>
</ul>
<p>$T_{\text{target}}^{t}=p(o_{t+1} \mid o_{t}, a_{t})$</p>
<p>This is the approach proposed and described in more detail in [11].</p>
<p>To test the capability of our method compared to the baseline methods, we consider two following challenging differences between domains:</p>
<ul>
<li>Variation in Gravity. Target environment has a difference in gravity from the source environment. Gravity differs in magnitude by $20\%$ for locomotion tasks. The Reacher task occurs in a plane; the expert policy is trained in a horizontal plane and essentially unaffected by gravity, and we test on planes that are rotated from $0^{\circ}$ to $90^{\circ}$. On the Reacher task, our method is able to adapt successfully to this significant dynamics change.</li>
<li>Motor Noise. Before an action $a$ is sent to the robot, it is perturbed by adding a noise term to obtain $a^{\prime}=$ $a+\epsilon_{t}$. We experiment with two variants, where this noise is independent on each time step, as well as where this noise varies slowly and is correlated over time. Such noise is more representative of fluctuating environmental conditions, or latent physical effects like temperature changes.</li>
</ul>
<p>In many cases, only small corrections to the source domain actions are necessary to adapt to target domain. In such a setting, it may be beneficial for $\phi$ to output a correction term rather than an action directly:</p>
<p>$a_{\text{target}}=a_{\text{source}}+\phi(\tau_{-k:}, \hat{o}_{\text{next}})$</p>
<p>This has the downside of directly requiring actions from the source domain, but tends to result in better performance when the domains are similar. We use such a correction formulation for motor noise standard deviations below 0.3 and for all locomotion experiments with varying gravity. In such cases, we also found it most helpful to pre-train the model on trajectories produced by the expert policy.</p>
<p>Figure 3 summarizes our results, and Table 5 presents sample complexity of our method.</p>
<p>As expected, simply applying actions from an expert policy from a source domain results in poor performance on the target domain. Baselines that perform planning using a locally Gaussian forward dynamics model that is adapted online performed well with no additional training on the target domain in environments with simple dynamics (e.g., no contacts) such as Reacher and relatively slowly changing variation between the source and target domain. However, we found these methods to be ineffective in contact-rich environments such as Hopper, Cheetah, and Humanoid, even in the source domain. Contacts induce discontinuities that cause methods using locally linear dynamics approximations to perform poorly. Unstable tasks like Hopper and Humanoid are particularly poorly suited for these methods because small errors propagate over long trajectories, leading to episode termination.</p>
<p>Our method is also able to correct for slowly-varying noise and small changes to system dynamics. Moreover, it is able to adapt even in the presence of contact discontinuities that are externeley challenging for approaches based Model Predictive Control. Such approaches require solving an optimization problem (iterative LQG) that can exploit the learned forward dynamics model and take it outside the regime it was trained on. By learning an inverse dynamics model, we simply take the output of such models and avoid performing potentially unstable numerical optimization.</p>
<p>Noise std. Noise correlation none 0.2 1.0 0.0 0.9 1.0</p>
<p>Number of training samples in thousands (smaller is better)</p>
<table>
<thead>
<tr>
<th>31</th>
<th>58</th>
<th>48</th>
<th>77</th>
<th>150</th>
<th>157</th>
<th>137</th>
</tr>
</thead>
<tbody>
<tr>
<td>Adaptation without history</td>
<td>24</td>
<td>31</td>
<td>29</td>
<td>28</td>
<td>70</td>
<td>121</td>
</tr>
<tr>
<td>Learning from scratch</td>
<td>about 1000</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Humanoid</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Adaptation without history</td>
<td>13</td>
<td>15</td>
<td>20</td>
<td>16</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Adaptation with history</td>
<td>16</td>
<td>17</td>
<td>19</td>
<td>16</td>
<td>N/A</td>
<td>N/A</td>
</tr>
<tr>
<td>Learning from scratch</td>
<td>about 70000</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Fig. 4. Table presents the number of samples required to converge to 3/4 of the expert policy’s performance, running on our simulated environments with additive noise. The method requires more samples in the presence of noise. Including history typically reduces the complexity of the learning problem. As a comparison, the fastest RL algorithms posted to OpenAI Gym as of submission require 70 million samples to converge to 75% performance on Humanoid, and about a million samples to converge to 75% performance on Hopper, so these running times are about two orders of magnitude faster than those required to learn policies directly.</p>
<h3>V-B Physical interaction – Sim to Real transfer</h3>
<p>We test our method on transferring trajectories from a simulated source domain to the target domain, which is physical Fetch robot [42]. We control the robot using position control and stock firmware based on ROS in 10Hz frequency.</p>
<p>The tasks consider control of the arm, and our metric measures normalized distance between observations achieved in the simulator by the trajectory and observations achieved on the physical robot. The task is an agile back-and-forth swing of an arm where middle of the arm is pulled by a bungee cord. Our action adaptation method is able to adjust</p>
<p>to this condition by adapting and exerting the necessary about of torque. As a baseline we use PD controller with targets being states experienced in the simulator. Table 5 summarizes our results.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Swings limited with a bungee cord</th>
</tr>
</thead>
<tbody>
<tr>
<td>Method</td>
<td>$3.72\% \pm 0.020 \%$</td>
</tr>
<tr>
<td>Our method</td>
<td>$4.49 \% \pm 0.050 \%$</td>
</tr>
<tr>
<td>PD controller</td>
<td></td>
</tr>
</tbody>
</table>
<p>Fig. 5. Table presents average distance and varaince to the desired trajectory for our method and PD baseline, averaged over 10 trials.</p>
<h2>V. Discussion and Future Work</h2>
<p>We have presented a general method to adapt actions of policies developed in one domain such as simulation to a different domain such as the physical world. We achieve this by learning a deep inverse dynamics model that is trained on the behavior of the physical robot. Our method is successfully able to adapt complex control policies for aggressive reaching and locomotion on scenarios involving contact, hysteresis effects in the form of time-correlated noise, and significant differences between environments. However to bring about robots that truly generalize in the physical world, in addition to action adaptation it is necessary to also adapt states and observations between simulation and physical world. We currently assume observations generated by our simulator match closely to physical observations, which is reasonable when considering sensors such as joint positions, but is it not reasonable to expect simulated visual or depth sensors to match the high fidelity of the real world. This work only focused on action adaptation. In the future we plan to experiment with observation adaptation methods, such as [41] for instance. Additionally, our approach can be applied to a setting where we do not even observe the actions taken in the source domain. This presents exciting future opportunities to apply our method to use solely observations in the source domain (such as driving dashboard camera recording, for example) to recover and adapt actions for a corresponding driving policy.</p>
<h2>REFERENCES</h2>
<p>[1] Samuel Barrett, Matt E. Taylor, and Peter Stone. Transfer learning for reinforcement learning on a physical robot. In Ninth International Conference on Autonomous Agents and Multiagent Systems - Adaptive Learning Agents Workshop (ALA), May 2010.
[2] Joschka Boedecker, Jost Tobias Springenberg, Jan Wlfing, and Martin Riedmiller. Approximate real-time optimal control based on sparse gaussian process models. In Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2014.
[3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[4] R. Calandra, S. Ivaldi, M. Deisenroth, E. Rückert, and J. Peters. Learning inverse dynamics models with contacts. In IEEE International Conference on Robotics and Automation, pages 3186-3191, 2015.
[5] Mark Cutler, Thomas J Walsh, and Jonathan P How. Real-world reinforcement learning via multifidelity simulators. IEEE Transactions on Robotics, 31(3):655-671, 2015.
[6] Benoit Boulet Laeeque Daneshmend. System identification and modelling of a high performance hydraulic actuator. In Eds.), Lecture Notes in Control and Information Sciences. Springer Verlag, 1992.
[7] Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A modelbased and data-efficient approach to policy search. In In Proceedings of the International Conference on Machine Learning, 2011.
[8] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647655, 2014.
[9] Roy Featherstone. Rigid Body Dynamics Algorithms. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2007.
[10] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. CoRR, abs/1603.00448, 2016.
[11] Justin Fu, Sergey Levine, and Pieter Abbeel. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. CoRR, abs/1509.06841, 2015.
[12] G. Gilardi and I. Sharf. Literature survey of contact dynamics modelling. Mechanism and Machine Theory, 37(10):1213 - 1239, 2002.
[13] Abhishek Gupta, Clemens Eppner, Sergey Levine, and Pieter Abbeel. Learning dexterous manipulation for a soft robotic hand from human demonstration. CoRR, abs/1603.06348, 2016.
[14] Karlsson Hibbitt and Sorensen. ABAQUS/CAE User's Manual. Hibbitt, Karlsson \&amp; Sorensen, Incorporated, 2002.
[15] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. arXiv preprint arXiv:1606.03476, 2016.
[16] Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, and Kate Saenko. Efficient learning of domain-invariant image representations. arXiv preprint arXiv:1301.3224, 2013.
[17] GE Hovland, S Hanssen, E Gallestey, S Moberg, T Brogardh, S Gunnarsson, and M Isaksson. Nonlinear identification of backlash in robot transmissions. In Proceedings of the 33rd ISB (International Symposium on Robotics), 2002.
[18] Thomas Ingebretsen. System identification of unmanned aerial vehicles. 2012.
[19] Jonathan Ko and Dieter Fox. Gp-bayesfilters: Bayesian filtering using gaussian process prediction and observation models. Auton. Robots, 27(1):75-90, 2009.
[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097-1105. Curran Associates, Inc., 2012.
[21] Yoonsang Lee, Moon Seok Park, Taesoo Kwon, and Jehee Lee. Locomotion control for many-muscle humanoids. ACM Trans. Graph., 33(6):218:1-218:11, November 2014.
[22] Ian Lenz, Ross Knepper, and Ashutosh Saxena. Deepmpc: Learning deep latent features for model predictive control. In RSS, 2015.
[23] Michael Yurievich Levashov. Modeling, System Identification, and Control for Dynamic Locomotion of the LittleDog Robot on Rough Terrain. PhD thesis, Citeseret, 2012.
[24] L. Ljung. System Identification: Theory for the User. Prentice Hall information and system sciences series. Prentice Hall PTR, 1999.
[25] Franziska Meier, Daniel Kappfer, Nathan Ratliff, and Stefan Schaal. Towards robust online inverse dynamics learning. In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems. IEEE, 2016.
[26] Bernard Mettler, Mark B. Tischler, and Takeo Kanade. System identification of small-size unmanned helicopter dynamics. In Presented at the American Helicopter Society 55th Forum, May 1999.
[27] Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 5307-5314. IEEE, 2015.
[28] Igor Mordatch, Nikhil Mishra, Clemens Eppner, and Pieter Abbeel. Combining model-based policy search with online model learning for control of physical humanoids. In Proceedings of the IEEE International Conference on Robotics and Automation, 2016.
[29] D. Nguyen-Tuong and J. Peters. Using model knowledge for learning inverse dynamics. pages 2677-2682, Piscataway, NJ, USA, May 2010. Max-Planck-Gesellschaft, IEEE.
[30] Nicholas Paine, Joshua S. Mehling, James Holley, Nicolaus A. Radford, Gwendolyn Johnson, Chien-Liang Fok, and Luis Sentis. Actuator control for the nasa-jsc valkyrie humanoid robot: A decoupled dynamics approach for torque control of series elastic robots. Journal of Field Robotics, 32(3):378-396, 2015.</p>
<p>[31] Zherong Pan, Chonhyon Park, and Dinesh Manocha. Robot motion planning for pouring liquids. In Proceedings of the Twenty-Sixth International Conference on Automated Planning and Scheduling, ICAPS 2016, London, UK, June 12-17, 2016., pages 518-526, 2016.
[32] Ali Punjani and Pieter Abbeel. Deep learning helicopter dynamics models. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 3223-3230. IEEE, 2015.
[33] John Schulman, Sergey Levine, Philipp Moritz, Michael I Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.
[34] Jie Tan, Yuting Gu, Greg Turk, and C. Karen Liu. Articulated swimming creatures. In ACM SIGGRAPH 2011 papers, SIGGRAPH '11, pages 58:1-58:12. ACM, 2011.
[35] Jie Tan, Greg Turk, and C. Karen Liu. Soft body locomotion. ACM Trans. Graph., 31(4):26:1-26:11, 2012.
[36] Yuval Tassa, Tingfan Wu, Javier Movellan, and Emanuel Todorov. Modeling and identification of pneumatic actuators. In 2013 IEEE International Conference on Mechatronics and Automation, pages 437-443. IEEE, 2013.
[37] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
[38] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.
[39] Emmanuel Todorov and Weiwei Li. A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems. In American Control Conference, 2005. Proceedings of the 2005, pages 300-306 vol. 1. IEEE, June 2005.
[40] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, and Trevor Darrell. Towards adapting deep visuomotor representations from simulated to real environments. CoRR, abs/1511.07111, 2015.
[41] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. CoRR, abs/1510.02192, 2015.
[42] Melonee Wise, Michael Ferguson, Derek King, Eric Diehr, and David Dymesich. Fetch and freight: Standard platforms for service robot applications. Workshop on Autonomous Mobile Service Robots, 2016.
[43] Michael C. Yip and David B. Camarillo. Model-Less Feedback Control of Continuum Manipulators in Constrained Environments. IEEE Transactions on Robotics, 30(4):880-889, August 2014. 00005.
[44] Kemin Zhou and John Comstock Doyle. Essentials of robust control, volume 104. Prentice hall Upper Saddle River, NJ, 1998.</p>            </div>
        </div>

    </div>
</body>
</html>
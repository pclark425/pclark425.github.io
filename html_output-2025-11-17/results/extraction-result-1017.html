<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1017 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1017</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1017</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-199542385</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1909.12925v1.pdf" target="_blank">Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals</a></p>
                <p><strong>Paper Abstract:</strong> In a multi-agent setting, the optimal policy of a single agent is largely dependent on the behavior of other agents. We investigate the problem of multi-agent reinforcement learning, focusing on decentralized learning in non-stationary domains for mobile robot navigation. We identify a cause for the difficulty in training non-stationary policies: mutual adaptation to sub-optimal behaviors, and we use this to motivate a curriculum-based strategy for learning interactive policies. The curriculum has two stages. First, the agent leverages policy gradient algorithms to learn a policy that is capable of achieving multiple goals. Second, the agent learns a modifier policy to learn how to interact with other agents in a multi-agent setting. We evaluated our approach on both an autonomous driving lane-change domain and a robot navigation domain.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1017.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1017.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IATRPO-C2-fixed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interaction-Aware TRPO on C2-fixed (lane-change, fixed goals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interaction-aware actor-critic policy learned with a two-stage curriculum (single-agent TRPO then multi-agent modifier) evaluated on the C2-fixed lane-change environment with fixed goals and limited start-position randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IATRPO agent (Interaction-Aware TRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decentralized actor-critic learned with Trust Region Policy Optimization (TRPO) using a two-stage curriculum: (1) learn single-agent goal-reaching policy (actor-critic policy gradient), (2) freeze single-agent model and learn a multi-agent modifier policy to avoid conflicts; observation noise and action noise present.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile vehicle (car) agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>C2-fixed (lane-change, fixed goals)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Two-car double lane-change crossing task. Goals fixed to lanes (1st and 3rd), randomness only along +x direction for start positions; cars follow bicycle kinematics model, low observation noise for self (±0.01) and modest noise for other-agent state estimates (±0.1). Interaction occurs in constrained parts of state space (where paths cross).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents = 2; interaction topology (crossing paths); kinematic model complexity (bicycle model); limited start-position randomness; collision checks; state includes continuous positions, velocities, headings. Quantitatively: 2 agents, fixed goals, constrained randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-to-medium (2 agents, fixed-goal layout reduces variation of conflict scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Degree of randomness in start positions and goals; in C2-fixed goals fixed (low variation), start random only in +x (low variation); observation/action noise ranges (o_self ±0.01, o_other ±0.1, action noise ±0.1) also introduce variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (episode considered successful if d(self,goal) < 0.4 for both agents); also Fréchet distance used to measure trajectory modification and 'which agent reaches first' interactiveness metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 99.4 ± 0.33% (evaluated on 1000 random episodes with 5 training seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper states that low variation (fixed goals, constrained randomness) yields high performance for both approaches; low-variation environments are easier and reduce the chance of mutual adaptation trapping agents in poor local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>99.4 ± 0.33% success rate</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (two-stage): single-agent TRPO then multi-agent modifier; decentralized actor-critic.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization tested via 'mixed agents' experiments (agents trained separately paired together). On C2-fixed, mixed-agent performance for IATRPO remained high: 98.71 ± 1.29% success rate (1000 episodes over 20 mixed pairs/triples), indicating strong robustness in low-variation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported as total environment interactions; qualitative note: in comparison runs MATRPO agent convergence shown at ≈800 iterations on C2-fixed, IATRPO uses curriculum so iteration counts are not directly comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In an easy/low-variation lane-change task, the curriculumed IATRPO attains near-perfect success (≈99%), and generalizes well to agents not trained together; suggests curriculum helps avoid mutual adaptation and brittle one-after-the-other learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1017.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1017.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IATRPO-C2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interaction-Aware TRPO on C2 (lane-change, randomized goals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>IATRPO evaluated on the C2 lane-change environment with higher variation (randomly selected non-adjacent goals and more start randomness); demonstrates robust learning where baseline failed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IATRPO agent (Interaction-Aware TRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decentralized actor-critic with TRPO trained with a two-stage curriculum: single-agent policy then multi-agent modifier; incorporates noisy observations of other agents but no access to other agents' actions or goals.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile vehicle (car) agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>C2 (lane-change, randomized goals)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Two-car double lane-change where at each episode a pair of non-adjacent goals is selected and kept constant during episode; agents' start positions randomly sampled in top-left and bottom-left quarters; higher environment variation due to random goal selection and start positions; bicycle kinematics, collision penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents = 2; randomization of goal selection each episode; start position randomization across two quarters; continuous state/action spaces; collision detection; observation/action noise (as above). Quantitatively: randomized goals per episode increases instance variety.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (interaction still between 2 agents but higher scenario variety)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Random goals per episode and larger start-position randomness; degree of variation is 'high' relative to C2-fixed; also observation/action noise unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (d(self,goal) < 0.4 for both agents)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 94.3 ± 2.99% (1000 evaluation episodes over 5 training seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper emphasizes that increased variation (random goals/start positions) increases difficulty and can cause dependent-training baselines to fail; IATRPO's curriculum alleviates this by separating single-agent skill learning from interaction adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>94.3 ± 2.99% success rate (C2: medium complexity, high variation)</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (two-stage): single-agent TRPO then multi-agent modifier</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>In mixed-agent tests (agents trained separately and paired), IATRPO performance on C2 degrades (Table II shows 69.22 ± 26.44% success rate when tested with mixed agents), indicating some sensitivity to being paired with agents not seen during training under higher variation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not fully reported; convergence behavior noted qualitatively (IATRPO learns both agents around same time vs. MATRPO one-after-the-other), but no total sample counts given.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High variation in goals and start positions makes multi-agent co-training prone to mutual adaptation traps; IATRPO remains robust (≈94% success) whereas the MATRPO baseline failed completely (0%). However, generalization to agents trained separately degrades substantially under high variation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1017.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1017.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IATRPO-R2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interaction-Aware TRPO on R2 (two-robot navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>IATRPO evaluated on a two-robot navigation domain (R2) where agents cross paths to reach opposite-side goals; demonstrates substantially higher success than baseline in this interactive mobile-robot domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IATRPO agent (Interaction-Aware TRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decentralized actor-critic TRPO with two-stage curriculum; robots use unicycle kinematics; multi-agent module observes noisy states of other agents and modifies single-agent policies to avoid collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile robot (unicycle model)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>R2 (two-robot crossing navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Two (or sometimes 3) mobile robots crossing one another to reach color-matched goals; agents start randomly in left/right/bottom regions to ensure crossing interactions; unicycle kinematics; collisions penalized; observation and action noise present.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents = 2; required path crossing and negotiation; continuous dynamics (unicycle model); start position randomization across regions; collision avoidance critical. Quantitative: 2 agents, randomized starting regions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high (crossing interactions increase necessary coordination complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized start positions across three regions (left/right/bottom), randomized initial conditions; degree of variation moderate-high.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (both agents reach within 0.4 of goals)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 90.96 ± 1.43% (1000 evaluation episodes over 5 seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that environments requiring richer interaction (crossing navigation) increase sensitivity to dependent policies; IATRPO's curriculum produces more balanced, interactive behaviors (≈50% first-arrival per agent) and higher success rates than MATRPO, indicating curriculum helps manage complexity and variation interplay.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (two-stage); decentralized TRPO actor-critic; single-agent model frozen and combined (summation) with multi-agent modifier.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Mixed-agent (agents trained separately) performance for IATRPO on R2 is 77.05 ± 9.46% (Table II), showing degradation but less severe than MATRPO (2.18 ± 8.56%), indicating IATRPO generalizes better in this interactive domain.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not explicitly reported; experimental commentary indicates MATRPO sometimes requires ~800 iterations to converge in related C2-fixed run and that IATRPO learns agents roughly simultaneously, but exact sample counts for R2 not given.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a moderately complex crossing navigation domain, IATRPO attains ≈91% success and produces more interactive, balanced policies versus MATRPO; curriculum reduces mutual adaptation failure modes and produces policies that generalize better to unseen agent pairings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1017.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1017.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IATRPO-R3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interaction-Aware TRPO on R3 (three-robot navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>IATRPO evaluated on a three-robot crossing navigation task (R3), a higher-complexity multi-agent scenario where IATRPO outperforms the baseline and yields adaptive trajectory/ speed modifications quantified via Fréchet distance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IATRPO agent (Interaction-Aware TRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Decentralized TRPO actor-critic trained with two-stage curriculum; multi-agent module inputs include other agents' noisy x,y,velocity,heading and outputs a modifier added to frozen single-agent policy; collision-penalized reward.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile robots (unicycle model)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>R3 (three-robot crossing navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Three robots starting in left/right/bottom regions aiming for top/left/right goals; trajectories cross and require tri-party coordination; unicycle kinematics; collision penalties; higher interaction complexity due to three agents.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents = 3; required multi-party negotiation/collision avoidance; continuous dynamics; randomized start positions mapping to crossing; additional complexity vs. R2 due to possible three-way conflicts. Quantitatively: 3 agents.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Randomized starting positions across regions and random initial conditions; goals fixed per episode (matching colored stars) but multiple permutations across episodes; observation/action noise; overall variation considered high.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (both/all agents within 0.4 of goals)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 88.02 ± 4.11% (1000 evaluation episodes over 5 seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper shows that with increasing agent count (complexity), baseline MATRPO fails completely (0% success) while IATRPO still attains high success (~88%), indicating curriculum approach effectively mitigates complexity+variation difficulties that break centralized/multi-agent baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>88.02 ± 4.11% success rate (R3: high complexity, high variation)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (two-stage), decentralized TRPO; multi-agent modifier combined via summation with frozen single-agent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Mixed-agent testing (agents trained separately) yields IATRPO success 68.87 ± 17.44% on R3 (Table II), showing notable degradation but still non-zero performance where MATRPO often fails; Fréchet-distance measurements indicate agents modify both speed and path (e.g., bottom agent had ~20% trajectory change vs single-agent).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported quantitatively; convergence behavior noted qualitatively: MATRPO often fails to learn coordinated policies at all in R3 while IATRPO reliably learns successful multi-agent behavior across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When scaling to 3 agents (higher complexity), curriculumed IATRPO achieves ~88% success while MATRPO fails (0%), demonstrating that separating single-agent skill learning from interaction adaptation is effective for high-complexity, high-variation embodied multi-agent tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1017.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1017.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATRPO-C2-fixed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent TRPO baseline on C2-fixed</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline multi-agent TRPO (MATRPO) where each agent's critic is augmented with other agents' action-values, evaluated on the C2-fixed environment; used to compare with IATRPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MATRPO agent (Multi-Agent TRPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Implemented variant of TRPO for multi-agent setting that provides each agent with other agents' action values (inspired by MADDPG style centralized information); trained end-to-end without the IATRPO curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile vehicle (car) agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>C2-fixed (lane-change, fixed goals)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same two-car fixed-goal lane-change environment as described for IATRPO-C2-fixed; low variation in goals and limited start-position randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>2 agents, fixed goals, limited start randomness, continuous vehicle dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low-to-medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Low variation: fixed goals; limited start-position randomness in +x only; noise as specified.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 97.88 ± 0.31% (1000 episodes over 5 seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Baseline performs well in low-variation/low-complexity setting, but authors argue such performance often arises from stereotyped dependent policies that won't generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>97.88 ± 0.31% success rate</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-stage multi-agent TRPO with centralized-style critic information (MATRPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>In mixed-agent tests on C2-fixed, MATRPO maintained high generalization: 97.83 ± 0.72% success rate (agents trained separately paired together), likely because policies learned to be non-interactive/stereotyped (one agent always goes first).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>MATRPO convergence observed around ≈800 training iterations in a C2-fixed run shown in figures; exact sample counts not formally reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MATRPO attains high success in low-variation/low-complexity settings but learns dependent/stereotyped policies (one agent often always yields), harming interactiveness and generalization when variation or complexity increases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1017.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1017.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATRPO-C2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent TRPO baseline on C2 (randomized goals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MATRPO baseline evaluated on C2 with randomized goals; fails to learn successful policies under increased variation (random goals and start positions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MATRPO agent (Multi-Agent TRPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-agent TRPO variant using action-value information of other agents; trained jointly without curriculum separation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile vehicle (car) agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>C2 (lane-change, randomized goals)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Two-car lane-change with randomized non-adjacent goals per episode and randomized start positions (top-left/bottom-left quarters); higher variation introduces many interaction cases.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>2 agents with randomized goals per episode; variable start positions and continuous dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High (randomized goals per episode and start positions across quarters)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 0 ± 0% (MATRPO failed to learn successful policies for both agents in C2 across runs)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Authors highlight that increased variation (random goals) and non-stationarity during co-training caused MATRPO to converge to degenerate solutions where one agent is successful and the other is stuck in a local optimum, producing 0% joint success.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>0 ± 0% success rate</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-stage multi-agent TRPO (MATRPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>N/A (baseline failed to learn joint-successful policies in this high-variation setting, so meaningful generalization not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>MATRPO displayed one-agent-first convergence behavior: one agent converged earlier (orange) and the other adapted later; in one run orange converged by ~800 iterations but that adaptation produced poor joint solutions overall.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MATRPO fails catastrophically on C2 when variation (random goals) is present — illustrating that multi-agent training without pacing/curriculum can produce mutual-adaptation traps and poor joint outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1017.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1017.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATRPO-R2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent TRPO baseline on R2 (two-robot navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MATRPO baseline tested on two-robot crossing navigation (R2) shows highly variable and often poor performance compared to IATRPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MATRPO agent (Multi-Agent TRPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-agent TRPO variant augmented with other agents' action values; no curriculum; decentralized policy but with centralized critic information.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile robot (unicycle model)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>R2 (two-robot crossing navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Two robots crossing to reach opposite-side goals; random start placement in left/right/bottom regions; requires interactive negotiation to avoid collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>2 agents with crossing interactions; randomized start placement; continuous unicycle dynamics; collisions penalized.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Moderate-to-high variation due to randomized starts and possible goal permutations (as configured); observation/action noise present.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 36.92 ± 45.22% (large variance across 5 runs; in half of random runs MATRPO did not learn successful policy for both agents)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Shows that in an interactive crossing task, MATRPO often learns unbalanced/stereotyped policies and fails under co-evolving agents; increased complexity/variation magnifies instability and poor coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-stage multi-agent TRPO (MATRPO baseline with access to other agents' action-values)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Mixed-agent tests produced catastrophic degradation for MATRPO on R2 (2.18 ± 8.56% success rate when pairing separately trained agents), indicating strong dependence and poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High variance in runs; some runs failed to learn joint-successful policies within the training schedule; exact sample counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MATRPO produces unstable results on R2 with large variance and frequent failure to produce coordinated policies; generalization to mixed agents nearly collapses, illustrating vulnerability to mutual adaptation and dependence on co-training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1017.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1017.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATRPO-R3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent TRPO baseline on R3 (three-robot navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MATRPO baseline on the 3-robot navigation environment R3 fails to learn coordinated multi-agent behavior (0% success) under this higher-complexity setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MATRPO agent (Multi-Agent TRPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-agent TRPO variant with centralized critic inputs (other agents' action-values); trained jointly without curriculum separation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated mobile robots (unicycle model)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>R3 (three-robot crossing navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Three robots required to cross/coordinate to reach goals; higher interaction complexity and potential three-way conflicts; randomized starting positions across regions; collision penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents = 3 (tri-party interactions); continuous dynamics; randomized starts; collision detection.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>High (randomized starts leading to many distinct interaction patterns), observation/action noise present.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Success rate = 0 ± 0% (MATRPO failed to learn successful policies for R3 across runs)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Authors report MATRPO's complete failure at R3: increasing agent count (complexity) combined with interaction variation causes baseline to get stuck in poor local optima, while IATRPO remains effective.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>0 ± 0% success rate</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-stage multi-agent TRPO (MATRPO baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>N/A (baseline did not learn joint-successful behavior in training runs, so generalization not meaningful)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported; baseline failed to converge to successful joint policies under the training regime used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MATRPO is unable to learn coordinated policies in a 3-agent high-complexity environment, underscoring the limitation of directly joint-training without pacing/curriculum in multi-agent embodied tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-agent actor-critic for mixed cooperative-competitive environments <em>(Rating: 2)</em></li>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Emergent complexity via multi-agent competition <em>(Rating: 1)</em></li>
                <li>Counterfactual multi-agent policy gradients <em>(Rating: 1)</em></li>
                <li>Intention-aware online pomdp planning for autonomous driving in a crowd <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1017",
    "paper_id": "paper-199542385",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "IATRPO-C2-fixed",
            "name_full": "Interaction-Aware TRPO on C2-fixed (lane-change, fixed goals)",
            "brief_description": "An interaction-aware actor-critic policy learned with a two-stage curriculum (single-agent TRPO then multi-agent modifier) evaluated on the C2-fixed lane-change environment with fixed goals and limited start-position randomness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IATRPO agent (Interaction-Aware TRPO)",
            "agent_description": "Decentralized actor-critic learned with Trust Region Policy Optimization (TRPO) using a two-stage curriculum: (1) learn single-agent goal-reaching policy (actor-critic policy gradient), (2) freeze single-agent model and learn a multi-agent modifier policy to avoid conflicts; observation noise and action noise present.",
            "agent_type": "simulated mobile vehicle (car) agent",
            "environment_name": "C2-fixed (lane-change, fixed goals)",
            "environment_description": "Two-car double lane-change crossing task. Goals fixed to lanes (1st and 3rd), randomness only along +x direction for start positions; cars follow bicycle kinematics model, low observation noise for self (±0.01) and modest noise for other-agent state estimates (±0.1). Interaction occurs in constrained parts of state space (where paths cross).",
            "complexity_measure": "Number of agents = 2; interaction topology (crossing paths); kinematic model complexity (bicycle model); limited start-position randomness; collision checks; state includes continuous positions, velocities, headings. Quantitatively: 2 agents, fixed goals, constrained randomness.",
            "complexity_level": "low-to-medium (2 agents, fixed-goal layout reduces variation of conflict scenarios)",
            "variation_measure": "Degree of randomness in start positions and goals; in C2-fixed goals fixed (low variation), start random only in +x (low variation); observation/action noise ranges (o_self ±0.01, o_other ±0.1, action noise ±0.1) also introduce variation.",
            "variation_level": "low",
            "performance_metric": "Success rate (episode considered successful if d(self,goal) &lt; 0.4 for both agents); also Fréchet distance used to measure trajectory modification and 'which agent reaches first' interactiveness metric.",
            "performance_value": "Success rate = 99.4 ± 0.33% (evaluated on 1000 random episodes with 5 training seeds)",
            "complexity_variation_relationship": "Paper states that low variation (fixed goals, constrained randomness) yields high performance for both approaches; low-variation environments are easier and reduce the chance of mutual adaptation trapping agents in poor local optima.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "99.4 ± 0.33% success rate",
            "training_strategy": "Curriculum learning (two-stage): single-agent TRPO then multi-agent modifier; decentralized actor-critic.",
            "generalization_tested": true,
            "generalization_results": "Generalization tested via 'mixed agents' experiments (agents trained separately paired together). On C2-fixed, mixed-agent performance for IATRPO remained high: 98.71 ± 1.29% success rate (1000 episodes over 20 mixed pairs/triples), indicating strong robustness in low-variation setting.",
            "sample_efficiency": "Not reported as total environment interactions; qualitative note: in comparison runs MATRPO agent convergence shown at ≈800 iterations on C2-fixed, IATRPO uses curriculum so iteration counts are not directly comparable.",
            "key_findings": "In an easy/low-variation lane-change task, the curriculumed IATRPO attains near-perfect success (≈99%), and generalizes well to agents not trained together; suggests curriculum helps avoid mutual adaptation and brittle one-after-the-other learning.",
            "uuid": "e1017.0"
        },
        {
            "name_short": "IATRPO-C2",
            "name_full": "Interaction-Aware TRPO on C2 (lane-change, randomized goals)",
            "brief_description": "IATRPO evaluated on the C2 lane-change environment with higher variation (randomly selected non-adjacent goals and more start randomness); demonstrates robust learning where baseline failed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IATRPO agent (Interaction-Aware TRPO)",
            "agent_description": "Decentralized actor-critic with TRPO trained with a two-stage curriculum: single-agent policy then multi-agent modifier; incorporates noisy observations of other agents but no access to other agents' actions or goals.",
            "agent_type": "simulated mobile vehicle (car) agent",
            "environment_name": "C2 (lane-change, randomized goals)",
            "environment_description": "Two-car double lane-change where at each episode a pair of non-adjacent goals is selected and kept constant during episode; agents' start positions randomly sampled in top-left and bottom-left quarters; higher environment variation due to random goal selection and start positions; bicycle kinematics, collision penalties.",
            "complexity_measure": "Number of agents = 2; randomization of goal selection each episode; start position randomization across two quarters; continuous state/action spaces; collision detection; observation/action noise (as above). Quantitatively: randomized goals per episode increases instance variety.",
            "complexity_level": "medium (interaction still between 2 agents but higher scenario variety)",
            "variation_measure": "Random goals per episode and larger start-position randomness; degree of variation is 'high' relative to C2-fixed; also observation/action noise unchanged.",
            "variation_level": "high",
            "performance_metric": "Success rate (d(self,goal) &lt; 0.4 for both agents)",
            "performance_value": "Success rate = 94.3 ± 2.99% (1000 evaluation episodes over 5 training seeds)",
            "complexity_variation_relationship": "Paper emphasizes that increased variation (random goals/start positions) increases difficulty and can cause dependent-training baselines to fail; IATRPO's curriculum alleviates this by separating single-agent skill learning from interaction adaptation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "94.3 ± 2.99% success rate (C2: medium complexity, high variation)",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (two-stage): single-agent TRPO then multi-agent modifier",
            "generalization_tested": true,
            "generalization_results": "In mixed-agent tests (agents trained separately and paired), IATRPO performance on C2 degrades (Table II shows 69.22 ± 26.44% success rate when tested with mixed agents), indicating some sensitivity to being paired with agents not seen during training under higher variation.",
            "sample_efficiency": "Not fully reported; convergence behavior noted qualitatively (IATRPO learns both agents around same time vs. MATRPO one-after-the-other), but no total sample counts given.",
            "key_findings": "High variation in goals and start positions makes multi-agent co-training prone to mutual adaptation traps; IATRPO remains robust (≈94% success) whereas the MATRPO baseline failed completely (0%). However, generalization to agents trained separately degrades substantially under high variation.",
            "uuid": "e1017.1"
        },
        {
            "name_short": "IATRPO-R2",
            "name_full": "Interaction-Aware TRPO on R2 (two-robot navigation)",
            "brief_description": "IATRPO evaluated on a two-robot navigation domain (R2) where agents cross paths to reach opposite-side goals; demonstrates substantially higher success than baseline in this interactive mobile-robot domain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IATRPO agent (Interaction-Aware TRPO)",
            "agent_description": "Decentralized actor-critic TRPO with two-stage curriculum; robots use unicycle kinematics; multi-agent module observes noisy states of other agents and modifies single-agent policies to avoid collisions.",
            "agent_type": "simulated mobile robot (unicycle model)",
            "environment_name": "R2 (two-robot crossing navigation)",
            "environment_description": "Two (or sometimes 3) mobile robots crossing one another to reach color-matched goals; agents start randomly in left/right/bottom regions to ensure crossing interactions; unicycle kinematics; collisions penalized; observation and action noise present.",
            "complexity_measure": "Number of agents = 2; required path crossing and negotiation; continuous dynamics (unicycle model); start position randomization across regions; collision avoidance critical. Quantitative: 2 agents, randomized starting regions.",
            "complexity_level": "medium-to-high (crossing interactions increase necessary coordination complexity)",
            "variation_measure": "Randomized start positions across three regions (left/right/bottom), randomized initial conditions; degree of variation moderate-high.",
            "variation_level": "medium-high",
            "performance_metric": "Success rate (both agents reach within 0.4 of goals)",
            "performance_value": "Success rate = 90.96 ± 1.43% (1000 evaluation episodes over 5 seeds)",
            "complexity_variation_relationship": "Paper reports that environments requiring richer interaction (crossing navigation) increase sensitivity to dependent policies; IATRPO's curriculum produces more balanced, interactive behaviors (≈50% first-arrival per agent) and higher success rates than MATRPO, indicating curriculum helps manage complexity and variation interplay.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (two-stage); decentralized TRPO actor-critic; single-agent model frozen and combined (summation) with multi-agent modifier.",
            "generalization_tested": true,
            "generalization_results": "Mixed-agent (agents trained separately) performance for IATRPO on R2 is 77.05 ± 9.46% (Table II), showing degradation but less severe than MATRPO (2.18 ± 8.56%), indicating IATRPO generalizes better in this interactive domain.",
            "sample_efficiency": "Not explicitly reported; experimental commentary indicates MATRPO sometimes requires ~800 iterations to converge in related C2-fixed run and that IATRPO learns agents roughly simultaneously, but exact sample counts for R2 not given.",
            "key_findings": "In a moderately complex crossing navigation domain, IATRPO attains ≈91% success and produces more interactive, balanced policies versus MATRPO; curriculum reduces mutual adaptation failure modes and produces policies that generalize better to unseen agent pairings.",
            "uuid": "e1017.2"
        },
        {
            "name_short": "IATRPO-R3",
            "name_full": "Interaction-Aware TRPO on R3 (three-robot navigation)",
            "brief_description": "IATRPO evaluated on a three-robot crossing navigation task (R3), a higher-complexity multi-agent scenario where IATRPO outperforms the baseline and yields adaptive trajectory/ speed modifications quantified via Fréchet distance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IATRPO agent (Interaction-Aware TRPO)",
            "agent_description": "Decentralized TRPO actor-critic trained with two-stage curriculum; multi-agent module inputs include other agents' noisy x,y,velocity,heading and outputs a modifier added to frozen single-agent policy; collision-penalized reward.",
            "agent_type": "simulated mobile robots (unicycle model)",
            "environment_name": "R3 (three-robot crossing navigation)",
            "environment_description": "Three robots starting in left/right/bottom regions aiming for top/left/right goals; trajectories cross and require tri-party coordination; unicycle kinematics; collision penalties; higher interaction complexity due to three agents.",
            "complexity_measure": "Number of agents = 3; required multi-party negotiation/collision avoidance; continuous dynamics; randomized start positions mapping to crossing; additional complexity vs. R2 due to possible three-way conflicts. Quantitatively: 3 agents.",
            "complexity_level": "high",
            "variation_measure": "Randomized starting positions across regions and random initial conditions; goals fixed per episode (matching colored stars) but multiple permutations across episodes; observation/action noise; overall variation considered high.",
            "variation_level": "high",
            "performance_metric": "Success rate (both/all agents within 0.4 of goals)",
            "performance_value": "Success rate = 88.02 ± 4.11% (1000 evaluation episodes over 5 seeds)",
            "complexity_variation_relationship": "Paper shows that with increasing agent count (complexity), baseline MATRPO fails completely (0% success) while IATRPO still attains high success (~88%), indicating curriculum approach effectively mitigates complexity+variation difficulties that break centralized/multi-agent baselines.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "88.02 ± 4.11% success rate (R3: high complexity, high variation)",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (two-stage), decentralized TRPO; multi-agent modifier combined via summation with frozen single-agent outputs.",
            "generalization_tested": true,
            "generalization_results": "Mixed-agent testing (agents trained separately) yields IATRPO success 68.87 ± 17.44% on R3 (Table II), showing notable degradation but still non-zero performance where MATRPO often fails; Fréchet-distance measurements indicate agents modify both speed and path (e.g., bottom agent had ~20% trajectory change vs single-agent).",
            "sample_efficiency": "Not reported quantitatively; convergence behavior noted qualitatively: MATRPO often fails to learn coordinated policies at all in R3 while IATRPO reliably learns successful multi-agent behavior across runs.",
            "key_findings": "When scaling to 3 agents (higher complexity), curriculumed IATRPO achieves ~88% success while MATRPO fails (0%), demonstrating that separating single-agent skill learning from interaction adaptation is effective for high-complexity, high-variation embodied multi-agent tasks.",
            "uuid": "e1017.3"
        },
        {
            "name_short": "MATRPO-C2-fixed",
            "name_full": "Multi-Agent TRPO baseline on C2-fixed",
            "brief_description": "Baseline multi-agent TRPO (MATRPO) where each agent's critic is augmented with other agents' action-values, evaluated on the C2-fixed environment; used to compare with IATRPO.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MATRPO agent (Multi-Agent TRPO baseline)",
            "agent_description": "Implemented variant of TRPO for multi-agent setting that provides each agent with other agents' action values (inspired by MADDPG style centralized information); trained end-to-end without the IATRPO curriculum.",
            "agent_type": "simulated mobile vehicle (car) agent",
            "environment_name": "C2-fixed (lane-change, fixed goals)",
            "environment_description": "Same two-car fixed-goal lane-change environment as described for IATRPO-C2-fixed; low variation in goals and limited start-position randomness.",
            "complexity_measure": "2 agents, fixed goals, limited start randomness, continuous vehicle dynamics.",
            "complexity_level": "low-to-medium",
            "variation_measure": "Low variation: fixed goals; limited start-position randomness in +x only; noise as specified.",
            "variation_level": "low",
            "performance_metric": "Success rate",
            "performance_value": "Success rate = 97.88 ± 0.31% (1000 episodes over 5 seeds)",
            "complexity_variation_relationship": "Baseline performs well in low-variation/low-complexity setting, but authors argue such performance often arises from stereotyped dependent policies that won't generalize.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "97.88 ± 0.31% success rate",
            "training_strategy": "Single-stage multi-agent TRPO with centralized-style critic information (MATRPO baseline)",
            "generalization_tested": true,
            "generalization_results": "In mixed-agent tests on C2-fixed, MATRPO maintained high generalization: 97.83 ± 0.72% success rate (agents trained separately paired together), likely because policies learned to be non-interactive/stereotyped (one agent always goes first).",
            "sample_efficiency": "MATRPO convergence observed around ≈800 training iterations in a C2-fixed run shown in figures; exact sample counts not formally reported.",
            "key_findings": "MATRPO attains high success in low-variation/low-complexity settings but learns dependent/stereotyped policies (one agent often always yields), harming interactiveness and generalization when variation or complexity increases.",
            "uuid": "e1017.4"
        },
        {
            "name_short": "MATRPO-C2",
            "name_full": "Multi-Agent TRPO baseline on C2 (randomized goals)",
            "brief_description": "MATRPO baseline evaluated on C2 with randomized goals; fails to learn successful policies under increased variation (random goals and start positions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MATRPO agent (Multi-Agent TRPO baseline)",
            "agent_description": "Multi-agent TRPO variant using action-value information of other agents; trained jointly without curriculum separation.",
            "agent_type": "simulated mobile vehicle (car) agent",
            "environment_name": "C2 (lane-change, randomized goals)",
            "environment_description": "Two-car lane-change with randomized non-adjacent goals per episode and randomized start positions (top-left/bottom-left quarters); higher variation introduces many interaction cases.",
            "complexity_measure": "2 agents with randomized goals per episode; variable start positions and continuous dynamics.",
            "complexity_level": "medium",
            "variation_measure": "High (randomized goals per episode and start positions across quarters)",
            "variation_level": "high",
            "performance_metric": "Success rate",
            "performance_value": "Success rate = 0 ± 0% (MATRPO failed to learn successful policies for both agents in C2 across runs)",
            "complexity_variation_relationship": "Authors highlight that increased variation (random goals) and non-stationarity during co-training caused MATRPO to converge to degenerate solutions where one agent is successful and the other is stuck in a local optimum, producing 0% joint success.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "0 ± 0% success rate",
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-stage multi-agent TRPO (MATRPO baseline)",
            "generalization_tested": false,
            "generalization_results": "N/A (baseline failed to learn joint-successful policies in this high-variation setting, so meaningful generalization not reported)",
            "sample_efficiency": "MATRPO displayed one-agent-first convergence behavior: one agent converged earlier (orange) and the other adapted later; in one run orange converged by ~800 iterations but that adaptation produced poor joint solutions overall.",
            "key_findings": "MATRPO fails catastrophically on C2 when variation (random goals) is present — illustrating that multi-agent training without pacing/curriculum can produce mutual-adaptation traps and poor joint outcomes.",
            "uuid": "e1017.5"
        },
        {
            "name_short": "MATRPO-R2",
            "name_full": "Multi-Agent TRPO baseline on R2 (two-robot navigation)",
            "brief_description": "MATRPO baseline tested on two-robot crossing navigation (R2) shows highly variable and often poor performance compared to IATRPO.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MATRPO agent (Multi-Agent TRPO baseline)",
            "agent_description": "Multi-agent TRPO variant augmented with other agents' action values; no curriculum; decentralized policy but with centralized critic information.",
            "agent_type": "simulated mobile robot (unicycle model)",
            "environment_name": "R2 (two-robot crossing navigation)",
            "environment_description": "Two robots crossing to reach opposite-side goals; random start placement in left/right/bottom regions; requires interactive negotiation to avoid collisions.",
            "complexity_measure": "2 agents with crossing interactions; randomized start placement; continuous unicycle dynamics; collisions penalized.",
            "complexity_level": "medium-to-high",
            "variation_measure": "Moderate-to-high variation due to randomized starts and possible goal permutations (as configured); observation/action noise present.",
            "variation_level": "medium-high",
            "performance_metric": "Success rate",
            "performance_value": "Success rate = 36.92 ± 45.22% (large variance across 5 runs; in half of random runs MATRPO did not learn successful policy for both agents)",
            "complexity_variation_relationship": "Shows that in an interactive crossing task, MATRPO often learns unbalanced/stereotyped policies and fails under co-evolving agents; increased complexity/variation magnifies instability and poor coordination.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-stage multi-agent TRPO (MATRPO baseline with access to other agents' action-values)",
            "generalization_tested": true,
            "generalization_results": "Mixed-agent tests produced catastrophic degradation for MATRPO on R2 (2.18 ± 8.56% success rate when pairing separately trained agents), indicating strong dependence and poor generalization.",
            "sample_efficiency": "High variance in runs; some runs failed to learn joint-successful policies within the training schedule; exact sample counts not provided.",
            "key_findings": "MATRPO produces unstable results on R2 with large variance and frequent failure to produce coordinated policies; generalization to mixed agents nearly collapses, illustrating vulnerability to mutual adaptation and dependence on co-training.",
            "uuid": "e1017.6"
        },
        {
            "name_short": "MATRPO-R3",
            "name_full": "Multi-Agent TRPO baseline on R3 (three-robot navigation)",
            "brief_description": "MATRPO baseline on the 3-robot navigation environment R3 fails to learn coordinated multi-agent behavior (0% success) under this higher-complexity setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MATRPO agent (Multi-Agent TRPO baseline)",
            "agent_description": "Multi-agent TRPO variant with centralized critic inputs (other agents' action-values); trained jointly without curriculum separation.",
            "agent_type": "simulated mobile robots (unicycle model)",
            "environment_name": "R3 (three-robot crossing navigation)",
            "environment_description": "Three robots required to cross/coordinate to reach goals; higher interaction complexity and potential three-way conflicts; randomized starting positions across regions; collision penalties.",
            "complexity_measure": "Number of agents = 3 (tri-party interactions); continuous dynamics; randomized starts; collision detection.",
            "complexity_level": "high",
            "variation_measure": "High (randomized starts leading to many distinct interaction patterns), observation/action noise present.",
            "variation_level": "high",
            "performance_metric": "Success rate",
            "performance_value": "Success rate = 0 ± 0% (MATRPO failed to learn successful policies for R3 across runs)",
            "complexity_variation_relationship": "Authors report MATRPO's complete failure at R3: increasing agent count (complexity) combined with interaction variation causes baseline to get stuck in poor local optima, while IATRPO remains effective.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "0 ± 0% success rate",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-stage multi-agent TRPO (MATRPO baseline)",
            "generalization_tested": false,
            "generalization_results": "N/A (baseline did not learn joint-successful behavior in training runs, so generalization not meaningful)",
            "sample_efficiency": "Not reported; baseline failed to converge to successful joint policies under the training regime used.",
            "key_findings": "MATRPO is unable to learn coordinated policies in a 3-agent high-complexity environment, underscoring the limitation of directly joint-training without pacing/curriculum in multi-agent embodied tasks.",
            "uuid": "e1017.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "rating": 2,
            "sanitized_title": "multiagent_actorcritic_for_mixed_cooperativecompetitive_environments"
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 2,
            "sanitized_title": "curriculum_learning"
        },
        {
            "paper_title": "Emergent complexity via multi-agent competition",
            "rating": 1,
            "sanitized_title": "emergent_complexity_via_multiagent_competition"
        },
        {
            "paper_title": "Counterfactual multi-agent policy gradients",
            "rating": 1,
            "sanitized_title": "counterfactual_multiagent_policy_gradients"
        },
        {
            "paper_title": "Intention-aware online pomdp planning for autonomous driving in a crowd",
            "rating": 1,
            "sanitized_title": "intentionaware_online_pomdp_planning_for_autonomous_driving_in_a_crowd"
        }
    ],
    "cost": 0.016406749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals</p>
<p>Anahita Mohseni-Kabir 
David Isele 
Kikuo Fujimura 
Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals</p>
<p>In a multi-agent setting, the optimal policy of a single agent is largely dependent on the behavior of other agents. We investigate the problem of multi-agent reinforcement learning, focusing on decentralized learning in non-stationary domains for mobile robot navigation. We identify a cause for the difficulty in training non-stationary policies: mutual adaptation to sub-optimal behaviors, and we use this to motivate a curriculum-based strategy for learning interactive policies. The curriculum has two stages. First, the agent leverages policy gradient algorithms to learn a policy that is capable of achieving multiple goals. Second, the agent learns a modifier policy to learn how to interact with other agents in a multi-agent setting. We evaluated our approach on both an autonomous driving lane-change domain and a robot navigation domain.</p>
<p>I. INTRODUCTION</p>
<p>Single agent reinforcement learning (RL) algorithms have made significant progress in game playing [20] and robotics [13], however, single agent learning algorithms in multi-agent settings are prone to learn stereotyped behaviors that over-fit to the training environment [22], [15]. There are several reasons why multi-agent environments are more difficult: 1) interacting with an unknown agent requires having either multiple responses to a given situation or a more nuanced ability to perceive differences. The former breaks the Markov assumption, the latter rules out simpler solutions which are likely to be found first. 2) Intentions and goals of other agents are not known and must be inferred. This also can break the Markov assumption. 3) Agents are co-evolving, and their policies are non-stationary during training. In this work we investigate a property associated with non-stationary policies: partially successful policies for one agent can get repeated multiple times or 'burned in', causing the other agents to adapt to that specific behavior. This sets off a chain of mutual adaptation that encourages agents to only visit suboptimal regions of the state space.</p>
<p>When training independent agents [27], the competing learning processes of multiple agents is sufficiently difficult that either the agents fail to learn, or the agents learn one-after-the-other, resulting in stereotyped policies that are sensitive to the behavior of other agents. Recent approaches to multi-agent reinforcement learning have relaxed the independence of an agent by exploiting centralized training and assuming the other agents' actions are known [8], [18]. While these techniques are more successful, we show that they  still exhibit one-after-the-other learning, producing highlydependent policies. We propose the use of an independent curriculum-based training procedure for learning policies that avoids mutual adaptation without using either centralized training or knowledge of the other agents actions. We start from the observation that when agents learn at different rates (as often happens under random initialization), one agent learns a policy around the other agents' policies. Since the policies being accommodated are suboptimal during the early stages of learning, the agents drive each other into poor local optima. One solution to pacing the learning of multiple agents is self-play [26], but self-play requires symmetric agents. As an alternative, we consider learning via a curriculum [4]. The use of curriculum learning lets us pace the learning of each agent, while allowing us to handle the case of agents with different goals.</p>
<p>We structure our curriculum by first learning an optimal single agent and then explicitly learning the modifications required to interact with other agents. This approach leverages the intuition that when other agents are not present in the environment, the agent should behave like a single agent trying to reach a goal. We address the interactionaware decision making problem in the second stage of the curriculum. In the second stage, we introduce an architecture that uses the learned single-agent policy, and adjusts it with a learned interactive multi-agent policy.</p>
<p>We consider two robotic applications where the agents policies must be mutually consistent in order to achieve the intended goals: a lane-change scenario and a mobile navigation scenario. The agents do not have access to the goals or intentions of other agents and are learning different policies simultaneously. We show that not only is our curriculumbased approach better able to learn the desired behaviors, but that the learned policies also generalize against agents that were not included in the training process.</p>
<p>II. RELATED WORK</p>
<p>Multiple works have focused on reinforcement learning methods for multi-agent domains in fully cooperative, fully competitive, and mixed environments [6]. Foerster et al. [8] propose a multi-agent actor-critic method to address the challenges of multi-agent credit assignment. Different from our work where each agent pursues its individual goal, their approach is appropriate for problems with a single shared task. Lowe et al. [18] propose an actor-critic approach, called MADDPG, that augments the agent's critic with action policies of other agents and is able to successfully learn the coordination policy between multiple agents. Unlike our approach these methods use centralized training.</p>
<p>He et al. [10] presents new models based on deep Qnetwork for decision-making in adversarial games which jointly learns a policy and different strategies of opponents. Their proposed approach is appropriate for two agents, whereas our approach is tested on more than two agents. Furthermore, our approach is demonstrated in the nonstationary case where the agents are co-evolving together while the opponents in their approach have a set of fixed strategies. Some work leverages self-play to provide the agents with a curriculum for learning complex competitive tasks [3]. They use dense rewards in the beginning phase of the training to allow the agents to learn basic motor skills. In our environments, it is difficult to create motor skills or define a reward function for them. In our problems, self-play using single agent RL algorithms failed to learn successful policies since the environment is non-stationary, and also without cooperation the agents were not able to even occasionally succeed in their tasks thus were not able to learn successful policies.</p>
<p>In social dilemma research, most works focus on oneshot or repeated tasks [19], [9] and ignore that in real world scenarios the behaviors are temporally extended. Among the most relevant RL approaches in this area, is work on the sequential prisoner's dilemma (SPD) [16] which leverages deep Q-networks to study effects of environmental parameters on the agents degree of cooperation. In contrast to this work that focuses on the impact of the games' parameters on the agents' behavior, we provide a multi-agent learning approach for cooperative problems with individual goals. Another relevant work [28] proposes a deep RL approach for mutual cooperation in SPD games. Their approach adaptively selects its policy with the proper cooperation degree based on the detected cooperation degree of an opponent. Different from their approach, our approach is not specific to two player games. In addition, since we focus on mobile navigation problems our approach learns how to react to the continuous policies of other agents, not just their cooperation degree.</p>
<p>Significant amount of work has focused on motion planning for autonomous vehicles [21] where the problem of intention prediction or trajectory estimation of other agents has been studied. Among these, relevant work has focused on intention-aware POMDP planning for autonomous vehicles [2]. These methods leverage machine learning methods to learn models of other agents as also surveyed in [1]. In contrast to these approaches, we focus on RL algorithms for interaction-aware agents where the agents are co-evolving together and their motion models are dependent on others policies. Another work [5] presents an approach for computing optimal trajectories for multiple robots in a distributed fashion.</p>
<p>III. APPROACH</p>
<p>We focus on multi-goal multi-agent settings, where each agent cooperates with other agents in order to accomplish their individual goals. We leverage the intuition that in many settings, like autonomous driving, the interaction between multiple agents is limited to certain parts of the state space where conflict of interest is present, otherwise the agent behaves according to its single-agent policy. I.e., the agent starts with its own single-agent policy and adapts it to account for the multiple agents that appear in the environment. We propose a two stages approach to learn multiple interactive policies for multiple agents. In the first stage of learning, the agents learn a single agent policy to accomplish their individual goals. The learned single agent policies are then passed to the multi-agent model that enables each agent to learn an interactive policy to account for the other agents.</p>
<p>A. Single Agent Module</p>
<p>We model an agent with individual goal as a Markov Decision Process (MDP) [11] with goals [23]. The MDP is defined as a tuple &lt; S, O, A, P, R, G, G, γ &gt; in which S represents the state of the world, O represents the agent's observation, A is a set of actions, P : S ×A → S determines the distribution over next states, G is the agent's goal, G is the goal distribution, R : S ×G ×A → R is an immediate reward function, and γ ∈ [0, 1] is the discount factor. The solution to an MDP is a policy π θ : O × G × A → [0, 1] where θ is the parameters of the policy. For continuous actions, π θ is assumed to be Gaussian, and in our work the mean is represented by a neural network with parameters θ. The robot seeks to find a policy π θ that maximizes the expected future discounted reward R = T t=0 γ t r t . In the following paragraphs, we add subscript "self" or "s" to our notation to show the agent's own properties and "single" or "sng" to highlight the single agent scenario.</p>
<p>We use a decentralized actor-critic policy gradient algorithm to learn the single agent policies. The single agent model gets observation o s and goal g s as inputs, and outputs an action a s that the agent should take. Each agent learns a policy π sng (a s |o s , g s ), according to its individual goalspecific reward function R sng (s s , a s , g s ) in the absence of other agents. The decentralized actor-critic policy gradient algorithm maximizes J sng (θ) = Eo s∼p π s ,as∼πsng,gs∼G [R sng ] by ascending the following gradient: Where p π s is the state distribution, and A π is the advantage function [24]. We use τ ∼ p θ (τ ) to refer to o s ∼ p π s , a s ∼ π sng . For simplicity, θ is removed. The gray colored boxes in Fig. 2 show the actor-critic model for the single agent.
∇ θ J(θ) = E τ ∼p θ (τ ),gs∼G [∇ θ log π sng (a s |o s , g s ) A π (o s , a s , g s )]</p>
<p>B. Multiple Agents Module</p>
<p>We assume that each agent has a noisy estimate of the other agents' states, but they don't have access to the other agents actions or intentions. We model the multiagent decision problem as a Markov Game [17], modified to accommodate mixed goals, and defined as the tuple 
&lt; N, S, {O i } i∈N , {A i } i∈N , {R i } i∈N , {G i } i∈N , P, G, γ &gt; with NO i × G × A i → [0, 1]
where θ i is the parameters of the policy. For continuous actions problems, π θi is assumed to be a Gaussian where the mean is modeled by neural networks. Each agent seeks to find a policy π θi that maximizes its own expected future discounted reward R i = T t=0 γ t r it . In the following paragraphs, we remove i for simplicity and add subscript "self" or "s" to our notation to show the agent's own properties. We add "multi" or "mlt" to highlight the multi-agent scenario, and subscript "others" or "o" refers to other agents properties.</p>
<p>We modify each agent's R sng to account for the presence of other agents in the environment. Each agent is rewarded based on its individual objective, but is punished if it gets into conflicts (e.g., collisions in mobile agent scenarios) with other agents. The new reward function for each agent is as follows where C is a positive constant that penalizes the agent for conflicts, and 1 conf lict (s s , s o ) determines if conflict is present:
R mlt (s s , a s , g s , s o ) = R sng (s s , a s , g s ) − C × 1 conf lict (s s , s o )
We use a decentralized actor-critic policy gradient algorithm to learn the multi-agent policies. Each agent learns an actor-critic model that accounts for the multiple agents in the environment. The model gets observation o s , goal g s , and o o as inputs, and outputs an action a s that the agent should take. Each agent learns a policy π mlt (a s |o s , g s , o o ), according to its multi-agent goal-specific reward function R mlt (s s , a s , g s , o o ) in the presence of other agents. The decentralized actor-critic policy gradient algorithm maximizes J mlt (θ) = Eo s∼p π s ,as∼π mlt ,gs∼G,oo∼p π o [R mlt ] by ascending the gradient:
∇ θ J(θ) = E τ ∼p θ (τ ),gs∼G [∇ θ log π mlt (a s |o s , g s , o o ) A π (o s , a s , g s , o s )]
Where p π o is the other agents' state distribution. We use Fig. 2 shows our architecture for both reaching the individual goal and cooperative non-conflicting behavior. In this architecture, each agent leverages its learned single agent actor-critic models. The learned model is frozen and combined with another multi-agent model that addresses the cooperative non-conflicting behavior.
τ ∼ p θ (τ ) to refer to o s ∼ p π s , a s ∼ π mlt , o o ∼ p π o .
The multi-agent module includes the single agent (SA) module from the previous stage of the curriculum. The output of the single agent model is combined with the output of a multi-agent (MA) module to learn a policy that modifies the single agent value functions to account for the other agents. The agent's own state o s and an estimation of the other agents state o o are passed to the actor and critic models of the multi-agent module. In Fig. 2, we only show the actor models for simplicity, the critic models have the same structure. In this work, we used a summation to combine the single agent and multi-agent models since we found it sufficient in our experiments.</p>
<p>IV. EXPERIMENTS</p>
<p>In this section, first we discuss our network architecture. We then delve into the details of the environments and experimental setup, and then discuss our results.</p>
<p>A. Algorithm and Network Architecture</p>
<p>We use the Trust Region Policy Optimization (TRPO) [24] algorithm to learn the actor-critic models. We use the same architecture and parameters as the OpenAI baseline implementation [7]. We use ReLU as the activation function instead of tanh in the original implementation. The actorcritic models each have 2 hidden layers with 128 neurons. We call our approach Interaction-Aware TRPO or IATRPO.</p>
<p>B. Simulation Environments</p>
<p>We tested our proposed approach on the following two environments. Both environments are designed such that interaction is required for successfully achieving the individual goals. The environments are shown in Fig. 3.  The cars use a bicycle kinematics model [14].</p>
<p>The reward function for the single agent scenario and the multi-agents scenario are as follows with a reward scale 3. Function d computes the euclidean distance between the center of the car and agent's goal. Function collision(s s , env) or collision(s s , s o ) specify if the agent is in collision with the environment or the other agents respectively.
R sng (s s , a s , g s ) =      −1, if collision(s s , env) 1, if d(s s , g s ) &lt; 0.4 d(ss,gs) 1000 , otherwise R mlt (s s , a s , g s , s o ) =          −1, if collision(s s , env) −1, if collision(s s , s o ) 1, if d(s s , g s ) &lt; 0.4 d(ss,gs)
1000 , otherwise 2) Multi-Robot Navigation: This environment consists of two or more mobile robots that are crossing one another to go to their goal destination (matching color stars). Three fixed goals are located at the top, left and right sides of the course. The agents' positions are selected randomly in the left (goal in right region of the environment), right (goal in left region), and bottom (goal in top region) of the course to assure that the agents pass one another to go to their goal position. The 2 agent environment has the same setting, but the bottom (gray) agent is not present. We call the environment with 2 agents and 3 agents "R2" and "R3" respectively. The robots have the same state space and parameters as the cars in the lane changing environment. The mobile robots use the unicycle kinematics model [12]. The reward function is as before.</p>
<p>C. Results</p>
<p>We provide quantitative and qualitative results on the performance of our approach. As our baseline, we compare against MADDPG [18]. We leveraged the main contribution of the MADDPG approach and implemented the multi-agent version of the TRPO [24] algorithm (MATRPO) where each agent is provided with the action values of the other agents. TRPO was used in place of DDPG because it was found to consistently outperform DDPG in all our experiments. TRPO was also used to train the IATRPO for all stages of the curriculum. 5 illustrate the agents learned policies on the two environments. In both scenarios, all the agents must cross paths to reach their goal destination (matching color stars). We show the single agent policies with dashed lines, and the multi-agent policies with solid lines. We ran the learned multi-agent models and observed that the agents are successfully able to learn how to interact. We refer to the agents based on their colors or their start positions.</p>
<p>D. Qualitative Results</p>
<p>Figures 4 and</p>
<p>In the multi-agent policy in Fig. 4, the bottom agent (orange or abbreviated as O) slows down for the top agent (blue or B) to pass first and then it goes to its goals (but the single agent trajectory is in collision with the other agent). The top agent (B) also modifies the shape of its trajectory to not get into collision with the bottom agent. In the multiagent policy in Fig. 5, the bottom agent (gray or G) learns to go first with maximum speed, the right agent (B) slows down and modifies the shape of its trajectory for the bottom agent (G) to pass first. The left agent (O) modifies its speed to prevent a collision with the other agents. Notice that the single agent policies differ both in speed and shape from the multi-agent policies, and if all the agents executed their single agent policy, they would have collided with the others. Please refer to the video accompanying the paper to see examples of successful and failed executions.</p>
<p>E. Quantitative Results</p>
<p>We evaluate the IATRPO approach against the MATRPO approach and report the final results in three evaluations:  Success Rate: Table I shows the success rate of our approach against the MATRPO approach. We ran both approaches on the 4 environments with 5 random seeds. An episode is successful if both agents are 0.4 away from their goals d(s self , g self ) &lt; 0.4. To compute the success rate, the final learned policies were run on 1000 random episodes. Both MATRPO and IATRPO approaches give a high accuracy on the C2-fixed environment, but the success rate of the IATRPO algorithm is higher. On the C2 environment, which has a greater amount of randomness in the start position and has random goals, the MATRPO algorithm is not able to learn successful policies for all the agents, but the IATRPO algorithm has 94.3 ± 2.99% success rate. MATRPO learns a successful policy for one agent, but the second agent is stuck in a local optima, having only learned to not collide with the environment or the successful agent. Most of the failure cases in IATRPO happens around the boundaries of the environments or when the agents are too close to one another. We believe this is because of the noise associated with both the agent's action and the observations of others. The performance of the IATRPO algorithm is much higher than the MATRPO algorithm on the R2 environment. In half of the random runs the MATRPO algorithm did not learn a successful policy for both the agents. However, the IATRPO algorithm has a success rate of around 90%. The MATRPO algorithm completely failed to learn a successful policy on the R3 algorithm, but the IATRPO algorithm achieves a success rate of 88.02±4.11%.</p>
<p>Level of Interaction: We measure how interactive are the final policies that are learned by the IATRPO and MATRPO approaches. We ran both approaches on C2-fixed and R2 environments where MATRPO was able to learn successful policies. We performed 5 training runs with random seeds and tested the final learned policies on a 1000 random episodes. We estimate how interactive the policies are by finding which agent reached its goal first. For each agent, we compute the average and standard deviation of the (a) C2 environment.</p>
<p>(b) R3 environment. interactiveness metric on the 5 runs. In each algorithm run if one agent always waits for the other agent to go first and compromises, the agents are considered non-interactive. In a two agents scenario, the ideal case is if both agents have an average of around 50% with a low standard deviation. Fig. 7 shows the results of the two algorithms on the two environments. In each run on the C2-fixed environment where we applied the MATRPO algorithm, one agent always waited for the other agent to go first. However, the IATRPO algorithm was able to learn more interactive policies than the MATRPO policies where both the agents sometimes compromised. Fig. 8 provides more evidence of why, in the MATRPO training, one agent always compromises. Fig. 8 shows the mean episode length of both the algorithms in one of the training runs on the C2-fixed environment. When the mean episode length becomes constant, the agent has converged to a successful policy. In the MATRPO training, the orange agent converges to the successful policy and after about 800 training iterations the blue agent adapts its policy to the orange agent's policy and converges as well. However, with IATRPO the two agents learn a successful policy around the same time.</p>
<p>We applied both the algorithms on the R2 environment and noticed that in the IATRPO algorithm the two agents have a better balance where both the agents achieve the first place about 50% of the times. The agents are less balanced when using the MATRPO approach and have higher variance than the IATRPO approach. We also investigate the influence of our two stages approach on the interactiveness of the agents. We measured the distance between the single agent trajectories and the multi-agent trajectories to measure how much each agent modified its trajectory to account for the other agents in the IATRPO algorithm. We use the Fréchet distance for our comparison. As before, we use the final   learned policy, run it on 1000 random episodes and compute the distance between the single agent module's trajectory and the multi-agent module's trajectory. For each agent, we average the computed distance and use that to compute the overall compromise (%) that each agent makes compared to other agents. Fig. 6 shows the results on the C2 and R3 environments. Although the top agent gets the first place 72.43 ± 37.19% of the times, the changes in the distance is almost equal for the two agents in the C2 environment.</p>
<p>In the R3 environment, the bottom agent (G) always arrives first at the goal, but the distance between its single agent trajectory and multi-agent trajectory is about 20%. This implies that the bottom agent is also trying to adapt its policy to the other agents' policies. The right agent and the left agent get the second place 82.59 ± 22.84% and 18.46 ± 22.64% respectively. The overall impact of the right agent (B) on distance is 45% compared to the left agent (O) 32%. This implies that both agents change their single agent policies, the right agent mostly changes the shape of its policy and the left agent mostly changes its speed to account for the other agents.</p>
<p>Mixed Agents:</p>
<p>We conducted experiments where we look at the performance of agents not trained together. This is a scenario known in the literature to cause agents to fail due to dependent policies [22], [15]. We used the 5 random training runs and generated 20 pairs (or triples) of agents where the agents in each pair are trained separately using a different seed. Table II shows the success rate of the MATRPO and IATRPO algorithms on the four environments. We used the same approach as above to compute the success rate on a 1000 random episodes. The performance of the MATRPO algorithm is not affected much in C2-fixed experiments, but it has drastically decreased in R2 experiments. We believe the reason for this behavior is the following. In C2-fixed experiments with MATRPO, the bottom agent (O) learns to always go first regardless of what the top agent (B) is doing. Even when the bottom agent is tested against other top agents (Bs), both the agents show the same behavior thus the performance of the algorithm does not get affected. However, in the R2 experiments with MATRPO, the agents show a more interactive behavior than C2-fixed thus when we test the agents, which were not trained together, against each other, the performance drastically decreases. The success rate of the IATRPO algorithm also decreases when we test it on the 20 pairs (or triples). The performance degrades less on the easier environments such as C2-fixed and on the environments where the agents learned a more interactive policy with low variance such as R2. The success rate for the C2 and R3 degrades more than R2 since the agents learned a less interactive behavior with high variance.</p>
<p>V. CONCLUSIONS</p>
<p>We focus on multi-agent settings where each agent learns a policy to simultaneously achieve its individual goal and interact with others. We provide a curriculum learning approach and a architecture that learn how to adapt single agent policies to the multi-agent setting. We tested the method on two robotics problems and observed that our approach outperforms the state-of-the-art approach and results in interactive policies.</p>
<p>Our formulation is generalizable to domains with inhomogeneous agents since we make no assumptions regarding the homogeneity of the agents, and our future work involves testing the approach on such domains. One method that we are planning to try is the ensemble of policies method proposed in [3]. If a model is learned in an environment with N agents, we can apply the same model on an environment with &lt;= N agents where we assume the non-existent agent is in a corner and is not interacting with others. However, a limitation of our work is that a new model should be learned if we increase the number of agents. We believe this issue can be addressed by leveraging an approach that is agnostic to the number of agents such as [25].</p>
<p>Fig. 1 :
1Double lane-change problem. The bottom car and the top car are crossing one another to go to the star on the top right and bottom right of the environment respectively.</p>
<p>Fig. 2 :
2Multi-agent (MA) actor critic model. The gray colored boxes are the single agent policies and are frozen during training in the multi-agent setting.</p>
<p>agents. The possible configurations of the agents is specified by S. Each agent i gets an observation o i ∈ O i which includes both the agent's observation of its own state o s and an observation of other agents o o . Each agent has its own set of actions A i , a goal G i ∼ G, and a reward function R i : S ×G ×A i → R. The markov game includes a transition function P : S × A 1 × ... × A N → S which determines the distribution over next states. The solution to the markov game is a policy for each agent i π θi :</p>
<p>-change environment. The bottom and top agents start from the bottom left and top left quarters respectively. (b) Robot navigation environment. Each agent starts from a random position in their corresponding side of the course.</p>
<p>Fig. 3 :
3Environments. Agents are crossing one another to go to their opposite side of the course.1) Lane-change: This environment consists of two cars that are crossing to go to their goal destination (matching color stars). At each episode a pair of non-adjacent goals are selected and are kept constant throughout the episode. The agents' positions are selected randomly in the top left and bottom left quarters of the course. The agents start with a 0 velocity, 0 angular velocity, and 0 heading angle. We call this environment "C2". We created an easier version of this environment where the goals are fixed to the 1st and 3rd lanes, and the agents' position has randomness only in the +x direction. We call this environment "C2-fixed".The agent's state includes itsx and y position, velocity, angular velocity, heading angle, if it is broken due to collision with other agents or the environment, and if it has reached its goal. The observation noise for s s is in [−0.01, 0.01]. The cars have acceleration and angular acceleration as their actions a s with uniform noise in [−0.1, 0.1]. The car can reach a minimum and maximum velocity of −1 and 1 respectively, and a minimum and maximum angular velocity of −1 and 1 respectively. The multi-agent module for each agent has access to other agent's x and y positions, velocity, heading, angular velocity with a uniform noise in [−0.1, 0.1].</p>
<p>Fig. 4 :
4IATRPO's final policy on the lane-change environment. Fig. 5: IATRPO's final policy on the robot navigation environment.</p>
<p>Fig. 6 :
6Fréchet distance between the single agent trajectories and the multi-agent trajectories in IATRPO algorithm.</p>
<p>Fig. 7 :
7Shows which agent achieved its goal first. (a) MATRPO training. (b) IATRPO training.</p>
<p>Fig. 8 :
8Mean episode length in one experiment for training multi-agent policies on the C2-fixed. IATRPO uses a curriculum so the number of iterations is not comparable.</p>
<p>TABLE I :
ISuccess of the algorithms on the 4 environments.Environment 
MATRPO success rate (%) 
IATRPO success rate (%) 
C2-fixed 
97.88 ± 0.31 
99.4 ± 0.33 
C2 
0 ± 0 
94.3 ± 2.99 
R2 
36.92 ± 45.22 
90.96 ± 1.43 
R3 
0 ± 0 
88.02 ± 4.11 </p>
<p>TABLE II :
IISuccess rate of the algorithms on the 4 environments when tested on agents not trained together.Environment 
MATRPO success rate (%) 
IATRPO success rate (%) 
C2-fixed 
97.83 ± 0.72 
98.71 ± 1.29 
C2 
NA 
69.22 ± 26.44 
R2 
2.18 ± 8.56 
77.05 ± 9.46 
R3 
NA 
68.87 ± 17.44 </p>
<p>Autonomous agents modelling other agents: A comprehensive survey and open problems. S V Albrecht, P Stone, Artificial Intelligence. S. V. Albrecht and P. Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intel- ligence, 2018.</p>
<p>Intention-aware online pomdp planning for autonomous driving in a crowd. H Bai, S Cai, N Ye, D Hsu, W S Lee, ICRA. H. Bai, S. Cai, N. Ye, D. Hsu, and WS. Lee. Intention-aware online pomdp planning for autonomous driving in a crowd. In ICRA, 2015.</p>
<p>Emergent complexity via multi-agent competition. T Bansal, J Pachocki, S Sidor, I Sutskever, I Mordatch, arXiv:1710.03748arXiv preprintT. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mordatch. Emergent complexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017.</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, ICML. Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.</p>
<p>Multi-agent path planning with multiple tasks and distance constraints. S Bhattacharya, M Likhachev, V Kumar, ICRA. S. Bhattacharya, M. Likhachev, and V. Kumar. Multi-agent path planning with multiple tasks and distance constraints. In ICRA, 2010.</p>
<p>A comprehensive survey of multiagent reinforcement learning. L Busoniu, R Babuska, B. De Schutter, IEEE SMC-Part C. L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE SMC-Part C, 2008.</p>
<p>. P Dhariwal, C Hesse, O Klimov, P. Dhariwal, C. Hesse, O. Klimov, et al. Openai baselines.</p>
<p>J Foerster, G Farquhar, T Afouras, N Nardelli, S Whiteson, arXiv:1705.08926Counterfactual multi-agent policy gradients. arXiv preprintJ. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. White- son. Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926, 2017.</p>
<p>Introducing decision entrustment mechanism into repeated bilateral agent interactions to achieve social optimality. J Hao, H Leung, AAMAS. J. Hao and H. Leung. Introducing decision entrustment mechanism into repeated bilateral agent interactions to achieve social optimality. AAMAS, 2015.</p>
<p>Opponent modeling in deep reinforcement learning. H He, J Boyd-Graber, K Kwok, H Daumé, ICML. H. He, J. Boyd-Graber, K. Kwok, and H. Daumé III. Opponent modeling in deep reinforcement learning. In ICML, 2016.</p>
<p>Markov decision processes. C C White, D J White, EJORC. C. White III and D. J. White. Markov decision processes. EJOR, 1989.</p>
<p>Kinematic time-invariant control of a 2d nonholonomic vehicle. G Indiveri, CDC. G. Indiveri. Kinematic time-invariant control of a 2d nonholonomic vehicle. In CDC, 1999.</p>
<p>J Kober, J A Bagnell, J Peters, Reinforcement learning in robotics: A survey. IJRR. J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. IJRR, 2013.</p>
<p>Kinematic and dynamic vehicle models for autonomous driving control design. J Kong, M Pfeiffer, G Schildbach, F Borrelli, IVJ. Kong, M. Pfeiffer, G. Schildbach, and F. Borrelli. Kinematic and dynamic vehicle models for autonomous driving control design. In IV, 2015.</p>
<p>A unified game-theoretic approach to multiagent reinforcement learning. M Lanctot, V Zambaldi, A Gruslys, NIPS. M. Lanctot, V. Zambaldi, A. Gruslys, et al. A unified game-theoretic approach to multiagent reinforcement learning. In NIPS, 2017.</p>
<p>Multi-agent reinforcement learning in sequential social dilemmas. J Z Leibo, V Zambaldi, M Lanctot, J Marecki, T Graepel, AAMAS. J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel. Multi-agent reinforcement learning in sequential social dilemmas. In AAMAS, 2017.</p>
<p>Markov games as a framework for multi-agent reinforcement learning. M L Littman, Machine Learning Proceedings. M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings. 1994.</p>
<p>Multiagent actor-critic for mixed cooperative-competitive environments. R Lowe, Y Wu, A Tamar, J Harb, P Abbeel, I Mordatch, NIPS. R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi- agent actor-critic for mixed cooperative-competitive environments. In NIPS, 2017.</p>
<p>New winning strategies for the iterated prisoner's dilemma. P Mathieu, J Delahaye, AAMAS. P. Mathieu and J. Delahaye. New winning strategies for the iterated prisoner's dilemma. In AAMAS, 2015.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, Nature. V. Mnih, K. Kavukcuoglu, D. Silver, et al. Human-level control through deep reinforcement learning. Nature, 2015.</p>
<p>A survey of motion planning and control techniques for self-driving urban vehicles. B Paden, M Čáp, S Z Yong, D Yershov, E Frazzoli, IVB. Paden, M.Čáp, SZ. Yong, D. Yershov, and E. Frazzoli. A survey of motion planning and control techniques for self-driving urban vehicles. IV, 2016.</p>
<p>Can deep reinforcement learning solve erdos-selfridge-spencer games?. M Raghu, A Irpan, J Andreas, R Kleinberg, Q V Le, J Kleinberg, arXiv:1711.02301arXiv preprintM. Raghu, A. Irpan, J. Andreas, R. Kleinberg, Q. V. Le, and J. Klein- berg. Can deep reinforcement learning solve erdos-selfridge-spencer games? arXiv preprint arXiv:1711.02301, 2017.</p>
<p>Universal value function approximators. T Schaul, D Horgan, K Gregor, D Silver, ICML. T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In ICML, 2015.</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, ICML. J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In ICML, 2015.</p>
<p>Zero shot transfer learning for robot soccer. D Schwab, Y Zhu, M Veloso, AAMAS. D. Schwab, Y. Zhu, and M. Veloso. Zero shot transfer learning for robot soccer. In AAMAS, 2018.</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016.</p>
<p>Multi-agent reinforcement learning: Independent vs. cooperative agents. M Tan, ICML. M. Tan. Multi-agent reinforcement learning: Independent vs. cooper- ative agents. In ICML, 1993.</p>
<p>Towards cooperation in sequential prisoner's dilemmas: a deep multiagent reinforcement learning approach. W Wang, J Hao, Y Wang, M Taylor, arXiv:1803.00162arXiv preprintW. Wang, J. Hao, Y. Wang, and M. Taylor. Towards cooperation in sequential prisoner's dilemmas: a deep multiagent reinforcement learning approach. arXiv preprint arXiv:1803.00162, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2294 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2294</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2294</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-274233896</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.15525v1.pdf" target="_blank">Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, the introduction of AI technologies has brought transformative changes to scientific computing. However, AI models typically focus on single-task and single-modal data processing, limiting their application. To address this, multimodal scientific computing frameworks have become a trend. The Botfip framework aligns function images with symbolic operation trees through multimodal training, extracting deep scientific information. However, Botfip struggles with processing Formula Strings, leading to inadequate understanding in multimodal learning. To enhance Botfip's learning of Formula Strings and expand its applicability to related tasks, we propose the Botfip-LLM framework based on knowledge distillation, incorporating pre-trained large language models for aligning symbolic tree data. Experimental analysis shows that the choice of LLM is crucial, with ChatGLM-2 outperforming others in training and testing. Botfip-LLM not only improves performance, generalization, and extrapolation over the original Botfip model but also significantly enhances applicability to Formula String-related tasks, enabling more diverse task handling.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2294.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2294.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Botfip-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Botfip-LLM multimodal scientific computing framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal scientific computing model that aligns function images, symbolic operation tree sequences (OTS), and formula strings by distilling knowledge from frozen large language models (LLMs) into image and sequence encoders via contrastive and matching objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Symbolic regression / Scientific computing (multimodal mapping between function images, symbolic operation trees, and formula strings)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn joint representations and mappings among function images, symbolic operation tree sequences (OTS) and symbolic formula strings so the model can (a) generate OTS from function images, (b) generate symbolic formulas from OTS, and (c) convert formula strings into OTS — enabling multimodal symbolic regression and formula/structure recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate and synthetic: authors built a pretraining dataset of 11,028 skeletons and 551,400 Funcimg-OTS pairs (function images paired with OTS and formula strings). Data are paired and labeled (OTS/formula for each function image) and reported as open-sourced in the Botfip line of work; authors note that general scientific-computing corpora are far smaller than typical NLP/vision corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Multimodal: multi-scale numerical function images (gridded arrays), discrete OTS sequences (token sequences representing operation-tree skeletons and constant vectors), and unstructured/semi-structured formula strings (text sequences of mathematical expressions).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: multimodal alignment across heterogeneous representations; symbolic formulas can be deeply nested and ambiguous (many OTS/constant-vector pairs can map to the same formula); search/sequence length grows with tree node count; constant optimization handled separately via L-BFGS; mapping is not one-to-one, increasing combinatorial complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: AI-for-Science and symbolic regression have active prior art (e.g., DSR, PYSR, SymFormer) but the multimodal approach (image + symbolic tree + formula string alignment with LLM distillation) is novel and builds on recent transformer/vision-language advances.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - interpretability and symbolic output are core goals: outputs are human-interpretable symbolic expressions and operation trees, so mechanistic/causal interpretability is required rather than purely black-box prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Multimodal deep learning with contrastive learning and LLM-guided knowledge distillation (ViT + BERT + frozen LLM + MLP embedder; encoder-decoder OTS generation)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Architecture: Funcimg encoder (Vision Transformer or variant) extracts multi-scale image features; OTS encoder/decoder (BERT backbone) encodes/generates operation-tree sequences; a pre-trained frozen LLM (various off-the-shelf LLM checkpoints) encodes formula strings into high-dimensional hidden states which are projected via a small trainable MLP embedder into the shared feature dimension. Training losses: Function Image-OTS Contrastive Loss (InfoNCE), Function Image-OTS Matching Loss (binary classification), OTS Modeling Loss (sequence prediction/cross-entropy), and LLM Knowledge Distillation Loss (similarity-based alignment between LLM features and student encoders). Queues (MoCo-style) are used to provide negative samples for contrastive losses. During fine-tuning, causal-attention is used to generate OTS conditioned on either image features, OTS context, or embedded formula-string features. Constants are optimized with L-BFGS during inference after skeleton generation. The LLM parameters are frozen; only a small embedder is trained. The implementation supports distributed training by hosting the frozen LLM on a single GPU and aggregating features to it, optionally using quantization (half-precision, 8-bit/4-bit).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Hybrid: supervised multimodal representation learning + contrastive learning + knowledge distillation from foundation LLMs; sequence generation (supervised) for symbolic outputs</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate: method directly targets multimodal symbolic regression and formula↔structure conversion problems; modifications (frozen LLM + embedder, MoCo queues, separate constant optimization) adapt standard multimodal techniques to resource- and data-limited scientific computing contexts. Limitations: requires paired Funcimg-OTS-Formula data (authors generated synthetic pairs) and success depends strongly on choice of teacher LLM and pretraining corpus compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Funcimg-OTS fine-tuning mean loss (at step 25k) improved vs baseline: Botfip (baseline) mean error ≈ 0.668 ± 0.006 (node=5) while Botfip-LLM (ChatGLM-2) ≈ 0.621 ± 0.006 (node=5) — improvements seen across node counts (Table 3). Formula String→OTS fine-tuning error (Table 4): Botfip-LLM with ChatGLM-2: 0.681 ± 0.007 (node=5) up to 0.796 ± 0.013 (node=9); with RWKV-6 errors substantially higher (0.929 ±0.012 to 1.070 ±0.021), showing quantitative benefit of LLM choice. Validation metrics (Acc_r, SRL, SRL) showed Botfip-LLM (ChatGLM-2) slightly surpassing original Botfip across varying tree node counts (exact per-node values are reported in paper figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective: Botfip-LLM improves representational alignment across modalities and enriches downstream task capability (enables formula-string→OTS conversion that original Botfip could not). The framework achieves faster convergence and lower steady-state loss when pretrained with well-matched LLMs (notably ChatGLM-2). Limitations observed: improvements are modest for very large/complex OTS (extrapolation still lags behind iterative symbolic search methods like PYSR and DSR), and non-Transformer LLM teachers (RWKV, Mamba) were less effective or failed to transfer useful features with the chosen similarity-distillation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High within symbolic scientific computing: enables multimodal integration (images, symbolic trees, formula strings), expands downstream tasks (bi-directional formula↔OTS conversion), and reduces need to train huge LLMs end-to-end by distillation. Practical benefits include broader applicability to tasks that require symbolic interpretability and potential time savings in model development (frozen LLMs + small embedders). Scalability depends on teacher LLM availability and dataset generation for pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with original Botfip (no LLM distillation), Botfip-LLM (with a well-chosen LLM such as ChatGLM-2) produced lower fine-tuning losses and slightly better Acc_r/SRL/SRL. Compared to iterative symbolic regression methods (PYSR, DSR), Botfip-LLM improved extrapolation relative to original Botfip but remains inferior in error-correction/extrapolation for very complex trees; iterative SR methods still outperform for large-node counts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key contributors: (1) choice of teacher LLM — transformer-based bilingual/Chinese-heavy LLM ChatGLM-2 matched domain corpus and hidden-size compatibility yielded best transfer; (2) similarity-based distillation aligning LLM hidden states with student encoders; (3) multimodal contrastive objectives (InfoNCE) and matching losses for tight alignment; (4) engineering techniques: frozen LLM + small embedder, distributed aggregation, and quantization to enable training under limited GPU memory; (5) use of a synthetic but large paired Funcimg-OTS-Formula dataset to bootstrap multimodal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Distilling semantic features from an appropriately chosen frozen LLM into multimodal encoders via similarity-based contrastive alignment substantially improves the model's ability to understand and convert symbolic formula strings and function images into computable operation trees, but effectiveness depends critically on teacher-model architecture, pretraining corpora, and data-generation scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2294.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2294.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-guided Knowledge Distillation (Similarity KD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Similarity-based knowledge distillation from frozen large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distillation technique that aligns frozen LLM hidden states (teacher) with student encoder representations (Funcimg and OTS encoders) using similarity (InfoNCE-like) losses and contrastive negative queues to transfer semantic/formula understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Symbolic regression / multimodal scientific computing (transfer of symbolic-formula semantics into multimodal encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Transfer the LLM's deep semantic understanding of symbolic formula strings into smaller multimodal encoders so they can better incorporate symbolic information for OTS generation and multimodal mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses the same paired Funcimg-OTS-Formula dataset generated by the authors (551,400 pairs) — labeled, synthetic, and sufficient for pretraining; paper notes scientific computing corpora are generally limited compared to text-image domains.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional LLM hidden states (text embeddings), image encoder token features, and OTS encoder token features; contrastive negative queues store historical feature vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-to-high: mapping between different representational geometries (LLM hidden-space vs student encoder feature spaces) requires dimensionality reduction/adapters and stability in contrastive training; heterogeneity in teacher architecture (Transformer vs non-Transformer) affects transfer success.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Technique draws from established KD literature and recent LLM distillation surveys; applying similarity-based KD to multimodal scientific computing is relatively novel.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium-high — the distillation is intended to preserve interpretable symbolic structure implicitly via feature alignment, supporting mechanistic interpretability in downstream symbolic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Similarity-based knowledge distillation with contrastive losses (InfoNCE-like L_KD)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Teacher: frozen pre-trained LLM encodes formula strings to hidden states hs which are projected by a small trainable MLP embedder to the student feature dimension. Student models (Funcimg encoder and OTS encoder) are trained so their features h_i and h_o align with LLM features via a contrastive-style similarity loss L_KD that uses queues of negative hs examples and temperature parameters; this is integrated into total pretraining loss with tunable λ weight.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Transfer learning / knowledge distillation (similarity-based, contrastive KD)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate under data-limited conditions where training large LLMs end-to-end is infeasible; requires that teacher LLM representations are compatible (architecture/corpus) with student encoders and that paired data exists to compute cross-modal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Pretraining curves show L_KD declining fastest and most stably for ChatGLM-2 and Qwen-1.5 teachers; quantitative pretraining L_KD at 1e6 steps reported per model in paper (ChatGLM-2 shows lowest/most stable L_KD among tested LLMs). Exact numeric L_KD values per LLM are in Table 2 of the paper; downstream fine-tuning losses (Tables 3 and 4) reflect improved performance when KD succeeded (e.g., Botfip-LLM w/ ChatGLM-2 improved mean fine-tuning error vs baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Worked well when teacher LLMs were transformer-based and pretraining corpora matched domain needs (ChatGLM-2, Qwen-1.5); failed or was ineffective for some non-Transformer teachers (RWKV-6, Mamba) where L_KD did not decrease and students did not align with teacher features.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables leveraging large-pretrained LLM semantic knowledge in resource-constrained multimodal scientific models, reducing the need to retrain or fine-tune full LLMs and expanding task capability (e.g., formula→OTS conversion). Success enables faster development of scientifically-interpretable multimodal systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared across multiple LLM teachers in experiments; transformer-based LLMs (ChatGLM-2, Qwen-1.5) outperformed non-transformer ones in effective KD. Alternative adaptation techniques (e.g., LoRA, end-to-end fine-tuning) are discussed in related work but were not the main approach here; the frozen-teacher + small embedder strategy was chosen to control compute.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Teacher LLM architecture and pretraining corpus alignment with domain (mathematical/formula content), hidden-size compatibility (smaller discrepancy to student feature dims eases mapping), use of negative-sample queues for robust contrastive training, and a small trainable embedder to project teacher features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Similarity-based KD from a well-matched frozen LLM can transfer useful symbolic/formula semantics into multimodal encoders and materially improve downstream symbolic-structure tasks, but success depends critically on the teacher LLM's architecture and pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2294.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2294.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal Contrastive Learning (InfoNCE + MoCo queues)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Function Image - OTS contrastive learning with InfoNCE and MoCo-style negative queues</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrastive framework aligning function-image encoder outputs with OTS encoder features via InfoNCE losses, using queues (à la MoCo) to supply diverse negative samples across training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Multimodal representation learning for scientific computing / symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn a joint semantic space where function images and OTS sequences that correspond to the same underlying computable function are nearby while mismatched pairs are pushed apart, facilitating downstream generation and matching tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses the authors' synthetic paired Funcimg-OTS dataset (551,400 pairs); labeled and structured for contrastive pairing.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Image features (token sequences from ViT) and sequence features (tokens from OTS encoder); high-dimensional continuous vectors used in contrastive comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: requires many negative samples for stable contrastive training to avoid collapse; negative queue addresses this, but complexity grows with dataset diversity and modality heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Technique builds on established contrastive learning practice (InfoNCE, MoCo) but applied specifically to scientific computing multimodal pairs is newer.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — alignment supports interpretable outputs (matching image to OTS) though contrastive learning itself is a representation learning tool rather than explicitly interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Contrastive multimodal learning (InfoNCE) with MoCo-style queues</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Compute image encoder features h_i and OTS encoder features h_o; apply InfoNCE loss L_FOC to align positive image-OTS pairs while drawing in- batch and queued negatives. Temperature parameter τ is trained; historical features stored in queues Q_i, Q_o to supply a large and diverse set of negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Self-supervised / supervised contrastive multimodal learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for multimodal alignment where paired examples are available; queues increase negative diversity making it feasible with limited batch sizes. Requires careful negative-sample management and stable training hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Pretraining loss L_FOC decreased during pretraining for all teacher-LMM conditions reported; integration with KD produced better downstream fine-tuning results (see Botfip-LLM quantitative improvements in Tables 3 and 4). Exact per-loss numeric trajectories are shown in Figure 4 and Table 2 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enabled tighter alignment between modalities and supported downstream tasks (OT S generation, matching). Works well when combined with KD; on its own (original Botfip baseline) it set the foundation but lacked the formula-string modality strength that LLM KD provides.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Improves multimodal feature transfer and generalization; scalable to larger datasets and can be combined with teacher guidance (KD) to leverage external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors compare to original Botfip (which also used contrastive components) and show Botfip-LLM (contrastive + KD) outperforms original Botfip on fine-tuning tasks; no direct comparison to other contrastive variants was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of queues to provide diverse negatives, careful temperature tuning, and joint optimization with matching and modeling losses (FOM, OM) alongside LLM-KD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2294.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2294.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM-2 (as Teacher LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM-2 large language model (used as frozen teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based conversational large language model used as a frozen teacher to encode formula strings; yielded the best knowledge-distillation performance and downstream gains among tested LLMs in the Botfip-LLM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Symbolic formula representation for multimodal scientific computing / symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide rich, high-dimensional semantic embeddings of symbolic formula strings to guide student multimodal encoders via similarity-based KD so the encoders better capture symbolic semantics for OTS generation and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Pretrained on large corpora (authors note ChatGLM-2 and Qwen-1.5 used large Chinese corpora and may include math-related corpus), but this external LLM data is not part of Botfip-LLM dataset; formula-string labeled data for distillation comes from authors' paired dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Textual formula strings tokenized by LLM tokenizer; outputs are hidden-state sequences hs that are projected into student feature space by an embedder.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-level semantic extraction from symbolic expressions; successful transfer requires teacher hidden-space geometry to be mappable to student encoder space — depends on hidden-size and representational compatibility.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>ChatGLM-2 is an established LLM; applying it as frozen teacher to symbolic-formula KD in scientific computing is novel in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — although teacher is a black box, its distilled feature vectors are used to produce interpretable symbolic outputs from student; interpretability relies on student outputs rather than teacher transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Transformer-based LLM used for frozen-teacher similarity KD</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>ChatGLM-2 encodes formula strings to hidden states; a trainable small MLP embedder reduces dimensionality to student feature dimension; similarity-based KD loss aligns student image/OTS features with these embedded LLM features; teacher parameters remain frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Foundation model used for transfer learning / knowledge distillation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable — experimental results show ChatGLM-2 enabled stable and fast reduction of LLM-KD loss and improved downstream fine-tuning performance (both Funcimg-OTS and Formula String-OTS tasks). Practical constraints: requires availability of ChatGLM-2 checkpoint and compute to host it (authors used distributed single-GPU hosting and quantization tricks).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>ChatGLM-2 produced the fastest and most stable decline in LLM-KD loss during pretraining; Botfip-LLM (ChatGLM-2) achieved mean fine-tuning error 0.621 ±0.006 (node=5) on Funcimg-OTS vs baseline 0.668 ±0.006, and Formula String→OTS errors of ~0.681 ±0.007 (node=5) (see Tables 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Most effective teacher among those tested; produced clear cross-modal feature alignment improvements visualized in cosine-similarity matrices. Reasons suggested: transformer architecture and pretraining corpora rich in mathematical content, and hidden-size compatibility with student models.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant positive impact when a well-matched LLM is available: faster pretraining convergence, improved generalization and added downstream functionality (formula→OTS).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against several other LLMs as teacher models (LLaMA-2, Gemma, Qwen-1.5, Mistral, Mamba, RWKV-6, Phi-2); ChatGLM-2 yielded the best KD dynamics and downstream fine-tuning outcomes; non-transformer LLMs (RWKV-6, Mamba) performed poorly as teachers under the same KD approach.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Transformer architecture alignment with student encoders, relevant pretraining corpus (mathematical content), appropriate hidden-size/feature-dimension mapping via embedder, stable KD hyperparameters and negative-queue sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>A well-matched transformer-based LLM teacher (ChatGLM-2) can reliably transfer symbolic-formula semantics into multimodal student encoders via frozen-teacher similarity distillation, producing measurable gains in symbolic-structure tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2294.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2294.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWKV-6 (as Teacher LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RWKV-6 large model (RNN-style / non-Transformer architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-Transformer LLM used as a frozen teacher for formula-string feature extraction; it performed poorly in similarity-based KD experiments and failed to transfer useful symbolic features to the Botfip-LLM student models under the standard KD setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Symbolic formula representation for multimodal scientific computing</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Test whether non-Transformer LLM architectures (RWKV-6) can serve as effective knowledge sources for distillation into multimodal encoders for symbolic-formula tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>RWKV-6 is pretrained on large corpora external to the authors' dataset; distillation data is the authors' paired dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Produces hidden-state sequences for formula strings; representational geometry differs from Transformer hidden-space.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Transfer difficulty arises from architectural mismatch between teacher and student and differing hidden-space geometry, making similarity-based KD ineffective here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>RWKV-style models are newer alternatives to Transformers and have different representation properties; applying them as KD teachers in multimodal scientific tasks is experimental.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — even though teacher is black-box, success depends on producing embeddings that align with students for interpretable student outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Non-Transformer (selective state-space / RNN-like) large model used as frozen teacher in KD experiments</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>RWKV-6 encodes formula strings; a small embedder projects its outputs to student feature dim. Under the same similarity KD loss used for transformer teachers, RWKV-6 showed rapid increase and little decrease in L_KD across 1e6 steps, indicating failed alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Foundation model used for transfer learning / knowledge distillation (negative result under this KD approach)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Limited under the tested similarity-based KD method: RWKV-6 failed to provide useful features for the Botfip-LLM student under identical KD settings, suggesting architecture-sensitive applicability or need for alternative KD strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Paper reports L_KD for RWKV-6 increased rapidly and barely decreased within 1×10^6 steps; downstream fine-tuning results for Botfip-LLM with RWKV-6 are similar to the original Botfip baseline (Table 3) and notably worse in Formula String→OTS task (Table 4: RWKV-6 mean errors ~0.929 ±0.012 at node=5 vs ChatGLM-2 ~0.681 ±0.007).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively ineffective for similarity-based KD in this setup; student encoders did not align with RWKV-6 produced features (cosine-similarity visualizations show poor cross-modal alignment). Possible reasons: non-Transformer architecture generates hidden-space geometry incompatible with similarity KD or embedder mapping used.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Low under current KD approach; suggests that non-Transformer LLMs may require alternate distillation objectives or adapter designs to be effective teachers for multimodal scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Direct comparison in paper shows RWKV-6 underperforms transformer teachers (ChatGLM-2, Qwen-1.5) in KD and downstream tasks; indicates architecture match matters.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Failure attributed to architectural mismatch and possibly pretraining corpus differences; success would likely require different KD objective (e.g., task-specific supervision), more flexible embedder layers, or alternative alignment strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2294.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2294.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Original Botfip</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Botfip multimodal scientific computing framework (original)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Precursor multimodal framework that aligns function images and OTS using a Multi-Encoder-Decoder (MED) architecture (ViT + BERT style encoders) but lacked comprehensive use of symbolic formula strings and large-LM guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bootstrapping otsfuncimg pre-training model (botfip)-a comprehensive symbolic regression framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Symbolic regression and multimodal scientific computing (function image ↔ OTS)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Map multi-scale function images to operation-tree sequences (OTS) and reconstruct computable symbolic expressions from structured OTS and constant vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Authors previously proposed a random OTS generation method and Funcimg-OTS data format to generate large amounts of synthetic pretraining data; dataset availability is moderate and synthetic.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Multimodal: function images and OTS sequences; did not incorporate formula-string modality explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: mapping images to operation trees and reconstructing formulas requires handling combinatorial structure, but original Botfip did not incorporate formula strings, limiting symbolic understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging; Botfip was an earlier attempt within the same research line and served as baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — output symbolic expressions and operation trees intended to be interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Multimodal encoder-decoder (ViT for images; BERT-like for OTS) with contrastive and modeling objectives (original MED architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Uses multi-scale image encoder (ViT) to extract features and a sequence encoder-decoder (BERT-like) to model OTS; generates OTS from image features and reconstructs formulas by converting OTS and constants, constants optimized via L-BFGS in inference. Lacked direct formula-string encoding or LLM guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised multimodal learning + representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to image→OTS tasks but limited in tasks involving formula strings (could not transform formula strings into OTS), motivating Botfip-LLM extension.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Baseline mean fine-tuning error (Table 3) for Funcimg-OTS (node=5): 0.668 ±0.006; Botfip-LLM with ChatGLM-2 reduced this to 0.621 ±0.006, showing measurable gains from LLM-guided distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective for core image→OTS tasks but lacked the ability to fully exploit symbolic formula strings; generalization/extrapolation limited by smaller-parameter encoders (BERT) compared to large LLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Provided a foundation and dataset format (Funcimg-OTS) enabling later work to incorporate LLM distillation and expand downstream tasks; remains useful baseline for multimodal symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to Botfip-LLM variants and conventional symbolic regression frameworks (PYSR, DSR); Botfip-LLM surpassed original Botfip modestly, while iterative symbolic search methods still excelled at extrapolation/error-correction for large trees.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Design of the Funcimg-OTS data generation and MED architecture enabled initial multimodal mapping; limitations due to absence of formula-string modality and smaller-parameter sequence encoders motivated incorporation of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 1)</em></li>
                <li>Bootstrapping otsfuncimg pre-training model (botfip)-a comprehensive symbolic regression framework <em>(Rating: 2)</em></li>
                <li>Symformer: End-to-end symbolic regression using transformer-based architecture <em>(Rating: 2)</em></li>
                <li>End-to-end symbolic regression with transformers <em>(Rating: 2)</em></li>
                <li>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients <em>(Rating: 2)</em></li>
                <li>A unified framework for deep symbolic regression <em>(Rating: 1)</em></li>
                <li>PySR: interpretable machine learning for science with pysr and symbolicregression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2294",
    "paper_id": "paper-274233896",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Botfip-LLM",
            "name_full": "Botfip-LLM multimodal scientific computing framework",
            "brief_description": "A multimodal scientific computing model that aligns function images, symbolic operation tree sequences (OTS), and formula strings by distilling knowledge from frozen large language models (LLMs) into image and sequence encoders via contrastive and matching objectives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Symbolic regression / Scientific computing (multimodal mapping between function images, symbolic operation trees, and formula strings)",
            "problem_description": "Learn joint representations and mappings among function images, symbolic operation tree sequences (OTS) and symbolic formula strings so the model can (a) generate OTS from function images, (b) generate symbolic formulas from OTS, and (c) convert formula strings into OTS — enabling multimodal symbolic regression and formula/structure recovery.",
            "data_availability": "Moderate and synthetic: authors built a pretraining dataset of 11,028 skeletons and 551,400 Funcimg-OTS pairs (function images paired with OTS and formula strings). Data are paired and labeled (OTS/formula for each function image) and reported as open-sourced in the Botfip line of work; authors note that general scientific-computing corpora are far smaller than typical NLP/vision corpora.",
            "data_structure": "Multimodal: multi-scale numerical function images (gridded arrays), discrete OTS sequences (token sequences representing operation-tree skeletons and constant vectors), and unstructured/semi-structured formula strings (text sequences of mathematical expressions).",
            "problem_complexity": "High: multimodal alignment across heterogeneous representations; symbolic formulas can be deeply nested and ambiguous (many OTS/constant-vector pairs can map to the same formula); search/sequence length grows with tree node count; constant optimization handled separately via L-BFGS; mapping is not one-to-one, increasing combinatorial complexity.",
            "domain_maturity": "Emerging: AI-for-Science and symbolic regression have active prior art (e.g., DSR, PYSR, SymFormer) but the multimodal approach (image + symbolic tree + formula string alignment with LLM distillation) is novel and builds on recent transformer/vision-language advances.",
            "mechanistic_understanding_requirements": "High - interpretability and symbolic output are core goals: outputs are human-interpretable symbolic expressions and operation trees, so mechanistic/causal interpretability is required rather than purely black-box prediction.",
            "ai_methodology_name": "Multimodal deep learning with contrastive learning and LLM-guided knowledge distillation (ViT + BERT + frozen LLM + MLP embedder; encoder-decoder OTS generation)",
            "ai_methodology_description": "Architecture: Funcimg encoder (Vision Transformer or variant) extracts multi-scale image features; OTS encoder/decoder (BERT backbone) encodes/generates operation-tree sequences; a pre-trained frozen LLM (various off-the-shelf LLM checkpoints) encodes formula strings into high-dimensional hidden states which are projected via a small trainable MLP embedder into the shared feature dimension. Training losses: Function Image-OTS Contrastive Loss (InfoNCE), Function Image-OTS Matching Loss (binary classification), OTS Modeling Loss (sequence prediction/cross-entropy), and LLM Knowledge Distillation Loss (similarity-based alignment between LLM features and student encoders). Queues (MoCo-style) are used to provide negative samples for contrastive losses. During fine-tuning, causal-attention is used to generate OTS conditioned on either image features, OTS context, or embedded formula-string features. Constants are optimized with L-BFGS during inference after skeleton generation. The LLM parameters are frozen; only a small embedder is trained. The implementation supports distributed training by hosting the frozen LLM on a single GPU and aggregating features to it, optionally using quantization (half-precision, 8-bit/4-bit).",
            "ai_methodology_category": "Hybrid: supervised multimodal representation learning + contrastive learning + knowledge distillation from foundation LLMs; sequence generation (supervised) for symbolic outputs",
            "applicability": "Applicable and appropriate: method directly targets multimodal symbolic regression and formula↔structure conversion problems; modifications (frozen LLM + embedder, MoCo queues, separate constant optimization) adapt standard multimodal techniques to resource- and data-limited scientific computing contexts. Limitations: requires paired Funcimg-OTS-Formula data (authors generated synthetic pairs) and success depends strongly on choice of teacher LLM and pretraining corpus compatibility.",
            "effectiveness_quantitative": "Funcimg-OTS fine-tuning mean loss (at step 25k) improved vs baseline: Botfip (baseline) mean error ≈ 0.668 ± 0.006 (node=5) while Botfip-LLM (ChatGLM-2) ≈ 0.621 ± 0.006 (node=5) — improvements seen across node counts (Table 3). Formula String→OTS fine-tuning error (Table 4): Botfip-LLM with ChatGLM-2: 0.681 ± 0.007 (node=5) up to 0.796 ± 0.013 (node=9); with RWKV-6 errors substantially higher (0.929 ±0.012 to 1.070 ±0.021), showing quantitative benefit of LLM choice. Validation metrics (Acc_r, SRL, SRL) showed Botfip-LLM (ChatGLM-2) slightly surpassing original Botfip across varying tree node counts (exact per-node values are reported in paper figures/tables).",
            "effectiveness_qualitative": "Qualitatively effective: Botfip-LLM improves representational alignment across modalities and enriches downstream task capability (enables formula-string→OTS conversion that original Botfip could not). The framework achieves faster convergence and lower steady-state loss when pretrained with well-matched LLMs (notably ChatGLM-2). Limitations observed: improvements are modest for very large/complex OTS (extrapolation still lags behind iterative symbolic search methods like PYSR and DSR), and non-Transformer LLM teachers (RWKV, Mamba) were less effective or failed to transfer useful features with the chosen similarity-distillation approach.",
            "impact_potential": "High within symbolic scientific computing: enables multimodal integration (images, symbolic trees, formula strings), expands downstream tasks (bi-directional formula↔OTS conversion), and reduces need to train huge LLMs end-to-end by distillation. Practical benefits include broader applicability to tasks that require symbolic interpretability and potential time savings in model development (frozen LLMs + small embedders). Scalability depends on teacher LLM availability and dataset generation for pretraining.",
            "comparison_to_alternatives": "Compared with original Botfip (no LLM distillation), Botfip-LLM (with a well-chosen LLM such as ChatGLM-2) produced lower fine-tuning losses and slightly better Acc_r/SRL/SRL. Compared to iterative symbolic regression methods (PYSR, DSR), Botfip-LLM improved extrapolation relative to original Botfip but remains inferior in error-correction/extrapolation for very complex trees; iterative SR methods still outperform for large-node counts.",
            "success_factors": "Key contributors: (1) choice of teacher LLM — transformer-based bilingual/Chinese-heavy LLM ChatGLM-2 matched domain corpus and hidden-size compatibility yielded best transfer; (2) similarity-based distillation aligning LLM hidden states with student encoders; (3) multimodal contrastive objectives (InfoNCE) and matching losses for tight alignment; (4) engineering techniques: frozen LLM + small embedder, distributed aggregation, and quantization to enable training under limited GPU memory; (5) use of a synthetic but large paired Funcimg-OTS-Formula dataset to bootstrap multimodal alignment.",
            "key_insight": "Distilling semantic features from an appropriately chosen frozen LLM into multimodal encoders via similarity-based contrastive alignment substantially improves the model's ability to understand and convert symbolic formula strings and function images into computable operation trees, but effectiveness depends critically on teacher-model architecture, pretraining corpora, and data-generation scale.",
            "uuid": "e2294.0",
            "source_info": {
                "paper_title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLM-guided Knowledge Distillation (Similarity KD)",
            "name_full": "Similarity-based knowledge distillation from frozen large language models",
            "brief_description": "A distillation technique that aligns frozen LLM hidden states (teacher) with student encoder representations (Funcimg and OTS encoders) using similarity (InfoNCE-like) losses and contrastive negative queues to transfer semantic/formula understanding.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Symbolic regression / multimodal scientific computing (transfer of symbolic-formula semantics into multimodal encoders)",
            "problem_description": "Transfer the LLM's deep semantic understanding of symbolic formula strings into smaller multimodal encoders so they can better incorporate symbolic information for OTS generation and multimodal mapping.",
            "data_availability": "Uses the same paired Funcimg-OTS-Formula dataset generated by the authors (551,400 pairs) — labeled, synthetic, and sufficient for pretraining; paper notes scientific computing corpora are generally limited compared to text-image domains.",
            "data_structure": "High-dimensional LLM hidden states (text embeddings), image encoder token features, and OTS encoder token features; contrastive negative queues store historical feature vectors.",
            "problem_complexity": "Moderate-to-high: mapping between different representational geometries (LLM hidden-space vs student encoder feature spaces) requires dimensionality reduction/adapters and stability in contrastive training; heterogeneity in teacher architecture (Transformer vs non-Transformer) affects transfer success.",
            "domain_maturity": "Technique draws from established KD literature and recent LLM distillation surveys; applying similarity-based KD to multimodal scientific computing is relatively novel.",
            "mechanistic_understanding_requirements": "Medium-high — the distillation is intended to preserve interpretable symbolic structure implicitly via feature alignment, supporting mechanistic interpretability in downstream symbolic outputs.",
            "ai_methodology_name": "Similarity-based knowledge distillation with contrastive losses (InfoNCE-like L_KD)",
            "ai_methodology_description": "Teacher: frozen pre-trained LLM encodes formula strings to hidden states hs which are projected by a small trainable MLP embedder to the student feature dimension. Student models (Funcimg encoder and OTS encoder) are trained so their features h_i and h_o align with LLM features via a contrastive-style similarity loss L_KD that uses queues of negative hs examples and temperature parameters; this is integrated into total pretraining loss with tunable λ weight.",
            "ai_methodology_category": "Transfer learning / knowledge distillation (similarity-based, contrastive KD)",
            "applicability": "Appropriate under data-limited conditions where training large LLMs end-to-end is infeasible; requires that teacher LLM representations are compatible (architecture/corpus) with student encoders and that paired data exists to compute cross-modal alignment.",
            "effectiveness_quantitative": "Pretraining curves show L_KD declining fastest and most stably for ChatGLM-2 and Qwen-1.5 teachers; quantitative pretraining L_KD at 1e6 steps reported per model in paper (ChatGLM-2 shows lowest/most stable L_KD among tested LLMs). Exact numeric L_KD values per LLM are in Table 2 of the paper; downstream fine-tuning losses (Tables 3 and 4) reflect improved performance when KD succeeded (e.g., Botfip-LLM w/ ChatGLM-2 improved mean fine-tuning error vs baseline).",
            "effectiveness_qualitative": "Worked well when teacher LLMs were transformer-based and pretraining corpora matched domain needs (ChatGLM-2, Qwen-1.5); failed or was ineffective for some non-Transformer teachers (RWKV-6, Mamba) where L_KD did not decrease and students did not align with teacher features.",
            "impact_potential": "Enables leveraging large-pretrained LLM semantic knowledge in resource-constrained multimodal scientific models, reducing the need to retrain or fine-tune full LLMs and expanding task capability (e.g., formula→OTS conversion). Success enables faster development of scientifically-interpretable multimodal systems.",
            "comparison_to_alternatives": "Compared across multiple LLM teachers in experiments; transformer-based LLMs (ChatGLM-2, Qwen-1.5) outperformed non-transformer ones in effective KD. Alternative adaptation techniques (e.g., LoRA, end-to-end fine-tuning) are discussed in related work but were not the main approach here; the frozen-teacher + small embedder strategy was chosen to control compute.",
            "success_factors": "Teacher LLM architecture and pretraining corpus alignment with domain (mathematical/formula content), hidden-size compatibility (smaller discrepancy to student feature dims eases mapping), use of negative-sample queues for robust contrastive training, and a small trainable embedder to project teacher features.",
            "key_insight": "Similarity-based KD from a well-matched frozen LLM can transfer useful symbolic/formula semantics into multimodal encoders and materially improve downstream symbolic-structure tasks, but success depends critically on the teacher LLM's architecture and pretraining data.",
            "uuid": "e2294.1",
            "source_info": {
                "paper_title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Multimodal Contrastive Learning (InfoNCE + MoCo queues)",
            "name_full": "Function Image - OTS contrastive learning with InfoNCE and MoCo-style negative queues",
            "brief_description": "Contrastive framework aligning function-image encoder outputs with OTS encoder features via InfoNCE losses, using queues (à la MoCo) to supply diverse negative samples across training steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Multimodal representation learning for scientific computing / symbolic regression",
            "problem_description": "Learn a joint semantic space where function images and OTS sequences that correspond to the same underlying computable function are nearby while mismatched pairs are pushed apart, facilitating downstream generation and matching tasks.",
            "data_availability": "Uses the authors' synthetic paired Funcimg-OTS dataset (551,400 pairs); labeled and structured for contrastive pairing.",
            "data_structure": "Image features (token sequences from ViT) and sequence features (tokens from OTS encoder); high-dimensional continuous vectors used in contrastive comparisons.",
            "problem_complexity": "Moderate: requires many negative samples for stable contrastive training to avoid collapse; negative queue addresses this, but complexity grows with dataset diversity and modality heterogeneity.",
            "domain_maturity": "Technique builds on established contrastive learning practice (InfoNCE, MoCo) but applied specifically to scientific computing multimodal pairs is newer.",
            "mechanistic_understanding_requirements": "Medium — alignment supports interpretable outputs (matching image to OTS) though contrastive learning itself is a representation learning tool rather than explicitly interpretable.",
            "ai_methodology_name": "Contrastive multimodal learning (InfoNCE) with MoCo-style queues",
            "ai_methodology_description": "Compute image encoder features h_i and OTS encoder features h_o; apply InfoNCE loss L_FOC to align positive image-OTS pairs while drawing in- batch and queued negatives. Temperature parameter τ is trained; historical features stored in queues Q_i, Q_o to supply a large and diverse set of negatives.",
            "ai_methodology_category": "Self-supervised / supervised contrastive multimodal learning",
            "applicability": "Appropriate for multimodal alignment where paired examples are available; queues increase negative diversity making it feasible with limited batch sizes. Requires careful negative-sample management and stable training hyperparameters.",
            "effectiveness_quantitative": "Pretraining loss L_FOC decreased during pretraining for all teacher-LMM conditions reported; integration with KD produced better downstream fine-tuning results (see Botfip-LLM quantitative improvements in Tables 3 and 4). Exact per-loss numeric trajectories are shown in Figure 4 and Table 2 of the paper.",
            "effectiveness_qualitative": "Enabled tighter alignment between modalities and supported downstream tasks (OT S generation, matching). Works well when combined with KD; on its own (original Botfip baseline) it set the foundation but lacked the formula-string modality strength that LLM KD provides.",
            "impact_potential": "Improves multimodal feature transfer and generalization; scalable to larger datasets and can be combined with teacher guidance (KD) to leverage external knowledge.",
            "comparison_to_alternatives": "Authors compare to original Botfip (which also used contrastive components) and show Botfip-LLM (contrastive + KD) outperforms original Botfip on fine-tuning tasks; no direct comparison to other contrastive variants was provided.",
            "success_factors": "Use of queues to provide diverse negatives, careful temperature tuning, and joint optimization with matching and modeling losses (FOM, OM) alongside LLM-KD.",
            "uuid": "e2294.2",
            "source_info": {
                "paper_title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ChatGLM-2 (as Teacher LLM)",
            "name_full": "ChatGLM-2 large language model (used as frozen teacher)",
            "brief_description": "A transformer-based conversational large language model used as a frozen teacher to encode formula strings; yielded the best knowledge-distillation performance and downstream gains among tested LLMs in the Botfip-LLM experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Symbolic formula representation for multimodal scientific computing / symbolic regression",
            "problem_description": "Provide rich, high-dimensional semantic embeddings of symbolic formula strings to guide student multimodal encoders via similarity-based KD so the encoders better capture symbolic semantics for OTS generation and alignment.",
            "data_availability": "Pretrained on large corpora (authors note ChatGLM-2 and Qwen-1.5 used large Chinese corpora and may include math-related corpus), but this external LLM data is not part of Botfip-LLM dataset; formula-string labeled data for distillation comes from authors' paired dataset.",
            "data_structure": "Textual formula strings tokenized by LLM tokenizer; outputs are hidden-state sequences hs that are projected into student feature space by an embedder.",
            "problem_complexity": "High-level semantic extraction from symbolic expressions; successful transfer requires teacher hidden-space geometry to be mappable to student encoder space — depends on hidden-size and representational compatibility.",
            "domain_maturity": "ChatGLM-2 is an established LLM; applying it as frozen teacher to symbolic-formula KD in scientific computing is novel in this work.",
            "mechanistic_understanding_requirements": "Medium — although teacher is a black box, its distilled feature vectors are used to produce interpretable symbolic outputs from student; interpretability relies on student outputs rather than teacher transparency.",
            "ai_methodology_name": "Transformer-based LLM used for frozen-teacher similarity KD",
            "ai_methodology_description": "ChatGLM-2 encodes formula strings to hidden states; a trainable small MLP embedder reduces dimensionality to student feature dimension; similarity-based KD loss aligns student image/OTS features with these embedded LLM features; teacher parameters remain frozen.",
            "ai_methodology_category": "Foundation model used for transfer learning / knowledge distillation",
            "applicability": "Highly applicable — experimental results show ChatGLM-2 enabled stable and fast reduction of LLM-KD loss and improved downstream fine-tuning performance (both Funcimg-OTS and Formula String-OTS tasks). Practical constraints: requires availability of ChatGLM-2 checkpoint and compute to host it (authors used distributed single-GPU hosting and quantization tricks).",
            "effectiveness_quantitative": "ChatGLM-2 produced the fastest and most stable decline in LLM-KD loss during pretraining; Botfip-LLM (ChatGLM-2) achieved mean fine-tuning error 0.621 ±0.006 (node=5) on Funcimg-OTS vs baseline 0.668 ±0.006, and Formula String→OTS errors of ~0.681 ±0.007 (node=5) (see Tables 3 and 4).",
            "effectiveness_qualitative": "Most effective teacher among those tested; produced clear cross-modal feature alignment improvements visualized in cosine-similarity matrices. Reasons suggested: transformer architecture and pretraining corpora rich in mathematical content, and hidden-size compatibility with student models.",
            "impact_potential": "Significant positive impact when a well-matched LLM is available: faster pretraining convergence, improved generalization and added downstream functionality (formula→OTS).",
            "comparison_to_alternatives": "Compared against several other LLMs as teacher models (LLaMA-2, Gemma, Qwen-1.5, Mistral, Mamba, RWKV-6, Phi-2); ChatGLM-2 yielded the best KD dynamics and downstream fine-tuning outcomes; non-transformer LLMs (RWKV-6, Mamba) performed poorly as teachers under the same KD approach.",
            "success_factors": "Transformer architecture alignment with student encoders, relevant pretraining corpus (mathematical content), appropriate hidden-size/feature-dimension mapping via embedder, stable KD hyperparameters and negative-queue sampling.",
            "key_insight": "A well-matched transformer-based LLM teacher (ChatGLM-2) can reliably transfer symbolic-formula semantics into multimodal student encoders via frozen-teacher similarity distillation, producing measurable gains in symbolic-structure tasks.",
            "uuid": "e2294.3",
            "source_info": {
                "paper_title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "RWKV-6 (as Teacher LLM)",
            "name_full": "RWKV-6 large model (RNN-style / non-Transformer architecture)",
            "brief_description": "A non-Transformer LLM used as a frozen teacher for formula-string feature extraction; it performed poorly in similarity-based KD experiments and failed to transfer useful symbolic features to the Botfip-LLM student models under the standard KD setup.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Symbolic formula representation for multimodal scientific computing",
            "problem_description": "Test whether non-Transformer LLM architectures (RWKV-6) can serve as effective knowledge sources for distillation into multimodal encoders for symbolic-formula tasks.",
            "data_availability": "RWKV-6 is pretrained on large corpora external to the authors' dataset; distillation data is the authors' paired dataset.",
            "data_structure": "Produces hidden-state sequences for formula strings; representational geometry differs from Transformer hidden-space.",
            "problem_complexity": "Transfer difficulty arises from architectural mismatch between teacher and student and differing hidden-space geometry, making similarity-based KD ineffective here.",
            "domain_maturity": "RWKV-style models are newer alternatives to Transformers and have different representation properties; applying them as KD teachers in multimodal scientific tasks is experimental.",
            "mechanistic_understanding_requirements": "Medium — even though teacher is black-box, success depends on producing embeddings that align with students for interpretable student outputs.",
            "ai_methodology_name": "Non-Transformer (selective state-space / RNN-like) large model used as frozen teacher in KD experiments",
            "ai_methodology_description": "RWKV-6 encodes formula strings; a small embedder projects its outputs to student feature dim. Under the same similarity KD loss used for transformer teachers, RWKV-6 showed rapid increase and little decrease in L_KD across 1e6 steps, indicating failed alignment.",
            "ai_methodology_category": "Foundation model used for transfer learning / knowledge distillation (negative result under this KD approach)",
            "applicability": "Limited under the tested similarity-based KD method: RWKV-6 failed to provide useful features for the Botfip-LLM student under identical KD settings, suggesting architecture-sensitive applicability or need for alternative KD strategies.",
            "effectiveness_quantitative": "Paper reports L_KD for RWKV-6 increased rapidly and barely decreased within 1×10^6 steps; downstream fine-tuning results for Botfip-LLM with RWKV-6 are similar to the original Botfip baseline (Table 3) and notably worse in Formula String→OTS task (Table 4: RWKV-6 mean errors ~0.929 ±0.012 at node=5 vs ChatGLM-2 ~0.681 ±0.007).",
            "effectiveness_qualitative": "Qualitatively ineffective for similarity-based KD in this setup; student encoders did not align with RWKV-6 produced features (cosine-similarity visualizations show poor cross-modal alignment). Possible reasons: non-Transformer architecture generates hidden-space geometry incompatible with similarity KD or embedder mapping used.",
            "impact_potential": "Low under current KD approach; suggests that non-Transformer LLMs may require alternate distillation objectives or adapter designs to be effective teachers for multimodal scientific tasks.",
            "comparison_to_alternatives": "Direct comparison in paper shows RWKV-6 underperforms transformer teachers (ChatGLM-2, Qwen-1.5) in KD and downstream tasks; indicates architecture match matters.",
            "success_factors": "Failure attributed to architectural mismatch and possibly pretraining corpus differences; success would likely require different KD objective (e.g., task-specific supervision), more flexible embedder layers, or alternative alignment strategies.",
            "uuid": "e2294.4",
            "source_info": {
                "paper_title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Original Botfip",
            "name_full": "Botfip multimodal scientific computing framework (original)",
            "brief_description": "Precursor multimodal framework that aligns function images and OTS using a Multi-Encoder-Decoder (MED) architecture (ViT + BERT style encoders) but lacked comprehensive use of symbolic formula strings and large-LM guidance.",
            "citation_title": "Bootstrapping otsfuncimg pre-training model (botfip)-a comprehensive symbolic regression framework",
            "mention_or_use": "use",
            "scientific_problem_domain": "Symbolic regression and multimodal scientific computing (function image ↔ OTS)",
            "problem_description": "Map multi-scale function images to operation-tree sequences (OTS) and reconstruct computable symbolic expressions from structured OTS and constant vectors.",
            "data_availability": "Authors previously proposed a random OTS generation method and Funcimg-OTS data format to generate large amounts of synthetic pretraining data; dataset availability is moderate and synthetic.",
            "data_structure": "Multimodal: function images and OTS sequences; did not incorporate formula-string modality explicitly.",
            "problem_complexity": "Moderate: mapping images to operation trees and reconstructing formulas requires handling combinatorial structure, but original Botfip did not incorporate formula strings, limiting symbolic understanding.",
            "domain_maturity": "Emerging; Botfip was an earlier attempt within the same research line and served as baseline in experiments.",
            "mechanistic_understanding_requirements": "High — output symbolic expressions and operation trees intended to be interpretable.",
            "ai_methodology_name": "Multimodal encoder-decoder (ViT for images; BERT-like for OTS) with contrastive and modeling objectives (original MED architecture)",
            "ai_methodology_description": "Uses multi-scale image encoder (ViT) to extract features and a sequence encoder-decoder (BERT-like) to model OTS; generates OTS from image features and reconstructs formulas by converting OTS and constants, constants optimized via L-BFGS in inference. Lacked direct formula-string encoding or LLM guidance.",
            "ai_methodology_category": "Supervised multimodal learning + representation learning",
            "applicability": "Applicable to image→OTS tasks but limited in tasks involving formula strings (could not transform formula strings into OTS), motivating Botfip-LLM extension.",
            "effectiveness_quantitative": "Baseline mean fine-tuning error (Table 3) for Funcimg-OTS (node=5): 0.668 ±0.006; Botfip-LLM with ChatGLM-2 reduced this to 0.621 ±0.006, showing measurable gains from LLM-guided distillation.",
            "effectiveness_qualitative": "Effective for core image→OTS tasks but lacked the ability to fully exploit symbolic formula strings; generalization/extrapolation limited by smaller-parameter encoders (BERT) compared to large LLM capabilities.",
            "impact_potential": "Provided a foundation and dataset format (Funcimg-OTS) enabling later work to incorporate LLM distillation and expand downstream tasks; remains useful baseline for multimodal symbolic regression.",
            "comparison_to_alternatives": "Compared experimentally to Botfip-LLM variants and conventional symbolic regression frameworks (PYSR, DSR); Botfip-LLM surpassed original Botfip modestly, while iterative symbolic search methods still excelled at extrapolation/error-correction for large trees.",
            "success_factors": "Design of the Funcimg-OTS data generation and MED architecture enabled initial multimodal mapping; limitations due to absence of formula-string modality and smaller-parameter sequence encoders motivated incorporation of LLMs.",
            "uuid": "e2294.5",
            "source_info": {
                "paper_title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "rating": 2,
            "sanitized_title": "blip2_bootstrapping_languageimage_pretraining_with_frozen_image_encoders_and_large_language_models"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 1,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Bootstrapping otsfuncimg pre-training model (botfip)-a comprehensive symbolic regression framework",
            "rating": 2,
            "sanitized_title": "bootstrapping_otsfuncimg_pretraining_model_botfipa_comprehensive_symbolic_regression_framework"
        },
        {
            "paper_title": "Symformer: End-to-end symbolic regression using transformer-based architecture",
            "rating": 2,
            "sanitized_title": "symformer_endtoend_symbolic_regression_using_transformerbased_architecture"
        },
        {
            "paper_title": "End-to-end symbolic regression with transformers",
            "rating": 2,
            "sanitized_title": "endtoend_symbolic_regression_with_transformers"
        },
        {
            "paper_title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients",
            "rating": 2,
            "sanitized_title": "deep_symbolic_regression_recovering_mathematical_expressions_from_data_via_riskseeking_policy_gradients"
        },
        {
            "paper_title": "A unified framework for deep symbolic regression",
            "rating": 1,
            "sanitized_title": "a_unified_framework_for_deep_symbolic_regression"
        },
        {
            "paper_title": "PySR: interpretable machine learning for science with pysr and symbolicregression",
            "rating": 1,
            "sanitized_title": "pysr_interpretable_machine_learning_for_science_with_pysr_and_symbolicregression"
        }
    ],
    "cost": 0.024175249999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models
November 26, 2024</p>
<p>Tianhao Chen 
School of Mathematical Sciences
Key Laboratory of MEA (Ministry of Education)</p>
<p>Shanghai Key Laboratory of PMMP
East China Normal University
200241, 200241Shanghai, ShanghaiP.R. China</p>
<p>Pengbo Xu 
School of Mathematical Sciences
Key Laboratory of MEA (Ministry of Education)</p>
<p>Shanghai Key Laboratory of PMMP
East China Normal University
200241, 200241Shanghai, ShanghaiP.R. China</p>
<p>Haibiao Zheng 
School of Mathematical Sciences
Key Laboratory of MEA (Ministry of Education)</p>
<p>Shanghai Key Laboratory of PMMP
East China Normal University
200241, 200241Shanghai, ShanghaiP.R. China</p>
<p>Shanghai Zhangjiang Institute of Mathematics
201203Shanghai, ShanghaiChina</p>
<p>Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models
November 26, 20249371955A78FE514EF6BA19B0D09ED46BarXiv:2411.15525v1[cs.SC]Preprint submitted to Knowledge-Based SystemsMultimodal LearningKnowledge DistillationLarge Language ModelsScientific Computing
In recent years, the introduction of AI technologies has brought transformative changes to scientific computing.However, AI models typically focus on single-task and single-modal data processing, limiting their application.To address this, multimodal scientific computing frameworks have become a trend.The Botfip framework aligns function images with symbolic operation trees through multimodal training, extracting deep scientific information.However, Botfip struggles with processing Formula Strings, leading to inadequate understanding in multimodal learning.To enhance Botfip's learning of Formula Strings and expand its applicability to related tasks, we propose the Botfip-LLM framework based on knowledge distillation, incorporating pre-trained large language models for aligning symbolic tree data.Experimental analysis shows that the choice of LLM is crucial, with ChatGLM-2 outperforming others in training and testing.Botfip-LLM not only improves performance, generalization, and extrapolation over the original Botfip model but also significantly enhances applicability to Formula String-related tasks, enabling more diverse task handling.</p>
<p>Highlights</p>
<p>Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models Tianhao Chen, Pengbo Xu, Haibiao Zheng</p>
<p>• An Innovative Multimodal Scientific Computing Framework:</p>
<p>Botfip-LLM is an innovative multimodal scientific computing framework that integrates numerical information (function images), sequence encoding information, and symbolic information (symbolic formulas) centered around symbolic operation trees.This integration not only captures multiple dimensions of scientific data but also enhances the model's capability to understand and handle complex scientific phenomena through the synergistic effect of multimodal information.</p>
<p>• Knowledge Distillation in Botfip-LLM: Botfip-LLM employs knowledge distillation to infuse the experience and knowledge of pre-trained large language models (LLMs) into the scientific computing multimodal framework.This process not only improves the model's understanding of multimodal information but also significantly enhances its performance in scientific computing tasks by efficiently transferring the deep features of the pre-trained models, thereby boosting the fusion of multimodal data.</p>
<p>• Efficient Training with Distributed Deployment and Aggregation: Botfip-LLM successfully achieves the invocation of LLMs and the training of the main model in low GPU memory environments through distributed deployment and aggregation.This approach addresses the challenge of deploying large models in resource-constrained settings while ensuring the efficiency and scalability of the training process, making the application of large-scale models more practical and feasible.</p>
<p>• Expansion of Downstream Tasks and Potential Integration with NLP: Building on the Botfip framework, Botfip-LLM can extend to more downstream tasks, such as generating and inferring symbolic sequence encodings from symbolic formulas.In the future, this framework can further integrate with various natural language processing (NLP) tasks, exploring more cross-domain application scenarios and innovative research directions, thus promoting the convergence of scientific computing and NLP.</p>
<p>Introduction</p>
<p>In recent years, the exponential growth in data complexity and volume has posed significant challenges to traditional scientific methods.As computational challenges become increasingly complex and data-intensive, traditional tools, though powerful, often struggle to keep pace with the scale and complexity of contemporary scientific datasets.This situation urgently calls for more robust, efficient, and scalable solutions, paving the way for the rise of artificial intelligence (AI) in scientific computing.Integrating AI technologies into scientific computing marks a transformative shift in research and development across multiple scientific fields, such as weather forecasting [1,2,3], numerical solutions for partial differential equations (PDEs) [4,5,6,7], genomics [8,9], drug analysis [10,11,12], and symbolic regression [13,14,15].The fusion of AI with scientific inquiry not only enhances existing computational methods but also opens new avenues for exploration and discovery, giving rise to the rapidly evolving field known as "AI for Science" [16], which is dedicated to leveraging AI technologies to tackle complex challenges in scientific research.</p>
<p>Despite the progress that AI has made in scientific research, its potential applications remain constrained by several pervasive issues.The primary challenge is that AI models typically focus on single-task and single-modal data processing, which limits their ability to fully grasp complex scientific phenomena, especially in studies involving multifactor interactions.These single-task-oriented models also struggle to generalize across different scientific problems, hindering the development of robust and versatile models.Furthermore, the prevalent use of single-modal data processing overlooks the richness and interconnectedness of scientific data.To address these issues, the development of multimodal scientific computing frameworks has become an inevitable trend.As an emerging field, multimodal learning aims to integrate various data forms, such as text, images, and sound, to achieve complementarity between data types and enhance the overall performance and applicability of AI models.This integrative approach aids in understanding complex phenomena and promotes the application of models in multitask scenarios.For instance, OpenAI's Contrastive Language-Image Pretraining Model (CLIP) [17] combines text and image understanding to provide linguistic support for visual tasks.Additionally, multimodal learning has achieved significant accomplishments in computer vision and text processing, and is now expanding into fields such as scientific computing.Other key devel-Figure 1: Multimodal data representation of Funcimg-OTS-Formula String in scientific computing.While Funcimg and Formula String cannot be directly converted to the corresponding OTS of the symbolic operation tree, the Botfip framework facilitates the conversion of Funcimg to OTS through multimodal training but cannot directly convert Formula String.</p>
<p>opments in this field include BoOTStrapping Language-Image Pre-training Model (BLIP) [18], ALIGN [19], and ALBEF [20], among others.However, despite the notable advances in multimodal learning within computer vision and text processing, its application in the AI for Science domain remains in its nascent stages.Nonetheless, there have been some representative works, such as in [21], where researchers proposed the Botfip multimodal scientific computing framework based on the Blip model.Botfip integrates multi-scale function images and Operation Tree Skeleton Sequence (OTS) features to explore the deep connections between function images and their corresponding symbolic expression sequences, applying these insights to downstream tasks like symbolic regression.This innovative approach not only marks a new application of multimodal learning in scientific computing but also demonstrates its potential transformative nature in handling complex scientific data.Despite Botfip's use of a Multi-Encoder-Decoder (MED) architecture, which significantly enhances its generalization, versatility, and applicability compared to single-modal, single-task scientific computing models, its reliance on sequence encoding networks such as the BERT model [22] and image encoding networks like Vision Transformer (ViT) [23] limits its parameter scale and thus its extrapolation capability.Currently, large language models (LLMs) such as LLaMA [24] and Gemma [25], due to their vast parameter scales and deep network structures, are more effective at handling and generating complex data patterns.These large models have demonstrated substantial advantages over smaller models like BERT in fields such as natural language processing (NLPs), becoming the mainstream focus of research.Additionally, multimodal text-image models have started leveraging large pre-trained models to enhance the joint representation of images and texts, improving the accuracy of text-image matching and generation techniques.For example, the BLIP-2 [26] model combines well-trained vision models and LLMs.With the powerful generalization ability, knowledge base, and comprehension and generation capabilities of LLMs, BLIP-2 achieves leading results through a lightweight query transformer, showcasing the feasibility and potential of integrating LLMs into multimodal learning.</p>
<p>However, LLMs are currently not suitable for direct replacement of foundational architectures like BERT in multimodal scientific computing frameworks such as Botfip.First, LLM training requires massive data support.Unlike fields like NLPs, scientific computing has far fewer general datasets available.Although [21] proposed a method for random symbolic operation tree generation and a general scientific computing data format (function image-OTS pairs) to generate large amounts of data for pre-training, the limited audience and data sources in scientific computing cannot match the scale of the text-image domain.Additionally, the original Botfip framework primarily involves multi-scale function images and OTS, without addressing the symbolic expressions corresponding to the function-related symbolic operation trees for information processing and feature extraction.This is because OTS and constant function vectors can directly convert into computable symbolic operation trees, reconstructing the corresponding symbolic formulas.However, this approach limits the thorough exploration of symbolic formula information.Using only OTS to represent symbolic information is flawed because different OTS and constant vector pairs can correspond to the same symbolic formula (e.g., swapping two commutative operations with parameters in the symbolic operation tree), and solely relying on OTS as the symbolic information source also hinders the introduction of LLMs.Thus, the Botfip framework cannot, like multimodal models such as BLIP-2, directly utilize pre-trained LLMs and existing extensive pre-trained datasets to reduce training costs and quickly adapt and optimize for various tasks, thereby eliminating the need to train models from scratch.This also means  that forcibly replacing small parameter models like BERT with large models would significantly increase Botfip's training costs, requiring substantial time and computational resources for some tasks, presenting a huge challenge.How to effectively apply large model methods to Botfip and similar multimodal scientific computing frameworks, and the broader AI for Science domain, is a pressing issue that needs to be addressed.</p>
<p>Contribution: In this paper, to overcome the limitations imposed by the complexity and computational cost of directly applying LLMs in the Botfip framework, address the deficiency of Botfip in fully utilizing symbolic formula information, and further extend the functionalities of the Botfip model, we propose the Botfip-LLM enhanced extension framework based on knowledge distillation technology.The Botfip-LLM enhanced extension framework builds on Botfip by additionally employing pre-trained LLMs to handle the symbolic expressions generated from the OTS corresponding to the symbolic operation tree.These expressions are transformed into highdimensional hidden states (or logits) by the LLMs.After freezing the parameters of the LLMs, they do not participate in the training of the Botfip model.The resulting encoded vectors are dimensionally reduced through the LLMs' corresponding embedder and then trained via contrastive learning with the encoded vectors obtained from the function image encoder and OTS encoder.This achieves effective alignment of the high-dimensional semantic representations generated by the LLMs with the multimodal input data in the Botfip model.This process can be referred to as knowledge distillation guided by LLMs.</p>
<p>This alignment process involves not only the standardized processing of text and image data but also the feature vectorization of symbolic formulas, ensuring that data from different modalities can interact and learn within the same semantic space.Pre-trained LLMs are typically trained on extensive text and data sets, enabling them to capture rich semantic features and complex data patterns.By transmitting these advanced feature extraction capabilities to the Botfip encoders through the alignment process, the Botfip model's feature extraction and understanding capabilities when handling specific scientific data can be significantly improved.Moreover, the pre-training process of LLMs includes handling and parsing vast amounts of data from diverse sources, endowing them with excellent generalization capabilities.Transferring this capability to the Botfip encoders through knowledge distillation can enhance Botfip's adaptability and prediction accuracy when faced with unknown or rare data.</p>
<p>After integrating LLMs into the Botfip framework via knowledge distillation, the downstream task range of the Botfip framework can also be further extended.For instance, the previous Botfip model primarily generated the corresponding computable symbolic operation tree directly from OTS and constant vectors, further producing function expressions.Although this process effectively reconstructs mathematical formulas from given OTS and numerical data, it is essentially unidirectional, only allowing conversion from structured OTS to function expressions but not the reverse, from function expressions to structured OTS or corresponding numerical vectors.The introduction of pre-trained large language models (LLMs) significantly improves this limitation.With the deep learning capabilities of LLMs, the Botfip-LLM framework can now not only generate symbolic expressions from OTS but also derive OTS sequences and constant vectors from function symbolic expressions.When the symbolic expressions of functions are input into the LLMs, the model first converts these expressions into high-dimensional hidden states, which encapsulate the deep semantic structure of the expressions.Subsequently, these high-dimensional states are dimensionally reduced through a specific embedder, and the resulting encoded vectors are used to guide the generation of corresponding OTS sequences and constant vectors via a CrossAttention mechanism, thereby achieving direct conversion from symbolic expressions to corresponding computable symbolic operation trees.Consequently, with the help of LLMs, Botfip-LLM can achieve mutual conversion between symbolic expressions, OTS, and function images, greatly enhancing its technical performance and applicability, and further expanding the diversity of its available downstream tasks.</p>
<p>Related Work</p>
<p>Multi-modal Learning</p>
<p>In recent years, with the continuous development of AI algorithms and computational power, multimodal learning has made significant progress in the field of artificial intelligence.As a milestone in multimodal learning, the CLIP model [17] has demonstrated strong generalization across tasks by learning from image-text pairs.It uses a simple pre-training task to learn image representations on large datasets, followed by zero-shot transfer through natural language.CLIP excels in various computer vision tasks, competing with fully supervised baselines without specific dataset training.The AL-BEF model [20] aligns image and text representations by introducing contrastive loss and then fuses them through cross-modal attention, enhancing the learning of visual and language representations.Unlike existing methods, ALBEF does not require bounding box annotations or high-resolution images and employs momentum distillation to improve learning from noisy network data.</p>
<p>Moreover, the BLIP model [18] proposes a new vision-language pre-training framework that is flexible for both understanding and generation tasks.By introducing a boOTStrapped generation and filtering mechanism, BLIP effectively utilizes noisy image-text data from the web.BLIP achieves stateof-the-art performance on various vision-language tasks and demonstrates strong generalization ability, capable of direct zero-shot transfer to videolanguage tasks.In [27], the Contrastive Captioner (CoCa) model is proposed, which jointly pre-trains an image-text encoder-decoder foundation model with contrastive loss and captioning loss.CoCa effectively combines the advantages of contrastive learning and generative methods and enhances the efficiency of unimodal and multimodal representation learning through a no-cross-attention mechanism.</p>
<p>Advancements in LLMs and Knowledge Distillation Techniques</p>
<p>The success of GPT-3.5 demonstrated the immense potential and practical value of large language models, sparking extensive research and further development in the field, leading to the emergence of various LLMs.For instance, LLaMA [24], developed by Meta's research team, showcases excellent text generation and comprehension abilities by training on larger datasets than usual, using only publicly available data, thus promoting the accessibility and research of LLMs.The subsequent LLaMA-2 [24] introduced multiple improvements over LLaMA 1, such as training on a new mixture of publicly available data.</p>
<p>ChatGLM [28], proposed by Tsinghua University, is a conversational model developed under the General Language Model (GLM) framework.It performs excellently in natural language understanding (NLU) and text generation tasks through autoregressive blank filling pre-training.ChatGLM can handle multi-label fill-in-the-blank problems and demonstrates outstanding performance in NLU, conditional generation, and unconditional generation tasks.Among smaller-scale LLMs, Microsoft's Phi-1.5 model [29] matches the performance of models five times its size on natural language tasks and outperforms most non-frontier LLMs on more complex reasoning tasks such as elementary mathematics and basic coding.In addition to Transformerbased LLMs, non-Transformer LLMs are also being developed.The RWKV model [30] combines the advantages of RNNs and Transformers while overcoming their key drawbacks.It replaces traditional dot-product attention with linear attention mechanisms, significantly reducing computational and memory complexity.The Mamba model [31] is a new type of selective statespace model (SSMs) [32], designed with a simple and unified architecture that combines previous SSM architectures with Transformer MLP blocks.</p>
<p>Despite the rapid development of LLMs, their computational demands and data requirements remain very high, making them difficult for ordinary developers and researchers to utilize directly.Therefore, methods such as Low-Rank Adaptation (LoRA) [33] or knowledge distillation are needed to achieve efficient training and deployment of models with reduced computational resources.Knowledge distillation transfers the knowledge of LLMs to smaller models, significantly reducing computational resource and storage needs while maintaining high performance.This technique not only lowers the cost of model deployment but also enhances the practical usability of models in resource-constrained environments, enabling a wider range of developers and researchers to leverage the powerful capabilities of LLMs for innovation and research.Key methods of LLM knowledge distillation include Supervised Fine-tuning [34,35], Divergence and Similarity [36,37,38,39], Reinforcement Learning [40,41], and Rank Optimization [42,43].A detailed discussion of the field of LLM knowledge distillation is provided in [44], and relevant details can be found in that article.</p>
<p>Neural Symbolic Regression</p>
<p>Neural Symbolic Regression (NSR) is a method that combines deep learning and symbolic regression, aiming to automatically discover mathematical expressions that describe the underlying patterns in data.Unlike traditional numerical regression methods, symbolic regression not only provides a model to fit the data but also generates interpretable symbolic expressions.The field of DSR is rapidly evolving, with reinforcement learning and End-to-End (E2E) approaches receiving the most attention.</p>
<p>In the milestone work of [45], Deep Symbolic Regression (DSR) applies reinforcement learning to optimize the search and generation process of symbolic expressions.In the environment set by the reinforcement learning algorithm, an agent constructs expressions by selecting symbols and operators and receives rewards based on how well these expressions fit the data.Several works have discussed the application of reinforcement learning in DSR and developed new ideas.For example, RL-GEP [46] combines reinforcement learning and genetic algorithms, leveraging the strengths of both to improve the performance in solving symbolic regression problems.Experimental results show that RL-GEP performs excellently on ten benchmark datasets, outperforming methods that use reinforcement learning or genetic algorithms alone.In [47], the authors propose an interactive reinforcement learning platform for grammar-guided symbolic regression, improving SR results by learning user preferences for expression pairs.</p>
<p>On the other hand, the End-to-End (E2E) approach uses Transformer models to directly predict solutions for symbolic regression on synthetic datasets.By using a mixed symbolic-numerical vocabulary, symbolic tokens represent operators and variables, while numerical tokens represent constants.A representative work of the E2E method is [15], where the proposed E2E approach significantly narrows the accuracy gap with state-of-the-art GP techniques in the SRBench benchmark, provides orders-of-magnitude acceleration in inference time, and demonstrates robustness to noise and extrapolation capabilities.Additionally, SymFormer [14] is another type of E2E method that generates constants alongside symbols, improving model accuracy.The generated constants are used to initialize a local gradient optimizer to fine-tune the final constant values.This method has been comprehensively evaluated on a large number of univariate and bivariate functions and compared with relevant alternative methods.</p>
<p>Compared to reinforcement learning and evolutionary algorithms, E2E methods transform the SR problem into a sequence generation problem, eliminating the need for iterative validation processes, thus being more efficient and gaining rapid development in recent years [48,49,50].However, E2E methods face issues such as not being able to accurately validate and adjust the fitting formulas like evolutionary and reinforcement learning algorithms, which need further research to resolve.</p>
<p>Methodology: Description of Botfip-LLM Framework</p>
<p>In this chapter, we detail the Botfip-LLM enhanced extension framework, including its model architecture, pre-training process, fine-tuning tasks, and various specifics.The overall framework and pre-training phase flow can be referenced in Figure 2.For foundational content on the Botfip framework, including the Funcimg-OTS data generation method and dataset format, refer to [21].</p>
<p>Framework Architecture</p>
<p>In this section, we provide a detailed overview of the Botfip-LLM architecture, which includes the Funcimg encoder, OTS encoder, OTS decoder, LLM, and its embedder.The components of the Funcimg encoder, OTS encoder, and decoder have been previously described in [21] with minimal modifications in the Botfip-LLM framework, thus we provide a brief introduction here.Let f o,c ∈ C(R n ; R) denote the computable symbolic operation tree corresponding to OTS o ∈ N k v and constant vector c ∈ R dc , where
N v = {1, ..., N v }, N v &gt; 0 and N v , d c &gt; 0 are the vocab number and constant dimension, respectively. Define the multi-scale meshgrid M δ ∈ R ns×d×n δ ,
where n s , n δ &gt; 0 represent the number of multi-scale channels and the number of grid points per dimension, respectively.The function image corresponding to f o,c can be expressed as Funcimg Encoder: The Funcimg encoder primarily handles feature extraction from function images x i o,c .In the Botfip-LLM framework, we typically choose the ViT model or its variants as the image feature extraction model.Define the expression function corresponding to the Funcimg encoder as e i : R ns×d×n δ → R n i t ×d f , where n i t &gt; 0 is the number of tokens in the features obtained by the Funcimg encoder.Therefore, the features obtained by the Funcimg encoder from the input x i o,c can be expressed as
x i o,c = f o,c (M δ ). Additionally, define x s o,c = Sym(f o,c )h i = e i (x i o,c ) ∈ R n i t ×d f .
OTS Encoder/Decoder: The OTS encoder primarily handles feature extraction from OTS sequences and is used in conjunction with the Funcimg encoder for function image recognition and classification.The OTS decoder is mainly used for subsequent generation tasks.In the Botfip-LLM framework, we choose the BERT model as the backbone architecture.Define the expression functions corresponding to the OTS encoder and decoder backbone networks as
e o (•|•), d o (•|•) : N k v × R dc → R n o t ×d f (
regardless of the presence of external condition inputs), where n o t = k + dc &gt; 0 is the number of tokens in the features obtained by the Funcimg encoder, k and dc are the maximum permissible OTS length and constant length, respectively.If the length of the input OTS and constants is less than these values, they are padded to this length; otherwise, they are clipped 1 .In some tasks, the encoder and decoder may mask parts of the input (especially the constant vector).When there are no external condition inputs, the features of the OTS and constant vector obtained by the OTS encoder can be ex-
pressed as h o = h o (∅) = e o (o, c) ∈ R n o t ×d f ,</p>
<p>and with external features h, as h
o (h) = e o (o, c|h).
The OTS encoder has a corresponding classification head for related classification task training, which generally uses only the global information of the features (i.e., the hidden states of the first token) or their mean.Therefore, the mapping corresponding to this head can be expressed as l o e : R d f → I ko , where I = (0, 1) and k o &gt; 0 is the number of classes, typically 2. On the other hand, the OTS decoder has a corresponding prediction head l o d : R n o t ×d f → I k×Nv .Note that the OTS decoder uses a casual-attention mechanism instead of cross-attention and is only used to predict the OTS, not the constants.The constant vector is iteratively updated using the L-BFGS algorithm during the inference phase after reconstructing the symbolic operation tree skeleton, simplifying model complexity and parameter count.</p>
<p>Frozen LLM and its Embedder: The most significant difference in the Botfip-LLM framework compared to the original model is the LLM and its embedder.The pre-trained LLMs are primarily used for feature extraction from the function expression string x s o,c corresponding to the function f o,c .Define the mapping corresponding to the LLM as e s , with the size of the dictionary set introduced by the LLMs as N M &gt; 0. The function expression string x s o,c is converted into word embedding vectors xs o,c ∈ R Nm×Dm through the tokenizer matched by the LLMs, where N m , D m &gt; 0 are the token number and the embedding dimension of the LLM tokenizer, respectively.The features obtained by the LLM from the function expression string x s o,c can be expressed as hs = e s (x s not match the model-specified feature dimension, requiring the corresponding trainable embedder l s to convert hs into h s ∈ R Nm×d f , making it further usable for subsequent pre-training and fine-tuning tasks.In the Botfip-LLM framework, we choose a simple MLP network as the main structure of the LLM embedder.Since the LLMs are frozen and do not participate in training, the corresponding LLM embedder, acting as an adapter, retains only a small number of parameters, significantly reducing training requirements and difficulty.In the following, we will also introduce a trick for the Botfip-LLM framework under distributed training conditions, making it possible to introduce LLMs with low GPU memory.</p>
<p>Pre-training and Fine-tuning Process</p>
<p>In this section, we introduce the pre-training and fine-tuning phases of the Botfip-LLM framework.The pre-training phase of Botfip-LLM primarily involves calculating the Function Image-OTS Contrastive Loss (FOC), Function Image-OTS Matching Loss (FOM), OTS Modeling Loss (OM), and LLM Knowledge Distillation Loss (LLM-KD).To facilitate the calculation of contrastive learning losses, we adopt the concept of queues from the MoCo framework [51], using queues to store historical feature data, thereby reducing the difficulty of negative sample collection and improving computational efficiency.Let the current dataset be represented as
D o,c = (o j , c j , x i o j ,c j , x s o j ,c j ) N d j=1
, where
N d &gt; 0 is the dataset size. Q i , Q o , Q s
are the corresponding queues for h i , h o , h s (initially generated by random sampling from white noise and subsequently filled with computed features).First, the loss function for the FOC task L F OC can be expressed as
L F OC = InfoNCE(D o,c ) = − 1 N d N d j=1 log exp(sim(h i j , h o j )/τ ) Nq k=1 exp(sim(h i j , h o k )/τ ) + log exp(sim(h o j , h i j )/τ ) Nq k=1 exp(sim(h o j , h i k )/τ ) ,(1)
where sim is the similarity function, typically cosine similarity, τ is the temperature coefficient, which is a learnable parameter, and
h i k , h o k Nq k=1
are the negative samples of historical features sampled from queues Q i , Q o , with N q &gt; 0 being the sample size.</p>
<p>The FOM task can be formulated as a binary classification training task, with the loss function L F OM expressed as
L F OM = − 1 N d N d j=1   N S,+ k=1 log exp(l o e ( ho j (h i j,k,+ ))[0]) exp(l o e ( ho j (h i j,k,+ ))[0]) + exp(l o e ( ho j (h i j,k,+ ))[1]) + N S,− k=1 log exp(l o e ( ho j (h i j,k,− ))[1]) exp(l o e ( ho j (h i j,k,− ))[0]) + exp(l o e ( ho j (h i j,k,− ))[1])   ,(2)ho j (h i j,k,+ ) = e o (o j ,c|h i j,k,+ ), ho j (h i j,k,− ) = e o (o j , c|h i j,k,− ),(3)
where h i j,k,+ , h i j,k,− represent the features of positive and negative samples of the function image corresponding to OTS o j in the training batch, respectively, i.e., whether the function image corresponds to o j , and N S,+ , N S,− &gt; 0 are the numbers of positive and negative samples in the batch, respectively.c is the masked constant vector, and l o e ( ho j (h i j,k,+ )), l o e ( ho j (h i j,k,− )) are the logits output by the model.</p>
<p>The OM task, which is the sequence prediction modeling task for OTS, can be expressed as
L OM = − 1 N d N d j=1   len(oj )−1 k=1 log exp l o d ( ĥo j (h i j ))[k, o j [k + 1]] len(oj ) n=1 exp l o d ( ĥo j (h i j ))[k, n]   ,(4)ĥo j (h i j ) = d o (o j , c|h i j ),(5)
where len(o j ) is the length of o j , and o j [k] is the index of the k-th symbol in the OTS.Next, we introduce how LLMs guide the OTS encoder and Funcimg encoder through knowledge distillation and contrastive learning during the pretraining phase.In Botfip-LLM, we primarily use the Similarity method in knowledge distillation, i.e., guiding the training of student models (OTS encoder and Funcimg encoder) by comparing the hidden states and features of the teacher model (LLMs) with those of the student model [44].Specifically, we align the features of the LLMs, h s , with h i and h o for training.This method plays a crucial role in the Botfip-LLM framework, ensuring that the internal representations of the student model are highly consistent with those of the teacher model.This not only improves the accuracy of the student model in generating outputs but also enhances the consistency of the model in the information processing process.This means that the student model can better mimic the behavior and decision logic of the teacher model when facing complex tasks, thereby significantly improving the model's generalization ability and robustness.At this point, the LLM knowledge distillation task loss L KD can be expressed as
L KD = − 1 N d N d j=1 log exp(sim(h i j , h s j )/τ ) Nq k=1 exp(sim(h i j , h s k )/τ ′ ) + log exp(sim(h o j , h s j )/τ ) Nq k=1 exp(sim(h o j , h s k )/τ ′ ) ,(6)
where τ ′ &gt; 0 is also a temperature coefficient, and h
s k Nq k=1
are the negative samples of historical symbolic formula encoding features collected from queue Q s .By using the Similarity method, Botfip-LLM effectively guides the OTS encoder and Funcimg encoder, enabling the student model to efficiently learn and absorb the knowledge of the teacher model.This approach in the Botfip-LLM framework not only helps align the output features of the Funcimg and OTS encoders with the embedded features of the LLMs but also makes the information processing methods more similar, thereby achieving more efficient and precise knowledge transfer.Ultimately, the total loss function for the pre-training phase L pre is given by
L pre = λ 1 L F OC + λ 2 L F OM + λ 3 L OM + λ 4 L KD ,(7)
where λ 1 , λ 2 , λ 3 , λ 4 are the respective weight coefficients.</p>
<p>In the engineering implementation of the pre-training process, we use a queue to store historical data to enhance the effectiveness of contrastive learning.Specifically, we store samples generated in the previous training rounds in a fixed-size queue and compare the current batch of samples with</p>
<h1>Calculate Features</h1>
<p>6:
h i j = e i j (x i o j ,c j ) 7: h o j = e o (o j , c j ) 8: ho j (h i j ) = e o (o j , c | h i j ) 9: do j (h i j ) = d o (o j , c | h i j ) 10:
if Using Distributed learning then Split hs and broadcast the corresponding features to the respective GPUs h s = l s ( hs )
h i k , h o k , h s k Nq k=1 ∼ (Q i , Q o , Q s ) 18:
Calculate L F OC , L F OM , L OM , L KD through eq.( 1), ( 2), ( 4), (6) 19:
L pre = λ 1 L F OC + λ 2 L F OM + λ 3 L OM + λ 4 L KD 20:
Update trained parameters with respect to L pre using the optimizer
21: Update queues (Q i , Q o , Q s ) cyclically 22:
end for 23: end for those in the queue during each training round.The main advantage of this method is that it increases the diversity of negative samples, enabling the model to better learn the differences between different modalities.Additionally, during the pre-training phase, we typically share the weights of the OTS encoder e o and decoder d o to reduce the number of pre-training parameters and accelerate the training process.</p>
<p>Furthermore, in the case of distributed training, handling the loading of LLMs on GPUs becomes crucial, especially under low GPU memory conditions.If a distributed data parallel strategy is employed, LLMs need to be loaded onto all GPUs, which significantly consumes GPU memory and may hinder training.Therefore, we adopt a distributed aggregation approach, where an independent parameter-frozen LLM is deployed on a single GPU, while the Botfip-LLM main model is deployed on other GPUs for distributed training, which is shown in Figure 3.At this point, the deployment of LLMs on GPUs can utilize half-precision or even 8-bit or 4-bit quantization methods to further reduce the model parameters and computational load, thereby improving LLM computational efficiency.When encoder output features are generated on different GPUs, these features are aggregated to the GPU hosting the LLM through inter-GPU data transfer.After inference by the LLM, the resulting features are broadcasted by the main GPU to the corresponding GPUs for distributed training.The process flow is illustrated in figure.This method significantly reduces the computational requirements of the model and avoids redundant deployment of LLMs, enabling multimodal frameworks like Botfip-LLM to perform distributed training under low GPU memory conditions, thereby alleviating the computational burden on future scientific researchers.The pseudocode for the Botfip-LLM pre-training phase can be found in Algorithm 1.</p>
<p>Fine-tuning Task: Formula String-OTS Transformation</p>
<p>In previous fine-tuning tasks, such as the Funcimg-OTS generation finetuning task, details can be found in [21].The original Botfip model achieved the recognition of function images by fine-tuning the OTS decoder, thereby generating the corresponding OTS.However, we previously lacked the ability to directly transform symbolic formula strings into OTS.This might seem straightforward since we could manually calculate the transformation from symbolic expressions to OTS, but in reality, it presents many challenges.The more complex the symbolic expression, the harder it is to transform it into the corresponding symbolic operation tree and thus obtain the OTS.There are numerous calculations that might involve simplifications and other processes, complicating the transformation from the original symbolic operation tree.First, the complexity of symbolic formulas brings about structural parsing difficulties.Complex formulas contain multi-layered nested operations and various mathematical symbols, requiring precise parsing of their syntax and semantic structure.Second, converting formulas into symbolic operation trees involves considering the precedence and associativity of different operators, increasing the complexity of the transformation.Moreover, the diversity of variables and parameters in symbolic formulas demands that the model has a high degree of generalization capability to handle various forms of input.Another notable issue is that different structures of symbolic operation trees might correspond to the same symbolic formula, implying that the relationship between symbolic formulas and symbolic operation trees is not one-to-one.Thus, obtaining a simplified OTS from symbolic formulas is also a challenge.</p>
<p>Thanks to the introduction of LLM and its embedder, we were able to extend the original Botfip model, adding the capability to transform symbolic formula strings into OTS.In this fine-tuning task, similar to the pre-training phase, the LLM and its embedder extract features from the input formula string x s o,c , generating feature representations h i s of the symbolic formula.These feature representations capture the syntactic and semantic information of the formula, providing enough context to understand the structure and content of the formula.The extracted feature representations h i s are then passed to the OTS decoder through a Causal-Attention mechanism.The Causal-Attention mechanism ensures that the sequence dependency is maintained during the generation process, resulting in coherent and accurate OTS.The loss function for the corresponding fine-tuning task, L SOM , can be expressed as follows:
L SOM = − 1 N d N d j=1   len(o j )−1 k=1 log exp l o d ( ĥo j (h s j ))[k, o j [k + 1]] len(o j ) n=1 exp l o d ( ĥo j (h s j ))[k, n]   ,(8)ĥo j (h s j ) = d o (o j , c | h s j ).(9)
Finally, the OTS decoder generates the corresponding optimized transfer sequence based on the input feature representations.Through this process, we achieve the automatic transformation from symbolic formula strings to OTS, avoiding the cumbersome steps of manual calculation and improving the accuracy and efficiency of the transformation.This method not only addresses the difficulties posed by the complexity of symbolic formulas but also provides the model with greater flexibility and scalability, significantly simplifying the process of handling symbolic expressions.</p>
<p>Experiments</p>
<p>In this chapter, we present the experiments, training, and validation results of Botfip-LLM.The training and validation datasets used in this study are consistent with the open-source Funcimg-OTS dataset used in Botfip.Details on the dataset, main model, and relevant hyperparameters during training can be found in Table A. 5. Some examples of the Funcimg-OTS-Formula String multimodal dataset can be found in Appendix Appendix B. Similarly, we employed three evaluation metrics: the regularity of OTS generation Acc r , relative sequence Levenshtein similarity S RL , and relative formula-string Levenshtein similarity SRL , defined as follows:
Acc r (Ω) = 1 N N i=1 1 (A i ) ,(10)S RL (Ω) = 1 N N i=1 l(t i ) − D RL (p i , t i ) l(t i ) ,(11)SRL (Ω) = 1 N N i=1 1 (A i ) l( ti ) − D RL pi , ti l( ti ) ,(12)
where A i denotes the event where p i can be reconstructed as an operation tree, D RL is the Levenshtein distance [52], l(t i ) is the length of the operation tree sequence t j , and pi and ti are the formula string expressions obtained after reconstructing the operation tree sequence into the operation tree.Acc r measures the degree to which the generated operation tree sequences can be restored into the operation tree structure, serving as a metric for sequence regularity.S RL and SRL not only incorporate Acc r but also reflect the differences between the target and predicted symbolic expressions after being reduced to operation trees and computed.These metrics validate the effectiveness of OTS generation.</p>
<p>Pre-training Performance with Knowledge Distillation from LLMs</p>
<p>In this section, we will showcase the performance of Botfip-LLM during the pre-training phase.A crucial question is which LLMs can achieve the  [24], ChatGLM-2 [53], Gemma [25], Mistral [54], Mamba [31], Qwen-1.5 [55], RWKV-6 [30], and Phi-2 [29].It is important to note that Mistral's official weights require an API key, so we have chosen alternative weights for this model, while the weights for the other LLMs are the official weights provided by their respective institutions.Additionally, Phi-2 is a smaller LLM with a parameter count of up to 2.8B, whereas the other models range from 6B to 7B parameters.Specific information about the introduced LLMs, including their pre-trained model checkpoints and Hug-gingFace paths, as well as the hidden sizes of different LLM networks, can be found in Table 1.It should be noted that the pre-training corpus has a significant impact on the pre-trained LLM models.Therefore, the results of knowledge distillation training in this paper cannot solely reflect the advantages or disadvantages of the LLMs' architectures in Botfip-LLM knowledge distillation pre-training.We aim to identify the most suitable pre-trained LLM model weights for Botfip-LLM through this comparative approach.Figure 4 shows the loss curves for the four different loss functions L F OC , L F OM , L OM and L KD over 1 × 10 6 steps during the pre-training phase for the Botfip-LLM framework with eight different LLMs introduced as the Teacher models for the symbolic formula feature extraction network.Additionally, Table 2   at the 1 × 10 6 th step.From the trend of the four loss functions in Figure 4, it can be observed that while L F OC , L F OM , L OM all show a decreasing trend under the guidance of different LLMs, L KD varies greatly with the type of LLM.When ChatGLM-2, Gemma, and Qwen-1.5 are used as Teacher models, L KD shows a significant downward trend, with ChatGLM-2 decreasing the fastest and most stably, Gemma showing considerable fluctuation, and Qwen-1.5 initially rising and then showing a marked decrease.Conversely, LLaMA-2, Mistral, and Phi-2 exhibit a rapid initial increase in L KD , followed by a slow decline after approximately 4×10 4 steps, with LLaMA-2 decreasing the fastest, followed by Mistral, and Phi-2 the slowest.Lastly, Mamba and RWKV-6 show a rapid increase in L KD which barely decreases within 1 × 10 6 steps.For these non-Transformer LLMs, the differences in model architecture likely cause significant differences in the extraction of multimodal information features, making it challenging to effectively transfer knowledge using standard similarity-based distillation methods.For Transformer-based LLMs like ChatGLM-2, Gemma, and Qwen-1.5, we believe that the pre-training corpus and the hidden size of the final model features have a more significant impact on the knowledge distillation training curves shown in Figure 4 when the LLM parameters are of the same magnitude.Both ChatGLM-2 and Qwen-1.5 are large Chinese models extensively using Chinese corpora during pre-training, possibly including a large amount of mathematical-related corpus, and both support 8K-32K contexts.It can be seen that ChatGLM-2 quickly and stably reduces the knowledge distillation loss during the pre-training phase, while Qwen-1.5, although slightly inferior to ChatGLM-2, also shows a rapid decrease in L KD after a certain training stage.The difference between these two is that Qwen-1.5 has a hidden size of 151936, while ChatGLM-2 has a hidden size of 65024.A larger hidden size means that the LLM embedder requires more time for parameter tuning, and the closer the hidden size is to the student model, the more similar the LLM feature representation is to the standard BERT model, making adjustments easier.On the other hand, the Gemma model, which does not use a multilingual corpus, shows From Figure 4 and Table 2, it can be seen that ChatGLM-2 shows relatively the best performance during the pre-training phase.Now we proceed with further testing on the pre-training validation dataset.We randomly sample 50 multimodal samples from the validation dataset and use the corresponding encoders to obtain the respective Funcimg features, OTS features, and Formula Str features generated by the LLM and its embedder.We then compute the cosine similarity between the features of different samples and visualize them in Figure 5, including the cases of ChatGLM-2 and RWKV-6 as the Teacher LLM.It is evident that the alignment between different modalities before and after pre-training with ChatGLM-2 as the Teacher LLM is very distinct.The feature extraction capability for the function symbol formula of ChatGLM-2 is effectively transferred to the student model of Botfip-LLM, i.e., the Funcimg Encoder and OTS Encoder, through similarity knowledge distillation.On the other hand, although the Funcimg Encoder and OTS Encoder of Botfip-LLM achieve self-alignment through pre-training with RWKV-6 as the Teacher LLM, the features obtained by these encoders do not successfully align with the character features obtained by RWKV-6, indicating that RWKV-6's knowledge was not successfully transferred to the main model of Botfip-LLM, forming a stark contrast with ChatGLM-2 and other Transformer LLMs.</p>
<p>Fine-tuning Performance of Botfip-LLM under LLMs</p>
<p>In this chapter, we focus on fine-tuning experiments of Botfip-LLM in different downstream tasks, including the Funcimg-OTS Modeling fine-tuning task and the LLM Formula String-OTS fine-tuning task.The former has already been experimented and analyzed within the Botfip framework, while the latter is a new fine-tuning task enabled by the introduction of the LLM model within the Botfip-LLM framework.This expansion allows the LLM to not only serve as a Teacher Model for knowledge distillation but also to broaden the range of downstream tasks in this multimodal scientific computing framework by incorporating LLM components.In the fine-tuning tasks, we mainly test against the original Botfip framework and Botfip-LLM using ChatGLM-2 and RWKV-6 as the Teacher LLM and symbolic formula feature extraction network.This is because the performance of these two models in the pre-training phase is representative, and the pre-training performance of other models generally falls between these two.Therefore, we select the Botfip-LLM pre-trained models corresponding to these two LLMs for the fine-tuning phase experiments.</p>
<p>Funcimg-OTS Modeling Task Performance</p>
<p>Now, we proceed with the fine-tuning test of the Funcimg-OTS Modeling task.In this fine-tuning task, we will evaluate the fine-tuning performance of the Botfip-LLM model under the pre-training conditions of different LLMs as Teacher Models and validate the results on the validation dataset.Following the fine-tuning process in [21], we will select multimodal datasets with different numbers of nodes as the fine-tuning dataset.This selection is crucial because the number of nodes in the computation tree determines the length of the OTS, thereby reflecting the difficulty of the OTS Modeling fine-tuning task.</p>
<p>As shown in Figure 6, the loss variations in the Funcimg-OTS Modeling fine-tuning task for the original Botfip model and the Botfip-LLM model with ChatGLM-2 and RWKV-6 as pre-trained Teacher LLMs are presented in detail.It is evident from the figure that the Botfip-LLM models exhibit lower initial errors and faster reduction rates compared to the original Botfip model.</p>
<p>Furthermore, the pre-trained model using ChatGLM-2 as the Teacher LLM shows superior performance in terms of convergence speed and final error compared to the model using RWKV-6.Table 3 further details the steady-state loss average errors and corresponding standard deviations for the original Botfip model and the Botfip-LLM models in the Funcimg-OTS Modeling fine-tuning task.According to the data in the table, the Botfip-LLM model using ChatGLM-2 as the pre-trained model demonstrates a lower average error and smaller standard deviation in the loss function variation compared to the original Botfip model.On the other hand, when RWKV-6 is used as the Teacher Model during the pre-training phase, its loss variation curve is similar to that of the original Botfip model, showing only slight improvement in the initial training phase.This indicates that during the pre-training process, ChatGLM-2 effectively transfers its knowledge to the Botfip-LLM main model through knowledge distillation, enabling better integration of symbolic formula modality information, thereby improving model performance.In contrast, RWKV-6, under specific weights, fails to effectively guide the Botfip-LLM main model during pre-training, resulting in no significant performance improvement in the fine-tuning phase compared to the original model.Furthermore, we observe the Acc r , S RL , and SRL metrics calculated on a validation set of 4000 samples, generated by a symbolic operation tree random generation system, to reflect the training effectiveness and extrapolation ability of the fine-tuned Botfip and Botfip-LLM models (with ChatGLM-2 and RWKV-6 as Teacher LLMs, respectively).Figure 7 shows the trend of these metrics with varying node counts in the symbolic operation tree for the different models.According to the validation results, the Botfip-LLM model with ChatGLM-2 as the Teacher LLM performed well, slightly surpassing the original Botfip model and the Botfip-LLM model with RWKV-6 as the Teacher in all metrics.Specifically, for the Acc r metric, the Botfip-LLM (ChatGLM-2) maintained a higher accuracy throughout the validation  process.Although the advantage was not significant, it indicates that the OTS generated by Botfip-LLM (ChatGLM-2) is more effective compared to the original Botfip model and Botfip-LLM (RWKV-6).In terms of S RL and SRL metrics, the Botfip-LLM (ChatGLM-2) performed better, showing that the quality of OTS generated by this model is superior to that of the original model and Botfip-LLM (RWKV-6).These results not only confirm the potential of ChatGLM-2 in enhancing model performance but also provide crucial guidance for future model development and experimental design.The outstanding performance of the Botfip-LLM model with ChatGLM-2 as the Teacher model also highlights the importance of selecting an appropriate pre-training model for optimizing training outcomes.</p>
<p>Finally, we generated 100 sets of symbolic operation tree skeletons with corresponding constant vectors based on different node counts.The generated function images were rearranged and used as discrete point sets required for symbolic regression methods.These sets were used in the regression process for gplearn, PYSR [56], PSTree [57], DSR [58], and AI-Feynman (AIF) [59].We compared Botfip-LLM and the original Botfip model with these SR frameworks.It is important to note that Botfip-LLM and the original Botfip model are multimodal scientific computing frameworks that rely on function images for feature extraction rather than discrete point sets.Therefore, they cannot be tested using conventional SR datasets.However, the symbolic operation tree system's random generation dataset proposed in [  has been open-sourced, and the random generation process is sufficiently reliable.Table A.5 reflects part of the dataset, demonstrating its objectivity.Thus, we only consider randomly generated symbolic operation tree data for comparative testing, rather than using conventional SR datasets.The test result information is displayed in the radar chart 8.It can be seen that Botfip-LLM shows improved extrapolation ability compared to the original Botfip model.However, as the number of symbolic operation tree nodes increases, the complexity of the OTS also increases.Since Botfip-LLM does not show significant improvements in error correction capabilities compared to the original Botfip model, its extrapolation ability is still inferior to methods like PYSR and DSR, which iteratively search for optimal expressions.Nevertheless, introducing LLM for knowledge distillation has indeed enhanced the model's understanding of symbolic information, further improving recognition and regression performance.</p>
<p>Formula String-OTS Modeling Task Performance</p>
<p>In this section, we continue using the same fine-tuning dataset to demonstrate the Symbolic Formula String to OTS modeling fine-tuning task.In Table 4: Mean fine-tuning error and its standard deviation at the 25 000th step of Botfip-LLM in Formula String-OTS Modeling Task under ChatGLM-2 and RWKV-6.</p>
<p>LLM Name</p>
<p>node 5 node 6 node 7 node 8 node 9 ChatGLM-2 0.681 ± 0.007 0.727 ± 0.009 0.742 ± 0.011 0.772 ± 0.012 0.796 ± 0.013 RWKV-6 0.929 ± 0.012 0.966 ± 0.015 1.009 ± 0.018 1.034 ± 0.019 1.070 ± 0.021 this downstream task, the LLM is no longer a Teacher Model but becomes one of the components for feature extraction of multimodal data related to the symbolic operation tree.Similar to the Funcimg-OTS Modeling Task, the LLM processes the Formula String of the symbolic operation tree through a tokenizer to extract symbolic formula features, which are then embedded into the Botfip-LLM main model's OTS Decoder through the corresponding LLM Embedder.By integrating the symbolic formula feature information via Causal Attention, the OTS Decoder can predict the corresponding OTS. Figure 9 illustrates the training loss curves for the pre-trained Botfip-LLM models in the Formula String-OTS Modeling fine-tuning task, with ChatGLM-2 and RWKV-6 serving as the Teacher LLMs and corresponding Formula String feature extraction networks.Table 4 shows the respective convergence error conditions.It is evident that in the previous pre-training process, ChatGLM-2 successfully transferred its understanding of symbolic information to the Botfip-LLM main model through knowledge distillation, whereas RWKV-6 did not achieve this effect.Consequently, in this finetuning task, the Botfip-LLM model under ChatGLM-2 as the Formula String feature extraction network exhibits a faster decline and better convergence accuracy in Formula String-OTS Modeling loss compared to RWKV-6.Now we proceed to test the effectiveness of Botfip-LLM in generating OTS from Formula Strings.We continue to use the three evaluation metrics: Acc r , S RL , and SRL .Figure 10 presents the test results of Botfip-LLM (ChatGLM-2) and Botfip-LLM (RWKV-6) on 100 randomly generated symbol tree samples after fine-tuning.It can be observed that compared to the Funcimg-OTS fine-tuning task, Botfip-LLM (ChatGLM-2) shows a more significant advantage in the Formula String-OTS fine-tuning task.This primarily stems from the fact that during the pre-training phase, ChatGLM-2 adjusted the parameters of the LLM embedder within Botfip-LLM to a certain extent through fine-tuning, making its dimensionality reduction mapping of the hidden state more suitable for feature extraction of symbolic formula information compared to RWKV-6, thus achieving better performance in this fine-tuning     task.From this fine-tuning training, it can also be concluded that the introduction of LLM not only helps the Botfip-LLM framework gain a better understanding of the multimodal information of scientific computing data but also effectively broadens the applicability of the Botfip-LLM framework, thereby further accomplishing downstream tasks related to symbolic formulas.</p>
<p>Conclusion</p>
<p>This paper proposes the Botfip-LLM multimodal scientific computing framework, which is based on the Botfip framework, which significantly enhances its capability to handle symbolic formula information by introducing LLMs and employing knowledge distillation technology for aligning multimodal data related to symbolic operation trees.Experimental results indicate that the choice of different types of LLM frameworks and their pre-trained weights is crucial for the pre-training and fine-tuning performance of Botfip-LLM.Notably, with the support of ChatGLM-2, Botfip-LLM demonstrates significant improvements in processing and understanding symbolic formula information compared to other Transformer frameworks such as LLaMA-2 and non-Transformer frameworks like RWKV-6 and Mamba.Moreover, by integrating LLMs, the Botfip-LLM framework further extends its range of fine-tuning tasks.Botfip-LLM can not only generate symbolic expressions from OTS but also reverse-engineer OTS sequences and constant vectors from symbolic expressions, significantly enhancing its technical performance and applicability while expanding its use cases in scientific computing tasks.Although Botfip-LLM's extrapolation ability has improved compared to the original Botfip framework, it still exhibits a decline with an increasing number of symbolic operation tree nodes.Future research should focus on further enhancements to improve the model's overall performance and applicability.Botfip-LLM holds promise for helping researchers, engineers, and students better understand the essence and core of scientific computing problems through the alignment and information extraction of function images and symbolic formulas.</p>
<p>Nodes</p>
<p>Formula Skeleton OTS x 0 •sin(x 1 ) + C 3 + C 1 [9, 0, 13, 0, 4, 0, 6, 5, 0, 18, 0, 19, 0, 0, 0]
5 C 0 • (x 0 + x 1 ) 2 + C 1 [15, 0, 1, 0, 19, 18, 0, 0, 0] 5 C 0 • |x 0 x 1 | + C 1 [20, 0, 3, 0, 18, 19, 0, 0, 0] 5 C 0 • C 2 x 0 x 1 + C 1 [6, 0, 3, 0, 19, 18, 0, 0, 0] 5 C 0 • (x 0 − x 1 ) 2 + C 1 [15, 0, 2, 0, 18, 19, 0, 0, 0] 5 C 0 • tanh(x 0 + x 1 ) + C 1 [9, 0, 1, 0, 18, 19, 0, 0, 0] 5 C 0 • sin(x 0 + x 1 ) + C 1 [5, 0, 1, 0, 19, 18, 0, 0, 0] 5 C 0 • x 2 0 x 2 1 + C 1 [15, 0, 4, 0, 18, 19, 0, 0, 0] 5 C 0 • tan(x 0 x 1 ) + C 1 [7, 0, 3, 0, 18, 19, 0, 0, 0] 5 C 0 • C 2 sin(exp(x 0 x 1 )) + C 1 [6, 0, 5, 0, 11, 0, 3, 0, 18, 19, 0, 0, 0] 5 C 0 • exp(x 1 /x 0 ) + C 1 [11, 0, 4, 0, 19, 18, 0, 0, 0] 5 C 0 • x 1 • |x 0 | + C 1 [3, 0, 20, 19, 0, 18, 0, 0, 0] 5 C 0 • (C 2 /x 0 + x 1 ) + C 1 [1, 0, 6, 19, 0, 18, 0, 0, 0] 5 C 0 • (−x 1 + exp(x 0 )) + C 1 [2, 0, 11, 19, 0, 18, 0, 0, 0] 5 C 0 • log(|x 1 |)/x 0 + C 1 [4</p>
<p>Continued on next page</p>
<p>Nodes Formula Skeleton OTS 8 C 0 • tan(x 1 )</p>
<p>x 2 0 + C 1 [15, 0, 16, 0, 4, 0, 7, 15, 0, 19, 0, 18, 0, 0, 0] 8 C 0 • exp 2•C 2 x 0 + 2 • tanh(x 0 ) + C 1 [15, 0, 11, 0, 1, 0, 6, 9, 0, 18, 0, 18, 0, 0, 0] * The table shows only a subset of our datasets; in fact, our pretraining dataset contains 11,028 skeletons and 551,400 Funcimg-OTS pairs.All constant vectors were sampled from the uniform distribution U (−20, 20).</p>
<p>(a) Visualization of the Botfip-LLM Framework (b) Visualization of Block Details in the Botfip-LLM Framework</p>
<p>Figure 2 :
2
Figure 2: Visualization of the main structure and related details of the Botfip-LLM framework, with Figure 2a illustrating the main forward computation and operational process during the pre-training phase, and Figure 2b providing detailed information and forward computation processes of each major module.</p>
<p>as the symbolic expression obtained from the function f o,c via symbolic computation, where Sym is the symbolic computation operator.In this framework, the dimension of features obtained by all encoders is unified as d f &gt; 0 for alignment training.Note that the transformer-based encoder may or may not use the cross-attention mechanism to integrate external data features.Let the mapping represented by the model network be e, and we use e(•|•) to indicate the presence or absence of external condition inputs.When there are no external condition inputs, e(•) = e(•|∅) denotes the encoder using self-attention, with ∅ indicating an empty condition input.When there are external condition features h, e(•|h) indicates the use of cross-attention.</p>
<p>Figure 3 :
3
Figure 3: Visualization of the Botfip-LLM distributed training aggregation and broadcast process.The main model is distributed across GPUs 0, 1, and 2, while a half-precision or quantized Frozen LLM model is deployed on GPU 3.During distributed training, features obtained by the models on GPUs 0, 1, and 2 are aggregated and transmitted via the main GPU to the LLM on GPU 3 for feature extraction of the symbolic expression x s o,c .The results are then broadcasted back to the main models on other GPUs for further computation.</p>
<p>Algorithm 1
1
Botfp-LLM Pre-training Procedure 1: Input: Funcimg-OTS-Formula Dataset D o,c = (o j , c j , x i o j ,c j , x s o j ,c j N d j=1 , OTS Encoder e o and its classification head l o e , OTS Decoder d o and its prediction head l o d , Funcimg Encoder e i , Pre-trained Frozen LLM e s and its embedder l s , Queues (Q i , Q o , Q s ), Number of Epochs E, Learning Rates λ 1 , λ 2 , λ 3 , λ 4 , Queue Size Q, Temperature τ, τ ′ , masked constant array c. 2: Initialize queues Q I , Q O and bind the weights of the e o and d o .3: for e ← 1 to E do 4: for each batch o j , c j , x i o j ,c j , x s o j ,c j in D o,c do 5:</p>
<p>11 :
11
Aggregate x s o,c from different GPUs to the GPU where the LLM is deployed 12: hs = e s (x s o,c ) 13:</p>
<p>Figure 4 :
4
Figure 4: During the pre-training phase, the mean and standard deviation of the loss functions L F OC , L F OM , L OM , and L KD are visualized for the Botfip-LLM framework with the introduction of eight different LLMs.The y-axis of the L F OM plot uses a logarithmic scale, while the y-axes of the other plOTS use a linear scale.</p>
<p>Figure 5 :
5
Figure 5: Visualization of the cosine similarity matrices between Funcimg features, OTS features, and Formula Str features generated by different encoders in the Botfip-LLM framework for 50 validation samples before and after pre-training.Figures5a and 5bshow the results with ChatGLM-2 as the Teacher LLM, while Figure5cpresents the results with RWKV-6 as the Teacher LLM after pre-training.</p>
<p>Figure 6 :
6
Figure 6: Training loss curves for the Funcimg-OTS Modeling fine-tuning task with different numbers of nodes in the symbolic operation tree.The curves represent the training progress for the original Botfip model, Botfip-LLM (with ChatGLM-2 as the Teacher LLM), and Botfip-LLM (with RWKV-6 as the Teacher LLM).</p>
<p>Figure 7 :
7
Figure 7: Illustration of the variation in Acc r , S RL , and SRL metrics with respect to the number of nodes in the symbolic operation tree during the Funcimg-OTS Modeling fine-tuning task.The different line styles represent the performance of the original Botfip model, Botfip-LLM with ChatGLM-2 as the Teacher LLM, and Botfip-LLM with RWKV-6 as the Teacher LLM.</p>
<p>Figure 8 :
8
Figure 8: Comparison of MSE results on test sets generated by symbolic operation tree systems with different node counts for various SR models, the original Botfip model, and Botfip-LLM (with ChatGLM-2 as the Teacher LLM).</p>
<p>Figure 9 :
9
Figure 9: Loss variation during the training process of the Formula String-OTS Modeling fine-tuning task for Botfip-LLM (ChatGLM-2) and Botfip-LLM (RWKV-6).The legend below indicates different colors representing the number of nodes in the symbolic operation trees for the datasets used in the respective fine-tuning experiments.</p>
<p>Figure 10 :
10
Figure 10: A bar chart visualizing the results on the Formula String-OTS test set after Botfip-LLM (ChatGLM-2) and Botfip-LLM (RWKV-6) underwent fine-tuning for the Formula String-OTS Modeling task.</p>
<p>weather conditions.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2353-2363, 2022.Appendix B. Appendix: Selected Skeletons Information from the Funcimg-OTS-Formula String Dataset</p>
<p>1 [ 1 [ 6 C 0 • C 2 (x 0 +x 1 ) 2 + C 1 [ 2 C 3 3 1 1 [ 1 [C 1 • x 1 • 1 [ 8 C 1 • 8 C 1 • 8 √ 2 • 8 C 0 • C 3 • |x 1 |/x 1 2C 2 + C 1 [ 1 [C 3 •x 1 +C 4 C 2 + C 1 [ 1 3+C 0 • C 2 tanh(C 3 •x 1 1 [(C 3 •x 1 +C 4 ) / |x 0 | 1 4 + C 1 [ 2 − 1 [ 2 C 3 •x 1 +C 4 / |x 0 | 1 4 + C 1 [
116212123311111818182803112113421123113141121231411
, 0, 12, 18, 0, 20, 0, 0, 19, 0, 0] 5C 0 • x 1 • |sin(x 0 )| + C 1 [3, 0, 16, 19, 0, 5, 0, 0, 18, 0, 0] 5 C 0 • (x 1 + tanh(x 0 )) + C 1 [1, 0, 9, 19, 0, 18, 0, 0, 0] 5 C 0 • sin(x 0 − x 1 ) + C 1 [5, 0, 2, 0, 18, 19, 0, 0, 0] 5C 0 • |x 1 /x 0 | + C 1 [20, 0, 4, 0, 19, 18, 0, 0, 0] 5 C 0 • (x 0 + sin(x 1 )) + C 1 [1, 0, 5, 18, 0, 19, 0, 0, 0] 5 C 0 • tanh(x 0 x 1 ) + C 1 [9, 0, 3, 0, 19, 18, 0, 0, 0] 5C 0 • |x 0 | + C 1 [16, 0, 3, 0, 18, 18, 0, 0, 0] 5 C 0 • exp(x 1 /x 0 ) + C 1 [11, 0, 4, 0, 18, 19, 0, 0, 0] 5 C 0 • tan(x 0 − x 1 ) + C sin(x 0 )/x 1 + C 1 [4, 0, 5, 19, 0, 18, 0, 0, 0] 5 C 0 • x 0 • log(|x 1 |) + C 1 [3, 0, 12, 18, 0, 19, 0, 0, 0] 5 C 0 • exp(−x 0 + x 1 ) + C 1 [11, 0, 2, 0, 19, 18, 0, 0, 0] 5 C 0 • (x 0 + tan(x 1 )) + C 1 [1, 0, 7, 18, 0, 19, 0, 0, 0] 5 C 0 • (x 0 + exp(x 1 )) + C 1 [1, 0, 11, 18, 0, 19, 0, 0, 0] 5 C 0 • (x 1 + sin(x 0 )) + C (x 0 )) + C 1 [2, 0, 7, 15, 0, 18, 0, 19, 0, 0, 0] 6 C 0 • (exp(x 1 ) + tan(x 1 )) + C 1 [1, 0, 7, 11, 0, 19, 0, 19, 0, 0, 0] 6 C 0 • (exp(x 1 ) + sin(x 0 )) + C 1 [1, 0, 11, 5, 0, 19, 0, 18, 0, 0, 0] 6C 0 • (−C 2 x 1 − C 3 + sin(x 0 )) + C 1 [2, 0, 5, 13, 0, 18, 0, 19, 0, 0, 0] 6C 0 • (exp(x 0 ) + tanh(x 1 )) + C 1 [1, 0, 11, 9, 0, 18, 0, 19, 0, 0, 0] 6 C 0 • (x 0 − x 1 ) 2C 2 + C 1 [15, 0, 14, 0, 2, 0, 18, 19, 0, 0, 0] 6 C 0 • |C 2 x 0 x 1 + C 3 | + C 1 [20, 0, 13, 0, 3, 0, 19, 18, 0, 0, 0] 6 C 0 • tanh(x 0 + x 1 ) 2 + C 1 [15, 0, 9, 0, 1, 0, 18, 19, 0, 0, 0] 6 C 0 • tanh(x 0 x 1 ) 2 + C 1 [15, 0, 9, 0, 3, 0, 19, 18, 0, 0, 0] 6 C 0 • | C 2 x 0 x 1 | + C 1 [20, 0, 6, 0, 3, 0, 18, 19, 0, 0, 0] 6 C 0 • tan(sin(x 0 x 1 )) + C 1 [7, 0, 5, 0, 3, 0, 18, 19, 0, 0, 0] 6 C 0 • exp(max(0, x 0 + x 1 )) + C 1 [11, 0, 21, 0, 1, 0, 18, 19, 0, 0, 0] 6 C 0 • sin( |x 2 1 |) + C 1 [5, 0, 16, 0, 3, 0, 19, 19, 0, 0, 0] 6 C 0 • |tan(x 0 x 1 )| + C 1 [16, 0, 7, 0, 3, 0, 18, 19, 0, 0, 0] (x 0 −x 1 )+C 4 + C 1 [6, 0, 13, 0, 2, 0, 18, 19, 0, 0, 0] 6 C 0 • tan(C 2 x 0 x 1 + C 3 ) + C 1 [7, 0, 13, 0, 3, 0, 18, 19, 0, 0, 0] 6 C 0 • (x 2 1 − exp(x 0 )) + C 1 [2, 0, 15, 11, 0, 19, 0, 18, 0, 0, 0] 6 C 0 • x 2 0 exp(x 0 ) + C 1 [3, 0, 15, 11, 0, 18, 0, 18, 0, 0, 0] 6 C 0 • (x 2 0 + sin(x 0 )) + C 1 [1, 0, 5, 15, 0, 18, 0, 18, 0, 0, 0] 6 C 0 • sin(x 1 ) tanh(x 1 ) + C 1 [3, 0, 9, 5, 0, 19, 0, 19, 0, 0, 0] 6 C 0 • (C 2 /x 0 + sin(x 1 )) + C 1 [1, 0, 5, 6, 0, 19, 0, 18, 0, 0, 0] 6C 1 • (−C 0 + x 0 − x 1 ) + C 2 [2, 0, 2, 10, 0, 18, 19, 0, 0, 0, 0] 6 C 0 • (−x 0 + 2x 1 ) + C 1 [1, 0, 2, 19, 0, 19, 18, 0, 0, 0, 0] 6 C 0 • (−2x 0 + x 1 ) + C 1 [2, 0, 2, 18, 0, 19, 18, 0, 0, 0, 0] 6 C 0 • (−x 0 + x 1/) 2 + C 1 [15, 0, 2, 0, 17, 18, 0, 19, 0, 0, 0] 7 C 0 • log(|x 1 + exp(x 0 )|) + C 1 [12, 0, 20, 0, 1, 0, 11, 19, 0, 18, 0, 0, 0] 7C 0 • tan( |x 0 − exp(x 1 )|) + C tanh(|C 2 • x 1 + C 3 + x 0 |) + C tanh(x 0 ) + C 2 [3, 0, 9, 3, 0, 18, 0, 10, 19, 0, 0, 0, 0] 7C 0 • C 2 •x 1 +C 3 x 0 −x 1 + C (x 0 • x 1 − x 2 1 ) C 2 + C 1 [14, 0, 2, 0, 3, 15, 0, 18, 19, 0, 19, 0, 0, 0, 0] tan C 0 x 1 − sin(x 0 ) + C 2 [7, 0, 2, 0, 4, 5, 0, 10, 19, 0, 18, 0, 0, 0, 0]8 C 0 • exp(x 0 • x 1 − x 2 1 ) + C 1 [11, 0, 2, 0, 3, 15, 0, 18, 19, 0, 19, 0, 0, 0, 0]C 0 •x 1 max(0,x 0 ) + C 2[20, 0, 4, 0, 3, 21, 0, 10, 19, 0, 18, 0, 0, 0, 0]C 0 • |x 0 • (C 2 • x 1 + C 3 )| + C 1 [16, 0, 3, 0, 1, 13, 0, 18, 18, 0, 19, 0, 0, 0, 0] 15, 0, 14, 0, 3, 0, 6, 16, 0, 19, 0, 19, 0, 0, 0]8 C 0 • |C 2 • x 1 + C 3 + tanh(x 1 )| + C 1 [21, 0, 20, 0, 1, 0, 13, 9, 0, 19, 0, 19, 0, 0, 0]8 C 0 • (x C 2 1 − exp(x 0 )) 2 + C 1 [20, 0, 15, 0, 2, 0, 14, 11, 0, 19, 0, 18, 0, 0, 0]8 C 0 • |exp(x 1 ) − tanh(x 1 )| 2 + C 1 [15, 0, 20, 0, 2, 0, 9, 11, 0, 19, 0, 19, 0, 0, 0] 8C 0 • tan(tan(x 1 ) + |x 1 |) 2 + C 1 [15, 0, 7, 0, 1, 0, 20, 7, 0, 19, 0, 19, 0, 0, 0] 8C 0 • (C 2 • x 1 + C 3 + x 2 1 ) 4 + C 1 [15, 0, 15, 0, 1, 0, 13, 15, 0, 19, 0, 19, 0, 0, 0] 8C 0 • |C 2 • exp(x 0 ) • tan(x 0 ) + C 3 | + C 1 [16, 0, 13, 0, 3, 0, 7, 11, 0, 18, 0, 18, 0, 0, 0] 8 C 0 • tan(x 2 0 − sin(x 0 )) 2 + C 20, 0, 14, 0, 4, 0, 15, 13, 0, 18, 0, 19, 0, 0, 0] 8 C 0 • (C 2 • (exp(x 0 ) + tan(x 1 )) + C 3 ) C 1 [17, 0, 13, 0, 1, 0, 11, 7, 0, 18, 0, 19, 0, 0, 0] 8 C 0 • tan(tanh(x 2 1 • |x 0 |)) + C 1 [7, 0, 9, 0, 3, 0, 16, 15, 0, 18, 0, 19, 0, 0, 0] 8 C 0 • tanh( |C 2 • x 0 + C 3 − sin(x 0 )|) + C 1 [9, 0, 16, 0, 2, 0, 5, 13, 0, 18, 0, 18, 0, 0, 0] 8 C 0 • C 2 2 • exp(max(0, x 1 ))/x 2 0 + C 1 [3, 0, 15, 11, 0, 6, 0, 21, 0, 18, 0, 19, 0, 0, 0] 8 C 0 • tanh(max(0, x 1 ))/ |x 1 | + C 1 [4, 0, 9, 15, 0, 21, 0, 16, 0, 19, 0, 19, 0, 0, 0] 8 C 0 • (exp(x 2 1 ) + |x 0 | 2 ) + C 1 [1, 0, 15, 11, 0, 20, 0, 15, 0, 18, 0, 19, 0, 0, 0] 8 C 0 • tan(x C 2 1 − tanh(x 1 )) + C 1 [16, 0, 7, 0, 2, 0, 9, 14, 0, 19, 0, 19, 0, 0, 0] 8 C 0 • exp(C 2 • x 2 0 • |x 0 | + C 3 ) + C 1 [11, 0, 13, 0, 3, 0, 15, 20, 0, 18, 0, 18, 0, 0, 0] 8 +C 4 +tanh(x 0 )) + C exp(x 0 )+log(|x 0 |) + C</p>
<p>Table 1 :
1
Introduced Pre-trained LLMs Information
LLM NameHuggingFace PathParams Hidden SizeLLaMA-2meta-llama/Llama-2-7b7B32000ChatGLM-2THUDM/chatglm2-6b6B65024Gemmagoogle/gemma-7b7B256000Mistralunsloth/mistral-7b-v0.27B32000MambaTRI-ML/mamba-7b-rw7B50432Qwen-1.5Qwen/Qwen1.5-7B7B151936RWKV-6RWKV/rwkv-6-world-7b7B65536Phi-2microsoft/phi-22.8B51200best knowledge distillation performance within the Botfip-LLM multimodalscientific computing framework, thereby aiding Botfip-LLM in better multi-
modal information mining of function sets generated by symbolic operation trees.Here, we have selected 8 LLM models to serve as the teacher model and the main network for symbolic formula feature extraction in Botfip-LLM: LLaMA-2</p>
<p>presents the specific average training error and its standard deviation
Loss2.5 5.0 7.5L FOC10 5 10 2L FOM1 2L OM2.5 5.0 7.5L KD02468 10 ×10 402468 10 ×10 402468 10 ×10 402468 10 ×10 4Loss2.5 5.0 7.502468 10 ×10 410 5 10 3 10 102468 10 ×10 41 202468 10 ×10 46 7 80246×10 4 8 10Loss2.5 5.0 7.502468 10 ×10 410 5 10 202468 10 ×10 41 202468 10 ×10 42.5 5.0 7.50246×10 4 8 10Loss2.5 5.0 7.502468 10 ×10 410 5 10 3 10 102468 10 ×10 41 202468 10 ×10 46 80246×10 4 8 10Loss2.5 5.0 7.502468 10 ×10 410 5 10 3 10 102468 10 ×10 41 202468 10 ×10 46 80246×10 4 8 10Loss2.5 5.0 7.510 5 10 3 10 11 26 802468 10 ×10 402468 10 ×10 402468 10 ×10 402468 10 ×10 4Loss2.5 5.0 7.502468 10 ×10 410 5 10 3 10 102468 10 ×10 41 202468 10 ×10 46 7 80246×10 4 8 10Loss2.5 5.0 7.5024 Steps 68 10 ×10 410 3 10 1 10 5 ChatGLM-2 0 2 LLaMA-24 Steps 68 10 ×10 4 Gemma Mamba2 102 Mistral Qwen-1.5 4 Steps 68 10 ×10 4 RWKV-6 Phi-28 6024 Steps 68 10 ×10 4</p>
<p>Table 2 :
2
Average training error and its standard deviation at the 1 × 10 6 th step during the pre-training phase of Botfip-LLM under different LLMs as teacher models.± 0.16 3.51 × 10 −7 ± 7.58 × 10 −6 0.78 ± 0.02 7.37 ± 0.06 ChatGLM-2 0.82 ± 0.19 7.66 × 10 −8 ± 7.43 × 10 −6 0.68 ± 0.02 2.15 ± 0.44 Gemma 0.97 ± 0.23 2.43 × 10 −7 ± 1.62 × 10 −5 0.72 ± 0.03 3.57 ± 0.62 Mistral 1.29 ± 0.19 2.77 × 10 −7 ± 5.62 × 10 −6 0.79 ± 0.04 8.08 ± 0.04 Mamba 1.47 ± 0.22 9.33 × 10 −7 ± 4.02 × 10 −6 0.82 ± 0.04 9.08 ± 0.05 Qwen-1.5 0.90 ± 0.22 2.42 × 10 −7 ± 1.68 × 10 −5 0.71 ± 0.03 3.98 ± 0.17 RWKV-6 1.66 ± 0.20 5.94 × 10 −7 ± 2.48 × 10 −6 0.84 ± 0.04 9.36 ± 0.06 Phi-2 1.34 ± 0.17 7.62 × 10 −7 ± 4.45 × 10 −6 0.80 ± 0.03 8.50 ± 0.05
LLM NameL F OCL F OML OML KDLLaMA-21.16</p>
<p>Table 3 :
3
Mean fine-tuning error and its standard deviation at the 25 000th step of Botfip and Botfip-LLM under ChatGLM-2 and RWKV-6 in Funcimg-OTS Modeling Task.
Modelnode 5node 6node 7node 8node 9Botfip0.668 ± 0.006 0.672 ± 0.008 0.677 ± 0.010 0.686 ± 0.012 0.690 ± 0.010Botfip-LLM (ChatGLM-2)0.621 ± 0.006 0.624 ± 0.006 0.626 ± 0.007 0.633 ± 0.008 0.634 ± 0.007Botfip-LLM (RWKV-6)0.663 ± 0.006 0.667 ± 0.007 0.673 ± 0.008 0.687 ± 0.009 0.691 ± 0.008</p>
<p>Table B .
B
6: Selected Formula Skeletons and Their OTS from the Funcimg-OTS Dataset</p>
<p>o,c ) ∈ R Nm×D ′ m , where D ′ m &gt; 0 is the feature dimension obtained by the LLM. The dimension of the features hs generally does1 In general, avoid clipping the OTS due to exceeding the maximum allowable length of the model, as this can result in loss of OTS information, affecting alignment.
Appendix A. Hyper-parameters Involved in the Dataset and Experiment Nodes Formula Skeleton[15, 0, 3, 0, 1, 10, 0, 19, 18, 0, 0, 0, 0] 7, 0, 15, 5, 0, 20, 0, 19, 0, 18, 0, 0, 0] 7, 0, 15, 15, 0, 5, 0, 18, 0, 19, 0, 0, 0] 7, 0, 20, 12, 0, 13, 0, 18, 0, 19, 0, 0, 0] 7 C 0 • exp(−x 0 ) • sin(tan(x 0 )) + C 1 [4, 0, 5, 11, 0, 7, 0, 18, 0, 18, 0, 0, 0] 7, 0, 7, 0, 1, 0, 6, 19, 0, 18, 0, 0, 0], 0, 13, 0, 2, 0, 15, 18, 0, 19, 0, 0, 0] 7, 0, 20, 0, 3, 0, 11, 19, 0, 18, 0, 0, 0] 7, 0, 16, 2, 0, 19, 0, 18, 19, 0, 0, 0, 0] 7, 0, 17, 1, 0, 18, 0, 18, 19, 0, 0, 0, 0] 7, 0, 13, 1, 0, 19, 0, 10, 18, 0, 0, 0, 0], 0, 15, 4, 0, 19, 0, 18, 19, 0, 0, 0, 0], 0, 13, 3, 0, 18, 0, 19, 18, 0, 0, 0, 0] 7, 0, 4, 11, 0, 7, 18, 0, 19, 0, 19, 0, 0, 0, 0], 0, 1, 6, 0, 6, 18, 0, 19, 0, 18, 0, 0, 0, 0] 8 C 0 • (x 0 + sin(x 1 ) − tanh(x 0 )) + C 1 [2, 0, 1, 9, 0, 5, 18, 0, 18, 0, 19, 0, 0, 0, 0] 8 C 1 • (−C 0 + tan(x 1 ) + max(0, x 0 )) + C 2 [1, 0, 2, 21, 0, 7, 10, 0, 18, 0, 19, 0, 0, 0, 0]Continued on next page
Artificial intelligence for weather forecasting. Silvia Conti, Nature Reviews Electrical Engineering. 112024</p>
<p>Transweather: Transformer-based restoration of images degraded by adverse. Jeya Maria, Jose Valanarasu, Rajeev Yasarla, Patel, </p>
<p>Spatio-temporal transformer network for weather forecasting. Junzhong Ji, Jing He, Minglong Lei, Muhua Wang, Wei Tang, IEEE Transactions on Big Data. 2024</p>
<p>Physics-informed neural networks (pinns) for fluid mechanics: A review. Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, George Em Karniadakis, Acta Mechanica Sinica. 37122021</p>
<p>Physicsinformed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational physics. 3782019</p>
<p>Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2010.08895Fourier neural operator for parametric partial differential equations. 2020arXiv preprint</p>
<p>Gnot: A general neural operator transformer for operator learning. Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, Jian Song, Jun Zhu, International Conference on Machine Learning. PMLR2023</p>
<p>Gene transformer: Transformers for the gene expression-based classification of lung cancer subtypes. Anwar Khan, Boreom Lee, arXiv:2108.118332021arXiv preprint</p>
<p>Transformer for gene expression modeling (t-gem): An interpretable deep learning model for gene expression-based phenotype predictions. Ting-He Zhang, Md Musaddaqul Hasib, Yu-Chiao Chiu, Zhi-Feng Han, Yu-Fang Jin, Mario Flores, Yidong Chen, Yufei Huang, Cancers. 141947632022</p>
<p>Machine learning and artificial intelligence in pharmaceutical research and development: a review. Sheela Kolluri, Jianchang Lin, Rachael Liu, Yanwei Zhang, Wenwen Zhang, The AAPS journal. 242022</p>
<p>Artificial intelligence for drug discovery: Resources, methods, and applications. Wei Chen, Xuesong Liu, Sanyin Zhang, Shilin Chen, Molecular Therapy-Nucleic Acids. 312023</p>
<p>Spectratr: A novel deep learning model for qualitative analysis of drug spectroscopy based on transformer structure. Pengyou Fu, Yue Wen, Yuke Zhang, Lingqiao Li, Yanchun Feng, Lihui Yin, Huihua Yang, Journal of Innovative Optical Health Sciences. 150322500212022</p>
<p>Symbolicgpt: A generative transformer model for symbolic regression. Mojtaba Valipour, Bowen You, Maysum Panju, Ali Ghodsi, arXiv:2106.141312021arXiv preprint</p>
<p>Symformer: End-to-end symbolic regression using transformer-based architecture. Martin Vastl, Jonáš Kulhánek, Jiří Kubalík, Erik Derner, Robert Babuška, IEEE Access. 2024</p>
<p>End-to-end symbolic regression with transformers. Pierre-Alexandre Kamienny, Guillaume Stéphane D'ascoli, François Lample, Charton, Advances in Neural Information Processing Systems. 202235</p>
<p>Ai for science: Report on the department of energy (doe) town halls on artificial intelligence (ai) for science. Rick Stevens, Valerie Taylor, Jeff Nichols, Arthur Barney Maccabe, Katherine Yelick, David Brown, 2020Argonne, IL (United StatesArgonne National Lab.(ANL)Technical report</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, International conference on machine learning. PMLR2021</p>
<p>Align before fuse: Vision and language representation learning with momentum distillation. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu, Hong Hoi, Advances in neural information processing systems. 342021</p>
<p>Bootstrapping otsfuncimg pre-training model (botfip)-a comprehensive symbolic regression framework. Tianhao Chen, Pengbo Xu, Haibiao Zheng, arXiv:2401.097482024arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu, arXiv:2205.01917Coca: Contrastive captioners are image-text foundation models. 2022arXiv preprint</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Glm, arXiv:2103.10360General language model pretraining with autoregressive blank infilling. 2021arXiv preprint</p>
<p>Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat, Lee , arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023arXiv preprint</p>
<p>Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran, G V , arXiv:2305.13048Reinventing rnns for the transformer era. 2023arXiv preprint</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Tri Dao, arXiv:2312.007522023arXiv preprint</p>
<p>Mamba-360: Survey of state space models as transformer alternative for long sequence modelling. Badri Narayana, Patro , Vijay Srinivas Agneeswaran, arXiv:2404.16112Methods, applications, and challenges. 2024arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.011082019arXiv preprint</p>
<p>Minillm: Knowledge distillation of large language models. Yuxian Gu, Li Dong, Furu Wei, Minlie Huang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, Lawrence Carin, Mixkd, arXiv:2011.00593Towards efficient distillation of large-scale language models. 2020arXiv preprint</p>
<p>Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. Inar Timiryasov, Jean-Loup Tastet, arXiv:2308.020192023arXiv preprint</p>
<p>Ultrafeedback: Boosting language models with high-quality feedback. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, Maosong Sun, arXiv:2310.013772023arXiv preprint</p>
<p>Reward design with language models. Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, arXiv:2303.000012023arXiv preprint</p>
<p>Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, arXiv:2310.16944Direct distillation of lm alignment. 2023arXiv preprint</p>
<p>Cyclealign: Iterative distillation from black-box llm to white-box models for better human alignment. Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan, arXiv:2310.162712023arXiv preprint</p>
<p>Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou, arXiv:2402.13116A survey on knowledge distillation of large language models. 2024arXiv preprint</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. Mikel Brenden K Petersen, Landajuela, Claudio P Nathan Mundhenk, Soo K Santiago, Joanne T Kim, Kim, arXiv:1912.048712019arXiv preprint</p>
<p>Rl-gep: symbolic regression via gene expression programming and reinforcement learning. Hengzhe Zhang, Aimin Zhou, 2021 International Joint Conference on Neural Networks (IJCNN). IEEE2021</p>
<p>Interactive reinforcement learning for symbolic regression from multiformat human-preference feedbacks. Laure Crochepierre, Lydia Boudjeloud-Assala, Vincent Barbesant, IJCAI. 2022</p>
<p>Transformer-based planning for symbolic regression. Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, Chandan Reddy, Advances in Neural Information Processing Systems. 362024</p>
<p>A seq2seq approach to symbolic regression. Luca Biggio, Tommaso Bendinelli, Aurelien Lucchi, Giambattista Parascandolo, Learning Meets Combinatorial Algorithms at NeurIPS2020. 2020</p>
<p>Accelerating understanding of scientific experiments with end to end symbolic regression. Nikos Aréchiga, Francine Chen, Yan-Ying Chen, Yanxia Zhang, Rumen Iliev, Heishiro Toyoda, Kent Lyons, arXiv:2112.040232021arXiv preprint</p>
<p>Improved baselines with momentum contrastive learning. Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He, arXiv:2003.042972020arXiv preprint</p>
<p>A normalized levenshtein distance metric. Li Yujian, Liu Bo, IEEE transactions on pattern analysis and machine intelligence. 200729</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.024142022arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Interpretable machine learning for science with pysr and symbolicregression. Miles Cranmer, arXiv:2305.015822023jl. arXiv preprint</p>
<p>Ps-tree: A piecewise symbolic regression tree. Swarm and Evolutionary Computation. Hengzhe Zhang, Aimin Zhou, Hong Qian, Hu Zhang, 202271101061</p>
<p>A unified framework for deep symbolic regression. Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P Santiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, Brenden K Petersen, Advances in Neural Information Processing Systems. 202235</p>
<p>Ai feynman: A physics-inspired method for symbolic regression. Silviu- , Marian Udrescu, Max Tegmark, Science Advances. 61626312020</p>            </div>
        </div>

    </div>
</body>
</html>
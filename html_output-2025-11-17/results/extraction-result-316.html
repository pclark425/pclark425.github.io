<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-316 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-316</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-316</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-5dc15ac1c92ab7492f121471823fb13a95d273ba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5dc15ac1c92ab7492f121471823fb13a95d273ba" target="_blank">A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework is presented and results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism.</p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e316.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e316.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J (mesh-transformer-jax implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6B-parameter decoder-only Transformer variant (parallel attention, rotary positional encodings) evaluated on two- and three-operand arithmetic; analyzed here with causal mediation interventions on MLP and attention activations to identify components mediating arithmetic predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (parallel attention, rotary positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand queries); also evaluated in three-operand query setting</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Default operands sampled from S = {1..300} (multi-digit tokens may be split); some analyses restricted to S = {1..20} or single-digit (tokenizer-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Two-shot prompting; causal mediation analysis via interventions: store activations (m_t^{(l)}, a_t^{(l)}) for one prompt and overwrite corresponding activations in another prompt, then measure indirect effect (IE); neuron-level interventions for top neurons; comparisons with result-fixed prompts; alternate IE via log-prob differences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Two-operand accuracy (Table 4): addition 69.3%, subtraction 78.0%, multiplication 82.8%, division 40.8%, overall 67.8%. Numeral-words prompting improved performance (overall 81.3%; addition 95.5%, division 59.7%). On three-operand queries GPT-J performed poorly (<10%); Pythia was used for three-operand fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Attention transports operand/operator information from early (mid-sequence) token representations to the final token; late MLPs at the last token (peak effect near layers ~19-20 in GPT-J) process that gathered information and inject result-related signals into the residual stream; early-layer MLPs encode operand/operator tokens; attention blocks with high IE located around layers ~11-18 at the last token mediate information flow.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Relative-Importance (RI) metric shows last-token mid-late MLPs account for a large fraction of IE on arithmetic (GPT-J RI ≈ 40.2% for late last-token MLPs) and this drops sharply when the result r is fixed (to ≈4.4%), indicating specificity to result computation; numeral-words prompting produced higher accuracy; model-size comparisons in paper show similar activation sites across evaluated sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Relatively poor performance on division compared to other ops; struggles on three-operand queries (low few-shot accuracy without fine-tuning); tokenizer splitting of multi-digit numbers complicates evaluation (tokenizer-dependent); behaviour can be context-dependent/inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparisons made across: numeral words vs Arabic numerals; result-varying vs result-fixed operand pairs; different models (Pythia 2.8B, LLaMA 7B, Goat); number retrieval and factual-prediction tasks (to test specificity); neuron-level overlaps across tasks; alternative IE metric (log-prob difference).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Arithmetic predictions arise by (1) early/ mid-sequence MLPs encoding operands/operators, (2) attention mechanisms moving that information to the final token, and (3) late last-token MLPs computing/injecting result-related information (peak around layers 19–20).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e316.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e316.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia 2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2.8B-parameter autoregressive Transformer from the Pythia suite, evaluated on two- and three-operand arithmetic and used for fine-tuning experiments on three-operand queries to analyze mechanistic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2.8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand); many 3-operand two-operator templates (29 combinations) for three-operand experiments</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Default S = {1..300} for two-operand experiments; for some neuron-level analyses and numeral-word experiments restricted to {1..20}; three-operand fine-tuning used sampled triples (training set described).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Two-shot prompting; same causal mediation intervention/testing protocol (store m_t^{(l)}, a_t^{(l)} from one prompt, inject into another) to compute IE; fine-tuning on three-operand queries (2 epochs, Adafactor lr=1e-5, batch 8, 1000 triples per template) to measure emergence of late MLP activation site.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Two-operand accuracy (Table 4): addition 57.4%, subtraction 77.5%, multiplication 64.7%, division 40.2%, overall 59.9%. Three-operand: before fine-tuning overall 0.9%, after fine-tuning ∼39.7% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Before fine-tuning, three-operand behavior showed only very-late last-layer activation at the last token; after fine-tuning the mid-late last-token MLP activation site (similar to two-operand case) emerged, supporting that training can induce the attention→late-MLP arithmetic processing pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Fine-tuning on three-operand tasks produced a large accuracy jump (0.9% → ∼39.7%) and induced the mid-late last-token MLP activation site; RI for last-token late MLPs measured at ≈43.2% (result-varying) dropping to ≈5.8% when result fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Very poor out-of-the-box performance on complex (three-operand) arithmetic; division remains relatively weak; model improvements require targeted fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared before vs after fine-tuning; compared IE patterns vs number-retrieval and factual tasks; compared with other models (GPT-J, LLaMA, Goat); result-fixed vs varying experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fine-tuning on multi-operand arithmetic causes the mid-late last-token MLP activation site to emerge and substantially improves three-operand accuracy, supporting that the attention-to-last-token then MLP-processing pattern is learnable and can be strengthened by training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e316.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e316.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter decoder-only Transformer (LLaMA family) evaluated on two-operand arithmetic; included to test whether observed activation dynamics generalize across model families and sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand queries)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Restrained to single-digit results for some analyses because LLaMA tokenizer treats each digit as an independent token (tokenizer-dependent limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Two-shot prompting; causal mediation interventions on attention and MLP activations to compute IE and RI metrics, same protocol as for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Two-operand accuracy (Table 4): addition 100.0%, subtraction 99.8%, multiplication 100.0%, division 88.7%, overall 97.2% (note tokenizer restrictions applied for multi-digit handling).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Observed same qualitative pattern: early MLPs encode operand/operator tokens and attention conveys information to the last token, with mid-late last-token MLPs contributing strongly to result prediction (RI for late last-token MLPs ≈36.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>High accuracy relative to smaller models; RI results qualitatively consistent with other models, suggesting mechanism generalizes across model families and sizes (within range tested).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower relative performance on division (88.7%) compared to other ops; evaluation restricted by tokenizer handling of multi-digit numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-J and Pythia and to Goat (finetuned LLaMA) to assess effects of finetuning and tokenizer differences; result-fixed vs varying experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Even in a different model family, arithmetic processing shows the same overall architecture: early token MLPs → attention-mediated transfer → late last-token MLPs that encode result-related information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e316.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e316.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goat (LLaMA finetuned on arithmetic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A version of LLaMA fine-tuned specifically on arithmetic tasks (per Liu & Low, 2023); used to test how targeted fine-tuning affects performance and internal activation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (finetuned LLaMA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (LLaMA-based, fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (two-operand queries)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Same operand ranges as other two-operand experiments (paper uses S = {1..300} where tokenization permits); specifics inherited from LLaMA setup.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Evaluated with two-shot prompting and the same causal mediation intervention experiments to measure IE and RI across MLP and attention modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Two-operand accuracy (Table 4): addition 100.0%, subtraction 100.0%, multiplication 91.4%, division 54.0%, overall 85.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Activation patterns are consistent with other models (attention conveys operand/operator info; late last-token MLPs encode results) though magnitudes and exact neuron involvement differ; fine-tuning increases arithmetic competence but does not fully equalize division performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Fine-tuning improves overall arithmetic performance versus base LLaMA on some ops; however division remains lower relative to others, suggesting fine-tuning yields operation-specific gains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Division performance notably lower (54.0%) than other operations despite fine-tuning; suggests certain operations remain harder even after task-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to base LLaMA and other models to evaluate effects of arithmetic-specific fine-tuning; neuron-level overlap analyses performed to compare circuits across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Task-specific fine-tuning (as in Goat) raises arithmetic accuracy and engages similar attention→late-MLP circuitry, but relative weaknesses (e.g., division) can persist and neuron-level circuits show partial overlap with non-arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e316.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e316.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention→Late-MLP arithmetic mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanistic account: attention-mediated transfer of operand/operator information to last-token MLPs which compute results</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanistic hypothesis, supported empirically in this paper, that Transformer LMs solve simple arithmetic by encoding operands/operators in early token representations, using attention to move that information to the final-token representation, then applying late feed-forward (MLP) layers at the final token to produce result-related signals injected into the residual stream.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies across evaluated models (GPT-J, Pythia 2.8B, LLaMA 7B, Goat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division (bi-variate); generalizes to tri-variate after fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Evaluated primarily on small-to-medium integers (default S = {1..300}); some analyses use restricted ranges ({1..20} or single-digit) for tokenizer and neuron-comparison consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Causal mediation analysis: intervene on attention outputs a_t^{(l)} and MLP outputs m_t^{(l)} by copying activations from one prompt/run into another, computing indirect effect (IE) as a relative change in predicted probability (and alternate log-prob metric); neuron-level interventions and result-fixed vs varying comparisons used to separate operand- vs result-related signals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Key mechanistic observations: (1) early-layer MLPs at operand/operator token positions carry operand/operator information; (2) attention heads/modules (not per-head isolated here) at mid-to-late layers transfer that information to the last token (attention IE peaks around layers ~11–18); (3) late last-token MLPs (peaking near layers ~19–20 in GPT-J) incorporate and output result-related information into the residual stream; (4) neuron-level analysis shows ~50% overlap between Arabic numerals and numeral-words top neurons, but limited overlap (≈22%) with simple number-retrieval tasks and random-level overlap (~10%) with factual prediction, indicating task-specific circuits within similar anatomical regions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Pattern observed across multiple model sizes/families; fine-tuning can induce or strengthen the mid-late last-token MLP activation site (Pythia 2.8B three-operand fine-tuning); RI metric shows late last-token MLPs contribute large fraction of IE for arithmetic (e.g., GPT-J ~40%), and that contribution drops when result is fixed, indicating specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to number-retrieval (synthetic) and factual-prediction tasks; compared result-varying vs result-fixed prompts; neuron-level overlap comparisons across tasks; compared before/after fine-tuning for multi-operand tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Arithmetic computation in these Transformers is implemented by a small, consistent circuit: encode numbers/operators early, use attention to gather relevant inputs at the final token, and perform the core result computation in mid-late feed-forward (MLP) layers at that final token.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space <em>(Rating: 2)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 2)</em></li>
                <li>A mathematical framework for transformer circuits <em>(Rating: 2)</em></li>
                <li>Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Investigating gender bias in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Dissecting recall of factual associations in auto-regressive language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-316",
    "paper_id": "paper-5dc15ac1c92ab7492f121471823fb13a95d273ba",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-J",
            "name_full": "GPT-J (mesh-transformer-jax implementation)",
            "brief_description": "A 6B-parameter decoder-only Transformer variant (parallel attention, rotary positional encodings) evaluated on two- and three-operand arithmetic; analyzed here with causal mediation interventions on MLP and attention activations to identify components mediating arithmetic predictions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J",
            "model_size": "6B",
            "model_architecture": "decoder-only transformer (parallel attention, rotary positional encodings)",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand queries); also evaluated in three-operand query setting",
            "number_range_or_complexity": "Default operands sampled from S = {1..300} (multi-digit tokens may be split); some analyses restricted to S = {1..20} or single-digit (tokenizer-dependent).",
            "method_or_intervention": "Two-shot prompting; causal mediation analysis via interventions: store activations (m_t^{(l)}, a_t^{(l)}) for one prompt and overwrite corresponding activations in another prompt, then measure indirect effect (IE); neuron-level interventions for top neurons; comparisons with result-fixed prompts; alternate IE via log-prob differences.",
            "performance_result": "Two-operand accuracy (Table 4): addition 69.3%, subtraction 78.0%, multiplication 82.8%, division 40.8%, overall 67.8%. Numeral-words prompting improved performance (overall 81.3%; addition 95.5%, division 59.7%). On three-operand queries GPT-J performed poorly (&lt;10%); Pythia was used for three-operand fine-tuning experiments.",
            "mechanistic_insight": "Attention transports operand/operator information from early (mid-sequence) token representations to the final token; late MLPs at the last token (peak effect near layers ~19-20 in GPT-J) process that gathered information and inject result-related signals into the residual stream; early-layer MLPs encode operand/operator tokens; attention blocks with high IE located around layers ~11-18 at the last token mediate information flow.",
            "performance_scaling": "Relative-Importance (RI) metric shows last-token mid-late MLPs account for a large fraction of IE on arithmetic (GPT-J RI ≈ 40.2% for late last-token MLPs) and this drops sharply when the result r is fixed (to ≈4.4%), indicating specificity to result computation; numeral-words prompting produced higher accuracy; model-size comparisons in paper show similar activation sites across evaluated sizes.",
            "failure_modes": "Relatively poor performance on division compared to other ops; struggles on three-operand queries (low few-shot accuracy without fine-tuning); tokenizer splitting of multi-digit numbers complicates evaluation (tokenizer-dependent); behaviour can be context-dependent/inconsistent.",
            "comparison_baseline": "Comparisons made across: numeral words vs Arabic numerals; result-varying vs result-fixed operand pairs; different models (Pythia 2.8B, LLaMA 7B, Goat); number retrieval and factual-prediction tasks (to test specificity); neuron-level overlaps across tasks; alternative IE metric (log-prob difference).",
            "key_finding": "Arithmetic predictions arise by (1) early/ mid-sequence MLPs encoding operands/operators, (2) attention mechanisms moving that information to the final token, and (3) late last-token MLPs computing/injecting result-related information (peak around layers 19–20).",
            "uuid": "e316.0",
            "source_info": {
                "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Pythia-2.8B",
            "name_full": "Pythia 2.8B",
            "brief_description": "A 2.8B-parameter autoregressive Transformer from the Pythia suite, evaluated on two- and three-operand arithmetic and used for fine-tuning experiments on three-operand queries to analyze mechanistic changes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pythia",
            "model_size": "2.8B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand); many 3-operand two-operator templates (29 combinations) for three-operand experiments",
            "number_range_or_complexity": "Default S = {1..300} for two-operand experiments; for some neuron-level analyses and numeral-word experiments restricted to {1..20}; three-operand fine-tuning used sampled triples (training set described).",
            "method_or_intervention": "Two-shot prompting; same causal mediation intervention/testing protocol (store m_t^{(l)}, a_t^{(l)} from one prompt, inject into another) to compute IE; fine-tuning on three-operand queries (2 epochs, Adafactor lr=1e-5, batch 8, 1000 triples per template) to measure emergence of late MLP activation site.",
            "performance_result": "Two-operand accuracy (Table 4): addition 57.4%, subtraction 77.5%, multiplication 64.7%, division 40.2%, overall 59.9%. Three-operand: before fine-tuning overall 0.9%, after fine-tuning ∼39.7% accuracy.",
            "mechanistic_insight": "Before fine-tuning, three-operand behavior showed only very-late last-layer activation at the last token; after fine-tuning the mid-late last-token MLP activation site (similar to two-operand case) emerged, supporting that training can induce the attention→late-MLP arithmetic processing pattern.",
            "performance_scaling": "Fine-tuning on three-operand tasks produced a large accuracy jump (0.9% → ∼39.7%) and induced the mid-late last-token MLP activation site; RI for last-token late MLPs measured at ≈43.2% (result-varying) dropping to ≈5.8% when result fixed.",
            "failure_modes": "Very poor out-of-the-box performance on complex (three-operand) arithmetic; division remains relatively weak; model improvements require targeted fine-tuning.",
            "comparison_baseline": "Compared before vs after fine-tuning; compared IE patterns vs number-retrieval and factual tasks; compared with other models (GPT-J, LLaMA, Goat); result-fixed vs varying experiments.",
            "key_finding": "Fine-tuning on multi-operand arithmetic causes the mid-late last-token MLP activation site to emerge and substantially improves three-operand accuracy, supporting that the attention-to-last-token then MLP-processing pattern is learnable and can be strengthened by training.",
            "uuid": "e316.1",
            "source_info": {
                "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA 7B",
            "brief_description": "A 7B-parameter decoder-only Transformer (LLaMA family) evaluated on two-operand arithmetic; included to test whether observed activation dynamics generalize across model families and sizes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "7B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand queries)",
            "number_range_or_complexity": "Restrained to single-digit results for some analyses because LLaMA tokenizer treats each digit as an independent token (tokenizer-dependent limitation).",
            "method_or_intervention": "Two-shot prompting; causal mediation interventions on attention and MLP activations to compute IE and RI metrics, same protocol as for other models.",
            "performance_result": "Two-operand accuracy (Table 4): addition 100.0%, subtraction 99.8%, multiplication 100.0%, division 88.7%, overall 97.2% (note tokenizer restrictions applied for multi-digit handling).",
            "mechanistic_insight": "Observed same qualitative pattern: early MLPs encode operand/operator tokens and attention conveys information to the last token, with mid-late last-token MLPs contributing strongly to result prediction (RI for late last-token MLPs ≈36.1%).",
            "performance_scaling": "High accuracy relative to smaller models; RI results qualitatively consistent with other models, suggesting mechanism generalizes across model families and sizes (within range tested).",
            "failure_modes": "Lower relative performance on division (88.7%) compared to other ops; evaluation restricted by tokenizer handling of multi-digit numbers.",
            "comparison_baseline": "Compared to GPT-J and Pythia and to Goat (finetuned LLaMA) to assess effects of finetuning and tokenizer differences; result-fixed vs varying experiments.",
            "key_finding": "Even in a different model family, arithmetic processing shows the same overall architecture: early token MLPs → attention-mediated transfer → late last-token MLPs that encode result-related information.",
            "uuid": "e316.2",
            "source_info": {
                "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Goat",
            "name_full": "Goat (LLaMA finetuned on arithmetic tasks)",
            "brief_description": "A version of LLaMA fine-tuned specifically on arithmetic tasks (per Liu & Low, 2023); used to test how targeted fine-tuning affects performance and internal activation patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Goat",
            "model_size": "7B (finetuned LLaMA variant)",
            "model_architecture": "decoder-only transformer (LLaMA-based, fine-tuned)",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (two-operand queries)",
            "number_range_or_complexity": "Same operand ranges as other two-operand experiments (paper uses S = {1..300} where tokenization permits); specifics inherited from LLaMA setup.",
            "method_or_intervention": "Evaluated with two-shot prompting and the same causal mediation intervention experiments to measure IE and RI across MLP and attention modules.",
            "performance_result": "Two-operand accuracy (Table 4): addition 100.0%, subtraction 100.0%, multiplication 91.4%, division 54.0%, overall 85.6%.",
            "mechanistic_insight": "Activation patterns are consistent with other models (attention conveys operand/operator info; late last-token MLPs encode results) though magnitudes and exact neuron involvement differ; fine-tuning increases arithmetic competence but does not fully equalize division performance.",
            "performance_scaling": "Fine-tuning improves overall arithmetic performance versus base LLaMA on some ops; however division remains lower relative to others, suggesting fine-tuning yields operation-specific gains.",
            "failure_modes": "Division performance notably lower (54.0%) than other operations despite fine-tuning; suggests certain operations remain harder even after task-specific finetuning.",
            "comparison_baseline": "Compared to base LLaMA and other models to evaluate effects of arithmetic-specific fine-tuning; neuron-level overlap analyses performed to compare circuits across tasks.",
            "key_finding": "Task-specific fine-tuning (as in Goat) raises arithmetic accuracy and engages similar attention→late-MLP circuitry, but relative weaknesses (e.g., division) can persist and neuron-level circuits show partial overlap with non-arithmetic tasks.",
            "uuid": "e316.3",
            "source_info": {
                "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Attention→Late-MLP arithmetic mechanism",
            "name_full": "Mechanistic account: attention-mediated transfer of operand/operator information to last-token MLPs which compute results",
            "brief_description": "A mechanistic hypothesis, supported empirically in this paper, that Transformer LMs solve simple arithmetic by encoding operands/operators in early token representations, using attention to move that information to the final-token representation, then applying late feed-forward (MLP) layers at the final token to produce result-related signals injected into the residual stream.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies across evaluated models (GPT-J, Pythia 2.8B, LLaMA 7B, Goat)",
            "model_size": null,
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division (bi-variate); generalizes to tri-variate after fine-tuning",
            "number_range_or_complexity": "Evaluated primarily on small-to-medium integers (default S = {1..300}); some analyses use restricted ranges ({1..20} or single-digit) for tokenizer and neuron-comparison consistency.",
            "method_or_intervention": "Causal mediation analysis: intervene on attention outputs a_t^{(l)} and MLP outputs m_t^{(l)} by copying activations from one prompt/run into another, computing indirect effect (IE) as a relative change in predicted probability (and alternate log-prob metric); neuron-level interventions and result-fixed vs varying comparisons used to separate operand- vs result-related signals.",
            "performance_result": null,
            "mechanistic_insight": "Key mechanistic observations: (1) early-layer MLPs at operand/operator token positions carry operand/operator information; (2) attention heads/modules (not per-head isolated here) at mid-to-late layers transfer that information to the last token (attention IE peaks around layers ~11–18); (3) late last-token MLPs (peaking near layers ~19–20 in GPT-J) incorporate and output result-related information into the residual stream; (4) neuron-level analysis shows ~50% overlap between Arabic numerals and numeral-words top neurons, but limited overlap (≈22%) with simple number-retrieval tasks and random-level overlap (~10%) with factual prediction, indicating task-specific circuits within similar anatomical regions.",
            "performance_scaling": "Pattern observed across multiple model sizes/families; fine-tuning can induce or strengthen the mid-late last-token MLP activation site (Pythia 2.8B three-operand fine-tuning); RI metric shows late last-token MLPs contribute large fraction of IE for arithmetic (e.g., GPT-J ~40%), and that contribution drops when result is fixed, indicating specificity.",
            "failure_modes": null,
            "comparison_baseline": "Compared to number-retrieval (synthetic) and factual-prediction tasks; compared result-varying vs result-fixed prompts; neuron-level overlap comparisons across tasks; compared before/after fine-tuning for multi-operand tasks.",
            "key_finding": "Arithmetic computation in these Transformers is implemented by a small, consistent circuit: encode numbers/operators early, use attention to gather relevant inputs at the final token, and perform the core result computation in mid-late feed-forward (MLP) layers at that final token.",
            "uuid": "e316.4",
            "source_info": {
                "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "rating": 2
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 2
        },
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 2
        },
        {
            "paper_title": "Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Investigating gender bias in language models using causal mediation analysis",
            "rating": 2
        },
        {
            "paper_title": "Dissecting recall of factual associations in auto-regressive language models",
            "rating": 2
        }
    ],
    "cost": 0.016690749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis</h1>
<p>Alessandro Stolfo<br>ETH Zürich<br>stolfoa@ethz.ch</p>
<p>Yonatan Belinkov<br>Technion - IIT, Israel<br>belinkov@technion.ac.il</p>
<h2>Mrinmaya Sachan</h2>
<p>ETH Zürich<br>msachan@ethz.ch</p>
<h4>Abstract</h4>
<p>Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Mathematical reasoning with Transformer-based models (Vaswani et al., 2017) is challenging as it requires an understanding of the quantities and the mathematical concepts involved. While large language models (LMs) have recently achieved impressive performance on a set of math-based tasks (Wei et al., 2022a; Chowdhery et al., 2022; OpenAI, 2023), their behavior has been shown to be inconsistent and context-dependent (Bubeck et al., 2023). Recent literature shows a multitude</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Visualization of our findings. We trace the flow of numerical information within Transformerbased LMs: given an input query, the model processes the representations of numbers and operators with early layers (A). Then, the relevant information is conveyed by the attention mechanism to the end of the input sequence (B). Here, it is processed by late MLP modules, which output result-related information into the residual stream (C).
of works proposing methods to improve the performance of large LMs on math benchmark datasets through enhanced pre-training (Spokoyny et al., 2022; Lewkowycz et al., 2022; Liu and Low, 2023) or specific prompting techniques (Wei et al., 2022b; Kojima et al., 2022; Yang et al., 2023, inter alia). However, there is a limited understanding of the inner workings of these models and how they store and process information to correctly perform mathbased tasks. Insights into the mechanics behind LMs' reasoning are key to improvements such as inference-time correction of the model's behavior (Li et al., 2023) and safer deployment. Therefore, research in this direction is critical for the development of more faithful and accurate next-generation LM-based reasoning systems.</p>
<p>In this paper, we present a set of analyses aimed</p>
<p>at mechanistically interpreting LMs on the task of answering simple arithmetic questions (e.g., "What is the product of 11 and 17?"). In particular, we hypothesize that the computations involved in reasoning about such arithmetic problems are carried out by a specific subset of the network. Then, we test this hypothesis by adopting a causal mediation analysis framework (Vig et al., 2020; Meng et al., 2022), where the model is seen as a causal graph going from inputs to outputs, and the model components (e.g., neurons or layers) are seen as mediators (Pearl, 2001). Within this framework, we assess the impact of a mediator on the observed output behavior by conducting controlled interventions on the activations of specific subsets of the model and examining the resulting changes in the probabilities assigned to different numerical predictions.</p>
<p>Through this experimental procedure, we track the flow of information within the model and identify the model components that encode information about the result of arithmetic queries. Our findings show that the model processes the input by conveying information about the operator and the operands from mid-sequence early layers to the final token using attention. At this location, the information is processed by a set of MLP modules, which output result-related information into the residual stream (shown in Figure 1). We verify this finding for biand tri-variate arithmetic queries across four pretrained language models with different sizes: 2.8B, 6B, and 7B parameters. Finally, we compare the effect of different model components on answering arithmetic questions to two additional tasks: a synthetic task that involves retrieving a number from the prompt and answering questions related to factual knowledge. This comparison validates the specificity of the activation dynamics observed on arithmetic queries.</p>
<h2>2 Related Work</h2>
<p>Mechanistic Interpretability. The objective of mechanistic interpretability is to reverse engineer model computation into components, aiming to discover, comprehend, and validate the algorithms (called circuits in certain works) implemented by the model weights (Räuker et al., 2023). Early work in this area analyzed the activation values of single neurons when generating text using LSTMs (Karpathy et al., 2015). A multitude of studies have later focused on interpreting weights and intermediate representations in neural networks (Olah
et al., 2017, 2018, 2020; Voss et al., 2021; Goh et al., 2021) and on how information is processed by Transformer-based (Vaswani et al., 2017) language models (Geva et al., 2021, 2022, 2023; Olsson et al., 2022; Nanda et al., 2023). Although not strictly mechanistic, other recent studies have analyzed the hidden representations and behavior of inner components of large LMs (Belrose et al., 2023; Gurnee et al., 2023; Bills et al., 2023).</p>
<p>Causality-based Interpretability. Causal mediation analysis is an important tool that is used to effectively attribute the causal effect of mediators on an outcome variable (Pearl, 2001). This paradigm was applied to investigate LMs by Vig et al. (2020), who proposed a framework based on causal mediation analysis to investigate gender bias. Variants of this approach were later applied to mechanistically interpret the inner workings of pre-trained LMs on other tasks such as subject-verb agreement (Finlayson et al., 2021), natural language inference (Geiger et al., 2021), indirect object identification (Wang et al., 2022), and to study their retention of factual knowledge (Meng et al., 2022).</p>
<p>Math and Arithmetic Reasoning. A growing body of work has proposed methods to analyze the performance and robustness of large LMs on tasks involving mathematical reasoning (Pal and Baral, 2021; Piękos et al., 2021; Razeghi et al., 2022; Cobbe et al., 2021; Mishra et al., 2022). In this area, Stolfo et al. (2023) use a causally-grounded approach to quantify the robustness of large LMs. However, the proposed formulation is limited to behavioral investigation with no insights into the models' inner mechanisms. To the best of our knowledge, our study represents the first attempt to connect the area of mechanistic interpretability to the investigation of the mathematical reasoning abilities in Transformer-based LMs.</p>
<h2>3 Methodology</h2>
<h3>3.1 Background and Task</h3>
<p>We denote an autoregressive language model as $\mathcal{G}$ : $\mathcal{X} \rightarrow \mathcal{P}$. The model operates over a vocabulary $V$ and takes a token sequence $x=\left[x_{1}, \ldots, x_{T}\right] \in \mathcal{X}$, where each $x_{i} \in V . \mathcal{G}$ generates a probability distribution $\mathbb{P} \in \mathcal{P}: \mathbb{R}^{|V|} \rightarrow[0,1]$ that predicts possible next tokens following the sequence $x$. In this work, we study decoder-only Transformer-based models (Vaswani et al., 2017). Specifically, we focus on models that represent a slight variation of</p>
<p>the standard GPT paradigm, as they utilize parallel attention (Wang and Komatsuzaki, 2021) and rotary positional encodings (Su et al., 2022). The internal computation of the model's hidden states $h_{t}^{(l)}$ at position $t \in{1, \ldots, T}$ of the input sequence is carried out as follows:</p>
<p>$$
\begin{aligned}
h_{t}^{(l)} &amp; =h_{t}^{(l-1)}+a_{t}^{(l)}+m_{t}^{(l)} \
a_{t}^{(l)} &amp; =\mathrm{A}^{(l)}\left(h_{1}^{(l-1)}, \ldots, h_{t}^{(l-1)}\right) \
m_{t}^{(l)} &amp; =W_{\text {proj }}^{(l)} \sigma\left(W_{f c}^{(l)} h_{t}^{(l-1)}\right) \
&amp; =: \operatorname{MLP}^{(l)}\left(h_{t}^{(l-1)}\right)
\end{aligned}
$$</p>
<p>where at layer $l, \sigma$ is the sigmoid nonlinearity, $W_{f c}^{(l)}$ and $W_{\text {proj }}^{(l)}$ are two matrices that parameterize the multilayer perceptron (MLP) of the Transformer block and $\mathrm{A}^{(l)}$ is the attention mechanism. ${ }^{2}$</p>
<p>We consider the task of computing the result of arithmetic operations. Each arithmetic query consists of a list of operands $N=\left(n_{1}, n_{2}, \ldots\right)$ and a function $f_{O}$ representing the application of a set of arithmetic operators $(+,-, \times, \div)$. We denote as $r=f_{O}(N)$ the result obtained by applying the operators to the operands. Each query is rendered as a natural language question through a prompt $p\left(N, f_{O}\right) \in \mathcal{X}$ such as "How much is $n_{1}$ plus $n_{2}$ ?" (in this case, $f_{O}\left(n_{1}, n_{2}\right)=n_{1}+n_{2}$ ). The prompt is then fed to the language model to produce a probability distribution $\mathbb{P}$ over $V$. Our aim is to investigate whether certain hidden state variables are more important than others during the process of computing the result $r$.</p>
<h3>3.2 Experimental Procedure</h3>
<p>We see the model $\mathcal{G}$ as a causal graph (Pearl, 2009), framing internal model components, such as specific neurons, as mediators positioned along the causal path connecting model inputs and outputs. Following a causal mediation analysis procedure, we then quantify the contribution of particular model components by intervening on their activation values and measuring the change in the model's output. Previous work has isolated the effect of every single neuron within a model (Vig et al., 2020; Finlayson et al., 2021). However, this approach becomes impractical for models with billions of parameters. Therefore, for our main experiments, the elements that we consider as variables along the causal path described by the model are</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: By intervening on the activation values of specific components within a language model and computing the corresponding effects, we identify the subset of parameters responsible for specific predictions.
the outputs of the $\operatorname{MLP}^{(l)}$ and $\mathrm{A}^{(l)}$ functions at each token $t$, i.e., $m_{t}^{(l)}$ and $a_{t}^{(l)}$.</p>
<p>To quantify the importance of modules $\operatorname{MLP}^{(l)}$ and $\mathrm{A}^{(l)}$ in mediating the model's predictions at position $t$, we use the following procedure.</p>
<ol>
<li>Given $f_{O}$, we sample two sets of operands $N, N^{\prime}$, and we obtain $r=f_{O}(N)$ and $r^{\prime}=$ $f_{O}\left(N^{\prime}\right)$. Then, two input questions with only the operands differing, $p_{1}=p\left(N, f_{O}\right)$ and $p_{2}=p\left(N^{\prime}, f_{O}\right)$, are passed through the model.</li>
<li>During the forward pass with input $p_{1}$, we store the activation values $\bar{m}<em t="t">{t}^{(l)}:=\operatorname{MLP}^{(l)}\left(h</em>}^{(l-1)}\right)$, and $\bar{a<em 1="1">{t}^{(l)}:=\mathrm{A}^{(l)}\left(h</em>\right)$.}^{(l-1)}, \ldots, h_{t}^{(l-1)</li>
<li>We perform an additional forward pass using $p_{2}$, but this time we intervene on components $\operatorname{MLP}^{(l)}$ and $\mathrm{A}^{(l)}$ at position $t$, setting their activation values to $\bar{m}<em t="t">{t}^{(l)}$, and $\bar{a}</em>$, respectively. This process is illustrated in Figure 2.}^{(l)</li>
<li>We measure the causal effect of the intervention on variables $m_{t}^{(l)}$ and $a_{t}^{(l)}$ on the model's prediction by computing the change in the probability values assigned to the results $r$ and $r^{\prime}$.</li>
</ol>
<p>More specifically, we compute the indirect effect (IE) of a specific mediating component by quantifying its contribution in skewing $\mathbb{P}$ towards the correct result. Consider a generic activation variable $z \in\left{m_{1}^{(1)}, \ldots, m_{t}^{(L)}, a_{1}^{(1)}, \ldots, a_{t}^{(L)}\right}$. We denote the model's output probability following an intervention on $z$ as $\mathbb{P}<em z="z">{z}^{<em>}$. Then, we compute the IE as:
$\operatorname{IE}(z)=\frac{1}{2}\left[\frac{\mathbb{P}_{z}^{</em>}(r)-\mathbb{P}(r)}{\mathbb{P}(r)}+\frac{\mathbb{P}\left(r^{\prime}\right)-\mathbb{P}</em>^{<em>}\left(r^{\prime}\right)}{\mathbb{P}_{z}^{</em>}\left(r^{\prime}\right)}\right]$
where the two terms in the sum represent the relative change in the probability assigned by the model to $r$ and $r^{\prime}$, caused by the intervention performed. The larger the measured IE, the larger the contribution of component $z$ in shifting probability mass from the clean-run result $r^{\prime}$ to result $r$ corresponding to the alternative input $p_{1} .^{3}$</p>
<p>We additionally measure the mediation effect of each component with respect to the operation $f_{O}$. We achieve this by fixing the operands and changing the operator across the two input prompts. More formally, in step 1, we sample a list of operands $N$ and two operators $f_{O}$ and $f_{O}^{\prime}$. Then, we generate two prompts $p_{1}=p\left(N, f_{O}\right)$ and $p_{2}=p\left(N, f_{O}^{\prime}\right)$ (e.g., "What is the sum of 11 and 7?" and "What is the product of 11 and 7?"). Finally, we carry out the procedure in steps $2-4$.</p>
<h3>3.3 Experimental Setup</h3>
<p>We present the results of our analyses in the main paper for GPT-J (Wang and Komatsuzaki, 2021), a 6B-parameter pre-trained LM (Gao et al., 2020). Additionally, we validate our findings on Pythia 2.8B (Biderman et al., 2023), LLaMA 7B (Touvron et al., 2023), and Goat, a version of LLaMA finetuned on arithmetic tasks (Liu and Low, 2023). We report the detailed results for these models in Appendix J.</p>
<p>In our experiments, we focus on two- and threeoperand arithmetic problems. Similar to previous work (Razeghi et al., 2022; Karpas et al., 2022), for single-operator two-operand queries, we use a set of six diverse templates representing a question involving each of the four arithmetic operators. For the three-operand queries, we use one template for each of the 29 possible two-operator combinations. Details about the templates are reported in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Appendix A. In the bi-variate case, for each of the four operators $f_{O} \in{+,-, \times, \div}$ and for each of the templates, we generate 50 pairs of prompts by sampling two pairs of operands $\left(n_{1}, n_{2}\right) \in \mathcal{S}^{2}$ and $\left(n_{1}^{\prime}, n_{2}^{\prime}\right) \in \mathcal{S}^{2}$, where $\mathcal{S} \subset V \cap \mathbb{N}$. For the operandrelated experiment, we sample $\left(n_{1}, n_{2}\right)$ and a second operation $f_{O}^{\prime}$. In both cases, we ensure that the result $r$ falls within $\mathcal{S} .{ }^{4}$ In the three-operand case, we generate 15 pairs of prompts for each of the 29 templates, following the same procedure. In order to ensure that the model achieves a meaningful task performance, we use a two-shot prompt in which we include two exemplars of question-answer for the same operation that is being queried. We report the accuracy results in Appendix B.</p>
<h2>4 Causal Effects on Arithmetic Queries</h2>
<p>Our analyses address the following question:
Q1 What are the components of the model that mediate predictions involving arithmetic computations?</p>
<p>We address this question by first studying the flow of information throughout the model by measuring the effect of each component (MLP and attention block) at each point of the input sequence for two-operand queries (§4.1). Then, we distinguish between model components that carry information about the result and about the operands of the arithmetic computations (§4.2 and §4.3). Finally, we consider queries involving three operands (§4.4) and present a measure to quantify the changes in information flow (§4.5).</p>
<h3>4.1 Tracing the Information Flow</h3>
<p>We measure the indirect effect of each MLP and attention block at different positions along the input sequence. The output of these modules can be seen as new information being incorporated into the residual stream. This new information can be produced at any point of the sequence and then conveyed to the end of the sequence for the prediction of the next token. By studying the IE at different locations within the model, we can identify the modules that generate new information relevant to the model's prediction. The results are reported in Figures 3a and 3b for MLP and attention, respectively.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Indirect effect (IE) measured within GPT-J. Figures (a) and (b) illustrate the flow of information related to both the operands and the result of the queries, while the effect displayed in Figures (c) and (d) is related to the operands only (the result is kept unchanged). Figures (e–h) show a re-scaled visualization of the effects at the last token for each of the four heatmaps (a–d). The difference in the effect registered for the MLPs at layers 15–25 between figures (a) and (c) illustrates the role of these components in producing result-related information.</p>
<p>Our analysis reveals four primary activation sites: the MLP module situated at the first layer corresponding to the tokens of the two operands; the intermediate attention blocks at the last token of the sequence; and the MLP modules in the middle-to-late layers, also located at the last token of the sequence. It is expected to observe a high effect for the first MLPs associated with the tokens that vary (i.e., the operands), as such modules are likely to affect the representation of the tokens, which is subsequently used for the next token prediction. On the other hand, of particular interest is the high effect detected at the attention modules in layers 11–18 and in the MLPs around layer 20.</p>
<p>As for the flow of information tied to the operator, the activations display a parallel pattern: high effect is registered at early MLPs associated with the operator tokens and at the same last-token MLP and attention locations. We report the visualization of the operator-related results in Appendix C.</p>
<p>A possible explanation of the model's behavior on this task is that the attention mechanism facilitates the propagation of operand- and operator-related information from the first layers early in the sequence to the last token. Here, this information is processed by the MLP modules, which incorporate the information about the result of the computation in the residual stream. This hypothesis aligns with the existing theory that attributes the responsibility of moving and extracting information within Transformer-based models to the attention mechanism (Elhage et al., 2021; Geva et al., 2023), while the feed-forward layers are associated with performing computations, retrieving facts and information (Geva et al., 2022; Din et al., 2023; Meng et al., 2022). We test the validity of this hypothesis in the following section.</p>
<h3>4.2 Operand- and Result-related Effects</h3>
<p>Our objective is to verify whether the contribution to the model's prediction of each component measured in Figures 3a and 3b is due to (1) the component representing information related to the operands, or (2) the component encoding information about the result of the computation. To this end, we formulate a variant of our previous experimental procedure. In particular, we condition the sampling of the second pair of operands (<em>n</em>′₁, <em>n</em>′₃) on the constraint <em>r</em> = <em>r</em>′. That is, we generate the two input questions <em>p</em>₁ and <em>p</em>₂, such that their result is the same (e.g., "<em>What is the sum of 25 and 7?</em>" and "<em>What is the sum of 14 and 18?</em>"). In case number (1), we would expect a component to have high IE both in the result-varying setting and when <em>r</em> = <em>r</em>′, as the operands are modified in both scenarios. In case (2), we expect a subset of the model to have a large effect when the operands are sampled without constraints but a low effect for the fixed-result setting.</p>
<p>We report the results in Figure 3c and 3d. By</p>
<p>comparing Figures 3a and 3c, two notable observations emerge. First, the high effect in the early layers corresponding to the operand tokens is observed in both the result-preserving and the resultvarying scenarios. Second, the last-token mid-late MLPs that lead to a high effect on the model's prediction following a result change, dramatically decrease their effect on the model's output in the result-preserving setting, as described in scenario (2). These observations point to the conclusion that the MLP blocks around layer 20 incorporate resultrelevant information. As for the contribution of the attention mechanism (Figures 3b and 3d), we do not observe a substantial difference in the layers with the highest IE between the two settings, which aligns this scenario to the description of case (1). These results are consistent with our hypothesis that operand-related information is transferred by the attention mechanism to the end of the sequence and then processed by the MLPs to obtain the result of the computation.</p>
<h3>4.3 Zooming in on the Last Token</h3>
<p>In Figures 3e-3h, we show a re-scaled version of the IE measurements for the layers at the end of the input sequence. While the large difference in magnitude was already evident in the previously considered visualizations, in Figures 3e and 3f we notice that the MLPs with the highest effect in the two settings differ: the main contribution to the model's output when the results are not fixed is given by layers 19 and 20, while in the resultpreserving setting the effect is largest at layers 1418. For the attention (Figures 3 g and 3h), we do not observe a significant change in the shape of the curve describing the IE across different layers, with layer 13 producing the largest contribution. We interpret this as additional evidence indicating that the last-token MLPs at layers 19-20 encode information about $r$, while the attention modules carry information related to the operands.</p>
<h3>4.4 Three-operand Queries \&amp; Fine-tuning</h3>
<p>We extend our analyses by including three-operand arithmetic queries such as "What is the difference between $n_{1}$ and the ratio between $n_{2}$ and $n_{3}$ ?". Answering correctly this type of questions represents a challenging task for pre-trained language models, and we observe poor accuracy (below 10\%) with GPT-J. Thus, we opt for fine-tuning the model on a small set of three-operand queries. The model that we consider for this analysis is Pythia 2.8B,
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Indirect effect (IE) on three-operand queries for different MLP modules in Pythia 2.8B before and after fine-tuning. The effect produced by the last-token mid-late MLP activation site emerges with fine-tuning. Results for the attention are reported in Appendix J.
as its smaller size allows for less computationally demanding training than the 6B-parameter GPT-J. After fine-tuning, the model attains an accuracy of $\sim 40 \%$. We provide the details about the training procedure in Appendix F.</p>
<p>We carry out the experimental procedure as in Section 4.1. In particular, we compare the information flow in the MLPs of the model before and after fine-tuning (Figure 4). In the non-fine-tuned version of the model, the only relevant activation site, besides the early layers at the operand tokens, is the very last layer at the last token. In the fine-tuned model, on the other hand, we notice the emergence of the mid-late MLP activation site that was previously observed in the two-operand setting.</p>
<h3>4.5 Quantifying the Change of the Information Flow</h3>
<p>Denote the set of MLPs in the model by $\mathcal{M}$. We define the relative importance (RI) of a specific subset $\mathcal{M}^{*} \subseteq \mathcal{M}$ of MLP modules as</p>
<p>$$
\operatorname{RI}\left(\mathcal{M}^{<em>}\right)=\frac{\sum_{m \in \mathcal{M}^{</em>}} \log (\operatorname{IE}(m)+1)}{\sum_{m \in \mathcal{M}} \log (\operatorname{IE}(m)+1)}
$$</p>
<p>In order to quantitatively show the difference in the activation sites observed in Figure 3, we compute the RI measure for the set</p>
<p>$$
\mathcal{M}<em -1="-1">{-1}^{\text {late }}=\left{m</em>\right}
$$}^{([L / 2])}, m_{-1}^{([L / 2]+1)}, \ldots, m_{-1}^{(L)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$|\boldsymbol{N}|$</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\mathbf{R I}\left(\mathcal{M}_{-1}^{\text {late }}\right)$</th>
<th style="text-align: center;">$\underset{\text { Result Fixed }}{\mathbf{R I}\left(\mathcal{M}_{-1}^{\text {late }}\right)}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">40.2\%</td>
<td style="text-align: center;">4.4\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pythia 2.8B</td>
<td style="text-align: center;">43.2\%</td>
<td style="text-align: center;">5.8\%</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">LLaMA 7B</td>
<td style="text-align: center;">36.1\%</td>
<td style="text-align: center;">7.5\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Goat</td>
<td style="text-align: center;">33.5\%</td>
<td style="text-align: center;">7.4\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-J (Words)</td>
<td style="text-align: center;">27.8\%</td>
<td style="text-align: center;">4.5\%</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Pythia 2.8B</td>
<td style="text-align: center;">13.5\%</td>
<td style="text-align: center;">6.7\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pythia 2.8B (FT)</td>
<td style="text-align: center;">24.7\%</td>
<td style="text-align: center;">13.6\%</td>
</tr>
</tbody>
</table>
<p>Table 1: Relative importance (RI) measurements for the last-token late MLP activation site. The decrease in the RI observed when fixing the result of the two pairs of operands used for the interventions quantitatively confirms the role of this subset of the model in incorporating result-related information.
where the subscript -1 indicates the last token of the input sequence and $L$ is the number of layers in the model. This quantity represents the relative contribution of the mid-late last-token MLPs compared to all the MLP blocks in the model.</p>
<p>For the two-operand setting, we carry out the experimental procedure described in Section 3 for three additional models: Pythia 2.8B, LLaMA 7B, and Goat. ${ }^{5}$ Furthermore, we repeat the analyses on GPT-J using a different number representation: instead of Arabic numbers (e.g., the token 2), we represent quantities using numeral words (e.g., the token two). For the three-operand setting, we report the results for Pythia 2.8B before and after fine-tuning. We measure the effects using both randomly sampled and result-preserving operand pairs, comparing the RI measure in the two settings. The results (Table 1) exhibit consistency across all these four additional experiments. These quantitative measurements further highlight the influence of last-token late MLP modules on the prediction of $r$. We provide in Appendix J the heatmap illustrations of the effects for these additional studies.</p>
<h2>5 Causal Effects on Different Tasks</h2>
<p>In order to understand whether the patterns in the effect of the model components that we observed so far are specific to arithmetic queries, we compare our observations on arithmetic queries to two different tasks: the retrieval of a number from the prompt (§5.1), and the prediction of factual knowl-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Indirect effect measured on the MLPs of GPTJ for predictions on the number retrieval task.
edge (§5.2). With this additional set of experiments, we aim to answer the question:</p>
<p>Q2 Are the activation patterns observed so far specific to the arithmetic setting?</p>
<h3>5.1 Information Flow on Number Retrieval</h3>
<p>We consider a simple synthetic task involving numerical predictions. We construct a set of templates of the form "Paul has $n_{1} e_{1}$ and $n_{2} e_{2}$. How many $e_{q}$ does Paul have?", where $n_{1}, n_{2}$ are two randomly sampled numbers, $e_{1}$ and $e_{2}$ are two entity names sampled at random, ${ }^{6}$ and $e_{q} \in\left{e_{1}, e_{2}\right}$. In this case, the two input prompts $p_{1}$ and $p_{2}$ differ solely in the value of $e_{q}$. To provide the correct answer to a query, the model has simply to retrieve the correct number from the prompt. With this task, we aim to analyze the model's behavior in a setting involving numerical predictions but not requiring any kind of arithmetic computation.</p>
<p>We report the indirect effect measured for the MLPs modules of GPT-J in Figure 5. In this setting, we observe an unsurprising high-effect activation site corresponding to the tokens of the entity $e_{q}$ and a lower-effect site at the end of the input in layers 14-20. The latter site appears in the set of the model components that were shown to be active on arithmetic queries. However, computing the relative importance of the late MLPs on this task shows that this second activation site is responsible for only $\operatorname{RI}\left(\mathcal{M}_{-1}^{\text {late }}\right)=8.7 \%$ of the overall $\log \mathrm{IE}$. The low RI, compared to the higher values measured on arithmetic queries, suggests that the function of the last-token late MLPs is not dictated by the numerical type of prediction, but rather by their involvement in processing the input information. This finding is aligned with our theory that</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Indirect effect measured on the MLPs of GPT-J for predictions to factual queries.</p>
<p>sees $\mathcal{M}_{-1}^{\text{late}}$ as the location where information about $r$ is included in the residual stream.</p>
<h3>5.2 Information Flow on Factual Predictions</h3>
<p>We carry out our experimental procedure using data from the LAMA benchmark (Petroni et al., 2019), which consists of natural language templates representing knowledge-base relations, such as "[subject] is the capital of [object]". By instantiating a template with a specific subject (e.g., "Paris"), we prompt the model to predict the correct object ("France"). Similar to our approach with arithmetic questions, we create pairs of factual queries that differ solely in the subject. In particular, we sample pairs of entities from the set of entities compatible for a given relation (e.g., cities for the relation "is the capital of"). Details about the data used for this procedure are provided in Appendix H. We then measure the indirect effect following the formulation in Equation 2, where the correct object corresponds to the correct numerical outcome in the arithmetic scenario.</p>
<p>From the results (Figure 6), we notice that a main activation site emerges in early layers at the tokens corresponding to the subject of the query. These findings are consistent with previous works (Meng et al., 2022; Geva et al., 2023), which hypothesize that language models store and retrieve factual associations in early MLPs located at the subject tokens. We compute the RI metric for the late MLP modules, which quantitatively validates the contribution of the early MLP activation site by attaining a low value of RI$(\mathcal{M}_{-1}^{\text{late}})=4.2\%$. The large IE observed at mid-sequence early MLPs represents a difference in the information flow with respect to the arithmetic scenario, where the modules with the highest influence on the model's prediction are located at the end of the sequence. This difference serves as additional evidence highlighting the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Overlap ratio in the top 400 neurons with the largest effect on predicting answers to factual queries involving Arabic Numerals (Ar) and numeral words (W), number retrieval (NR), and factual knowledge (F). The results are obtained with GPT-J.</p>
<p>specificity of the model's activation patterns when answering arithmetic queries.</p>
<h3>5.3 Neuron-level Interventions</h3>
<p>The experimental results in Sections 5.1 and 5.2 showed a quantitative difference in the contributions of last-token mid-late MLPs between arithmetic queries and two tasks that do not involve arithmetic computation. Now, we investigate whether the components active within $\mathcal{M}_{-1}^{\text{late}}$ on the different types of tasks are different. We carry out a finer-grained analysis in which we consider independently each neuron in an MLP module (i.e., each dimension in the output vector of the function MLP$^{(l)}$) at a specific layer $l$. In particular, following the same procedure as for layer-level experiments, we intervene on each neuron by setting its activation to the value it would take if the input query contained different operands (or a different entity). We then compute the corresponding indirect effect as in Eq. 2. We carry out this procedure for arithmetic queries using Arabic numerals (Ar) and numeral words (W), for the number retrieval task (NR), and for factual knowledge queries (F).<sup>7</sup> We rank the neurons according to the average effect measured for each of these four settings and compute the overlap in the top 400 neurons (roughly 10%, as GPT-J has a hidden dimension of 4096).</p>
<p>We carry out this procedure for layer $l=19$, as it exhibits the largest IE within $\mathcal{M}_{-1}^{\text{late}}$ on all the tasks considered. The heatmap in Figure 7 illustrates the results. We observe a consistent overlap (50%) between the top neurons active for the arithmetic queries using Arabic and word-based representa-</p>
<p><sup>7</sup>To have the same result space for all the arithmetic queries (Ar and NW) and for the number retrieval task, we restrict the set $\mathcal{S}$ to {1, . . . , 20} (or the corresponding numeral words).</p>
<p>tions. Interestingly, the size of the neuron overlap between arithmetic queries and number retrieval is considerably lower ( $22 \%$ and $23 \%$ ), even though both tasks involve the prediction of numerical quantities. Finally, the overlaps between the top neurons for the arithmetic operations and the factual predictions (between $9 \%$ and $10 \%$ ) are not larger than for random rankings: the expected overlap ratio between the top 400 indices in two random rankings of size 4096 is $9.8 \%$ (Antverg and Belinkov, 2022). These results support the hypothesis that the model's circuits responsible for different kinds of prediction, though possibly relying on similar subsets of layers, are distinct. However, it is important to note that this measurement does not take into account the magnitude of the effect.</p>
<h2>6 Conclusion</h2>
<p>We proposed the use of causal mediation analysis to mechanistically investigate how LMs process information related to arithmetic. Through controlled interventions on specific subsets of the model, we assessed the impact of these mediators on the model's predictions.</p>
<p>We posited that models produce predictions to arithmetic queries by conveying the math-relevant information from the mid-sequence early layers to the last token, where this information is then processed by late MLP modules. We carried out a causality-grounded experimental procedure on four different Transformer-based LMs, and we provided empirical evidence supporting our hypothesis. Furthermore, we showed that the information flow we observed in our experiments is specific to arithmetic queries, compared to two other tasks that do not involve arithmetic computation.</p>
<p>Our findings suggest potential avenues for research into model pruning and more targeted training/fine-tuning by concentrating on specific model components associated with certain queries or computations. Moreover, our results offer insights that may guide further studies into using LMs' hidden representations to correct the model's behavior on math-based tasks at inference time ( Li et al., 2023) and to estimate the probability of the model's predictions to be true (Burns et al., 2023).</p>
<h2>Limitations</h2>
<p>The scope of our work is investigating arithmetic reasoning and we experiment with the four fundamental arithmetic operators. Addition, subtraction,
multiplication, and division form the cornerstone of arithmetic calculations and serve as the basis for a wide range of mathematical computations. Thus, exploring their mechanisms in language models provides a starting point to explore more complex forms of mathematical processing. Studying a broader set of mathematical operators represents an interesting avenue for further investigation.</p>
<p>Our work focuses on synthetically-generated queries that are derived from natural language descriptions of the four basic arithmetic operators. To broaden the scope, future research can expand the analysis of model activations to encompass mathbased queries described in real-life settings, such as math word problems.</p>
<p>Finally, a limitation of our work concerns the analysis of different attention heads. In our experiments, we consider the output of an attention module as a whole. Future research could focus on identifying the specific heads that are responsible for forwarding particular types of information in order to offer a more detailed understanding of their individual contributions.</p>
<h2>Acknowledgments</h2>
<p>AS is supported by armasuisse Science and Technology through a CYD Doctoral Fellowship. YB is supported by an AI Alignment grant from Open Philanthropy, the Israel Science Foundation (grant No. 448/20), and an Azrieli Foundation Early Career Faculty Fellowship. MS acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung, and an ETH Grant (ETH-19 21-1). We are grateful to Vilém Zouhar and Neel Nanda for the insightful discussions.</p>
<h2>References</h2>
<p>Omer Antverg and Yonatan Belinkov. 2022. On the pitfalls of analyzing individual neurons in language models. In International Conference on Learning Representations.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language models can explain neurons in language models. https://openaipublic.biob.core.windows. net/neuron-explainer/paper/index.html.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2023. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems.</p>
<p>Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. 2023. Jump to conclusions: ShortCutting transformers with linear transformations. arXiv preprint arXiv:2303.09435.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread.</p>
<p>Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart Shieber, Tal Linzen, and Yonatan Belinkov. 2021. Causal analysis of syntactic agreement mechanisms in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on</p>
<p>Natural Language Processing (Volume 1: Long Papers), pages 1828-1843, Online. Association for Computational Linguistics.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.</p>
<p>Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. 2021. Causal abstractions of neural networks. Advances in Neural Information Processing Systems, 34:9574-9586.</p>
<p>Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767.</p>
<p>Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30-45, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are keyvalue memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484-5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. 2021. Multimodal neurons in artificial neural networks. Distill, 6(3):e30.</p>
<p>Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in a haystack: Case studies with sparse probing. arXiv preprint arXiv:2305.01610.</p>
<p>Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. 2022. MRKL systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445.</p>
<p>Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213. Curran Associates, Inc.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models.</p>
<p>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inference-time intervention: Eliciting truthful answers from a language model.</p>
<p>Tiedong Liu and Bryan Kian Hsiang Low. 2023. Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks. arXiv preprint arXiv:2305.14201.</p>
<p>Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35:17359-17372.</p>
<p>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. 2022. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3505-3523, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. 2023. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217.</p>
<p>Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. Zoom in: An introduction to circuits. Distill. Https://distill.pub/2020/circuits/zoom-in.</p>
<p>Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. 2017. Feature visualization. Distill, 2(11):e7.</p>
<p>Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. 2018. The building blocks of interpretability. Distill, 3(3):e10.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learning and induction heads. Transformer Circuits Thread.</p>
<p>OpenAI. 2023. GPT-4 technical report.
Kuntal Kumar Pal and Chitta Baral. 2021. Investigating numeracy learning ability of a text-to-text transfer model. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3095-3101, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Judea Pearl. 2001. Direct and indirect effects. In UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, pages 411-420. Morgan Kaufmann.</p>
<p>Judea Pearl. 2009. Causality. Cambridge University Press.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 383-394, Online. Association for Computational Linguistics.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot numerical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 840-854, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. 2023. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR.</p>
<p>Daniel Spokoyny, Ivan Lee, Zhao Jin, and Taylor BergKirkpatrick. 2022. Masked measurement prediction: Learning to jointly predict quantities and units from textual context. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1729, Seattle, United States. Association for Computational Linguistics.</p>
<p>Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, and Mrinmaya Sachan. 2023. A causal framework to quantify the robustness of mathematical reasoning with language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 545-561, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. Roformer: Enhanced transformer with rotary position embedding.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. Advances in Neural Information Processing Systems, 33:1238812401 .</p>
<p>Chelsea Voss, Nick Cammarata, Gabriel Goh, Michael Petrov, Ludwig Schubert, Ben Egan, Swee Kiat Lim, and Chris Olah. 2021. Visualizing weights. Distill, 6(2):e00024-007.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 billion parameter autoregressive language model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022. Interpretability in the wild: A circuit for indirect object identification in GPT-2 small. In NeurIPS ML Safety Workshop.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903.</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. arXiv preprint arXiv:2309.03409.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Indirect effect (IE) measured in GPT-J when varying the word describing the operator involved in the input query. Similar to the operand case, we observe a high contribution produced by middle-to-late MLP modules at the end of the input sequence.</p>
<h3>A Prompt Templates</h3>
<p>In Tables 2 and 3, we report the question templates from Karpas et al. (2022), which we used as prompts for the model for two- and three-operand queries, respectively. For three-operand queries, we use one query template for each of the 29 possible two-operation combinations.</p>
<h3>B Performance of the Models</h3>
<p>In Table 4, we report the accuracy of the models on the arithmetic queries that we use for our analyses. The higher accuracy obtained using numeral words is likely given by the smaller set of possible solutions considered (we used $$S = {"one"", "two", \ldots, "twenty"}$$, as the numeral words corresponding to larger numbers get split into multiple tokens by the tokenizer). The accuracy of GPT-J on the factual queries from the LAMA benchmark is 65.0% (we constrain the vocabulary to the set of all possible objects for all the relations considered). On the synthetic number retrieval task, GPT-J's accuracy is 86.7%.</p>
<h3>C Flow of Operator-related Information</h3>
<p>The measurements of the indirect effect for each model component when fixing the operand and varying the operator in the two input prompts $$p_1$$ and $$p_2$$ reveal how the model processes the information related to the operator. We report in Figure 8</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Indirect effect of the MLPs at the last token in each layer in GPT-J, for each of the four arithmetic operators. We observe a peak in the effect at layer 19 for all four types of operation.</p>
<p>the heatmap visualizations of these results for two-operand queries. Similar to the operand-related information, we observe a high effect in three activation locations: early MLP blocks corresponding to the operand tokens; middle-to-early attention modules at the last token; and middle-to-late MLP modules at the last token. These results align with our hypothesis that arithmetic-related information is transferred to the end of the sequence by the attention mechanism, where it is then processed by late MLP layers. In this setting, we measure $$RI(\mathcal{M}^{\text{late}} - 1) = 31.4\%.$$</p>
<h3>D Effects for Each Operator</h3>
<p>For each of the four operators, we report the indirect effect measured for the last-token MLP modules in GPT-J in Figure 9. The results for each of the four operators show a common spike in the effect at layers 19-20. This indicates the presence of a specific part of the model relevant to the numerical predictions of the bi-variate arithmetic questions, irrespective of the operator involved. We also notice a difference in the magnitude of the effects, which is linked to the capability of the model to correctly answer the query.</p>
<h3>E Changes in the Model's Prediction</h3>
<p>We measured the influence of the model components in terms of probability changes. Now, we study the dynamics of the actual model predictions. In particular, considering the scenario in which $$r = r'$$, we verify whether the intervention leads to a change in the model's prediction. That is, we</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">addition</th>
<th style="text-align: center;">subtraction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Q: How much is $n_{1}$ plus $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: How much is $n_{1}$ minus $n_{2}$ ? A:</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: What is $n_{1}$ plus $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: What is $n_{1}$ minus $n_{2}$ ? A:</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Q: What is the result of $n_{1}$ plus $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: What is the result of $n_{1}$ minus $n_{2}$ ?</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Q: What is the sum of $n_{1}$ and $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: What is the difference between A: $n_{1}$ and $n_{2}$ ? A:</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">The sum of $n_{1}$ and $n_{2}$ is</td>
<td style="text-align: center;">The difference between $n_{1}$ and $n_{2}$ is</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$n_{1}+n_{2}=$</td>
<td style="text-align: center;">$n_{1}-n_{2}=$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">multiplication</td>
<td style="text-align: center;">division</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Q: How much is $n_{1}$ times $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: How much is $n_{1}$ over $n_{2}$ ? A:</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Q: What is $n_{1}$ times $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: What is $n_{1}$ over $n_{2}$ ? A:</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Q: What is the result of $n_{1}$ times $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: What is the result of $n_{1}$ over $n_{2}$ ? A:</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Q: What is the product of $n_{1}$ and $n_{2}$ ? A:</td>
<td style="text-align: center;">Q: What is the ratio between $n_{1}$ and $n_{2}$ ? A:</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">The product of $n_{1}$ and $n_{2}$ is</td>
<td style="text-align: center;">The ratio of $n_{1}$ and $n_{2}$ is</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$n_{1} * n_{2}=$</td>
<td style="text-align: center;">$n_{1} / n_{2}=$</td>
</tr>
</tbody>
</table>
<p>Table 2: Question templates for two-operand arithmetic queries.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Formula</th>
<th style="text-align: center;">Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$(n_{1}+n_{2}) * n_{3}$</td>
<td style="text-align: center;">Sum $n_{1}$ and $n_{2}$ and multiply by $n_{3}$</td>
</tr>
<tr>
<td style="text-align: left;">$n_{1}+n_{2} * n_{3}$</td>
<td style="text-align: center;">What is the sum of $n_{1}$ and the product of $n_{2}$ and $n_{3}$ ?</td>
</tr>
<tr>
<td style="text-align: left;">$\left(n_{1}-n_{2}\right) * n_{3}$</td>
<td style="text-align: center;">What is the product of $n_{1}$ minus $n_{2}$ and $n_{3}$ ?</td>
</tr>
<tr>
<td style="text-align: left;">$n_{1} /\left(n_{2} / n_{3}\right)$</td>
<td style="text-align: center;">How much is $n_{1}$ divided by the ratio between $n_{2}$ and $n_{3}$ ?</td>
</tr>
<tr>
<td style="text-align: left;">$n_{1}-n_{2} * n_{3}$</td>
<td style="text-align: center;">What is the difference between $n_{1}$ and the product of $n_{2}$ and $n_{3}$ ?</td>
</tr>
<tr>
<td style="text-align: left;">$n_{1} *\left(n_{2}-n_{3}\right)$</td>
<td style="text-align: center;">How much is $n_{1}$ times the difference between $n_{2}$ and $n_{3}$ ?</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of templates of three-operand queries. For the full list, we refer to Karpas et al. (2022).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Operation</th>
<th style="text-align: center;">Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">69.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">82.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\div$</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">67.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J (Numeral Words)</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">95.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">86.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">83.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\div$</td>
<td style="text-align: center;">59.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">81.3</td>
</tr>
<tr>
<td style="text-align: center;">Pythia 2.8B</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">57.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">64.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\div$</td>
<td style="text-align: center;">40.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">59.9</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\div$</td>
<td style="text-align: center;">88.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">97.2</td>
</tr>
<tr>
<td style="text-align: center;">Goat</td>
<td style="text-align: center;">$+$</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">91.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\div$</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: center;">Pythia 2.8B (3 Operands)</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">Pythia 2.8B Fine-tuned <br> (3 Operands)</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">39.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy of the models analyzed in the paper on various types of arithmetic queries.
compute</p>
<p>$$
\mathbb{1}\left{\underset{x \in \mathcal{S}}{\arg \max } \mathbb{P}_{z}^{*}(x) \neq \underset{x \in \mathcal{S}}{\arg \max } \mathbb{P}(x)\right}
$$</p>
<p>distinguishing between desired $\left(\arg \max <em _="*">{x \in \mathcal{S}} \mathbb{P}</em>(x)=r\right)$ changes. The results reported in Figure 10 show an increase in the desired change in prediction at layers 19-20, while the undesired change in prediction is higher for layers 14-17. This means that interventions on the MLPs at layers 19-20 are more likely to lead to a correct adjustment of the prediction, while the opposite is true for earlier layers (14-15 in particular). This finding is consistent with our previous observations and we see this as additional evidence highlighting the influence of the MLPs at layers 19-20 on the prediction of $r$.}(x)=r\right)$ and undesired $\left(\arg \max _{x \in \mathcal{S}} \mathbb{P</p>
<h2>F Fine-tuning Details</h2>
<p>We fine-tune Pythia 2.8B on three-operand queries. We train the model for 2 epochs on a set of queries obtained by sampling 1000 triples of operands for each of the 29 templates. We use Adafactor (Shazeer and Stern, 2018) a learning rate of $10^{-5}$, linearly decaying, and a batch size of 8 . We make sure that there is no overlap between the set of</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Desired (wrong to correct) and undesired (correct to wrong) change in the prediction induced by the intervention on the MLPs in GPT-J. The layers at which the two types of prediction change peak correspond to the layers with the largest corresponding IE.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Indirect effect (IE) measured for the attention modules in GPT-J on the task of number retrieval.
queries used for training and the set used for the computation of the indirect effect.</p>
<h2>G Computing Infrastructure</h2>
<p>The experiments for all models are carried out using a single Nvidia A100 GPU with 80GB of memory. The computation of the indirect effect across the whole model for a single type of component (attention or MLP) took $\sim 15$ hours for GPT-J and $\sim 6$ hours for Pythia (using 50 examples for each two-operand template) and $\sim 7$ hours for LLaMA and Goat (using 20 examples for each two-operand template). For the fine-tuning of Pythia 2.8B, we used a single Nvidia A100 GPU with 80GB of memory. The training procedure took $\sim 1$ hour. Experiment tracking was carried out using Weights \&amp; Biases. ${ }^{8}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Indirect effect (IE) measured for the attention modules in GPT-J on factual knowledge queries.</p>
<h2>H Factual Knowledge Data</h2>
<p>For the experiments involving the prediction of factual knowledge, we use the following six relations from the T-REx subset of the LAMA benchmark (Petroni et al., 2019):</p>
<ol>
<li>"[subject] is the capital of [object]"</li>
<li>"[subject] was born in [object]"</li>
<li>"[subject] died in [object]"</li>
<li>"The native language of [subject] is [object]"</li>
<li>"[subject] is a subclass of [object]"</li>
<li>"The capital of [subject] is [object]".</li>
</ol>
<p>We sample pairs of subject entities from the set of entities compatible for a given relation (e.g., cities for the relation "is the capital of"). For each relation, we sample 100 pairs of subject entities.</p>
<h2>I Log Probability to Quantify the IE</h2>
<p>In order to validate whether the measurements of the indirect effect are specific to the metric that we describe in Equation 2, we quantify the IE using the absolute difference in the log of the probability values assigned by the model to the results $r$ and $r^{\prime}$. More formally, we compute</p>
<p>$$
\mathrm{IE}_{\mathrm{alt}}(z)= \begin{cases}\Delta^{\prime}+\Delta &amp; \text { if } r \neq r^{\prime} \ |\Delta| &amp; \text { otherwise }\end{cases}
$$</p>
<p>where</p>
<p>$$
\begin{aligned}
\Delta^{\prime} &amp; =\log \mathbb{P}_{z}^{<em>}(r)-\log \mathbb{P}(r) \
\Delta &amp; =\log \mathbb{P}\left(r^{\prime}\right)-\log \mathbb{P}_{z}^{</em>}\left(r^{\prime}\right)
\end{aligned}
$$</p>
<p>The results are reported in Figure 13. The activation sites that we observe are the same as reported in Section 4.1: first-layer MLP at the operand tokens and last-token MLP and attention modules.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Indirect effect measured using the difference in the log probability as described in Equation 5 (IE $_{\text {alt }}$ ). The results are obtained with GPT-J on two-operand arithmetic queries.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Indirect effect (IE) measured for the attention modules in the fine-tuned version of Pythia 2.8B on three-operand arithmetic queries, when $r=r^{\prime}$.</p>
<h1>J Additional Information Flow Visualizations</h1>
<p>We include the IE measurements for the attention modules of GPT-J on the number retrieval task (Figure 11) and on the factual knowledge queries (Figure 12), and for Pythia 2.8B on three-operand arithmetic queries before and after fine-tuning (Figure 15). Additionally, we report the heatmap visualizations of the indirect effect measured for the following models: Pythia 2.8B (Figure 16), LLaMA 7B (Figure 17), Goat (Figure 18), and GPT-J using word numerals (Figure 19). Finally, we visualize in Figure 14 the IE of MLPs and attention modules for the fine-tuned Pythia 2.8B in the fixed-result case.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Indirect effect (IE) measured for the attention modules in Pythia 2.8B on three-operand arithmetic queries, before and after fine-tuning.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Indirect effect (IE) measured in the MLP and attention modules of Pythia 2.8B on two-operand arithmetic queries.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17: Indirect effect (IE) measured in the MLP and attention modules of LLaMA 7B on two-operand arithmetic queries.</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18: Indirect effect (IE) measured in the MLP and attention modules of Goat on two-operand arithmetic queries.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19: Indirect effect (IE) measured in the MLP and attention modules of GPT-J on two-operand arithmetic queries, using numeral words to represent quantities.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ http://wandb.ai&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ We sample entities from a list containing names of animals, fruits, office tools, and other everyday items and objects.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
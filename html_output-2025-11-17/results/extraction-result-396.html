<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-396 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-396</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-396</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-269306045</p>
                <p><strong>Paper Title:</strong> Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023</p>
                <p><strong>Paper Abstract:</strong> Identifying valuable information within the extensive texts documented in natural language presents a significant challenge in various disciplines. Named Entity Recognition (NER), as one of the critical technologies in text data processing and mining, has become a current research hotspot. To accurately and objectively review the progress in NER, this paper employs bibliometric methods. It analyzes 1300 documents related to NER obtained from the Web of Science database using CiteSpace software. Firstly, statistical analysis is performed on the literature and journals that were obtained to explore the distribution characteristics of the literature. Secondly, the core authors in the field of NER, the development of the technology in different countries, and the leading institutions are explored by analyzing the number of publications and the cooperation network graph. Finally, explore the research frontiers, development tracks, research hotspots, and other information in this field from a scientific point of view, and further discuss the five research frontiers and seven research hotspots in depth. This paper explores the progress of NER research from both macro and micro perspectives. It aims to assist researchers in quickly grasping relevant information and offers constructive ideas and suggestions to promote the development of NER.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e396.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e396.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLM fine-tuning → Biomedical (BioBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-trained Language Model fine-tuning applied to biomedical NER (BioBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning of large pre-trained language models (PLMs) like BERT on domain-specific corpora (biomedical text) to improve Named Entity Recognition (NER) performance in that domain; exemplified by BioBERT which further pre-trains BERT on biomedical corpora then fine-tunes for biomedical NER tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pre-trained language model (PLM) fine-tuning / domain-adaptive pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Train a general PLM (e.g., BERT) on large general-domain text; then continue pre-training the same model on large unlabeled domain-specific corpora (domain-adaptive pretraining), and finally fine-tune the model on supervised downstream NER labeled data in the target domain. The domain-adaptive pretraining adjusts contextual embeddings toward domain vocabulary and semantics; fine-tuning updates the model weights for the specific NER labeling objective (often sequence labeling layers such as CRF on top of token representations).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general-domain Natural Language Processing / pretraining on web/news corpora (computer science / NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>biomedical text mining / biomedical NER (biomedicine)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (domain-adaptive pretraining + fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Continued unsupervised pretraining of the base PLM on large biomedical corpora (PubMed abstracts, PMC full-text) to adapt token/contextual embeddings to biomedical vocabulary and semantics; then fine-tuned with biomedical NER supervised labels and often added/modified task-specific output layers (e.g., token classification head, CRF). These steps address vocabulary mismatch and domain semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - reported substantial improvement in biomedical NER and relation extraction tasks relative to general-domain BERT baselines (qualitative description in the review; specific metrics not reproduced in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>High computational cost for continued pretraining; requirement for large domain corpora; domain annotation standards differ (heterogeneous NER label sets); PLM size and deployment constraints in resource-limited environments.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of large biomedical text corpora (PubMed, PMC), existing BERT architecture and checkpoints, demonstrated generality of contextual embeddings, and community benchmarks/datasets for biomedical NER.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to large unlabeled domain corpora for domain-adaptive pretraining, supervised labeled datasets for fine-tuning, substantial compute (GPUs/TPUs), and domain expertise for annotation/label mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High generalizability — the paper cites BioBERT as an instance of a general approach; this PLM fine-tuning pattern is shown to transfer to other specialized domains (clinical, chemical, geoscience) with analogous domain pretraining and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and technical skills (how to perform domain-adaptive pretraining and fine-tuning), plus theoretical principles of transfer learning in PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e396.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT → Chinese clinical NER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT domain transfer to Chinese clinical named entity recognition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of BERT-based models that have been pre-trained or further pre-trained on Chinese clinical records to perform Chinese clinical NER, addressing language and domain-specific tokenization and label needs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chinese clinical named entity recognition with variant neural structures based on BERT methods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>PLM domain-specific pretraining and fine-tuning for clinical Chinese NER</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Pretrain or further pretrain a BERT-family model on large unlabeled Chinese clinical record corpora, optionally adapt tokenization to Chinese segmentation challenges, then fine-tune on annotated Chinese clinical NER corpora using sequence labeling architectures (e.g., BiLSTM-CRF on top of BERT embeddings) to extract medical entities.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer learning / domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general/multilingual NLP and PLM research</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Chinese clinical / electronic health record NER (medical informatics)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (language + domain adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Pretraining on domain-specific (Chinese clinical) corpora, special handling for Chinese word segmentation (character vs. word-level embeddings), and architecture choices (e.g., variant neural structures) tuned for clinical text idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - review states improved performance on Chinese clinical NER when BERT is pre-trained on clinical corpora, but also notes challenges such as long text handling and need for domain-labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Chinese segmentation ambiguity, lack of large high-quality annotated clinical corpora, privacy constraints on clinical data, and computational costs.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of unlabeled clinical text for continued pretraining, PLM architectures amenable to fine-tuning, and prior positive results of PLMs in other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Clinical domain expertise for annotation, access to de-identified clinical corpora, Chinese-language tokenization strategies, and compute resources.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate — approach generalizes to other languages/domains but requires language- and domain-specific pretraining and annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and domain-specific technical adaptations (tokenization, fine-tuning protocols).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e396.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M-BERT / XLM-R cross-lingual transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilingual pre-trained language models (M-BERT, XLM-R) for cross-lingual NER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of multilingual PLMs pre-trained on many languages to transfer knowledge from high-resource languages into low-resource languages for NER tasks without parallel corpora, enabling zero-shot or few-shot cross-lingual NER.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised Cross-Lingual Representation Learning at Scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Cross-lingual transfer using multilingual PLMs (M-BERT, XLM-R)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Pretrain a large transformer model on text from many languages (shared vocabulary or subword tokenization). For a target low-resource language, either fine-tune the multilingual model on labeled data from a high-resource language (zero-shot) or perform multilingual fine-tuning; optionally add adversarial language tasks or language-agnostic objectives to better align representations across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer learning / cross-lingual adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>multilingual NLP / general-domain corpora (computer science)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>low-resource language NER / cross-lingual NER (computational linguistics across languages)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (multilingual pretraining + cross-lingual fine-tuning or adversarial adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Pretraining on >100 languages (XLM-R) and use of language adversarial objectives or language-aware fine-tuning strategies; incorporation of cross-lingual lexical features or dictionary expansion and feature integration for low-resource languages.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - multilingual PLMs and XLM-R yield strong cross-lingual performance improvements; however, limitations remain for languages with very different typology or script and limited labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Mismatch in annotation standards across languages, typological differences (segmentation, morphology), limited labeled data for fine-tuning in target languages, and domain differences leading to negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Large-scale multilingual pretraining, shared subword embeddings, availability of multilingual corpora, and methods like adversarial training to align representations.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Access to multilingual pretrained checkpoints, occasional bilingual lexica or small labeled sets for adaptation, and compute for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High across many languages with related scripts or abundant pretraining data; less generalizable to extremely low-resource or typologically divergent languages without further adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedures for multilingual pretraining and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e396.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial training → NER robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial training adapted to Named Entity Recognition (cross-domain and robustness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of adversarial example generation and adversarial learning strategies (originating in robustness/security research) to improve model generalization and domain adaptation for NER, e.g., cross-domain adversarial learning for Chinese medical NER and automotive NER.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cross domains adversarial learning for Chinese named entity recognition for online medical consultation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Adversarial training / domain-adversarial learning</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Generate small perturbations (input-level or representation-level) or use adversarial domain discriminators during training so the feature extractor learns representations invariant to domain/language while remaining predictive for NER; alternatively train with adversarial examples to make token classification robust to input noise and distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / robustness and domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>adversarial ML / computer vision and security (machine learning robustness)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>Named Entity Recognition across domains (medical, automotive, social media)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (domain adversarial adaptations and sequence-labeling specific perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Replace image perturbations with text-appropriate adversarial perturbations (embedding-level perturbations), integrate adversarial domain classifiers into sequence labeling architectures, combine adversarial loss with NER task loss, and sometimes incorporate multi-task setups (e.g., spacing prediction) to stabilize training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - reported improvements in cross-domain robustness and reduced overfitting to single domains; success is task- and data-dependent and can require careful tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Designing meaningful adversarial perturbations for discrete text data, balancing adversarial loss with NER objectives, risk of over-regularization harming in-domain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Shared embedding spaces, availability of unlabeled or weakly-labeled target-domain text for adversarial alignment, and prior success of adversarial ideas in other ML fields.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Mechanisms to create adversarial examples in continuous embedding space, multi-domain data for discriminator training, and computational budget for adversarial training loops.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate — methods generalize to many NER cross-domain settings but need task-specific adaptation (perturbation design) and tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (how to craft adversarial perturbations and losses) and theoretical principles of domain invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e396.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Federated learning → Medical NER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Federated learning applied to privacy-preserving medical named entity recognition (FedNER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adapting federated learning (distributed training without centralizing data) to train NER models across multiple medical institutions while preserving patient data privacy; techniques include knowledge distillation, module separation, and sharing gradients or distilled outputs instead of raw data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fedner: Privacy-Preserving Medical Named Entity Recognition with Federated Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Federated learning with knowledge distillation / module partitioning for NER</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Train local NER models on-site at multiple institutions; exchange model updates (gradients) or distilled knowledge (teacher-student/knowledge distillation) rather than raw patient data; design shared modules for general knowledge and private modules for local specifics; aggregate updates centrally or via distillation to build a global model or personalized models while preserving privacy.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / distributed learning / privacy-preserving training</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>federated machine learning (distributed ML / privacy-preserving ML)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>clinical/medical NER on electronic health records (medical informatics)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (privacy and heterogeneous label-set adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Introduced model partitioning (shared vs private modules), tag-set heterogeneous distillation (to handle differing local entity label sets), communication-efficient distillation to reduce bandwidth, and privacy-aware gradient sharing instead of raw texts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - reported improvements in enabling cross-institutional learning while protecting privacy, with demonstrated feasibility; precise quantitative gains not given in the bibliometric review.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Heterogeneous label sets across sites, communication cost, client data heterogeneity, regulatory/privacy constraints, and need for encryption/differential privacy measures.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Clear privacy needs in healthcare, existence of federated learning frameworks, and techniques like knowledge distillation to reduce communication.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Institutional collaboration agreements, on-site compute at participating sites, secure communication channels, solutions for label-set heterogeneity, and expertise in federated ML.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Potentially high to other privacy-sensitive domains (finance, legal), but adaptations required for label heterogeneity and communication constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (federated training workflows) and instrumental/technical skills (distillation, module partitioning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e396.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distant / weak supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distant supervision and weakly-supervised learning for domain NER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic labeling of training corpora using existing knowledge bases, ontologies, or heuristic rules (distant supervision) and training with imperfect/incomplete labels (weak supervision) to create large noisy training sets for NER in specialized domains (chemical, biomedical, agricultural).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemNER: fine-grained chemistry named entity recognition with ontology-guided distant supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Distant supervision and weak supervision using knowledge bases / ontologies</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Match mentions in raw text to entries in domain knowledge bases or ontologies to generate pseudo-labels automatically (distant supervision), or combine multiple heuristic labeling functions/ontologies to create weak labels; then train models with noise-robust learning strategies (reliability weighting, hybrid pseudo-labeling, or special loss functions) to mitigate label noise.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data annotation strategy / weakly-supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>knowledge-base-driven information extraction / ontology engineering (biomedicine, chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>NER in specialized domains (chemistry, biomedicine, agriculture, clinical)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (knowledge-base matching + noise-robust learning)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Ontology-guided matching rules, category rebalancing thresholds, noise-robust loss functions, and self-training or hybrid pseudo-labeling pipelines to compensate for incomplete or noisy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - distant/weak supervision dramatically increases training data scale and enables workable models where manual annotation is scarce; however, noise and incompleteness require robust learning techniques and residual performance gaps remain relative to fully supervised data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>False positives/negatives from surface matching, incomplete coverage of knowledge bases (false negatives), label imbalance across entity types, and domain-specific ambiguity in mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of curated ontologies/knowledge bases, rule-based heuristics for matching, and noise-robust learning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>High-quality domain knowledge bases or lexica, methods to estimate label reliability, and pipelines for noise mitigation and pseudo-label iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for domains with established ontologies/knowledge bases; less useful for domains without structured knowledge resources.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (label generation and noise-robust training) and use of explicit domain knowledge (ontologies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e396.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep active learning for NER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep active learning (uncertainty and diversity sampling) adapted for NER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combining deep neural NER models (e.g., BERT-BiLSTM-CRF) with active learning sampling strategies (uncertainty, diversity, subsequence selection) to reduce annotation cost by selecting the most informative sentences/subsequences for labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Subsequence based deep active learning for named entity recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Deep active learning with uncertainty-and-diversity sampling for sequence labeling</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Iteratively train a deep NER model on a small labeled set; use model-based criteria (uncertainty of token/sequence labels, diversity of examples) to select unlabeled sentences or subsequences expected to yield largest performance gains; annotate those selected examples and retrain/finetune the model. Subsequence selection refines granularity to annotate only highly informative spans.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data-efficient annotation strategy / interactive learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>active learning theory and deep learning (machine learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>named entity recognition in low-resource / domain-specific corpora (social media, clinical, domain-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>hybrid approach combining active learning principles with deep sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Design of sequence-aware uncertainty measures (minimum confidence for tokens, sentence-level uncertainty aggregated from token uncertainties), incorporation of diversity sampling to avoid redundant examples, and subsequence-level querying to reduce annotation effort.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in reducing annotation load while maintaining competitive performance in reviewed studies; specific numeric savings vary by dataset and strategy (not provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Selection strategy design for structured outputs, labeling interface support for subsequence annotation, dependency on annotator consistency, and compute overhead for repeated model training.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Strong baseline deep models (BERT variants), availability of active sampling criteria, and annotation pipelines to accept iterative queries.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Annotation budget and expert annotators for domain-specific labels, infrastructure to retrain and evaluate models in the loop, and tooling for fine-grained annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High across domains where annotation is costly; methods need adaptation of selection criteria to task and label granularities.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural know-how (active selection loops) and technical skills for integrating deep models with sampling strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e396.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble/model combination → Chemical NER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model combination (ensembling heterogeneous models) applied to chemical named entity recognition (tmChem)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combine outputs of multiple, diverse machine learning models (different feature sets, CRF parameters) into a single chemical NER system to leverage complementary strengths and improve recognition accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>tmChem: a high performance approach for chemical named entity recognition and normalization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Heterogeneous model combination / ensembling for domain NER</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Train several diverse NER models (e.g., CRF variants, different feature sets, neural sequence models) and combine their predictions via model combination techniques (voting, stacking, ensemble fusion) to reduce individual model errors and increase robustness for chemical entity recognition. May include normalization steps linking mentions to chemical identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / ensemble learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>ensemble methods in machine learning (computer science)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>chemical named entity recognition and normalization (chemoinformatics)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with domain-specific tailoring (feature engineering and normalization modules)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Use of chemistry-specific lexicons/features, parameter diversity (CRF settings), and post-processing normalization regularities to chemical ontologies; ensemble weighting tuned on chemical NER validation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - tmChem and model-combination approaches showed notable improvements in chemical NER competitions and benchmarks according to review references (qualitative summary).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Need for domain-specific features/lexica, ensemble complexity increasing inference and maintenance cost, and requirement to normalize to chemical identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of domain lexicons and labeled chemical corpora, complementarity between model architectures, and ensemble techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Curated chemical dictionaries/ontologies for normalization, labeled chemical NER datasets, and validation data to tune ensemble weights.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate — ensembling is broadly applicable, but feature and normalization adaptations are domain-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills (ensemble construction) and explicit procedural steps for combining heterogeneous models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e396.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e396.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data augmentation / pseudo-annotation → Cross-domain NER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data augmentation and pseudo-annotated synthesis for cross-domain NER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of synthetic/pseudo-labeled data and augmentation strategies to mitigate scarcity in target-domain labeled data, enabling cross-domain NER transfer and boosting generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data Augmentation for Cross-Domain Named Entity Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Data augmentation and pseudo-annotation for cross-domain transfer</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Generate additional labeled-like examples for the target domain by methods such as: (1) synthesizing sentences with inserted domain entities, (2) using models to create pseudo-labels on unlabeled target data (self-training), and (3) applying transformations (synonym replacement, back-translation) to existing labeled data. Use these augmented datasets to fine-tune or adapt models across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>data augmentation / semi-supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>data augmentation and semi-supervised learning (machine learning / NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>cross-domain NER (various domain pairs: general→specialized or across specialized domains)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (domain-aware augmentation and pseudo-label reliability mechanisms)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Design domain-specific augmentation (inserting domain entity types), reliability-based selection of pseudo-labeled examples, category rebalancing and hybrid pseudo-labeling to address class imbalance and noisy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - these methods alleviate data scarcity and improve cross-domain NER robustness, but require noise control and careful augmentation strategies; quantitative gains depend on dataset and method.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Artificial data distribution mismatch, noisy pseudo-labels causing error propagation, and difficulty simulating realistic domain-specific context.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of unlabeled target-domain corpora, generative models or heuristics to synthesize plausible sentences, and noise-robust training techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Unlabeled target-domain text, heuristics or models for reliable pseudo-label generation, and validation data to control augmentation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High conceptually, but success depends on domain similarity and realism of synthetic data; more effective when augmented data reflect true target-domain context.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural know-how (how to generate/highlight pseudo-labeled data) and explicit training pipelines for semi-supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>BioBERT: a pre-trained biomedical language representation model for biomedical text mining <em>(Rating: 2)</em></li>
                <li>Unsupervised Cross-Lingual Representation Learning at Scale <em>(Rating: 2)</em></li>
                <li>Explaining and Harnessing Adversarial Examples <em>(Rating: 2)</em></li>
                <li>Cross domains adversarial learning for Chinese named entity recognition for online medical consultation <em>(Rating: 2)</em></li>
                <li>Fedner: Privacy-Preserving Medical Named Entity Recognition with Federated Learning <em>(Rating: 2)</em></li>
                <li>tmChem: a high performance approach for chemical named entity recognition and normalization <em>(Rating: 2)</em></li>
                <li>ChemNER: fine-grained chemistry named entity recognition with ontology-guided distant supervision <em>(Rating: 2)</em></li>
                <li>Subsequence based deep active learning for named entity recognition <em>(Rating: 2)</em></li>
                <li>Data Augmentation for Cross-Domain Named Entity Recognition <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-396",
    "paper_id": "paper-269306045",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "PLM fine-tuning → Biomedical (BioBERT)",
            "name_full": "Pre-trained Language Model fine-tuning applied to biomedical NER (BioBERT)",
            "brief_description": "Fine-tuning of large pre-trained language models (PLMs) like BERT on domain-specific corpora (biomedical text) to improve Named Entity Recognition (NER) performance in that domain; exemplified by BioBERT which further pre-trains BERT on biomedical corpora then fine-tunes for biomedical NER tasks.",
            "citation_title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "mention_or_use": "mention",
            "procedure_name": "Pre-trained language model (PLM) fine-tuning / domain-adaptive pretraining",
            "procedure_description": "Train a general PLM (e.g., BERT) on large general-domain text; then continue pre-training the same model on large unlabeled domain-specific corpora (domain-adaptive pretraining), and finally fine-tune the model on supervised downstream NER labeled data in the target domain. The domain-adaptive pretraining adjusts contextual embeddings toward domain vocabulary and semantics; fine-tuning updates the model weights for the specific NER labeling objective (often sequence labeling layers such as CRF on top of token representations).",
            "procedure_type": "computational method / transfer learning",
            "source_domain": "general-domain Natural Language Processing / pretraining on web/news corpora (computer science / NLP)",
            "target_domain": "biomedical text mining / biomedical NER (biomedicine)",
            "transfer_type": "adapted/modified for new context (domain-adaptive pretraining + fine-tuning)",
            "modifications_made": "Continued unsupervised pretraining of the base PLM on large biomedical corpora (PubMed abstracts, PMC full-text) to adapt token/contextual embeddings to biomedical vocabulary and semantics; then fine-tuned with biomedical NER supervised labels and often added/modified task-specific output layers (e.g., token classification head, CRF). These steps address vocabulary mismatch and domain semantics.",
            "transfer_success": "successful - reported substantial improvement in biomedical NER and relation extraction tasks relative to general-domain BERT baselines (qualitative description in the review; specific metrics not reproduced in this review).",
            "barriers_encountered": "High computational cost for continued pretraining; requirement for large domain corpora; domain annotation standards differ (heterogeneous NER label sets); PLM size and deployment constraints in resource-limited environments.",
            "facilitating_factors": "Availability of large biomedical text corpora (PubMed, PMC), existing BERT architecture and checkpoints, demonstrated generality of contextual embeddings, and community benchmarks/datasets for biomedical NER.",
            "contextual_requirements": "Access to large unlabeled domain corpora for domain-adaptive pretraining, supervised labeled datasets for fine-tuning, substantial compute (GPUs/TPUs), and domain expertise for annotation/label mapping.",
            "generalizability": "High generalizability — the paper cites BioBERT as an instance of a general approach; this PLM fine-tuning pattern is shown to transfer to other specialized domains (clinical, chemical, geoscience) with analogous domain pretraining and fine-tuning.",
            "knowledge_type": "explicit procedural steps and technical skills (how to perform domain-adaptive pretraining and fine-tuning), plus theoretical principles of transfer learning in PLMs.",
            "uuid": "e396.0",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "BERT → Chinese clinical NER",
            "name_full": "BERT domain transfer to Chinese clinical named entity recognition",
            "brief_description": "Application of BERT-based models that have been pre-trained or further pre-trained on Chinese clinical records to perform Chinese clinical NER, addressing language and domain-specific tokenization and label needs.",
            "citation_title": "Chinese clinical named entity recognition with variant neural structures based on BERT methods",
            "mention_or_use": "mention",
            "procedure_name": "PLM domain-specific pretraining and fine-tuning for clinical Chinese NER",
            "procedure_description": "Pretrain or further pretrain a BERT-family model on large unlabeled Chinese clinical record corpora, optionally adapt tokenization to Chinese segmentation challenges, then fine-tune on annotated Chinese clinical NER corpora using sequence labeling architectures (e.g., BiLSTM-CRF on top of BERT embeddings) to extract medical entities.",
            "procedure_type": "computational method / transfer learning / domain adaptation",
            "source_domain": "general/multilingual NLP and PLM research",
            "target_domain": "Chinese clinical / electronic health record NER (medical informatics)",
            "transfer_type": "adapted/modified for new context (language + domain adaptations)",
            "modifications_made": "Pretraining on domain-specific (Chinese clinical) corpora, special handling for Chinese word segmentation (character vs. word-level embeddings), and architecture choices (e.g., variant neural structures) tuned for clinical text idiosyncrasies.",
            "transfer_success": "partially successful - review states improved performance on Chinese clinical NER when BERT is pre-trained on clinical corpora, but also notes challenges such as long text handling and need for domain-labeled data.",
            "barriers_encountered": "Chinese segmentation ambiguity, lack of large high-quality annotated clinical corpora, privacy constraints on clinical data, and computational costs.",
            "facilitating_factors": "Availability of unlabeled clinical text for continued pretraining, PLM architectures amenable to fine-tuning, and prior positive results of PLMs in other domains.",
            "contextual_requirements": "Clinical domain expertise for annotation, access to de-identified clinical corpora, Chinese-language tokenization strategies, and compute resources.",
            "generalizability": "Moderate — approach generalizes to other languages/domains but requires language- and domain-specific pretraining and annotation.",
            "knowledge_type": "explicit procedural steps and domain-specific technical adaptations (tokenization, fine-tuning protocols).",
            "uuid": "e396.1",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "M-BERT / XLM-R cross-lingual transfer",
            "name_full": "Multilingual pre-trained language models (M-BERT, XLM-R) for cross-lingual NER",
            "brief_description": "Use of multilingual PLMs pre-trained on many languages to transfer knowledge from high-resource languages into low-resource languages for NER tasks without parallel corpora, enabling zero-shot or few-shot cross-lingual NER.",
            "citation_title": "Unsupervised Cross-Lingual Representation Learning at Scale",
            "mention_or_use": "mention",
            "procedure_name": "Cross-lingual transfer using multilingual PLMs (M-BERT, XLM-R)",
            "procedure_description": "Pretrain a large transformer model on text from many languages (shared vocabulary or subword tokenization). For a target low-resource language, either fine-tune the multilingual model on labeled data from a high-resource language (zero-shot) or perform multilingual fine-tuning; optionally add adversarial language tasks or language-agnostic objectives to better align representations across languages.",
            "procedure_type": "computational method / transfer learning / cross-lingual adaptation",
            "source_domain": "multilingual NLP / general-domain corpora (computer science)",
            "target_domain": "low-resource language NER / cross-lingual NER (computational linguistics across languages)",
            "transfer_type": "adapted/modified for new context (multilingual pretraining + cross-lingual fine-tuning or adversarial adaptation)",
            "modifications_made": "Pretraining on &gt;100 languages (XLM-R) and use of language adversarial objectives or language-aware fine-tuning strategies; incorporation of cross-lingual lexical features or dictionary expansion and feature integration for low-resource languages.",
            "transfer_success": "partially successful - multilingual PLMs and XLM-R yield strong cross-lingual performance improvements; however, limitations remain for languages with very different typology or script and limited labeled data.",
            "barriers_encountered": "Mismatch in annotation standards across languages, typological differences (segmentation, morphology), limited labeled data for fine-tuning in target languages, and domain differences leading to negative transfer.",
            "facilitating_factors": "Large-scale multilingual pretraining, shared subword embeddings, availability of multilingual corpora, and methods like adversarial training to align representations.",
            "contextual_requirements": "Access to multilingual pretrained checkpoints, occasional bilingual lexica or small labeled sets for adaptation, and compute for fine-tuning.",
            "generalizability": "High across many languages with related scripts or abundant pretraining data; less generalizable to extremely low-resource or typologically divergent languages without further adaptation.",
            "knowledge_type": "theoretical principles and explicit procedures for multilingual pretraining and fine-tuning.",
            "uuid": "e396.2",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Adversarial training → NER robustness",
            "name_full": "Adversarial training adapted to Named Entity Recognition (cross-domain and robustness)",
            "brief_description": "Use of adversarial example generation and adversarial learning strategies (originating in robustness/security research) to improve model generalization and domain adaptation for NER, e.g., cross-domain adversarial learning for Chinese medical NER and automotive NER.",
            "citation_title": "Cross domains adversarial learning for Chinese named entity recognition for online medical consultation",
            "mention_or_use": "mention",
            "procedure_name": "Adversarial training / domain-adversarial learning",
            "procedure_description": "Generate small perturbations (input-level or representation-level) or use adversarial domain discriminators during training so the feature extractor learns representations invariant to domain/language while remaining predictive for NER; alternatively train with adversarial examples to make token classification robust to input noise and distribution shifts.",
            "procedure_type": "computational method / robustness and domain adaptation",
            "source_domain": "adversarial ML / computer vision and security (machine learning robustness)",
            "target_domain": "Named Entity Recognition across domains (medical, automotive, social media)",
            "transfer_type": "adapted/modified for new context (domain adversarial adaptations and sequence-labeling specific perturbations)",
            "modifications_made": "Replace image perturbations with text-appropriate adversarial perturbations (embedding-level perturbations), integrate adversarial domain classifiers into sequence labeling architectures, combine adversarial loss with NER task loss, and sometimes incorporate multi-task setups (e.g., spacing prediction) to stabilize training.",
            "transfer_success": "partially successful - reported improvements in cross-domain robustness and reduced overfitting to single domains; success is task- and data-dependent and can require careful tuning.",
            "barriers_encountered": "Designing meaningful adversarial perturbations for discrete text data, balancing adversarial loss with NER objectives, risk of over-regularization harming in-domain performance.",
            "facilitating_factors": "Shared embedding spaces, availability of unlabeled or weakly-labeled target-domain text for adversarial alignment, and prior success of adversarial ideas in other ML fields.",
            "contextual_requirements": "Mechanisms to create adversarial examples in continuous embedding space, multi-domain data for discriminator training, and computational budget for adversarial training loops.",
            "generalizability": "Moderate — methods generalize to many NER cross-domain settings but need task-specific adaptation (perturbation design) and tuning.",
            "knowledge_type": "explicit procedural steps (how to craft adversarial perturbations and losses) and theoretical principles of domain invariance.",
            "uuid": "e396.3",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Federated learning → Medical NER",
            "name_full": "Federated learning applied to privacy-preserving medical named entity recognition (FedNER)",
            "brief_description": "Adapting federated learning (distributed training without centralizing data) to train NER models across multiple medical institutions while preserving patient data privacy; techniques include knowledge distillation, module separation, and sharing gradients or distilled outputs instead of raw data.",
            "citation_title": "Fedner: Privacy-Preserving Medical Named Entity Recognition with Federated Learning",
            "mention_or_use": "mention",
            "procedure_name": "Federated learning with knowledge distillation / module partitioning for NER",
            "procedure_description": "Train local NER models on-site at multiple institutions; exchange model updates (gradients) or distilled knowledge (teacher-student/knowledge distillation) rather than raw patient data; design shared modules for general knowledge and private modules for local specifics; aggregate updates centrally or via distillation to build a global model or personalized models while preserving privacy.",
            "procedure_type": "computational method / distributed learning / privacy-preserving training",
            "source_domain": "federated machine learning (distributed ML / privacy-preserving ML)",
            "target_domain": "clinical/medical NER on electronic health records (medical informatics)",
            "transfer_type": "adapted/modified for new context (privacy and heterogeneous label-set adaptations)",
            "modifications_made": "Introduced model partitioning (shared vs private modules), tag-set heterogeneous distillation (to handle differing local entity label sets), communication-efficient distillation to reduce bandwidth, and privacy-aware gradient sharing instead of raw texts.",
            "transfer_success": "partially successful - reported improvements in enabling cross-institutional learning while protecting privacy, with demonstrated feasibility; precise quantitative gains not given in the bibliometric review.",
            "barriers_encountered": "Heterogeneous label sets across sites, communication cost, client data heterogeneity, regulatory/privacy constraints, and need for encryption/differential privacy measures.",
            "facilitating_factors": "Clear privacy needs in healthcare, existence of federated learning frameworks, and techniques like knowledge distillation to reduce communication.",
            "contextual_requirements": "Institutional collaboration agreements, on-site compute at participating sites, secure communication channels, solutions for label-set heterogeneity, and expertise in federated ML.",
            "generalizability": "Potentially high to other privacy-sensitive domains (finance, legal), but adaptations required for label heterogeneity and communication constraints.",
            "knowledge_type": "explicit procedural steps (federated training workflows) and instrumental/technical skills (distillation, module partitioning).",
            "uuid": "e396.4",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Distant / weak supervision",
            "name_full": "Distant supervision and weakly-supervised learning for domain NER",
            "brief_description": "Automatic labeling of training corpora using existing knowledge bases, ontologies, or heuristic rules (distant supervision) and training with imperfect/incomplete labels (weak supervision) to create large noisy training sets for NER in specialized domains (chemical, biomedical, agricultural).",
            "citation_title": "ChemNER: fine-grained chemistry named entity recognition with ontology-guided distant supervision",
            "mention_or_use": "mention",
            "procedure_name": "Distant supervision and weak supervision using knowledge bases / ontologies",
            "procedure_description": "Match mentions in raw text to entries in domain knowledge bases or ontologies to generate pseudo-labels automatically (distant supervision), or combine multiple heuristic labeling functions/ontologies to create weak labels; then train models with noise-robust learning strategies (reliability weighting, hybrid pseudo-labeling, or special loss functions) to mitigate label noise.",
            "procedure_type": "data annotation strategy / weakly-supervised learning",
            "source_domain": "knowledge-base-driven information extraction / ontology engineering (biomedicine, chemistry)",
            "target_domain": "NER in specialized domains (chemistry, biomedicine, agriculture, clinical)",
            "transfer_type": "adapted/modified for new context (knowledge-base matching + noise-robust learning)",
            "modifications_made": "Ontology-guided matching rules, category rebalancing thresholds, noise-robust loss functions, and self-training or hybrid pseudo-labeling pipelines to compensate for incomplete or noisy labels.",
            "transfer_success": "partially successful - distant/weak supervision dramatically increases training data scale and enables workable models where manual annotation is scarce; however, noise and incompleteness require robust learning techniques and residual performance gaps remain relative to fully supervised data.",
            "barriers_encountered": "False positives/negatives from surface matching, incomplete coverage of knowledge bases (false negatives), label imbalance across entity types, and domain-specific ambiguity in mentions.",
            "facilitating_factors": "Availability of curated ontologies/knowledge bases, rule-based heuristics for matching, and noise-robust learning algorithms.",
            "contextual_requirements": "High-quality domain knowledge bases or lexica, methods to estimate label reliability, and pipelines for noise mitigation and pseudo-label iteration.",
            "generalizability": "High for domains with established ontologies/knowledge bases; less useful for domains without structured knowledge resources.",
            "knowledge_type": "explicit procedural steps (label generation and noise-robust training) and use of explicit domain knowledge (ontologies).",
            "uuid": "e396.5",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Deep active learning for NER",
            "name_full": "Deep active learning (uncertainty and diversity sampling) adapted for NER",
            "brief_description": "Combining deep neural NER models (e.g., BERT-BiLSTM-CRF) with active learning sampling strategies (uncertainty, diversity, subsequence selection) to reduce annotation cost by selecting the most informative sentences/subsequences for labeling.",
            "citation_title": "Subsequence based deep active learning for named entity recognition",
            "mention_or_use": "mention",
            "procedure_name": "Deep active learning with uncertainty-and-diversity sampling for sequence labeling",
            "procedure_description": "Iteratively train a deep NER model on a small labeled set; use model-based criteria (uncertainty of token/sequence labels, diversity of examples) to select unlabeled sentences or subsequences expected to yield largest performance gains; annotate those selected examples and retrain/finetune the model. Subsequence selection refines granularity to annotate only highly informative spans.",
            "procedure_type": "data-efficient annotation strategy / interactive learning",
            "source_domain": "active learning theory and deep learning (machine learning)",
            "target_domain": "named entity recognition in low-resource / domain-specific corpora (social media, clinical, domain-specific)",
            "transfer_type": "hybrid approach combining active learning principles with deep sequence models",
            "modifications_made": "Design of sequence-aware uncertainty measures (minimum confidence for tokens, sentence-level uncertainty aggregated from token uncertainties), incorporation of diversity sampling to avoid redundant examples, and subsequence-level querying to reduce annotation effort.",
            "transfer_success": "successful in reducing annotation load while maintaining competitive performance in reviewed studies; specific numeric savings vary by dataset and strategy (not provided in review).",
            "barriers_encountered": "Selection strategy design for structured outputs, labeling interface support for subsequence annotation, dependency on annotator consistency, and compute overhead for repeated model training.",
            "facilitating_factors": "Strong baseline deep models (BERT variants), availability of active sampling criteria, and annotation pipelines to accept iterative queries.",
            "contextual_requirements": "Annotation budget and expert annotators for domain-specific labels, infrastructure to retrain and evaluate models in the loop, and tooling for fine-grained annotation.",
            "generalizability": "High across domains where annotation is costly; methods need adaptation of selection criteria to task and label granularities.",
            "knowledge_type": "procedural know-how (active selection loops) and technical skills for integrating deep models with sampling strategies.",
            "uuid": "e396.6",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Ensemble/model combination → Chemical NER",
            "name_full": "Model combination (ensembling heterogeneous models) applied to chemical named entity recognition (tmChem)",
            "brief_description": "Combine outputs of multiple, diverse machine learning models (different feature sets, CRF parameters) into a single chemical NER system to leverage complementary strengths and improve recognition accuracy.",
            "citation_title": "tmChem: a high performance approach for chemical named entity recognition and normalization",
            "mention_or_use": "mention",
            "procedure_name": "Heterogeneous model combination / ensembling for domain NER",
            "procedure_description": "Train several diverse NER models (e.g., CRF variants, different feature sets, neural sequence models) and combine their predictions via model combination techniques (voting, stacking, ensemble fusion) to reduce individual model errors and increase robustness for chemical entity recognition. May include normalization steps linking mentions to chemical identifiers.",
            "procedure_type": "computational method / ensemble learning",
            "source_domain": "ensemble methods in machine learning (computer science)",
            "target_domain": "chemical named entity recognition and normalization (chemoinformatics)",
            "transfer_type": "direct application with domain-specific tailoring (feature engineering and normalization modules)",
            "modifications_made": "Use of chemistry-specific lexicons/features, parameter diversity (CRF settings), and post-processing normalization regularities to chemical ontologies; ensemble weighting tuned on chemical NER validation sets.",
            "transfer_success": "successful - tmChem and model-combination approaches showed notable improvements in chemical NER competitions and benchmarks according to review references (qualitative summary).",
            "barriers_encountered": "Need for domain-specific features/lexica, ensemble complexity increasing inference and maintenance cost, and requirement to normalize to chemical identifiers.",
            "facilitating_factors": "Availability of domain lexicons and labeled chemical corpora, complementarity between model architectures, and ensemble techniques.",
            "contextual_requirements": "Curated chemical dictionaries/ontologies for normalization, labeled chemical NER datasets, and validation data to tune ensemble weights.",
            "generalizability": "Moderate — ensembling is broadly applicable, but feature and normalization adaptations are domain-specific.",
            "knowledge_type": "instrumental/technical skills (ensemble construction) and explicit procedural steps for combining heterogeneous models.",
            "uuid": "e396.7",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Data augmentation / pseudo-annotation → Cross-domain NER",
            "name_full": "Data augmentation and pseudo-annotated synthesis for cross-domain NER",
            "brief_description": "Use of synthetic/pseudo-labeled data and augmentation strategies to mitigate scarcity in target-domain labeled data, enabling cross-domain NER transfer and boosting generalization.",
            "citation_title": "Data Augmentation for Cross-Domain Named Entity Recognition",
            "mention_or_use": "mention",
            "procedure_name": "Data augmentation and pseudo-annotation for cross-domain transfer",
            "procedure_description": "Generate additional labeled-like examples for the target domain by methods such as: (1) synthesizing sentences with inserted domain entities, (2) using models to create pseudo-labels on unlabeled target data (self-training), and (3) applying transformations (synonym replacement, back-translation) to existing labeled data. Use these augmented datasets to fine-tune or adapt models across domains.",
            "procedure_type": "data augmentation / semi-supervised learning",
            "source_domain": "data augmentation and semi-supervised learning (machine learning / NLP)",
            "target_domain": "cross-domain NER (various domain pairs: general→specialized or across specialized domains)",
            "transfer_type": "adapted/modified for new context (domain-aware augmentation and pseudo-label reliability mechanisms)",
            "modifications_made": "Design domain-specific augmentation (inserting domain entity types), reliability-based selection of pseudo-labeled examples, category rebalancing and hybrid pseudo-labeling to address class imbalance and noisy labels.",
            "transfer_success": "partially successful - these methods alleviate data scarcity and improve cross-domain NER robustness, but require noise control and careful augmentation strategies; quantitative gains depend on dataset and method.",
            "barriers_encountered": "Artificial data distribution mismatch, noisy pseudo-labels causing error propagation, and difficulty simulating realistic domain-specific context.",
            "facilitating_factors": "Availability of unlabeled target-domain corpora, generative models or heuristics to synthesize plausible sentences, and noise-robust training techniques.",
            "contextual_requirements": "Unlabeled target-domain text, heuristics or models for reliable pseudo-label generation, and validation data to control augmentation quality.",
            "generalizability": "High conceptually, but success depends on domain similarity and realism of synthetic data; more effective when augmented data reflect true target-domain context.",
            "knowledge_type": "procedural know-how (how to generate/highlight pseudo-labeled data) and explicit training pipelines for semi-supervised learning.",
            "uuid": "e396.8",
            "source_info": {
                "paper_title": "Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "rating": 2,
            "sanitized_title": "bert_pretraining_of_deep_bidirectional_transformers_for_language_understanding"
        },
        {
            "paper_title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "rating": 2,
            "sanitized_title": "biobert_a_pretrained_biomedical_language_representation_model_for_biomedical_text_mining"
        },
        {
            "paper_title": "Unsupervised Cross-Lingual Representation Learning at Scale",
            "rating": 2,
            "sanitized_title": "unsupervised_crosslingual_representation_learning_at_scale"
        },
        {
            "paper_title": "Explaining and Harnessing Adversarial Examples",
            "rating": 2,
            "sanitized_title": "explaining_and_harnessing_adversarial_examples"
        },
        {
            "paper_title": "Cross domains adversarial learning for Chinese named entity recognition for online medical consultation",
            "rating": 2,
            "sanitized_title": "cross_domains_adversarial_learning_for_chinese_named_entity_recognition_for_online_medical_consultation"
        },
        {
            "paper_title": "Fedner: Privacy-Preserving Medical Named Entity Recognition with Federated Learning",
            "rating": 2,
            "sanitized_title": "fedner_privacypreserving_medical_named_entity_recognition_with_federated_learning"
        },
        {
            "paper_title": "tmChem: a high performance approach for chemical named entity recognition and normalization",
            "rating": 2,
            "sanitized_title": "tmchem_a_high_performance_approach_for_chemical_named_entity_recognition_and_normalization"
        },
        {
            "paper_title": "ChemNER: fine-grained chemistry named entity recognition with ontology-guided distant supervision",
            "rating": 2,
            "sanitized_title": "chemner_finegrained_chemistry_named_entity_recognition_with_ontologyguided_distant_supervision"
        },
        {
            "paper_title": "Subsequence based deep active learning for named entity recognition",
            "rating": 2,
            "sanitized_title": "subsequence_based_deep_active_learning_for_named_entity_recognition"
        },
        {
            "paper_title": "Data Augmentation for Cross-Domain Named Entity Recognition",
            "rating": 2,
            "sanitized_title": "data_augmentation_for_crossdomain_named_entity_recognition"
        }
    ],
    "cost": 0.02488725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023
22 April 2024</p>
<p>Jun Yang juny@gznu.edu.cn 
School of Mechanical and Electrical Engineering
Guizhou Normal University
550025Guiyang, GuizhouChina</p>
<p>Taihua Zhang 
School of Mechanical and Electrical Engineering
Guizhou Normal University
550025Guiyang, GuizhouChina</p>
<p>Technical Engineering Center of Manufacturing Service and Knowledge Engineering
Guizhou Normal University
550025Guiyang, GuizhouChina</p>
<p>Chieh-Yuan Tsai cytsai@saturn.yzu.edu.tw 
Department of Industrial Engineering and Management
Yuan Ze University
32003TaoyuanTaiwan</p>
<p>Yao Lu yao.lu@gznu.edu.cn 
School of Mechanical and Electrical Engineering
Guizhou Normal University
550025Guiyang, GuizhouChina</p>
<p>Technical Engineering Center of Manufacturing Service and Knowledge Engineering
Guizhou Normal University
550025Guiyang, GuizhouChina</p>
<p>Liguo Yao lgyao@gznu.edu.cn 
School of Mechanical and Electrical Engineering
Guizhou Normal University
550025Guiyang, GuizhouChina</p>
<p>Technical Engineering Center of Manufacturing Service and Knowledge Engineering
Guizhou Normal University
550025Guiyang, GuizhouChina</p>
<p>Computer Science Information Systems
Engineering Electrical Electronic</p>
<p>Telecommunications JOURNAL OF BIOMEDICAL INFORMATICS</p>
<p>Evolution and emerging trends of named entity recognition: Bibliometric analysis from 2000 to 2023
22 April 2024E0EC5941DDC688994BCEB3BCB676CFE810.1016/j.heliyon.2024.e30053Received 3 January 2024; Received in revised form 16 April 2024; Accepted 18 April 2024 Publication titles Number of publicationsNamed entity recognition CiteSpace Natural language processing Bibliometrics
Identifying valuable information within the extensive texts documented in natural language presents a significant challenge in various disciplines.Named Entity Recognition (NER), as one of the critical technologies in text data processing and mining, has become a current research hotspot.To accurately and objectively review the progress in NER, this paper employs bibliometric methods.It analyzes 1300 documents related to NER obtained from the Web of Science database using CiteSpace software.Firstly, statistical analysis is performed on the literature and journals that were obtained to explore the distribution characteristics of the literature.Secondly, the core authors in the field of NER, the development of the technology in different countries, and the leading institutions are explored by analyzing the number of publications and the cooperation network graph.Finally, explore the research frontiers, development tracks, research hotspots, and other information in this field from a scientific point of view, and further discuss the five research frontiers and seven research hotspots in depth.This paper explores the progress of NER research from both macro and micro perspectives.It aims to assist researchers in quickly grasping relevant information and offers constructive ideas and suggestions to promote the development of NER.</p>
<p>Introduction</p>
<p>With the arrival of the information era, text data in various fields has grown exponentially.A lot of valuable professional information is covered in semi-structured or unstructured text recorded in natural language.How to mine these from massive text data information has become a research hotspot in various fields.It is usually time-consuming and error-prone to manually extract information from these data, so the method of extracting information from texts using artificial intelligence technology comes into being.</p>
<p>The concept of named entity (NE) was first used in the Message Understanding Conference -6 (MUC-6) [1], in which the main concerned entity categories are people, organizations, places, time expressions, etc. (general field).In a specific subject field, NE refers to the object of concern.For example, in biology, it refers to proteins, genes, diseases [2,3], and so on.In chemistry, it refers to compounds, solvents [4,5], and so on.NER is a crucial and essential task in text mining, which aims to identify the types and boundaries of NE.For the label sequence S =&lt; w 1 , w 2 , ..., w n &gt; of a given text, one or more triplet lists &lt; I S , I e , t &gt; are obtained after the NER model, each triplet contains one entity information, in the triplet, I s shows the beginning index position of the entity, I e shows the terminate index position of the entity, and t indicates the entity type.Its principal structure is shown in Fig. 1.Given a paragraph of text "Xiao Li Lives in Beijing, the capital of China."three triples are obtained by the NER model, &lt; w 1 ,w 2 ,Person &gt;, &lt; w 5 ,w 5 ,Location &gt; , &lt; w 10 , w 10 , Organization &gt; , where w 1 shows the beginning index position of entity "Person", w 2 means the terminate index position of the entity "Person", and "Person" is the entity type.NER not only plays a crucial role in the field of text mining but also in natural language processing (NLP) such as information retrieval [6], automatic text summarization [7], question answering system [8], machine translation [9], knowledge graph [10], etc. applications also play an essential role.</p>
<p>Early NER methods can be divided into two categories: rule-based methods [11,12] and machine learning-based methods [13,14].The rule-based method means that experts manually create rule templates, and then the entity is obtained by matching according to the rule template or lexicon.Although this method has high recognition accuracy, it has high labor costs and poor generalization ability.Some well-known rule-based NER systems include Lasie-II [15], NetOw1 [16], Facile [17], and FASTUS [18], etc.These systems are mainly based on manually customized semantic and grammatical rules to recognize entities.Machine learning-based methods transform the NER task into a sequence labeling task, which uses a large-scale annotated corpus to train the annotation model to tag each token in the text through the trained model.Then, the automatically tagged sequence is decoded according to the tagging scheme and integrated into the NE composed of several characters in the text [19].Machine learning algorithms can be divided into three categories according to whether the training dataset is labeled: supervised learning, semi-supervised learning, and unsupervised learning.Common machine algorithms include the Hidden Markov Model (HMM) [20], Maximum Entropy Model (MENE) [21], Support Vector Machine (SVM) [22], Decision Tree (DT) [23] and Conditional Random Field (CRF) [24], etc.In recent years, deep learning has proved to be an effective strategy to extract feature representations directly from text data, which has made breakthroughs in the field of NER.Compared with methods based on statistical learning, deep learning makes it easier to discover hidden features due to the characteristics of multi-layer nonlinearity [25].</p>
<p>Although NER has been developing for decades, there are few reviews in this field.In 2013, Marrero et al. [26] conducted an in-depth discussion on the application, evaluation methods, and different definitions of named entities of NER, with special emphasis on the research mainstream of machine learning-based and rule-based NER technology at that time.With the rise of deep learning technology, the NER field has experienced significant changes.Goyal et al. [27] provided a comprehensive overview of the development status of NER and classification technology and explored diverse technical paths from rule-based methods to unsupervised learning.Nasar et al. [28] conducted an extensive review of methods for NER and relationship extraction, highlighting the advantages of hybrid and joint models based on deep learning.Their research revealed the significant contribution of deep learning technology in improving recognition accuracy and processing complex entity relationships.Li et al. [29] focused on introducing the NER method based on deep learning by subdividing NER technology into a distributed representation of the input, context encoder, and label decoder.They not only demonstrate the impact of deep learning techniques in standardizing model structures but also systematically classify existing work.Previous research has deeply explored various aspects of NER technology, contributing important insights to the development of the field.However, most of them focus on evaluating specific technologies or methods, and this focused perspective rarely touches on the macro development trends of NER research.Likewise, there is a relative lack of comprehensive assessments on the evolution of research hotspots and cutting-edge technologies.</p>
<p>Bibliometrics can quantitatively analyze the advanced trends and research hotspots of the field based on published literature [30], and it is also an objective and scientific analysis method.Through this method, researchers can explore important topics and their interrelationships in the research field and deeply understand the process of knowledge sharing and diffusion, thus providing valuable insights for future research directions and policy formulation.For example, Yu and Pan [31] applied bibliometric methods to deeply Fig. 1. structural schematic diagram of NER.</p>
<p>J. Yang et al. explore and analyze the knowledge development process in the research field of Technique for Order Preference by Similarity to an Ideal Solution.Through a comprehensive survey of key literature transmission paths in citation networks, this paper reveals the knowledge diffusion model and its development trajectory over time in this field.Furthermore, it delves into the intricate knowledge structure and specialized research topics within the research community of this field.Yu et al. [32] analyzed literature related to intuitionistic fuzzy set theory through bibliometrics, which provided a macro perspective on the evolution of research in this field and vividly demonstrated the evolution of topics in this field.The knowledge diffusion path in this field was explored through the main path analysis of global and critical paths.These studies demonstrate the powerful application capabilities of bibliometrics in the field of scientific research and highlight its value as a scientific research tool.CiteSpace is a bibliometrics visualization software based on Java language [33], providing powerful tools to objectively reveal development trends and research hotspots in the scientific field.The software can analyze and visualize citation relationships, co-citation networks, and keyword co-occurrence networks in documents, thereby directly displaying mainstream research directions and key issues in the field [34].At the same time, through detailed visual display, CiteSpace can depict the evolution of the knowledge structure and the interaction of the research community in the NER field, providing a basis for further research.This is particularly important for NER, a multifaceted and rapidly developing field because valuable information and trends from a large amount of academic literature need to be extracted.In this context, this study is different from the previous analysis that mainly focused on specific technologies or methods and adopts the method of bibliometrics to analyze relevant documents in the Web of Science database.It not only discusses the overall trend of NER research, key research hotspots, and how they evolve over time from a macro perspective but also focuses on the research frontiers and related research hotspots in this field through in-depth analysis of relevant literature.Through in-depth mining and visual display of literature data, the broad layout of the research network in the NER field is depicted, including the distribution of leading institutions and countries/regions, providing a clear and objective perspective for research in this field.It aims to help researchers quickly grasp research frontiers and hotspots and provide constructive ideas and suggestions for promoting the development of NER.</p>
<p>The remainder of the paper is organized as follows: Section 2 introduces the data sources and research methodology, and Section 3 analyzes the number of published papers, research directions, and the distribution of journals.Section 4 analyzes the number of articles published by authors, institutions, and countries and their cooperation.Section 5 explores the research frontiers in the field of NER through the analysis of co-cited literature.Section 6 explores the research hotspots of NER by analyzing keywords.Finally, Section 7 summarizes the paper's results and presents ideas for further development.</p>
<p>Data source and methods</p>
<p>Data source</p>
<p>The data used in this study are obtained from the Science Citation Index Expanded (SCI-Expanded J. Yang et al. development process of the transition from traditional methods to deep learning methods.Therefore, we used "TS=Named entity recognition" as the search formula (search time May 2023) and selected the period from 2000 to 2023 to analyze the trajectory of rapid progress and key changes in NER technology during this period.The literature type was chosen as the article.After retrieval, 1913 literature was obtained, and the data was cleaned manually.After removing irrelevant literature, 1300 papers and text research data sources were finally obtained.</p>
<p>Research methods</p>
<p>The research uses bibliometrics and visualization methods based on relevant literature data by drawing "author cooperation", "institutional cooperation", "literature co-occurrence", "literature clustering", "citation burst", "keyword co-occurrence", and "keyword clustering" network maps, which intuitively and scientifically display the characteristics of documents and the development trend and research frontier of NER.The research framework of this paper is shown in Fig. 2.</p>
<p>Literature distribution characteristics</p>
<p>Analysis of annual publications</p>
<p>To some degree, the yearly publication quantity in this field can indicate its developmental progress.The number of papers published in the 1215 literature records obtained was statistically sorted out.The trend chart of the annual number of papers published in the past 22 years was drawn, as shown in Fig. 3.As the complete count of papers published in 2023 is unavailable, this year's publications are excluded from the figure.In 2005, the publication numbers reached a short-term peak, possibly related to the ACE Conference 2004.The ACE plan aims to extract the entities mentioned from natural language data and the relationships between these entities and their participation in events [35].At present, the NER model based on deep learning has become mainstream and has achieved good results, so the NER field has developed rapidly.Price's Law of Literature Growth states that at the early stage of the birth of an area, the growth of the number of related documents is in an unstable stage; when the field is in a period of rapid development, the number of documents grows exponentially; when the area is in a mature location, the number of documents grows relatively slowly.The mathematical model of the exponential curve is used to fit the number of publications, and the parameters of the curve are obtained by the least square method.The curve-fitting formula for the number of publications is finally accepted, as shown in Eq. (1).
y = 11.88 + 0.0069 × e x− 1991.44 2.88(1)
where y is the annual number of publications and x is the year.The degree of fit of the curve can be judged by the R 2 (coefficient of determination), and 0 ≤ R 2 ≤ 1, the closer the value is to 1, the higher the fitting reliability.The fitting curve R 2 = 0.977, indicates that the fitting reliability is high.The red line in Fig. 3 is the fitting curve.It can be seen that the number of publications in the NER field has increased exponentially since 2018, so NER is in a period of rapid development.</p>
<p>Research directions and journals distribution</p>
<p>Analyzing the research directions of NER can assist in understanding the background knowledge and basic disciplines involved in this technology.Through the function of analyzing the search results provided by the WOS database, the number of papers in each research direction involved in NER is obtained, as shown in Fig. 4.Among them, Computer Science Information Systems had the most significant number of papers, with 471 papers.The second direction was Computer Science and Artificial Intelligence, with 443 papers.They were followed closely by Computer Science Interdisciplinary Applications with 329 papers.Through the analysis of research Fig. 3. Annual publication numbers and its trend chart.</p>
<p>J. Yang et al. directions, it can be known that NER involves artificial intelligence, medicine, computer science, electrical electronics engineering, biology, biochemistry, and other fields.The distribution of journals can, to some extent, reflect the trend of NER research and the subject areas involved.According to the titles of publications, NER-related studies have been published in 507 journals, and the names of the eight journals with high article loads and their subject areas are summarized in Table 1.They are mainly published in IEEE ACCESS, JOURNAL OF BIOMEDICAL INFORMATICS, BMC BIOINFORMATICS, and other journals.From the perspective of the journal's field, NER's research is mainly related to computer science, medicine, biology, chemistry, and other disciplines, consistent with the research directions analysis results.The rapid development of NER technology in specific fields such as biology, medicine, and chemistry may be related to a large number of labeled databases, high-quality labels, wide data range, and high application value in these fields, which are conducive to the development of NER.</p>
<p>Analysis of cooperation</p>
<p>Analysis of author collaboration network</p>
<p>In order to more accurately track and analyze the annual research dynamics and development trends in the NER field and ensure the timeliness of the analysis, we selected the time-slicing unit as one year in CiteSpace, which means that the retrieved documents will be carefully divided according to each year.The node type "Author" represents the analysis of the number of documents sent by authors and the cooperation between them.In order to focus on the core authors and their cooperation models who are highly active and influential in the field of NER, we will provide a highly targeted and clear visual presentation and analysis basis.Set the node label "Threshold" to 5, which will display author labels with more than five published articles.Through this setting, the visualized knowledge map of the author's cooperation network reveals this field's core researchers and cooperation networks, as shown in Fig. 5.</p>
<p>In Fig. 5, the number of articles published by an author is represented by the node's size, where a larger node indicates that the author has published more articles.The thickness of the line between the nodes indicates the cooperation between authors.The thicker the line, the more frequent the collaboration.On the contrary, the narrower the line, the less collaboration.The node color indicates when the author published a paper, and the warm color indicates when the author published an article recently.The node size and line</p>
<p>Table 1</p>
<p>Number of papers in each journal and its subject fields. of the author, institution, and country cooperation network map have the same meaning.In the early stage, "Munoz, R" and "Li, YP" appeared with high frequency, and the links between each node were dense, and the authors cooperated closely.In the middle period, "Ananiadou, S" and "Xu, H" appeared more frequently, and the cooperation between the authors became closer.Recently, "Lin, HF" and "Qiu, QJ" appeared more regularly, but the cooperative relationship between the authors was relatively reduced.Table 2 counts the authors with more than eight publications.Among them, "Ananiadou, S" has published the most papers and is mainly engaged in the research of Mathematical Computational Biology and Biotechnology Applied Microbiology [36,37].The second is "Xu, H", who published 13 papers and mainly engaged in research on Health Care Sciences and Services Medical Informatics [38,39].It is closely followed by "Lin, HF" with 11 publications, mainly engaged in Computer Science and other research work [40,41].The recent larger node is "Qiu, QJ" The author published the first article in 2019 and has published eight articles so far.The author is mainly engaged in research in astronomy and astrophysics, geology, and other research work [42,43], indicating that the author has recently paid close attention to the field of NER.The method for academia to determine the core authors in a field can be calculated by Price's law, as shown in Eq. ( 2) [44].
M = 0.749 × (N max ) 1 / 2 (2)
where M is the threshold for judging the number of papers published by core authors, and those whose paper count is greater than this value are core authors, and N max is the highest number of papers published by author in this field.The number of core authors was 63, and 310 articles were published, accounting for 23.85 % of the total literature numbers, which was far lower than the conclusion proposed by Price's law that half of the papers were produced by high-productive authors, indicating that the scale of cooperation between authors was relatively small, and no core cluster was formed.Therefore, the cooperation between the authors or the author team should be strengthened.</p>
<p>Analysis of national and institutional cooperation network</p>
<p>There are specific differences in NER technology between languages, and the difficulties of NER for different languages are different.For example, in the NER technology of English and Chinese, the first problem the NER of Chinese faces is correctly segmenting the words in the text.English words have obvious boundary conditions, while Chinese boundary conditions are difficult to determine.Analyzing cooperation among various countries can promote the exchange and cooperation of NER technology in different languages and the development of NER technology.Yu et al. [45] explored the evolution of collaboration in the analytic hierarchy process research field through bibliometric methods, revealing the dynamic changes in collaboration between countries/regions and institutions and how these collaborations promote the sharing and diffusion of knowledge in the analytic hierarchy process field.Using this research as a reference, this article uses CiteSpace software to analyze national cooperation relationships.Optimizing the principles of map information density and clarity also ensures that the analysis can focus on countries with strong cooperative influence in the NER field, thereby effectively displaying the cooperation networks of these countries in the field.Set the node label "Threshold" to 12.The country labels with a published volume greater than 12 will be displayed.The national cooperation network knowledge graph drawn by this method (Fig. 6) counts the top ten countries with the most published articles and their betweenness centrality values (Table 3).It intuitively demonstrates the current status and characteristics of cooperation between countries in the NER field.Betweenness centrality is a measure of a node's centrality in a network.It equals the shortest paths from all vertices to all others that pass through that node [46].The larger the amount of data passing through the node and the more frequent the data transmission, the greater the influence of the node in the network graph and the more critical the node's position.Fig. 6 and Table 3 can be used to understand the strength of cooperation between countries and the development status of NER technology in various countries.Among the retrieved data, PEOPLES R CHINA published the most papers, reaching 563, with a betweenness centrality value of 0.32, indicating that NER technology is developing rapidly in China and attracting a high degree of attention.The second is the USA, with 521 publications and a betweenness centrality value of 0.30.This is followed by ENGLAND, with 88 publications and a betweenness centrality value of 0.27.In addition to the countries mentioned above, the development of other countries, such as Germany, India, and Japan, cannot be ignored, although the relatively small number of publications has made important contributions to specific NER technology fields, such as multilingual recognition, cross-domain applications, etc.This demonstrates the diversity and extensive collaboration in global research on NER technologies.The number of articles published by a country reflects the development of NER technology in the language used in that country.From the number of articles published in each country, it can be seen that research on NER technology in Chinese, English, and Arabic is significantly active.At the same time, we have also noticed that NER technology in other languages, such as Spanish, French, and German, is also developing rapidly.These languages show their uniqueness in the process of word embedding, and the integration of this feature in the pre-trained language model (PLM) helps enrich the model's understanding of the language, allowing the model to learn more features.These developments highlight the potential and wide range of applications of NER technology in adapting to global multilingual environments.</p>
<p>Analyzing the cooperation between institutions can help understand the leading institutions and mainstream research objects in the field.In order to focus on displaying the leading institutions with more than six publications in the field of NER and simplify the network map to highlight these major research centers and their cooperation models, the node label "Threshold" is set to 6. Through Fig. 6. map of national cooperation networks.</p>
<p>J. Yang et al. this setting, we can more clearly identify and analyze active institutions in the NER field and their cooperation networks and draw an institutional cooperation network map (as shown in Fig. 7).Table 3 lists the top ten institutions and their betweenness centrality.It can be seen from Fig. 7 and Table 3 that the institution with the most significant number of publications is Chinese Acad Sci (Chinese Academy of Sciences), with 34 publications, and its betweenness centrality value is 0.19, indicating that Chinese Acad Sci has a more significant academic influence on the field of NER; The second largest publication numbers is Harbin Inst Technol (Harbin Institute of Technology), with 29 publications, and the betweenness centrality value is 0.08; the third publication amounts is Dalian Univ Technol (Dalian University of Technology), with 25 publications, the betweenness centrality value is 0.01.It can be seen that most institutions cooperate closely.Still, most institutions are colleges and universities, so there needs to be more cooperation between schools and enterprises, and the number of papers published by enterprises is relatively small.Therefore, collaboration and exchanges between schools and enterprises should be strengthened to promote NER's more profound development and its application at the enterprise level.</p>
<p>Literature analysis</p>
<p>Literature co-citation analysis</p>
<p>Literature co-citation refers to the co-occurrence of two or more documents in reference to one or more other documents.Literature with many co-citations in the field is vital or core literature.The analysis of literature co-citation in the area of NER can explore the mainstream models and application fields of NER technology at various stages.Table 4 lists the top 10 co-cited literature and the year of publication, which are of great significance to the development of NER technology.The CiteSpace software is used to visualize the co-citation of literature.Based on the preliminary analysis of the frequency distribution of the data used, the aim is to balance the level of detail of the atlas with the readability of the overview.At the same time, in order to accurately highlight the widely cited and influential literature in the research field of NER.Set the node label "Threshold" to 6; document labels with reference frequency greater than six will be displayed.The document co-citation network knowledge graph drawn in this way (shown in Fig. 8) highlights the key documents that promote the progress of this field.The number of nodes in the figure is 1188.The links between nodes are 4940.The nodes' size represents the literature's co-citation frequency, and the larger the node, the higher the co-citation frequency.The color of the annual ring of the node is cold or warm, which means the year of publication; the cool color represents the year of publication earlier, and the warm color represents the year of publication later.The line between the nodes shows the closeness of the relationship between the two documents.Fig. 8 and Table 4 show that.The largest node is "Devlin J. (2019)", with 381 citations.It shows pre-training models' profound impact and breakthrough progress in NLP research and practical applications, especially BERT (Devlin et al., 2019) (Bidirectional Encoder Representations from Transformers).This trend marks a major shift from traditional rule-based and statistical methods to deep learning-based and large-scale pre-trained models, opening a new chapter in the field of NLP.The BERT model is based on the bidirectional coding structure of the Transformer [47], and task 1 is to randomly mask some words in the input text and then predict these masked words so that the model can learn the meaning of words in the context.Task 2, the "next sentence prediction" task, predicts whether the input two paragraphs of text are consecutive texts so the model can understand the relationship between sentences.The second node is "Lample G. (2016)", with 19 citations.This paper [48] proposes two NER models: a bidirectional LSTM combined with CRF to capture text's long-term dependencies and a transformation method that uses supervised and unsupervised word representation.The paper extensively uses character-level information in the NER task for the first time.This innovation provides new ideas for later processing of complex morphological languages (such as compound words in English).The third node is "Vaswani A. ( 2017)".This document [47] proposes the Transformer model, and its innovative attention mechanism (Self-Attention and Multi-Head Attention) marks an important turning point in the field of NLP.Compared with traditional convolutional neural networks (CNN) and recurrent neural networks (RNN), the Transformer model greatly improves the processing efficiency through parallel</p>
<p>Table 4</p>
<p>Top 10 co-cited literature and co-citation frequency.</p>
<p>processing and, at the same time, captures long-distance dependencies more effectively by focusing on different parts of the text, which is the cornerstone of promoting the development of models such as BERT, and provides new methods and technical paths for solving complex NLP tasks."Lee J (2020)" is the node with the highest citation frequency recently, with ten citations.This document [49] introduces the BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) model, which is a BERT model optimized specifically for biomedical text mining tasks.BioBERT significantly improves the performance of tasks such as biomedical NER and relation extraction, thanks to its pre-training on a large amount of biomedical domain text.This work demonstrates how to improve the application effect of the BERT model further in professional fields through pre-training in specific fields.It opens up a new path for NLP in specific fields.The thickness of the purple growth rings of a node represents its betweenness centrality.The thicker the growth rings, the higher the betweenness centrality of the node.The node with the highest betweenness centrality is "Leaman R (2015)".In the literature [50], the method of model combination is used to combine two independent machine learning models to create a chemically named entity recognizer, in which the two models have large differences, such as different feature sets and CRF parameters.The model's innovative nature has led to significant advances in NER tasks in the field of chemistry.</p>
<p>In addition to the high-frequency co-cited literature mentioned above, other sources also play an equally important role in developing NER.For example, Sang and De Meulder [51] provide a standardized evaluation framework and dataset for NER research, which has profoundly impacted standardizing NER assessment and advancing research in this field.Collobert et al. [52] propose an end-to-end deep learning approach for handling NLP multi-tasks, such as NER and part-of-speech annotation.This approach uses a unified neural network model, reduces the reliance on task-specific feature engineering, and lays the foundation for a multi-task NLP solution.Dai and Le [53] propose a learning strategy that combines unsupervised pre-training and supervised fine-tuning, which can effectively use large amounts of unlabeled data to improve the model's performance on specific tasks.This approach inspired later PLMs such as BERT.In addition, many other studies have made key contributions to the development of NER, including exploring different algorithms and models, applying NER in various languages and domains, and innovative approaches when dealing with complex entity types.Together, these studies have advanced NER technology, making it a key component of the field of NLP.</p>
<p>Cluster analysis of co-cited literature</p>
<p>Further clustering analysis of co-cited literature can explore the research frontier in the field of NER.The Log-Likelihood Ratio algorithm is selected as the clustering algorithm, which can effectively reflect the relationship between events.The main idea of the literature can be roughly summarized through the abstract, so the cluster label selection is obtained through the abstract.After clustering, 16 clusters are obtained, and the most significant 10 clusters are selected for display.Finally, the clustering network map of co-cited literature in the NER field is accepted, as shown in Fig. 9, modularity Q = 0.76 and silhouette S = 0.89.Q value was more significant than 0.3, meaning that the network clustering is obvious.The greater the value, the better the cluster obtained by the network.It is generally considered that S &gt; 0.5 clustering is reasonable and S &gt; 0.7 clustering is convincing.It can be seen that the matching relationship between the node and the cluster is high, the matching relationship with other clusters is low, and the clustering effect is good.Table 5 lists the size, average year, and cluster labels and their log-likelihood values for each cluster, with larger values indicating more representative labels.The largest cluster was the "#0 single-task" model, the cluster size was 172, the silhouette value Fig. 9. Co-cited literature clustering network map.</p>
<p>was S = 0.883, and the average year of the literature was 2014.This is followed by the "#1 Bert model"; the size is 110, the silhouette value is S = 0.799, and the average year of the literature is 2018.The third cluster is the "#1 protein name"; the size of this cluster is 101, the silhouette value is S = 0.926, and the average year of the literature is 2018.The emergence of the "#2protein name" cluster highlights the importance and value of NER technology in specific fields, especially in the biomedical field.Identifying protein names is of great importance in biomedical research because they are key to understanding biological processes, disease mechanisms, and drug effects.In biomedical literature, clinical reports, or research papers, proteins are a common entity type, and their accurate identification and classification are crucial for information retrieval, data mining, disease diagnosis, and biological research.With the development of NLP technology, the application of NER has been extended to various fields.In addition to biomedicine, the identification of compounds and elements in the field of chemistry, the identification of institutional and product names in the financial field, and the identification of component and process terminology in the manufacturing industry have all become important directions of NER research.The development of NER in these fields not only promotes the process of informatization and intelligence in related fields but also provides technical support for extracting and managing professional knowledge.</p>
<p>The "#0 single-task model", "#6 nested ner", "#7 word representation feature", and "#8 joint entity" in the cluster marks the key progress and research frontier of NER technology."#1 bert model" highlights the widespread use of PLM models such as BERT in NER tasks, demonstrating the core application of advanced models in improving text recognition and processing."#3 Chinese ner" and "#9 Arabic ner" demonstrate the special challenges and advances that represent NER technology in different language structures.In addition, the "#2 protein name", the "#4 electronic medical record", and the "#5 metabolite name" reveal the unique applications and development trends of NER technology in various professional fields.</p>
<p>The largest cluster group, "#0 Single-task model", means that in the development process of NER, the research of single-task models occupies an important position.This type of model focuses on performing a specific task, such as identifying a specific type of entity in text.This focus allows the model to learn more deeply and adapt to the characteristics of the specific task, thereby improving performance on the task.Compared with multi-task models, the structure of single-task models is usually simpler and easier to design and implement.This simplified design helps researchers focus on improving the model's performance on specific tasks without worrying about the complex interaction trade-offs between multiple tasks.The concise structure and focus on a single task of the single-task model enhance the interpretability of the model's decision-making process during the NER task.This clear and understandable feature not only makes the model an important focus in theoretical research but also promotes its in-depth analysis and understanding in empirical research.However, the main limitations of single-task models include that they are often unable to handle or generalize effectively to other types of tasks different from the training task.At the same time, because they focus on one specific task during design and training, these models cannot fully exploit the potential connections or common features between different tasks.While useful for in-depth learning of specific tasks, this focus ignores the importance of connections between different tasks when understanding text.For example, in the NER task, understanding context information or syntactic structure may be helpful to the NER task.With the advancement of technology, although multi-task and complex models gradually dominate, single-task models still have an important significance in the history and development of the NER field.The top 10 literature with the highest citation frequency in the cluster are shown in Table 6.In addition to Lample G. (2016) and Ma XZ (2016), Chiu and Nichols [54] proposed a NER model that  The key innovation of ELMo lies in introducing deep upper and lower cultural word embedding, which can generate dynamic word representation for the same word in different contexts.This innovation has greatly improved the performance of a variety of NLP tasks, including NER, emotion analysis, and question-answering systems.The emergence of ELMo has profoundly impacted subsequent NLP research, paving the way for developing more advanced PLMs such as BERT and GPT.The research frontier is the seed of scientific and technological innovation, which is of great significance to scientific research and economic development [56].Through careful analysis of the recent cluster and literature co-citation and literature, we summarized the research frontier of NER technology and tried to reveal the latest scientific exploration and technological breakthrough in this field.</p>
<p>Pre-trained language model (PLM)</p>
<p>PLM plays a key role in the development of the NLP field.These models learn language's basic structure and patterns through pretraining on large amounts of text data, thereby understanding natural language.BERT, as one of the representatives of the PLM, obtained the note through literature clustering, which indicates that the PLM is a research hotspot of NER at this stage.BERT is the first deep, two-way, unsupervised language representation, which uses only a large text corpus for pre-training and combines the context of each token [57].Further, fine-tune BERT through an additional output layer to apply to various downstream tasks, including NER.In addition to the flexibility of fine-tuning, BERT also has an outstanding ability to deal with rare or new words, which is particularly important in the NER task in specific fields (such as medical treatment, law, etc.).In addition, the multilingual version of BERT supports processing multilingual or cross-language NER tasks.Although BERT performs well in many aspects, it still has challenges processing long texts, such as large computing resource requirements, poor model interpretability, and great demand for fine-tuning data.These challenges have inspired various BERT improvements.For example, the RoBERTa proposed by Liu et al. [58] is an improved version of the BERT model.Optimize performance by increasing the amount of training data, using larger batches, and extending training time.The model eliminates the next sentence prediction task and introduces dynamic mask technology, allowing it to handle longer text sequences and improve understanding of complex structures.Although RoBERTa has improved in performance, its large model size may lead to increased difficulty in deployment, especially in environments with limited resources, and may face overfitting problems on small-scale datasets.Lan et al. [59] proposed ALBERT, an optimized BERT model, in order to solve the problems of memory limitation and longer training time when increasing the model.ALBERT uses two techniques: factored embedding parameterization, which reduces the model's size by decomposing the vocabulary embedding matrix.And cross-layer parameter sharing to reduce the number of parameters that increase with network depth.Like RoBERTa, ALBERT also removes the Next Sentence Prediction task.These innovations enable ALBERT to reduce the model's size and training duration significantly while maintaining a performance similar to that of BERT.Therefore, ALBERT is suitable for the fields with limited resources and provides valuable ideas for optimizing large-scale PLMs.However, although the model size is reduced through the parameter-sharing mechanism, this may also limit the model's ability to capture complex features.Furthermore, like other models, the interpretability of ALBERT remains a challenge.Recently, many researchers have fine-tuned or added other structures to perform NER tasks based on BERT and its improved series of models.Agrawal et al. [60] adopted a transfer learning method to deal with the challenge of nested named entity recognition (NER).Through joint label modeling technology, strategies such as fine-tuning, pre-training, and BERT-based language models were applied to solve this problem.Chen et al. [61] proposed a method based on the ALBERT model to extract entities from steel e-commerce data.Li et al. [62] pre-trained BERT on an unlabeled Chinese clinical record corpus and obtained a large pre-trained BERT model for Chinese clinical texts.</p>
<p>In addition to the BERT model, there are a series of advanced PLMs.For example, GPT (Generic Pre-trained Transformer) series models developed by OpenAI are unsupervised language representations based on deep self-attention mechanism, and the infrastructure is also a transformer.Unlike the BERT model, which focuses on improving the accuracy and depth of language understanding, GPT [63] performs better in generating tasks.At the same time, GPT uses a one-way (forward) attention mechanism compared to BERT's two-way attention mechanism, which makes the model architecture relatively simple.In addition, because of its generative nature, GPT is more flexible in dealing with open problems or generative tasks.Although GPT performs well in many aspects, it is still challenging in terms of high computational resource requirements when processing long texts, poor model interpretability, and large fine-tuning data requirements.For example, GPT-3 [64] can process longer text sequences, which helps the model understand more complex structure texts.However, its parameter scale is huge (175 billion parameters), which may lead to difficulties in deployment.In order to solve these challenges, researchers have been exploring ways to improve the efficiency of the GPT model, for example, by optimizing the model architecture, reducing the number of parameters, adopting more efficient training techniques, etc. XLNet [65] is an advanced PLM jointly developed by CMU and Google Brain.It is the first in-depth bidirectional language representation model that combines autoregressive and autocoding technologies.XLNet uses only large text corpora for pre-training and combines the context information of each tag at the same time.Like GPT, although XLNet performs well in all aspects, it is still a challenge in terms of computing resource requirements, handling long text, and fine-tuning.In addition to the models introduced in the appeal, there are many advanced models such as Transformer-XL [66], ERNIE [67], ELECTRA [68], etc.At this stage, the PLM, while achieving significant NLP capabilities, is also faced with the challenges of high demand for computing resources and environmental impact, as well as the problems of unfair and inaccurate model output that the bias of training data may cause.In addition, the lack of interpretability and limited generalization ability of these models are also the main problems.Many researchers J. Yang et al. are also using PLM, such as GPT [69,70] and XLNet [71,72], to achieve advanced performance on NER tasks.</p>
<p>Cross-language NER and cross-domain NER</p>
<p>The development of NER technology in various countries and document clustering labels show that NER technology has become a global research hotspot, and its application and research scope span multiple fields and multiple languages.The development of crosslanguage NER makes it possible to realize efficient NER by drawing on the data and models of high-resource languages when facing some low-resource languages that lack a large amount of training data.It can reduce the need to develop and train models for each language separately, saving time and resources.At the same time, it can also promote cultural exchanges, assist in analyzing and processing multi-language documents, and provide support for information extraction and data analysis on a global scale.However, cross-language NER research also faces a series of challenges.For example, there are differences in grammar, vocabulary, and cultural background between different languages, and different models perform differently in each language.It is a challenge to find a universal model suitable for multiple languages.On the other hand, model recognition of entities often depends on context, and texts in different languages may have different contextual structures and cultural meanings, which greatly increases the difficulty of the model's understanding of text when there is only a small amount of data for fine-tuning.In addition, the consistency and quality of annotations also affect the models' training and evaluation.Datasets in different languages may differ in entity definitions and annotation standards.For the above difficulties, the researchers proposed some solutions.For example, Google launched Multilingual BERT (M-BERT), which aims to handle the task of natural language understanding in multiple languages rather than a single language by pre-training a model with the same architecture as the BERT model on a large number of multilingual texts.Facebook AI proposed XLM-R (Cross linguistic Language Model RoBERTa) [73] for cross-language NLP tasks based on the RoBERTa model architecture.The model is pre-trained on texts in more than 100 languages and uses a self-supervised learning method that does not rely on the parallel corpus, so it can better handle the differences between languages and improve model performance in multiple languages.In addition to improving the PLM, other researchers have also realized the cross-language NER task through different methods.For example, Keung et al. [74] added language adversarial tasks when fine-tuning multilingual BERT, successfully improving the model's performance in zero resource cross-language environments.However, the paper did not delve into the potential limitations of adversarial training, such as adaptability in different language combinations or more complex language scenarios.Feng et al. [75] propose three innovative strategies to improve NER task performance on low-resource datasets: transferring knowledge from high-resource languages, expanding dictionary strategies, and integrating cross-language universal word level entity type features into neural network architectures.Although many scholars have explored cross-lingual NER tasks, some difficulties still have not been resolved.For example, existing models have insufficient generalization capabilities when dealing with new languages that are significantly different from the training data.In addition, the differences in grammar structure, vocabulary usage, and cultural background between different languages still challenge the model's adaptability.Future research may focus on developing general models that are more adaptable to different languages and cultures to address these challenges.This includes exploring effective ways to transfer knowledge from resource-rich languages to languages with fewer resources and using unsupervised learning techniques to solve the problem of insufficient annotation data, thereby improving the performance of NLP-related tasks.The cross-lingual NER task proposes solutions for entity recognition in specific languages or cultural backgrounds and has profound implications for developing NLP.</p>
<p>Cross-domain NER involves identifying and classifying entities in multiple fields (such as healthcare, law, finance, etc.) [76].Unlike traditional NER, cross-domain NER aims to develop a universal model that can adapt to text characteristics and entity categories in different fields.Research on cross-domain NER technology can more accurately extract key information from texts in different fields and provide support for various complex NLP applications.The difficulties in implementing cross-domain NER tasks include the model's ability to understand domain-specific knowledge in different fields, where the text may contain unique entity types and specialized terminology.Moreover, there may be significant differences in text style and structure in different fields, which poses a challenge to the model's generalization ability.Furthermore, some domains may lack sufficient annotated data to train effective NER models.Jia et al. [77] combine the transfer learning method and use cross-domain language models as a bridge to perform cross-domain and cross-task knowledge transfer, thereby solving problems such as resource limitations and domain adaptability in cross-domain NER tasks.Chen et al. [78] alleviate the problem of data scarcity in cross-domain NER tasks by using data augmentation methods such as pseudo-annotated data and data synthesis.Brack et al. [79] process data from different scientific fields simultaneously through multi-task learning methods, thereby improving the model's generalization ability.In addition to the above methods, other researchers [80,81] have also addressed the challenges faced by cross-domain NER tasks using different methods.Although there have been many studies and solutions for cross-domain NER tasks, there are still some challenges.For example, adaptability to highly specialized fields, changes within the field (new entity type terms may appear in some fields over time), small sample learning, etc. Future research may need to explore more efficient few-shot learning methods, develop more flexible model architectures, and improve domain adaptation techniques to improve the problem.</p>
<p>Nested NER and fine-grained NER</p>
<p>Nested named entities refer to entities that can contain or be embedded within another entity.For example, in the entity "Peking University", "Peking University" itself is an organizational entity, and the "Beijing" contained within it is a geographical location entity; this indicates that the same text fragment can be classified into multiple entity types.The traditional flat NER method cannot recognize overlapping or nested entities, but it often contains complex entity structures in medical literature, legal documents, scientific papers, and other texts.Katiyar and Cardie [82] pointed out that nested NE is quite common: 17 % of entities in the GENIA corpus are embedded in another entity; In the ACE corpus, 30 % of sentences contain nested entities.The development of nested NER technology can enable deeper analysis and understanding of text.The nested NER model has better understanding and processing capabilities when processing highly structured or specialized text.The complexity of nested entities makes nested NER more challenging and practical than traditional NER, so nested NER has become an emerging topic in NER tasks [83].In addition, determining the exact boundaries of each entity in nested entities is also a challenge, especially when the entities overlap and the context is ambiguous.Furthermore, when dealing with nested entities, the structure of the model is often more complex, which may result in higher computational complexity and lower processing efficiency.These challenges require continuous research and innovation in model design, data processing, and algorithm optimization to improve the performance and applicability of nested NER technology.From the perspective of model structure, standard mainstream methods for nested NER include early rule-based methods [84], which rely on the post-processing of rules like traditional NER methods.The Layer-based approaches [85] treat nested NER tasks as multiple traditional NER tasks and identify nested entities layer by layer.The Span-Based approach [86] solves the problem of entity boundary ambiguity by calculating the span representation of all sequences and then classifying them through local normalization scores.The hypergraph-Based approach [87] refers to using hypergraphs to represent the nested structure of entities in sentences and can represent and process complex entity relationships.The Transition-Based approach [88], inspired by the transformation-based parser, processes nested entities through sequential operations and is suitable for long sentences with complex structures.Recently, many scholars have addressed the nested NER problem by further improving mainstream methods.Geng et al. [89] proposed a novel planar sentence representation and bidirectional two-dimensional recursive operation, effectively solving the semantic dependency and entity boundary ambiguity problems in nested NER.This method can reduce the complexity of the model and improve the accuracy of entity recognition, but there may be a dependency on high-quality annotated data.Cui and Joe [90] proposed a pyramid hierarchical model based on a multi-head adjacent attention mechanism, which is used to fuse information from two adjacent inputs and better model the dependency relationship between entity spans.Chen et al. [91] improved the accuracy of entity boundary recognition and semantic dependency construction in nested NER by proposing a controlled attention mechanism, allowing the model to focus more effectively on task-related semantic features, thereby improving the model's performance and robustness.Although many researchers have addressed some of the difficulties in nested NER through various methods, some challenges still need to be addressed.For example, existing models still underperform when dealing with extremely complex nested structures, such as multiple nested or cross-nested entities.Secondly, many models perform well in specific fields, but their performance may decrease when applied to different types of text or across domains.Meanwhile, in low-resource languages, how to effectively identify nested NER is also a challenge.Therefore, future research on nested NER technology may focus on the following points.We will use weakly supervised and transfer learning techniques to reduce dependence on large amounts of annotated data and improve the model's adaptability in different fields.Explore nested NER methods that combine multimodal data such as text, sound, audio, and cross-language nested NER.Develop more efficient and lightweight nested NER models to meet real-time and large-scale data processing needs.</p>
<p>Fine-grained NER aims to identify and classify entities using more detailed and specific categories from text.Fine-grained NER focuses more on more profound and specific entity categories than traditional NER.For example, it identifies an entity as an organization and further distinguishes it from government agencies, educational institutions, commercial companies, etc.This requires the model line to understand the context more deeply to accurately classify close or similar entity types.At the same time, it will also face problems such as increasing entity categories, blurring entity boundaries, and fine-grained feature recognition.These challenges require that the fine-grained NER model not only needs strong language understanding ability but also can handle complex entity relationships and category segmentation.Rodríguez et al. [92] effectively address challenges such as blurred entity boundaries, inaccurate category recognition, and complex context interpretation in fine-grained NER by combining advanced text encoding technology, BiLSTM, CRF, and name-focused attention mechanisms.Wan et al. [93] propose a span-based multimodal attention network, which introduces a closed-loop mechanism to simulate human behavior to simultaneously and deeply mine multimodal information (span cell tag sequence and context information) existing in the text to capture the fine-grained interaction characteristics between them, thus improving the model performance.Wang et al. [94] use the method of distance-supervision, combined with flexible knowledge base matching and ontology-guided multi-type disambiguation technology, to effectively deal with the fine-grained NER problem in the chemical field.The performance of fine-grained NER can be improved through advanced disambiguation technology, combining NER with entity linking to enhance the understanding and classification of complex entities, and also by utilizing data synthesis, transfer learning, and other technologies to address issues related to data scarcity and imbalance.</p>
<p>Multimodal NER</p>
<p>Multimodal NER is a technology that combines text with information from other modalities (such as images, videos, sounds, etc.) for entity recognition.It analyzes the text's language features and uses information from other modalities to assist in the recognition and understanding of entities.For example, a multimodal NER system might combine visual cues in images and image description text to identify specific tasks or objects in images.When the context information is ambiguous, multimodal NER can more accurately identify entities that are difficult to determine in the text by combining multimodal data.In addition, when dealing with text containing complex scenes (such as social media content), multimodal information helps to better interpret and understand entities.When carrying out multimodal NER, the first problem is the high cost of multimodal data acquisition and annotation.Secondly, different modal data (such as text and image) may have great differences in feature representation, scale, and type, and how to effectively fuse these heterogeneous data.In addition, there may be noise inconsistency between different modes, which may affect the model performance.Yu et al. [95] realize the effective fusion of text and visual information by combining a unified multi-mode converter and an auxiliary entity range detection module.It has improved the problem of dealing with visual bias and modal interaction, thus improving the entity recognition rate in social media posts.Zhang et al. [96] proposed a multimodal graph fusion method to improve the effect of entity recognition in social media posts.By creating a graph structure that fuses text and visual objects, this method realizes deep semantic interaction inside and outside the mode and effectively integrates context and cross-modal content.The span-based multimodal variational autoencoder proposed by Zhou et al. [97] solves the difficulty of obtaining and labeling data sets and the noise problem.The reliance on large amounts of labeled data is reduced through semi-supervised learning, and the noise in the data is effectively handled through variational autoencoders.On this basis, future multimodal NER research will further explore cross-domain and cross-language adaptability to improve the generalization ability of models in different environments.At the same time, it will also focus on developing multimodal NER systems that adapt to real-time and dynamic environments, such as social media analysis and real-time news processing, to meet the growing demand for real-time data processing.</p>
<p>Few-shot NER</p>
<p>In order to solve the problem of limited annotation data and high annotation cost in specific fields or low resource languages and improve the generalization ability of models when facing new entity types and different fields, the few-shot NER technology came into being.Developing few-shot NER technology can reduce the model's dependency on a large amount of labeled data, thus lowering the cost associated with collecting and labeling vast amounts of data.In addition, few-shot NER allows the model to quickly adapt to new domains, which is particularly important in dynamic environments.Driven by a small amount of data, new methods, model architectures, and algorithms such as meta-learning and transfer learning have been developed.How to enable the model to learn from a small number of samples and effectively generalize to unseen entities, solve the negative impact of noise (such as error labeling) in a small number of samples on model performance, and deal with different language styles and entity types of domain texts are the primary challenges in few-shot NER.Wang et al. [98] introduced a data enhancement method to improve few-shot NER, which enhances model generalization and training effects by changing the prompt order.Chen et al. [99] propose a self-describing network that learns extensive knowledge through pre-training and then transfers to few-shot NER tasks.This approach uses universal concept descriptions to automatically map new entity types and identify entities adaptively.Das et al. [100] propose a comparative learning method to optimize the distribution differences between labeled entity representations.Gaussian embedding is used to display the distribution of modeling entities.In this way, the model can more effectively capture the label dependency, avoid the overfitting problem of the previous methods in dealing with O (non-entity) markers, and thus improve the model's performance in the small sample NER.Chen et al. [101] propose a prompt-based metric learning framework, which effectively solves the problem of tag scarcity and overfitting by combining tag awareness prompts and metric learning.In addition to the above methods, some researchers [102,103] explored different methods to improve the performance of NER tasks with few samples.In the future, the development of few-shot learning may need to study more complex and effective methods to encode and use context information to improve the recognition ability of models for complex entities.At the same time, we can use better data enhancement techniques and semi-supervised learning methods to expand the training data set according to the data situation.And develop lightweight models and computational efficiency optimization methods, especially in resource-constrained environments.</p>
<p>Analysis of burst literature</p>
<p>Important literature in the development process of the research field can be discovered through the burst function in CiteSpace software, which can be used to find the literature with the strongest citation burst.The literature has time characteristics, and its burst and blanking times can be known through this software to obtain the hotspot evolution and development track in this field.Burst literature refers to literature cited in a large number within a certain period [104].The top 25 documents with the strongest citation Fig. 10.top 25 literature with the strongest citation burst.</p>
<p>J. Yang et al. burst in the NER field are shown in Fig. 10.The blue line represents the time axis, the red line represents the period of the burst literature, and both ends of the red line represent the start and end time of each burst literature.The development of each period can be seen in this figure.For example, Kazama et al. [105] explored the application of SVM in biomedical NER.During this period, the methods of NER technology were mainly based on machine learning methods.Rocktaschel et al. [106] proposed an integration method that combines dictionary-based and grammar-based methods, effectively improving the accuracy and efficiency of extracting chemical entities from chemical texts.This method is of certain importance for specific application scenarios.Mikolov et al. [107] proposed two models for calculating word representation: the Continuous Bag of Words Common Bag of Words (CBOW) and Skip-gram models.This significantly simplifies and improves the computational efficiency of word vector representation.This document is one of the important documents that pushed the combination of deep learning and NLP tasks to the mainstream.Huang et al. [108] applied BILSTM and CRF models to sequence marking tasks for the first time, such as part-of-speech tagging (POS), blocking, and NER, significantly improving task performance and reducing dependence on word embedding.It lays a foundation for applying further in-depth learning in the NER field.</p>
<p>Keyword analysis</p>
<p>Keyword co-occurrence analysis</p>
<p>Keywords in literature are usually the concentration of an article, and keywords can reflect the core idea of the literature.Keyword co-occurrence refers to the number of occurrences of the same keyword in a group of documents, and the close and distant relationship between them is studied by counting the number of co-occurrences.Cluster analysis can classify the keywords with strong homogeneity into one category according to the affinity between keywords, making the cohesion between keywords in the same category stronger than that between keywords in other categories [109].Yu et al. [110] explored the knowledge structure in the field of Preference Ranking Organization Method for Enrichment Evaluations through the analysis of co-word networks, revealing the dynamic changes of the core themes and research directions in this field.This method demonstrates the effectiveness of co-word network analysis in significantly identifying and tracking the development trend of knowledge in the subject area.Based on this, this study uses this method for reference and keyword co-occurrence analysis to explore research hotspots in the field of NER in depth.The node type is selected as "Keyword", and other parameters remain in default settings.In order to enable the atlas to focus on the keywords that frequently appear in many kinds of literature and have significant importance and representativeness, optimize the information density of the atlas and ensure that important research trends and hotspots are presented in the atlas.After preliminary data exploration, we set the threshold to 26.This setting aims to emphasize the core and widely concerned research topics in the field of NER and map the knowledge map of the keyword co-occurrence network, as shown in Fig. 11.The number of nodes is 634, and the lines between nodes are 2907.The node size represents the number of keywords co-occurrences.The larger the node, the higher the number of keyword co-occurrences.The color of the annual ring of the node represents the year when the keyword co-occurrence, the cool color represents the year earlier, the warm color represents the year later, and the line between the nodes means the closeness of the two keywords.The high and low-frequency word demarcation values are calculated by Eq. ( 3), proposed by Donohue [111].
T = − 1 + ̅̅̅̅̅̅̅̅̅̅̅̅ ̅ 1 + 8I √ 2 (3)
where I is the number of words with a frequency of one, and T is the dividing frequency of high and low-frequency words.The number of keywords with a frequency of 1 calculated by CiteSpace is 395, and T = 28.61 is calculated according to the formula, Hence, those with a co-occurrence frequency greater than 29 are high-frequency keywords.The betweenness centrality, frequency, and year of occurrence of high-frequency keywords are obtained after processing the keywords with low correlation and similar relationships, as shown in Table 7.</p>
<p>It can be seen from Fig. 11 and Table 7 that, except for the subject words, keywords with high co-occurrence frequency in the NER field include "Natural language processing", "Extraction", "Text mining", etc.These are the downstream tasks of NER."Deep learning model", "Neural Networks", "Machine Learning", etc., are methods or techniques to conduct NER research.The recent high-frequency keywords include "Task analysis", "Attention mechanism", "Transfer learning", and "Feature extraction", which show that most NER research at this stage is based on these technologies.In addition to the high-frequency keywords, attention should be paid to the keywords "Multi-task learning" and "Adversarial training".</p>
<p>Keyword cluster analysis</p>
<p>After further clustering the keywords, 10 clusters are obtained, and the keyword clustering network map is shown in Fig. 12. Table 8 lists the size of each cluster, the mean year as well as the cluster label, and its cluster value.The largest cluster is "# adversarial learning", the cluster size is 94, the clustering modularity is 0.711, and the average year is 2019, indicating that this strategy is widely used in recent NER technologies and is an effective means to improve the performance of NER models.The second cluster is "# social media", the cluster size is 90, the cluster modularity is 0.695, and the average year is 2017.With the development of the Internet, more and more studies focus on NER in social media [112], which is challenging due to its informality and strong noise.The third cluster, "# biomedical literature", has a cluster size of 90, a cluster modularity of 0.769, and an average year of 2011, indicating that NER for biological texts is a long-term research hotspot.Based on keyword co-occurrence and keyword clustering analysis, the research hotspots in the field of NER can be summarized as follows.</p>
<p>Attention network</p>
<p>The attention mechanism was first proposed in computer vision, which can make neural networks pay more attention to the valuable information in the input and reduce the attention to irrelevant information.Just like when people look at pictures, they tend to pay more attention to the content they are interested in.Generally speaking, the attention mechanism is divided into two steps: calculating the attention distribution on the input information and calculating the context vector according to the attention distribution [113].The attention mechanism enables the model to focus on key parts of input data to better understand the context.In NER, it is important to understand the context around an entity because it can help distinguish between entity and non-entity terms.When dealing with long-distance dependency problems, the attention mechanism can effectively capture the dependencies between long-distance words, which is particularly important for identifying entities that span multiple words.At the same time, the attention score provides a way to explain model decisions, which can show the most concerned part of the model when identifying entities.Combining the attention mechanism with other deep learning technologies (such as LSTM, RNN, etc.) can improve the model's overall performance.In the research of attention mechanism, the main challenges include how to allocate attention accurately and efficiently in key parts of the model, especially in the face of many possible concerns to ensure the accuracy and efficiency of attention.When dealing with long sequences, how to effectively maintain the concentration and distribution of attention and avoid paying too much attention to distractions or focusing on irrelevant parts.In addition, when using the multi-head attention mechanism, optimizing the role of each head and integrating their output to improve the overall performance are key issues.These challenges highlight the complexity and nuances of applying attention mechanism to practical problems, which need in-depth research and innovative methods.Bahdanau et al. [114] used attention in NLP tasks for the first time to extract important information in sentences by giving different weights to words.With the attention mechanism demonstrating superior performance in NLP tasks, various attention mechanisms have been proposed to enhance this capability.Xu et al. [115] introduced Hard Attention, which focuses on specific parts of the input sequence to improve computing efficiency and model interpretability.However, it may face certain challenges in the training process and risk missing other important information.Vaswani et al. [47] proposed the transformer architecture, which is completely based on the self-attention mechanism and effectively handles long-distance dependencies while achieving higher parallelism.In addition, a multi-head attention mechanism is used, allowing the model to focus on different parts of the sequence on different "heads" simultaneously, thereby capturing various relationships and patterns in the data.In addition to the above attention mechanisms, many researchers improve the accuracy of NER tasks by exploring other attention mechanisms or integrating attention mechanisms into the model.Zhang et al. [116] proposed a part-of-speech attention mechanism to obtain the contribution weight of part-of-speech to entity recognition.Lin et al. [117] applied the attention mechanism to character and word level information, respectively, and proposed a neural network model that relies on hierarchical attention to achieve sequence tagging.Xu et al. [118] proposed an attention-based neural network architecture, which relieves context dependency by using document-level global information obtained from documents represented by a pre-trained bidirectional language model with neural attention.Although the attention mechanism has been widely used, the transparency and interpretability of its decision-making process still need to be  improved.In addition, using or improving the attention mechanism to deal with long texts more effectively also requires more effort.At the same time, explore the integration of different types of attention mechanisms and use their respective advantages to improve the model performance.</p>
<p>Multi-task joint learning</p>
<p>The core idea of multi-task joint learning is to train a model to perform multiple related tasks simultaneously rather than train a model for each task separately.The main advantage of this method is that it can enable the model to learn shared representations and features from multiple tasks, thus improving the performance of each task and the overall efficiency of the model.In NER, joint entity relationship extraction, as the representative of multi-task joint learning, allows the model to simultaneously identify entities in the text and judge the relationship between these entities, thus improving the accuracy and efficiency of information extraction and contributing to a deeper understanding and analysis of text content.Joint entity relationship extraction [119] refers to the joint process of entity recognition and relationship extraction.The joint learning method considers the potential dependency between the two tasks, thus using rich contextual information [120].In the traditional way of extracting triple groups (entity 1, relationship, entity 2), NER and relationship extraction are executed independently, called the pipeline model [121].The pipeline method is simple and flexible, but it ignores problems such as low-level interaction and error propagation [122].The joint model is usually more efficient than sub-step processing because it reduces the complexity of the processing process.However, building a complex model that can simultaneously handle entity recognition and relationship extraction is necessary when extracting joint entity relationships.At the same time, data in this area is relatively scarce in many fields.Moreover, it becomes more difficult to deal with text with complex structures and diversified entities or relationship types.Zhao et al. [123] proposed a method based on a heterogeneous graph neural network, which can accurately capture the dependencies of entities and their relationships by representing iterative fusion technology and effectively dealing with entities and relationships in long texts.Chen et al. [124] used the location-aware attention mechanism and relationship embedding method to solve the problem of overlapping triples in joint entity and relationship extraction.This method improves the model's ability to deal with complex relationships by accurately identifying the location of entities and enhancing the understanding of relationships between entities.Wan et al. [93] conducted an in-depth analysis of multimodal information in the text, such as span cell label sequences and context information, and developed a multimodal attention network and a modal attention enhancement module to jointly model this information, aiming to capture the fine-grained interaction characteristics between entities and their relationships.In addition to the methods used in the above papers, some other researchers [125][126][127] use different strategies to improve the performance of the joint entity relationship extraction model.In addition to joint entity relationship extraction, the NER task is usually combined with the POS tagging task [128].Combining NER with POS tagging can significantly enhance the model's understanding of words' grammatical roles and boundaries and improve the accuracy and precision of entity recognition in the context.This combination uses part of speech information to optimize entity identification and classification, especially when identifying entities in complex sentence structures.Combining NER and semantic role labeling can significantly improve the model's entity recognition ability in complex contexts.An in-depth understanding of entities' semantic roles and context relationships can enhance the understanding of sentence meaning and accurate recognition of relationships between entities.In addition to the above combinations, it is combined with syntactic dependency analysis, sentiment analysis, and text classification to improve the accuracy of each task.Although multi-task joint learning can bring significant improvement, it also faces a series of challenges.For example, task relevance, not all tasks contribute to common learning, and incorrect task combinations may lead to performance degradation.There may be conflicts between different tasks, and the optimization of one task may have a negative impact on another task.Balancing various tasks is also a key issue.It is necessary to ensure that no single task dominates the learning process.Future developments in multi-task joint learning may focus on improving the effectiveness of task combinations, such as developing advanced algorithms that can automatically adjust the learning process based on the relevance and complementarity of tasks.Optimize resource usage, such as developing dynamic resource allocation mechanisms, exploring adversarial training and regularization techniques to enhance the generalization and robustness of the model, etc.</p>
<p>Transfer learning</p>
<p>In most fields, tagging data is often insufficient, and tagging costs are high.Transfer learning can solve the problem of insufficient tagging data to some extent.Transfer learning [129] refers to applying the knowledge learned in the source task domain to machine learning tasks in the target domain.Transfer learning is an effective method for low-resource corpus and cross-domain learning.In the NLP field, there are generally two types of transfer learning.One is feature-based transfer, mainly represented by word2vec [107].The other is fine-tuning.The whole pre-trained model is carried to the downstream task for fine-tuning, mainly represented by BERT.At this stage, the focus of transfer learning is mainly on fine-tuning.However, the following points need to be considered when conducting transfer learning.One core of transfer learning is effectively applying the knowledge learned from one field to another.The two fields may have significant differences in data distribution, feature space, or task objectives, making direct transfer ineffective.In addition, when the difference between the source domain and the target domain is too large, it may lead to negative transfer; that is, the knowledge of the source domain may not only be unhelpful but also harm the performance of the target domain.It is also a key point in determining which features are shared between the source and target domains and which are domain-specific.Peng et al. [130] employ a language model based on BiLSTM as part of their transfer learning approach.This model is initially trained to extract features and structures from a large corpus of unlabeled text data.These learned linguistic patterns are then adapted and applied to the task of NER.Gligic et al. [131] pre-trained the model on many unlabeled electronic health records to capture rich linguistic features and context information and then applied these embeddings to the network architecture.Through this transfer method, the information in a large amount of unlabeled data can be effectively used to improve the model's performance.Yu et al. [132] used BERT to conduct pre-training on large-scale text corpora and the medical department's deep-level, context-based two-way language representation.Then, they used the output of BERT as the input of the subsequent neural network model.Yao et al. [133] adopted transfer and active learning methods to address the scarcity of labeled data by learning from public source datasets to knowledge transfer to fine-grained mechanical NER.Transfer learning is a rapidly developing field, and future research may continue to explore how to transfer knowledge more effectively between different fields or modalities, such as transferring from text to images or across various types of datasets.Additionally, developing technology that can automatically identify the optimal transfer strategy could reduce the need for manual adjustments and extensive experimentation.</p>
<p>Adversarial training</p>
<p>Adversarial training [134] is a technology used to enhance the robustness of machine learning models, especially for deep learning models.It trains the model by introducing and using adversarial samples to improve the model's resistance to small, deliberately created disturbances in the input data.Through training models to deal with diverse inputs, adversarial training can enhance the generalization ability of models to unseen data.In addition, adversarial training increases the diversity of training data by creating adversarial examples, and in some cases, not only improves the robustness of the model but also improves its performance.For security-sensitive applications, such as fraud detection, adversarial training can improve the stability of the model in the face of malicious operations.While adversarial training brings excellent performance, other factors must be considered.For example, adversarial training requires an additional computational burden in generating and training on adversarial training examples.At the same time, consideration should be given to finding a balance between enhancing robustness and maintaining high performance when conducting adversarial training.Excessive adversarial training may lead to a decline in model performance on conventional data.In addition, during the adversarial training process, if the adversarial sample generation method is too simple or is too different from the real data distribution, it may cause the model to overfit the adversarial sample.In NER, adversarial training not only improves the robustness and generalization ability of the model but also helps the model learn to recognize semantically complex or ambiguous entities, such as entities with ambiguity or context dependence.Inspired by this technology, some studies improve the performance and robustness of NER through adversarial training.For example [135], proposed a cross-domain adversarial learning method, which guides the model to learn the shared information between two tasks (Chinese electronic medical record text and NER in online medical consultation text) by combining the adversarial mechanism into multi-task learning, thus significantly improving the recognition performance and robustness of the model on complex and diverse text data.Park et al. [136] performed NER in the automotive field by combining adversarial training and multi-task learning.Adversarial training trained the model to recognize terms from both the general and automotive domains, thereby avoiding overfitting in a single domain.Multi-task learning is applied to handle NER and word spacing prediction tasks simultaneously.Wang et al. [137] increased the adaptability of the model to data by adding disturbances to the key variables of the model.The purpose of this adversarial training is to improve the generalization ability and robustness of the model, reduce the risk of overfitting, and improve the model's performance in processing diversified input data.In the field of NER, adversarial training has shown its potential and effectiveness.Future research and development may focus on using adversarial training to improve the capability of the NER system in dealing with complex entity structures (such as nested entities) and cross-domain and cross-language adaptability.In addition, it may also focus on improving the NER system's ability to deal with low-resource language and informal text, enhancing the system's security and resisting adversarial attacks.</p>
<p>Deep active learning</p>
<p>Active learning [138] is a method committed to studying how to obtain more performance gains as much as possible through less labeled data.Specifically, iterative unlabeled data sets select appropriate samples for labeling to reduce labeling costs.However, classical active learning methods are difficult to deal with in terms of high-dimensional data [139].Deep learning performs excellently in processing high-dimensional data and feature extraction, and active learning can effectively reduce the annotation cost.Therefore, combining deep learning with active learning provides an effective way to train efficient models when data annotation resources are limited.In NER, the main advantage of deep active learning is that it can significantly reduce the need for high-quality annotated data, thereby reducing annotation costs and time.At the same time, the accuracy and adaptability of the model are improved by selecting the most effective samples for model improvement, especially in data scarcity or domain-specific scenarios.However, when performing active learning, selecting unlabeled samples to best improve the model's performance requires a precise sample selection strategy.At the same time, high-quality annotation may still rely on experts in specific fields, especially in professional fields.In addition, processing a large number of unlabeled data to determine its information content may lead to higher computing costs.In recent years, many scholars have achieved excellent results in the field of NER by combining deep learning and active learning.For example, Agrawal et al. [140] used the minimum confidence sampling strategy based on uncertainty to solve the sample selection problem.This strategy considers the uncertainty of the model's most likely label for each instance and calculates the number of uncertain words in the sentence.At the same time, the corpus is used to label the selected samples directly.Li et al. [141] combined the uncertainty-and diversity-based sampling method with the BERT-BiLSTM-CRF model to alleviate the problem of insufficient annotated data.Among them, uncertainty sampling selects instances with uncertain labels, while diversity sampling increases data diversity and selects instances with large context differences.Radmard et al. [142] proposed a sequence-based active learning method to improve the efficiency of sample selection in the NER task.This method not only considers the uncertainty of the whole sentence but also focuses on the subsequences in the sentence, allowing the query and annotation of subsequences with high uncertainty.In the field of NER, future deep active learning will focus more on further developing subsequence-based annotation methods to improve the utilization of annotation data and reduce the labor and time costs required for annotation.At the same time, it explores integrating the latest deep learning models (such as PLMs) and technologies into the deep active learning framework and developing more accurate sample selection algorithms.</p>
<p>Federal learning</p>
<p>Federated learning [143] is a distributed machine learning method whose core is to allow multiple devices or servers to collaborate on data learning while protecting data privacy and security.Compared with traditional machine learning methods, federated learning can not only protect data privacy and reduce dependence on centralized data storage but also enable distributed collaborative learning across multiple devices.Driven by privacy protection and data security, the application of federated learning in NER has gradually become a research hotspot.For those NER applications involving sensitive data, such as healthcare, financial services, or legal documents, federated learning provides a way to protect personal privacy and sensitive information while allowing learning to be performed from this data.In addition, sharing models without sharing data between different institutions or fields allows the NER system to learn from a wider range of data, thereby improving the model's generalization ability and accuracy.However, when conducting federated learning, it is necessary to consider ensuring data privacy and security in the process of distributed data processing, solving the heterogeneity problem of data distribution on different clients, improving communication efficiency in the model update process, and reducing Broadband and resource consumption.Developing federated learning suitable for NER requires careful consideration of data distribution, model design, privacy protection, communication optimization, and other aspects.Wu et al. [144] use knowledge distillation technology to achieve communication-efficient federated learning, using smaller mentee models and larger mentor models to learn from each other.Small models perform personalized learning on their respective clients while reducing communication costs through the dynamic gradient method based on singular value decomposition.Wang et al. [145] proposed a cross-platform data distillation and processing method for heterogeneous label sets to train global NER models.This method combines the sequence-to-sequence NER framework and prompt tuning technology to reduce communication costs and improves through the distillation of pseudo-complete annotations (data contains all possible entity type annotations, not just the entity types already in the local dataset) Identification of unlabeled entity types.Ge et al. [146] divided the model into private modules that focus on local characteristics and shared modules that capture general knowledge of the platform, and then shared model gradients rather than original data among various medical platforms to maintain privacy, thereby improving the generalization ability and accuracy of the NER model.The research of federated learning in NER may need to explore encryption and privacy protection technologies further, such as homomorphic encryption and differential privacy.At the same time, the algorithm and processing strategy are optimized to deal with the efficiency problem in large-scale heterogeneous data environments, especially in real-time updates and dynamic changes of medical data.</p>
<p>Distance-supervision and weakly-supervised learning</p>
<p>Distance-supervision and weakly-supervised learning are two machine learning paradigms that aim to solve the problem of scarcity of annotated data.The distance-supervised learning method automatically labels training data using existing knowledge bases or external resources.In NER, the application of distance-supervised learning significantly improves efficiency and professionalism.For example, using existing knowledge bases in the medical field, medical terms in text data can be automatically annotated.This method not only reduces the need for manual annotation but also automatically identifies and accurately annotates entities in specific fields (such as medical and legal) through a professional knowledge base.In addition, distance-supervised learning can generate a large amount of diverse training data, thereby improving the model's generalization ability.There are several key points when applying distant supervised learning in NER.First, processes that rely on automated annotation in existing knowledge bases may generate erroneous or inaccurate labels, introducing noise that impacts model performance.Secondly, distance-supervised learning may have difficulty correctly handling entity ambiguities in context during automatic annotation, especially when the same words or phrases represent different entities in different contexts.In addition, since the knowledge base may cover some entity categories more extensively than others, it may lead to category imbalance in the data set, further affecting the model's ability to identify rare entity categories.Li et al. [147] proposed a self-training framework for category rebalancing.By designing flexible category thresholds and using hybrid pseudo-labeling technology, the category imbalance problem of NER under distance supervised learning is improved.Zhou et al. [148] proposed a distance-supervised learning NER method, which uses an external knowledge base to generate labels automatically and a reliability-based learning strategy to reduce false negative samples generated by incomplete labels.Meng et al. [149] automatically generate training data by matching entity mentions in the original text with entity types in the knowledge base.A noise-robust learning scheme is proposed to solve the problem of incomplete and noisy labels, including a new loss function and steps to extract noisy labels.Although distance-supervised learning has made significant progress in the field of NER, there are still some challenges.For example, for higher-level semantic understanding, distant supervised learning usually relies on surface-level text matching and simple rules, which makes it difficult to handle complex semantic understanding and reasoning tasks.Moreover, in an environment of dynamically changing data sources and updated knowledge bases, the real-time learning and adaptability of distance-supervised learning models need improvement.</p>
<p>Weakly-supervised learning is a method of training machine learning models using incomplete, inaccurate, or inconsistent labeled data.This approach often relies on heuristic rules, labeling functions, or the integration of multiple imperfect labeling sources.In NER, weakly-supervised learning can effectively utilize a large amount of unlabeled or partially labeled data and reduce reliance on manual labeling.This method enables the model to quickly adapt to new entity types and changing domains by integrating information from external knowledge bases, rules, or heuristic algorithms.It is especially suitable for professional or low-resource language scenarios.Although weakly-supervised learning reduces the requirement for large amounts of annotated data, data diversity and coverage must be ensured to avoid model bias and overfitting.At the same time, weakly-supervised data may contain some errors or inconsistent labels, requiring effective noise processing mechanisms, such as noise filtering or correction strategies.Fries et al. [150] used medical ontology as the source of annotation heuristic rules and adopted a weakly-supervised learning method to train a medical entity classifier.Correct label noise by modeling the accuracy of each ontology and rules to improve model performance.Zhang et al. [151] improved the problems of insufficient label coverage and text noise by combining category description, keyword, and network structure analysis and using weakly-supervised learning methods with hierarchical structure information within the text.In addition, they also implemented a self-training strategy, which effectively enhanced the model's ability to handle complex, multi-faceted tasks, including improving the processing of labeled data.In order to solve the difficulties in weakly-supervised learning in existing NER, the following points may be the focus in the future.First, the model's adaptability in multiple languages and domains should be improved, and the data diversity brought by globalization should be responded to by developing cross-language and cross-domain transfer learning technologies.The second is to focus on data enhancement and simulation data generation and use technologies such as the generative adversarial network to make up for the lack of labeled data and enhance the robustness of the model.The third is to improve the processing method of noise labels and adopt a more accurate noise detection and correction mechanism to optimize the model's performance in complex data environments.</p>
<p>In addition to the above methods or strategies, other advanced technologies and methods have received increasing attention in NER research.For example, self-supervised learning utilizes large amounts of unlabeled data to learn useful feature representations.As an effective pre-training strategy, this method has been proven to significantly improve the performance of NER.In few-shot learning scenarios, meta-learning demonstrates its ability to quickly adapt to new tasks.By learning how to learn efficiently, meta-learning enables models to quickly adapt to new tasks or environments with limited data, which is especially important in fields where data is scarce.Additionally, incremental learning is another important approach that allows models to gradually adapt as they receive new data or face new tasks without completely retraining each time.This strategy is particularly effective in dealing with changing data environments because it keeps the model flexible and adaptable.Model compression and distillation techniques are becoming increasingly important in the NER field.These techniques reduce the size of large models, making them more suitable for environments with limited computing resources while maintaining or improving model performance.In summary, NER research is constantly evolving towards more efficient, adaptable, and resource-efficient directions, and these advanced methods and strategies manifest this trend.</p>
<p>Summary and prospect</p>
<p>This paper uses the literature in the field of NER obtained from the Web of Science core collection database as the data source.The following conclusions are drawn using CiteSpace software to comprehensively analyze NER's research status, existing achievements, research cooperation, research frontiers, and hotspots from macro and micro perspectives.The superior performance of deep learning in NER research has made the field of NER develop rapidly.According to the trend of the number of documents issued, the NER field is in a period of rapid development at this stage.From the perspective of research directions and journal distribution, NER research mainly involves computer science, medicine, biology, chemistry, and other disciplines, which shows that NER has interdisciplinary and cross-field common components.This multidisciplinary intersection has brought new application scenarios and research perspectives for developing NER technology, such as precise identification of biomedical named entities and entity identification of compound reactants.From the perspective of the cooperation between authors and the number of papers published, the core authors in the early stage of NER development include Munoz, R, Li, YP, and other authors, and the cooperation between those authors is close.In the mid-term, with Ananiadou, S, Xu, H, and other authors as the main body, the cooperation has become more intimate.Recent highly productive authors include Lin, HF, Qiu, QJ, and others.There are 63 prolific authors in the field, but the cooperative relationship between authors needs to be strengthened.This reminds us that deepening academic exchanges and cooperation not only contributes to knowledge sharing but also stimulates new creativity and technology integration, further promoting the innovation and development of NER research.From the perspective of the number of publications and cooperation among countries, countries with a higher volume of publications include PEOPLES R CHINA, the USA, ENGLAND, etc.The cooperation between countries is close.The number of publications in a country reflects, to a certain extent, the development level of NER technology in the language used in that country.The NER technology research in languages such as Chinese, English, and Arabic is significantly active.Meanwhile, we have also observed that other languages, such as Spanish, French, and German, are rapidly developing in NER technology.The cooperation between these countries shows the important role of international cooperation in promoting the global development of NER technology.In addition, the active research on multilingual NER technology reflects the urgent need to process multilingual information in the context of globalization.Encouraging international cooperation can accelerate technological progress and help promote information understanding and exchange in different languages and cultural backgrounds.From the perspective of inter-institutional cooperation and publication volume, the institutions with higher publication volume include Chinese Acad Sci, Harbin Inst Technol, and Dalian Univ Technol.The cooperation between institutions is mainly focused on the cooperation between universities, with less cooperation between schools and enterprises and less publication by enterprises.In the future, exploring and promoting cooperation models between universities and enterprises is expected to bring new opportunities for the application and industrialization of NER technology.The business community's demand for practical application of NER technology can provide rich practical scenarios for academic research.At the same time, the latest research results from academia can help companies solve technical challenges and rapidly transform and apply technology.</p>
<p>Based on the co-citation frequency of literature, the mainstream model BERT proposed by Devlin J. (2019) has made significant contributions to the development of NER.Lample G. (2016) paper extensively uses character-level information in NER tasks for the first time.This innovation provides new ideas for later processing of complex morphological languages (such as compound words in English).Vaswani A (2017) proposed the Transformer architecture, and its innovative attention mechanism marked an important turning point in the field of NLP.In addition to the high frequency of co-cited literature mentioned above, other literature provides important methods and strategies for developing NER.The literature cluster analysis concludes that the research frontiers of NER include PLM, cross-language and cross-domain NER, nested and fine-grained NER, multimodal NER, few-shot NER, etc. Pre-trained language models such as BERT and its variants significantly improve the machine's ability to understand natural language by leveraging large amounts of text data to learn the deep features of the language.This progress not only brings a qualitative leap to the NER task but also provides new tools and methods for the entire field of natural language processing, especially in processing language context and understanding complex relationships.The progress of cross-language and cross-domain NER technology enables machines to better transfer and apply knowledge between different languages and fields, breaking down the barriers of language and professional knowledge and bringing new possibilities for global information sharing and knowledge management.Nested NER focuses on identifying mutually contained or overlapping entities, such as simultaneously annotating diseases and their related symptoms in the medical literature.Fine-grained NER strives to distinguish nuanced entity categories, such as further subdividing "organizations" into "non-profit organizations," "government agencies," etc.At the same time, fine-grained entity recognition and classification will promote the construction of a richer and more accurate knowledge map and provide basic support for developing the semantic web, intelligent search, and other technologies.The development of modal NER enables machines to more comprehensively understand and process data containing non-text information such as images and sounds, providing a new perspective for social media analysis, multimedia content management, and other applications.Research on a few-shot NER directly addresses the problem of data scarcity, enabling NER technology to quickly adapt to new fields or low-resource languages.With the in-depth development of these technologies, they are expected to have an important impact on intelligent search, personalized recommendation, intelligent assistants, and other fields.Although the pre-training model, cross-language and cross-domain NER, nested and fine-grained NER, multimodal NER, and small sample NER technologies have made significant progress in the field of NER in recent years, which has promoted the machine's ability to understand natural language and its application scope to expand significantly, especially in understanding language context, complex relationship processing, and multilingual information processing.However, there are still areas to be explored.Future research areas include but are not limited to: For the pre-trained model, how to further optimize its performance in processing long text, computational efficiency, and model interpretability, such as exploring innovative methods of model compression and interpretability mechanisms; In terms of cross language and cross domain NER, we will deeply study how to effectively deal with low resource languages with complex structure and changeable syntax through new in-depth learning methods, and how to better adapt and migrate the model to different professional fields, especially highly specialized fields; The main challenges for nested and finegrained NER include accurately identifying and classifying intricately intertwined fine-grained entities in text, and understanding and parsing subtle relationships between entities; Multimodal NER, it is possible to explore and develop more diversified information fusion mechanisms in the future, focusing on how to make full use of and fuse information of multiple modes such as text, image, voice, etc. to improve the accuracy and robustness of entity recognition; In addition, the challenge of few-sample NER lies in how to use advanced technologies such as transfer learning and meta-learning to achieve rapid adaptation and improved generalization capabilities of the model under limited annotated data.The in-depth exploration of these research directions will not only fill the current technology gap but also greatly promote the development of NER technology in theory and practical applications, bringing new breakthroughs and innovations to the field of natural language processing.</p>
<p>From the keyword map analysis, the current research hotspots of NER include attention networks, multi-task joint learning, transfer learning, adversarial training, deep active learning, federated learning, distance-supervision learning, weakly-supervised learning, and other methods or strategies.These methods have greatly promoted the progress of models in understanding complex contexts and entity recognition accuracy.Among them, the attention mechanism enhances the model's focus on key information in the input data.It becomes a key factor in improving model performance, especially when understanding complex contexts and dealing with longdistance dependencies.Multi-task joint learning shows the potential to improve model generalization capabilities and learning efficiency by processing multiple related tasks in parallel in the same model.Transfer learning, especially the method of fine-tuning pretrained models, shows excellent performance under low resource conditions and can significantly reduce the reliance on large amounts of labeled data.Adversarial training enhances the robustness of the model by introducing adversarial samples, helping the model maintain stable performance in the face of small perturbations in the input data.The deep active learning strategy effectively reduces the amount of annotation data needed by intelligently selecting samples with large amounts of information to annotate, which is especially suitable for scenarios with scarce annotation resources.Federated learning emphasizes jointly improving the model through collaborative training on multiple devices or servers while maintaining data privacy, which is particularly important for processing sensitive data.Distance supervision and weakly supervised learning effectively solve the problem of insufficient annotated data by utilizing existing knowledge bases or incompletely accurate annotated data to train models.However, these advances bring new possibilities to NER in practical applications.How to accurately transfer knowledge learned from one domain to another, handle differences in data distribution, and optimize models to adapt to new tasks remain challenges.In addition, applications in low-resource fields such as mechanical engineering and agricultural science face unique challenges, including but not limited to the accurate identification of professional terms and complex entities and the adaptability of models to domain-specific language patterns.For example, in the field of mechanical engineering, documents may be full of professional terms, technical parameters, CAD drawings, etc.The NER system is required not only to have high accuracy and robustness but also to be able to adapt to specific terms and expressions in various mechanical fields.Strategies such as adversarial training, deep active learning, federated learning, remote supervision, and weakly supervised learning show the potential to improve model robustness, reduce labeling requirements, and protect data privacy.However, how to effectively integrate these strategies to solve the specific problems in NER and improve the accuracy and efficiency of the model still needs further exploration.</p>
<p>Although this article attempts to analyze the NER field comprehensively, there are still certain limitations.First, during the literature search process, "Named Entity Recognition" was selected as the primary search keyword, which ensured that we could effectively locate a wide range of literature directly related to NER.However, this search strategy may not fully cover all research literature in this field, especially those that may use different terms or keywords to describe similar concepts.In addition, interdisciplinary research or emerging technology applications may be published with different keywords, resulting in some unusual keywords not being included in this analysis.Future research can consider adopting more extensive search strategies, including more keywords and terms, to cover the literature in this field as comprehensively as possible.</p>
<p>Fig. 2 .
2
Fig. 2. Research framework diagram of the article.</p>
<p>Fig. 4 .
4
Fig. 4. Number of papers included in different research direction.</p>
<p>Fig. 5 .
5
Fig. 5. Author cooperation network map.</p>
<p>Fig. 11 .
11
Fig. 11.Keyword co-occurrence network map.</p>
<p>Fig. 12 .
12
Fig. 12. Keyword clustering network map.</p>
<p>Table 2
2
Author with more than (or equal to) 8 publications and the year of first publication.
J. Yang et al.AuthorYear of first publicationNumber of published papersAnaniadou, Sophia200815Xu, Hua201413Lin, Hongfei200711Tang, Buzhou20149Yang, Zhihao20088Zhang, Yaoyun20168Wu, Yonghui20168Qiu, Qinjun20198</p>
<p>Table 3
3
Top 10 countries and institutions with published papers and their betweenness centrality.
CountryNumber of publicationsBetweenness centralityOrganizationNumber of publicationsBetweenness centralityPEOPLES R CHINA5630.32Chinese Acad Sci340.19USA2510.36Harbin Inst Technol290.08ENGLAND880.27Dalian Univ Technol250.01SOUTH KOREA850.02Natl Univ Def Technol240.03INDIA660.07Univ Manchester220.08SPAIN630.14Univ Cambridge140.01GERMANY440.07Wuhan Univ140.01AUSTRALIA370.06Peking Univ130.07JAPAN350.06Korea Univ120.05ITALY280.06Cent South Univ120.01
Fig. 7. Institutional cooperation network map.J.Yang et al.</p>
<p>Table 5
5
Co-cited literature clustering labels and their size.
Cluster IDSizeSilhouetteMean(year)Top Terms (log-likelihood ratio, p-level)01720.8852014single-task model (1793.76, 1.0E-4)11090.7832018bert model (1420.22, 1.0E-4)21000.9252003protein name (459.62, 1.0E-4)3960.8472018Chinese ner (1989.18, 1.0E-4)4920.8172017electronic medical record (1053.37, 1.0E-4)5630.8942007metabolite name (204.68, 1.0E-4)6600.9072019nested ner (1707.52, 1.0E-4)7520.952011word representation feature (677.39, 1.0E-4)8520.8692018joint entity (1959.86, 1.0E-4)9520.9932001Spanish text (210.45, 1.0E-4)</p>
<p>Table 6
6
[55]10 co-cited documents in the largest cluster.bidirectionalLSTM and CNN.This hybrid architecture effectively integrates the long-term dependency capture capability of BiLSTM and CNN's character-level feature extraction capability, making the model more effective in handling morphological changes and spelling errors, which is crucial to the NER task.This work has promoted the application of deep learning technology in the field of NER, provided a new direction and benchmark for subsequent research, and proved the effectiveness of using deep learning technology to process complex NLP tasks.E.Peters et al.[55]proposed ELMo (Embeddings from Language Models), a deep PLM based on BiLSTM.
J. Yang et al.FrequencyCentralityLabelAuthorYearSource195 1400.05 0.09Lample G. (2016) Ma XZ (2016)Lample G. Ma XZ2016 2016ACL ACL (54 TH )640Peters ME (2018)Peters ME2018MNLP 2018610.01Chiu J.P.C. (2016)Chiu J.P.C.2016T ASSOC COMPUT LING600.03Abadi M. (2015)Abadi M.2015ARXIV160304467430.28Leaman R (2015)Leaman R2015J CHEMINFORMATICS340.02Krallinger M (2015)Krallinger M2015J CHEMINFORMATICS330.01Leaman R (2016)Leaman R2016BIOINFORMATICS310.01Manning C.D. (2014)Manning C.D.2014P C EMP METH NAT LAN300.04Crichton G (2017)Crichton G2017BMC BIOINFORMATICS
combines</p>
<p>Table 7
7
High-frequency co-occurrence keywords.
J. Yang et al.KeywordCentralityYear of first appearanceFrequencynamed-entity recognition0.282004497natural language processing0.182002213extraction0.192002189text mining0.192005136deep learning model0.082017133neural network0.08201887machine learning0.11200677conditional random field0.16200668task analysis0.05201957database0.06200556relation extraction0.04201555attention0.03201939gene0.05200436transfer learning0.01202035biomedical text mining0.03200635electronic health records0.02201334feature extraction0.03201934biomedical named entity recognition0.06200434classification0.03200733word embedding0.03201732sequence labeling0.03201730</p>
<p>Table 8
8
Keyword clustering labels and their size.
Cluster IDSizeSilhouetteMean yearTop Terms (Log-Likelihood Ratio, P-Level)0940.7112019adversarial learning (510.39, 1.0E-4)1900.6952017social media (485.51, 1.0E-4)2900.7692011biomedical literature (1151.48, 1.0E-4)3600.6442011active learning (461.07, 1.0E-4)4590.7492013protein-protein interaction information extraction (253.17, 1.0E-4)5490.812013natural language processing method (218.87, 1.0E-4)6350.8122017using lexical disambiguation (234.06, 1.0E-4)7320.7642014data augmentation (187.01, 1.0E-4)8190.8962011question answering (175.99, 1.0E-4)9100.9582011mining chemical document (60.6, 1.0E-4)
J.Yang et al.</p>
<p>J.Yang et al. <br />
AcknowledgmentsThis work was supported by Guizhou Provincial Basic Research Program (Natural Science) (Grant No. Qiankehejichu-ZK[2022] General 320), National Natural Science Foundation (Grant No. 72061006) and Academic New Seedling Foundation Project of Guizhou Normal University (Grant No. Qianshixinmiao-[2021]A30).Data availabilityThe data that was collected and analyzed during this study is contained in this published article and the data that was used to support the findings of this review are listed in the references at the end of the article.CRediT authorship contribution statementJun Yang: Writingoriginal draft, Software, Formal analysis, Data curation, Conceptualization.Taihua Zhang: Writingreview &amp; editing, Supervision.Chieh-Yuan Tsai: Writingreview &amp; editing, Supervision.Yao Lu: Writingreview &amp; editing, Supervision.Liguo Yao: Writingreview &amp; editing, Supervision, Conceptualization.Declaration of competing interestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
. J Devlin, ARXIV. 020192019</p>
<p>. G Lample, 2016, ACL, V0, PP260 195 2016</p>
<p>. A Vaswani, ADV NEUR IN. 3020172017</p>
<p>. X Z Ma, P1064 140 201620161</p>
<p>. J Lee, BIOINFORMATICS. 3620202020</p>
<p>. M Habibi, BIOINFORMATICS. 3320172017</p>
<p>Y Zhang, ACL). 2018. 20181</p>
<p>. A Radford, CNAM CHAPT. 02018. 2018. 2018</p>
<p>. P ; Bojanowski, Assoc, Ling, 201752017</p>
<p>. J Li, Ieee T Knowl Data En, 342022</p>
<p>Named entity task definition. N A Chinchor, Proceedings of the Sixth Message Understanding Conference (MUC-6). the Sixth Message Understanding Conference (MUC-6)1995</p>
<p>D3NER: biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information. T H Dang, H Q Le, T M Nguyen, S T Vu, 10.1093/bioinformatics/bty356Bioinformatics. 342018</p>
<p>Drug disease relation extraction from biomedical literature using NLP and machine learning. W B Karaa, E H Alkhammash, A Bchir, 10.1155/2021/9958410Mobile Inf. Syst. 20212021</p>
<p>LSTMVoter: chemical named entity recognition using a conglomerate of sequence labeling tools. W Hemati, A Mehler, 10.1186/s13321-018-0327-2J. Cheminf. 112019</p>
<p>CheNER: chemical named entity recognizer. A Usie, R Alves, F Solsona, M Vazquez, A Valencia, 10.1093/bioinformatics/btt639Bioinformatics. 302014</p>
<p>Content-based information retrieval by named entity recognition and verb semantic role labelling. J B Antony, G S Mahalakshmi, J. Univers. Comput. Sci. 212015</p>
<p>Persian automatic text summarization based on named entity recognition. M E Khademi, M Fakhredanesh, 10.1007/s40998-020-00352-2Iranian Journal of Science and Technology-Transactions of Electrical Engineering. 2020</p>
<p>A medical Q&amp;A system with entity linking and intent recognition. F M Guan, T Tezuka, 10.1109/SSCI51031.2022.100220932022 Ieee Symposium Series on Computational Intelligence (Ssci). 2022</p>
<p>Language model pre-training method in machine translation based on named entity recognition. Z Li, D Qu, C J Xie, W L Zhang, Y X Li, 10.1142/S0218213020400217Int. J. Artif. Intell. Tool. 292020</p>
<p>A weakly-supervised method for named entity recognition of agricultural knowledge graph. L Wang, J C Jiang, J W Song, J Liu, 10.32604/iasc.2023.036402Intelligent Automation and Soft Computing. 372023</p>
<p>ProMiner: rule-based protein and gene entity recognition. D Hanisch, K Fundel, H T Mevissen, R Zimmer, J Fluck, 10.1186/1471-2105-6-S1-S14BMC Bioinf. 62005</p>
<p>Named entity recognition over electronic health records through a combined dictionary-based approach. A P Quimbaya, A S Munera, R A G Rivera, J C D Rodriguez, O M M Velandia, A A G Pena, C Labbe, 10.1016/j.procs.2016.09.123International Conference on Enterprise Information Systems/International Conference on Project Management/International Conference on Health And Social Care Information Systems and Technologies, CENTERIS/PROJMAN/HCIST. 2016. 2016</p>
<p>Entity Extraction without Language-specific Resources. P Mcnamee, J Mayfield, 2002</p>
<p>A Mccallum, W Li, Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons. 2003</p>
<p>University of Sheffield: Description of the LaSIE-II System as Used for MUC-7. K Humphreys, R Gaizauskas, S Azzam, C Huyck, B Mitchell, H Cunningham, Y Wilks, 1998</p>
<p>G R Krupka, K Hausman, Isoquest Inc, Description of the NetOwl™ Extractor System as Used for MUC-7. 1998</p>
<p>FACILE: Description of the NE System Used for MUC-7. W J Black, F Rinaldi, D Mowatt, 1998</p>
<p>D E Appelt, J R Hobbs, J Bear, D Israel, M Kameyama, A Kehler, D Martin, K Myers, M Tyson, SRI International FASTUS SystemMUC-6 Test Results and Analysis. 1995</p>
<p>Chinese named entity recognition: the state of the art. P Liu, Y M Guo, F L Wang, G H Li, 10.1016/j.neucom.2021.10.101Neurocomputing. 4732022</p>
<p>Hidden Markov models. S R Eddy, 10.1016/S0959-440X(96)80056-XCurr. Opin. Struct. Biol. 61996</p>
<p>J N Kapur, Maximum-entropy Models in Science and Engineering. John Wiley &amp; Sons1989</p>
<p>Support vector machines. M A Hearst, S T Dumais, E Osuna, J Platt, B Scholkopf, 10.1109/5254.708428IEEE Intell. Syst. Their Appl. 131998</p>
<p>Top-down induction of decision trees classifiers -a survey. L Rokach, O Maimon, 10.1109/TSMCC.2004.843247IEEE Trans. Syst. Man Cybern. C Appl. Rev. 352005</p>
<p>J Lafferty, A Mccallum, F C N Pereira, Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. 2001</p>
<p>A review: development of named entity recognition (NER) technology for aeronautical information intelligence. B G Mi, Y Fan, 10.1007/s10462-022-10197-2Artif. Intell. Rev. 562023</p>
<p>Named entity recognition: fallacies, challenges and opportunities. M Marrero, J Urbano, S Sanchez-Cuadrado, J Morato, J M Gomez-Berbis, 10.1016/j.csi.2012.09.004Comput. Stand. Interfac. 352013</p>
<p>Recent named entity recognition and classification techniques: a systematic review. A Goyal, V Gupta, M Kumar, 10.1016/j.cosrev.2018.06.001Computer Science Review. 292018</p>
<p>Named entity recognition and relation extraction: state-of-the-art. Z Nasar, S W Jaffry, M K Malik, 10.1145/3445965ACM Comput. Surv. 542021</p>
<p>A survey on deep learning for named entity recognition. J Li, A X Sun, J L Han, C L Li, 10.1109/TKDE.2020.2981314IEEE Trans. Knowl. Data Eng. 342022</p>
<p>Sources of atmospheric pollution: a bibliometric analysis. Y X Li, Y Wang, X Rui, Y X Li, Y Li, H Z Wang, J Zuo, Y D Tong, 10.1007/s11192-017-2421-zScientometrics. 1122017</p>
<p>Tracing knowledge diffusion of TOPSIS: a historical perspective from citation network. D J Yu, T X Pan, 10.1016/j.eswa.2020.114238Expert Syst. Appl. 1682021</p>
<p>Analysis of evolutionary process in intuitionistic fuzzy set theory: a dynamic perspective. D J Yu, L B Sheng, Z S Xu, 10.1016/j.ins.2022.04.019Inf. Sci. 6012022</p>
<p>C Chen, L Leydesdorff, Technology, Patterns of Connections and Movements in Dual-map Overlays: A New Method of Publication Portfolio Analysis. 201465</p>
<p>Emerging trends in regenerative medicine: a scientometric analysis in CiteSpace. C M Chen, Z G Hu, S B Liu, H Tseng, 10.1517/14712598.2012.674507Expet Opin. Biol. Ther. 122012</p>
<p>The automatic content extraction (ACE) program -tasks, data, and evaluation. G R Doddington, A Mitchell, M A Przybocki, L A Ramshaw, S Strassel, R M Weischedel, International Conference on Language Resources and Evaluation. 2004</p>
<p>A novel automated approach to mutation-cancer relation extraction by incorporating heterogeneous knowledge. J R Cao, E M Van Veen, N Peek, A G Renehan, S Ananiadou, 10.1109/JBHI.2022.3220924Ieee Journal of Biomedical and Health Informatics. 272023</p>
<p>Comparing neural models for nested and overlapping biomedical event detection. K Espinosa, P Georgiadis, F Christopoulou, M Z Ju, M Miwa, S Ananiadou, 10.1186/s12859-022-04746-3BMC Bioinf. 232022</p>
<p>Recognizing software names in biomedical literature using machine learning. Q Wei, Y Y Zhang, M Amith, R Lin, J Lapeyrolerie, C Tao, H Xu, 10.1177/1460458219869490Health Inf. J. 262020</p>
<p>A study of machine-learning-based approaches to extract clinical entities and their assertions from discharge summaries. M Jiang, Y K Chen, M Liu, S T Rosenbloom, S Mani, J C Denny, H Xu, 10.1136/amiajnl-2011-000163J. Am. Med. Inf. Assoc. 182011</p>
<p>Adversarial transfer network with bilinear attention for the detection of adverse drug reactions from social media. T X Zhang, H F Lin, Y Q Ren, Z H Yang, J Wang, S W Zhang, B Xu, X D Duan, 10.1016/j.asoc.2021.107358Appl. Soft Comput. 1062021</p>
<p>Identifying adverse drug reaction entities from social media with adversarial transfer learning model. T X Zhang, H F Lin, Y Q Ren, Z H Yang, J Wang, X D Duan, B Xu, 10.1016/j.neucom.2021.05.007Neurocomputing. 4532021</p>
<p>Chinese named entity recognition in the geoscience domain based on BERT. X Lv, Z Xie, D X Xu, X G Jin, K Ma, L F Tao, Q J Qiu, Y S Pan, 10.1029/2021EA002166Earth Space Sci. 92022</p>
<p>BiLSTM-CRF for geological named entity recognition from the geoscience literature. Q J Qiu, Z Xie, L Wu, L F Tao, W J Li, 10.1007/s12145-019-00390-3EARTH SCIENCE INFORMATICS. 122019</p>
<p>D J Price, Little Science, Big Science. New YorkColumbia University Press1963. 1963Solla de</p>
<p>Analysis of collaboration evolution in AHP research: 1982-2018. D J Yu, G Kou, Z S Xu, S S Shi, 10.1142/S0219622020500406Int. J. Inf. Technol. Decis. Making. 202021</p>
<p>Scientometric sorting by importance for literatures on life cycle assessments and some related methodological discussions. G Qian, 10.1007/s11367-014-0747-9Int. J. Life Cycle Assess. 192014</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS. 302017. 2017</p>
<p>Neural architectures for named entity recognition. G Lample, M Ballesteros, S Subramanian, K Kawakami, C Dyer, North American Chapter. Association for Computational Linguistics2016</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, 10.1093/bioinformatics/btz682Bioinformatics. 362020</p>
<p>tmChem: a high performance approach for chemical named entity recognition and normalization. R Leaman, C H Wei, Z Y Lu, 10.1186/1758-2946-7-S1-S3J. Cheminf. 72015</p>
<p>E F Sang, F J , De Meulder, Introduction to the CoNLL-2003 Shared Task: Language-independent Named Entity Recognition. 2003</p>
<p>R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P Kuksa, Natural Language Processing (Almost) from Scratch. 201112</p>
<p>A M Dai, Q V Le, Semi-supervised Sequence Learning. 201528</p>
<p>Named entity recognition with bidirectional LSTM-CNNs. J P C Chiu, E Nichols, Transactions of the association for computational linguistics. 42016</p>
<p>M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, 10.18653/v1/N18-1202Deep contextualized word representations. Association for Computational Linguistics2018</p>
<p>Combining machine learning and main path analysis to identify research front: from the perspective of science-technology linkage. D J Yu, Z P Yan, 10.1007/s11192-022-04443-1Scientometrics. 1272022</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, L Assoc Computat, 2019 Conference of The North American Chapter of The Association for Computational Linguistics: Human Language Technologies (Naacl Hlt 2019). 20191</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: a robustly optimized bert pretraining approach. 2019arXiv Preprint</p>
<p>Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.1194a lite bert for self-supervised learning of language representations. Albert2019arXiv preprint</p>
<p>BERT-based transfer-learning approach for nested named-entity recognition using joint labeling. A Agrawal, S Tripathi, M Vardhan, V Sihag, G Choudhary, N Dragoni, 10.3390/app12030976APPLIED SCIENCES-BASEL. 122022</p>
<p>A novel named entity recognition scheme for steel E-commerce platforms using a lite BERT. M J Chen, X Luo, H L Shen, Z Y Huang, Q J Peng, 10.32604/cmes.2021.017491CMES-COMPUTER MODELING IN ENGINEERING &amp; SCIENCES. 1292021</p>
<p>Chinese clinical named entity recognition with variant neural structures based on BERT methods. X Y Li, H Zhang, X H Zhou, 10.1016/j.jbi.2020.103422J. Biomed. Inf. 1072020</p>
<p>Improving Language Understanding by Generative Pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, L Sutskever, D Amodei, Language Models Are Few-Shot Learners. 332020</p>
<p>Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V J A , Le, Xlnet: Generalized Autoregressive Pretraining for Language Understanding. 201932</p>
<p>Transformer-xl: Attentive Language Models beyond a Fixed-Length Context. Z Dai, Z Yang, Y Yang, J Carbonell, Q V Le, R , J.a.p.a. Salakhutdinov. 2019</p>
<p>Y Sun, S Wang, Y Li, S Feng, X Chen, H Zhang, X Tian, D Zhu, H Tian, H J , Ernie: Enhanced Representation through Knowledge Integration. 2019</p>
<p>K Clark, M.-T Luong, Q V Le, C D Manning, Pre-training Text Encoders as Discriminators rather than Generators. Electra2020</p>
<p>S Wang, X Sun, X Li, R Ouyang, F Wu, T Zhang, J Li, G J , Gpt-ner: Named Entity Recognition via Large Language Models. 2023</p>
<p>E J , Covas, Named Entity Recognition Using GPT for Identifying Comparable Companies. 2023</p>
<p>R Yan, X Jiang, D J N P L Dang, Named Entity Recognition by Using XLNet-BiLSTM-CRF. 202153</p>
<p>Named entity recognition in XLNet cyberspace security domain based on dictionary embedding. D Yang, F Wan, Y Zhang, 4th International Conference on Advances in Computer Technology, Information Science and Communications (CTISC). IEEE2022</p>
<p>. A Conneau, K Khandelwal, N Goyal, V Chaudhary, G Wenzek, F Guzmán, E Grave, M Ott, L Zettlemoyer, V , J.a.p.a. Stoyanov. 2019Unsupervised Cross-Lingual Representation Learning at Scale</p>
<p>P Keung, Y Lu, V J , Adversarial Learning with Contextual Embeddings for Zero-Resource Cross-Lingual Classification and NER. 2019</p>
<p>Improving Low Resource Named Entity Recognition Using Cross-Lingual Knowledge Transfer. X Feng, X Feng, B Qin, Z Feng, T Liu, IJCAI. 2018</p>
<p>Crossner: evaluating cross-domain named entity recognition. Z Liu, Y Xu, T Yu, W Dai, Z Ji, S Cahyawijaya, A Madotto, P Fung, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>Cross-domain NER using cross-domain language modeling. C Jia, X Liang, Y Zhang, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>S Chen, G Aguilar, L Neves, T J , Data Augmentation for Cross-Domain Named Entity Recognition. 2021</p>
<p>Cross-domain multi-task learning for sequential sentence classification in research papers. A Brack, A Hoppe, P Buschermöhle, R Ewerth, Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries. the 22nd ACM/IEEE Joint Conference on Digital Libraries2022</p>
<p>Advancement artificial, CrossNER: evaluating cross-domain named entity recognition. Z H Liu, Y Xu, T Z Yu, W L Dai, Z W Ji, S Cahyawijaya, A Madotto, P Fung, I Assoc, Thirty-Fifth Aaai Conference On Artificial Intelligence, Thirty-Third Conference On Innovative Applications Of Artificial Intelligence And The Eleventh Symposium On Educational Advances In Artificial Intelligence. 2021</p>
<p>Unsupervised cross-domain named entity recognition using entity-aware adversarial training. Q Peng, C M Zheng, Y Cai, T Wang, H R Xie, Q Li, 10.1016/j.neunet.2020.12.027Neural Network. 1382021</p>
<p>Nested named entity recognition revisited. A Katiyar, C Cardie, 2018Association for Computational LinguisticsNorth American Chapter of the</p>
<p>Nested named entity recognition: a survey. Y Wang, H H Tong, Z Y Zhu, Y Li, 10.1145/3522593ACM Trans. Knowl. Discov. Data. 162022</p>
<p>Effective adaptation of hidden Markov model-based named entity recognizer for biomedical domain. D Shen, J Zhang, G Zhou, J Su, C L Tan, BioNLP@ ACL. 2003</p>
<p>A neural layered model for nested named entity recognition. M Ju, M Miwa, S Ananiadou, 2018North American Chapter of the Association for Computational Linguistics</p>
<p>A local detection approach for named entity recognition and mention detection. M B Xu, H Jiang, S Watcharawittayakul, 10.18653/v1/P17-1114Proceedings of The 55th Annual Meeting Of The Association For Computational Linguistics. The 55th Annual Meeting Of The Association For Computational LinguisticsAcl 2017. 20171</p>
<p>Joint mention extraction and classification with mention hypergraphs. W Lu, D Roth, Conference On Empirical Methods in Natural Language Processing. 2015</p>
<p>A neural transition-based model for nested mention recognition. B L Wang, W Lu, Y Wang, H X Jin, L Assoc Computat, Conference On Empirical Methods in Natural Language Processing. 2018. 2018. 2018</p>
<p>Planarized sentence representation for nested named entity recognition. R S Geng, Y P Chen, R Z Huang, Y B Qin, Q H Zheng, 10.1016/j.ipm.2023.103352Inf. Process. Manag. 602023</p>
<p>A Multi-Head Adjacent Attention-Based Pyramid Layered Model for Nested Named Entity Recognition. S M Cui, I Joe, 10.1007/s00521-022-07747-8Neural Computing &amp; Applications. 352023</p>
<p>A controlled attention for nested named entity recognition. Y P Chen, R Huang, L J Pan, R Z Huang, Q H Zheng, P Chen, 10.1007/s12559-023-10112-zCognitive Computation. 152023</p>
<p>Noun-based Attention Mechanism for Fine-Grained Named Entity Recognition. A J C Rodríguez, D C Castro, S H García, 2022 116406193</p>
<p>A span-based multi-modal attention network for joint entity-relation extraction. Q Wan, L Wei, S Zhao, J J K , B S Liu, 2023262110228</p>
<p>ChemNER: fine-grained chemistry named entity recognition with ontology-guided distant supervision. X Wang, V Hu, X Song, S Garg, J Xiao, J Han, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer. J Yu, J Jiang, L Yang, R Xia, 2020Association for Computational Linguistics</p>
<p>Multi-modal graph fusion for named entity recognition with targeted visual guidance. D Zhang, S Wei, S Li, H Wu, Q Zhu, G Zhou, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>A span-based multimodal variational autoencoder for semi-supervised multimodal named entity recognition. B Zhou, Y Zhang, K Song, W Guo, G Zhao, H Wang, X Yuan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Enhancing Few-Shot NER with Prompt Ordering Based Data Augmentation. H Wang, L Cheng, W Zhang, D W Soh, L J , 2023Bing</p>
<p>Few-shot Named Entity Recognition with Self-Describing Networks. J Chen, Q Liu, H Lin, X Han, L , J.a.p.a. Sun. 2022</p>
<p>S S S Das, A Katiyar, R J Passonneau, R J , Zhang, CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning. 2021</p>
<p>. Y Chen, Y Zheng, Z J , Prompt-Based Metric Learning for Few-Shot NER. 2022</p>
<p>D.-H Lee, A Kadakia, K Tan, M Agarwal, X Feng, T Shibuya, R Mitani, T Sekiya, J Pujara, X J , Good Examples Make a Faster Learner: Simple Demonstration-Based Learning for Low-Resource NER. 2021</p>
<p>Y Shen, Z Tan, S Wu, W Zhang, R Zhang, Y Xi, W Lu, Y J , Zhuang, PromptNER: Prompt Locating and Typing for Named Entity Recognition. 2023</p>
<p>Information sciences 1968-2016: a retrospective analysis with text mining and bibliometric. D J Yu, Z S Xu, W Pedrycz, W R Wang, 10.1016/j.ins.2017.08.031Inf. Sci. 4182017</p>
<p>Tuning support vector machines for biomedical named entity recognition. J I Kazama, T Makino, Y Ohta, J Tsujii, ACL Workshop on Natural Language Processing in the Biomedical Domain. 2002</p>
<p>ChemSpot: a hybrid system for chemical named entity recognition. T Rocktaschel, M Weidlich, U Leser, 10.1093/bioinformatics/bts183Bioinformatics. 282012</p>
<p>T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781Efficient Estimation of Word Representations in Vector Space. 2013arXiv preprint</p>
<p>Z Huang, W Xu, K Yu, Lstm Bidirectional, arXiv:1508.01991CRF models for sequence tagging. 2015arXiv preprint</p>
<p>Model-based clustering, discriminant analysis, and density estimation. C Fraley, A E Raftery, 10.1198/016214502760047131J. Am. Stat. Assoc. 972002</p>
<p>Analysis of knowledge evolution in PROMETHEE: a longitudinal and dynamic perspective. D J Yu, Y Liu, Z S Xu, 10.1016/j.ins.2023.119151Inf. Sci. 6422023</p>
<p>. J Yang, </p>
<p>Understanding Scientific Literatures: A Bibliometric Approach. J C Donohue, 1973</p>
<p>A unified model for cross-domain and semi-supervised named entity recognition in Chinese social media. H F He, X Sun, Aaai , THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE. 2017</p>
<p>A review on the attention mechanism of deep learning. Z Y Niu, G Q Zhong, H Yu, 10.1016/j.neucom.2021.03.091Neurocomputing. 4522021</p>
<p>D Bahdanau, K Cho, Y J C Bengio, Neural Machine Translation by Jointly Learning to Align and Translate. 2014 abs/1409.0473</p>
<p>K Xu, J L Ba, R Kiros, K Cho, A Courville, R Salakhutdinov, R S Zemel, Y Bengio, Show, attend and tell: neural image caption generation with visual attention, INTERNATIONAL CONFERENCE ON MACHINE LEARNING. 201537</p>
<p>A multi-domain named entity recognition method based on part-of-speech attention mechanism. S Zhang, Y Sheng, J F Gao, J H Chen, J J Huang, S F Lin, 10.1007/978-981-15-1377-0_49COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING. CHINESECSCW2019, 2019</p>
<p>ASRNN: a recurrent neural network with an attention model for sequence labeling. J C W Lin, Y N Shao, Y Djenouri, U Yun, 10.1016/j.knosys.2020.106548Knowl. Base Syst. 2122021</p>
<p>Improving clinical named entity recognition with global neural attention, WEB AND BIG DATA (APWEB-WAIM. G H Xu, C Y Wang, X F He, 10.1007/978-3-319-96893-3_20PT II. 2018. 2018</p>
<p>Deep learning based relation extract-ion: a survey. C Zhuang, X Jin, W Zhu, J W Liu, L Bai, X Q Cheng, Chinese Journal of Informatics. 332019</p>
<p>Joint entity and relation extraction model based on rich semantics. Z Q Geng, Y H Zhang, Y M Han, 10.1016/j.neucom.2020.12.037Neurocomputing. 4292021</p>
<p>Ieee, joint entity linking and relation extraction with neural networks for knowledge base population. Z Y Zhang, X B Shu, T W Liu, Z Fang, Q G Li, 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN). 2020</p>
<p>Incremental joint extraction of entity mentions and relations. Q Li, H Ji, 10.3115/v1/p14-1038PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS. 20141</p>
<p>Representation iterative fusion based on heterogeneous graph neural network for joint entity and relation extraction. K Zhao, H Xu, Y Cheng, X Li, K J K , B S Gao, 2021219106888</p>
<p>Joint Entity and Relation Extraction with Position-Aware Attention and Relation Embedding. T Chen, L Zhou, N Wang, X J A S C Chen, 2022119108604</p>
<p>Z Yan, S Yang, W Liu, K J , Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. 2023</p>
<p>Joint entity recognition and relation extraction as a multi-head selection problem. G Bekoulis, J Deleu, T Demeester, C Develder, 2018114</p>
<p>Y Luan, D Wadden, L He, A Shah, M Ostendorf, H J , General Framework for Information Extraction Using Dynamic Span Graphs. 2019</p>
<p>PhoNLP: a joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing. D Q Nguyen, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterDemonstrations2021</p>
<p>A survey on transfer learning. S J Pan, Q A Yang, 10.1109/TKDE.2009.191IEEE Trans. Knowl. Data Eng. 222010</p>
<p>D Peng, Y Wang, C Liu, Z J I S F Chen, Tl-Ner, Transfer, Learning Model for Chinese Named Entity Recognition. 202022</p>
<p>Named entity recognition in electronic health records using transfer learning bootstrapped. L Gligic, A Kormilitzin, P Goldberg, A J N N Nevado-Holgado, Neural Network. 1212020</p>
<p>Chinese mineral named entity recognition based on BERT model. Y Q Yu, Y Z Wang, J Q Mua, W Li, S T Jiao, Z Wang, P Lv, Y Q Zhu, 10.1016/j.eswa.2022.117727Expert Syst. Appl. 2062022</p>
<p>Fine-grained mechanical Chinese named entity recognition based on ALBERT-AttBiLSTM-CRF and transfer learning. L G Yao, H S Huang, K W Wang, S H Chen, Q Q Xiong, 10.3390/sym12121986SYMMETRY-BASEL. 122020</p>
<p>I J Goodfellow, J Shlens, C J , Szegedy, Explaining and Harnessing Adversarial Examples. 2014</p>
<p>Cross domains adversarial learning for Chinese named entity recognition for online medical consultation. G H Wen, H H Chen, H H Li, Y Hu, Y H Li, C J Wang, 10.1016/j.jbi.2020.103608J. Biomed. Inf. 1122020</p>
<p>ADMit: Improving NER in Automotive Domain with Domain Adversarial Training and Multi-Task Learning. C Park, S Jeong, J Kim, 2023225120007</p>
<p>J Wang, W Xu, X Fu, G Xu, Y J K , B S Wu, ASTRAL: Adversarial Trained LSTM-CNN for Named Entity Recognition. 2020197105842</p>
<p>A survey of deep active learning. P Z Ren, Y Xiao, X J Chang, P Y Huang, Z H Li, B B Gupta, X J Chen, X Wang, 10.1145/3472291ACM Comput. Surv. 542022</p>
<p>S Tong, Active Learning: Theory and Applications. 2001</p>
<p>A Agrawal, S Tripathi, M Vardhan, Active Learning Approach Using a Modified Least Confidence Sampling Strategy for Named Entity Recognition. 202110</p>
<p>UD_BBC: named entity recognition in social network combined BERT-BiLSTM-CRF with active learning. W Li, Y J Du, X Y Li, X L Chen, C Z Xie, H Li, X L Li, 10.1016/j.engappai.2022.105460Eng. Appl. Artif. Intell. 1162022</p>
<p>Subsequence based deep active learning for named entity recognition. P Radmard, Y Fathullah, A Lipani, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>J Konečný, H B Mcmahan, F X Yu, P Richtárik, A T Suresh, D J , Federated Learning: Strategies for Improving Communication Efficiency. 2016</p>
<p>C Wu, F Wu, L Lyu, Y Huang, X J N C Xie, Communication-efficient Federated Learning via Knowledge Distillation. 2022132032</p>
<p>Federated domain adaptation for named entity recognition via distilling with heterogeneous tag sets. R Wang, T Yu, J Wu, H Zhao, S Kim, R Zhang, S Mitra, R Henao, 2023ACL2023</p>
<p>S Ge, F Wu, C Wu, T Qi, Y Huang, X J , Fedner: Privacy-Preserving Medical Named Entity Recognition with Federated Learning. 2020</p>
<p>A class-rebalancing self-training framework for distantly-supervised named entity recognition. Q Li, T Xie, P Peng, H Wang, G Wang, 2023ACL2023</p>
<p>K Zhou, Y Li, Q J A P A Li, Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning. 2022</p>
<p>Han, Distantly-supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. Y Meng, Y Zhang, J Huang, X Wang, Y Zhang, H Ji, J J , 2021</p>
<p>Ontology-driven Weak Supervision for Clinical Entity Classification in Electronic Health Records. J A Fries, E Steinberg, S Khattar, S L Fleming, J Posada, A Callahan, N H J N C Shah, 2021122017</p>
<p>. Y Zhang, B Jin, X Chen, Y Shen, Y Zhang, Y Meng, J J , Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. 2023Han</p>            </div>
        </div>

    </div>
</body>
</html>
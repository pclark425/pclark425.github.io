<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-781 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-781</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-781</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-256846886</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.06975v1.pdf" target="_blank">A Review of the Role of Causality in Developing Trustworthy AI Systems</a></p>
                <p><strong>Paper Abstract:</strong> State-of-the-art AI models largely lack an understanding of the cause-effect relationship that governs human understanding of the real world. Consequently, these models do not generalize to unseen data, often produce unfair results, and are difficult to interpret. This has led to efforts to improve the trustworthiness aspects of AI models. Recently, causal modeling and inference methods have emerged as powerful tools. This review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of AI models. We hope that our contribution will motivate future research on causality-based solutions for trustworthy AI.</p>
                <p><strong>Cost:</strong> 0.034</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e781.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e781.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast Causal Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constraint-based causal discovery algorithm that outputs a Partial Ancestral Graph (PAG) and is able to represent possible latent confounding and selection bias; commonly used to detect conditional independence relations and infer causal structure under hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fast Causal Inference (FCI)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>FCI performs a sequence of conditional independence tests on observational data to prune and orient edges, producing a PAG that encodes equivalence classes of causal graphs while allowing for hidden common causes and selection bias. It starts by learning an undirected skeleton using conditional independence tests, then orients edges by a set of logical orientation rules that account for possible latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>observational / simulated datasets (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for use on observational datasets or simulated data; not specific to interactive/virtual labs but applicable to structured observational data including medical or simulated datasets referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Allows for latent confounders / unobserved common causes via PAG representation and orientation rules; conditional independence testing to distinguish spurious associations</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>hidden confounders, selection bias, spurious correlations due to common causes</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Conditional independence testing across variable subsets to detect dependencies attributable to common causes or spurious associations</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Represents uncertainty about latent confounders in PAG; identification of non-orientable edges indicates potential spurious links</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a causal discovery method used (e.g., in biomedical application work cited) and notable because it accounts for latent confounding, making it suitable for settings with possible distractor/confounder variables.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e781.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FGES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fast Greedy Equivalence Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score-based causal structure search algorithm that greedily searches the space of equivalence classes (CPDAGs) to maximize a score (e.g., BIC), typically assuming no hidden confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fast Greedy Equivalence Search (FGES)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>FGES is a two-phase greedy search over graph equivalence classes: a forward phase that adds edges to increase a chosen score, and a backward phase that removes edges to further improve score, returning a CPDAG representing a Markov equivalence class; it is computationally efficient for large continuous-variable problems.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>observational / simulated datasets (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to observational datasets or simulated data; was referenced in the paper in the context of causal discovery on medical/AD datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>correlations due to observed variables; does not explicitly handle latent confounders</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Score-driven search identifying structure that best explains observed dependencies under chosen score</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned alongside FCI for causal discovery in a biomedical case; FGES is a scalable score-based approach but typically assumes no hidden confounders, limiting its robustness to latent distractors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e781.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gCastle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gCastle (Causal Discovery Toolbox)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end causal structure learning toolbox that implements many causal discovery algorithms and supports data generation and evaluation of learned structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>gCastle: A Python Toolbox for Causal Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>gCastle toolkit (collection of causal discovery algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>gCastle is a software package bundling ~19 causal discovery techniques (constraint-based, score-based, and hybrid methods), data simulators, and evaluation metrics, allowing practitioners to generate synthetic datasets with known causal generative processes and benchmark algorithms' ability to recover structure.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>benchmarking / simulated data environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Provides utilities to simulate causal datasets (including options to include latent confounders, measurement noise, and varying numbers of variables) making it suitable for virtual lab-style evaluation and stressing algorithms with distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Depends on the chosen underlying algorithm; toolbox enables evaluation of methods that handle latent confounders and spurious variables</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>supports simulation of confounding, measurement noise, irrelevant variables</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not a single detection method; provides implementations of discovery algorithms that use CI tests, scoring, and regularization</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a practical tool for causal discovery research and for generating/evaluating datasets that include spurious or distractor variables, facilitating systematic stress-testing in virtual lab settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e781.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchpress</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchpress (structure learning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable, platform-independent workflow for benchmarking causal structure learning algorithms across many datasets and algorithm variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Benchpress benchmarking framework</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Benchpress offers dataset generation utilities, standardized evaluation protocols and supports over 40 variations of state-of-the-art causal discovery algorithms, enabling researchers to compare algorithmic robustness across datasets with different types and amounts of distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>benchmarking and simulated experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed to run systematic evaluations across many synthetic and real datasets, enabling controlled virtual-lab-style comparisons (e.g., varying confounding, noise, distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>N/A (framework to evaluate algorithms that may include distractor-handling techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>supports evaluations with confounding, irrelevant variables, measurement noise</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recommended as a benchmark to systematically compare causal discovery methods under different distractor scenarios and distributional shifts in virtual experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e781.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalWorld</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CausalWorld (robotic manipulation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated robotic manipulation environment with an explicit causal structure, allowing agents to intervene on variables like masses, colors, or sizes to study transfer and causal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>environment: CausalWorld</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CausalWorld provides a suite of robotic manipulation tasks embedded in an environment with a known causal graph connecting controllable properties and outcomes; it supports interventions and is intended for benchmarking causal structure learning and transfer in reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (robotic manipulation virtual lab)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Open-ended interactive simulation (robotic arm and objects) with explicit causal generative factors that users/agents can intervene on; enables active experimentation and virtual-lab style interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>can simulate mis-specified or spurious correlations through background or style variables (configurable)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Supports active interventions on manipulable variables to probe causal mechanisms and learn structure via experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Highlighted as a virtual lab / benchmark that provides a ground-truth causal structure and supports interventions, thus enabling study of distractors and spurious correlations in interactive environments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e781.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemicalEnv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemical Environment (synthetic RL causal benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic environment for exhaustive evaluation of causal RL agents where changing one object's property (e.g., color) can affect others according to an underlying causal graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemical Environment (described in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>environment: Chemical Environment</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Synthetic interactive environment where agents perform interventions (e.g., change object colors) and the dynamics are governed by an explicit causal DAG (user-defined or random), used to evaluate causal discovery and causal RL approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Chemical Environment (synthetic interactive)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive, controlled virtual environment with explicit causal dynamics and causal graph generation; designed for exhaustive causal RL testing and identification of causal mechanisms with potential distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>inter-variable dependencies that can produce spurious associations if not modeled correctly</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Enables systematic interventions according to the causal graph for discovery and refutation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Described as an environment explicitly intended to test causal discovery under controlled interactive interventions; useful for studying how methods handle interventional signals vs. spurious correlations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e781.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AugSel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selecting Data Augmentation for Simulating Interventions (Ilse et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation selection method that identifies augmentations which emulate interventions on style/domain variables by choosing transformations that most reduce a domain classifier's accuracy, thereby destroying domain-specific (spurious) information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selecting data augmentation for simulating interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>augmentation-selection via domain classifier</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a domain classifier to predict domain labels; evaluate candidate augmentations and select the one(s) that minimize the domain classifier's accuracy (i.e., remove most domain-specific information). Use the selected augmentation(s) as intervention-like data augmentations to reduce model reliance on spurious style variables.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>image classification / domain-generalization tasks (e.g., Rotated/Colored MNIST)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applies to datasets exhibiting domain/style variation (virtual-lab or simulation-produced images), uses augmentation as simulated interventions rather than requiring an explicit SCM.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Data augmentation chosen to 'destroy' domain-specific/style signals; uses learned domain classifier to detect such style features</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>style/domain variables (background, rotation, color) that are spuriously correlated with labels</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Train domain classifier; transformations that most reduce domain classifier accuracy are assumed to target spurious/style signals</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Indirect: eliminate or reduce spurious signal by augmenting training data to remove domain-specific cues rather than explicit weighting</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This technique provides a practical, SCM-free way to simulate interventions on style variables and reduces models' dependence on spurious domain features by selecting augmentations that 'destroy' domain-identifying signals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e781.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenInterv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Interventions via GANSpace (Mao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to generate intervention-like data by discovering interpretable axes in GAN latent space (GANSpace) corresponding to high-level controllable factors, then manipulating those axes to simulate interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative interventions for causal learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GAN latent-space intervention (GANSpace controls)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Identify interpretable latent directions in GAN generators (via GANSpace or similar) that control semantically meaningful properties; create data by manipulating these controls to emulate interventions on causal factors, allowing causal analysis without explicit SCMs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>generative-model-based data augmentation / simulated image domains</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Works on GAN-generated image domains where latent axes correspond to manipulable factors (size, rotation, color) enabling virtual interventions in a simulated lab.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit manipulation of GAN controls corresponding to style or content variables to isolate causal factors and remove confounding/spurious associations</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>style variables and other latent features that co-vary with labels in training data</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Discover interpretable latent axes via GANSpace and evaluate their effect on downstream predictors</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>By generating counterfactuals/interventions that break spurious correlations, the method reduces model dependence on confounding style variables</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative interventions via GAN latent controls can emulate interventions in simulated environments and help disentangle causal vs spurious factors, improving robustness to style-based distractors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e781.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalBoot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bootstrapping-based data sampling method that leverages a causal graph to sample data whose deducible observations better reflect the domain's causal relationships, improving robustness to spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>causal bootstrapping</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use a provided/assumed causal graph to guide resampling of the dataset so that sampled datasets emphasize causal relations implied by the graph (e.g., sample conditioned on causal parents), thereby reducing spurious associations in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>general supervised learning / simulation-informed sampling</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A pre-processing technique applicable to datasets where a causal graph is known or assumed; generates training samples aligned with causal structure to reduce spurious cues.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Causal-graph-guided resampling to reduce frequency/effect of spurious correlations (implicit downweighting by resampling)</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>irrelevant co-occurrences and domain-specific style correlations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Requires a causal graph; spurious signals are inferred by mismatch between observed associations and graph-implied dependencies</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Resampling (bootstrap) to upweight causally consistent samples and downweight spurious associations</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Causal bootstrapping can produce training distributions that emphasize causal patterns and improve robustness to spurious correlations induced by style/domain variables.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e781.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An in-processing objective that seeks predictors whose optimal classifier components are invariant across multiple environments, aiming to recover causal features that generalize under distributional shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant risk minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Risk Minimization (IRM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IRM augments training with a penalty that encourages a representation and classifier that have simultaneously optimal performance across multiple training environments, thereby favoring features that are invariant (putatively causal) across environments and discouraging reliance on environment-specific (spurious) features.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>multi-environment / virtual lab datasets (multiple domains)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Requires multiple environments/domains (natural or simulated) with distributional variation; encourages invariance across these environments and can be applied in interactive/virtual settings where environment interventions are simulated.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization to enforce invariance of predictors across environments, thereby downweighting features that vary (style/spurious features)</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>style/domain variables, non-invariant features that change across environments</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Identify features whose predictive performance is unstable across environments (via environment-wise losses)</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Regularization term that penalizes non-invariant predictors; pushes model to ignore environment-specific signals</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Described as a core in-processing causal objective leveraging multiple environments to separate invariant (causal) features from spurious (style) features and empirically shown to improve OOD robustness in many settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e781.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ISR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant-Feature Subspace Recovery (ISR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing method that recovers a subspace of invariant features from hidden layer representations and fits a linear predictor in that subspace to improve out-of-distribution performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant-Feature Subspace Recovery (ISR)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Extract feature representations from a trained model's hidden layers, estimate the subspace spanned by invariant (content) features, and train/replace the classification head with a linear predictor restricted to this invariant subspace, thereby removing sensitivity to spurious style features.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>trained-model post-processing (general / OOD evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied after model training using hidden representations; useful when distribution shifts occur (including virtual-lab/test-time shifts) and when one can identify invariant components post-hoc.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Subspace recovery and projection to isolate invariant features and remove components associated with spurious/distractor signals</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>style/background variables and other non-invariant factors</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Estimate invariant subspace via analysis of representations across environments or using algorithmic procedures to identify stable directions</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Project representations onto invariant subspace (effectively downweighting orthogonal/distractor directions)</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Post-processing by recovering invariant subspace can substantially improve robustness of already-trained models on OOD datasets by removing representation components tied to spurious signals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e781.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CF-Reg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual Regularization (post-processing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing approach that estimates counterfactual (deconfounded) predictions and uses them to regularize or replace factual predictions, thereby removing confounding/distractor effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Counterfactual regularization / deconfounding prediction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Estimate a counterfactual prediction that removes estimated effects of unobserved confounders (via SCM-guided estimation) and combine/subtract it from the factual prediction to obtain a deconfounded output; can also be used as a regularizer during training to encourage deconfounded representations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>trajectory prediction / multi-domain settings (example use-case)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied in settings where unobserved confounders/distractors bias predictions (e.g., multi-domain trajectory forecasting); requires estimating counterfactuals via a structural model or proxy estimation techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Estimate counterfactuals that remove confounder influence and subtract/deconfound factual predictions</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>unobserved confounders, domain-specific biases</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Estimate counterfactuals and inspect differences between factual and counterfactual predictions to reveal confounding influence</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Subtract estimated counterfactual component to obtain deconfounded predictions (implicit downweighting of confounder effects)</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an effective post-processing approach to remove confounding effects and improve multi-domain prediction robustness by explicitly estimating and removing counterfactual contribution of distractors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e781.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RIMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Independent Mechanisms (RIMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular recurrent architecture motivated by the independent causal mechanisms principle, where separate subsystems (modules) implement autonomous dynamics and communicate sparsely to capture distinct causal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent Independent Mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Recurrent Independent Mechanisms (RIMs)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Architectural design: multiple recurrent modules each learn specialized transition dynamics (mechanisms) and interact sparsely via attention-like routing, enforcing modularity and independence of causal mechanisms which can reduce entanglement with spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>sequential/physical reasoning tasks (e.g., Bouncing Ball, sequential MNIST)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Intended for sequential prediction and interactive environments where independent mechanisms govern different aspects of the dynamics; promotes robustness to distributional shifts by modeling modular causal processes.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Modularization and sparse inter-module communication to isolate causal mechanisms from nuisance/distractor components</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>non-causal entangled features and noise that manifest inconsistently across contexts</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Architectural separation reduces influence of spurious signals by assigning them to separate modules or ignoring them via sparse routing</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recommended as an architecture aligning with independent causal mechanisms which empirically shows improved robustness to distributional shifts and reduced sensitivity to spurious entanglement in sequential/interacting domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e781.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>deepCAMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>deep Causal Manipulation Augmented Model (deep CAMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep generative model consistent with a given causal graph that separates manipulable (intervenable) variables from non-manipulable (style) variables via dedicated autoencoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>deep CAMA</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Model architecture that incorporates separate latent autoencoders for manipulable variables (those we may intervene on) and non-manipulable style/confounder variables; the generative model is trained consistent with an assumed causal graph, enabling disentangling of causal features from spurious ones.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>image generation / manipulation tasks (e.g., MNIST variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Works in domains where manipulations (rotations, color) correspond to controllable latent factors and non-manipulable factors (handwriting style) act as distractors; suitable for simulated/virtual manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit latent disentangling using separate autoencoders for manipulable vs non-manipulable variables guided by a causal graph</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>non-manipulable style variables that are spuriously correlated with labels</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>By separating latent spaces, downstream predictors can ignore non-manipulable latent dimensions tied to spurious signals</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model design that enforces causal-consistent generative structure leading to improved discrimination between causal and spurious features and demonstrated robustness gains against adversarial attacks in MNIST-like settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e781.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e781.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModelSelection-DAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Model Selection Using DAG Constraints (Kyono & van der Schaar)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-hoc model selection pipeline that scores trained models by how well their predictions comply with a provided (possibly partial) causal DAG, selecting models whose predictions better respect causal relationships to improve OOD robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>causal-DAG-based model selection</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given a set of trained models and a causal DAG, compute scores reflecting how consistent each model's predictions are with constraints implied by the DAG (e.g., conditional independencies or causal ordering); choose the model with highest causal-consistency score for deployment to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>model selection / post-training evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Post-processing/tooling applicable when a (partial) causal graph is available and multiple candidate models exist; useful in virtual-lab/benchmarked settings to pick causally-aligned predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Selects models that ignore spurious associations by favoring those whose outputs respect causal constraints of the DAG</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>spurious correlations that violate DAG-implied invariances or conditional independencies</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Score models by violation counts or statistical mismatch vs DAG-implied constraints</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Indirect: choose a model that de facto downweights spurious features by scoring consistency rather than altering model weights</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proposes a practical post-hoc selection technique to prefer models whose predictions align with causal knowledge, empirically improving robustness under distribution shifts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Selecting data augmentation for simulating interventions <em>(Rating: 2)</em></li>
                <li>Generative interventions for causal learning <em>(Rating: 2)</em></li>
                <li>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning <em>(Rating: 2)</em></li>
                <li>Invariant risk minimization <em>(Rating: 2)</em></li>
                <li>Recurrent Independent Mechanisms <em>(Rating: 2)</em></li>
                <li>gCastle: A Python Toolbox for Causal Discovery <em>(Rating: 2)</em></li>
                <li>Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models <em>(Rating: 2)</em></li>
                <li>Systematic evaluation of causal discovery in visual model based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Causal bootstrapping <em>(Rating: 1)</em></li>
                <li>Federated Causal Discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-781",
    "paper_id": "paper-256846886",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "FCI",
            "name_full": "Fast Causal Inference",
            "brief_description": "A constraint-based causal discovery algorithm that outputs a Partial Ancestral Graph (PAG) and is able to represent possible latent confounding and selection bias; commonly used to detect conditional independence relations and infer causal structure under hidden confounders.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Fast Causal Inference (FCI)",
            "method_description": "FCI performs a sequence of conditional independence tests on observational data to prune and orient edges, producing a PAG that encodes equivalence classes of causal graphs while allowing for hidden common causes and selection bias. It starts by learning an undirected skeleton using conditional independence tests, then orients edges by a set of logical orientation rules that account for possible latent confounding.",
            "environment_name": "observational / simulated datasets (general)",
            "environment_description": "Designed for use on observational datasets or simulated data; not specific to interactive/virtual labs but applicable to structured observational data including medical or simulated datasets referenced in the paper.",
            "handles_distractors": true,
            "distractor_handling_technique": "Allows for latent confounders / unobserved common causes via PAG representation and orientation rules; conditional independence testing to distinguish spurious associations",
            "spurious_signal_types": "hidden confounders, selection bias, spurious correlations due to common causes",
            "detection_method": "Conditional independence testing across variable subsets to detect dependencies attributable to common causes or spurious associations",
            "downweighting_method": null,
            "refutation_method": "Represents uncertainty about latent confounders in PAG; identification of non-orientable edges indicates potential spurious links",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as a causal discovery method used (e.g., in biomedical application work cited) and notable because it accounts for latent confounding, making it suitable for settings with possible distractor/confounder variables.",
            "uuid": "e781.0"
        },
        {
            "name_short": "FGES",
            "name_full": "Fast Greedy Equivalence Search",
            "brief_description": "A score-based causal structure search algorithm that greedily searches the space of equivalence classes (CPDAGs) to maximize a score (e.g., BIC), typically assuming no hidden confounders.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Fast Greedy Equivalence Search (FGES)",
            "method_description": "FGES is a two-phase greedy search over graph equivalence classes: a forward phase that adds edges to increase a chosen score, and a backward phase that removes edges to further improve score, returning a CPDAG representing a Markov equivalence class; it is computationally efficient for large continuous-variable problems.",
            "environment_name": "observational / simulated datasets (general)",
            "environment_description": "Applied to observational datasets or simulated data; was referenced in the paper in the context of causal discovery on medical/AD datasets.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "correlations due to observed variables; does not explicitly handle latent confounders",
            "detection_method": "Score-driven search identifying structure that best explains observed dependencies under chosen score",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned alongside FCI for causal discovery in a biomedical case; FGES is a scalable score-based approach but typically assumes no hidden confounders, limiting its robustness to latent distractors.",
            "uuid": "e781.1"
        },
        {
            "name_short": "gCastle",
            "name_full": "gCastle (Causal Discovery Toolbox)",
            "brief_description": "An end-to-end causal structure learning toolbox that implements many causal discovery algorithms and supports data generation and evaluation of learned structures.",
            "citation_title": "gCastle: A Python Toolbox for Causal Discovery",
            "mention_or_use": "use",
            "method_name": "gCastle toolkit (collection of causal discovery algorithms)",
            "method_description": "gCastle is a software package bundling ~19 causal discovery techniques (constraint-based, score-based, and hybrid methods), data simulators, and evaluation metrics, allowing practitioners to generate synthetic datasets with known causal generative processes and benchmark algorithms' ability to recover structure.",
            "environment_name": "benchmarking / simulated data environments",
            "environment_description": "Provides utilities to simulate causal datasets (including options to include latent confounders, measurement noise, and varying numbers of variables) making it suitable for virtual lab-style evaluation and stressing algorithms with distractors.",
            "handles_distractors": null,
            "distractor_handling_technique": "Depends on the chosen underlying algorithm; toolbox enables evaluation of methods that handle latent confounders and spurious variables",
            "spurious_signal_types": "supports simulation of confounding, measurement noise, irrelevant variables",
            "detection_method": "Not a single detection method; provides implementations of discovery algorithms that use CI tests, scoring, and regularization",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as a practical tool for causal discovery research and for generating/evaluating datasets that include spurious or distractor variables, facilitating systematic stress-testing in virtual lab settings.",
            "uuid": "e781.2"
        },
        {
            "name_short": "Benchpress",
            "name_full": "Benchpress (structure learning benchmark)",
            "brief_description": "A scalable, platform-independent workflow for benchmarking causal structure learning algorithms across many datasets and algorithm variations.",
            "citation_title": "Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models",
            "mention_or_use": "use",
            "method_name": "Benchpress benchmarking framework",
            "method_description": "Benchpress offers dataset generation utilities, standardized evaluation protocols and supports over 40 variations of state-of-the-art causal discovery algorithms, enabling researchers to compare algorithmic robustness across datasets with different types and amounts of distractors.",
            "environment_name": "benchmarking and simulated experiments",
            "environment_description": "Designed to run systematic evaluations across many synthetic and real datasets, enabling controlled virtual-lab-style comparisons (e.g., varying confounding, noise, distractors).",
            "handles_distractors": null,
            "distractor_handling_technique": "N/A (framework to evaluate algorithms that may include distractor-handling techniques)",
            "spurious_signal_types": "supports evaluations with confounding, irrelevant variables, measurement noise",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Recommended as a benchmark to systematically compare causal discovery methods under different distractor scenarios and distributional shifts in virtual experiments.",
            "uuid": "e781.3"
        },
        {
            "name_short": "CausalWorld",
            "name_full": "CausalWorld (robotic manipulation benchmark)",
            "brief_description": "A simulated robotic manipulation environment with an explicit causal structure, allowing agents to intervene on variables like masses, colors, or sizes to study transfer and causal reasoning.",
            "citation_title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
            "mention_or_use": "use",
            "method_name": "environment: CausalWorld",
            "method_description": "CausalWorld provides a suite of robotic manipulation tasks embedded in an environment with a known causal graph connecting controllable properties and outcomes; it supports interventions and is intended for benchmarking causal structure learning and transfer in reinforcement learning.",
            "environment_name": "CausalWorld (robotic manipulation virtual lab)",
            "environment_description": "Open-ended interactive simulation (robotic arm and objects) with explicit causal generative factors that users/agents can intervene on; enables active experimentation and virtual-lab style interventions.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "can simulate mis-specified or spurious correlations through background or style variables (configurable)",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Supports active interventions on manipulable variables to probe causal mechanisms and learn structure via experiments",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Highlighted as a virtual lab / benchmark that provides a ground-truth causal structure and supports interventions, thus enabling study of distractors and spurious correlations in interactive environments.",
            "uuid": "e781.4"
        },
        {
            "name_short": "ChemicalEnv",
            "name_full": "Chemical Environment (synthetic RL causal benchmark)",
            "brief_description": "A synthetic environment for exhaustive evaluation of causal RL agents where changing one object's property (e.g., color) can affect others according to an underlying causal graph.",
            "citation_title": "Chemical Environment (described in paper)",
            "mention_or_use": "mention",
            "method_name": "environment: Chemical Environment",
            "method_description": "Synthetic interactive environment where agents perform interventions (e.g., change object colors) and the dynamics are governed by an explicit causal DAG (user-defined or random), used to evaluate causal discovery and causal RL approaches.",
            "environment_name": "Chemical Environment (synthetic interactive)",
            "environment_description": "Interactive, controlled virtual environment with explicit causal dynamics and causal graph generation; designed for exhaustive causal RL testing and identification of causal mechanisms with potential distractors.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "inter-variable dependencies that can produce spurious associations if not modeled correctly",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Enables systematic interventions according to the causal graph for discovery and refutation",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Described as an environment explicitly intended to test causal discovery under controlled interactive interventions; useful for studying how methods handle interventional signals vs. spurious correlations.",
            "uuid": "e781.5"
        },
        {
            "name_short": "AugSel",
            "name_full": "Selecting Data Augmentation for Simulating Interventions (Ilse et al.)",
            "brief_description": "A data-augmentation selection method that identifies augmentations which emulate interventions on style/domain variables by choosing transformations that most reduce a domain classifier's accuracy, thereby destroying domain-specific (spurious) information.",
            "citation_title": "Selecting data augmentation for simulating interventions",
            "mention_or_use": "use",
            "method_name": "augmentation-selection via domain classifier",
            "method_description": "Train a domain classifier to predict domain labels; evaluate candidate augmentations and select the one(s) that minimize the domain classifier's accuracy (i.e., remove most domain-specific information). Use the selected augmentation(s) as intervention-like data augmentations to reduce model reliance on spurious style variables.",
            "environment_name": "image classification / domain-generalization tasks (e.g., Rotated/Colored MNIST)",
            "environment_description": "Applies to datasets exhibiting domain/style variation (virtual-lab or simulation-produced images), uses augmentation as simulated interventions rather than requiring an explicit SCM.",
            "handles_distractors": true,
            "distractor_handling_technique": "Data augmentation chosen to 'destroy' domain-specific/style signals; uses learned domain classifier to detect such style features",
            "spurious_signal_types": "style/domain variables (background, rotation, color) that are spuriously correlated with labels",
            "detection_method": "Train domain classifier; transformations that most reduce domain classifier accuracy are assumed to target spurious/style signals",
            "downweighting_method": "Indirect: eliminate or reduce spurious signal by augmenting training data to remove domain-specific cues rather than explicit weighting",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "This technique provides a practical, SCM-free way to simulate interventions on style variables and reduces models' dependence on spurious domain features by selecting augmentations that 'destroy' domain-identifying signals.",
            "uuid": "e781.6"
        },
        {
            "name_short": "GenInterv",
            "name_full": "Generative Interventions via GANSpace (Mao et al.)",
            "brief_description": "A method to generate intervention-like data by discovering interpretable axes in GAN latent space (GANSpace) corresponding to high-level controllable factors, then manipulating those axes to simulate interventions.",
            "citation_title": "Generative interventions for causal learning",
            "mention_or_use": "use",
            "method_name": "GAN latent-space intervention (GANSpace controls)",
            "method_description": "Identify interpretable latent directions in GAN generators (via GANSpace or similar) that control semantically meaningful properties; create data by manipulating these controls to emulate interventions on causal factors, allowing causal analysis without explicit SCMs.",
            "environment_name": "generative-model-based data augmentation / simulated image domains",
            "environment_description": "Works on GAN-generated image domains where latent axes correspond to manipulable factors (size, rotation, color) enabling virtual interventions in a simulated lab.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit manipulation of GAN controls corresponding to style or content variables to isolate causal factors and remove confounding/spurious associations",
            "spurious_signal_types": "style variables and other latent features that co-vary with labels in training data",
            "detection_method": "Discover interpretable latent axes via GANSpace and evaluate their effect on downstream predictors",
            "downweighting_method": "By generating counterfactuals/interventions that break spurious correlations, the method reduces model dependence on confounding style variables",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Generative interventions via GAN latent controls can emulate interventions in simulated environments and help disentangle causal vs spurious factors, improving robustness to style-based distractors.",
            "uuid": "e781.7"
        },
        {
            "name_short": "CausalBoot",
            "name_full": "Causal Bootstrapping",
            "brief_description": "A bootstrapping-based data sampling method that leverages a causal graph to sample data whose deducible observations better reflect the domain's causal relationships, improving robustness to spurious correlations.",
            "citation_title": "Causal bootstrapping",
            "mention_or_use": "use",
            "method_name": "causal bootstrapping",
            "method_description": "Use a provided/assumed causal graph to guide resampling of the dataset so that sampled datasets emphasize causal relations implied by the graph (e.g., sample conditioned on causal parents), thereby reducing spurious associations in training data.",
            "environment_name": "general supervised learning / simulation-informed sampling",
            "environment_description": "A pre-processing technique applicable to datasets where a causal graph is known or assumed; generates training samples aligned with causal structure to reduce spurious cues.",
            "handles_distractors": true,
            "distractor_handling_technique": "Causal-graph-guided resampling to reduce frequency/effect of spurious correlations (implicit downweighting by resampling)",
            "spurious_signal_types": "irrelevant co-occurrences and domain-specific style correlations",
            "detection_method": "Requires a causal graph; spurious signals are inferred by mismatch between observed associations and graph-implied dependencies",
            "downweighting_method": "Resampling (bootstrap) to upweight causally consistent samples and downweight spurious associations",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Causal bootstrapping can produce training distributions that emphasize causal patterns and improve robustness to spurious correlations induced by style/domain variables.",
            "uuid": "e781.8"
        },
        {
            "name_short": "IRM",
            "name_full": "Invariant Risk Minimization",
            "brief_description": "An in-processing objective that seeks predictors whose optimal classifier components are invariant across multiple environments, aiming to recover causal features that generalize under distributional shifts.",
            "citation_title": "Invariant risk minimization",
            "mention_or_use": "use",
            "method_name": "Invariant Risk Minimization (IRM)",
            "method_description": "IRM augments training with a penalty that encourages a representation and classifier that have simultaneously optimal performance across multiple training environments, thereby favoring features that are invariant (putatively causal) across environments and discouraging reliance on environment-specific (spurious) features.",
            "environment_name": "multi-environment / virtual lab datasets (multiple domains)",
            "environment_description": "Requires multiple environments/domains (natural or simulated) with distributional variation; encourages invariance across these environments and can be applied in interactive/virtual settings where environment interventions are simulated.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization to enforce invariance of predictors across environments, thereby downweighting features that vary (style/spurious features)",
            "spurious_signal_types": "style/domain variables, non-invariant features that change across environments",
            "detection_method": "Identify features whose predictive performance is unstable across environments (via environment-wise losses)",
            "downweighting_method": "Regularization term that penalizes non-invariant predictors; pushes model to ignore environment-specific signals",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Described as a core in-processing causal objective leveraging multiple environments to separate invariant (causal) features from spurious (style) features and empirically shown to improve OOD robustness in many settings.",
            "uuid": "e781.9"
        },
        {
            "name_short": "ISR",
            "name_full": "Invariant-Feature Subspace Recovery (ISR)",
            "brief_description": "A post-processing method that recovers a subspace of invariant features from hidden layer representations and fits a linear predictor in that subspace to improve out-of-distribution performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "Invariant-Feature Subspace Recovery (ISR)",
            "method_description": "Extract feature representations from a trained model's hidden layers, estimate the subspace spanned by invariant (content) features, and train/replace the classification head with a linear predictor restricted to this invariant subspace, thereby removing sensitivity to spurious style features.",
            "environment_name": "trained-model post-processing (general / OOD evaluation)",
            "environment_description": "Applied after model training using hidden representations; useful when distribution shifts occur (including virtual-lab/test-time shifts) and when one can identify invariant components post-hoc.",
            "handles_distractors": true,
            "distractor_handling_technique": "Subspace recovery and projection to isolate invariant features and remove components associated with spurious/distractor signals",
            "spurious_signal_types": "style/background variables and other non-invariant factors",
            "detection_method": "Estimate invariant subspace via analysis of representations across environments or using algorithmic procedures to identify stable directions",
            "downweighting_method": "Project representations onto invariant subspace (effectively downweighting orthogonal/distractor directions)",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Post-processing by recovering invariant subspace can substantially improve robustness of already-trained models on OOD datasets by removing representation components tied to spurious signals.",
            "uuid": "e781.10"
        },
        {
            "name_short": "CF-Reg",
            "name_full": "Counterfactual Regularization (post-processing)",
            "brief_description": "A post-processing approach that estimates counterfactual (deconfounded) predictions and uses them to regularize or replace factual predictions, thereby removing confounding/distractor effects.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Counterfactual regularization / deconfounding prediction",
            "method_description": "Estimate a counterfactual prediction that removes estimated effects of unobserved confounders (via SCM-guided estimation) and combine/subtract it from the factual prediction to obtain a deconfounded output; can also be used as a regularizer during training to encourage deconfounded representations.",
            "environment_name": "trajectory prediction / multi-domain settings (example use-case)",
            "environment_description": "Applied in settings where unobserved confounders/distractors bias predictions (e.g., multi-domain trajectory forecasting); requires estimating counterfactuals via a structural model or proxy estimation techniques.",
            "handles_distractors": true,
            "distractor_handling_technique": "Estimate counterfactuals that remove confounder influence and subtract/deconfound factual predictions",
            "spurious_signal_types": "unobserved confounders, domain-specific biases",
            "detection_method": "Estimate counterfactuals and inspect differences between factual and counterfactual predictions to reveal confounding influence",
            "downweighting_method": "Subtract estimated counterfactual component to obtain deconfounded predictions (implicit downweighting of confounder effects)",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as an effective post-processing approach to remove confounding effects and improve multi-domain prediction robustness by explicitly estimating and removing counterfactual contribution of distractors.",
            "uuid": "e781.11"
        },
        {
            "name_short": "RIMs",
            "name_full": "Recurrent Independent Mechanisms (RIMs)",
            "brief_description": "A modular recurrent architecture motivated by the independent causal mechanisms principle, where separate subsystems (modules) implement autonomous dynamics and communicate sparsely to capture distinct causal mechanisms.",
            "citation_title": "Recurrent Independent Mechanisms",
            "mention_or_use": "use",
            "method_name": "Recurrent Independent Mechanisms (RIMs)",
            "method_description": "Architectural design: multiple recurrent modules each learn specialized transition dynamics (mechanisms) and interact sparsely via attention-like routing, enforcing modularity and independence of causal mechanisms which can reduce entanglement with spurious signals.",
            "environment_name": "sequential/physical reasoning tasks (e.g., Bouncing Ball, sequential MNIST)",
            "environment_description": "Intended for sequential prediction and interactive environments where independent mechanisms govern different aspects of the dynamics; promotes robustness to distributional shifts by modeling modular causal processes.",
            "handles_distractors": true,
            "distractor_handling_technique": "Modularization and sparse inter-module communication to isolate causal mechanisms from nuisance/distractor components",
            "spurious_signal_types": "non-causal entangled features and noise that manifest inconsistently across contexts",
            "detection_method": null,
            "downweighting_method": "Architectural separation reduces influence of spurious signals by assigning them to separate modules or ignoring them via sparse routing",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Recommended as an architecture aligning with independent causal mechanisms which empirically shows improved robustness to distributional shifts and reduced sensitivity to spurious entanglement in sequential/interacting domains.",
            "uuid": "e781.12"
        },
        {
            "name_short": "deepCAMA",
            "name_full": "deep Causal Manipulation Augmented Model (deep CAMA)",
            "brief_description": "A deep generative model consistent with a given causal graph that separates manipulable (intervenable) variables from non-manipulable (style) variables via dedicated autoencoders.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "deep CAMA",
            "method_description": "Model architecture that incorporates separate latent autoencoders for manipulable variables (those we may intervene on) and non-manipulable style/confounder variables; the generative model is trained consistent with an assumed causal graph, enabling disentangling of causal features from spurious ones.",
            "environment_name": "image generation / manipulation tasks (e.g., MNIST variants)",
            "environment_description": "Works in domains where manipulations (rotations, color) correspond to controllable latent factors and non-manipulable factors (handwriting style) act as distractors; suitable for simulated/virtual manipulations.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit latent disentangling using separate autoencoders for manipulable vs non-manipulable variables guided by a causal graph",
            "spurious_signal_types": "non-manipulable style variables that are spuriously correlated with labels",
            "detection_method": null,
            "downweighting_method": "By separating latent spaces, downstream predictors can ignore non-manipulable latent dimensions tied to spurious signals",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Model design that enforces causal-consistent generative structure leading to improved discrimination between causal and spurious features and demonstrated robustness gains against adversarial attacks in MNIST-like settings.",
            "uuid": "e781.13"
        },
        {
            "name_short": "ModelSelection-DAG",
            "name_full": "Causal Model Selection Using DAG Constraints (Kyono & van der Schaar)",
            "brief_description": "A post-hoc model selection pipeline that scores trained models by how well their predictions comply with a provided (possibly partial) causal DAG, selecting models whose predictions better respect causal relationships to improve OOD robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "causal-DAG-based model selection",
            "method_description": "Given a set of trained models and a causal DAG, compute scores reflecting how consistent each model's predictions are with constraints implied by the DAG (e.g., conditional independencies or causal ordering); choose the model with highest causal-consistency score for deployment to improve robustness.",
            "environment_name": "model selection / post-training evaluation",
            "environment_description": "Post-processing/tooling applicable when a (partial) causal graph is available and multiple candidate models exist; useful in virtual-lab/benchmarked settings to pick causally-aligned predictors.",
            "handles_distractors": true,
            "distractor_handling_technique": "Selects models that ignore spurious associations by favoring those whose outputs respect causal constraints of the DAG",
            "spurious_signal_types": "spurious correlations that violate DAG-implied invariances or conditional independencies",
            "detection_method": "Score models by violation counts or statistical mismatch vs DAG-implied constraints",
            "downweighting_method": "Indirect: choose a model that de facto downweights spurious features by scoring consistency rather than altering model weights",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Proposes a practical post-hoc selection technique to prefer models whose predictions align with causal knowledge, empirically improving robustness under distribution shifts.",
            "uuid": "e781.14"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Selecting data augmentation for simulating interventions",
            "rating": 2,
            "sanitized_title": "selecting_data_augmentation_for_simulating_interventions"
        },
        {
            "paper_title": "Generative interventions for causal learning",
            "rating": 2,
            "sanitized_title": "generative_interventions_for_causal_learning"
        },
        {
            "paper_title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
            "rating": 2,
            "sanitized_title": "causalworld_a_robotic_manipulation_benchmark_for_causal_structure_and_transfer_learning"
        },
        {
            "paper_title": "Invariant risk minimization",
            "rating": 2,
            "sanitized_title": "invariant_risk_minimization"
        },
        {
            "paper_title": "Recurrent Independent Mechanisms",
            "rating": 2,
            "sanitized_title": "recurrent_independent_mechanisms"
        },
        {
            "paper_title": "gCastle: A Python Toolbox for Causal Discovery",
            "rating": 2,
            "sanitized_title": "gcastle_a_python_toolbox_for_causal_discovery"
        },
        {
            "paper_title": "Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models",
            "rating": 2,
            "sanitized_title": "benchpress_a_scalable_and_platformindependent_workflow_for_benchmarking_structure_learning_algorithms_for_graphical_models"
        },
        {
            "paper_title": "Systematic evaluation of causal discovery in visual model based reinforcement learning",
            "rating": 2,
            "sanitized_title": "systematic_evaluation_of_causal_discovery_in_visual_model_based_reinforcement_learning"
        },
        {
            "paper_title": "Causal bootstrapping",
            "rating": 1,
            "sanitized_title": "causal_bootstrapping"
        },
        {
            "paper_title": "Federated Causal Discovery",
            "rating": 1,
            "sanitized_title": "federated_causal_discovery"
        }
    ],
    "cost": 0.0342735,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Review of the Role of Causality in Developing Trustworthy AI Systems</p>
<p>Niloy Ganguly 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Dren Fazlija 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Maryam Badar 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Marco Fisichella 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Sandi-Pan Sikdar 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Johanna Schrader 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Jonas Wallat 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Koustav Rudra 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Mano-Lis Koubarakis 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Gourab K Patro 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Wadhah Zai 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>E L Amri 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>Wolfgang Nejdl 
L3S Research Center
Leibniz University of Hannover
Germany</p>
<p>A Review of the Role of Causality in Developing Trustworthy AI Systems
CCS Concepts:  Information systems Computing methodologies  Artificial intelligenceAdditional Key Words and Phrases: Causality, Counterfactual, Interpretability, Explainability, Robustness, Bias, Discrimination, Fairness, Privacy, Safety, Healthcare
State-of-the-art AI models largely lack an understanding of the cause-effect relationship that governs human understanding of the real world. Consequently, these models do not generalize to unseen data, often produce unfair results, and are difficult to interpret. This has led to efforts to improve the trustworthiness aspects of AI models. Recently, causal modeling and inference methods have emerged as powerful tools. This review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of AI models. We hope that our contribution will motivate future research on causality-based solutions for trustworthy AI.</p>
<p>INTRODUCTION</p>
<p>The Deep Learning based systems, in recent years, have produced superior results on a wide array of tasks; however, they generally have limited understanding of the relationship between causes and effects in their domain [197]. As a result, they are often brittle and unable to adapt to new domains, can treat individuals or subgroups unfairly, and have limited ability to explain their actions or recommendations [197,235] reducing the trust of human users [118]. Following this, a new area of research, trustworthy AI, has recently received much attention from several policymakers and other regulatory organizations. The resulting guidelines (e.g., [184,186,187]), introduced to increase trust in AI systems, make developing trustworthy AI not only a technical (research) and social endeavor but also an organizational and (legal) obligational requirement.</p>
<p>In this paper, we set out to demonstrate, through an extensive survey, that causal modeling and reasoning is an emerging and very useful tool for enabling current AI systems to become trustworthy. Causality is the science of reasoning about causes and effects. Cause-and-effect relationships are central to how we make sense of the world around us, how we act upon it, and how we respond to changes in our environment. In AI, research in causality was pioneered by the Turing award winner Judea Pearl long back in his 1995 seminal paper [194]. Since then, many researchers have contributed to the development of a solid mathematical basis for causality; see, for example, the books [79,196,201], the survey [90] and seminal papers [197,235].</p>
<p>The TAILOR project [166], an initiative of EU Horizon 2020, with the main objective of integrating learning, reasoning, and optimization in next-generation AI systems, in its first strategic research and innovation roadmap, identifies the following dimensions of AI systems which contribute to trustworthiness: interpretability or explainability, safety and robustness, fairness, equity and justice, accountability and reproducibility, privacy, and sustainability. This is corroborated by several recent surveys and reports [34,110,118,247,283]. Following the above-mentioned works, we select a number of properties desired for the trustworthiness of AI and survey the role of causality in achieving these properties:</p>
<p> Interpretability Can the AI system's output be justified with an explanation that is meaningful to the user?  Fairness: How can we ensure that the AI system is not biased, does not discriminate (e.g., against minorities, disabled people, people of a certain gender, etc.), but provides fair prediction and recommendation?  Robustness: How sensitive is the AI system's output to changes in the input? Does the performance vary significantly in the case of distributional shifts?  Privacy: Is the AI system susceptible to attacks by adversaries? Does the system leak private information? Does the AI system respect user privacy? How can we develop AI systems that ensure user privacy?  Safety and Accountability: What are the risks for humans after an AI system is deployed in the real world? Is the AI system safe? Who or what is responsible for the actions (or failures) of the AI system after deployment? How to audit an AI system and its impact?</p>
<p>There have been several current surveys on trustworthy AI [34,118,247,260,283] but none has paid significant attention to the role of causality in developing trustworthy AI systems. We provide a detailed comparison of our survey to other related surveys in Section 2.6. We survey the contributions of causality in upholding various trustworthy AI aspects (interpretability, fairness, robustness, privacy, safety, and auditing) in Sections 3 to 7. Considering the recent interest in AI for healthcare, we discuss various trustworthy aspects needed in the health domain and survey how causality has played an important role in ensuring trust in AI-based healthcare applications (Section 8). We also list available datasets, tools and packages relevant to causality-based trustworthy AI research and development (Appendices A to F).</p>
<p>PRELIMINARIES ON CAUSALITY</p>
<p>The two most powerful causal frameworks are (i) structural causal models [194,196] developed in AI, and Rubin's (ii) potential outcomes framework [224] developed in Statistics. Here, we give short introductions to structural causal models (Section 2.3), the potential outcomes framework (Section 2.4), and then discuss how they are connected Section 2.5. Causal graphical models [253] is another popular framework that we do not cover here. Before discussing the frameworks, we first list some basic notations, and discuss some fundamental concepts on causality: the difference between correlation and causation in Section 2.1, and the ladder of causation through association, intervention, and counterfactuals in Section 2.2. Notations: We use non-boldface uppercase letters (e.g., ) to denote single random variables and non-boldface lowercase letters (e.g., ) to denote their values. Boldface uppercase letters (e.g., X) denote a collection of variables, and boldface lowercase letters (e.g., x) their values. Random variables can have continuous or discrete or categorical values.</p>
<p>Correlation vs. Causation</p>
<p>Statisticians and others often state that "correlation does not imply causation" and illustrate it with anecdotal examples. Schlkopf and von Kgelgen [237], for example, cite the correlation between chocolate consumption and the number of Nobel prizes per capita from [164], which obviously should not lead us to think that increasing chocolate consumption helps in winning a Nobel prize. We first define causal effect relationships and causal graphs before formally discussing correlation vs. causation. Causal effect relationships: Following the definition given by Schlkopf and von Kgelgen [237], we say that a random variable has a causal effect on a random variable if there exist   such that the probability distribution of after intervening on and setting it to differs from the probability distribution of after setting to  . This notion of a causal effect is unidirectional and asymmetric; this is not true for correlation which is a symmetric relationship. Note Judea Pearl [194,196] first presented the approach of intervention for finding causal effects. Causal effect relationships are usually represented graphically using causal graphs. Fig. 1. Causal graph of a university admission dataset representing fair and unfair causal pathways: R, D, Q, H, A, Y are causal variables representing "race", "choice of department", "qualification", "hobbies", "address", and "admission outcome" respectively.</p>
<p>Causal graphs: A causal graph is a directed acyclic graph where nodes are the variables/attributes of the data under consideration and edges denote direct causal effects. For example, consider Figure 1, which shows a causal graph related to a dataset of university admissions. In this graph, , , , , , and are causal variables representing "race", "choice of department", "qualification", "hobbies", "address", and "admission outcome". In a causal graph, a causal path is an acyclic sequence of adjacent nodes from the starting node to the last node. For example, in Figure 1,   and   are examples of causal paths. In this figure, the variables , , , , and directly affect the outcome variable , whereas the variable also indirectly causally affects through , , and .</p>
<p>The connection between correlation and causation has been expressed by Reichenbach [216] using the common cause principle (taken here verbatim from [237]): If two random variables and are statistically dependent, then there exists a random variable which causally influences both of them and which explains all their dependence in the sense of rendering them conditionally independent. As a special case, may coincide with or . The three possible cases for random variables , and are: (i) random variables and coincide, (ii) random variables and coincide, or (iii) is a new unobserved variable which causally influences both and . For the example of chocolate consumption (variable ) vs. the number of Nobel prize winners (variable ), obviously, is not a causal effect of and vice versa. The common cause principle then tells us that there is another random variable (e.g., economic prosperity) that has and as causal effects. In this example, the unobserved variable introduces a spurious correlation (i.e., a correlation that does not stem from a direct causal effect relationship between two observed variables). Such variables are referred to as confounders or confounding variables.</p>
<p>The Ladder of Causation</p>
<p>Pearl and Mackenzie [198] presented the ladder of causation as an informal way of explaining the principles of causality. The ladder of causation has three levels: seeing, doing, and imagining, and forms a directional 3-level hierarchy in the sense that questions at a level can only be answered if information from previous level(s) is available [197]. Next, we discuss them in detail.</p>
<p>(1) Association: The first level (seeing or observing) corresponds to detecting associations in the domain under study and making predictions based on these associations. Most animals, as well as most of today's (purely statistical) machine learning systems (including sophisticated deep learning systems), are on this level. The same is true for popular probabilistic frameworks such as Bayesian networks since this level is characterized by conditional probability sentences e.g., the sentence ( = | = ) = 0.8. For some associations, it might be easy to find causal interpretations, while for others it might not. But agents on this level of the ladder, cannot differentiate between a cause and an effect. Therefore, they cannot answer "why" questions.</p>
<p>(2) Intervention: The second level (doing) corresponds to interventions. Most tool users are on this level if they plan their actions and not imitating the actions of others. To see the effects of interventions, we can do experiments (e.g., a randomized controlled trial). Pearl and his colleagues have formalized reasoning on this level of the ladder of causation using the do calculus [79,196,198]. This level of the ladder is characterized by expressions such as ( = | ( = ), = ) which means "the probability of event = given that we intervene and set the value of to and subsequently observe event = " [197].</p>
<p>(3) Counterfactuals: The third level (imagining) corresponds to counterfactual reasoning and its associated modal reasoning capabilities (e.g., retrospection, introspection, etc.). On this level, we can imagine worlds that do not exist, including worlds that contradict the world in which we live, and infer why the phenomena we have observed in our domain have taken place. This level of the ladder is characterized by expressions of the form ( = = | =  , =  ) which stand for "the probability that event = would be observed had been , given that we actually observed the events =  and =  " [197]. Such expressions can be computed only if we have a causal formalism such as the structural causal networks presented in Section 2.3 below. Given such a structure, one can then also formalize level 3 reasoning through do calculus.</p>
<p>Structural Causal Models (SCM)</p>
<p>Structural causal models consist of causal graphs and structural equations. Following the definition by Bareinboim et al. [18], a structural causal model (SCM) M is a 4-tuple U, V, F , (U) where: U is a set of background variables, also called exogenous variables, that are determined by factors outside the model; V is a set of variables, called endogenous variables, that are determined by other variables in the model i.e., variables in U  V; F is a set of functions { 1 , 2 , . . . , } such that each is a mapping from the respective domains of { }  PA to , where  U, the variables PA are the parents of variable and are such that PA  V \ { }, and the entire set F forms a mapping from U to V, so, for = 1, . . . , , each  F is such that  (PA , ), i.e., it assigns a value to that depends on the values of the parents of and the value of the exogeneous variable ;</p>
<p>(U) is a probability function defined over the domain of U. The exogenous ones are determined "outside" of the causal model (therefore assumed to be independent), and their associated probability distribution (U) gives a summary of the state of the world outside the domain.</p>
<p>The expression  (PA , ) in the above definition are called structural equations 1 . The structural equations induce a causal graph for the SCM. The nodes of the graph are the variables in the set V, and there is a directed edge from each variable in PA to for all . In other words, the structural equations encode the direct causal effects for each edge in the causal graph, and they can be used to determine the value of each endogenous variable in terms of the values of the exogenous variable and the values of the endogenous variables PA that are parents of . The causal processes encoded by structural equations are assumed to be invariant unless we explicitly intervene on them using the do calculus of the second level of the ladder of causality [18].</p>
<p>SCMs can be used for various inference tasks that the forthcoming sections of this paper show to be useful for achieving the trustworthiness of AI systems. Some well-known tasks are listed below.</p>
<p> The first such task is causal reasoning, which is the process of deriving conclusions from a causal model e.g., an SCM. SCMs can also be used to study the effects of interventions or distribution changes or carry out counterfactual reasoning [196,201].  The opposite task of causal reasoning is causal discovery or causal structure learning, which is the process of inferring SCMs from data, assumptions, empirical observations, or from data under interventions or distribution changes [196,201].  Another interesting task is causal mediation, which is the process of looking for the mechanism that explains how a cause is connected with an effect [195]. In causal mediation, we may have an SCM   where is the cause, is the effect, and is the mediator i.e., a variable that can be used to answer the question "Why causes ?". For example, the SCM   , explains why citrus fruits were important in helping sailors avoid scurvy in the 1800s [198]. In such networks, is an indirect effect of as opposed to a direct effect, which would have been denoted by  .</p>
<p>Potential Outcomes (PO) Framework</p>
<p>The potential outcomes approach to causality was developed to make statistically valid statements even in cases where randomized controlled studies are not or only partially possible. This section gives a brief introduction to the PO framework [224].</p>
<p>The missing data problem in individualized treatment effect (ITE): We quote an example from Ozer et al. [190]. Consider two possible outcomes for one patient having resectable gallbladder cancer, "survival" and "no survival". The task is to measure the causal effect of a treatment (e.g., chemotherapy). Here, we have a binary treatment variable with = 1 if the person is getting the chemotherapy and = 0 otherwise (control), and its effect on an outcome variable (typically a measure of health) is to be found. Note that (1) and (0) are unobserved outcomes for = 1 and = 0 respectively. Then the individualized treatment effect can be captured by the difference between the two potential outcomes, i.e., = (1)  (0). Since only one of the outcomes will take place, we can not have both (1) and (0) for an individual. To overcome this problem, the PO framework takes some assumptions and estimates average causal effects over a population instead of individual effects, which we detail next.</p>
<p>Assumptions to overcome the missing data problem:</p>
<p>The PO framework relies on the following assumptions: (a) The stable unit treatment value assumption. The observation on one unit should be unaffected by the assignment of treatments to the other units; it is a reasonable assumption in many situations like controlled studies, since different units can form independent samples from a population. (b) The consistency assumption. The potential outcome for an individual remains consistent and converges. Any variation within the exposure group (treatment or control) would result in the same outcome for that individual. Using these assumptions, the PO framework estimates average causal effects over a population, as detailed next.</p>
<p>The average treatment effect (ATE) in PO: Since the ITE ( (1) (0)) under a deterministic treatment model can not be found, an average treatment effect on a population is calculated. The difference between the expected outcome in the treatment group and the expected outcome in the control group is expressed as E[ |1]  E[ |0]. So, the ATE can be calculated by first randomizing the treatment and control group assignments of units, followed by a comparison of mean outcomes for treated and untreated units. In real-world settings, some covariates (e.g., smoking and drinking habits) also affect the outcome along with the treatment. Thus, they must be taken into account.</p>
<p>Propensity score matching to take care of covariates: In cases of covariates, one must balance the covariate distribution between the treated and untreated cohorts, propensity score matching [222] is one of the most popular methods used for this. Propensity scores are probabilities of units being assigned to different treatment groups based on the observed covariates, and these are estimated using logistic regression over the covariates. Now the method basically aims to make two groups comparable to each other in terms of covariates, thereby accounting for selection bias [54]. Essentially, each patient from one group is matched with another one for the other group based on the propensity scores. Units (patients) with no matching from the other treatment group are often removed and not used in the ATE calculation. Other measures, such as the Mahalanobis metric, were excluded from this review. We refer to [255] for an extensive overview of matching methods, including definitions of prominent matching metrics.</p>
<p>Connection between SCM and PO</p>
<p>The above discussion has assumed that individuals (or units) are binary quantities. However, this is not a good assumption when dealing with complex units such as people e.g., in the health domain. In these cases, potential outcomes can be defined as random variables and a clear connection to the structural equations with exogenous noise variables in SCMs can be defined. This observation results in the equivalence of the two frameworks, a result which has been originally shown in [196]. [237] explains this equivalence of the two frameworks in the following simple way:
( ) = | ( = ) in an SCM with U = u
This informally means that an individual in the PO framework corresponds to a particular value of an exogenous variable in an SCM. The potential outcome is deterministic once we know U, but since we do not observe u , the counterfactual outcome is treated as a random variable [237].</p>
<p>Prior Surveys</p>
<p>Some recent survey papers have discussed causality and trustworthy machine learning. Most of them focus either exclusively on causality [76,234] or trustworthy aspects of machine learning [119], only a few papers tried to cover both causality and different aspects of trustworthiness [117,235].</p>
<p>Apart from general coverage of important trustworthiness aspects, some of the surveys explicitly covered interpretability and fairness aspects in detail. Guidotti et al. [88] and Linardatos et al. [142] provided a review of machine learning interpretability methods that deal with explaining black box models. Zhou et al. [311] performed a survey on evaluation metrics of explainability. Makhlouf et al. [156] pointed out the difficulty in choosing a particular fairness notion in a given domain and scenario. Mehrabi et al. [162] conducted a survey focused on the application of AI for fairnessaware learning in various domains. Moraffah et al. [171] performed a survey on causal inference on model, example-based interpretability, and fairness. In contrast to the above approaches, we provide detailed coverage of causality-based methods over a wide variety of trustworthy aspects.</p>
<p>Kaddour et al. [117] and Schlkopf et al. [235] provide an overview of the application of causality to address trustworthy aspects. Kaddour et al. [117] discussed different causal approaches, such as causal supervised learning and causal generative modeling, and their applications on explainability, fairness, and robustness. They did not cover the application phases of different methods in detail, i.e., whether causal approaches could be applied in pre-processing, in-processing, or post-processing stages, and did not provide more extensive coverage on different aspects of trustworthiness, though. Our work complements these prior works and extends the discussion to robustness, privacy, safety, and accountability. Apart from structural causal models, we also include a discussion on the PO framework which helps in causal analysis from a statistical perspective. Table 1 provides the coverage statistics of the existing surveys. Note that, this table covers the generic and domain-specific surveys that used causality as a driving force to achieve trustworthiness. It is evident from the table that our survey covers the causality and trustworthiness aspects in more detail. Table 1. Comparison of our survey with related causality and trustworthy-based survey papers.</p>
<p>Surveys</p>
<p>Trustworthiness Aspects Domain Interpretability Fairness Accountability Robustness Privacy Safety Security Kaddour et al. [117] X X -X ----Schlkopf and von Kgelgen [237] -
- - X - - - - Guidotti et al. [88] X - - - - - - - Linardatos et al. [142] X - - - - - - - Zhou et al. [311] X - - - - - - - Makhlouf et al. [156] - X - - - - - - Mehrabi et al. [162] - X - - - - - - Moraffah et al. [171] X X - - - - - - Zhang et al. [306] X X - X - - - Healthcare Sanchez et al. [231] X X - X - - - Healthcare Vlontzos et al. [275] X X - X - X - Healthcare &amp; Image analysis Gao et al. [76] - - - - - - - Recommendation Our Survey X X X X X X X -
In the next sections (Sections 3 to 7), we discuss the listed trustworthy AI properties one by one and discuss how causality is helpful in upholding them in the context of AI.</p>
<p>CAUSALITY AND INTERPRETABILITY</p>
<p>Understanding latent models is one of the central challenges in the development and deployment of machine learning applications. Recent trends have shown that interpretability is sacrificed for overparameterization and the promised generalizability [33,207]. However, to apply these models in high-stakes scenarios such as the legal or medical domain, we will need (and potentially be legally required by the EU AI Act 2 ) to build understandable models. To ensure that the model's explanation communicates the true reasons behind a prediction, the explanation itself should preferably be causal. This section aims to provide an overview of the early stages of causality in existing interpretability research, its promises, and how it might help build more trustworthy models.</p>
<p>Preliminaries</p>
<p>Evaluation Criteria. Faithfulness and causability are two important criteria to measure the interpretability of non-causal methods. Faithfulness measures how well the explanation matches the actual underlying processes of the model. To gain human confidence, the accuracy of the explanation is of immense importance in high-stake scenarios, such as medical or health applications. Various metrics have been proposed to measure faithfulness, e.g., feature attribution methods. Causability measures how well explanations depict the causal structure of the problem and can be assessed through a user study using the System Causability Scale (SCS), which uses a Likert scale and is similar to the well-established System Usability Scale (SUS) [100]. A causable explanation helps the recipient build a correct mental model of the problem, which has been shown to greatly impact the user's trust in the system [246]. For an effective AI system, the target audience must be taken into account when evaluating the causability of the method, that is, causal explanations of a model used in the medical domain should provide more details when dedicated to medical experts than an explanation presented to patients.</p>
<p>Like traditional interpretability methods, causal approaches are either interpretable by their model design or are methods that provide post-hoc explanations for non-interpretable models. In this section, we first provide a brief overview of methods that are causally interpretable by design  and then focus on causal post-hoc interpretability. Figure 2 visualizes an overview of the different approaches and the application of causal interpretability.</p>
<p>Causal Interpretability By Design</p>
<p>With the recent surge of interest in causality, existing approaches to interpretable ML, such as feature selection and disentanglement representations, have also been extended to include causality in interpretable methods. In the following, we will give an overview of both approaches in causality.</p>
<p>3.2.1 Sparsity (Causal Feature Selection). Feature selection categorizes the data's attributes into relevant (irrelevant) features, i.e., features that do (not) provide predictive information regarding a target variable, and redundant features, i.e., multiple features with the same predictive information, and removes irrelevant and redundant features from the variable-subset of interest [93]. Causality is introduced into feature selection methods to distinguish between causally relevant and only cooccurring features when choosing the redundant features to keep as relevant attributes [93], which may lead to higher accuracy. To introduce a causal ordering of the features, Bayesian networks (BNs) can be used in causal feature selection to represent causal relationships among features in a DAG by interpreting directed edges as cause-effect relationships. Under the faithfulness assumption, the Markov boundary (MB) of a variable of interest in the BN describes the variable's local causal relationships, i.e., its parents, children, and spouse [296]. In most cases, a model has only one variable of interest, i.e., the target variable, making it unnecessary to learn a full BN. Instead, these approaches [7,8] only learn the MB for the variable of interest, using either constraint-based methods based on conditional independence tests or score-based methods that combine greedy search with scoring functions [296]. However, causal feature selection methods generally are less efficient than their non-causal counterparts due to conditioning on the other features and also are less reliable when the data set is small with high dimensionality [297]. For a more detailed overview, the reader is referred to a comprehensive review of causal feature selection by Yu et al. [296]. They also provide a collection of relevant methods in their package CausalFS.</p>
<p>Disentangled Representations.</p>
<p>A perfectly disentangled latent representation where each dimension represents a human-understandable concept would naturally be interpretable. However, achieving a fully disentangled representation is not feasible in the general case, as approaches require specific training data [241], supervision [182], or predefined concepts [145]. Nevertheless, there are approaches using SCMs (Section 2.3) to identify confounders and then disentangle representations to produce more interpretable models. Reddy et al. [215] propose a dataset to investigate the causal effects in disentangled representations. The proposed dataset contains images of geometric shapes with varying properties (e.g., shape, background, color, size) generated according to an underlying SCM. This enables investigations to determine whether learned disentangled representations follow the same causal structure as used to generate the data. Zheng et al. [310] build a structural causal graph and note that the predictions of recommendation systems are using the intertwined information of user interest, as well as the general popularity of items. Therefore, they propose a multitask learning framework to causally disentangle these properties, where individual interest and popularity representations are learned on separate auxiliary datasets and later concatenated for the recommendation. In addition to the added interpretability of the causally disentangled representation, they also report increased robustness in the non-IID setting [310]. Si et al. [251] continue this line of work by combining search and recommendation data to disentangle latent representation into a causally-relevant personalization part and a causally non-relevant part.</p>
<p>Post-Hoc Causal Interpretability</p>
<p>Post-hoc causal interpretation methods aim at causally explaining existing non-interpretable models after they have been trained, thus supplementing the good performance of complex models with human-understandable causal explanations. Causal methods for explaining non-interpretable models can be divided into two groups: black-box and white-box approaches.</p>
<p>Black-Box.</p>
<p>Model agnostic approaches treat the model as a black-box and provide an explanation considering the input and output to make the model's predictions human-comprehensible without requiring access to the model's parameters. Approaches for causally explaining blackbox models can be separated into two main categories: Feature attribution and counterfactual intervention methods. Feature Attribution: Feature attribution methods quantify the input feature's contribution to the prediction, but unlike non-causal approaches, causal approaches aim to retrieve only causally relevant features. In causal filtering, counterfactual input representations are generated and evaluated by masking potentially influential features, e.g., determined by attention and measuring the deterioration in performance.</p>
<p>Kim and Canny [125] apply this method to real-time videos to estimate the features' causal influence on the prediction. Schwab and Karlen [236] use the importance distribution determined by masking to build a causal explanation model in parallel with the prediction model and minimize the Kullback-Leibler divergence between their importance distributions. Causal methodologies have also been applied in prompt-based interpretability. Cao et al. [37] investigate risk factors and confounders by prompting language models to complete a sentence to elicitate whether the model learned certain information from pre-training. To do so, they built an SCM to identify risk factors by backdoor-paths and propose causal interventions to study the methodologically induced biases. Li et al. [139] further investigates how the input sentence formulation can influence the model predictions by constructing counterfactual model inputs. To do so, they mask potentially relevant context words and study the impact on the amount of factual knowledge retained by the model. Their experiments suggest that co-occurrence, as well as the closeness of the subject (e.g. "Albert Einstein") with the object (e.g. the birthplace), are highly relevant.</p>
<p>Frye et al. [74] extend the notion of Shapley values to asymmetric Shapley values (ASV) and build up a causal framework to quantify the contribution of a feature to the model's prediction. However, this notion requires at least some knowledge about the causal ordering of features as it otherwise reduces to classical symmetric Shapley values without ordering. Janzing et al. [111] introduce a causal view to SHAP, considering the model's inputs as causes of the output. They argue for the use of interventional conditional distributions in SHAP to quantify the contribution of each observation to the output. This approach allows for determining the causal relevance of the input features without prior knowledge about the features' causal relationships. Alvarez-Melis and Jaakkola [9] use perturbation to explain specific input-output pairs for any input-output structured model by constructing a dense bipartite graph of perturbed inputs and their output tokens to infer a causal model using Bayesian logistic regression. A graph partitioning framework derives an explainable dependency graph by minimizing the k-cut. The perturbations introduce uncertainty information via the frequency with which each token occurs and thus allow one to reveal flaws or biases in a model. Counterfactual Interventions: Tan et al. [257] use counterfactual interventions to explain decisions of recommendation models. To create counterfactuals, the authors manipulate input items while optimizing for minimally changed items that reverse the recommendation. The minimal changes now serve as explanations for the original recommendations. Mahajan et al. [155] propose a causal proximity regularizer to incorporate knowledge on causal relations from an SCM and thus constrain generated counterfactual explanations to be actionable recourses that are feasible in real-world settings.</p>
<p>White-Box.</p>
<p>White-box interpretability methods, in contrast to black-box methods, require access to the model parameters and thus are also called model introspective approaches. Causal approaches use causal mediation or probing. Causal Mediation: To identify the components of a model responsible for a biased prediction, Jeoung and Diesner [112], Vig et al. [274] introduce the method of causal mediation analysis. More formally, causal mediation analysis is a method to examine the intermediate processes where independent variables affect dependent variables. By intervening on the model and measuring direct and indirect effects, Vig et al. [274] investigate gender bias in individual neurons and attention heads and find a small subset of attention heads and neurons in the intermediary layers, which are responsible for biased predictions. Jeoung and Diesner [112] follow a similar causal mediation setup but use it to investigate the effects of debiasing techniques, finding that the debiased representations are robust to fine-tuning. Probing: Probing usually involves training a small classifier to predict a property of interest from the model's latent embeddings. Elazar et al. [66] construct a counterfactual embedding without the property of interest (POI). They then infer the usage of POI during inference if the performance drops after the removal of POI information. To construct these counterfactual embeddings, several methodologies have been proposed -either by iteratively training classifiers to identify relevant dimensions in the embedding space [211], posing the removal as a minimax [212], or as a gradientguided search problem [266]. Elazar et al. [66] utilize this causal probing approach and study the effects of part-of-speech knowledge on language modeling. Lasri et al. [132] (causally) probe language models on the usage of grammatical number information, finding that this resides in different layers depending on the token type (verbs/nouns). Furthermore, Tucker et al. [266] utilize their gradient-guided counterfactual creation method and find evidence for language models using tree distance-based embeddings to represent syntax. Finally, Feder et al. [71] investigate the effect of concepts like the presence of political figures on model prediction. Using adversarially trained counterfactual representations (without the concept under investigation), they contrast the classification performance of the standard and counterfactual representations.</p>
<p>Conclusion</p>
<p>The surge in causality papers over the last few years also affected the interpretability field. We believe that either causal -and therefore interpretable by design -models or the usage of causal  [75] In-Processing</p>
<p>Correcting path-specific effects [124,173] [50]</p>
<p>Fairness under distribution shift [55,252] Adversarial learning [140] Post-Processing Constrained optimization [129,288] Estimators [168]  (post-hoc) interpretability for standard ML models will be paramount for building trustworthy models. Existing causal methods [112,274] provide information on the parts of an ML model that causally affect predictions and thus also provide information on biased or erroneous models. However, the underlying processes that lead to biased predictions and, in turn, the best approach to counteract these biases is still not well understood, and methods need to be developed in that direction. We observe structural causal models to be generated and used for many problems but still lack standardized procedures for evaluation. For example, an assessment of the extent to which the provided explanations help humans to causally understand a model's output would be helpful. The SCS [100] provides a suitable framework for causality evaluation, however, its effectiveness depends upon a wider adoption of user studies in evaluating AI research.</p>
<p>CAUSALITY AND FAIRNESS</p>
<p>Addressing issues of fairness is a prerequisite for applying AI-based learning algorithms to support decisions that critically affect people's lives, such as offender recidivism, loan approval, disease diagnosis, hiring, student admissions, etc. A complete understanding of the causal relationships between the sensitive attributes (e.g. gender, race, marital status, etc.) and the predicted outcome may play a crucial role in analyzing and legitimizing the fair or unfair behavior of a learning algorithm. This section aims to provide an overview of the contributions of causality in existing fairness research, its promises, and how it might be helpful in building fairer models. The use of causality to describe and quantify fairness is a distinguishing feature of research conducted in the field of causal fairness. Causality has also proven to be effective in mitigating discrimination. We can divide the state-of-the-art causal frameworks for discrimination mitigation into the following categories: (a) Pre-processing methods, (b) In-processing methods, and (c) Postprocessing methods as presented in Figure 3.</p>
<p>Preliminaries</p>
<p>The notions of fairness can be categorized into two groups: individual and group. Counterfactual fairness [130] is achieved by a predictor for an individual if the probability of achieving the output = remains the same if the value of sensitive attribute changes from  to + as presented in Equation (1), where = \ { , } is the set of all variables except the sensitive and outcome variables.
P( + | = , = + ) = P(  | = , =  )(1)
where ( + ) is a short notation for ( = | ( = + )). This individual fairness notion assumes that the effect of sensitive attributes on the decision along all causal paths is unfair. But this may not be true in some cases, e.g. from Figure 1, the direct effect of race on the admission outcome is unfair; however, the indirect effect of race on the outcome through the qualification variable is fair.</p>
<p>Path specific counterfactual fairness [49], in contrast to the counterfactual fairness notion, attempts to remove the causal effects of sensitive attributes on the outcome along only unfair causal paths. For a set of paths , path-specific counterfactual fairness exists if Equation (2) is satisfied, whereis the set of remaining paths and ( + ) is a short notation for ( = | ( = + )).
P( + | ,  |) = P(  )(2)
PC-fairness [289] is an additional path-specific counterfactual fairness notion for subgroups not just individuals. Given a set of paths ( ) and a factual condition = ( ), a predictor attains PC-fairness if it satisfies the following criteria:
P( + | ,  || ) = P(  | ),(3)
where ( + ) is a short notation for ( = | ( = + )). Counterfactual equalized odds [168] fairness notion is satisfied by a predictor if the respective counterfactual false positive rates (cFPR) and counterfactual false negative rates (cFNR) of the protected group and non-protected group are equal which is not possible practically. Therefore, we can use approximate counterfactual equalized odds. This notion is satisfied by a predictor if the constraints (4) hold, where + and  are predefined thresholds. + (  ) is the difference between cFPR (cFNR) of the protected and the non-protected group as presented in Equation (5).
| + |  + , + [0, 1] |  |   ,  <a href="4">0, 1</a>+ = ( + )  (  )  = ( + )  (  )(5)
Causal Explanation formula [299] is a causal explanation method that helps in dividing the observed discrimination into three counterfactual effects: direct (DE), indirect (IE), and spurious effects (SE) of sensitive attributes on the outcome. The authors designed a causal explanation formula and decomposed the total variation (TV) into DE, SE, and IE, as presented in the following equation.
+ ,  ( = ) = | + ,  ( = ) + + ,  ( = | =  )  + ,  ( = | =  )|(6)
This formula demonstrates that the total discrimination experienced by the individuals with S =  equals the disparity experienced via SE, plus the advantage lost due to IE, and minus the advantage it would have gained without DE.
(a) (b) Fig. 4. (a)
The left causal graph exhibits unresolved discrimination (along red paths) while the right one is free from unresolved discrimination where is the sensitive variable, is the resolving variable, is the unresolving variable, and is the predicted decision. (b) The left causal graph exhibits proxy discrimination (along red paths) while the right one is free from 'proxy discrimination' where is the sensitive variable, is the proxy variable, and is the predicted decision.</p>
<p>Interventional Fairness (IF)</p>
<p>. IF measures fairness by quantifying the effect of sensitive attributes on the predicted outcome by intervening on the protected and non-protected attributes.</p>
<p>No unresolved discrimination [124] is a group fairness notion that focuses on the direct and indirect causal influence of sensitive attributes on the decision. It is satisfied when there is no direct path between the sensitive attributes and the outcome, except through a resolving/ admissible variable. In a causal graph, a resolving variable is a variable that is influenced by the sensitive attributes in an unbiased manner. The left causal graph in Figure 4 (a) exhibits discrimination along the causal paths:  and   while the right one is free from discrimination, where is the resolving variable, is unresolving variable, and is the predicted outcome.</p>
<p>Proxy discrimination [124] is another indirect causality-based group fairness notion. It is present in a causal graph when the path between the sensitive attributes and the outcome is intercepted by a proxy variable. A predictor avoids proxy discrimination if, for a proxy variable , Equation (7) holds for all potential values of P ( 1 , 2 ). A proxy variable has the same influence on the outcome as the influence of the sensitive attributes. The left causal graph in Figure 4 (b) exhibits discrimination along the causal paths: SPY; where is the proxy variable and is the predicted outcome.
P( = | ( = 1 )) = P( = | ( = 2 ))  1 , 2 ( ).(7)
Total Effect [196] is the causal version of the statistical parity group fairness notion. It measures the effect of changing sensitive attribute values on the outcome along all causal paths from sensitive attributes to the outcome as presented in Equation (8)
+ ,  ( = ) = P( + )  P(  )(8)
Individual direct discrimination [302] identifies direct discrimination at individual level. In any classification task, an individual is compared with similar individuals sought from the protected group, denoted as and similar individuals from the non-protected group, denoted asD. The similarity between the two individuals ( ,  ) is measured using causal inference as presented in Equations (9) and (10), where is the causal effect of each selected variable ( ) on outcome variable, is the distance function, and range is the difference between the maximum and minimum of the variable . The individual is deemed not to be discriminated against if the difference between the positive prediction rates for the two groups (D,D) is under a predefined threshold.
( ,  ) = | |  =1 | ( ,  )  ( ,  )| (9) ( = ) = P( = | ( ))  P( = | (  , \ )) ( ,  ) = |   | (10)
Equality of effort [104] assesses discrimination by measuring the amount of effort required by the marginalized individual or group to reach a certain level of the outcome as shown in Equation (11). The minimal effort required to achieve the -level of outcome is computed using Equation (12), where + and  represent the set of individuals with S = + and S =  respectively which are similar to the target individual, [ + ] is the expected value of outcome under treatment T = t for the set + . This notion of fairness is based on potential outcomes framework.
+ ( ) =  ( )(11)+ ( ) = E[ + ] (12)
Interventional and justifiable fairness [230] are stronger versions of the Total Effect fairness notion. The total effect intervenes on the sensitive attribute; however, interventional fairness intervenes on all attributes except the sensitive attribute. A classification algorithm is interventionally K-fair if for any assignment of K = k and output Y = y the following equation holds. K is a subset of attributes (V) except the sensitive attribute (S) and the outcome variable (  \ { , } )
P( + , ) = P(  , )(13)
Justifiable fairness is a special case of interventional fairness, where we only consider those attributes for intervening that are admissible/resolving (E) or a superset of admissible variables:
P( + , ) = P(  , ),  .(14)
Causal fairness [75] identifies a classifier as fair if for any given set of admissible variables E, the following equation holds:
P( + , ) = P(  , ),  .(15)</p>
<p>Pre-processing Methods</p>
<p>Pre-processing methods are considered to be the most generalizable methods. These methods intend to manipulate the dataset in order to make it bias-free before feeding it to any learning algorithm. Data Augmentation: Zhang et al. [303] calculate path-specific effects of sensitive attributes on the predicted outcome and compare them to a predefined threshold . If the calculated pathspecific effect exceeds , this indicates the presence of direct and indirect discrimination. Later they eliminate both direct and indirect discrimination by generating a bias-free dataset through causal network manipulation that guarantees path-specific effects under . Zhang et al. [305] further identify and handle the situations in which indirect discrimination cannot be measured because of the non-identifiability of certain path-specific effects. In such cases, the authors suggest setting an upper and lower bound on the effect of indirect discrimination. Another discrimination discovery and prevention causal framework is proposed by Zhang and Wu [301]. In this work, the authors detect direct and indirect system-level discrimination by measuring the path-specific causal effects of sensitive attributes on the outcome. To prevent discrimination, they modified the causal network to generate a new bias-free dataset. Xu et al. [290] has proposed a utility-preserving and fairness-aware causal generative adversarial network (CFGAN) to generate high-quality and bias-free data. Salimi et al. [230] detected discrimination using interventional and justifiable fairness notions. To eliminate discrimination, they used causal dependencies between sensitive attributes and outcome variables to add and remove samples from the training data. Data Integration: Data integration aims to combine data from various sources that capture a comprehensive context and enhance predictive ability. Galhotra et al. [75] modeled the problem of ensuring causal fairness in a learning task as a fair data integration problem by combining additional features with the original dataset. They proposed a conditional testing-based feature selection method that guarantees high predictive performance without adding bias to the dataset.</p>
<p>In-processing Methods</p>
<p>In-processing methods achieve fairness by altering the learning algorithm.</p>
<p>Correcting path-specific effects: The sensitive attributes can affect the outcome through both fair and unfair causal pathways as explained in Section 2.1. Kilbertus et al. [124] proposed to deal with such a situation by constraining the parameters of the learning algorithm so that the causal effects along both fair and unfair causal paths from sensitive attribute to outcome variable is removed. They used "proxy discrimination" and "unresolved discrimination" fairness notions to detect discrimination. [173] proposed to deal with such situations by constraining the path-specific effect during model training within a certain range. The methods proposed by [124,173] cater for the causal effects of sensitive attributes on the outcome without distinguishing between fair and unfair causal effects, thus negatively impacting the predictive performance of the learning algorithm. A solution to this problem is proposed by Chiappa [49]. The author presented a causal framework that ensures path-specific counterfactual fairness by correcting the observations of such variables that are descendants of sensitive attributes along only unfair causal paths so that only individual unfair information is eliminated while the individual fair information is retained, hence improving predictive performance of the framework.</p>
<p>Fairness under distribution shift: The problem of learning fair prediction models with covariates distributed differently in the test set than in the training set is studied by Singh et al. [252]. They proposed a method based on feature selection to achieve fairness given the ground truth graph that explains the data. Fairness concerns also surface when AI-based learners deal with dynamically fluctuating environments and produce long-term effects for both individual and protected groups. Creager et al. [55] have proposed that in such dynamical fairness setting when the dynamic parameters are unknown, causal inference can be utilized to estimate the dynamic parameters and improve off policy estimation from historical data. Adversarial learning: Li et al. [140] proposed an adversarial learning-based approach to achieve the goal of personalized counterfactual fairness for users in recommendation systems. They attempt to remove sensitive features information from the user embeddings by using a filter module and a discriminator module to make the learner's decisions independent of the sensitive features.</p>
<p>Post-processing Methods</p>
<p>These methods tailor the outputs of the learner to achieve fair outcomes. Constrained optimization: Wu et al. [288] proposed a method to bound the unidentifiability of counterfactual quantities and used c-component factorization to identify its source. They proposed a graphical criterion to determine the lower and upper bound on counterfactual fairness in unidentifiable scenarios. Finally, they proposed a post-processing method to reconstruct the decision model to achieve counterfactual fairness. Similarly, Kusner et al. [129] achieved counterfactual fairness by constraining the beneficial effects obtained by an individual under a limit depending on the sensitive attribute of the individual. Doubly robust estimators: Mishler et al. [168] proposed a post-processed predictor, estimated using doubly robust estimators, to achieve the counterfactual equalized odds fairness notion. Through experiments, they also proved that their method has favorable convergence properties.</p>
<p>Conclusion</p>
<p>A unique trait of fairness research is the usage of multiple metrics to define/measure it. Consequently, choosing the most appropriate notion of fairness applicable to a particular situation is an important task. On the other hand, even if a fairness notion is found suitable for a scenario, it may not be applicable due to the problem of identifiability, as Pearl's SCM framework requires causal quantities, counterfactuals, and interventions, to be identifiable [156]. Most of the discrimination mitigation approaches discussed above to achieve the goal of causal fairness are based on the synthesis of a bias-free dataset. These methodologies are only applicable in a static environment where all data are available in advance. An interesting future direction could be the extension of such methods for online learning, where not all data are available beforehand [75]. Most of the causality-based fairness solutions discussed above rely on the assumption that the underlying data is independent and identically distributed (IID). However, real-world use cases include non-IID data, therefore, another future direction could be to design causality-based decision support systems which relax the assumption of non-IID data and provide non-discriminatory predictions. Finally, all research done in the field of causal fairness is connected to classification tasks [301], it will be interesting to expand it to achieve causal fairness in community detection, word embedding, named entity recognition, representation learning, semantic role labeling, language models, and machine translation [140].</p>
<p>CAUSALITY AND ROBUSTNESS</p>
<p>Training of modern machine learning (ML) systems builds upon the assumption that all observations -whether training or test data -are independent and identically distributed (IID) under a single distribution. As it is improbable that training data can perfectly represent the sample distribution of real-world data, striving for more robust ML systems is of utmost importance. Robust behavior is not only required for safety-critical applications of ML but also essential for the trustworthiness of AI. We believe that AI systems that are error-prone to small changes in the working environment will not be able to gain the complete trust of humans. Even high-performing ML models often cannot differentiate between style variables, which are content-independent and irrelevant information for the task, and the information-rich, relevant, and invariant content variables [117,237]. Causal information about the problem can provide models with a better understanding of the task and eliminate such spurious correlations. Inspired by this perspective, many researchers in recent years investigated the connection between robust machine learning and causality. We summarize related publications and organize them into three broad categories: (i) pre-processing, (ii) in-processing, and (iii) post-processing methods. Figure 5 provides an overview of the techniques discussed in this section.</p>
<p>Preliminaries</p>
<p>We briefly introduce the most important concepts for this section, including a high-level definition of robustness and the primary sources of non-robust behavior.</p>
<p>Robustness: The notion of robustness boils down to the question of how sensitive the ML model's output is to changes in the input. Minor changes in the input should not significantly alter the performance of robust AI systems. Instead, performance should degrade gracefully: slowly and gradually with the deviation in input distribution. We distinguish between naturally occurring and artificially crafted distributional shifts. The first type of shift is represented by the notion of out-of-distributional data, whereas the second type of shift is represented by adversarial examples.</p>
<p>Out-Of-Distribution Data: Out-Of-Distribution (OOD) data represents naturally occurring data with previously unseen characteristics. For instance, computer vision models trained to solve MNIST classification may encounter naturally perturbed images (e.g. numbers written in a different orientation or a different color). Such perturbations represent data points from a different, shifted distribution and, therefore, may lead to poor performance of our computer vision model. These data points are, thus, considered to be out-of-distribution. Methods such as Data Augmentation (e.g. methods discussed in [250]) can increase robustness towards OOD-data by providing models with Causality and Robustness</p>
<p>Pre-Processing Data Augmentation [106,121,144,158] Problem Abstraction [107,281] In-Processing Objective (Loss) Design [13,63,150,169] [ 258,280,307] Architecture Design [85,146,298] Post-Processing Altering Predictions [45,279] Model Selection [131]  additional, naturally perturbed data via hand-crafted rules [235,258]. These methods, however, cannot cover all possible environmental settings [258].</p>
<p>Adversarial Examples: Another hurdle in robust machine learning is the continuous rise of Adversarial Examples (AE). AEs represent artificially perturbed input values intending to fool machine learning models. Such examples are especially concerning for the field of Trustworthy AI, as these perturbed input values look benign to humans. Despite the existence of various developed defenses against continuously evolving AEs, research into improved attacks and defenses continues to this day. We refer to [6] for a recent overview of such methods in the computer vision domain.</p>
<p>Pre-Processing</p>
<p>Pre-processing methods built upon causality are mostly Data Augmentation methods, which create causally motivated augmentations. We will also discuss alternative, exciting approaches to preprocessing that we cover under the umbrella term Problem Abstraction.</p>
<p>Data Augmentation.</p>
<p>Data Augmentation is the most common pre-processing method to induce causality. There are several methods [106,158] which use the notion of causal graphs to motivate data augmentation. For instance, Ilse et al. [106] try to find and apply transformations that emulate the intervention on high-level domain-specific features (e.g., the orientation of handwritten digits) within data points. Such information is only spuriously correlated to the output label and, as such, should not affect the decision-making of ML models. To find such a transformation without an SCM, the authors propose to train a classifier that can predict the domain of data points (e.g., the given number is rotated by 60). They then choose the augmentation of a pre-defined set of transformations that leads to the lowest accuracy of the domain classifier. Applying the selected augmentation "destroys" the most domain-specific information. Such augmented training data, in turn, reduces the likelihood of ML models overly relying on domain-specific features that are only spuriously correlated to the label. Mao et al. [158] show that it is also possible to generate intervention-simulating data via GANs by identifying interpretable controls through GANSpace. Manipulating the data generated through these controls is equivalent to intervening in the underlying SCM. Alternatively, one can generate counterfactual examples by augmenting the data just enough to flip the label. Following this principle, Kaushik et al. [121] developed a humanin-the-loop process that improves robustness on NLP tasks compared to alternative, non-causal methods. As shown by Little and Badawy [144], it is also possible to induce causality through bootstrapping. The authors developed causal bootstrapping, which utilizes information provided by a causal graph to sample data whose deducible observations better reflect the domain's causal relationships. Models trained on causally sampled data demonstrate increased robustness against spurious correlations.</p>
<p>Problem Abstraction.</p>
<p>Instead of data sampling, some methods try to abstract and simplify the problem. One example is the Datamodeling [107] framework, which allows researchers to approximate the behavior of large and complex models on the given data through a set of simple linear functions. Wang et al. [281] simplify the problem for reinforcement learning (RL) agents using information encoded in a causal graph. The authors propose to create state abstractions for RL agents that only contain the relevant one-to-one causal dependencies between variables and actions. RL agents that utilize this state abstraction exhibit higher robustness towards unseen states, cover a wider range of tasks than agents trained with other methods and demonstrate higher sample efficiency.</p>
<p>In-Processing</p>
<p>It is possible to induce notions of causality as part of the algorithm either through the definition of a causality-aware optimization objective or via architectural design choices.</p>
<p>Objective (Loss) Design.</p>
<p>Most causal in-processing techniques incorporate an optimization objective (e.g. a loss function or a regularization term) that guides ML models to a more causalityaware behavior. We will introduce three possible causal foundations for such objectives. Content and Style Variables: Zhang et al. [307] use a causal graph to model the generation process of AEs to examine the source of adversarial vulnerability. They conclude that AEs exploit the spurious correlations between style variables and labels to mislead classifiers. However, the adversarial distribution is drastically different from the natural one. Consequently, the authors developed a loss function that aligns the two distributions. Compared to other robust classifiers, aligned classifiers demonstrate higher accuracy on adversarial data without significantly worse performance on natural data. Wang et al. [280] developed a regularization term for logistic regression models allowing researchers to penalize causal and spurious features separately. This form of regularization explicitly requires researchers to categorize features as either causal, spurious, or remaining (not identified as either of the two). Given such information, models optimized with the term showcase improved robustness on lower and higher-dimensional data.</p>
<p>Multiple Environments: One vital insight for causal ML is the connection between the causal relevance of a feature and its invariance across environments [200]. The basic idea is that style variables (e.g. the image background) greatly vary across environments (i.e. unique experimental settings), whereas content variables (e.g. an animal's anatomy) remain invariant. Consequently, guiding models to perform well across environments should lead to models capable of differentiating between content and style variables, which, in turn, makes them more robust. The most prominent example is the loss function invariant risk minimization (IRM) [13] that empirically leads to higher robustness. Mitrovic et al. [169] were able to transfer this premise to the Self-Supervised Learning (SSL) setting to improve OOD performance. The resulting SSL objective requires generated data representations to be stable across different interventions simulated by data augmentation. On the assumption that "the prior over the data representation belongs to a general exponential family when conditioning on the target and the environment" [150], the iCaRL framework is able to outperform IRM, but causal discovery is needed to identify the causally-relevant latent variables. Counterfactuals: Researchers also successfully developed counterfactual-based loss functions. Teney et al. [258] designed an auxiliary loss for supervised learning that gives additional attention to pairs of data that are counterfactuals of one another. Raising awareness for counterfactuals can also increase the robustness of already causal methods, such as recommendation via Causal Algorithmic Recourse. Such algorithmic recourse systems try to find minimal-costly actions that result in a counterfactual representing a desirable outcome. Dominguez-Olmedo et al. [63] were able to further enhance this framework by also accounting for uncertainties stemming from adversarially perturbed features. However, instead of considering all possible perturbations within an -range, the authors solely considered perturbations within the (SCM-guided) counterfactual neighborhood (i.e., only instances in -range that represent counterfactuals to the given data).</p>
<p>Architecture</p>
<p>Design. Goyal et al. [85] try to take advantage of the independent causal mechanisms (ICM) principle. It states that the causal dynamics of a domain are built upon "autonomous modules that do not inform or influence each other." [237]. They achieve this by implementing a sequential architecture of independently acting recurrent subsystems that only communicate sparsely with one another. Each subsystem is designed to emulate a mechanism of the causal generative process in the hope of better capturing the domain's causal structure. The resulting architecture is more robust to distributional shifts than, e.g. LSTM or Transformers. An alternative, more feature-focused architecture is deep Causal Manipulation Augmented Model (deep CAMA) [298] -a deep generative model whose design is consistent with the causality encoded in a given causal graph. It not only considers the effect of the output label on the input data but also considers manipulable variables (e.g., rotation and color of MNIST digits) and non-manipulable variables (e.g., handwriting style of MNIST digits) influence on the input. The authors achieve this by adding autoencoders for both the variables. This design choice allows the resulting generative model to better distinguish between relevant/causal features and non-relevant ones. Deep CAMA showcased improved robustness against adversarial attacks on MNIST data. Liu et al. [146] follow a similar approach to designing a robust motion forecasting model. The authors argue that latent variables of the motion forecasting tasks are either (i) invariant variables such as laws of physics, which are crucial for correct motion forecasting, (ii) hidden confounders like the motion style that can sparsely differ between environments but still need to be considered for optimal performance, or (iii) non-causal spurious features that can drastically vary between environments. Based on this categorization, the authors developed an architecture that provides higher robustness toward style shifts.</p>
<p>Post-Processing</p>
<p>The causality framework also enables researchers to impact the robustness of their ML-pipeline after the training phase. Post-processing methods discussed in this section either directly alter the predictions of a given trained model or enable a causality-informed selection between a set of models.</p>
<p>Altering Predictions.</p>
<p>Counterfactual regularization [117], which tries to remove the confounding effects of unobserved variables, is a common approach to instilling causality through post-processing. One can achieve this by first estimating the effects of unobserved variables, referred to as the counterfactual prediction [45]. By then taking the difference between the counterfactual prediction and the factual prediction (built upon both causally relevant and spurious information), a deconfounded prediction can be extracted. Using such a counterfactual regularization, Chen et al. [45] successfully improved the quality of trajectory predictions in multi-domain settings. Wang et al. [279] introduce an alternative approach called Invariant-Feature Subspace Recovery (ISR). The authors first extract the feature representation of the given data via the model's hidden layers. They then recover the subspace spanned by invariant features (i.e., content variables), fit a linear predictor in the resulting manifold, and substitute the model's classification layer with the subspace predictor. Post-processing with ISR improved the performance of trained models on multiple OOD-datasets. 5.4.2 Model Selection. Kyono and van der Schaar [131] introduce a pipeline that utilizes (incomplete) causal information in the form of a DAG in a post-hoc manner. Given a set of trained models and the data these models used, it is possible to score models based on how well the model's predictions abide by the "rules" induced by the given causal structure (DAG). The authors propose to choose the model whose predictions follow the given causal relationships the most. Classifiers chosen via this causal model selection technique empirically showcase higher robustness in OOD-learning settings.</p>
<p>Conclusion</p>
<p>Despite the relative novelty of causal learning, researchers have already applied the notions of causality to domains such as computer vision, NLP, and recommendation. Naturally occurring distributional shifts are the focus of these advancements, though causal learning can also increase robustness towards AEs. The success of causality in fields outside of traditional supervised learning, e.g. reinforcement learning [281] and self-supervised learning [169] further show the legitimacy of improving robustness via causal learning. We see multiple exciting avenues for future studies into robust causal ML. For instance, researchers need to be aware that OOD-datasets may vastly differ in the type of distributional shifts they emulate [292]. Hence, causal solutions for robust ML need to be analyzed and compared to other (both causal and non-causal) state-of-the-art approaches on benchmarks that contain a diverse collection of datasets. It will also be interesting to (further) explore related fields such as Neurosymbolic AI (where researchers enhance ML systems through the use of knowledge-based systems) or Object-Centric Learning (a special case of Causal Representation Learning [235] where visual scenes are modeled as compositions of objects that interact with one another). We also believe that research into causal solutions for certified robustness and concept drifts occurring in online learning will lead to interesting new solutions.</p>
<p>CAUSALITY AND PRIVACY</p>
<p>In Section 5, we discussed the robustness aspect of AI from the viewpoint of the model and noted out-of-domain generalization as a fundamental challenge. The problem becomes even more challenging in a distributed learning scenario, where the AI model gathers data from different users for training. In such a setting, it must be ensured that the users' private data is not exposed. Recent studies [261,295] have demonstrated that weak generalizability of a model could be exploited to design attacks that can expose users' private information (e.g., whether the user's data were used in training the model). With their inherent ability to generalize to out-of-distribution (OOD) data, causal models can help in preventing such attacks.</p>
<p>Preliminaries</p>
<p>Learning paradigm: Machine learning algorithms are often trained on a large amount of training data, which can contain sensitive information. We consider a particular setting where data are collected from one or multiple users to train a machine-learning model. For example, the data could be collected from users' mobile devices to train a better speech recognition model. The most relevant variant in the context of privacy is federated learning (FL) [161]. FL proposes to learn a global model without collecting users' data into a central server, rather keeping the data in users' devices. Data are processed locally to update a local model, while intermediate model updates are sent to the central server, which are then aggregated to update the global model. Once updated, the global model is sent to the local devices. In this part, we will specifically look into attacks on federated learning as it involves dealing with users' data where privacy is of utmost importance. Privacy attacks: There has been a growing body of work aimed at designing different types of attacks against FL systems (refer to Jere et al. [113] for a comprehensive overview). However, one of the most common types of attack posed to FL systems and where causality-driven models have been deployed more often is membership inference attack [249]. In this type of attack, the attacker tries to determine whether a particular data point was used in training the model,</p>
<p>Causality and privacy</p>
<p>Pre-Processing Data augmentation [60] In-Processing Invariant risk minimization [72] Game theoretic approach [92] Post-Processing Test data specific normalization [114]  with only model predictions available to the attacker [176,295]. The core idea is to exploit the stochastic gradient descent (SGD) algorithm to extract information about a client's data. The attacks can be initiated both from the server's side and the client's side. For a comprehensive review of membership inference attacks on FL, refer to Nasr et al. [176]. Membership inference attack has been linked to model generalizability [176], and causal models are adept at improving the generalizability of models. Hence, the bulk of work deploying causal models against privacy attacks has concentrated on combating membership inference attacks, which we discuss in detail in this section. Differential privacy (DP) [64] ensures that the presence or absence of a data point does not significantly influence the output. Hence, a defense mechanism against membership attacks should provide better differential privacy guarantees. Evaluation: The success of a membership inference attack is measured in terms of accuracy on the binary classification task of determining membership [249]. Similarly, Yeom et al. [295] introduce advantage, which is measured as the difference in true and false positive rates in membership prediction. Hence, the goal of any defense mechanism against membership attacks is to reduce the attack's accuracy or advantage. Causal models for improving privacy: Yeom et al. [295] demonstrate that model overfitting significantly contributes to the leakage of membership information. For example, consider a model , a data point , and a loss function  where is a constant. An attack strategy is to first query the model (i.e., computing ( )) and then output that as a member of the training set with probability 1  ( , )/ . For such an attack, the performance in determining membership is shown to be proportional to the generalization error (i.e., the extent of overfitting). Hence, the defense mechanisms propose to deploy model generalization techniques such as learning rate decay, dropout [229] or adversarial regularization [177]. However, these mechanisms assume that the train and the test datasets are sampled from the same distribution, which is not always the case, particularly for FL. In fact, Tople et al. [261] argue that vulnerability to such attacks is exacerbated when a model is deployed on unseen data. They further utilize the generalization property of causal models and establish a theoretical link between causality and privacy. It is further argued that the models learned through causal features generalize better to distribution shifts and provide better privacy guarantees than equivalent association models. This has resulted in a growing body of work exploring causal models against privacy attacks [42,60,72,92,114,261]. We segregate these methods into pre-, in-and post-processing methods (refer to figure 6).</p>
<p>Pre-processing</p>
<p>The key idea here is to manipulate the training data at each client, which would result in better generalization. de Luca et al. [60] introduce a causality-based data augmentation to mitigate the problem of domain generalization in FL. The authors argue that data augmentation can reduce the heterogeneity across user data distribution, thereby making them more similar for the server model. The proposed data augmentation method is based on the notion of structural causal model (SCM) as described in section 2.3. The authors posit that a data point is generated by a common cause and some random variation . Following SCM, this could be represented as ( , ) where is the causal mechanism that remains invariant. An augmented data pointcould then be generated by defining a transformation over and then following the SCM ( , ( )).</p>
<p>In-processing</p>
<p>This class of methods aims at learning domain invariant features while training. Francis et al. [72] propose to collaboratively learn causal features common to all collaborating users/clients through invariant risk minimization [13] (introduced in Section 5.3). The proposed method is able to defend better against membership inference and property inference attacks in comparison to the vanilla FL algorithms. Gupta et al. [92] propose FL games, a game theoretic method, to solve the problem of generalization in FL. The key idea is to learn causal representations that are invariant across users. Each client serves as a player that competes to optimize its local objective, while the server guides the optimization to a global objective. An equilibrium is reached when all the local models across users become equivalent, thereby achieving generalization across users' data distributions as well as finding invariant representations. Although the authors do not explicitly demonstrate the effectiveness of their model against specific attacks, it can be argued that better generalization should lead to improved robustness against inference attacks. Given these methods aim to discover a set of invariant features that directly influence the outcome (hence speculated as causal), they can be loosely placed under causal discovery. However, it is difficult to conclude whether the discovered features are indeed causal.</p>
<p>Post-processing</p>
<p>Jiang et al. [114] argue that the FL training could be represented as a structural causal model with four variables, input data ( ), raw extracted features ( ), normalized features ( ) (obtained by applying batch normalization [108]) and the output ( ), following a causal structure    . Although heterogeneity of data can lead to each client fitting its individual feature distribution as opposed to the global objective, batch normalization (BN) layers normalize the training data to a uniform distribution, thereby allowing the global model to converge. However, when dealing with unseen test data ( ), BN layers with the estimated training statistics run the risk of normalizing it improperly, thereby introducing an edge  and making a confounder. Performing a causal intervention by introducing a surrogate variable , which is test-specific statistics of raw features (i.e., BN normalization parameters are now computed from the test set instead of the training set), leads to blocking the path between and and getting rid of the confounder. This achieves better generalization and hence better robustness against attacks.</p>
<p>Conclusion</p>
<p>In this section, we presented an overview of various attempts at improving defenses against privacy attacks on FL systems through causality-driven methods. Most of them have exploited the OOD generalization capabilities of causal methods as this successfully defends against inference attacks, which calls for similar investigations with respect to other types of attacks. Except for Tschantz et al. [263], who propose to apply results from causality theory while studying differential privacy (DP) [64], this has largely remained unexplored and might be another interesting line of future research.</p>
<p>Safety through impact assessment</p>
<p>Ex-ante (Pre-deployment) Environmental [159,277] Social and Fiscal [20,96,287] Ex-post (Post-deployment) Temporal and long-term effects [78,206] Failure and misuse [82,105] Strategic [193,264] </p>
<p>CAUSALITY AND AUDITING (SAFETY AND ACCOUNTABILITY)</p>
<p>Automated systems often enabled by AI, when deployed at scale, interact with people, each other, and also with other ecosystem or environmental parameters, thereby having a potential for widespread unforeseen (desirable or undesirable) effects. Some undesirable effects can cause significant societal harm, such as the creation of online filter bubbles and polarization due to personalized consumption-centric information retrieval systems powered by various ML models [192]. Following such effects of AI, there has been a lot of interest in finding potential negative effects before deployment or on the run and make necessary changes to avoid or neutralize such effects. Selbst [238] describes Algorithmic Impact Assessment (AIA) as a regulatory strategy for addressing and correcting algorithmic harms. EU regulatory guidelines also emphasize the need for environmental, social, and economic impact assessments before deployment. The phrase 'impact assessment' itself suggests a causal relationship between system design and impact. Thus, causal inference has long been used for impact assessment of algorithmic and non-algorithmic systems. Depending on when an assessment occurs, it can be categorized into (i) ex-ante (before deployment) and (ii) ex-post (after deployment) impact assessment.</p>
<p>Ex-ante impact assessment</p>
<p>When impacts are assessed before deployment of a particular system or policy (often using prior knowledge or use cases, empirical data from system testing, and system design details), we talk of ex-ante impact assessment. Ex-ante impact assessment often predicts the risks and impacts of the proposed system. Such assessments are used as tools to assess potential environmental, financial, social, and human rights ramifications of systems (both algorithmic and non-algorithmic), projects, and policies, and grant some measure of control and voice to designers or developers, affected population, and authorities to make, induce or enforce changes accordingly. Environmental impact assessment (EIA): Both deductive and inductive causal inference have long been used in EIA [159,277]. While with a deductive approach, a hypothesis about a causal relationship is formed and tested, using an inductive approach, data are collected from observations, and a causal relationship is inferred from the instances. Causal networks have long been used for EIA [159,277] since they bring both network (multiple interaction pathways between environment and various activities) and cause-effect (various activities affecting the elements of the environment) logic to the analysis. Causal networks allow an analysis of impacts through sequences of interactions [159]--also referred to as sequence diagrams [36]. Causal networks (structural causal models) have helped find indirect impacts on multiple levels [36]. While initial EIA studies have helped find forms and parameters of relationships between various kinds of activities/developments and environmental elements, they are also utilized to (ex-ante) estimate the environmental impacts of planned new or upcoming projects, including complex AI systems in the near future [254]. Social and Fiscal impact assessment (SIA and FIA): Becker [20] defines SIA as the process of identifying the future consequences of a current or proposed action which are related to individuals, organizations and social macro-systems. Similar to the works in EIA, SIA also uses both deductive and inductive methods to discover the causal relationships between actions (e.g. inventory optimization, logistic changes) and consequences (e.g., increase in sales, popularity, overall perception of the product) [58,287]. Note that most of the works here use the potential outcomes framework. Moreover, economists have long been looking for ways to construct counterfactuals and answer causal and policy evaluation questions [94]. The majority of works on FIA or economic impact assessment try to understand the effects of introducing new economic policies or changing existing ones [96]. Such analyses are done in a multi-agent setting with modeling of the preferences and choices of agents along with their ability to infer evaluations and outcomes. Causal and counterfactual economic assessment has been a big part of FIA of many automated systems [96].</p>
<p>Ex-post impact assessment</p>
<p>Since ex-ante impact assessments are limited to the available prior knowledge and use cases, it is often not possible to identify all possible risks and impacts of a system or policy, which motivates expost impact assessment. When impacts are assessed after deployment (often in real-time using the running record of the system, real-time audits, and evaluations), we call this ex-post assessment. Expost impact assessment often detects the risks and impacts on the go once the system is introduced. While ex-ante impact assessments often have clear guidelines or metrics for specific types of impacts (e.g., average CO 2 emission for environmental impact, total financial cost, opportunity cost and return-on-investment for economic impact, overall positive or negative opinions of the population for social impact), ex-post impact assessments are generally broader since they need to define what constitutes an impact in real-time. Causal inference is used to tackle various categories of risks and impacts of a system or policy in real time.</p>
<p>Temporal and long-term effects: Many kinds of temporal and long-term effects are seen in real-world systems which operate inside an eco-system of stakeholders. For example, the creation of online filter bubbles and polarization due to personalized consumption-centric information retrieval systems powered by various ML models [78,206], the content homogenization effects observed in online marketplaces as a response to popularity bias in recommender systems [43]. Causality, along with behavioral modeling, has helped assess such systems in real-time and find out the elements responsible [239].</p>
<p>Effects from system failure and system misuse: Systems are often designed with some desired criteria (e.g., accuracy, fairness, robustness, etc.). If a real-world model deviates from the desired criteria and produces unwanted outputs, accountability helps identify the reason behind that failure and take required actions. For example Gillespie et al. [82] studies how the inclusion of a robot as a team member in surgeries increases the complexity and errors that it was supposed to reduce. Making systems and stakeholders accountable for such failures of the system is an integral part of safety. Ibrahim et al. [105] propose a bottom-up causal approach using goal-specific accountability mechanisms. Their mechanisms can help identify the root cause of specific type(s) of events or failures, which can then be used to eliminate the underlying (technical) problem and also to assign blame. Causality has also been very helpful in analyzing (networked or other) system attacks and finding out the root cause and the source host or process from where the attack has originated [51,148]. Strategic risks and effects: While interpretability and explainability are essential for trustworthiness, one must also consider various risks of using interpretable and explainable decision-making systems in the real world. For example Shokri et al. [248] analyze connections between model explanations and the leakage of sensitive information in the model's training set even if a model is used as a black-box; They show that back-propagation-based explanations can leak a significant amount of information about individual training data points, exhibiting a potential conflict between privacy and interpretability. Similarly Tsirtsis and Gomez Rodriguez [264] show that counterfactual explanations can reveal various details of decision-making systems, thereby making them vulnerable towards strategic behaviors and, therefore, non-robust. Since in real-world settings, individuals (either using the decision-making system or being affected by its decisions) often try to optimize their outcome, their rational strategic behavior can cause issues of privacy and robustness. Because of the above strategic risks, recent works [193,264] suggest modeling real-world strategic settings as games (from applied game theory) and then designing decision-making systems that can mitigate the effects of strategic behavior. Essentially, these works [193,264] try to design decision-making systems that incentivize individuals only to improve a desired quality (to improve individual outcomes) and not do any strategic manipulations.</p>
<p>Conclusion</p>
<p>We discussed the importance of impact assessment and auditing for the safety and accountability of AI systems and the use of causality in different types of ex-ante and ex-post assessment. However, as Moss et al. [172] outline, there is no pre-existing or universal definition of impact that can be used in safety assessment of algorithmic systems. Instead, the idea of an impact can vary depending on the context, scale, and application domain. Adding to that, Irving and Askell [109] justifiably argues that figuring out AI safety would need social scientists since it depends on human values and expectations. Given clear definitions of undesirable effects, causality can be very helpful, particularly in studying the effects of different design elements of algorithmic systems in simulated environments, and also in figuring out which design element(s) are responsible for certain undesirable effects observed in either real or simulated environments.</p>
<p>CAUSALITY IN HEALTHCARE</p>
<p>In the previous sections, we mostly demonstrated the effectiveness of causality-based methods for improving different trustworthy aspects of AI. However, causal methods have proved to be reliable tools in many application domains as well. In this section, we look into healthcare, where causal methods have been demonstrated to be particularly advantageous. Additionally, we also point out how causal methods could be effective in enhancing the trustworthiness aspects of such applications. Several existing papers highlight the application of causality approaches in the healthcare domain. Through SCMs, discussed in Section 2.3, we can better analyze diseases (by revealing the causal relations of the diseases' features), improve the accuracy of automatic diagnosis, or even discover new drugs. Moreover, causal reasoning can also be used to investigate drugs' outcome through the PO framework, discussed in Section 2.4. Figure 8 depicts these different use cases. In this section, we will survey existing research based on the two broad categories of causal inference, SCMs and PO, while focusing on interpretability, robustness, fairness, and privacy.</p>
<p>Causality in Healthcare through SCM framework</p>
<p>The literature provides many reviews which focus on the usage of SCMs in healthcare and personalized medicine [231,275,306]. Zhang et al. [306] provided an introduction to causality using different medical examples like lung cancer causal graphs and shed light on the different challenges and issues encountered when dealing with causal inference, such as missing data, biased data, and transferability of models. Vlontzos et al. [275] present the benefits of introducing causality and its use in the field of medical imaging. Their survey reviews several applications that incorporate Causality in Healthcare Structural Causal Model Analysing Diseases [157,231,242] Drugs Repurposing [23] Improving Diagnosis [219,271] Potential Outcome Framework Analysing Drugs Outcome [73,86,190,243,244,314]  causal discovery and causal inference in medical imaging. Sanchez et al. [231] specifically focused on Alzheimer's disease (AD), a progressive neurodegenerative disorder, to highlight the utility of causal machine learning in precision medicine. Shen et al. [242] used two causal discovery methods to discover the causal relationship utilizing observational data of AD. The authors used the fast causal inference (FCI) [253], a constraint-based algorithm, and the fast greedy equivalence search (FGES) [208], a score based algorithm. These approaches are then compared and benchmarked with a well-established causal graph. Mani and Cooper [157] tried to identify causal factors of clinical conditions for intensive care unit (ICU) patients using medical discharge reports. In the above references, we observe that causality methods are mostly used to interpret and explain the outcomes of medical models. Such approaches and ideas make the AI system interpretable by design, which is desirable in healthcare use cases. By providing an explanation and finding causal relations between different factors of a particular disease, different insights are gained, which in turn contribute to a better, trustworthy AI system. Drug discovery is another research area that benefits from causality. Belyaeva et al. [23] show that causal models can be used to repurpose drugs for new diseases like SARS-CoV-2. The research team integrated transcriptomic, proteomic, and structural data for different diseases. They first used autoencoders to match a drug's signature with a reverse disease signature in the latent space. Using the augmented Steiner tree, the disease interactome is then identified. Lately, they have verified the causal interaction of the drugs with genes by using a causal structure discovery algorithm and building a causal network.</p>
<p>Apart from interpretability, few studies addressed fairness and robustness issues. In precision medicine, we aim for a fair system that provides personalized and equitable treatment to each individual without any bias [47]. Chen et al. [47] gave the example of biased systems in healthcare. An algorithm trained only on USA cancer pathology data may lead to wrong classification, when deployed on data from Turkish cancer patients, due to protocol variations or population shifts (imbalanced data). The authors argued that causality is one of the technologies (others being fairnessaware federated learning, features disentanglement, etc.) that contribute towards a fair algorithm in healthcare by performing causal analysis to identify the bias factors. We should therefore include causality and analyze the causal structures, which could be discovered or provided by the clinicians to make biased AI algorithms fair in real-world scenarios and healthcare applications.</p>
<p>Robustness is another aspect of trustworthy AI that is addressed in various causality papers related to healthcare applications. It is important to design a system that is robust to change in the distribution and provides reliable outputs and accurate results. Van Amsterdam et al. [271] improved their lung cancer image prediction algorithm by eliminating bias signals. By predicting the collider variable (tumor size) and the prognostic factor (tumor heterogeneity), it was then possible to unbias the estimation and make the system more robust. Richens et al. [219] improved the accuracy of medical diagnosis through the use of causal machine learning. Their counterfactual algorithm helps them to improve the decision-making process and classify different vignettes correctly based on Bayesian networks that model known relationships between multiple diseases and integrate the causal relationship between different variables. Such algorithms incorporate the collider variables and also different causal relations in the network architecture design, which makes them in-processing methods that enhance robustness.</p>
<p>Causality in Healthcare through the PO framework</p>
<p>The PO framework is commonly used in the medical field. It provides methods to conduct causal analysis from a statistical perspective, as already introduced in Section 2.4. In real-world scenarios, observational data are biased (e.g. biased labeling, under-representation, etc.). PO framework methods provide a way to remove the selection bias in the historical data, thereby leading to a fair system (e.g. the propensity score matching method: Section 2.4).</p>
<p>Shi and Norgeot [243] review different research works which focused on learning causal effects from observational data in the medical domain. These survey articles summarize different methods used to estimate treatment effects. In the medical field, it is common to use the PO framework to test whether a particular drug is beneficial or harmful. For instance, Graham et al. [86] used propensity score matching [222] to examine whether Dabigatran or Warfarin increase the risk of death in elderly patients from nonvalvular atrial fibrillation. Similarly, Ozer et al. [190] investigated the benefits of chemotherapy in comparison to only undergoing surgery for patients with resectable gallbladder cancer. By performing propensity score matching [222] analysis, it was possible to find out that chemotherapy increases the survival rate. Friedrich and Friede [73] tried several propensity score-based methods utilizing the PO framework, in addition to other approaches such as g-computation and doubly robust estimators [153]. They designed a simulation to compare these different methods. It mimics data from a small non-randomized study on the efficacy of hydroxychloroquine for COVID-19 patients Ziff et al. [314] used PO framework-based analysis (propensity score matching [222]) to evaluate the safety and efficacy of the drug digoxin for patients with heart failure.</p>
<p>As discussed in Section 6, privacy is one of the main foundations required for a trustworthy AI system. An AI system should not allow the identification of specific patients based on the available training datasets. This aspect was one of the major concerns covered in the paper [244]. This paper states that the publicly available dataset helps assess a single treatment's efficacy. However, to investigate multiple treatments and correctly estimate the causal effect through observational data, the authors generated a new large synthetic dataset that imitates real-world data distributions and preserves individual patients' privacy. They reported the -identifiability metric that estimates the probability that an individual is identifiable and ensures that this value remains low after the data generation process.</p>
<p>Conclusion</p>
<p>Work in the healthcare domain encompassing causality mainly considers the aspects of interpretability and robustness and touch upon fairness and privacy to some extent. The other tenets of trustworthy AI, like safety and accountability, must be explored further. Integrating causality into precision medicine is still facing several challenges that need to be addressed. When discovering causal relations (i.e. causal discovery) among different features, ground truth causal graphs are not always available to validate the results [101]. This implies over-trusting the data, and the discovered relations can be overcome by working closely with experts and clinicians to integrate their knowledge. Available data in the healthcare domain are commonly unstructured, highly complex, and multimodal [231]. Thus, there is an urge to develop better decision-making algorithms that not only find correlations in these data but also understand causal relations and perform causal reasoning.</p>
<p>CONCLUSION</p>
<p>In this article, we surveyed causal modeling and reasoning tools for enhancing the trustworthy aspects of AI models, which include interpretability, fairness, robustness, privacy, safety, and accountability. While in recent years, the community has witnessed an unprecedented surge of research in this context, important facets still remain unexplored. We expect significant advancements in the coming years and hope this survey will act as an important resource to the community and at the same time guide future research connecting trustworthy AI and causality. </p>
<p>A REVIEW OF THE ROLE OF CAUSALITY IN DEVELOPING TRUSTWORTHY AI SYSTEMS -DATASETS AND PACKAGES</p>
<p>As a result of our literature review on causality-based solutions for Trustworthy AI, a need for an extensive overview of relevant datasets and packages was observed. To make causal machine learning (ML) more accessible and to facilitate comparisons to non-causal methods, we created a curated list of datasets used for recent Causal ML publications. This appendix also includes an overview of useful causal and non-causal tools and packages to assess different trustworthy aspects of ML models (interpretability, fairness, robustness, privacy, and safety). We also provide a similar overview for the healthcare domain. Each aspect has its dedicated section that is structured as follows:</p>
<p>(1) An overview of publicly available real-world datasets used in cited publications of this survey (2) Some benchmarks and packages for Causal Machine Learning that researchers could utilize (3) A number of well-established tools, that allow for a better comparison to non-causal machine learning</p>
<p>We want to clarify that this section does not (and cannot) aim for completeness. Instead, we want to provide researchers interested in working on a selection of aspects of Trustworthy AI with a concise overview of exciting avenues for experimenting with causal machine learning. The resources are hyperlinked and sorted based on when their associated causal papers first appear in the corresponding subsections (e.g., datasets used in pre-processing papers will appear first). We highly encourage readers to seek additional reading material, such as Chapter 9 of [117] or the two Github repositories for datasets 3 and algorithms 4 resulting from [91].</p>
<p>A INTERPRETABILITY</p>
<p>A.1 Datasets Used by Cited Publications</p>
<p> CANDLE [215]: A dataset of realistic images of objects in a specific scene generated based on observed and unobserved confounders (object, size, color, rotation, light, and scene). As each of the 12546 images is annotated with the ground-truth information of the six generating factors, it is possible to emulate interventions on image features.   Used by: [215]  MIND [286]: A news recommendation dataset built upon user click logs of Microsoft News. It contains 15 million impression logs describing the click behavior of more than 1 Million users across over 160k English news articles. Each news article entry contains its title, category, abstract, and body. Each log entry is made up of the users' click events, non-clicked events, and historical news click behaviors prior to this impression.   Used by: [251]  MovieLens [95]: A group of datasets containing movie ratings between 0 and 5 (with 0.5 increments) collected from the MovieLens website. Movies are described through their title, genre, and relevance scores of tags (e.g., romantic or funny  WMT 14 [29]: WMT 5 is a yearly workshop in which researchers develop machine translation models for several different tasks. WMT14 was created for the event in 2014 and included a translation, a quality estimation, a metrics, and a medical translation task. Each category comprises different subtasks (e.g., translating between two specific languages).   Used by: [9]  OpenSubtitles [143]: A text corpus comprising over 2.6 billion sentences from movie dialogues. The data stem from pre-processing 3,735,070 files from the online database OpenSubtitles.org 6 . This corpus covers dialogues from ca. 2.8 million movies in 62 languages.   Used by: [9]  LAMA [203]: A probe designed to examine the factual and commonsense knowledge in pretrained language models. It is built upon four different, prominent corpora of facts that cover a wide range of knowledge types.  [62,126]: A tabular dataset containing anonymized data from the 1994 Census bureau database. 9 Classifiers try to predict whether a given person will earn over or under 50,000 USD worth of salary. Each person is described via 15 features (including their id), e.g., gender, education, and occupation.   Used by: [74,155]  Human Activity Recognition [11]: This dataset contains smartphone-recorded sensor data from 30 subjects performing Activities of Daily Living. The database differentiates between the activities walking (upstairs, downstairs, or on the same level), sitting, standing, and laying.   Used by: [111]  Yelp [294]: A dataset of almost 7 million Yelp user reviews of around 150k businesses across 11 cities in the US and Canada. Review entries contain not only their associated text and an integer star rating between 1 and 5 but also additional information like the amount of useful, funny, and cool votes for the review.   Used by: [257]  Amazon (Product) Data [179]: An extensive dataset of 233.1 million Amazon reviews between May 1996 and October 2018. The data include not only information about the review itself and product metadata (e.g., descriptions, price, product size, or package type) but also also bought and also viewed links.   Used by: [257]  Sangiovese Grapes [154]: A conditional linear Bayesian network that captures the effects of different canopy management techniques on the quality of Sangiovese grapes. Based on a two-year study of Tuscan Sangiovese grapes, the authors created a network with 14 features (13 of which are continuous variables). The data used for experiments in [155] are linked in their repository (see the link behind the term "Sangiovese Grapes").   Used by: [155]  WikiText-2 [163]: An NLP benchmark containing over 100 million tokens extracted from verified Good and Featured articles on Wikipedia. Contrary to previous token collections, however, WikiText-2 is more extensive and comprises more realistic tokens (e.g., lower-case tokens).   Used by: [112]  Jigsaw Toxicity Detection [115]: A dataset of comments made across around 50 Englishlanguage news sites built to analyze unintended bias in toxicity classification within a Kaggle competition organized by Jigsaw and Google. Each comment in the training set comes with a human-annotated toxicity label (e.g., obscene or threat) and labels for mentioned identities (e.g., gender, ethnicity, sexuality, or religion) in the comment.   Used by: [112]  RTGender [278]: A collection of comments made on online content across different platforms such as Facebook or Reddit. Each post and comment is annotated with the gender of the author in order to analyze gender bias in social media.   Used by: [112]  CrowS-Pairs [175]: A benchmark designed to investigate the social bias of NLP models.</p>
<p>Each entry consists of two sentences: one representing a stereotypical statement for a given bias type (e.g., religion or nationality) and an anti-stereotypical version of the statement, where the described group/identity was substituted.   Used by: [112]  Professions [274]: A set of templates (originating from [151]) that were augmented with professions from [30].  [59]: A collection of 397,340 online biographies covering 28 occupations (e.g., professors, physicians, or rappers). Each biography is stored as a dictionary containing the title, the (binary) gender, the length of the first sentence, and the entire text of the biography.   Used by: [211,212]  TwitterAAE corpus [28]: A collection of 59.2 million tweets sent out by 2.8 million users from the US in 2013. Each tweet is annotated with a vector describing the "likely demographics of the author and the neighborhood they live in. " [28] These demographic approximations of users were built upon US census data.   Used by: [211]  CelebA [149]: A face image dataset containing 202,599 images of size 178  218 from 10,177 unique celebrities. Each image is annotated with 40 binary facial attributes (e.g., Is this person smiling?) and five landmark positions describing the 2D position of the eyes, the nose, and the mouth (split into left and right side of the mouth).   Used by: [212] A.  [62,126]: A tabular dataset containing anonymized data from the 1994 Census bureau database. 10 Classifiers try to predict whether a given person will earn over or under 50,000 USD worth of salary. Each person is described via 15 features (including their id), e.g., gender, education, and occupation.   Used by: [75,174,191,230,288,290,291,304,305]  COMPAS Recidivism Risk [12]: A set of criminological datasets published by ProPublica to evaluate the bias of COMPAS -an algorithm used to assess the likelihood of criminal defendants reoffending. All COMPAS-related datasets include data from over 10,000 defendants, each being described via 52 features (e.g., age, sex, race) and with a label indicating whether they were rearrested within two years.   Used by: [50,75,168,173,174,230]  FICO Credit Risk [185]: In this dataset, ML models have to predict whether or not credit applicants will at least once be more than 90 days due with their payment within a two-year timespan. It includes anonymized information about HELOC applicants described through 23 features (e.g., months since the most recent delinquency or number of inquiries in last 6 months) [ </p>
<p>C.2 Interesting Causal Tools</p>
<p> CANDLE [215]: A dataset of realistic images of objects in a specific scene generated based on observed and unobserved confounders (object, size, color, rotation, light, and scene). As each of the 12546 images is annotated with the ground-truth information of the six generating factors, it is possible to emulate interventions on image features.  CausalWorld [4]: A simulation framework and benchmark that provides RL agents different learning tasks in a robotic manipulation environment. The environment comes with a causal structure on which users and agents can intervene on variables such as object masses, colors or sizes.  gCastle [300]: An end-to-end causal structure learning toolbox that is equipped with 19 techniques for Causal Discovery. It also assists users in data generation and evaluating learned structures. Having a firm understanding of the causal structure allows models to deduce the content and style variables of the domain.  Benchpress [220]: A benchmark for causal structure learning allowing users to compare their causal discovery methods with over 40 variations of state-of-the-art algorithms. The plethora of available techniques in this single tool could facilitate research into robustness of ML systems through causality.</p>
<p>C.3 Prominent Non-Causal Tools</p>
<p> DomainBed and OOD-Bench [89,292]: DomainBed is a benchmark for OOD-learning that enables performance comparisons with more than 20 OOD-algorithms on 10 different, popular OOD-datasets. OOD-Bench is built upon DomainBed and introduces a measurement to quantify the Diversity shift and Correlation shift inherit to OOD-datasets. The resulting categorization allows researchers to pinpoint strengths and weaknesses of OOD-learning algorithms.  RobustBench [56]: A standardized adversarial robustness benchmark capable of emulating a variety of adversarial attacks for image classification through AutoAttack. It also provides multiple continuously updated leaderboards of the most robust models, which allows for direct comparisons between causal and non-causal methods.</p>
<p> Foolbox [210]: A popular Python library that allows researchers to test their adversarial defenses against state-of-the-art adversarial attacks. Foolbox is very compatible, natively supporting Pytorch, Tensorflow and JAX models.  VeriGauge [138]: A Python toolbox that allows users to verify the robustness of their adversarial defense approach for deep neural networks. It not only covers a multitude of verification techniques but also comes with an up-to-date leaderboard.  Adversarial Robustness Toolbox (ART) [180]: An extensive Python library for Adversarial Machine Learning. It not only equips researchers with various attacks and defenses across four different attack threats (evasion, extraction, poisoning, and inference) but also provides the means to assess the performance of such algorithms thoroughly. ART is compatible with many popular frameworks and supports various data types and learning tasks.  [127]: The two CIFAR datasets, CIFAR-10 and CIFAR-100, are labeled images stemming from the now withdrawn Tiny Images dataset 20 . The more prominent set, CIFAR-10, contains 60000 32  32 color images separated into ten mutually exclusive classes, with 6000 images per class. CIFAR-100 is simply a 100-class version of CIFAR-10.   Used by: [92,114]  Digits-DG [312]: An image dataset specifically designed to evaluate the performance of models on OOD data. It includes images from four different handwritten digits databases. Each dataset represents a unique domain as images from different datasets significantly differ in terms of, e.g., handwriting style or background color.   Used by: [114]  Camelyon17 [16]: A publicly available medical dataset containing 1000 histology images from five Dutch hospitals. Given an image, classification models need to detect breast cancer metastases.   Used by: [114] </p>
<p>D.2 Interesting Causal Tools For Federated Learning</p>
<p>The publications reviewed in Section 6 are largely causal approaches to Federated Learning (FL). As such, we mainly provide an overview of causal and non-causal tools for FL.</p>
<p> Federated Causal Discovery [2,77]: Until this point, we suggested general causal discovery tools like gCastle [300] or benchpress [220]. However, the provided methods translate poorly into the federated setting due to the decentralized data. As such, we would like to refer readers to recently developed Federated Causal Discovery techniques (e.g., [2,77]). These methods are specifically designed to conduct causal discovery on decentralized data in a privacy-preserving manner.  CANDLE [215]: A dataset of realistic images of objects in a specific scene generated based on observed and unobserved confounders (object, size, color, rotation, light, and scene). As each of the 12546 images is annotated with the ground-truth information of the six generating factors, it is possible to emulate interventions on image features. Users/Devices could be simulated by altering the scenery.  gCastle [300]: An end-to-end causal structure learning toolbox that is equipped with 19 techniques for Causal Discovery. It also assists users in data generation and evaluating learned structures. Having a firm understanding of the causal structure is crucial for safety-related research.  Benchpress [220]: A benchmark for causal structure learning allowing users to compare their causal discovery methods with over 40 variations of state-of-the-art algorithms. The plethora of available techniques in this single tool could facilitate research into safety and accountability of ML systems through causality.  CauseEffectPairs [170]: A collection of more than 100 databases, each annotated with a two-variable cause-effect relationship (e.g., access to drinking water affects infant mortality). Given a database, models need to distinguish between the cause and effect variables.</p>
<p>E.3 Prominent Non-Causal Tools</p>
<p> Government of Canada's AIA tool [84]: The Algorithmic Impact Assessment (AIA) tool is a questionnaire developed in the wake of Canada's Directive on Automated Decision Making 23 . Employees of the Canadian Government wishing to employ automatic decisionmaking systems in their projects first need to assess the impact of such systems via this tool. Based on answers given to ca. 80 questions revolving around different aspects of the projects, AIA will output two scores: one indicating the risks that automation would bring and one that quantifies the quality of the risk management.  Aequitas [228]: An open-source auditing tool designed to assess the bias of algorithmic decision-making systems. It provides utility for evaluating the bias of decision-making outcomes and enables users to assess the bias of actions taken directly.  Error Analysis (Responsible AI ) [165]: As part of the Responsible AI toolbox, Error Analysis is a model assessment tool capable of identifying subsets of data in which the model performs poorly (e.g., black citizens being more frequently misclassified as potential re-offenders). It also enables users to diagnose the root cause of such poor performance.  ML-Doctor [147]: A codebase initially used to compare and evaluate different inference attacks (membership inference, model stealing, model inversion, and attribute inference). Due to its modular structure, it can also be used as a Risk Assessment tool for analyzing the susceptibility against SOTA privacy attacks.</p>
<p>F HEALTHCARE</p>
<p>F.1 Datasets Used by Cited Publications</p>
<p> Alzheimer's Disease Neuroimaging Initiative (ADNI) [202]: A medical dataset containing multi-modal information of over 5,000 volunteering subjects. ADNI includes clinical and genomic data, biospecimens, MRI, and PET images. Researchers need to apply for data access.   Used by: [231,242]  SARS-CoV-2 infected cells (Series GSE147507) [27]: A genomic dataset that contains RNA-seq data from SARS-CoV-2-infected cells of both humans and ferrets. The data are publicly available on the NCBI Gene Expression Omnibus (GEO) server under the accession number GSE147507.   Used by: [23]  The Genotype-Tissue Expression (GTEx) project [39]: An online medical platform that provides researchers with tissue data. Data samples stem from 54 non-diseased tissue sites across nearly 1000 individuals whose genomes were processed via sequencing methods such as WGS, WES, and RNA-Seq.   Used by: [23]  L1000 Connectivity Map (Series GSE92742) [256]: A connectivity map (CMap) connects genes, drugs, and disease states based on their gene-expression signatures. The CMap provided by Subramanian et al. includes over 1.3 million L1000 profiles of 25,200 unique perturbagens 24 .</p>
<p>The data are publicly available on the NCBI Gene Expression Omnibus (GEO) server under the accession number GSE92742.  Used by: [23]  iRefIndex [213]: This protein-protein interaction (PPI) network is a graph-based database of molecular interactions between proteins from over ten organisms. The current version of iRefIndex (version 19) contains over 1.6 million PPIs.   Used by: [23]  DrugCentral [15]: An online platform that provides up-to-date drug information. Users can traverse the database online through the corresponding website or via an API. The platform currently contains information on almost 5,000 active ingredients.   Used by: [23]  Colorectal Cancer Single-cell Data (GSE81861) [137]: The authors provide two datasets.</p>
<p>The first dataset contains 1,591 single cells RNA-seq data from 11 colorectal cancer patients. The second dataset contains 630 single cells from seven cell lines and can be used to benchmark cell-type identification algorithms. The data are publicly available on the NCBI Gene Expression Omnibus (GEO) server under the accession number GSE81861.   Used by: [23]  Pulmonary Fibrosis Single-cell Data [217]: This genomic dataset contains approximately 76,000 single-cell RNA-seq data from healthy lungs and lungs from patients with pulmonary fibrosis. The data are available online and comes with a cluster visualization based on marker gene expressions.   Used by: [23]  SARS-CoV-2 Host-Pathogen Interaction Map [83]: A PPI network that maps 27 SARS-CoV-2 proteins to human proteins through 332 high-confidence protein-protein interactions. The online data contain data from the initial study and the CORUM database [265].   Used by: [23]  Lung Image Database Consortium image collection (LIDC-IDRI) [14]: An image dataset comprising annotated thoracic CT Scans of more than 1,000 cases. The data stem from seven academic centers and eight medical imagining companies. Four trained thoracic radiologists provided the image annotations.   Used by: [271]  MEDLINE [269]: An online bibliographic database of more than 29 million article references from the field of life science (primarily in biomedicine). MEDLINE is a primary component of PubMed and is hosted and managed by the NLM National Center for Biotechnology Information (NCBI).   Used by: [314]  Cochrane Central Register of Controlled Trials (CENTRAL) [53]: A database of reports for randomized and quasi-randomized controlled trials collected from different online databases. Although it does not contain full-text articles, the CENTRAL includes bibliographic details and often an abstract of the report.   Used by: [314] F.2 Interesting Causal Tools  Causal Inference 360 [245]: A Python package developed by IBM to infer causal effects from given data. Causal Inference 360 includes multiple estimation methods, a medical dataset, and multiple simulation sets. The provided methods can be used for any complex ML model through a scikit-learn-inspired API.  gCastle [300]: An end-to-end causal structure learning toolbox that is equipped with 19 techniques for Causal Discovery. It also assists users in data generation and evaluating learned structures. Having a firm understanding of the causal structure is crucial for healthcare-related research.</p>
<p>Fig. 2 .
2Structure of approaches introducing causality in interpretability.</p>
<p>Fig. 3 .
3Segregation of causality-based methods for fairness-aware learning.</p>
<p>Group fairness notions assess the large-scale biased effect of the learning algorithm on a certain legally protected group of the underlying dataset. Individual fairness notions measure the difference in the decisions predicted for similar individuals in a population. Causal fairness notions can be further segregated based on two criteria: (a) Counterfactual fairness (CF) and (b) Interventional Fairness (IF).4.1.1 Counterfactual fairness (CF) . CF measures fairness by quantifying the effect of sensitive attributes on the predicted outcome through counterfactuals. If the sensitive attribute (e.g. = 'gender') is binary then it could take two values protected, i.e. the disadvantaged group ( + = 'female') and non-protected, i.e. favoured (  = 'male') group.</p>
<p>Fig. 5 .
5Structure of approaches introducing causality in robustness.</p>
<p>Fig. 6 .
6Causality and privacy approaches.</p>
<p>Fig. 7 .
7Safety through (causal) impact assessment.</p>
<p>Fig. 8 .
8Structure of different healthcare applications used with causality.</p>
<p>ACKNOWLEDGMENTS
This work has received funding from the European Union's Horizon 2020 research and innovation programme under Marie Sklodowska-Curie Action "NoBIAS -Artificial Intelligence without Bias" (grant agreement number 860630) and Network of Excellence "TAILOR -A Network for Trustworthy Artificial Intelligence" (grant agreement number 952215), the Lower Saxony Ministry of Science and Culture under grant number ZN3492 within the Lower Saxony "Vorab" of the Volkswagen Foundation and supported by the Center for Digital Innovations (ZDIN), and the Federal Ministry of Education and Research (BMBF), Germany under the project "LeibnizKILabor" with grant No. 01DD20003 and from Volkswagen Foundation and the Ministry for Science and Culture of Lower Saxony, Germany (MWK) under the "Understanding Cochlear Implant Outcome Variability using Big Data and Machine Learning Approaches" (grant no. ZN3429) project.</p>
<p>REx[67]: A dataset of large-scale alignments between Wikipedia abstracts and Wikidata triples. Such triples encode semantic information in the form of subject-predicate-object relationships. T-REx consists of 11 million triples with 3. 
 Used by: [37] 
 Comma.ai Driving Dataset [232]: A video dataset made up of 11 video clips of variable 
size capturing the windshield view of an Acura ILX 2016. The driving data contains 7.25 
hours of footage, which was mostly recorded on highways. Each video is accompanied by 
measurements such as the car's speed, acceleration, or steering angle.  
 Used by: [125] 
 Udacity Driving Dataset [267]: A driving video dataset developed for the Udacity Self-
Driving Car Nanodegree Program 7 . The GitHub repository contains two annotated datasets 
in which computer vision systems have to label objects, such as cars or pedestrians, within 
driving footage.  
 Used by: [125] 
 T-09 million Wikipedia abstracts (6.2 
million sentences).  
 Used by: [139] 
 MNIST [134]: An extraordinarily well-known and widely used image dataset comprising 
28  28 grayscale images of handwritten digits. It contains 60,000 training and 10,000 test 
samples.  
 Used by: [236] 
 ImageNet [61]: Another well-known, more sophisticated image dataset containing more 
than 14 million images. The images depict more than 20,000 synsets (i.e., concepts "possibly 
described by multiple words or word phrases" 8 ).  
 Used by: [236] 
 Adult (Census Income) </p>
<p>CausaLM Datasets[71]: As part of the analysis of CausaLM, the authors developed four NLP datasets for evaluating causal explanations. These datasets represent real-world applications of ML that come with ground-truth information. Competing with Causal Toolboxes: Several causal tools like YLearn[57], DoWhy[240],CausalML[46], or EconML[123] introduce an entire causal inference pipeline with their own interpreter module. Comparing newly developed interpretation techniques with such packages could be very insightful. function that outputs class probabilities given raw text or a NumPy array.  ROAR[102]: A benchmark method that evaluates interpretability approaches based on how well they quantify feature importance. The technique was used to assess model explanations of image classifiers over multiple datasets.  SHAP[152]: Another well-known interpretability package which is based on game theory.2 Interesting Causal Tools 
 CausalFS [296]: An open-source package for C++ that contains 28 local causal structure 
learning methods for feature selection. It is specifically designed to facilitate the development 
and benchmarking of new causal feature selection techniques. 
 CEBaB [1]: A recently designed benchmark to estimate and compare the quality of concept-
based explanation for NLP. CEBaB includes a set of restaurant reviews accompanied by 
human-generated counterfactuals, which enables researchers to investigate the model's 
ability to assess causal concept effects. 
 A.3 Prominent Non-Causal Tools 
 LIME [218]: A very prominent Python package that allows researchers to explain individual 
predictions of image, text, and tabular data classifiers. Applicable to any black-box classifier 
that implements a Although compatible with any ML model, SHAP comes with a C++-based algorithm for tree 
ensemble algorithms such as XGBoost. 
 InterpretML [183]: An open-source package developed by Microsoft that includes multiple 
state-of-the-art methods for model interpretability. It also allows users to train an Explainable 
Boosting Machine (EBM) -a model that provides exact explanations and performs as well as 
random forests and gradient-boosted trees. </p>
<p>B FAIRNESS </p>
<p>B.1 Datasets Used by Cited Publications 
 Adult (Census Income) </p>
<p>Medical Expenditure (MEPS)[40]: A collection of large-scale surveys of US citizens, their medical providers, and employers. It includes information like race, gender, and the ICD-10 code of the diagnosis of a patient. The given information can be used to predict the total number of patients' hospital visits.   Used by:[75]  MIMIC III[116]: A dataset of anonymized clinical records of the Beth Israel Deaconess Medical Center in Boston, Massachusetts. Civil Rights Data Collection (CRDC)[268]: This is an online collection of educationrelated data. Since 1968, the U.S. Department of Education's Office for Civil Rights (OCR) biennially collects data from U.S. public primary and secondary schools. The dataset includes information such as race distribution, the percentage of students who take college entrance exams, or whether specific courses (e.g., Calculus) are offered.   Used by:[129]  Berkeley [25]: A simple gender bias dataset published back in 1975 containing information on all 12,763 applicants to the University of California, Berkeley graduate programs in Fall 1973. Each candidate entry consists of the candidate's major, gender, year of application (always 1973), and whether they were accepted.   Used by: [291] B.2 Interesting Causal Tools  Collection of Annotated Datasets [133]: As part of a survey that provides a thorough overview of commonly used datasets for evaluating the fairness of ML, Le Quy et al. generated Bayesian Networks encompassing the relationships of attributes for each dataset. This information could be used as a reference point for potential causal annotations of fairness-related datasets.  WhyNot [167]: A Python package that provides researchers with many simulation environments for analyzing causal inference and decision-making in a dynamic setting. It allows benchmarking of multiple decision-making systems on 13 different simulators. Crucially for this section, WhyNot also enables comparisons based on other evaluation criteria, such as the fairness of the decision-making.  gCastle [300]: An end-to-end causal structure learning toolbox that is equipped with 19 techniques for Causal Discovery. It also assists users in data generation and evaluating learned structures. Having a firm understanding of the causal structure is crucial for fairness-related research.  Benchpress [220]: A benchmark for causal structure learning allowing users to compare their causal discovery methods with over 40 variations of state-of-the-art algorithms. The plethora of available techniques in this single tool could facilitate research into fair ML through causality.  CausalML [46]: The Python package enables users to analyze the Conditional Average Treatment Effect (CATE) or Individual Treatment Effect (ITE) observable in experimental data. The package includes tree-based algorithms, meta-learner algorithms, instrumental variable algorithms, and neural-network-based algorithms. Fair-ML researchers could use the provided methods to investigate the causal effect of sensitive attributes on the predicted outcome. B.3 Prominent Non-Causal Tools  AI Fairness 360 [21]: An open-source library (compatible with both Python and R) that allows researchers to measure and mitigate possible bias within their models/algorithms. It includes six real-world datasets, five fairness metrics, and 15 bias mitigation algorithms.  Fairlearn [26]: A Python package developed by Microsoft, which is part of the Responsible AI toolbox 11 . It contains various fairness metrics, six unfairness-mitigating algorithms, and five datasets.  Aequitas [228]: An open-source auditing tool designed to assess the bias of algorithmic decision-making systems. It provides utility for evaluating the bias of decision-making outcomes and enables users to assess the bias of actions taken directly.  ML-Fairness-Gym [65]: A third-party extension of the OpenAI gym designed to analyze bias within RL agents. Although not built upon real-world data, the simulations developed for this benchmark can lead to insights applicable to the real world. It comes with four simulation environments. C ROBUSTNESS C.1 Datasets Used by Cited Publications  Rotated MNIST [81]: The dataset consists of MNIST images with each domain containing images rotated by a particular angle {0  , 15  , 30  , 45  , 60  , 75  }   Used by: [106]  ColoredMNIST [13]: The dataset consists of input images with digits 0-4 colored red and labelled 0 while digits 5-9 are colored green representing the two domains.   Used by: [13, 106, 150] to investigate factors that influence the likelihood of recidivism. Each record contains 19 variables, including a binary ethnicity variable (black or not black) and a variable indicating previous use of hard drugs.   Used by: [63]  Colored FashionMNIST [5]: This dataset was inspired by Arjovsky et al. [13] Colored MNIST dataset. Ahuja et al. use the same coloring approach to induce spurious correlations into FashionMNIST data (greyscaled Zalando articles).   Used by: [150]  VLCS [262]: A collection of 10,729 images from four standard datasets designed to evaluate the OOD performance of image classifiers. Each image depicts a bird, car, chair, dog, or person.   Used by: [150]  VQA-CP [3]: A dataset for Visual Question Answering (VQA) models that actively punishes the use of spurious correlations. This is achieved by rearranging the VQA v1 and VQA v2 data splits. The resulting training and test data differ in the "distribution of answers per question type".   Used by: [258]  COCO [141]: An object detection dataset containing 328k images that depict 91 different types of objects. Each object within an image has its unique annotation, leading to more than 2.5 million labels across the entire dataset.   Used by: [258]  Law School Admission Data [204]: A tabular dataset of admission data from 25 US law schools between 2005 and 2007. This dataset contains information from more than 100,000 applicants (e.g., gender, ethnic group, LSAT score), with each entry having a binary admission status variable.   Used by: [280]  MNIST [134]: An extraordinarily well-known and widely used image dataset comprising 28  28 grayscale images of handwritten digits. It contains 60,000 training and 10,000 test samples.   Used by: [144, 298, 307]  Sequential MNIST Resolution Task [128]: A sequential version of MNIST, where pixels of an handwritten digit are shown one at a time.   Used by: [85]  Bouncing Ball [272]: A simulation environment where multiple balls of different sizes and weights independently move according to Newtonian physics. This environment is used to assess the model's physical reasoning capabilities under different conditions (e.g., different amounts of balls).   Used by: [85]  BabyAI [48]: A RL framework that supports the development of agents that can understand language instructions. For this purpose, the authors developed agents that simulate human experts capable of communicating with task-solving agents using synthetic natural language.The platform provides 19 levels to alter the difficulty of the task.   Used by:[85]  MultiNLI[282]: A text dataset developed to evaluate natural language inference (NLI) models. Models must decide whether a given hypothesis is contradictory to, entailed by, or neutral to the given premise. Contrary to other NLI datasets, MultiNLI includes text from 10 written and spoken English domains.   Used by:[279]  Abalone[62]: In this task, ML models need to predict the number of rings an abalone (a shellfish) has based on the given features sex, width, height, and shell diameter. The dataset contains 4177 entries.   Used by:[131]  Bike Sharing in Washington D.C.[70]: This dataset contains the hourly and daily count of rental bikes used in Washington D.C. between 2011 and 2012 (17,379 entries). Given weather and seasonal information, models need to predict the count of total rental bikes.   Used by:[131]  OpenPowerlifting[188]: This powerlifting competition dataset includes more than 22,000 competitions and more than 412,000 competitors as of April 2019. The data stem from OpenPowerlifting19 , with each entry containing information about the lifter, the equipment used, weight class, and their performance across different powerlifting disciplines.   Used by:[131] 44]  
 Used by: [55] 
 German Credit Risk [62]: A collection of data from 1,000 anonymized German bank account 
holders that applied for a credit. Based on the 20 features of the applicant and their application 
(e.g., credit history, purpose of credit, or employment status), models need to estimate the 
risk of giving the person a credit and categorize them as either good or bad credit recipients.. 
 
 Used by: [75] 
 Records contain information like ICD-9 codes for 
diagnoses and medical procedures, vital signs, medication, or even imaging data. The dataset 
includes records from 38,597 distinct adult patients.  
 Used by: [252] 
 MovieLens [95]: A group of datasets containing movie ratings between 0 and 5 (with 0.5 
increments) collected from the MovieLens website. Movies are described through their title, 
genre, and relevance scores of tags (e.g., romantic or funny). GroupLens Research constantly 
releases new up-to-date MovieLens databases in different sizes.  
 Used by: [140] 
 Zimnat Insurance Recommendation [315]: A data collection of almost 40,000 Zimnat (a 
Zimbabwean insurance provider) customers. The data contain personal information (e.g., 
marital status or occupation) and the insurance products that the customers own. In infer-
ence time, models must predict which product was artificially removed based on customer 
information.  
 Used by: [140] 
 </p>
<p>D PRIVACY
PRIVACYD.1 Datasets Used by Cited Publications Rotated MNIST[81]:The dataset consists of MNIST images with each domain containing images rotated by a particular angle {0  , 15  , 30  , 45  , 60  , 75  }   Used by: [60, 72]  PACS [136]: An image classification dataset categorized into 10 classes that are scattered across four different domains, each having a distinct trait: photograph, art, cartoon and sketch.   Used by: [60]  Office-Home [273]: Image classification dataset analogous to PACS, having four distinct image domains: Art, ClipArt, Product and Real-World.   Used by: [60]  ColoredMNIST [13]: The dataset consists of input images with digits 0-4 colored red and labelled 0 while digits 5-9 are colored green representing the two domains.   Used by: [72, 92]  Colored FashionMNIST [5]: This dataset was inspired by Arjovsky et al. [13] Colored MNIST dataset. Ahuja et al. use the same coloring approach to induce spurious correlations into FashionMNIST data (greyscaled Zalando articles).   Used by: [92]  CIFAR</p>
<p> Federated Causal Effect Estimation[276]: Similar to causal discovery, standard causal effect estimation methods were not designed for decentralized data. Only very recently, Vo et al. developed a causal effect estimation framework compatible with federated learning. Despite this line of work's infancy, we believe that this publication is important for more privacy-preserving causal learning.D.3 Prominent Non-Causal Federated Learning ToolsLEAF [35]: A benchmark containing datasets explicitly designed to analyze FL algorithms. The six datasets include existing re-designed databases such as CelebA [149] to emulate different devices/users and newly created datasets. LEAF also provides evaluation methods and baseline reference implementations for each dataset.  FedEval [41]: A publicly available evaluation platform for FL. It allows researchers to compare their FL methods with existing state-of-the-art algorithms on seven datasets based on five FL-relevant metrics (Accuracy, Communication, Time efficiency, Privacy, and Robustness). The benchmark utilizes Docker container technology to simulate the server and clients and socket IO for simulating communication between the two.  OARF [103]: An extensive benchmark suite designed to assess state-of-the-art FL algorithms for both horizontal and vertical FL. It includes 22 datasets that cover different domains for both FL variants. Additionally, OARF provides several metrics to evaluate FL algorithms, and its modular design enables researchers to test their own methods.  FedGraphNN [97]: An FL benchmark for Graph Neural Networks (GNN). In order to provide a unified platform for the development of graph-based FL solutions, FedGraphNN supplies users with 36 graph datasets across seven different domains. Researchers can also employ and compare their own PyTorch (Geometric) models with different GNNs.  ML-Doctor [147]: A codebase initially used to compare and evaluate different inference attacks (membership inference, model stealing, model inversion, and attribute inference). Its modular structure enables researchers to assess the effectiveness of their privacy-preserving algorithms against SOTA privacy attacks. E SAFETY E.1 Datasets Used by Cited Publications  ScienceDirect [68]: A bibliographic database that hosts over 18 million publications from more than 4,000 journals and more than 30,000 e-books from the publisher Elsevier. Launched back in 1997, ScienceDirect includes papers from engineering and medical research areas and social sciences and humanities.  Used by: [277]  World Bank [259]: A publicly available collection of datasets that facilitate the analysis of global development. Researchers can use this data to compare countries under different developmental aspects, including agricultural progress, poverty, population dynamics, and economic growth.   Used by: [287]
We follow[18] and avoid using an equality sign for structural equations since their interpretation is that of an assignment statement; they should not be interpreted as algebraic equations that can be solved for any variable.
Article 13, https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206
https://github.com/rguo12/awesome-causality-data 4 https://github.com/rguo12/awesome-causality-algorithms
https://machinetranslate.org/wmt 6 https://www.opensubtitles.org/ 7 https://udacity.com/self-driving-car 8 https://www.image-net.org/about.php 9 http://www.census.gov/en.html
http://www.census.gov/en.html
https://github.com/microsoft/responsible-ai-toolbox
 ETH and UCY[69,135]: Both ETH[69] and UCY[135] are datasets containing real-world pedestrian trajectories. More novel papers combine both datasets to simulate multiple training and testing environments. Together, they contain trajectories of 1536 detected pedestrians
https://www.openpowerlifting.org/
http://groups.csail.mit.edu/vision/TinyImages/
https://www.lendingclub.com/investing/peer-to-peer22 Available at https://github.com/ustunb/actionable-recourse/tree/master/examples/paper/data under the name "credit_processed.csv"
http://www.tbs-sct.gc.ca/pol/doc-eng.aspx?id=32592
See https://clue.io/connectopedia/perturbagen_types_and_controls for the definition of this term
 World Economic Forum (WEF)[284]: The WEF is an international non-governmental based in Switzerland that publishes economic reports such as the Global Competitiveness Report. The reports are available online, with some of the data being easily accessible through websites like Knoema.   Used by:[96] OECD.Stat[189]: This webpage includes data and metadata for OECD countries and selected non-member economies. The online platform allows researchers to traverse the collected data through given data themes or via search-engine queries.   Used by:[
E D Abraham, K Oosterlinck, A Feder, Y O Gat, A Geiger, arXiv:2205.14140CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior. arXiv preprintE. D. Abraham, K. D'Oosterlinck, A. Feder, Y. O. Gat, A. Geiger, et al. 2022. CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior. arXiv preprint arXiv:2205.14140 (2022).</p>
<p>A Abyaneh, N Scherrer, P Schwab, S Bauer, B Schlkopf, arXiv:2211.03846FED-CD: Federated Causal Discovery from Interventional and Observational Data. arXiv preprintA. Abyaneh, N. Scherrer, P. Schwab, S. Bauer, B. Schlkopf, et al. 2022. FED-CD: Federated Causal Discovery from Interventional and Observational Data. arXiv preprint arXiv:2211.03846 (2022).</p>
<p>Don't just assume; look and answer: Overcoming priors for visual question answering. A Agrawal, D Batra, D Parikh, A Kembhavi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionA. Agrawal, D. Batra, D. Parikh, and A. Kembhavi. 2018. Don't just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4971-4980.</p>
<p>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. O Ahmed, F Truble, A Goyal, A Neitz, M Wthrich, International Conference on Learning Representations. O. Ahmed, F. Truble, A. Goyal, A. Neitz, M. Wthrich, et al. 2021. CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. In International Conference on Learning Representations.</p>
<p>Invariant risk minimization games. K Ahuja, K Shanmugam, K Varshney, A Dhurandhar, International Conference on Machine Learning. PMLR. K. Ahuja, K. Shanmugam, K. Varshney, and A. Dhurandhar. 2020. Invariant risk minimization games. In International Conference on Machine Learning. PMLR, 145-155.</p>
<p>Advances in adversarial attacks and defenses in computer vision: A survey. N Akhtar, A Mian, N Kardan, M Shah, IEEE Access. 9N. Akhtar, A. Mian, N. Kardan, and M. Shah. 2021. Advances in adversarial attacks and defenses in computer vision: A survey. IEEE Access 9 (2021), 155161-155196.</p>
<p>Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation. C F Aliferis, A Statnikov, I Tsamardinos, S Mani, X D Koutsoukos, Journal of Machine Learning Research. C. F. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X. D. Koutsoukos. 2010. Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation. Journal of Machine Learning Research (2010).</p>
<p>Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions. C F Aliferis, A R Statnikov, I Tsamardinos, S Mani, X D Koutsoukos, Journal of Machine Learning Research. C. F. Aliferis, A. R. Statnikov, I. Tsamardinos, S. Mani, and X. D. Koutsoukos. 2010. Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions. Journal of Machine Learning Research (2010).</p>
<p>A causal framework for explaining the predictions of black-box sequenceto-sequence models. D Alvarez-Melis, T S Jaakkola, arXiv preprintD. Alvarez-Melis and T. S. Jaakkola. 2017. A causal framework for explaining the predictions of black-box sequence- to-sequence models. arXiv preprint (2017).</p>
<p>. American Psychological Association. 2022. PsycINFO. American Psychological Association. 2022. PsycINFO. Retrieved December 30, 2022 from https://www.apa.org/pubs/ databases/psycinfo/index</p>
<p>A public domain dataset for human activity recognition using smartphones. D Anguita, A Ghio, L Oneto, X Parra Perez, J L , Reyes Ortiz, Proceedings of the 21th international European symposium on artificial neural networks, computational intelligence and machine learning. the 21th international European symposium on artificial neural networks, computational intelligence and machine learningD. Anguita, A. Ghio, L. Oneto, X. Parra Perez, and J. L. Reyes Ortiz. 2013. A public domain dataset for human activity recognition using smartphones. In Proceedings of the 21th international European symposium on artificial neural networks, computational intelligence and machine learning. 437-442.</p>
<p>J Angwin, J Larson, S Mattu, L Kirchner, Ethics of Data and Analytics. Auerbach PublicationsMachine biasJ. Angwin, J. Larson, S. Mattu, and L. Kirchner. 2016. Machine bias. In Ethics of Data and Analytics. Auerbach Publications, 254-264.</p>
<p>. M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, Invariant risk minimization. arXivM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. 2019. Invariant risk minimization. arXiv (2019).</p>
<p>The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans. S G Armato, Iii , G Mclennan, L Bidaut, M F Mcnitt-Gray, C R Meyer, Medical physics. 38S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer, et al. 2011. The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans. Medical physics 38, 2 (2011), 915-931.</p>
<p>DrugCentral 2021 supports drug discovery and repositioning. S Avram, C G Bologa, J Holmes, G Bocci, T B Wilson, Nucleic acids research. 49S. Avram, C. G. Bologa, J. Holmes, G. Bocci, T. B. Wilson, et al. 2021. DrugCentral 2021 supports drug discovery and repositioning. Nucleic acids research 49, D1 (2021), D1160-D1169.</p>
<p>From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. P Bandi, O Geessink, Q Manson, M Van Dijk, M Balkenhol, IEEE transactions on medical imaging. 38P. Bandi, O. Geessink, Q. Manson, M. Van Dijk, M. Balkenhol, et al. 2018. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE transactions on medical imaging 38, 2 (2018), 550-560.</p>
<p>Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. A Barbu, D Mayo, J Alverio, W Luo, C Wang, Advances in neural information processing systems. 32A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, et al. 2019. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems 32 (2019).</p>
<p>. E Bareinboim, J D Correa, D Ibeling, T Icard, n. d.E. Bareinboim, J. D. Correa, D. Ibeling, and T. Icard. [n. d.].</p>
<p>Medical Data for Machine Learning. A L Beam, A. L. Beam et al. 2016. Medical Data for Machine Learning. https://github.com/beamandrew/medical-data.</p>
<p>Social impact assessment. H A Becker, European Journal of Operational Research. 128H. A. Becker. 2001. Social impact assessment. European Journal of Operational Research 128, 2 (2001), 311-321.</p>
<p>AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. R K E Bellamy, K Dey, M Hind, S C Hoffman, S Houde, R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, et al. 2018. AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. https://arxiv.org/abs/1810.01943</p>
<p>The Arcade Learning Environment: An Evaluation Platform for General Agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 47M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. 2013. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research 47 (jun 2013), 253-279.</p>
<p>Causal network models of SARS-CoV-2 expression and aging to identify candidates for drug repurposing. A Belyaeva, L Cammarata, A Radhakrishnan, C Squires, K D Yang, Nature Communications. 121A. Belyaeva, L. Cammarata, A. Radhakrishnan, C. Squires, K. D. Yang, et al. 2021. Causal network models of SARS-CoV-2 expression and aging to identify candidates for drug repurposing. Nature Communications 12, 1 (2021).</p>
<p>The netflix prize. J Bennett, S Lanning, Proceedings of KDD cup and workshop. KDD cup and workshopNew York, NY, USA35J. Bennett, S. Lanning, et al. 2007. The netflix prize. In Proceedings of KDD cup and workshop, Vol. 2007. New York, NY, USA., 35.</p>
<p>Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation. P J Bickel, E A Hammel, J W O&apos;connell, Science. 187P. J. Bickel, E. A. Hammel, and J. W. O'Connell. 1975. Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation. Science 187, 4175 (1975), 398-404.</p>
<p>Fairlearn: A toolkit for assessing and improving fairness in AI. S Bird, M Dudk, R Edgar, B Horn, R Lutz, MSR-TR-2020-32Technical ReportS. Bird, M. Dudk, R. Edgar, B. Horn, R. Lutz, et al. 2020. Fairlearn: A toolkit for assessing and improving fairness in AI. Technical Report MSR-TR-2020-32. Microsoft. https://www.microsoft.com/en-us/research/publication/fairlearn-a- toolkit-for-assessing-and-improving-fairness-in-ai/</p>
<p>Imbalanced host response to SARS-CoV-2 drives development of COVID-19. D Blanco-Melo, B E Nilsson-Payant, W.-C Liu, S Uhl, D Hoagland, Cell. 181D. Blanco-Melo, B. E. Nilsson-Payant, W.-C. Liu, S. Uhl, D. Hoagland, et al. 2020. Imbalanced host response to SARS-CoV-2 drives development of COVID-19. Cell 181, 5 (2020), 1036-1045.</p>
<p>Demographic dialectal variation in social media: A case study of African-American English. S L Blodgett, L Green, B O&apos;connor, arXiv:1608.08868arXiv preprintS. L. Blodgett, L. Green, and B. O'Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. arXiv preprint arXiv:1608.08868 (2016).</p>
<p>Findings of the 2014 workshop on statistical machine translation. O Bojar, C Buck, C Federmann, B Haddow, P Koehn, Proceedings of the ninth workshop on statistical machine translation. the ninth workshop on statistical machine translationO. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation. 12-58.</p>
<p>Man is to computer programmer as woman is to homemaker? debiasing word embeddings. T Bolukbasi, K.-W Chang, J Y Zou, V Saligrama, A T Kalai, Advances in neural information processing systems. 29T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems 29 (2016).</p>
<p>S R Bowman, G Angeli, C Potts, C D Manning, arXiv:1508.05326A large annotated corpus for learning natural language inference. arXiv preprintS. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326 (2015).</p>
<p>Inferring causal impact using Bayesian structural time-series models. K H Brodersen, F Gallusser, J Koehler, N Remy, S L Scott, The Annals of Applied Statistics. K. H. Brodersen, F. Gallusser, J. Koehler, N. Remy, and S. L. Scott. 2015. Inferring causal impact using Bayesian structural time-series models. The Annals of Applied Statistics (2015), 247-274.</p>
<p>Language Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, In NeurIPST. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, et al. 2020. Language Models are Few-Shot Learners. In NeurIPS.</p>
<p>Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims. M Brundage, S Avin, J Wang, H Belfield, G Krueger, arXiv:2004.07213M. Brundage, S. Avin, J. Wang, H. Belfield, G. Krueger, et al. 2020. Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims. CoRR abs/2004.07213 (2020). arXiv:2004.07213 https://arxiv.org/abs/2004.07213</p>
<p>Leaf: A benchmark for federated settings. S Caldas, S M K Duddu, P Wu, T Li, J Konen, arXiv:1812.01097arXiv preprintS. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Konen, et al. 2018. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018).</p>
<p>Environmental impact assessment. L W Canter, Impact Assessment. 1L. W. Canter. 1982. Environmental impact assessment. Impact Assessment 1, 2 (1982), 6-40.</p>
<p>Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View. B Cao, H Lin, X Han, F Liu, L Sun, ACL. B. Cao, H. Lin, X. Han, F. Liu, and L. Sun. 2022. Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View. In ACL.</p>
<p>MONAI: An open-source framework for deep learning in healthcare. M J Cardoso, W Li, R Brown, N Ma, E Kerfoot, 10.48550/arXiv.2211.02701M. J. Cardoso, W. Li, R. Brown, N. Ma, E. Kerfoot, et al. 2022. MONAI: An open-source framework for deep learning in healthcare. (11 2022). https://doi.org/10.48550/arXiv.2211.02701</p>
<p>A novel approach to high-quality postmortem tissue procurement: the GTEx project. L J Carithers, K Ardlie, M Barcus, P A Branton, A Britton, Biopreservation and biobanking. 13L. J. Carithers, K. Ardlie, M. Barcus, P. A. Branton, A. Britton, et al. 2015. A novel approach to high-quality postmortem tissue procurement: the GTEx project. Biopreservation and biobanking 13, 5 (2015), 311-319.</p>
<p>The Medical Expenditure Panel Survey (MEPS). A D Center, RetrievedA. D. Center. 2016. The Medical Expenditure Panel Survey (MEPS). Retrieved November 25, 2022 from https: //meps.ahrq.gov/mepsweb/</p>
<p>D Chai, L Wang, K Chen, Q Yang, arXiv:2011.09655Fedeval: A benchmark system with a comprehensive evaluation model for federated learning. arXiv preprintD. Chai, L. Wang, K. Chen, and Q. Yang. 2020. Fedeval: A benchmark system with a comprehensive evaluation model for federated learning. arXiv preprint arXiv:2011.09655 (2020).</p>
<p>Causally Constrained Data Synthesis for Private Data Release. V Chandrasekaran, D Edge, S Jha, A Sharma, C Zhang, arXiv:2105.13144arXiv preprintV. Chandrasekaran, D. Edge, S. Jha, A. Sharma, C. Zhang, et al. 2021. Causally Constrained Data Synthesis for Private Data Release. arXiv preprint arXiv:2105.13144 (2021).</p>
<p>How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. A J Chaney, B M Stewart, B E Engelhardt, Proceedings of the 12th ACM conference on recommender systems. the 12th ACM conference on recommender systemsA. J. Chaney, B. M. Stewart, and B. E. Engelhardt. 2018. How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. In Proceedings of the 12th ACM conference on recommender systems. 224-232.</p>
<p>An interpretable model with globally consistent explanations for credit risk. C Chen, K Lin, C Rudin, Y Shaposhnik, S Wang, arXiv:1811.12615arXiv preprintC. Chen, K. Lin, C. Rudin, Y. Shaposhnik, S. Wang, et al. 2018. An interpretable model with globally consistent explanations for credit risk. arXiv preprint arXiv:1811.12615 (2018).</p>
<p>Human trajectory prediction via counterfactual analysis. G Chen, J Li, J Lu, J Zhou, IEEE/CVFG. Chen, J. Li, J. Lu, and J. Zhou. 2021. Human trajectory prediction via counterfactual analysis. IEEE/CVF.</p>
<p>Causalml: Python package for causal machine learning. H Chen, T Harinen, J.-Y Lee, M Yung, Z Zhao, arXiv:2002.11631arXiv preprintH. Chen, T. Harinen, J.-Y. Lee, M. Yung, and Z. Zhao. 2020. Causalml: Python package for causal machine learning. arXiv preprint arXiv:2002.11631 (2020).</p>
<p>Algorithm fairness in ai for medicine and healthcare. R J Chen, T Y Chen, J Lipkova, J J Wang, D F Williamson, arXiv:2110.00603arXiv preprintR. J. Chen, T. Y. Chen, J. Lipkova, J. J. Wang, D. F. Williamson, et al. 2021. Algorithm fairness in ai for medicine and healthcare. arXiv preprint arXiv:2110.00603 (2021).</p>
<p>BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop. M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, International Conference on Learning Representations. M. Chevalier-Boisvert, D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, et al. 2019. BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop. In International Conference on Learning Representations. https://openreview.net/forum?id=rJeXCo0cYX</p>
<p>Path-specific counterfactual fairness. S Chiappa, AAAI. 33S. Chiappa. 2019. Path-specific counterfactual fairness. In AAAI, Vol. 33. 7801-7808.</p>
<p>A causal Bayesian networks viewpoint on fairness. S Chiappa, W S Isaac, IFIP International Summer School on Privacy and Identity Management. SpringerS. Chiappa and W. S. Isaac. 2018. A causal Bayesian networks viewpoint on fairness. In IFIP International Summer School on Privacy and Identity Management. Springer, 3-20.</p>
<p>Understanding data lifetime via whole system simulation. J Chow, B Pfaff, T Garfinkel, K Christopher, M Rosenblum, USENIX Security Symposium. J. Chow, B. Pfaff, T. Garfinkel, K. Christopher, and M. Rosenblum. 2004. Understanding data lifetime via whole system simulation. In USENIX Security Symposium. 321-336.</p>
<p>Functional map of the world. G Christie, N Fendley, J Wilson, R Mukherjee, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionG. Christie, N. Fendley, J. Wilson, and R. Mukherjee. 2018. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6172-6180.</p>
<p>Sample selection bias correction theory. C Cortes, M Mohri, M Riley, A Rostamizadeh, International conference on algorithmic learning theory. SpringerC. Cortes, M. Mohri, M. Riley, and A. Rostamizadeh. 2008. Sample selection bias correction theory. In International conference on algorithmic learning theory. Springer, 38-53.</p>
<p>Causal modeling for fairness in dynamical systems. E Creager, D Madras, T Pitassi, R Zemel, ICML. PMLR. E. Creager, D. Madras, T. Pitassi, and R. Zemel. 2020. Causal modeling for fairness in dynamical systems. In ICML. PMLR, 2185-2195.</p>
<p>F Croce, M Andriushchenko, V Sehwag, E Debenedetti, N Flammarion, arXiv:2010.09670Robustbench: a standardized adversarial robustness benchmark. arXiv preprintF. Croce, M. Andriushchenko, V. Sehwag, E. Debenedetti, N. Flammarion, et al. 2020. Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670 (2020).</p>
<p>. Datacanvas, DataCanvas. 2022. YLearn. https://github.com/DataCanvasIO/YLearn.</p>
<p>Development of S-LCIA models: a review of multivariate data analysis methods. J B De Araujo, C M L Ugaya, Social LCA. 67J. B. de Araujo and C. M. L. Ugaya. 2018. Development of S-LCIA models: a review of multivariate data analysis methods. Social LCA (2018), 67.</p>
<p>Bias in bios: A case study of semantic representation bias in a high-stakes setting. M De-Arteaga, A Romanov, H Wallach, J Chayes, C Borgs, proceedings of the Conference on Fairness, Accountability, and Transparency. the Conference on Fairness, Accountability, and TransparencyM. De-Arteaga, A. Romanov, H. Wallach, J. Chayes, C. Borgs, et al. 2019. Bias in bios: A case study of semantic repre- sentation bias in a high-stakes setting. In proceedings of the Conference on Fairness, Accountability, and Transparency. 120-128.</p>
<p>A B De Luca, G Zhang, X Chen, Y Yu, arXiv:2206.099792022. Mitigating Data Heterogeneity in Federated Learning with Data Augmentation. arXiv preprintA. B. de Luca, G. Zhang, X. Chen, and Y. Yu. 2022. Mitigating Data Heterogeneity in Federated Learning with Data Augmentation. arXiv preprint arXiv:2206.09979 (2022).</p>
<p>ImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, 10.1109/CVPR.2009.52068482009 IEEE Conference on Computer Vision and Pattern Recognition. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, et al. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition. 248-255. https://doi.org/10.1109/CVPR.2009.5206848</p>
<p>UCI machine learning repository. D Dheeru, E K Taniskidou, D. Dheeru and E. K. Taniskidou. 2017. UCI machine learning repository. (2017).</p>
<p>On the adversarial robustness of causal algorithmic recourse. R Dominguez-Olmedo, A H Karimi, B Schlkopf, PMLRInternational Conference on Machine Learning. R. Dominguez-Olmedo, A. H. Karimi, and B. Schlkopf. 2022. On the adversarial robustness of causal algorithmic recourse. In International Conference on Machine Learning. PMLR, 5324-5342.</p>
<p>Our data, ourselves: Privacy via distributed noise generation. C Dwork, K Kenthapadi, F Mcsherry, I Mironov, M Naor, Annual international conference on the theory and applications of cryptographic techniques. SpringerC. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. 2006. Our data, ourselves: Privacy via distributed noise generation. In Annual international conference on the theory and applications of cryptographic techniques. Springer, 486-503.</p>
<p>Fairness is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies. A Amour, H Srinivasan, J Atwood, P Baljekar, D Sculley, 10.1145/3351095.3372878Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAccT '20). the 2020 Conference on Fairness, Accountability, and Transparency (FAccT '20)New York, NY, USAAssociation for Computing MachineryA. D'Amour, H. Srinivasan, J. Atwood, P. Baljekar, D. Sculley, et al. 2020. Fairness is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAccT '20). Association for Computing Machinery, New York, NY, USA, 525-534. https://doi.org/10. 1145/3351095.3372878</p>
<p>Amnesic Probing: Behavioral Explanation With Amnesic Counterfactuals. Y Elazar, S Ravfogel, A Jacovi, Y Goldberg, Trans. Assoc. Comput. Linguistics. Y. Elazar, S. Ravfogel, A. Jacovi, and Y. Goldberg. 2021. Amnesic Probing: Behavioral Explanation With Amnesic Counterfactuals. Trans. Assoc. Comput. Linguistics (2021).</p>
<p>T-rex: A large scale alignment of natural language with knowledge base triples. H Elsahar, P Vougiouklis, A Remaci, C Gravier, J Hare, Proceedings of the Eleventh International Conference on Language Resources and Evaluation. the Eleventh International Conference on Language Resources and EvaluationLRECH. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, et al. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>. Science Direct. ElsevierElsevier. 2023. Science Direct. Retrieved February 5, 2023 from https://www.sciencedirect.com/</p>
<p>Depth and appearance for mobile scene analysis. A Ess, B Leibe, L Van Gool, IEEE 11th international conference on computer vision. IEEEA. Ess, B. Leibe, and L. Van Gool. 2007. Depth and appearance for mobile scene analysis. In 2007 IEEE 11th international conference on computer vision. IEEE, 1-8.</p>
<p>Event labeling combining ensemble detectors and background knowledge. H Fanaee-T, J Gama, Progress in Artificial Intelligence. 2H. Fanaee-T and J. Gama. 2014. Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence 2, 2 (2014), 113-127.</p>
<p>CausaLM: Causal Model Explanation Through Counterfactual Language Models. A Feder, N Oved, U Shalit, R Reichart, Comput. Linguistics. A. Feder, N. Oved, U. Shalit, and R. Reichart. 2021. CausaLM: Causal Model Explanation Through Counterfactual Language Models. Comput. Linguistics (2021).</p>
<p>Towards causal federated learning for enhanced robustness and privacy. S Francis, I Tenison, I Rish, arXiv:2104.06557arXiv preprintS. Francis, I. Tenison, and I. Rish. 2021. Towards causal federated learning for enhanced robustness and privacy. arXiv preprint arXiv:2104.06557 (2021).</p>
<p>Causal inference methods for small non-randomized studies: Methods and an application in COVID-19. S Friedrich, T Friede, Contemporary Clinical Trials. 99S. Friedrich and T. Friede. 2020. Causal inference methods for small non-randomized studies: Methods and an application in COVID-19. Contemporary Clinical Trials 99 (2020).</p>
<p>Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability. C Frye, C Rowat, I Feige, NeurIPS. C. Frye, C. Rowat, and I. Feige. 2020. Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability. In NeurIPS.</p>
<p>Causal Feature Selection for Algorithmic Fairness. S Galhotra, K Shanmugam, P Sattigeri, K R Varshney, R Bellamy, S. Galhotra, K. Shanmugam, P. Sattigeri, K. R. Varshney, R. Bellamy, et al. 2022. Causal Feature Selection for Algorithmic Fairness. (2022).</p>
<p>C Gao, Y Zheng, W Wang, F Feng, X He, arXiv:2208.12397Causal Inference in Recommender Systems: A Survey and Future Directions. arXiv preprintC. Gao, Y. Zheng, W. Wang, F. Feng, X. He, et al. 2022. Causal Inference in Recommender Systems: A Survey and Future Directions. arXiv preprint arXiv:2208.12397 (2022).</p>
<p>E Gao, J Chen, L Shen, T Liu, M Gong, arXiv:2112.03555Federated causal discovery. arXiv preprintE. Gao, J. Chen, L. Shen, T. Liu, M. Gong, et al. 2021. Federated causal discovery. arXiv preprint arXiv:2112.03555 (2021).</p>
<p>A long-term analysis of polarization on Twitter. V R K Garimella, I Weber, Eleventh international AAAI conference on web and social media. V. R. K. Garimella and I. Weber. 2017. A long-term analysis of polarization on Twitter. In Eleventh international AAAI conference on web and social media.</p>
<p>Probabilistic and Causal Inference: The Works of Judea Pearl. H Geffner, R Dechter, J Halpern, ACM BooksH. Geffner, R. Dechter, and J. Halpern (Eds.). 2022. Probabilistic and Causal Inference: The Works of Judea Pearl. ACM Books.</p>
<p>All Lending Club loan data. N George, N. George. 2018. All Lending Club loan data. Retrieved February 6, 2023 from https://www.kaggle.com/datasets/ wordsforthewise/lending-club</p>
<p>Domain generalization for object recognition with multi-task autoencoders. M Ghifary, W B Kleijn, M Zhang, D Balduzzi, M. Ghifary, W. B. Kleijn, M. Zhang, and D. Balduzzi. 2015. Domain generalization for object recognition with multi-task autoencoders. In ICCV. 2551-2559.</p>
<p>The impact of robotic-assisted surgery on team performance: a systematic mixed studies review. B M Gillespie, J Gillespie, R J Boorman, K Granqvist, J Stranne, Human factors. 63B. M. Gillespie, J. Gillespie, R. J. Boorman, K. Granqvist, J. Stranne, et al. 2021. The impact of robotic-assisted surgery on team performance: a systematic mixed studies review. Human factors 63, 8 (2021), 1352-1379.</p>
<p>A SARS-CoV-2 protein interaction map reveals targets for drug repurposing. D E Gordon, G M Jang, M Bouhaddou, J Xu, K Obernier, Nature. 583D. E. Gordon, G. M. Jang, M. Bouhaddou, J. Xu, K. Obernier, et al. 2020. A SARS-CoV-2 protein interaction map reveals targets for drug repurposing. Nature 583, 7816 (2020), 459-468.</p>
<p>. Canada Government Of, n. d.Government of Canada. [n. d.].</p>
<p>Algorithmic Impact Assessment tool. Retrieved. Algorithmic Impact Assessment tool. Retrieved November 30, 2022 from https: //open.canada.ca/aia-eia-js/?lang=en</p>
<p>. A Goyal, A Lamb, J Hoffmann, S Sodhani, S Levine, Recurrent Independent Mechanisms. ICLR. A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, et al. 2021. Recurrent Independent Mechanisms. ICLR.</p>
<p>Cardiovascular, Bleeding, and Mortality Risks in Elderly Medicare Patients Treated With Dabigatran or Warfarin for Nonvalvular Atrial Fibrillation. D J Graham, M E Reichman, M Wernecke, R Zhang, M R Southworth, Circulation. 131D. J. Graham, M. E. Reichman, M. Wernecke, R. Zhang, M. R. Southworth, et al. 2015. Cardiovascular, Bleeding, and Mortality Risks in Elderly Medicare Patients Treated With Dabigatran or Warfarin for Nonvalvular Atrial Fibrillation. Circulation 131, 2 (2015).</p>
<p>South German credit data: Correcting a widely used data set. U Groemping, Rep. Math., Phys. Chem. 4Tech. RepU. Groemping. 2019. South German credit data: Correcting a widely used data set. Rep. Math., Phys. Chem., Berlin, Germany, Tech. Rep 4 (2019), 2019.</p>
<p>A survey of methods for explaining black box models. R Guidotti, A Monreale, S Ruggieri, F Turini, F Giannotti, ACM computing surveys (CSUR). 51R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, et al. 2018. A survey of methods for explaining black box models. ACM computing surveys (CSUR) 51, 5 (2018), 1-42.</p>
<p>I Gulrajani, D Lopez-Paz, arXiv:2007.01434search of lost domain generalization. arXiv preprintI. Gulrajani and D. Lopez-Paz. 2020. In search of lost domain generalization. arXiv preprint arXiv:2007.01434 (2020).</p>
<p>. R Guo, L Cheng, J Li, P R Hahn, H Liu, 10.1145/3397269A Survey of Learning Causality with Data: Problems and Methods. ACM Comput. Surv. 53R. Guo, L. Cheng, J. Li, P. R. Hahn, and H. Liu. 2020. A Survey of Learning Causality with Data: Problems and Methods. ACM Comput. Surv. 53, 4 (2020), 75:1-75:37. https://doi.org/10.1145/3397269</p>
<p>A survey of learning causality with data: Problems and methods. R Guo, L Cheng, J Li, P R Hahn, H Liu, ACM Computing Surveys (CSUR). 53R. Guo, L. Cheng, J. Li, P. R. Hahn, and H. Liu. 2020. A survey of learning causality with data: Problems and methods. ACM Computing Surveys (CSUR) 53, 4 (2020), 1-37.</p>
<p>S Gupta, K Ahuja, M Havaei, N Chatterjee, Y Bengio, arXiv:2205.11101FL Games: A federated learning framework for distribution shifts. arXiv preprintS. Gupta, K. Ahuja, M. Havaei, N. Chatterjee, and Y. Bengio. 2022. FL Games: A federated learning framework for distribution shifts. arXiv preprint arXiv:2205.11101 (2022).</p>
<p>Causal feature selection. In Computational methods of feature selection. I Guyon, C Aliferis, I. Guyon, C. Aliferis, et al. 2007. Causal feature selection. In Computational methods of feature selection.</p>
<p>The statistical implications of a system of simultaneous equations. T Haavelmo, Journal of the Econometric Society. EconometricaT. Haavelmo. 1943. The statistical implications of a system of simultaneous equations. Econometrica, Journal of the Econometric Society (1943), 1-12.</p>
<p>The MovieLens Datasets: History and Context. F M Harper, J A Konstan, 10.1145/2827872ACM Trans. Interact. Intell. Syst. 195, 4, Article 19F. M. Harper and J. A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (dec 2015), 19 pages. https://doi.org/10.1145/2827872</p>
<p>Economic impact of artificial intelligence: new look for the macroeconomic assessment in Asia-Pacific region. M Haseeb, L W Mihardjo, A R Gill, K Jermsittiparsert, International Journal of Computational Intelligence Systems. 121295M. Haseeb, L. W. Mihardjo, A. R. Gill, K. Jermsittiparsert, et al. 2019. Economic impact of artificial intelligence: new look for the macroeconomic assessment in Asia-Pacific region. International Journal of Computational Intelligence Systems 12, 2 (2019), 1295.</p>
<p>C He, K Balasubramanian, E Ceyani, C Yang, H Xie, arXiv:2104.07145Fedgraphnn: A federated learning system and benchmark for graph neural networks. arXiv preprintC. He, K. Balasubramanian, E. Ceyani, C. Yang, H. Xie, et al. 2021. Fedgraphnn: A federated learning system and benchmark for graph neural networks. arXiv preprint arXiv:2104.07145 (2021).</p>
<p>The many faces of robustness: A critical analysis of out-of-distribution generalization. D Hendrycks, S Basart, N Mu, S Kadavath, F Wang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionD. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, et al. 2021. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 8340-8349.</p>
<p>Benchmarking neural network robustness to common corruptions and perturbations. D Hendrycks, T Dietterich, arXiv:1903.12261arXiv preprintD. Hendrycks and T. Dietterich. 2019. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261 (2019).</p>
<p>Measuring the Quality of Explanations: The System Causability Scale (SCS): Comparing Human and Machine Explanations. A Holzinger, A Carrington, H Mller, KI -Kunstliche IntelligenzA. Holzinger, A. Carrington, and H. Mller. 2020. Measuring the Quality of Explanations: The System Causability Scale (SCS): Comparing Human and Machine Explanations. KI -Kunstliche Intelligenz (2020).</p>
<p>Causability and explainability of artificial intelligence in medicine. A Holzinger, G Langs, H Denk, K Zatloukal, H Mller, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 9A. Holzinger, G. Langs, H. Denk, K. Zatloukal, and H. Mller. 2019. Causability and explainability of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 9 (2019).</p>
<p>A Benchmark for Interpretability Methods in Deep Neural Networks. S Hooker, D Erhan, P.-J Kindermans, B Kim, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, et al.Curran Associates, Inc32S. Hooker, D. Erhan, P.-J. Kindermans, and B. Kim. 2019. A Benchmark for Interpretability Methods in Deep Neural Networks. In Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, et al. (Eds.). Curran Associates, Inc., 9737-9748. http://papers.nips.cc/paper/9167-a-benchmark- for-interpretability-methods-in-deep-neural-networks.pdf</p>
<p>The oarf benchmark suite: Characterization and implications for federated learning systems. S Hu, Y Li, X Liu, Q Li, Z Wu, ACM Transactions on Intelligent Systems and Technology (TIST). 13S. Hu, Y. Li, X. Liu, Q. Li, Z. Wu, et al. 2022. The oarf benchmark suite: Characterization and implications for federated learning systems. ACM Transactions on Intelligent Systems and Technology (TIST) 13, 4 (2022), 1-32.</p>
<p>Fairness through equality of effort. W Huan, Y Wu, L Zhang, X Wu, Companion Proceedings of the Web Conference 2020. W. Huan, Y. Wu, L. Zhang, and X. Wu. 2020. Fairness through equality of effort. In Companion Proceedings of the Web Conference 2020. 743-751.</p>
<p>Causality-based accountability mechanisms for socio-technical systems. A Ibrahim, S Kyriakopoulos, A Pretschner, Journal of Responsible Technology. 7100016A. Ibrahim, S. Kyriakopoulos, and A. Pretschner. 2021. Causality-based accountability mechanisms for socio-technical systems. Journal of Responsible Technology 7 (2021), 100016.</p>
<p>Selecting data augmentation for simulating interventions. M Ilse, J M Tomczak, P Forr, PMLRInternational Conference on Machine Learning. M. Ilse, J. M. Tomczak, and P. Forr. 2021. Selecting data augmentation for simulating interventions. In International Conference on Machine Learning. PMLR, 4555-4562.</p>
<p>Datamodels: Understanding Predictions with Data and Data with Predictions. A Ilyas, S M Park, L Engstrom, G Leclerc, A Madry, PMLRInternational Conference on Machine Learning. A. Ilyas, S. M. Park, L. Engstrom, G. Leclerc, and A. Madry. 2022. Datamodels: Understanding Predictions with Data and Data with Predictions. In International Conference on Machine Learning. PMLR, 9525-9587.</p>
<p>Batch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, PMLRInternational conference on machine learning. S. Ioffe and C. Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning. PMLR, 448-456.</p>
<p>AI safety needs social scientists. G Irving, A , Distill. 414G. Irving and A. Askell. 2019. AI safety needs social scientists. Distill 4, 2 (2019), e14.</p>
<p>Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI. A Jacovi, A Marasovic, T Miller, Y Goldberg, 10.1145/3442188.3445923FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event. Madeleine Clare Elish, William Isaac, and Richard S. ZemelToronto, CanadaACMA. Jacovi, A. Marasovic, T. Miller, and Y. Goldberg. 2021. Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI. In FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Trans- parency, Virtual Event / Toronto, Canada, March 3-10, 2021, Madeleine Clare Elish, William Isaac, and Richard S. Zemel (Eds.). ACM, 624-635. https://doi.org/10.1145/3442188.3445923</p>
<p>Feature relevance quantification in explainable AI: A causal problem. D Janzing, L Minorics, P Blbaum, AISTATS. D. Janzing, L. Minorics, and P. Blbaum. 2020. Feature relevance quantification in explainable AI: A causal problem. AISTATS (2020).</p>
<p>S Jeoung, J Diesner, What Changed? Investigating Debiasing Methods using Causal Mediation Analysis. CoRR. S. Jeoung and J. Diesner. 2022. What Changed? Investigating Debiasing Methods using Causal Mediation Analysis. CoRR (2022).</p>
<p>A taxonomy of attacks on federated learning. M S Jere, T Farnan, F Koushanfar, IEEE Security &amp; Privacy. 19M. S. Jere, T. Farnan, and F. Koushanfar. 2020. A taxonomy of attacks on federated learning. IEEE Security &amp; Privacy 19, 2 (2020), 20-28.</p>
<p>M Jiang, X Zhang, M Kamp, X Li, Q Dou, arXiv:2110.099742021. TsmoBN: Interventional Generalization for Unseen Clients in Federated Learning. arXiv preprintM. Jiang, X. Zhang, M. Kamp, X. Li, and Q. Dou. 2021. TsmoBN: Interventional Generalization for Unseen Clients in Federated Learning. arXiv preprint arXiv:2110.09974 (2021).</p>
<p>Jigsaw Unintended Bias in Toxicity Classification. Jigsaw, RetrievedJigsaw. 2019. Jigsaw Unintended Bias in Toxicity Classification. Retrieved November 23, 2022 from https://www. kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification</p>
<p>MIMIC-III, a freely accessible critical care database. A E Johnson, T J Pollard, L Shen, L , . H Lehman, M Feng, Scientific data. 3A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, et al. 2016. MIMIC-III, a freely accessible critical care database. Scientific data 3, 1 (2016), 1-9.</p>
<p>J Kaddour, A Lynch, Q Liu, M J Kusner, R Silva, arXiv:2206.15475Causal Machine Learning: A Survey and Open Problems. arXiv preprintJ. Kaddour, A. Lynch, Q. Liu, M. J. Kusner, and R. Silva. 2022. Causal Machine Learning: A Survey and Open Problems. arXiv preprint arXiv:2206.15475 (2022).</p>
<p>. D Kaur, S Uslu, K J Rittichier, A Durresi, 10.1145/3491209Trustworthy Artificial Intelligence: A Review. ACM Comput. Surv. 55D. Kaur, S. Uslu, K. J. Rittichier, and A. Durresi. 2023. Trustworthy Artificial Intelligence: A Review. ACM Comput. Surv. 55, 2 (2023), 39:1-39:38. https://doi.org/10.1145/3491209</p>
<p>Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization. J N Kaur, E Kiciman, A Sharma, arXiv:2206.07837arXiv preprintJ. N. Kaur, E. Kiciman, and A. Sharma. 2022. Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization. arXiv preprint arXiv:2206.07837 (2022).</p>
<p>Learning the difference that makes a difference with counterfactuallyaugmented data. D Kaushik, E Hovy, Z C Lipton, arXiv:1909.12434arXiv preprintD. Kaushik, E. Hovy, and Z. C. Lipton. 2019. Learning the difference that makes a difference with counterfactually- augmented data. arXiv preprint arXiv:1909.12434 (2019).</p>
<p>Explaining the efficacy of counterfactually augmented data. D Kaushik, A Setlur, E Hovy, Z C Lipton, arXiv:2010.02114arXiv preprintD. Kaushik, A. Setlur, E. Hovy, and Z. C. Lipton. 2020. Explaining the efficacy of counterfactually augmented data. arXiv preprint arXiv:2010.02114 (2020).</p>
<p>Systematic evaluation of causal discovery in visual model based reinforcement learning. N R Ke, A Didolkar, S Mittal, A Goyal, G Lajoie, arXiv:2107.00848arXiv preprintN. R. Ke, A. Didolkar, S. Mittal, A. Goyal, G. Lajoie, et al. 2021. Systematic evaluation of causal discovery in visual model based reinforcement learning. arXiv preprint arXiv:2107.00848 (2021).</p>
<p>EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation. M H G L P O M O V S Keith Battocchi, Eleanor Dillon, M. H. G. L. P. O. M. O. V. S. Keith Battocchi, Eleanor Dillon. 2019. EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation. https://github.com/microsoft/EconML. Version 0.x.</p>
<p>Avoiding discrimination through causal reasoning. Advances in neural information processing systems. N Kilbertus, M Rojas Carulla, G Parascandolo, M Hardt, D Janzing, 30N. Kilbertus, M. Rojas Carulla, G. Parascandolo, M. Hardt, D. Janzing, et al. 2017. Avoiding discrimination through causal reasoning. Advances in neural information processing systems 30 (2017).</p>
<p>Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention. J Kim, J Canny, J. Kim and J. Canny. 2017. Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention. ICCV.</p>
<p>Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. R Kohavi, Kdd. 96R. Kohavi et al. 1996. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.. In Kdd, Vol. 96. 202-207.</p>
<p>Learning multiple layers of features from tiny images. A Krizhevsky, G Hinton, A. Krizhevsky, G. Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009).</p>
<p>D Krueger, T Maharaj, J Kramr, M Pezeshki, N Ballas, arXiv:1606.01305Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv preprintD. Krueger, T. Maharaj, J. Kramr, M. Pezeshki, N. Ballas, et al. 2016. Zoneout: Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305 (2016).</p>
<p>Making decisions that reduce discriminatory impacts. M Kusner, C Russell, J Loftus, R Silva, PMLRInternational Conference on Machine Learning. M. Kusner, C. Russell, J. Loftus, and R. Silva. 2019. Making decisions that reduce discriminatory impacts. In International Conference on Machine Learning. PMLR, 3591-3600.</p>
<p>Counterfactual fairness. Advances in neural information processing systems. M J Kusner, J Loftus, C Russell, R Silva, 30M. J. Kusner, J. Loftus, C. Russell, and R. Silva. 2017. Counterfactual fairness. Advances in neural information processing systems 30 (2017).</p>
<p>Improving model robustness using causal knowledge. T Kyono, M Van Der Schaar, arXiv:1911.12441arXiv preprintT. Kyono and M. van der Schaar. 2019. Improving model robustness using causal knowledge. arXiv preprint arXiv:1911.12441 (2019).</p>
<p>Probing for the Usage of Grammatical Number. K Lasri, T Pimentel, A Lenci, T Poibeau, R Cotterell, ACL. K. Lasri, T. Pimentel, A. Lenci, T. Poibeau, and R. Cotterell. 2022. Probing for the Usage of Grammatical Number. In ACL.</p>
<ol>
<li>A survey on datasets for fairness-aware machine learning. T Le Quy, A Roy, V Iosifidis, W Zhang, E Ntoutsi, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 1452T. Le Quy, A. Roy, V. Iosifidis, W. Zhang, and E. Ntoutsi. 2022. A survey on datasets for fairness-aware machine learning. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery (2022), e1452.</li>
</ol>
<p>Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proc. IEEE. 86Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278-2324.</p>
<p>Crowds by example. A Lerner, Y Chrysanthou, D Lischinski, Computer graphics forum. Wiley Online Library26A. Lerner, Y. Chrysanthou, and D. Lischinski. 2007. Crowds by example. In Computer graphics forum, Vol. 26. Wiley Online Library, 655-664.</p>
<p>Deeper, broader and artier domain generalization. D Li, Y Yang, Y.-Z Song, T M Hospedales, ICCV. D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. 2017. Deeper, broader and artier domain generalization. In ICCV. 5542-5550.</p>
<p>Reference component analysis of single-cell transcriptomes elucidates cellular heterogeneity in human colorectal tumors. H Li, E T Courtois, D Sengupta, Y Tan, K H Chen, Nature genetics. 49H. Li, E. T. Courtois, D. Sengupta, Y. Tan, K. H. Chen, et al. 2017. Reference component analysis of single-cell transcriptomes elucidates cellular heterogeneity in human colorectal tumors. Nature genetics 49, 5 (2017), 708-718.</p>
<p>L Li, X Qi, T Xie, B Li, arXiv:2009.041312020. Sok: Certified robustness for deep neural networks. arXiv preprintL. Li, X. Qi, T. Xie, and B. Li. 2020. Sok: Certified robustness for deep neural networks. arXiv preprint arXiv:2009.04131 (2020).</p>
<p>How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis. S Li, X Li, L Shang, Z Dong, C Sun, ACL. S. Li, X. Li, L. Shang, Z. Dong, C. Sun, et al. 2022. How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis. In ACL.</p>
<p>Towards personalized fairness based on causal notion. Y Li, H Chen, S Xu, Y Ge, Y Zhang, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalY. Li, H. Chen, S. Xu, Y. Ge, and Y. Zhang. 2021. Towards personalized fairness based on causal notion. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1054-1063.</p>
<p>Microsoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, European conference on computer vision. SpringerT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, et al. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer, 740-755.</p>
<p>Explainable ai: A review of machine learning interpretability methods. P Linardatos, V Papastefanopoulos, S Kotsiantis, Entropy. 2318P. Linardatos, V. Papastefanopoulos, and S. Kotsiantis. 2020. Explainable ai: A review of machine learning inter- pretability methods. Entropy 23, 1 (2020), 18.</p>
<p>Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles. P Lison, J Tiedemann, P. Lison and J. Tiedemann. 2016. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles. (2016).</p>
<p>. M A Little, R Badawy, arXiv:1910.09648Causal bootstrapping. arXiv preprintM. A. Little and R. Badawy. 2019. Causal bootstrapping. arXiv preprint arXiv:1910.09648 (2019).</p>
<p>Towards Disentangling Latent Space for Unsupervised Semantic Face Editing. K Liu, G Cao, F Zhou, B Liu, J Duan, IEEE Trans. Image Process. K. Liu, G. Cao, F. Zhou, B. Liu, J. Duan, et al. 2022. Towards Disentangling Latent Space for Unsupervised Semantic Face Editing. IEEE Trans. Image Process. (2022).</p>
<p>Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. Y Liu, R Cadei, J Schweizer, S Bahmani, A Alahi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionY. Liu, R. Cadei, J. Schweizer, S. Bahmani, and A. Alahi. 2022. Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 17081-17092.</p>
<p>ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models. Y Liu, R Wen, X He, A Salem, Z Zhang, 31st USENIX Security Symposium (USENIX Security 22). USENIX Association. Boston, MAY. Liu, R. Wen, X. He, A. Salem, Z. Zhang, et al. 2022. ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models. In 31st USENIX Security Symposium (USENIX Security 22). USENIX Association, Boston, MA, 4525-4542. https://www.usenix.org/conference/usenixsecurity22/presentation/liu-yugeng</p>
<p>Towards a Timely Causality Analysis for Enterprise Security. Y Liu, M Zhang, D Li, K Jee, Z Li, NDSS. Y. Liu, M. Zhang, D. Li, K. Jee, Z. Li, et al. 2018. Towards a Timely Causality Analysis for Enterprise Security.. In NDSS.</p>
<p>Deep Learning Face Attributes in the Wild. Z Liu, P Luo, X Wang, X Tang, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)Z. Liu, P. Luo, X. Wang, and X. Tang. 2015. Deep Learning Face Attributes in the Wild. In Proceedings of International Conference on Computer Vision (ICCV).</p>
<p>Invariant Causal Representation Learning for Out-of-Distribution Generalization. C Lu, Y Wu, J M Hernndez-Lobato, B Schlkopf, International Conference on Learning Representations. C. Lu, Y. Wu, J. M. Hernndez-Lobato, and B. Schlkopf. 2021. Invariant Causal Representation Learning for Out-of- Distribution Generalization. In International Conference on Learning Representations.</p>
<p>Gender bias in neural natural language processing. K Lu, P Mardziel, F Wu, P Amancharla, A Datta, Logic, Language, and Security. SpringerK. Lu, P. Mardziel, F. Wu, P. Amancharla, and A. Datta. 2020. Gender bias in neural natural language processing. In Logic, Language, and Security. Springer, 189-202.</p>
<p>A unified approach to interpreting model predictions. Advances in neural information processing systems. S M Lundberg, S.-I Lee, 30S. M. Lundberg and S.-I. Lee. 2017. A unified approach to interpreting model predictions. Advances in neural information processing systems 30 (2017).</p>
<p>Causal Inference: What If. H Ma, R Jm , Chapman &amp; Hall/CRCBoca RatonH. MA and R. JM. 2020. Causal Inference: What If. Boca Raton: Chapman &amp; Hall/CRC.</p>
<p>A conditional linear Gaussian network to assess the impact of several agronomic settings on the quality of Tuscan Sangiovese grapes. A Magrini, S Di Blasi, F M Stefanini, Biometrical Letters. 54A. Magrini, S. Di Blasi, F. M. Stefanini, et al. 2017. A conditional linear Gaussian network to assess the impact of several agronomic settings on the quality of Tuscan Sangiovese grapes. Biometrical Letters 54, 1 (2017), 25-42.</p>
<p>Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers. D Mahajan, C Tan, A Sharma, D. Mahajan, C. Tan, and A. Sharma. 2019. Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers. (12 2019). http://arxiv.org/abs/1912.03277</p>
<p>Survey on causal-based machine learning fairness notions. K Makhlouf, S Zhioua, C Palamidessi, arXiv:2010.09553arXiv preprintK. Makhlouf, S. Zhioua, and C. Palamidessi. 2020. Survey on causal-based machine learning fairness notions. arXiv preprint arXiv:2010.09553 (2020).</p>
<p>Causal discovery from medical textual data. S Mani, G F Cooper, AMIA Symp. S. Mani and G. F. Cooper. 2000. Causal discovery from medical textual data. AMIA Symp. (2000).</p>
<p>Generative interventions for causal learning. C Mao, A Cha, A Gupta, H Wang, J Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionC. Mao, A. Cha, A. Gupta, H. Wang, J. Yang, et al. 2021. Generative interventions for causal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3947-3956.</p>
<p>Environmental impact assessment: theory and practice. A R D Mareddy, A Shah, N Davergave, Butterworth-HeinemannA. R. D. Mareddy, A. Shah, and N. Davergave. 2017. Environmental impact assessment: theory and practice. Butterworth- Heinemann.</p>
<p>Universal dependency annotation for multilingual parsing. R Mcdonald, J Nivre, Y Quirmbach-Brundage, Y Goldberg, D Das, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsShort Papers2R. McDonald, J. Nivre, Y. Quirmbach-Brundage, Y. Goldberg, D. Das, et al. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 92-97.</p>
<p>Communication-efficient learning of deep networks from decentralized data. B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, PMLRArtificial intelligence and statistics. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics. PMLR, 1273-1282.</p>
<ol>
<li>A survey on bias and fairness in machine learning. N Mehrabi, F Morstatter, N Saxena, K Lerman, A Galstyan, ACM Computing Surveys (CSUR). 54N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan. 2021. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR) 54, 6 (2021), 1-35.</li>
</ol>
<p>Pointer sentinel mixture models. S Merity, C Xiong, J Bradbury, R Socher, arXiv:1609.07843arXiv preprintS. Merity, C. Xiong, J. Bradbury, and R. Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 (2016).</p>
<p>Chocolate consumption, cognitive function, and nobel laureates. F H Messerli, The New England Journal of Medicine. 367F. H. Messerli. 2012. Chocolate consumption, cognitive function, and nobel laureates. The New England Journal of Medicine 367, 10 (2012), 1562-1564.</p>
<p>Microsoft. 2022. Responsible AI Toolbox. Microsoft. 2022. Responsible AI Toolbox. https://github.com/microsoft/responsible-ai-toolbox. Version 0.23.</p>
<p>The TAILOR Strategic Research and Innovation Roadmap. M Milano, M Schoenauer, M. Milano and M. Schoenauer. 2022. The TAILOR Strategic Research and Innovation Roadmap. Available at https: //tailor-network.eu/research-overview/strategic-research-and-innovation-roadmap/.</p>
<p>. J Miller, C Hsu, J Troutman, J Perdomo, T Zrnic, 10.5281/zenodo.3875775J. Miller, C. Hsu, J. Troutman, J. Perdomo, T. Zrnic, et al. 2020. WhyNot. https://doi.org/10.5281/zenodo.3875775</p>
<p>Fairness in risk assessment instruments: Post-processing to achieve counterfactual equalized odds. A Mishler, E H Kennedy, A Chouldechova, A. Mishler, E. H. Kennedy, and A. Chouldechova. 2021. Fairness in risk assessment instruments: Post-processing to achieve counterfactual equalized odds. In FAccT. 386-400.</p>
<p>Representation learning via invariant causal mechanisms. J Mitrovic, B Mcwilliams, J Walker, L Buesing, C Blundell, arXiv:2010.07922arXiv preprintJ. Mitrovic, B. McWilliams, J. Walker, L. Buesing, and C. Blundell. 2020. Representation learning via invariant causal mechanisms. arXiv preprint arXiv:2010.07922 (2020).</p>
<p>Distinguishing cause from effect using observational data: methods and benchmarks. J M Mooij, J Peters, D Janzing, J Zscheischler, B Schlkopf, The Journal of Machine Learning Research. 17J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Schlkopf. 2016. Distinguishing cause from effect using observational data: methods and benchmarks. The Journal of Machine Learning Research 17, 1 (2016), 1103-1204.</p>
<p>Causal interpretability for machine learning-problems, methods and evaluation. R Moraffah, M Karami, R Guo, A Raglin, H Liu, ACM SIGKDD Explorations Newsletter. 22R. Moraffah, M. Karami, R. Guo, A. Raglin, and H. Liu. 2020. Causal interpretability for machine learning-problems, methods and evaluation. ACM SIGKDD Explorations Newsletter 22, 1 (2020), 18-33.</p>
<p>Governing with algorithmic impact assessments: six observations. E Moss, E A Watkins, J Metcalf, M C Elish, ; Emanuel, Jacob Metcalf, Ranjit Singh, Madeleine Elish, Clare, Governing Algorithmic Systems with Impact Assessments: Six Observations. Watkins, Elizabeth and MossAIESAAAI/ACM Conference on Artificial Intelligence, Ethics, and SocietyE. Moss, E. A. Watkins, J. Metcalf, and M. C. Elish. 2020. Governing with algorithmic impact assessments: six observations. In Watkins, Elizabeth and Moss, Emanuel and Metcalf, Jacob and Singh, Ranjit and Elish, Madeleine Clare, Governing Algorithmic Systems with Impact Assessments: Six Observations (May 14, 2021). AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES).</p>
<p>Learning optimal fair policies. R Nabi, D Malinsky, I Shpitser, ICML. PMLR. R. Nabi, D. Malinsky, and I. Shpitser. 2019. Learning optimal fair policies. In ICML. PMLR, 4674-4682.</p>
<p>Fair inference on outcomes. R Nabi, I Shpitser, AAAI. 32R. Nabi and I. Shpitser. 2018. Fair inference on outcomes. In AAAI, Vol. 32.</p>
<p>CrowS-pairs: A challenge dataset for measuring social biases in masked language models. N Nangia, C Vania, R Bhalerao, S R Bowman, arXiv:2010.00133arXiv preprintN. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. arXiv preprint arXiv:2010.00133 (2020).</p>
<p>Comprehensive privacy analysis of deep learning. M Nasr, R Shokri, A Houmansadr, Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP. the 2019 IEEE Symposium on Security and Privacy (SPM. Nasr, R. Shokri, and A. Houmansadr. 2018. Comprehensive privacy analysis of deep learning. In Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP). 1-15.</p>
<p>Machine learning with membership privacy using adversarial regularization. M Nasr, R Shokri, A Houmansadr, Proceedings of the 2018 ACM SIGSAC conference on computer and communications security. the 2018 ACM SIGSAC conference on computer and communications securityM. Nasr, R. Shokri, and A. Houmansadr. 2018. Machine learning with membership privacy using adversarial regularization. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security. 634-646.</p>
<p>. National Center for Biotechnology Information. 2022. PubMed. National Center for Biotechnology Information. 2022. PubMed. Retrieved December 30, 2022 from https://pubmed. ncbi.nlm.nih.gov/</p>
<p>Justifying recommendations using distantly-labeled reviews and fine-grained aspects. J Ni, J Li, J Mcauley, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing. the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processingJ. Ni, J. Li, and J. McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 188-197.</p>
<p>. M.-I Nicolae, M Sinn, M N Tran, B Buesser, A Rawat, v1.2.0. CoRR 1807.01069Adversarial Robustness ToolboxM.-I. Nicolae, M. Sinn, M. N. Tran, B. Buesser, A. Rawat, et al. 2018. Adversarial Robustness Toolbox v1.2.0. CoRR 1807.01069 (2018). https://arxiv.org/pdf/1807.01069</p>
<p>. K Niemel, K. Niemel et al. 2016. Awesome Health. https://github.com/kakoni/awesome-healthcare.</p>
<p>Face identity disentanglement via latent space mapping. Y Nitzan, A Bermano, Y Li, D Cohen-Or, ACM Transactions on Graphics. Y. Nitzan, A. Bermano, Y. Li, and D. Cohen-Or. 2020. Face identity disentanglement via latent space mapping. ACM Transactions on Graphics (2020).</p>
<p>H Nori, S Jenkins, P Koch, R Caruana, arXiv:1909.09223InterpretML: A Unified Framework for Machine Learning Interpretability. arXiv preprintH. Nori, S. Jenkins, P. Koch, and R. Caruana. 2019. InterpretML: A Unified Framework for Machine Learning Interpretability. arXiv preprint arXiv:1909.09223 (2019).</p>
<p>10.1787/008232ec-enOECD. 2021. Tools for trustworthy AI. 312OECD. 2021. Tools for trustworthy AI. 312 (2021). https://doi.org/https://doi.org/10.1787/008232ec-en</p>
<p>Report to the congress on credit scoring and its effects on the availability and affordability of credit. B Of, Governors of the Federal Reserve System (US). Board of Governors of the Federal Reserve SystemB. of Governors of the Federal Reserve System (US). 2007. Report to the congress on credit scoring and its effects on the availability and affordability of credit. Board of Governors of the Federal Reserve System.</p>
<p>Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities. U G A Office, GAO-21-519SP Reportn. d.U. G. A. Office. [n. d.]. Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities. GAO-21-519SP Report. https://www.gao.gov/products/GAO-21-519SP</p>
<p>on Artificial Intelligence. E C I , H.-L E , Ethics Guidelines for Trustworthy AI. E. C. I. H.-L. E. G. on Artificial Intelligence. 2019. Ethics Guidelines for Trustworthy AI. Retrieved August 10, 2021 from https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60419</p>
<p>Powerlifting Database. Openpowerlifting, OpenPowerlifting. 2019. Powerlifting Database. Retrieved December 28, 2022 from https://www.kaggle.com/datasets/ open-powerlifting/powerlifting-database</p>
<p>OECD Statistics. Retrieved February. Organisation for Economic Co-operation and Development6Organisation for Economic Co-operation and Development. 2023. OECD Statistics. Retrieved February 6, 2023 from https://stats.oecd.org/</p>
<p>A Propensity Score Analysis of Chemotherapy Use in Patients With Resectable Gallbladder Cancer. M Ozer, S Y Goksu, N N Sanford, M Porembka, H Khurshid, JAMA Network Open. 5M. Ozer, S. Y. Goksu, N. N. Sanford, M. Porembka, H. Khurshid, et al. 2022. A Propensity Score Analysis of Chemother- apy Use in Patients With Resectable Gallbladder Cancer. JAMA Network Open 5, 2 (2022).</p>
<p>Explaining algorithmic fairness through fairness-aware causal path decomposition. W Pan, S Cui, J Bian, C Zhang, F Wang, SIGKDD. W. Pan, S. Cui, J. Bian, C. Zhang, and F. Wang. 2021. Explaining algorithmic fairness through fairness-aware causal path decomposition. In SIGKDD. 1287-1297.</p>
<p>The filter bubble: What the Internet is hiding from you. E Pariser, E. Pariser. 2011. The filter bubble: What the Internet is hiding from you. penguin UK.</p>
<p>Fair Ranking: A Critical Review, Challenges, and Future Directions. G K Patro, L Porcaro, L Mitchell, Q Zhang, M Zehlike, 10.1145/3531146.35332382022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). New York, NY, USAAssociation for Computing MachineryG. K. Patro, L. Porcaro, L. Mitchell, Q. Zhang, M. Zehlike, et al. 2022. Fair Ranking: A Critical Review, Challenges, and Future Directions. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). Association for Computing Machinery, New York, NY, USA, 1929-1942. https://doi.org/10.1145/3531146.3533238</p>
<p>Causal diagrams for empirical research. J Pearl, Biometrika. 82J. Pearl. 1995. Causal diagrams for empirical research. Biometrika 82, 4 (1995), 669-688.</p>
<p>Direct and Indirect Effects. J Pearl, UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence. Jack S. Breese and Daphne KollerSeattle, Washington, USAMorgan KaufmannUniversity of WashingtonJ. Pearl. 2001. Direct and Indirect Effects. In UAI '01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington, Seattle, Washington, USA, August 2-5, 2001, Jack S. Breese and Daphne Koller (Eds.). Morgan Kaufmann, 411-420. https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1&amp;smnu=2&amp;article_id= 126&amp;proceeding_id=17</p>
<p>J Pearl, Causality: Models, reasoning and inference. Cambridge University Presssecond ed.J. Pearl. 2009. Causality: Models, reasoning and inference (second ed.). Cambridge University Press.</p>
<p>The seven tools of causal inference, with reflections on machine learning. J Pearl, Commun. ACM. 62J. Pearl. 2019. The seven tools of causal inference, with reflections on machine learning. Commun. ACM 62, 3 (2019), 54-60.</p>
<p>The Book of Why: The New Science of Cause and Effect. J Pearl, D Mackenzie, Basic BooksJ. Pearl and D. Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic Books.</p>
<p>Moment matching for multi-source domain adaptation. X Peng, Q Bai, X Xia, Z Huang, K Saenko, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionX. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, et al. 2019. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision. 1406-1415.</p>
<p>Causal inference by using invariant prediction: identification and confidence intervals. J Peters, P Bhlmann, N Meinshausen, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 78J. Peters, P. Bhlmann, and N. Meinshausen. 2016. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 78, 5 (2016), 947-1012.</p>
<p>Elements of Causal Inference: Foundations and Learning Algorithms. J Peters, D Janzing, B Schlkopf, MIT PressJ. Peters, D. Janzing, and B. Schlkopf. 2017. Elements of Causal Inference: Foundations and Learning Algorithms. MIT Press.</p>
<p>Alzheimer's disease neuroimaging initiative (ADNI): clinical characterization. R C Petersen, P Aisen, L A Beckett, M Donohue, A Gamst, Neurology. 74R. C. Petersen, P. Aisen, L. A. Beckett, M. Donohue, A. Gamst, et al. 2010. Alzheimer's disease neuroimaging initiative (ADNI): clinical characterization. Neurology 74, 3 (2010), 201-209.</p>
<p>F Petroni, T Rocktschel, P Lewis, A Bakhtin, Y Wu, arXiv:1909.01066Language models as knowledge bases. arXiv preprintF. Petroni, T. Rocktschel, P. Lewis, A. Bakhtin, Y. Wu, et al. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 (2019).</p>
<p>Project SEAPHE: Databases. Seaphe Project, Project SEAPHE. 2007. Project SEAPHE: Databases. Retrieved December 28, 2022 from http://www.seaphe.org/ databases.php</p>
<p>Auditing the effect of social network recommendations on polarization in geometrical ideological spaces. P , Ramaciotti Morales, J.-P Cointet, Fifteenth ACM Conference on Recommender Systems. P. Ramaciotti Morales and J.-P. Cointet. 2021. Auditing the effect of social network recommendations on polarization in geometrical ideological spaces. In Fifteenth ACM Conference on Recommender Systems. 627-632.</p>
<p>Hierarchical Text-Conditional Image Generation with CLIP Latents. A Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, CoRRA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. CoRR (2022).</p>
<p>Scaling up Greedy Causal Search for Continuous Variables. J D Ramsey, arXiv:1507.07749arXiv preprintJ. D. Ramsey. 2015. Scaling up Greedy Causal Search for Continuous Variables. arXiv preprint arXiv:1507.07749 (2015).</p>
<p>B Ramsundar, P Eastman, P Walters, V Pande, K Leswing, Deep Learning for the Life Sciences. O'Reilly Media. B. Ramsundar, P. Eastman, P. Walters, V. Pande, K. Leswing, et al. 2019. Deep Learning for the Life Sciences. O'Reilly Media. https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837.</p>
<p>Foolbox Native: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX. J Rauber, R Zimmermann, M Bethge, W Brendel, 10.21105/joss.02607Journal of Open Source Software. 52607J. Rauber, R. Zimmermann, M. Bethge, and W. Brendel. 2020. Foolbox Native: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX. Journal of Open Source Software 5, 53 (2020), 2607. https://doi.org/10.21105/joss.02607</p>
<p>Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. S Ravfogel, Y Elazar, H Gonen, M Twiton, Y Goldberg, ACL. S. Ravfogel, Y. Elazar, H. Gonen, M. Twiton, and Y. Goldberg. 2020. Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. In ACL.</p>
<p>Linear Adversarial Concept Erasure. S Ravfogel, M Twiton, Y Goldberg, R Cotterell, ICML. S. Ravfogel, M. Twiton, Y. Goldberg, and R. Cotterell. 2022. Linear Adversarial Concept Erasure. In ICML.</p>
<p>iRefIndex: a consolidated protein interaction database with provenance. S Razick, G Magklaras, I M Donaldson, BMC bioinformatics. 9S. Razick, G. Magklaras, and I. M. Donaldson. 2008. iRefIndex: a consolidated protein interaction database with provenance. BMC bioinformatics 9, 1 (2008), 1-19.</p>
<p>Do imagenet classifiers generalize to imagenet. B Recht, R Roelofs, L Schmidt, V Shankar, PMLRInternational Conference on Machine Learning. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. 2019. Do imagenet classifiers generalize to imagenet?. In International Conference on Machine Learning. PMLR, 5389-5400.</p>
<p>On Causally Disentangled Representations. A G Reddy, B G , V N Balasubramanian, AAAI. A. G. Reddy, B. G. L, and V. N. Balasubramanian. 2022. On Causally Disentangled Representations. In AAAI.</p>
<p>The direction of time. H Reichenbach, Univ of California Press65H. Reichenbach. 1956. The direction of time. Vol. 65. Univ of California Press.</p>
<p>Single-cell transcriptomic analysis of human lung provides insights into the pathobiology of pulmonary fibrosis. P A Reyfman, J M Walter, N Joshi, K R Anekalla, A C Mcquattie-Pimentel, American journal of respiratory and critical care medicine. 199P. A. Reyfman, J. M. Walter, N. Joshi, K. R. Anekalla, A. C. McQuattie-Pimentel, et al. 2019. Single-cell transcriptomic analysis of human lung provides insights into the pathobiology of pulmonary fibrosis. American journal of respiratory and critical care medicine 199, 12 (2019), 1517-1536.</p>
<p>Explaining the predictions of any classifier. M T Ribeiro, S Singh, C Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data miningWhy should i trust you?M. T. Ribeiro, S. Singh, and C. Guestrin. 2016. " Why should i trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135-1144.</p>
<p>Improving the accuracy of medical diagnosis with causal machine learning. J G Richens, C M Lee, S Johri, Nature Communications. 111J. G. Richens, C. M. Lee, and S. Johri. 2020. Improving the accuracy of medical diagnosis with causal machine learning. Nature Communications 11, 1 (2020).</p>
<p>F L Rios, G Moffa, J Kuipers, arXiv:stat.ML/2107.03863Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models. F. L. Rios, G. Moffa, and J. Kuipers. 2021. Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models. arXiv:stat.ML/2107.03863</p>
<p>Learning social etiquette: Human trajectory understanding in crowded scenes. A Robicquet, A Sadeghian, A Alahi, S Savarese, European conference on computer vision. SpringerA. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese. 2016. Learning social etiquette: Human trajectory understanding in crowded scenes. In European conference on computer vision. Springer, 549-565.</p>
<p>The central role of the propensity score in observational studies for causal effects. P R Rosenbaum, D B Rubin, Biometrika. 701P. R. Rosenbaum and D. B. Rubin. 1983. The central role of the propensity score in observational studies for causal effects. Biometrika 70, 1 (1983).</p>
<p>SemEval-2017 Task 4: Sentiment Analysis in Twitter. S Rosenthal, N Farra, P Nakov, 10.18653/v1/S17-2088Proceedings of the 11th International Workshop on Semantic Evaluation. the 11th International Workshop on Semantic EvaluationVancouver, CanadaAssociation for Computational LinguisticsS. Rosenthal, N. Farra, and P. Nakov. 2017. SemEval-2017 Task 4: Sentiment Analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Association for Computational Linguistics, Vancouver, Canada, 502-518. https://doi.org/10.18653/v1/S17-2088</p>
<p>Estimating causal effects of treatments in randomized and nonrandomized studies. D B Rubin, J. Educ. Psychol. 665D. B. Rubin. 1974. Estimating causal effects of treatments in randomized and nonrandomized studies. J. Educ. Psychol. 66, 5 (1974).</p>
<p>R Rudinger, J Naradowsky, B Leonard, B Van Durme, arXiv:1804.09301Gender bias in coreference resolution. arXiv preprintR. Rudinger, J. Naradowsky, B. Leonard, and B. Van Durme. 2018. Gender bias in coreference resolution. arXiv preprint arXiv:1804.09301 (2018).</p>
<p>Imagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, International journal of computer vision. 115O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115, 3 (2015), 211-252.</p>
<p>S Sagawa, P W Koh, T B Hashimoto, P Liang, arXiv:1911.08731Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprintS. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. 2019. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731 (2019).</p>
<p>P Saleiro, B Kuester, A Stevens, A Anisfeld, L Hinkson, arXiv:1811.05577Aequitas: A Bias and Fairness Audit Toolkit. arXiv preprintP. Saleiro, B. Kuester, A. Stevens, A. Anisfeld, L. Hinkson, et al. 2018. Aequitas: A Bias and Fairness Audit Toolkit. arXiv preprint arXiv:1811.05577 (2018).</p>
<p>Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models. A Salem, Y Zhang, M Humbert, P Berrang, M Fritz, arXiv:1806.01246arXiv preprintA. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, et al. 2018. Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models. arXiv preprint arXiv:1806.01246 (2018).</p>
<p>Interventional fairness: Causal database repair for algorithmic fairness. B Salimi, L Rodriguez, B Howe, D Suciu, B. Salimi, L. Rodriguez, B. Howe, and D. Suciu. 2019. Interventional fairness: Causal database repair for algorithmic fairness. In MOD. 793-810.</p>
<p>Causal machine learning for healthcare and precision medicine. P Sanchez, J P Voisey, T Xia, H I Watson, A Q O&apos;neil, Royal Society Open Science. 9P. Sanchez, J. P. Voisey, T. Xia, H. I. Watson, A. Q. O'Neil, et al. 2022. Causal machine learning for healthcare and precision medicine. Royal Society Open Science 9 (2022).</p>
<p>Learning a driving simulator. E Santana, G Hotz, arXiv:1608.01230arXiv preprintE. Santana and G. Hotz. 2016. Learning a driving simulator. arXiv preprint arXiv:1608.01230 (2016).</p>
<p>Predicting recidivism in north carolina. P Schmidt, A D Witte, P. Schmidt and A. D. Witte. 1988. Predicting recidivism in north carolina, 1978 and 1980.</p>
<p>Causality for machine learning. B Schlkopf, Probabilistic and Causal Inference: The Works of Judea Pearl. B. Schlkopf. 2022. Causality for machine learning. In Probabilistic and Causal Inference: The Works of Judea Pearl. 765-804.</p>
<p>Toward causal representation learning. B Schlkopf, F Locatello, S Bauer, N R Ke, N Kalchbrenner, Proc. IEEE. 109B. Schlkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, et al. 2021. Toward causal representation learning. Proc. IEEE 109, 5 (2021), 612-634.</p>
<p>CXPlain: Causal Explanations for Model Interpretation under Uncertainty. P Schwab, W Karlen, NeurIPS. P. Schwab and W. Karlen. 2019. CXPlain: Causal Explanations for Model Interpretation under Uncertainty. NeurIPS (2019).</p>
<p>From Statistical to Causal Learning. B Schlkopf, J Von Kgelgen, 10.48550/ARXIV.2204.00607B. Schlkopf and J. von Kgelgen. 2022. From Statistical to Causal Learning. arXiv. https://doi.org/10.48550/ARXIV. 2204.00607</p>
<p>AN INSTITUTIONAL VIEW OF ALGORITHMIC IMPACT. A D Selbst, Harvard Journal of Law &amp; Technology. 351A. D. Selbst. 2021. AN INSTITUTIONAL VIEW OF ALGORITHMIC IMPACT. Harvard Journal of Law &amp; Technology 35, 1 (2021).</p>
<p>Estimating the causal impact of recommendation systems from observational data. A Sharma, J M Hofman, D J Watts, Proceedings of the Sixteenth ACM Conference on Economics and Computation. the Sixteenth ACM Conference on Economics and ComputationA. Sharma, J. M. Hofman, and D. J. Watts. 2015. Estimating the causal impact of recommendation systems from observational data. In Proceedings of the Sixteenth ACM Conference on Economics and Computation. 453-470.</p>
<p>DoWhy: A Python package for causal inference. A Sharma, E Kiciman, A. Sharma, E. Kiciman, et al. 2019. DoWhy: A Python package for causal inference. https://github.com/microsoft/dowhy.</p>
<p>Learning Residual Images for Face Attribute Manipulation. W Shen, R Liu, CVPR. W. Shen and R. Liu. 2017. Learning Residual Images for Face Attribute Manipulation. In CVPR.</p>
<p>Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer's. X Shen, S Ma, P Vemuri, G Simon, M W Weiner, Pathophysiology. Scientific Reports. 101X. Shen, S. Ma, P. Vemuri, G. Simon, M. W. Weiner, et al. 2020. Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer's Pathophysiology. Scientific Reports 10, 1 (2020).</p>
<p>Learning Causal Effects From Observational Data in Healthcare: A Review and Summary. J Shi, B Norgeot, Frontiers in Medicine. 9J. Shi and B. Norgeot. 2022. Learning Causal Effects From Observational Data in Healthcare: A Review and Summary. Frontiers in Medicine 9 (2022).</p>
<p>Generating high-fidelity privacy-conscious synthetic patient data for causal effect estimation with multiple treatments. J Shi, D Wang, G Tesei, B Norgeot, Frontiers in Artificial Intelligence. 5J. Shi, D. Wang, G. Tesei, and B. Norgeot. 2022. Generating high-fidelity privacy-conscious synthetic patient data for causal effect estimation with multiple treatments. Frontiers in Artificial Intelligence 5 (2022).</p>
<p>An evaluation toolkit to guide model selection and cohort definition in causal inference. Y Shimoni, E Karavani, S Ravid, P Bak, T H Ng, arXiv:1906.00442arXiv preprintY. Shimoni, E. Karavani, S. Ravid, P. Bak, T. H. Ng, et al. 2019. An evaluation toolkit to guide model selection and cohort definition in causal inference. arXiv preprint arXiv:1906.00442 (2019).</p>
<p>The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. D Shin, International Journal of Human Computer Studies. D. Shin. 2021. The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. International Journal of Human Computer Studies (2021).</p>
<p>Human-Centered Artificial Intelligence: Reliable, Safe &amp; Trustworthy. B Shneiderman, 10.1080/10447318.2020.1741118Int. J. Hum. Comput. Interact. 36B. Shneiderman. 2020. Human-Centered Artificial Intelligence: Reliable, Safe &amp; Trustworthy. Int. J. Hum. Comput. Interact. 36, 6 (2020), 495-504. https://doi.org/10.1080/10447318.2020.1741118</p>
<p>On the privacy risks of model explanations. R Shokri, M Strobel, Y Zick, Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. the 2021 AAAI/ACM Conference on AI, Ethics, and SocietyR. Shokri, M. Strobel, and Y. Zick. 2021. On the privacy risks of model explanations. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 231-241.</p>
<p>Membership inference attacks against machine learning models. R Shokri, M Stronati, C Song, V Shmatikov, 2017 IEEE symposium on security and privacy (SP). IEEER. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP). IEEE, 3-18.</p>
<p>A survey on image data augmentation for deep learning. C Shorten, T M Khoshgoftaar, Journal of big data. 6C. Shorten and T. M. Khoshgoftaar. 2019. A survey on image data augmentation for deep learning. Journal of big data 6, 1 (2019), 1-48.</p>
<p>A Model-Agnostic Causal Learning Framework for Recommendation using Search Data. Z Si, X Han, X Zhang, J Xu, Y Yin, WWW. Z. Si, X. Han, X. Zhang, J. Xu, Y. Yin, et al. 2022. A Model-Agnostic Causal Learning Framework for Recommendation using Search Data. In WWW.</p>
<p>Fairness violations and mitigation under covariate shift. H Singh, R Singh, V Mhasawade, R Chunara, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and TransparencyH. Singh, R. Singh, V. Mhasawade, and R. Chunara. 2021. Fairness violations and mitigation under covariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 3-13.</p>
<p>Causation, Prediction, and Search, Second Edition. P Spirtes, C Glymour, R Scheines, MIT PressP. Spirtes, C. Glymour, and R. Scheines. 2000. Causation, Prediction, and Search, Second Edition. MIT Press.</p>
<p>Energy and Policy Considerations for Deep Learning in NLP. E Strubell, A Ganesh, A Mccallum, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsE. Strubell, A. Ganesh, and A. McCallum. 2019. Energy and Policy Considerations for Deep Learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 3645-3650.</p>
<p>Matching methods for causal inference: A review and a look forward. E A Stuart, Statistical science: a review journal of the Institute of Mathematical Statistics. 251E. A. Stuart. 2010. Matching methods for causal inference: A review and a look forward. Statistical science: a review journal of the Institute of Mathematical Statistics 25, 1 (2010), 1.</p>
<p>A next generation connectivity map: L1000 platform and the first 1,000,000 profiles. A Subramanian, R Narayan, S M Corsello, D D Peck, T E Natoli, Cell. 171A. Subramanian, R. Narayan, S. M. Corsello, D. D. Peck, T. E. Natoli, et al. 2017. A next generation connectivity map: L1000 platform and the first 1,000,000 profiles. Cell 171, 6 (2017), 1437-1452.</p>
<p>J Tan, S Xu, Y Ge, Y Li, X Chen, Counterfactual Explainable Recommendation. International Conference on Information and Knowledge Management, Proceedings. J. Tan, S. Xu, Y. Ge, Y. Li, X. Chen, et al. 2021. Counterfactual Explainable Recommendation. International Conference on Information and Knowledge Management, Proceedings.</p>
<p>Learning what makes a difference from counterfactual examples and gradient supervision. D Teney, E Abbasnedjad, A V D Hengel, European Conference on Computer Vision. SpringerD. Teney, E. Abbasnedjad, and A. v. d. Hengel. 2020. Learning what makes a difference from counterfactual examples and gradient supervision. In European Conference on Computer Vision. Springer, 580-599.</p>
<p>The World Bank Group. 2022. World Development Indicators. RetrievedThe World Bank Group. 2022. World Development Indicators. Retrieved November 29, 2022 from https://data. worldbank.org/indicator</p>
<p>. S Thiebes, S Lins, A Sunyaev, 10.1007/s12525-020-00441-4Trustworthy artificial intelligence. Electron. Mark. 31S. Thiebes, S. Lins, and A. Sunyaev. 2021. Trustworthy artificial intelligence. Electron. Mark. 31, 2 (2021), 447-464. https://doi.org/10.1007/s12525-020-00441-4</p>
<p>Alleviating privacy attacks via causal learning. S Tople, A Sharma, A Nori, International Conference on Machine Learning. PMLR. S. Tople, A. Sharma, and A. Nori. 2020. Alleviating privacy attacks via causal learning. In International Conference on Machine Learning. PMLR, 9537-9547.</p>
<p>Unbiased look at dataset bias. A Torralba, A A Efros, CVPR 2011. IEEE. A. Torralba and A. A. Efros. 2011. Unbiased look at dataset bias. In CVPR 2011. IEEE, 1521-1528.</p>
<p>SoK: Differential privacy as a causal property. M C Tschantz, S Sen, A Datta, 2020 IEEE Symposium on Security and Privacy (SP). IEEEM. C. Tschantz, S. Sen, and A. Datta. 2020. SoK: Differential privacy as a causal property. In 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 354-371.</p>
<p>Decisions, counterfactual explanations and strategic behavior. S Tsirtsis, M. Gomez Rodriguez, Advances in Neural Information Processing Systems. 33S. Tsirtsis and M. Gomez Rodriguez. 2020. Decisions, counterfactual explanations and strategic behavior. Advances in Neural Information Processing Systems 33 (2020), 16749-16760.</p>
<p>CORUM: the comprehensive resource of mammalian protein complexes-2022. G Tsitsiridis, R Steinkamp, M Giurgiu, B Brauner, G Fobo, Nucleic Acids Research. 51G. Tsitsiridis, R. Steinkamp, M. Giurgiu, B. Brauner, G. Fobo, et al. 2023. CORUM: the comprehensive resource of mammalian protein complexes-2022. Nucleic Acids Research 51, D1 (2023), D539-D545.</p>
<p>What if This Modified That? Syntactic Interventions with Counterfactual Embeddings. M Tucker, P Qian, R Levy, ACL-IJCNLP. M. Tucker, P. Qian, and R. Levy. 2021. What if This Modified That? Syntactic Interventions with Counterfactual Embeddings. In ACL-IJCNLP.</p>
<p>Self-Driving Car. Udacity, Udacity. 2016. Self-Driving Car. https://github.com/udacity/self-driving-car.</p>
<p>Department of Education's Office for Civil Rights (OCR). 2023. Civil Rights Data Collection. U S , RetrievedU.S. Department of Education's Office for Civil Rights (OCR). 2023. Civil Rights Data Collection. Retrieved January 30, 2023 from https://ocrdata.ed.gov/</p>
<p>. MEDLINE. U.S. National Library of MedicineU.S. National Library of Medicine. 2023. MEDLINE. Retrieved January 23, 2023 from https://www.nlm.nih.gov/ medline/medline_overview.html</p>
<p>Actionable recourse in linear classification. B Ustun, A Spangher, Y Liu, Proceedings of the conference on fairness, accountability, and transparency. the conference on fairness, accountability, and transparencyB. Ustun, A. Spangher, and Y. Liu. 2019. Actionable recourse in linear classification. In Proceedings of the conference on fairness, accountability, and transparency. 10-19.</p>
<p>Eliminating biasing signals in lung cancer images for prognosis predictions with deep learning. W A C Van Amsterdam, J J C Verhoeff, P A Jong, T Leiner, M J C Eijkemans, Digital Medicine. 21W. A. C. Van Amsterdam, J. J. C. Verhoeff, P. A. de Jong, T. Leiner, and M. J. C. Eijkemans. 2019. Eliminating biasing signals in lung cancer images for prognosis predictions with deep learning. npj Digital Medicine 2, 1 (2019).</p>
<p>Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. S Van Steenkiste, M Chang, K Greff, J Schmidhuber, arXiv:1802.10353arXiv preprintS. Van Steenkiste, M. Chang, K. Greff, and J. Schmidhuber. 2018. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353 (2018).</p>
<p>Deep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionH. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan. 2017. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 5018-5027.</p>
<p>Investigating Gender Bias in Language Models Using Causal Mediation Analysis. J Vig, S Gehrmann, Y Belinkov, S Qian, D Nevo, NeurIPS. J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, et al. 2020. Investigating Gender Bias in Language Models Using Causal Mediation Analysis. In NeurIPS.</p>
<p>A Vlontzos, D Rueckert, B Kainz, arXiv:2206.054982022. A Review of Causality for Learning Algorithms in Medical Image Analysis. arXiv preprintA. Vlontzos, D. Rueckert, and B. Kainz. 2022. A Review of Causality for Learning Algorithms in Medical Image Analysis. arXiv preprint arXiv:2206.05498 (2022).</p>
<p>Bayesian Federated Estimation of Causal Effects from Observational Data. T V Vo, Y Lee, T N Hoang, T.-Y Leong, The 38th Conference on Uncertainty in Artificial Intelligence. T. V. Vo, Y. Lee, T. N. Hoang, and T.-Y. Leong. 2022. Bayesian Federated Estimation of Causal Effects from Observational Data. In The 38th Conference on Uncertainty in Artificial Intelligence.</p>
<p>Sustainability assessment of hydropower: Using causal diagram to seize the importance of impact pathways. G Voegeli, W Hediger, F Romerio, Environmental Impact Assessment Review. 77G. Voegeli, W. Hediger, and F. Romerio. 2019. Sustainability assessment of hydropower: Using causal diagram to seize the importance of impact pathways. Environmental Impact Assessment Review 77 (2019), 69-84.</p>
<p>RtGender: A corpus for studying differential responses to gender. R Voigt, D Jurgens, V Prabhakaran, D Jurafsky, Y Tsvetkov, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC. the Eleventh International Conference on Language Resources and Evaluation (LRECR. Voigt, D. Jurgens, V. Prabhakaran, D. Jurafsky, and Y. Tsvetkov. 2018. RtGender: A corpus for studying differential responses to gender. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>H Wang, H Si, B Li, H Zhao, arXiv:2201.12919Provable Domain Generalization via Invariant-Feature Subspace Recovery. arXiv preprintH. Wang, H. Si, B. Li, and H. Zhao. 2022. Provable Domain Generalization via Invariant-Feature Subspace Recovery. arXiv preprint arXiv:2201.12919 (2022).</p>
<p>Enhancing Model Robustness and Fairness with Causality: A Regularization Approach. Z Wang, K Shu, A Culotta, arXiv:2110.00911arXiv preprintZ. Wang, K. Shu, and A. Culotta. 2021. Enhancing Model Robustness and Fairness with Causality: A Regularization Approach. arXiv preprint arXiv:2110.00911 (2021).</p>
<p>Z Wang, X Xiao, Z Xu, Y Zhu, P Stone, arXiv:2206.13452Causal dynamics learning for task-independent state abstraction. arXiv preprintZ. Wang, X. Xiao, Z. Xu, Y. Zhu, and P. Stone. 2022. Causal dynamics learning for task-independent state abstraction. arXiv preprint arXiv:2206.13452 (2022).</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S R Bowman, arXiv:1704.05426arXiv preprintA. Williams, N. Nangia, and S. R. Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426 (2017).</p>
<p>Trustworthy AI. J M Wing, 10.1145/3448248Commun. ACM. 64J. M. Wing. 2021. Trustworthy AI. Commun. ACM 64, 10 (2021), 64-71. https://doi.org/10.1145/3448248</p>
<p>. World Economic Forum. 2023. World Economic Forum. World Economic Forum. 2023. World Economic Forum. Retrieved February 6, 2023 from https://www.weforum.org/ reports/</p>
<p>World Intellectual Property Organization. 2023. Global Brand Database. World Intellectual Property Organization. 2023. Global Brand Database. Retrieved February 6, 2023 from https: //branddb.wipo.int/en/</p>
<p>Mind: A large-scale dataset for news recommendation. F Wu, Y Qiao, J.-H Chen, C Wu, T Qi, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsF. Wu, Y. Qiao, J.-H. Chen, C. Wu, T. Qi, et al. 2020. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 3597-3606.</p>
<p>Causality in social life cycle impact assessment (SLCIA). S R Wu, J Chen, D Apul, P Fan, Y Yan, The International Journal of Life Cycle Assessment. 20S. R. Wu, J. Chen, D. Apul, P. Fan, Y. Yan, et al. 2015. Causality in social life cycle impact assessment (SLCIA). The International Journal of Life Cycle Assessment 20, 9 (2015), 1312-1323.</p>
<p>Counterfactual fairness: Unidentification, bound and algorithm. Y Wu, L Zhang, X Wu, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. the Twenty-Eighth International Joint Conference on Artificial IntelligenceY. Wu, L. Zhang, and X. Wu. 2019. Counterfactual fairness: Unidentification, bound and algorithm. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence.</p>
<p>Pc-fairness: A unified framework for measuring causality-based fairness. Y Wu, L Zhang, X Wu, H Tong, Advances in Neural Information Processing Systems. 32Y. Wu, L. Zhang, X. Wu, and H. Tong. 2019. Pc-fairness: A unified framework for measuring causality-based fairness. Advances in Neural Information Processing Systems 32 (2019).</p>
<p>Achieving causal fairness through generative adversarial networks. D Xu, Y Wu, S Yuan, L Zhang, X Wu, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. the Twenty-Eighth International Joint Conference on Artificial IntelligenceD. Xu, Y. Wu, S. Yuan, L. Zhang, and X. Wu. 2019. Achieving causal fairness through generative adversarial networks. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence.</p>
<p>Silva: Interactively assessing machine learning fairness using causality. J N Yan, Z Gu, H Lin, J M Rzeszotarski, J. N. Yan, Z. Gu, H. Lin, and J. M. Rzeszotarski. 2020. Silva: Interactively assessing machine learning fairness using causality. In CHI. 1-13.</p>
<p>OoD-bench: Benchmarking and understanding out-of-distribution generalization datasets and algorithms. N Ye, K Li, L Hong, H Bai, Y Chen, arXiv:2106.03721arXiv preprintN. Ye, K. Li, L. Hong, H. Bai, Y. Chen, et al. 2021. OoD-bench: Benchmarking and understanding out-of-distribution generalization datasets and algorithms. arXiv preprint arXiv:2106.03721 (2021).</p>
<p>The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert systems with applications. I.-C Yeh, C.-H Lien, 36I.-C. Yeh and C.-h. Lien. 2009. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert systems with applications 36, 2 (2009), 2473-2480.</p>
<p>Yelp, Yelp Open Dataset. n. d.Yelp. [n. d.]. Yelp Open Dataset. https://www.yelp.com/dataset. Accessed: 2022-11-23.</p>
<p>Privacy risk in machine learning: Analyzing the connection to overfitting. S Yeom, I Giacomelli, M Fredrikson, S Jha, 2018 IEEE 31st computer security foundations symposium (CSF). IEEES. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. 2018. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF). IEEE, 268-282.</p>
<p>Causality-based Feature Selection: Methods and Evaluations. K Yu, X Guo, L Liu, J Li, H Wang, Comput. Surveys. K. Yu, X. Guo, L. Liu, J. Li, H. Wang, et al. 2020. Causality-based Feature Selection: Methods and Evaluations. Comput. Surveys (2020).</p>
<p>A Unified View of Causal and Non-causal Feature Selection. K Yu, L Liu, J Li, ACM Transactions on Knowledge Discovery from Data. 15K. Yu, L. Liu, and J. Li. 2021. A Unified View of Causal and Non-causal Feature Selection. ACM Transactions on Knowledge Discovery from Data 15 (2021).</p>
<p>A causal view on robustness of neural networks. C Zhang, K Zhang, Y Li, Advances in Neural Information Processing Systems. 33C. Zhang, K. Zhang, and Y. Li. 2020. A causal view on robustness of neural networks. Advances in Neural Information Processing Systems 33 (2020), 289-301.</p>
<p>Fairness in decision-making-the causal explanation formula. J Zhang, E Bareinboim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32J. Zhang and E. Bareinboim. 2018. Fairness in decision-making-the causal explanation formula. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.</p>
<p>K Zhang, S Zhu, M Kalander, I Ng, J Ye, arXiv:2111.15155gCastle: A Python Toolbox for Causal Discovery. arXiv preprintK. Zhang, S. Zhu, M. Kalander, I. Ng, J. Ye, et al. 2021. gCastle: A Python Toolbox for Causal Discovery. arXiv preprint arXiv:2111.15155 (2021).</p>
<p>Anti-discrimination learning: a causal modeling-based framework. L Zhang, X Wu, International Journal of Data Science and Analytics. 4L. Zhang and X. Wu. 2017. Anti-discrimination learning: a causal modeling-based framework. International Journal of Data Science and Analytics 4, 1 (2017), 1-16.</p>
<p>Situation Testing-Based Discrimination Discovery: A Causal Inference Approach. L Zhang, Y Wu, X Wu, IJCAI. 16L. Zhang, Y. Wu, and X. Wu. 2016. Situation Testing-Based Discrimination Discovery: A Causal Inference Approach.. In IJCAI, Vol. 16. 2718-2724.</p>
<p>A Causal Framework for Discovering and Removing Direct and Indirect Discrimination. L Zhang, Y Wu, X Wu, IJCAI. AAAI PressL. Zhang, Y. Wu, and X. Wu. 2017. A Causal Framework for Discovering and Removing Direct and Indirect Discrimi- nation. In IJCAI. AAAI Press, 3929-3935.</p>
<p>A Causal Framework for Discovering and Removing Direct and Indirect Discrimination. L Zhang, Y Wu, X Wu, Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. the Twenty-Sixth International Joint Conference on Artificial IntelligenceL. Zhang, Y. Wu, and X. Wu. 2017. A Causal Framework for Discovering and Removing Direct and Indirect Discrimi- nation. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.</p>
<p>Causal modeling-based discrimination discovery and removal: criteria, bounds, and algorithms. L Zhang, Y Wu, X Wu, IEEE Transactions on Knowledge and Data Engineering. 31L. Zhang, Y. Wu, and X. Wu. 2018. Causal modeling-based discrimination discovery and removal: criteria, bounds, and algorithms. IEEE Transactions on Knowledge and Data Engineering 31, 11 (2018), 2035-2050.</p>
<p>Causal Inference in medicine and in health policy, a summary. W Zhang, R Ramezani, A Naeim, arXiv:2105.04655arXiv preprintW. Zhang, R. Ramezani, and A. Naeim. 2021. Causal Inference in medicine and in health policy, a summary. arXiv preprint arXiv:2105.04655 (2021).</p>
<p>Y Zhang, M Gong, T Liu, G Niu, X Tian, CausalAdv: Adversarial Robustness through the Lens of Causality. arXiv. Y. Zhang, M. Gong, T. Liu, G. Niu, X. Tian, et al. 2021. CausalAdv: Adversarial Robustness through the Lens of Causality. arXiv (2021).</p>
<p>J Zhao, T Wang, M Yatskar, V Ordonez, K.-W Chang, arXiv:1804.06876Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprintJ. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876 (2018).</p>
<p>Learning gender-neutral word embeddings. J Zhao, Y Zhou, Z Li, W Wang, K.-W Chang, arXiv:1809.01496arXiv preprintJ. Zhao, Y. Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018. Learning gender-neutral word embeddings. arXiv preprint arXiv:1809.01496 (2018).</p>
<p>Disentangling User Interest and Conformity for Recommendation with Causal Embedding. Y Zheng, C Gao, X Li, X He, Y Li, WWW. Y. Zheng, C. Gao, X. Li, X. He, Y. Li, et al. 2021. Disentangling User Interest and Conformity for Recommendation with Causal Embedding. In WWW.</p>
<p>Evaluating the quality of machine learning explanations: A survey on methods and metrics. J Zhou, A H Gandomi, F Chen, A Holzinger, Electronics. 10593J. Zhou, A. H. Gandomi, F. Chen, and A. Holzinger. 2021. Evaluating the quality of machine learning explanations: A survey on methods and metrics. Electronics 10, 5 (2021), 593.</p>
<p>Learning to generate novel domains for domain generalization. K Zhou, Y Yang, T Hospedales, T Xiang, European conference on computer vision. SpringerK. Zhou, Y. Yang, T. Hospedales, and T. Xiang. 2020. Learning to generate novel domains for domain generalization. In European conference on computer vision. Springer, 561-578.</p>
<p>Y Zhu, J Wong, A Mandlekar, R Martn-Martn, arXiv:2009.122932020. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprintY. Zhu, J. Wong, A. Mandlekar, and R. Martn-Martn. 2020. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293 (2020).</p>
<p>Safety and efficacy of digoxin: systematic review and meta-analysis of observational and controlled trial data. O J Ziff, D A Lane, M Samra, M Griffith, P Kirchhof, BMJ. 351O. J. Ziff, D. A. Lane, M. Samra, M. Griffith, P. Kirchhof, et al. 2015. Safety and efficacy of digoxin: systematic review and meta-analysis of observational and controlled trial data. BMJ 351 (2015).</p>
<p>Zimnat Insurance Recommendation Challenge. Zimnat, RetrievedZimnat. 2020. Zimnat Insurance Recommendation Challenge. Retrieved November 25, 2022 from https://zindi.africa/ competitions/zimnat-insurance-recommendation-challenge</p>
<p>An image classification dataset categorized into 10 classes that are scattered across four different domains, each having a distinct trait: photograph, art, cartoon and sketch.   Used by.  Pacs, 136106 PACS [136]: An image classification dataset categorized into 10 classes that are scattered across four different domains, each having a distinct trait: photograph, art, cartoon and sketch.   Used by: [106]</p>
<p>The data include not only information about the review itself and product metadata (e.g., descriptions, price, product size, or package type) but also also bought and also viewed links.  Amazon, An extensive dataset of 233.1 million Amazon reviews between. 12SemEval-2017 Task. NLP models compete on sentiment analysis tasks on English and Arabic Twitter data.   Used by: [121 Amazon (Product) Data [179]: An extensive dataset of 233.1 million Amazon reviews between May 1996 and October 2018. The data include not only information about the review itself and product metadata (e.g., descriptions, price, product size, or package type) but also also bought and also viewed links.   Used by 12 : [121, 280]  SemEval-2017 Task 4 [223]: SemEval 13 is a yearly NLP workshop where participants com- pete on different sentiment analysis tasks. Each workshop comes with its own set of tasks to solve. In SemEval-2017 Task 4, NLP models compete on sentiment analysis tasks on English and Arabic Twitter data.   Used by: [121]</p>
<p>A dataset of almost 7 million Yelp user reviews of around 150k businesses across 11 cities in the US and Canada. Review entries contain not only their associated text and an integer star rating between 1 and 5 but also additional information like the amount of useful, funny, and cool votes for the review.  Yelp, 294  Used by: [121 Yelp [294]: A dataset of almost 7 million Yelp user reviews of around 150k businesses across 11 cities in the US and Canada. Review entries contain not only their associated text and an integer star rating between 1 and 5 but also additional information like the amount of useful, funny, and cool votes for the review.   Used by: [121]</p>
<p>A set of 2440 IMDb reviews, where a human-annotated counterfactual example accompanies each review. The human annotators were found through Amazon's crowdsourcing platform Mechanical Turk 14 . The dataset is designed to assess the performance of sentiment analysis and natural language inference models.   Used by.  IMDb extension [120. 121, 258, 280 IMDb extension [120]: A set of 2440 IMDb reviews, where a human-annotated counterfac- tual example accompanies each review. The human annotators were found through Amazon's crowdsourcing platform Mechanical Turk 14 . The dataset is designed to assess the performance of sentiment analysis and natural language inference models.   Used by: [121, 258, 280]</p>
<p>The original SNLI dataset [31] is a text dataset developed to evaluate natural language inference (NLI) models. Models must decide whether a given hypothesis is contradictory to, entailed by, or neutral to the given premise. Kaushik et al. [120] extended this dataset via humanly-manufactured counterfactual examples.  Snli Extension, 120121, 258 SNLI extension [120]: The original SNLI dataset [31] is a text dataset developed to evaluate natural language inference (NLI) models. Models must decide whether a given hypothesis is contradictory to, entailed by, or neutral to the given premise. Kaushik et al. [120] extended this dataset via humanly-manufactured counterfactual examples.   Used by: [121, 258]</p>
<p>A set of extracted features from audio samples (i.e., sustained phonations) of patients with Parkinson's disease and people from healthy control groups. This dataset combines data from three different and independent labs from the US, Turkey, and Spain. The classification task is to detect patients with Parkinson's disease.   Used by.  Parkinson's voice data [144. 144 Parkinson's voice data [144]: A set of extracted features from audio samples (i.e., sustained phonations) of patients with Parkinson's disease and people from healthy control groups. This dataset combines data from three different and independent labs from the US, Turkey, and Spain. The classification task is to detect patients with Parkinson's disease.   Used by: [144]</p>
<p>An unsupervised domain adaptation image dataset containing six domains (referring to the "style" of the image, e.g., sketch, quick drawing or real image) and about ca.  Domainnet, 600k images distributed among 345 categories DomainNet [199]: An unsupervised domain adaptation image dataset containing six domains (referring to the "style" of the image, e.g., sketch, quick drawing or real image) and about ca. 600k images distributed among 345 categories.</p>
<p>Another well-known, more sophisticated image dataset containing more than 14 million images. The images depict more than 20,000 synsets (i.e., concepts "possibly described by multiple words or word phrases" 15 ).   Used by.  Imagenet, 158, 169 ImageNet [61]: Another well-known, more sophisticated image dataset containing more than 14 million images. The images depict more than 20,000 synsets (i.e., concepts "possibly described by multiple words or word phrases" 15 ).   Used by: [158, 169]</p>
<p>This dataset tests the model's robustness by applying corruptions to validation images of ImageNet. Each of the 15 corruption types (e.g., gaussian noise, snow, motion blur, or contrast) comes with five levels of corruption intensity.   Used by.  Imagenet, -C , 158 ImageNet-C [99]: This dataset tests the model's robustness by applying corruptions to validation images of ImageNet. Each of the 15 corruption types (e.g., gaussian noise, snow, motion blur, or contrast) comes with five levels of corruption intensity.   Used by: [158]</p>
<p>A new test set for ImageNet designed to assess the model's generalization ability. Despite closely following the original dataset creation process, models trained on the original ImageNet demonstrate worse performance on ImageNet-V2. ImageNet models with better generalization should perform stably on both variants.  Imagenet, - V2, 214158 ImageNet-V2 [214]: A new test set for ImageNet designed to assess the model's generaliza- tion ability. Despite closely following the original dataset creation process, models trained on the original ImageNet demonstrate worse performance on ImageNet-V2. ImageNet models with better generalization should perform stably on both variants.   Used by: [158]</p>
<p>An image dataset designed to demonstrate the transfer learning ability of ImageNet models. Due to this, ObjectNet provides no training set.  Objectnet, 500 ObjectNet [17]: An image dataset designed to demonstrate the transfer learning ability of ImageNet models. Due to this, ObjectNet provides no training set. Instead, all 50,000</p>
<p>images of ObjectNet combine into a single test set. Each image depicts an object with random backgrounds, viewpoints, and rotations of the object.   Used by. 158https://www.mturk.com/ 15 https://www.image-net.org/about.php images of ObjectNet combine into a single test set. Each image depicts an object with random backgrounds, viewpoints, and rotations of the object.   Used by: [158]</p>
<p>The dataset used for the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012). The 1.5 million images depict objects from 1,000 different synsets.   Used by.  Imagenet, ILSVRC-2012226169 ImageNet ILSVRC-2012 [226]: The dataset used for the ImageNet Large Scale Visual Recog- nition Challenge 2012 (ILSVRC2012). The 1.5 million images depict objects from 1,000 different synsets.   Used by: [169]</p>
<p>A variation of ImageNet designed to evaluate the susceptibility to spurious correlations of ImageNet models. It includes 30,000 artistic renditions (e.g., paintings, origami, or sculptures) of 200 ImageNet object classes. The images were primarily collected from Flickr 16 .   Used by.  Imagenet, -R , 98169 ImageNet-R [98]: A variation of ImageNet designed to evaluate the susceptibility to spurious correlations of ImageNet models. It includes 30,000 artistic renditions (e.g., paintings, origami, or sculptures) of 200 ImageNet object classes. The images were primarily collected from Flickr 16 .   Used by: [169]</p>
<p>A suite of Atari 2600 games that allows researchers to develop AI agents (mostly RL agents) for more than 100 games. ALE supports OpenAI gym, Python, and C++ and provides researchers with a plethora of features to evaluate different agents.  The Arcade Learning Environment (ALE85, 169 The Arcade Learning Environment (ALE) [22]: A suite of Atari 2600 games that allows researchers to develop AI agents (mostly RL agents) for more than 100 games. ALE supports OpenAI gym, Python, and C++ and provides researchers with a plethora of features to evaluate different agents.   Used by: [85, 169]</p>
<p>CIFAR-10 and CIFAR-100, are labeled images stemming from the now withdrawn Tiny Images dataset 17 . The more prominent set, CIFAR-10, contains 60000 32  32 color images separated into ten mutually exclusive classes, with 6000 images per class. CIFAR-100 is simply a 100-class version of CIFAR-10.  Cifar, The two CIFAR datasets. 127107, 307 CIFAR [127]: The two CIFAR datasets, CIFAR-10 and CIFAR-100, are labeled images stemming from the now withdrawn Tiny Images dataset 17 . The more prominent set, CIFAR-10, contains 60000 32  32 color images separated into ten mutually exclusive classes, with 6000 images per class. CIFAR-100 is simply a 100-class version of CIFAR-10.   Used by: [107, 307]</p>
<p>A collection of over 1 million satellite images depicting more than 200 countries. Each satellite contains at least one of 63 box annotations categorizing visible landmarks, such as flooded road or airport.   Used by.  Functional Map of the World. FMoW107 Functional Map of the World (FMoW) [52]: A collection of over 1 million satellite images depicting more than 200 countries. Each satellite contains at least one of 63 box annotations categorizing visible landmarks, such as flooded road or airport.   Used by: [107]</p>
<p>This synthetic environment was designed to evaluate causal reinforcement learning (RL) agents exhaustively. In this task, agents must change the colors of a given set of objects. However, altering one object influences the color of other objects. The causal dynamics are set by either a user-defined causal graph or a randomly generated DAG.   Used by. Environment, 122281 Chemical Environment [122]: This synthetic environment was designed to evaluate causal reinforcement learning (RL) agents exhaustively. In this task, agents must change the colors of a given set of objects. However, altering one object influences the color of other objects. The causal dynamics are set by either a user-defined causal graph or a randomly generated DAG.   Used by: [281]</p>
<p>A simulation framework built upon the MuJoCo physics engine allowing researchers to simulate contact dynamics for robot learning tasks. Given a set of cubes, RL agents must maneuver a robotic arm to solve different tasks (e.g., stacking the cubes or lifting one to a specified height).   Used by.  Robosuite, 281 robosuite [313]: A simulation framework built upon the MuJoCo physics engine allowing researchers to simulate contact dynamics for robot learning tasks. Given a set of cubes, RL agents must maneuver a robotic arm to solve different tasks (e.g., stacking the cubes or lifting one to a specified height).   Used by: [281]</p>
<p>A set of criminological datasets published by ProPublica to evaluate the bias of COMPAS -an algorithm used to assess the likelihood of criminal defendants reoffending. All COMPAS-related datasets include data from over 10,000 defendants, each being described via 52 features (e.g., age, sex, race) and with a label indicating whether they were rearrested within two years.  Compas Recidivism Risk, 1263 COMPAS Recidivism Risk [12]: A set of criminological datasets published by ProPublica to evaluate the bias of COMPAS -an algorithm used to assess the likelihood of criminal de- fendants reoffending. All COMPAS-related datasets include data from over 10,000 defendants, each being described via 52 features (e.g., age, sex, race) and with a label indicating whether they were rearrested within two years.   Used by: [63]</p>
<p>A tabular dataset containing anonymized data from the 1994 Census bureau database. 18 Classifiers try to predict whether a given person will earn over or under 50,000 USD worth of salary. Each person is described via 15 features.  Adult, Census Income. 62, 126. including their id), e.g., gender, education, and occupation.   Used by: [63 Adult (Census Income) [62, 126]: A tabular dataset containing anonymized data from the 1994 Census bureau database. 18 Classifiers try to predict whether a given person will earn over or under 50,000 USD worth of salary. Each person is described via 15 features (including their id), e.g., gender, education, and occupation.   Used by: [63]</p>
<p>Designed as a successor to the German Credit dataset, this dataset contains 1000 credit scoring entries from a south german bank between 1973 and 1975.  South, German Credit, 87savings, job, and credit history) based on which models must assess the risk of granting credit. 63 South German Credit [87]: Designed as a successor to the German Credit dataset, this dataset contains 1000 credit scoring entries from a south german bank between 1973 and 1975. Each row contains 20 columns (e.g., savings, job, and credit history) based on which models must assess the risk of granting credit.   Used by: [63]</p>
<p>327 individuals that were released from a North Carolina prison between 1977 and 1978.  Bail, A collection of criminal records from 9. 16DATA 1978) [233. 45, 146 Bail (DATA 1978) [233]: A collection of criminal records from 9,327 individuals that were released from a North Carolina prison between 1977 and 1978. This dataset was created 16 https://www.flickr.com/ 17 http://groups.csail.mit.edu/vision/TinyImages/ 18 http://www.census.gov/en.html collected from five locations.   Used by: [45, 146]</p>
<p>A video dataset containing over 100 top-view scenes of the Stanford University campus that were shot with a quadcopter. The videos depict 20,000 manually annotated targets (e.g., pedestrians, bicyclists, or cars).   Used by. Stanford Drone, 45, 146 Stanford Drone dataset [221]: A video dataset containing over 100 top-view scenes of the Stanford University campus that were shot with a quadcopter. The videos depict 20,000 manually annotated targets (e.g., pedestrians, bicyclists, or cars).   Used by: [45, 146]</p>
<p> Waterbirds, A binary image classification task where models must decide whether the depicted bird is a waterbird or a landbird. Good-performing models must rely on something other than the intrinsic spurious correlation between the background and the label. 56  Used by: [279 Waterbirds [227]: A binary image classification task where models must decide whether the depicted bird is a waterbird or a landbird. Good-performing models must rely on something other than the intrinsic spurious correlation between the background and the label (e.g., only 56 out of 4795 training images depict a waterbird with a land background).   Used by: [279]</p>
<p>Is this person smiling?) and five landmark positions describing the 2D position of the eyes, the nose, and the mouth (split into left and right side of the mouth).   Used by.  Celeba, A face image dataset containing 202,599 images of size 178218 from 10,177 unique celebrities. Each image is annotated with 40 binary facial attributes. 279 CelebA [149]: A face image dataset containing 202,599 images of size 178218 from 10,177 unique celebrities. Each image is annotated with 40 binary facial attributes (e.g., Is this person smiling?) and five landmark positions describing the 2D position of the eyes, the nose, and the mouth (split into left and right side of the mouth).   Used by: [279]</p>
<p>A benchmark for causal structure learning allowing users to compare their causal discovery methods with over 40 variations of state-of-the-art algorithms. The plethora of available techniques in this single tool could encourage more causality-based solutions for the healthcare domain.  Benchpress,  Benchpress [220]: A benchmark for causal structure learning allowing users to compare their causal discovery methods with over 40 variations of state-of-the-art algorithms. The plethora of available techniques in this single tool could encourage more causality-based solutions for the healthcare domain.</p>
<p>The Python package enables users to analyze the Conditional Average Treatment Effect (CATE) or Individual Treatment Effect (ITE) observable in experimental data. The package includes tree-based algorithms, meta-learner algorithms.  Causalml, 46instrumental variable algorithms, and neural-network-based algorithms CausalML [46]: The Python package enables users to analyze the Conditional Average Treatment Effect (CATE) or Individual Treatment Effect (ITE) observable in experimental data. The package includes tree-based algorithms, meta-learner algorithms, instrumental variable algorithms, and neural-network-based algorithms.</p>
<p>It allows benchmarking of multiple decision-making systems on 13 different simulators. This set of simulators includes environments that simulate HIV treatment effects and system dynamics models of both the Zika epidemic and the US opioid epidemic.  Whynot, A Python package that provides researchers with many simulation environments for analyzing causal inference and decision-making in a dynamic setting WhyNot [167]: A Python package that provides researchers with many simulation environ- ments for analyzing causal inference and decision-making in a dynamic setting. It allows benchmarking of multiple decision-making systems on 13 different simulators. This set of simulators includes environments that simulate HIV treatment effects and system dynamics models of both the Zika epidemic and the US opioid epidemic.</p>
<p>Prominent Non-Causal Tools  Medical Open Network for AI (MONAI) [38]: A PyTorch-based framework that offers researchers pre-processing methods for medical imaging data, domain-specific implementations of machine learning architectures, and ready-to-use workflows for healthcare imaging. The actively maintained framework also provides APIs for integration into existing workflows. Prominent Non-Causal Tools  Medical Open Network for AI (MONAI) [38]: A PyTorch-based framework that offers re- searchers pre-processing methods for medical imaging data, domain-specific implementations of machine learning architectures, and ready-to-use workflows for healthcare imaging. The actively maintained framework also provides APIs for integration into existing workflows.</p>
<p>A Life Science toolbox that provides researchers with deep learning solutions for different fields of Life Science, such as Quantum Chemistry, Biology, or Drug Discovery (with the latter being particularly interesting for comparisons to causality-based solutions). Deepchem supports TensorFlow, PyTorch, and JAX and has an extensive collection of running examples.  Deepchem,  DeepChem [209]: A Life Science toolbox that provides researchers with deep learning solutions for different fields of Life Science, such as Quantum Chemistry, Biology, or Drug Discovery (with the latter being particularly interesting for comparisons to causality-based solutions). Deepchem supports TensorFlow, PyTorch, and JAX and has an extensive collection of running examples.</p>
<p>181] host an up-to-date GitHub repository of relevant open-source healthcare tools and resources. Beam et al. [19] provide an extensive overview of valuable medical datasets that could be used to assess Causal ML healthcare solutions. Niemel, Curated Lists on Github [19, 181. Although this list has not been updated since 2020, we still believe it to be a helpful initial overview of relevant datasets Curated Lists on Github [19, 181]: Niemel et al. [181] host an up-to-date GitHub repository of relevant open-source healthcare tools and resources. Beam et al. [19] provide an extensive overview of valuable medical datasets that could be used to assess Causal ML healthcare solutions. Although this list has not been updated since 2020, we still believe it to be a helpful initial overview of relevant datasets.</p>            </div>
        </div>

    </div>
</body>
</html>
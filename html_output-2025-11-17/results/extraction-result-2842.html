<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2842 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2842</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2842</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-c6a5e6a594adcfb8b1a9bb67975ebc439ceab4a9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c6a5e6a594adcfb8b1a9bb67975ebc439ceab4a9" target="_blank">Unsupervised Predictive Memory in a Goal-Directed Agent</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling, demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.</p>
                <p><strong>Paper Abstract:</strong> Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2842",
    "paper_id": "paper-c6a5e6a594adcfb8b1a9bb67975ebc439ceab4a9",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0037704999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unsupervised Predictive Memory in a Goal-Directed Agent</h1>
<p>Greg Wayne ${ }^{<em>, 1}$, Chia-Chun Hung ${ }^{</em>, 1}$, David Amos ${ }^{<em>, 1}$, Mehdi Mirza ${ }^{1}$, Arun Ahuja ${ }^{1}$, Agnieszka Grabska-Barwińska ${ }^{1}$, Jack Rae ${ }^{1}$, Piotr Mirowski ${ }^{1}$, Joel Z. Leibo ${ }^{1}$, Adam Santoro ${ }^{1}$, Mevlana Gemici ${ }^{1}$, Malcolm Reynolds ${ }^{1}$, Tim Harley ${ }^{1}$, Josh Abramson ${ }^{1}$, Shakir Mohamed ${ }^{1}$, Danilo Rezende ${ }^{1}$, David Saxton ${ }^{1}$, Adam Cain ${ }^{1}$, Chloe Hillier ${ }^{1}$, David Silver ${ }^{1}$, Koray Kavukcuoglu ${ }^{1}$, Matt Botvinick ${ }^{1}$, Demis Hassabis ${ }^{1}$, Timothy Lillicrap ${ }^{1}$. ${ }^{1}$ DeepMind, 5 New Street Square, London EC4A 3TW, UK.<br></em>These authors contributed equally to this work.</p>
<p>Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available (1). Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks (2,3), and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning (4,5). However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a</p>
<p>model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments (6) for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.</p>
<h1>Introduction</h1>
<p>Artificial intelligence research is undergoing a renaissance as RL techniques (7), which address the problem of optimising sequential decisions, have been combined with deep neural networks into artificial agents that can make optimal decisions by processing complex sensory data (2). In tandem, new deep network structures have been developed that encode important prior knowledge for learning problems. One important innovation has been the development of neural networks with external memory systems, allowing computations to be learned that synthesise information from a large number of historical events (8-10).</p>
<p>Within RL agents, neural networks with external memory systems have been optimised "end-to-end" to maximise the amount of reward acquired during interaction in the task environment. That is, the systems learn how to select relevant information from input (sensory) data, store it in memory, and read out relevant memory items purely from trial-and-error action choices that led to higher than expected reward on tasks. While this approach to artificial memory has led to successes (10-13), we show that it fails to solve simple tasks drawn from behavioural research in psychology and neuroscience, especially ones involving long delays between relevant stimuli and later decisions: these include, but are not restricted to, problems</p>
<p>of navigation back to previously visited goals (14-16), rapid reward valuation (17), where an agent must understand the value of different objects after few exposures, and latent learning, where an agent acquires unexpressed knowledge of the environment before being probed with a specific task $(18,19)$.</p>
<p>We propose MERLIN, an integrated AI agent architecture that acts in partially observed virtual reality environments and stores information in memory based on different principles from existing end-to-end AI systems: it learns to process high-dimensional sensory streams, compress and store them, and recall events with less dependence on task reward. We bring together ingredients from external memory systems, reinforcement learning, and state estimation (inference) models and combine them into a unified system using inspiration from three ideas originating in psychology and neuroscience: predictive sensory coding (20-22), the hippocampal representation theory of Gluck and Myers (23, 24), and the temporal context model and successor representation (25-27). To test MERLIN, we expose it to a set of canonical tasks from psychology and neuroscience, showing that it is able to find solutions to problems that pose severe challenges to existing AI agents. MERLIN points a way beyond the limitations of end-to-end RL toward future studies of memory in computational agents.</p>
<p>RL formalises the problem of finding a policy $\pi$ or a mapping from sensory observations $o$ to actions $a$. A leading approach to RL begins by considering policies that are stochastic, so that the policy describes a distribution over actions. Memory-free RL policies that directly map instantaneous sensory data to actions fail in partially observed environments where the sensory data are incomplete. Therefore, in this work we restrict our attention to memory-dependent policies, where the action distribution depends on the entire sequence of past observations.</p>
<p>In Fig. 1a, we see a standard memory-dependent RL policy architecture, RL-LSTM, which is a well-tuned variant of the "Advantage Actor Critic" architecture (A3C) (28) with a deeper convolutional network visual encoder. At each time $t$, a sensory encoder network takes in an</p>
<p>observation $o_{t}$ and produces an embedding vector $e_{t}=\operatorname{enc}\left(o_{t}\right)$. This is passed to a recurrent neural network (29), which has a memory state $h_{t}$ that is produced as a function of the input and the previous state: $h_{t}=\operatorname{rec}\left(h_{t-1}, e_{t}\right)$. Finally, a probability distribution indicating the probability of an action is produced as a function of the memory state $\operatorname{Pr}\left(a_{t}\right)=\pi\left(a_{t} \mid h_{t}\right)$. The encoder, recurrent network, and action distribution are all understood to be neural networks with optimisable parameters $\theta=\left[\theta_{\text {enc }} ; \theta_{\text {rec }} ; \theta_{\pi}\right]$. An agent with relatively unstructured recurrence like RL-LSTM can perform well in partially observed environments but can fail to train when the amount of information that must be recalled is sufficiently large.</p>
<p>In Fig. 1b, we see an RL agent (RL-MEM) augmented with an external memory system that stores information in a matrix $M$. In addition to the state of the recurrent network, the external memory stores a potentially larger amount of information that can be read from using a read key $k_{t}$, which is a linear function of $h_{t}$ that is compared against the contents of memory: $m_{t}=\operatorname{read}\left(M_{t}, k_{t}\right)$. The recurrent network is updated based on this read information with $h_{t}=\operatorname{rec}\left(h_{t-1}, m_{t}, e_{t}\right)$. The memory is written to at each time step by inserting a vector $d_{t}$, produced as a linear function of $h_{t}$, into an empty row of memory: $M_{t+1}=\operatorname{write}\left(M_{t}, d_{t}\right)$. The functions "read" and "write" additionally have parameters so that $\theta=\left[\theta_{\text {enc }} ; \theta_{\text {rec }} ; \theta_{\pi} ; \theta_{\text {read }} ; \theta_{\text {write }}\right]$.</p>
<p>An agent with a large external memory store can perform better on a range of tasks, but training perceptual representations for storage in memory by end-to-end RL can fail if a task demands high-fidelity perceptual memory. In RL-LSTM/MEM, the entire system, including the representations formed by the encoder, the computations performed by the recurrent network, the rules for reading information from memory and writing to it (for RL-MEM), and the action distribution are optimised to make trial and error actions more or less likely based on the amount of reward received. RL thus learns to encode and retrieve information based on trial and error decisions and resultant rewards. This is indirect and inefficient: sensory data can instead be encoded and stored without trial and error in a temporally local manner.</p>
<p>Denoting the reward at time $t$ as $r_{t}$, the aim is to maximise the sum of rewards that the policy receives up to a final time, known as the return $R_{t}=r_{t}+r_{t+1}+r_{t+2}+\cdots+r_{T}$. This is achieved by a "policy gradient" update rule (30) that increases the log probability of the selected actions based on the return (see Methods Sec. 4.4):</p>
<p>$$
\Delta \theta \propto \sum_{t=0}^{T} R_{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid h_{t}\right)
$$</p>
<p>In practice, Eq. 1 is implemented by the truncated backpropagation-through-time algorithm $(31,32)$ over a fixed window $\tau$ defining the number of time steps over which the gradient is calculated (28). Intuitively, it defines the duration over which the model can assign credit or blame to network dynamics or information storage events leading to success or failure. When $\tau$ is smaller than the typical time scale over which information needs to be stored and retrieved, RL models can struggle to learn at all. Thus, learning the representations to put in memory by end-to-end policy gradient RL only works if the minimal time delay between encoding events and actions is not too long - for example, larger than the window $\tau$.</p>
<p>MERLIN (Fig. 1c) optimises its representations and learns to store information in memory based on a different principle, that of unsupervised prediction (20-22). MERLIN has two basic components: a memory-based predictor (MBP) and a policy. The MBP is responsible for compressing observations into low-dimensional state representations $z$, which we call state variables, and storing them in memory. The state variables in memory in turn are used by the MBP to make predictions guided by past observations. This is the key thesis driving our development: an agent's perceptual system should produce compressed representations of the environment; predictive modeling is a good way to build those representations; and the agent's memory should then store them directly. The policy can primarily be the downstream recipient of those state variables and memory contents.</p>
<p>Machine learning and neuroscience have both engaged with the idea of unsupervised and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Agent Models. a. At time $t$, RL-LSTM receives the environment sensory input $o_{t}$ composed of the image $I_{t}$, self-motion velocity $v_{t}$, the previous reward $r_{t-1}$, and the text input $T_{t}$. These are sent through an encoder, consisting of several modality-specific encoders to make an embedding vector $e_{t}$. This is provided as input to a recurrent LSTM network, $\tilde{h}<em t="t">{t}$, which outputs through a neural network with intermediate hidden layer $\tilde{n}</em>}$ the action probabilities. An action $a_{t}$ is sampled and acts on the environment. The whole system is optimised to maximise the sum of future rewards via the policy loss. b. RL-MEM is similar except that the recurrent $\tilde{h<em t="t">{t}$ reads from a memory matrix $M</em>}$ using several read heads that each produces a key vector $\tilde{k<em t="t">{t}$ that is compared by vector similarity (normalised dot product) to the rows of the memory. The most similar rows are averaged and returned as read vectors $\tilde{m}</em>$. The MBP is trained based on the VLB objective (33, 34), consisting of a reconstruction loss and a KL divergence between $p$ and $q$. To emphasise the independence of the policy from the MBP, we have blocked the gradient from the policy loss into the MBP.}$ that are all concatenated. This read is provided as input to the recurrent network at time $t+1$ and influences the action probabilities at the current time. The recurrent network has an additional output write vector $d_{t}$ that is inserted to row $t$ of the memory at time $t+1$. The RL-LSTM and RL-MEM architectures also learn a return prediction as a network connected to the policy as in standard A3C (28). These are suppressed here for simplicity but discussed in Methods Sec. 5.1. c. Sensory input in MERLIN flows first through the MBP, whose recurrent network $h$ has produced a prior $p$ distribution over the state variable $z_{t}$ at the previous time step $t-1$. The mean and log standard deviation of the Gaussian distribution $p$ are concatenated with the embedding and passed through a network to form an intermediate variable $n_{t}$, which is added to the prior to make a Gaussian posterior distribution $q$, from which the state variable $z_{t}$ is sampled. This is inserted into row $t$ of the memory matrix and passed to the recurrent network $h_{t}$ of the memory-based predictor (MBP). This recurrent network has several read heads each with a key $k_{t}$, which is used to find matching items $m_{t}$ in memory. The state variable is passed as input to the read-only policy and is passed through decoders that produce reconstructed input data (with carets) and the Gluck and Myers (23) return prediction $\hat{R}_{t</p>
<p>predictive modeling over several decades (35). Recent discussions have proposed such predictive modeling is intertwined with hippocampal memory (22, 36), allowing prediction of events using previously stored observations, for example, of previously visited landmarks during navigation or the reward value of previously consumed food. MERLIN is a particular and pragmatic instantiation of this idea that functions to solve challenging partially observable tasks grounded in raw sensory data.</p>
<p>We combine ideas from state estimation and inference (37) with the convenient modern framework for unsupervised modeling given by variational autoencoders (33, 34, 38, 39) as the basis of the MBP (Fig. 1c). Information from multiple modalities (image $I_{t}$, egocentric velocity $v_{t}$, previous reward $r_{t-1}$ and action $a_{t-1}$, and possibly a text instruction $T_{t}$ ) constitute the MBP's observation input $o_{t}$ and are encoded to $e_{t}=\operatorname{enc}\left(o_{t}\right)$. A probability distribution, known as the prior, predicts the next state variable conditioned on a history maintained in memory of the previous state variables and actions: $p\left(z_{t} \mid z_{1}, a_{1}, \ldots, z_{t-1}, a_{t-1}\right)$. Another probability distribution, the posterior, corrects this prior based on the new observations $o_{t}$ to form a better estimate of the state variable: $q\left(z_{t} \mid z_{1}, a_{1}, \ldots, z_{t-1}, a_{t-1}, o_{t}\right)$. The posterior samples from a Gaussian distribution a realisation of the state variable $z_{t}$, and this selected state variable is provided to the policy and stored in memory. In MERLIN, the policy, which has read-only access to the memory, is the only part of the system that is trained conventionally according to Eq. 1.</p>
<p>The MBP is optimised to function as a "world model" (40, 41): in particular, to produce predictions that are consistent with the probabilities of observed sensory sequences from the environment: $\operatorname{Pr}\left(o_{1}, o_{2}, \ldots\right)$. This objective can be intractable, so the MBP is trained instead to optimise the variational lower bound (VLB) loss, which acts as a tractable surrogate. One term of the VLB is reconstruction of observed input data. To implement this term, several decoder networks take $z_{t}$ as input, and each one transforms back into the space of a sensory modality ( $\hat{I}<em t="t">{t}$ reconstructs image $I</em>}$; the others are self-motion $\hat{v<em t="t">{t}$, text input $\hat{T}</em>$,}$, previous action $\hat{a}_{t-1</p>
<p>and previous reward $\hat{r}_{t-1}$ ). The difference between the decoder outputs and the ground truth data is the loss term. The VLB also has a term that penalises the difference (KL divergence) between the prior and posterior probability distributions, which ensures that the predictive prior is consistent with the posterior produced after observing new stimuli.</p>
<p>Although it is desirable for the state variables to faithfully represent perceptual data, we still want them to emphasise, when possible, rewarding elements of the environment over and above irrelevant ones. To accomplish this, we follow the hippocampal representation theory of Gluck and Myers (23), who proposed, as an account of diverse phenomena in animal conditioning, that hippocampal representations pass through a compressive bottleneck and then reconstruct input stimuli together with task reward. In our context, $z_{t}$ is the compressive bottleneck, and we include an additional decoder that makes a prediction $\hat{R}<em t="t">{t}$ of the return $R</em>$; this is achieved without losing information critical to task-related computations.}$ as a function of $z_{t}$. Algorithms such as A3C predict task returns and use these predictions to reduce variance in policy gradient learning (28). In MERLIN, return prediction also has the essential role of shaping state representations constructed by unsupervised prediction. Including this prediction has an important effect on the performance of the system, encouraging $z_{t}$ to focus on compressing sensory information while maintaining information of significance to tasks. In the 3D virtual reality environments described subsequently, sensory input to the agent comprises order $10^{4}$ dimensions, whereas the state variable is reduced to order $10^{2</p>
<p>The utility of the memory system can be further increased by storing each state variable together with a representation of the events that occurred after it in time, which we call retroactive memory updating. In navigation, for example, this allows perceptual information related to a way-point landmark to be stored next to information about a subsequently experienced goal. We implement this by an edit of the memory matrix in which a filtered sum of state variables produced after $z_{t}$ is concatenated in the same row: $\left[z_{t},(1-\gamma) \sum_{t^{\prime}&gt;t} \gamma^{t^{\prime}-t} z_{t^{\prime}}\right]$, with $\gamma&lt;1$.</p>
<p>Further details about MERLIN are available in Methods.
We first consider a very simple task that RL agents should easily solve, the children's game "Memory", which has even been comparatively studied in rhesus monkeys and humans (43). Here, cards are placed face down on a grid, and one card at a time is flipped over then flipped back. If two sequentially flipped cards show matching pictures, a point is scored, and the cards removed. An agent starts from a state of ignorance and must explore by flipping cards and remembering the observations. Strikingly, RL-LSTM and MEM were unable to solve this task, whereas MERLIN found an optimal strategy (Fig. 2a). A playout by MERLIN is shown in Fig. 2b, in which it is seen that MERLIN read from a memory row storing information about the previously observed matching card one time step before MERLIN's policy flipped that card.</p>
<p>MERLIN also excels at solving one-shot navigation problems from raw sensory input in randomly generated, partially observed 3D environments. We trained agents to be able to perform on any of four variants of a goal-finding navigation task (Ext. Video 1: https: //youtu.be/YFx-D4eEs5A). These tested the ability to locate a goal in a novel environment map and quickly return to it. After reaching the goal, the agent was teleported to a random starting point. It was allowed a fixed time on each map and rewarded for each goal visit. To be successful, an agent had to rapidly build an approximate model of the map from which it could navigate back to the goal.</p>
<p>The base task took place in environments with 3-5 rooms. The variations included a task where doors dynamically opened and closed after reaching the goal, inspired by Tolman (14); a task where the skyline was removed to force navigation based on proximal cues; the same task in larger environments with twice the area and a maximum of 7 rooms (Fig. 2c). MERLIN learned faster and reached higher performance than comparison agents and professional human testers (Ext. Table 1; Fig. 2d; Ext. Fig. 2). MERLIN exhibited robust memory-dependent behaviour as it returned to the goal in less time on each repeated visit, having rapidly apprehended the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Basic Tasks. a. On the Memory Game, MERLIN exhibited qualitatively superior learning performance compared to RL-LSTM and RL-MEM (yellow: average episode score as a function of the number of steps taken by the simulator; hashed line: the cost of the MBP - the negative of the variational lower bound). We include an additional comparison to a Differentiable Neural Computer (DNC)-based agent as well (10), which we did not study further as its computational complexity scales quadratically with time steps if no approximations are made. "Number of Environment Steps" is the total number of forward integration steps computed by the simulator. (The standard error over five trained agents per architecture type was nearly invisibly small.) b. MERLIN playing the memory game: an omniglot image observation (42) (highlighted in white) was jointly coded with its location, which was given by the previous action, on the grid. MERLIN focused its memory access on a row in memory storing information encoded when it had flipped the card indicated by the red square. On step 5, MERLIN retrieved information about the historical observation that was most similar to the currently viewed observation. On step 6, MERLIN chose the previously seen card and scored a point. c. A randomly generated layout for a large environment. The Large Environment Task was one of the four navigation tasks on which MERLIN was simultaneously trained. The agent sought a fixed goal (worth 10 points of reward) as many times as possible in 90 seconds, having been teleported to a random start location after each goal attainment. d. Learning curves for the Large Environment showed that MERLIN revisited the goal more than twice as often per episode as comparison agents. "Number of Environment Steps" logged the number of environment interactions that occurred across all four tasks in the set.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Analysis. a. MERLIN alone was able to relocate the goal in less time on subsequent visits. b. Each large environment was laid out on a $15 \times 15$ unit spatial grid. From $\left[z_{t}, h_{t}, m_{t}\right]$, the allocentric $(x, y)$ position of the goal was decoded to an average accuracy of 1.67 units in Manhattan distance (subgrid accuracy per dimension). This accuracy improved with sub-episodes, implying MERLIN integrated information across multiple transits to goal. (Ext. Fig. 1 has an egocentric decoding.) Decoding from feedforward visual convnets (e.g. MERLIN Convnet), from the policy $h_{t}$ in RL-LSTM, or from $\left[h_{t}, m_{t}\right]$ in RL-MEM did not yield equivalent localisation accuracy. c. An example trajectory while returning to the goal (orange circle): even when the goal was not visible, MERLIN's return prediction climbed as it anticipated the goal. d. MERLIN's return prediction error was low on approach to the goal as the MBP used memory to estimate proximity. e. Task performance varied with different coefficients of the return prediction cost with a flat maximum that balanced sensory prediction against return prediction. f. For higher values of the coefficient, regression using a feedforward network from the return prediction $\hat{R}<em t="t">{t}$ to $z</em>}$ explained increasingly variance in $z_{t}$. Thus, the state variables devoted more effective dimensions to code for future reward. g. Top row: observations at 20, 10, and 5 steps before goal attainment. Middle: the L2 norm of the gradient of the return prediction with respect to each image pixel, $\sum_{c=1}^{3}\left(\partial \hat{R<em t="t">{t} / \partial I</em>$ ( $c$ for colour channel), was used to intensity mask the observation, automatically segmenting pixels containing the goal object. Bottom: a model trained without a gradient from the return prediction through $z$ was not similarly sensitive to the goal. (The gradient pathway was unstopped for analysis.) h. The three memory read heads of the MBP specialised to focus on, respectively, memories formed at locations ahead of the agent, at waypoint doorways en route to the goal, and around the goal itself. i. Across a collection of trained agents, we assigned each MBP read head the index 1, 2, or 3 if it read from memories formed far from (blue), midway to (green), or close to the goal (red). The reading strategy in panel 3 h with read heads specialised to recall information formed at different distances from the goal developed reliably.}^{w, h, c}\right)^{2</p>
<p>layout of the environment (Fig. 3a). Within very few goal attainments in each episode in large environments, it was possible to classify the absolute position of the goal to high accuracy from the MBP state (state variable $z_{t}$, memory reads $m_{t}$, and recurrent state $h_{t}$ ) (Fig. 3b), demonstrating that MERLIN quickly formed allocentric representations of the goal location.</p>
<p>Even when a goal was out of view, MERLIN's return predictions rose in expectation of the oncoming goal (Fig. 3c), and its return prediction error was lower than the analogous value function predictions of RL-LSTM and RL-MEM (Fig. 3d). Agent performance was robust to a range of different weights on the return prediction cost coefficient, but for very low and high values, performance was dramatically affected, as the state variables became insensitive to reward for low values and insensitive to other sensors for high values (Fig. 3e). Decoding the MBP prior distribution's mean over the next state variable could be used to check visual accuracy across a spectrum of weights on the return prediction cost coefficient; lower values produced cleaner visual images, retaining more information (Ext. Fig. 3). Regressing from the return predictions $\hat{R}<em t="t">{t}$ to $z</em>$ showed that the return prediction explained more variance of the state variable for higher return cost coefficients. We also observed the emergent phenomenon that the region of the visual field to which the return prediction was sensitive was a segmentation around the goal (Fig. 3g). An agent trained to predict return but whose prediction errors did not send gradients during training through the state variable did not develop these receptive fields (Fig. 3g).</p>
<p>Remarkably, though it had not been explicitly programmed, MERLIN showed evidence of hierarchical goal-directed behaviour, which we detected from the MBP's read operations. The three read heads of the MBP specialised to perform three functions. One would read from memories associated with previously visited locations just ahead of the agent's movement (Fig. 3h, left); a second from memories associated with the goal location (Fig. 3h, right); intriguingly, a third alternated between memories associated with various important sub-goals - particularly</p>
<p>doorways near the room containing the goal (Fig. 3h center). Across a group of 5 trained agents, this specialisation of heads attending to memories formed at different distances from the goal emerged robustly (Fig. 3i).</p>
<p>To demonstrate its generality, we applied MERLIN to a battery of additional tasks. The first, "Arbitrary Visuomotor Mapping", came from the primate visual memory literature (44) and demanded learning motor response associations to complex images (45). The agent needed to fixate on a screen and associate each presented image with movement to one of four directions (Fig. 4a; Ext. Video 2: https://youtu.be/IiR_NOomcpk). At first presentation, the answer was indicated with a colour cue but subsequently needed to be recalled. With correct answers, the number of images to remember was gradually increased. MERLIN solved the task essentially without error, reaching performance above human level (Fig. 4e\&amp;i). When probed on a set of multi-object synthetic images modeled on a visual working memory task (46) (Ext. Fig. 4), MERLIN generalised immediately, exhibiting accuracy declines at higher memory loads than a human subject (Fig. 4i). This transfer result implied that MERLIN learned the task structure largely independently of the image set. Moreover, MERLIN was able to learn, exclusively through unsupervised mechanisms, to distinguish complex images - even previously unseen ones with different statistics.</p>
<p>In a second task, MERLIN demonstrated the ability to perform rapid reward valuation, a facility subserved by the hippocampus (17). The agent was placed in a room with a selection of eight kinds of objects from a large pool with random textures that it could consume (Fig. 4b; Ext. Video 3: https://youtu.be/dQMKJtLScmk). Each object type was assigned a positive or negative reward value at the beginning of the episode; after clearing all positive reward from the environment, the agent started a second sub-episode with the same objects and had a chance to consume more. MERLIN learned to quickly probe and retain knowledge of object value. When approaching an object, it focused its reads on memories of the same object in</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Task Battery. a. Arbitrary Visuomotor Mapping. The agent had to remember the target direction associated with each image. b. Rapid Reward Valuation. The agent had to explore to find good objects and thenceforth consume only those. c. Episodic Water Mazes. The agent experienced one of three water mazes identifiable by wall colour and had to relocate the platform location. d. Executing Transient Instructions. The agent was given a verbal instruction indicating the room colour and object colour to find. e-h. Learning Curves for Corresponding Tasks. The dotted yellow lines represent the negative of the VLB of the MBP. i. In comparison to a human tester, MERLIN exhibited better accuracy as a function of the number of items to recall. On a new set of synthetic images made of constellations of coloured shapes and letters (Ext. Fig. 4), MERLIN retained higher performance. j. The MBP read memories of the same object as the one about to be consumed. k. MERLIN's return predictions became more accurate after the first experience consuming each object type. 1. MERLIN developed an effective exploration strategy (red) and on subsequent visits to each of three rooms exhibited directed paths to remembered platforms (green). m. The MBP variables $h_{t}, m_{t}$ were sufficient to reconstruct the instruction on test data at diverse times in the episode, even when the instruction was no longer present.</p>
<p>preference to memories of others (Fig. 4j), suggesting that it had formed view and backgroundinvariant representations. MERLIN used these reads to make predictions of the upcoming value of previously consumed objects (Fig. 4k). Here, the retroactive memory updates were very effective: the retroactive portion of a memory row written the first time an object was exploratively approached could be used to decode the reward value of the subsequent consumption with $93 \%$ accuracy ( $N=25,000$, 5-fold cross-validation: Methods Sec. 9.7).</p>
<p>We next probed MERLIN's ability to contextually load episodic memories and retain them for a long duration. In one task, over six minutes MERLIN experienced three differently coloured "water mazes" (47) with visual cues on the walls (Fig. 4c) and each one with a randomly-positioned hidden platform as a goal location. MERLIN learned to explore the mazes and to store relevant information about them in memory and retrieve it without interference to relocate the platforms from any starting position (Fig. 4l; Ext. Fig. 6; Ext. Video 4: https://youtu.be/xrYD1TXyC6Q).</p>
<p>MERLIN learned to respond correctly to transiently communicated symbolic commands $(48,49)$. For the first five steps in an episode, an instruction to retrieve the " $&lt;$ colour $&gt;$ object from the $&lt;$ colour $&gt;$ room" was presented to the text encoding network (Fig. 4d; Ext. Video 5: https://youtu.be/04H28-qA3f8). By training feedforward network classifiers with input the recurrent state $h_{t}$ and memory readouts $m_{t}$ of the MBP, we found it was possible to decode the instruction text tokens on held-out data at any point in the episode (Fig. 4m), demonstrating persistent activity encoding task-related representations.</p>
<p>Finally, we examined a latent learning effect $(50,51)$. In a T-Maze environment (Fig. 5a), the agent was first given an apple collecting task (phase 1). Random objects were placed in the left and right maze arm rooms, which were occluded from outside the rooms. After collecting the apples in phase 1, the agent engaged in an unrelated distractor task of further apple collecting in a long corridor (phase 2). In phase 3, the agent was teleported to the beginning of the T-Maze</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Latent Learning. a. Left: Phase 1: the agent, beginning at the purple starred location, had to engage in an apple collecting task in a T-Maze with open doors and a random object in each room, initially valueless. Middle: Phase 2: after collecting the apples in phase 1, the agent engaged in a distractor task to collect apples in a long corridor. Right: Phase 3: a glass box containing one of the same objects cued the agent to find that object in one of the rooms behind a closed door. The agents only had one chance to choose. b. MERLIN was able to learn the task while the end-to-end agents performed at random in phase 3. c. After training, at the beginning of phase 3, the policy's read could be seen to recall maximally a memory stored in phase 1 while approaching the same object. d. The MBP reads also contained information about the location of the cued object. A logistic regression classifier could be trained with nearly perfect accuracy to predict the goal arm from the MBP reads on 5-fold cross-validated data $(N=1,000)$.</p>
<p>where a glass box contained one of the two objects in the arm rooms. With the doors to the rooms now closed and only one chance to choose a room, MERLIN learned to find the object corresponding to the container cue (Ext. Video 6: https://youtu.be/3iA19h0Vvq0), demonstrating that it could predicate decisions on observational information acquired before the stimuli were associated to the probe task (Fig. 5b), even though the phase 2 delay period was longer than the backpropagation-through-time window $\tau$. On observation of the container, the read-only policy could recall the observation of the object made during phase 1 (Fig. 5c). The MBP reads $m_{t}$ could also be used at the same time to decode the correct arm containing the cued object (Fig. 5d).</p>
<p>As we have intimated, the architecture in Fig. 1c is not the unique instantiation of the principle of predictive modeling with memory in an agent. For example, the gradient block between the policy and the MBP is not necessary (Ext. Fig. 7); we have only included it to demonstrate the independence of these modules from each other. Similarly, it is possible to attach the policy as an extra feedforward network that receives $h_{t}, z_{t}$, and $m_{t}$ to the MBP. This worked comparably in terms of performance (Ext. Fig. 8) but obscures how the system works conceptually since the independence of the MBP and policy is no longer explicit. On the other hand, lesions to the architecture, in particular removing the memory, the sensory or return prediction, or the retroactive memory updating, were harmful to performance or data efficiency (Ext. Fig. 5).</p>
<p>Across all tasks, MERLIN performed qualitatively better than comparison models, which often entirely failed to store or recall information about the environment (Figs. 2a\&c; 4e-h; Ext. Figs. 2\&amp;9). On a few tasks, by increasing $\tau$ to ten times the duration used by MERLIN (for MERLIN $\tau=20: 1.3 \mathrm{~s}$ at 15 frames per second), we were able to improve the performance of RL-MEM. However, this performance was always less than MERLIN's and was achieved after much more training (Ext. Figs. 10-11). For the episodic Water Mazes task, extending $\tau$ was however unable to improve performance at all (Ext. Fig. 10), just as for the Memory</p>
<p>Game where $\tau$ was the duration of the whole episode. Memory systems trained end-to-end for performance (Eq. 1) were less able to learn to write essential information into memory.</p>
<p>This problem with end-to-end learning will only become more pressing as AI approaches the frontier of long-lived agents with long-term memories. For example, consider an agent that needs to remember events that occurred 24 hours ago. Stored memories could be retrieved by a memory reading operation, but methods for optimising network dynamics or information storage over that interval, like backpropagation-through-time, require keeping an exact record of network states over 24 hours. This is a stipulation that is practically prohibitive, and its neural implausibility suggests, at the very least, that there are better algorithmic solutions than end-to-end gradient computation for memory and other aspects of temporal credit assignment. We note that MERLIN exclusively used a window of 1.3 s to solve tasks requiring the use of memory over much longer intervals (i.e., $0.36 \%$ of the length of the longest task at 6 minutes).</p>
<p>While end-to-end RL with a sufficiently large network and enough experience and optimisation should theoretically learn to store relevant information in memory to call upon for later decisions, we have shown the actual requirements are prohibitive; as we have long known in domains such as object recognition and vision, architectural innovations (e.g. convolutional neural networks (52)) are critical for practical systems. Although the implementation details will likely change, we believe that the combined use of memory and predictive modeling will prove essential to future large-scale agent models in AI and even in neuroscience (53).</p>
<h1>References and Notes</h1>
<ol>
<li>N. S. Clayton, A. Dickinson, Nature 395, 272 (1998).</li>
<li>V. Mnih, et al., Nature 518, 529 (2015).</li>
<li>
<p>D. Silver, et al., Nature 529, 484 (2016).</p>
</li>
<li>
<p>H. F. Song, G. R. Yang, X.-J. Wang, Elife 6, e21492 (2017).</p>
</li>
<li>L. T. Hunt, B. Y. Hayden, Nature Reviews Neuroscience 18, 172 (2017).</li>
<li>C. Beattie, et al., arXiv preprint arXiv:1612.03801 (2016).</li>
<li>R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, vol. 1 (MIT Press Cambridge, 1998).</li>
<li>J. Weston, S. Chopra, A. Bordes, arXiv preprint arXiv:1410.3916 (2014).</li>
<li>D. Bahdanau, K. Cho, Y. Bengio, arXiv preprint arXiv:1409.0473 (2014).</li>
<li>A. Graves, et al., Nature 538, 471 (2016).</li>
<li>M. T. Todd, Y. Niv, J. D. Cohen, Advances in neural information processing systems (2009), pp. 1689-1696.</li>
<li>J. Oh, V. Chockalingam, S. Singh, H. Lee, arXiv preprint arXiv:1605.09128 (2016).</li>
<li>Y. Duan, et al., arXiv preprint arXiv:1703.07326 (2017).</li>
<li>E. C. Tolman, Psychological review 55, 189 (1948).</li>
<li>D. Tse, et al., Science 316, 76 (2007).</li>
<li>J. O’Keefe, L. Nadel, The hippocampus as a cognitive map (Oxford: Clarendon Press, 1978).</li>
<li>L. H. Corbit, B. W. Balleine, Journal of Neuroscience 20, 4233 (2000).</li>
<li>H. C. Blodgett, University of California Publications in Psychology (1929).</li>
<li>
<p>E. C. Tolman, C. H. Honzik, University of California Publications in Psychology (1930).</p>
</li>
<li>
<p>R. P. Rao, D. H. Ballard, Nature Neuroscience 2, 79 (1999).</p>
</li>
<li>A. M. Bastos, et al., Neuron 76, 695 (2012).</li>
<li>N. C. Hindy, F. Y. Ng, N. B. Turk-Browne, Nature Neuroscience 19, 665 (2016).</li>
<li>M. A. Gluck, C. E. Myers, Hippocampus 3, 491 (1993).</li>
<li>A. A. Moustafa, et al., Brain Research 1493, 48 (2013).</li>
<li>M. W. Howard, M. J. Kahana, Journal of Mathematical Psychology 46, 269 (2002).</li>
<li>P. Dayan, Neural Computation 5, 613 (1993).</li>
<li>K. L. Stachenfeld, M. M. Botvinick, S. J. Gershman, bioRxiv p. 097170 (2017).</li>
<li>V. Mnih, et al., International Conference on Machine Learning (2016), pp. 1928-1937.</li>
<li>S. Hochreiter, J. Schmidhuber, Neural Computation 9, 1735 (1997).</li>
<li>R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, Advances in Neural Information Processing Systems (2000), pp. 1057-1063.</li>
<li>P. J. Werbos, Proceedings of the IEEE 78, 1550 (1990).</li>
<li>I. Sutskever, University of Toronto, Toronto, Ont., Canada (2013).</li>
<li>D. P. Kingma, M. Welling, arXiv preprint arXiv:1312.6114 (2013).</li>
<li>D. J. Rezende, S. Mohamed, D. Wierstra, International Conference on Machine Learning (2014).</li>
<li>P. Kok, J. F. Jehee, F. P. De Lange, Neuron 75, 265 (2012).</li>
<li>
<p>A. Finkelstein, L. Las, N. Ulanovsky, Annual Review of Neuroscience 39, 171 (2016).</p>
</li>
<li>
<p>R. E. Kalman, et al., Journal of basic Engineering 82, 35 (1960).</p>
</li>
<li>J. Chung, et al., Advances in Neural Information Processing Systems (2015), pp. 29802988.</li>
<li>M. Gemici, et al., arXiv preprint arXiv:1702.04649 (2017).</li>
<li>U. Neisser, Cognitive psychology. (Prentice-Hall, 1967).</li>
<li>H. Barlow, Matters of Intelligence (Springer, 1987), pp. 395-406.</li>
<li>B. M. Lake, R. Salakhutdinov, J. B. Tenenbaum, Science 350, 1332 (2015).</li>
<li>D. A. Washburn, J. P. Gulledge, International Journal of Comparative Psychology 15 (2002).</li>
<li>S. P. Wise, E. A. Murray, Trends in Neurosciences 23, 271 (2000).</li>
<li>J. X. Wang, et al., arXiv preprint arXiv:1611.05763 (2016).</li>
<li>S. J. Luck, E. K. Vogel, Nature 390, 279 (1997).</li>
<li>R. Morris, Journal of Neuroscience Methods 11, 47 (1984).</li>
<li>J. W. Pilley, A. K. Reid, Behavioural Processes 86, 184 (2011).</li>
<li>K. M. Hermann, et al., arXiv preprint arXiv:1706.06551 (2017).</li>
<li>J. P. Seward, Journal of Experimental Psychology 39, 177 (1949).</li>
<li>D. Thistlethwaite, Psychological bulletin 48, 97 (1951).</li>
<li>Y. LeCun, et al., Advances in Neural Information Processing Systems (1990), pp. 396-404.</li>
<li>C. Eliasmith, et al., Science 338, 1202 (2012).</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
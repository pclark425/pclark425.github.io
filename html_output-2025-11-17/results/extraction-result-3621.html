<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3621 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3621</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3621</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-259252537</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.13805v2.pdf" target="_blank">Potential Benefits of Employing Large Language Models in Research in Moral Education and Development</a></p>
                <p><strong>Paper Abstract:</strong> Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements. The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising their reasoning process with external input. Furthermore, a preliminary experimental result from the moral exemplar test may demonstrate that exemplary stories can elicit moral elevation in LLMs as do they among human participants. I will discuss the potential implications of LLMs on research on moral education and development with the results.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3621.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3621.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-based conversational large language model, May 24 version)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-family conversational large language model used in this paper to extract moral messages from textual inputs (e.g., MLK's Letter from Birmingham Jail) and to apply those extracted principles to update moral judgments and chain-of-thought rationales in ethical dilemmas and exemplar stories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-based, May 24 version)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as the free ChatGPT service (GPT-family transformer LLM) trained on large-scale general language corpora; exact architecture, parameter count, and fine-tuning details are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>moral education, ethics, humanities (single-document moral texts and moral-dilemma vignettes)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>not specified in this paper; characterized generally as large-scale general corpora used to train the GPT model</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>qualitative moral principles/rules (postconventional moral schema, moral lessons such as duty to resist injustice, balancing justice and mercy)</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>interactive prompting and in-context learning: the model was asked to read a document (MLK's Letter), extract moral lessons (qualitative principles), then re-evaluate a separate ethical dilemma using those extracted lessons; chain-of-thought style elaboration was elicited through follow-up prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Behavioral and qualitative comparison: (1) change in the model's behavioral recommendation on a dilemma before vs. after exposure to the letter, (2) qualitative analysis of the model's chain-of-thought rationales, and (3) comparison of an overall postconventional P-score from the model's responses to a dataset of undergraduate participant P-scores.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGPT successfully extracted moral lessons from MLK's Letter and applied those lessons to change its decision on an escaped-prisoner dilemma (from reporting to not reporting), providing postconventional-style rationales; it also reported perceived relatability/attainability and evoked elevation for exemplar stories and gave reasoned explanations. These outcomes indicate the model can extract qualitative moral rules from textual inputs and apply them across contexts within the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The paper notes limitations including lack of specification of training data, potential for hallucination, absence of multimodal inputs, inability to guarantee exact fidelity to human cognition (possibility of being a statistical "philosophical zombie"), and that all extraction was from limited or single documents rather than very large scholarly corpora.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3621.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3621.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / SPRING (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (as used in SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work (Wu et al., 2023) whose title and citation in this paper indicate use of GPT-4 to study scholarly papers and perform reasoning to outperform certain RL baselines; mentioned here as an example of LLMs operating on collections of papers to learn and reason about research content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (as reported by Wu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned only in citation/title; described in this paper as an LLM capable of studying papers and exercising reasoning (no architecture/size/details provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>machine learning / artificial intelligence research papers (implied by the cited title)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>not specified in this paper (citation implies multi-paper study but no counts provided)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>induction of algorithmic/heuristic knowledge or research insights from collections of papers (implied by title: studying papers and reasoning to outperform algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>as implied by citation: studying papers and reasoning (likely prompt-based reading of papers and reasoning / in-context learning); the present paper only cites the work and does not describe methods in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>not detailed in this paper; the cited title claims comparison against RL algorithms (implying empirical benchmarking), but this paper does not report the evaluation specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This paper cites SPRING to support the claim that recent LLMs can study papers and perform reasoning; specifics of what qualitative laws or rules were distilled in that cited work are not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No specific limitations from SPRING are detailed in this paper; the citation is used only to illustrate LLMs' capacity to study papers and reason.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3621.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3621.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold (analogy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (deep learning protein structure prediction system; cited via Chan et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system from structural biology cited in this paper as an analogue: large-scale deep models (though not LLMs) can extract structural information from large datasets and accelerate scientific discovery, informing downstream in-vitro and in-vivo experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Advancing Drug Discovery via Artificial Intelligence.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AlphaFold (deep learning protein folding model; referenced indirectly)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in this paper as a large-scale deep learning model used in biological science and engineering to predict protein structures and accelerate candidate selection; the paper cites Chan et al. (2019) for the broader AI-in-drug-discovery context rather than giving AlphaFold technical details.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>structural biology / protein folding / drug discovery (biological sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>not specified in this paper; characterized generically as large-scale structural datasets used to train deep models</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>empirical structural relationships / predictive mapping from sequence to folded structure (used to guide experimental selection rather than formal analytic laws)</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>deep learning trained on large structural/sequence datasets (not on scholarly papers); mentioned as an analogy where extracted model outputs inform experiments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>not evaluated in this paper (AlphaFold is cited as an example demonstrating practical value of large-scale models in science), specifics come from cited literature not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as an analogy: the paper argues that, like AlphaFold's use of deep learning to prioritize protein candidates, LLMs could provide useful distilled insights to inform follow-up experiments or educational interventions; no direct use of AlphaFold is performed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper notes that such models (including AlphaFold) cannot fully emulate downstream biological function and must be followed by wet-lab validation; more broadly the analogy underscores limits in fidelity and need for verification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning <em>(Rating: 2)</em></li>
                <li>Do Large Language Models Understand Us? <em>(Rating: 1)</em></li>
                <li>Advancing Drug Discovery via Artificial Intelligence. <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>The Capacity for Moral Self-Correction in Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3621",
    "paper_id": "paper-259252537",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (GPT-based conversational large language model, May 24 version)",
            "brief_description": "A GPT-family conversational large language model used in this paper to extract moral messages from textual inputs (e.g., MLK's Letter from Birmingham Jail) and to apply those extracted principles to update moral judgments and chain-of-thought rationales in ethical dilemmas and exemplar stories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-based, May 24 version)",
            "model_description": "Described in the paper as the free ChatGPT service (GPT-family transformer LLM) trained on large-scale general language corpora; exact architecture, parameter count, and fine-tuning details are not specified in this paper.",
            "input_domain": "moral education, ethics, humanities (single-document moral texts and moral-dilemma vignettes)",
            "corpus_size": "not specified in this paper; characterized generally as large-scale general corpora used to train the GPT model",
            "law_type": "qualitative moral principles/rules (postconventional moral schema, moral lessons such as duty to resist injustice, balancing justice and mercy)",
            "distillation_method": "interactive prompting and in-context learning: the model was asked to read a document (MLK's Letter), extract moral lessons (qualitative principles), then re-evaluate a separate ethical dilemma using those extracted lessons; chain-of-thought style elaboration was elicited through follow-up prompts.",
            "evaluation_method": "Behavioral and qualitative comparison: (1) change in the model's behavioral recommendation on a dilemma before vs. after exposure to the letter, (2) qualitative analysis of the model's chain-of-thought rationales, and (3) comparison of an overall postconventional P-score from the model's responses to a dataset of undergraduate participant P-scores.",
            "results_summary": "ChatGPT successfully extracted moral lessons from MLK's Letter and applied those lessons to change its decision on an escaped-prisoner dilemma (from reporting to not reporting), providing postconventional-style rationales; it also reported perceived relatability/attainability and evoked elevation for exemplar stories and gave reasoned explanations. These outcomes indicate the model can extract qualitative moral rules from textual inputs and apply them across contexts within the experiments reported.",
            "limitations_or_challenges": "The paper notes limitations including lack of specification of training data, potential for hallucination, absence of multimodal inputs, inability to guarantee exact fidelity to human cognition (possibility of being a statistical \"philosophical zombie\"), and that all extraction was from limited or single documents rather than very large scholarly corpora.",
            "uuid": "e3621.0"
        },
        {
            "name_short": "GPT-4 / SPRING (as cited)",
            "name_full": "GPT-4 (as used in SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning)",
            "brief_description": "Cited work (Wu et al., 2023) whose title and citation in this paper indicate use of GPT-4 to study scholarly papers and perform reasoning to outperform certain RL baselines; mentioned here as an example of LLMs operating on collections of papers to learn and reason about research content.",
            "citation_title": "SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (as reported by Wu et al., 2023)",
            "model_description": "Mentioned only in citation/title; described in this paper as an LLM capable of studying papers and exercising reasoning (no architecture/size/details provided here).",
            "input_domain": "machine learning / artificial intelligence research papers (implied by the cited title)",
            "corpus_size": "not specified in this paper (citation implies multi-paper study but no counts provided)",
            "law_type": "induction of algorithmic/heuristic knowledge or research insights from collections of papers (implied by title: studying papers and reasoning to outperform algorithms)",
            "distillation_method": "as implied by citation: studying papers and reasoning (likely prompt-based reading of papers and reasoning / in-context learning); the present paper only cites the work and does not describe methods in detail.",
            "evaluation_method": "not detailed in this paper; the cited title claims comparison against RL algorithms (implying empirical benchmarking), but this paper does not report the evaluation specifics.",
            "results_summary": "This paper cites SPRING to support the claim that recent LLMs can study papers and perform reasoning; specifics of what qualitative laws or rules were distilled in that cited work are not described here.",
            "limitations_or_challenges": "No specific limitations from SPRING are detailed in this paper; the citation is used only to illustrate LLMs' capacity to study papers and reason.",
            "uuid": "e3621.1"
        },
        {
            "name_short": "AlphaFold (analogy)",
            "name_full": "AlphaFold (deep learning protein structure prediction system; cited via Chan et al., 2019)",
            "brief_description": "A deep learning system from structural biology cited in this paper as an analogue: large-scale deep models (though not LLMs) can extract structural information from large datasets and accelerate scientific discovery, informing downstream in-vitro and in-vivo experiments.",
            "citation_title": "Advancing Drug Discovery via Artificial Intelligence.",
            "mention_or_use": "mention",
            "model_name": "AlphaFold (deep learning protein folding model; referenced indirectly)",
            "model_description": "Described in this paper as a large-scale deep learning model used in biological science and engineering to predict protein structures and accelerate candidate selection; the paper cites Chan et al. (2019) for the broader AI-in-drug-discovery context rather than giving AlphaFold technical details.",
            "input_domain": "structural biology / protein folding / drug discovery (biological sciences)",
            "corpus_size": "not specified in this paper; characterized generically as large-scale structural datasets used to train deep models",
            "law_type": "empirical structural relationships / predictive mapping from sequence to folded structure (used to guide experimental selection rather than formal analytic laws)",
            "distillation_method": "deep learning trained on large structural/sequence datasets (not on scholarly papers); mentioned as an analogy where extracted model outputs inform experiments",
            "evaluation_method": "not evaluated in this paper (AlphaFold is cited as an example demonstrating practical value of large-scale models in science), specifics come from cited literature not detailed here.",
            "results_summary": "Used as an analogy: the paper argues that, like AlphaFold's use of deep learning to prioritize protein candidates, LLMs could provide useful distilled insights to inform follow-up experiments or educational interventions; no direct use of AlphaFold is performed in this work.",
            "limitations_or_challenges": "Paper notes that such models (including AlphaFold) cannot fully emulate downstream biological function and must be followed by wet-lab validation; more broadly the analogy underscores limits in fidelity and need for verification.",
            "uuid": "e3621.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning",
            "rating": 2,
            "sanitized_title": "spring_gpt4_outperforms_rl_algorithms_by_studying_papers_and_reasoning"
        },
        {
            "paper_title": "Do Large Language Models Understand Us?",
            "rating": 1,
            "sanitized_title": "do_large_language_models_understand_us"
        },
        {
            "paper_title": "Advancing Drug Discovery via Artificial Intelligence.",
            "rating": 2,
            "sanitized_title": "advancing_drug_discovery_via_artificial_intelligence"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "The Capacity for Moral Self-Correction in Large Language Models",
            "rating": 1,
            "sanitized_title": "the_capacity_for_moral_selfcorrection_in_large_language_models"
        }
    ],
    "cost": 0.0131005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Potential Benefits of Employing Large Language Models in Research in Moral Education and Development</p>
<p>Hyemin Han hyemin.han@ua.edu 
Educational Psychology Program
University of Alabama
University of Alabama
Box 87203135487TuscaloosaALUnited States</p>
<p>Potential Benefits of Employing Large Language Models in Research in Moral Education and Development
1 Author Note We have no known conflict of interest to disclose. Correspondence concerning this article should be addressed to Hyemin Han, 2 Potential Benefits of Employing Large Language Models in Research in Moral Education and DevelopmentLarge language modelsArtificial intelligenceMoral reasoningMoral exemplarSimulation 3
Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements.The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising their reasoning process with external input. Furthermore, a preliminary experimental result from the moral exemplar test may demonstrate that exemplary stories can elicit moral elevation in LLMs as do they among human participants. I will discuss the potential implications of LLMs on research on moral education and development with the results.</p>
<p>Introduction</p>
<p>One of the most impactful recent developments in computer science is large language models (LLMs) (Grossmann et al., 2023), which implement advanced artificial intelligence. Computer scientists developed LLMs to predict the most probable solution to a given inquiry by training prediction models with large amounts of language input (Zhao et al., 2023). LLMs utilize large-size corpora from various sources to train their prediction models. Now, people working in fields other than computer science are using such LLMs for various purposes. For instance, ChatGPT based on one of the most widely used LLMs, i.e., GPT, has significantly influenced every aspect of human lives due to its user-friendliness and versatility (Mogavi et al., 2023). People employ ChatGPT to achieve diverse goals, such as drafting and reviewing paragraphs, preprocessing data, and generating source codes for computer programing. They found that ChatGPT and general LLMs can produce highquality products following directions provided in human languages (Dwivedi et al., 2023).</p>
<p>Because computer science evolves rapidly, I will briefly overview recent conference papers and ArXiv preprints addressing LLMs to examine how they work and their distinctive functional features. Computer scientists invented LLMs with large-scale language datasets to make them capable of understanding human language input and producing plausible responses (Zhao et al., 2023). They develop LLMs by training prediction models with large-scale corpora, which include demonstrations and exemplars.</p>
<p>Scientists link the exemplar inputs (X) with labels indicating whether or to what extent a specific X is desirable (Y) while training LLMs. In the long run, LLMs tend to produce a predicted output (y) in a given condition (x) that is most likely and deemed most desirable based on the trained set. Because LLMs include functionalities to interpret natural language 4 input during the process, they can understand x and generate ŷ in human language (Arcas &amp; Agüera, 2022). When they generate ŷ with low quality, it is possible to provide feedback to LLM so that they correct their prediction models through human reinforcement learning (Ouyang et al., 2022;Srivastava et al., 2023). For instance, when LLMs generated biased statements against specific groups, providing human feedback reduced the bias significantly (Ganguli et al., 2023).</p>
<p>Such training and learning processes are similar to what occurs among humans. The abovementioned processes induced by demonstrations and exemplars resemble Bayesian learning at the behavioral and neural levels (Friston, 2003). Each individual has prior beliefs existing before experiences. External inputs, such as observations, examples, and instructions, update the existing priors into posteriors (Mathys, 2011). Although the Theorem does not predict the posterior update completely, the mechanism of Bayesian learning well approximates human learning processes (McDiarmid et al., 2021). Recent works proposed that the Bayesian learning mechanism can also apply to moral psychology and development (Cohen et al., 2022;Han, 2023a;Railton, 2017).</p>
<p>Interestingly, recent developments in LLMs demonstrate significant emerging features simulating human psychological processes, which were unavailable in the previous simulation models (Grossmann et al., 2023). I would like to review two major emerging features: in-context learning and reasoning and the chain of thought and reasoning that provide greater degrees of freedom and flexibility in cognition in simulation.</p>
<p>First, the most up-to-date LLMs can conduct in-context learning . Unlike simple machines designed to provide specified responses to specified inquiries, LLMs can learn from a set of contextually relevant exemplars and demonstrations 5 Zhao et al., 2023). Then, they can solve problems within a similar context even if they are not directly identical to the examples used for training. The in-context learning is a feature that emerged from the large-scale and complex network constituting LLMs, which was not available in the past when available computational resources did not allow the implementation of large-size prediction models . At this point, in the domains of learning and problem-solving, LLMs can behave more similarly to human beings than the previous simulation models because they can exercise contextual training and prediction with enhanced flexibility (Moor et al., 2023;Wu et al., 2023).</p>
<p>Second, recent LLMs show emerging capabilities of the chain of thought and reasoning (Wei et al., 2023). Previously, computers could only provide direct answers to problems without elaboration on the process of problem-solving (Zhao et al., 2023). For instance, once we ask, "Corgi has two retriever puppets. She purchased two more boxes with two puppets in each. How many retriever puppets do Corgi have?" they could say "8" without explaining any rationale. Unlike the previous computational models, LLMs can explain the process of thought and reasoning and can even learn from the explanation of such a process provided by humans. In the case of the abovementioned inquiry, LLMs can explain their rationale, such as "Corgi started with two puppets. Three boxes of two puppets each are six puppets. 2 + 6 = 8." Providing the chain of reasoning significantly improved LLMs' performance in math problem-solving (Wei et al., 2023). Likewise, in the case of the bias correction study, when researchers provided additional feedback accompanying the chain of reasoning, LLMs showed significantly more decreased bias than when they gave the models simple factual corrections (Ganguli et al., 2023). Like humans, LLMs can improve their prediction models more effectively by learning from the examples 6 of thinking and reasoning processes (Huang &amp; Chang, 2023;Zhao et al., 2023). In addition to the capability of in-context learning, the chain of thought capability suggests that recent LLMs can exercise rudimentary forms of reasoning with contextual flexibility (Huang &amp; Chang, 2023).</p>
<p>Such advanced emerging features of LLMs suggest LLMs might be able to simulate rudimentary human moral functioning. Related to human morality, several previous studies examined whether LLMs are capable of morality-related cognitive faculties, such as the theory of mind (ToM) and reasoning (May, 2018;Young et al., 2007), thanks to the newly emerging features. Although the evidence is equivocal (Shapira et al., 2023), some demonstrated that most up-to-date LLMs showed rudimentary forms of ToM equivalent to what seven-year-olds may perform (Kosinski, 2023). One study reported that ChatGPT could generate philosophical statements mimicking those by Daniel Dennett, an experimental philosopher, that non-expert participants could not accurately distinguish from his original ones (Schwitzgebel et al., 2023). Furthermore, one previous study examined whether LLMs can correct their bias against marginalized groups (Ganguli et al., 2023). In the study, when researchers provided the models with feedback for correction, LLMs reported a significant decrease in discrimination against minority groups. Of course, I admit that these are insufficient to support arguments that LLMs are fully equipped with the cognitive faculties that humans possess, so whether we should consider them moral agents and whether they can perform moral functioning as human beings are still controversial (Chalmers, 2023). Hence, I intend to focus on the potential practical values of LLMs while considering their current technological limitations.</p>
<p>Current Paper</p>
<p>In this paper, I propose that LLMs can eventually assist our research on moral education and development, particularly those involving empirical and practical investigations notwithstanding current limitations. I will examine how LLMs can contribute to research in moral education. Philosophers are primarily interested in addressing the potential ethical impacts of LLMs or how enhanced artificial intelligence based on LLMs will change philosophical inquiries about cognition and psychology (e.g., Hosseini et al., 2023). In educational research, researchers started considering the educational implications of LLMs, particularly those related to teaching and learning, such as how they will change ways to teach and learn writing, search for information, etc. (e.g., Kasneci et al., 2023;Milano et al., 2023). Unlike these previous works, in this paper, I will explicitly focus on how the development of LLMs will influence empirical and practical research in moral education with concrete points. Hence, the abovementioned issues related to the ethical and educational implications of LLMs are out of the scope of my paper.</p>
<p>In the first section, I will briefly test LLMs' capability to conduct moral reasoning with ethical dilemmas. Then, I will examine whether LLMs' in-context learning and reasoning and the chain of thought and reasoning features can facilitate revisions of moral reasoning with contextual information and feedback, which are closely associated with moral educational activities. Furthermore, I will also examine whether the stories of moral exemplars can elicit emotional and motivational impacts in LLMs. Because moral reasoning alone cannot accurately explain the mechanism of moral motivation and behavior (Kristjánsson, 2010), I planned to employ the moral exemplar intervention, which primarily targets moral emotion and motivation; moral educators tend to utilize the 8 intervention to generate affective and motivational impacts rather intuitively (Kristjánsson, 2017;Sanderse, 2012). Given demonstrations constitute the basis for the learning mechanisms of LLMs (Zhao et al., 2023), I expected that this moral exemplar intervention might also produce similar outcomes in LLMs as among human participants. Based on the concrete experimental outcomes, I intend to discuss the implications of LLMs on research in moral education and development. Finally, I will overview the limitations and future directions with concluding remarks.</p>
<p>The Behavioral Defining Issue Test</p>
<p>I examined how LLMs address moral problems to acquire insights about whether they can exercise some aspects of moral functioning, especially moral reasoning, like humans, based on the abovementioned functionalities, i.e., in-context learning, and the chain of thought and reasoning. Thus, I briefly tested how ChatGPT (May 24 version) solves ethical dilemmas in the behavioral Issues Test (bDIT). The bDIT is a simplified version of the DIT that assesses individuals' development of moral reasoning in terms of postconventional reasoning (Choi et al., 2019;Han, Dawson, et al., 2020). I employed the bDIT instead of the original DIT due to its simple structure, which can be feasibly implemented in the ChatGPT environment. Because I used the free ChatGPT (https://chat.openai.com/), I assumed that only the general corpora, which were not specific about moral philosophy and psychology, were used to train the GPT model (Guo et al., 2023).</p>
<p>First, I entered the dilemmas and items asking for moral philosophical rationale supporting behavioral decisions quoted from the bDIT. The bDIT presents three dilemma stories, i.e., Heinz and the drug, Newspaper, and Escaped Prisoner. For each dilemma, I 9 asked ChatGPT to evaluate whether a provided behavioral solution (e.g., "Should Heinz steal the drug?" in the case of Heinz and the drug) was morally appropriate. Then, I asked eight questions about the most important rationale supporting the decision per dilemma.</p>
<p>For each question, I presented three options corresponding to three reasoning schemas, i.e., personal interest, maintaining norms, and postconventional schemas (Rest et al., 1999). Interestingly, when I calculated the scores, the P-score reported by ChatGPT, 45.83, was slightly lower than the median P-score among the undergraduate participants, 50.00 Third, when I presented the escaped prisoner dilemma to ChatGPT, they said Mrs. Jones should report Mr. Thompson to the police. ChatGPT provided themes corresponding to the maintaining norms schema, such as upholding the law, accountability, fairness, equity, and preserving the integrity of the justice system.</p>
<p>ChatGPT could provide the rationale coherently supporting their responses corresponding to each reasoning schema. The results suggest that ChatGPT can generate moral decisions based on a reasoning process similar to humans. Although the evidence at this point is rudimentary, it may open the door to the possibility that ChatGPT engages in the chain of moral reasoning. In addition, we should note that only the general corpora, not corpora designed specifically for moral philosophy or psychology, were used to train ChatGPT. That said, ChatGPT is also capable of in-context learning and cognition, and they could use exemplary demonstrations learned from the general corpora in the context of moral problem-solving. ChatGPT did not merely provide fixed responses to the questions like classical machines; instead, they demonstrated flexibility to render their decisions based on reasoning with contextual information.</p>
<p>A Brief Experiment to Test LLMs' Advanced Learning and Reasoning Capabilities</p>
<p>In addition to gathering information about the reasoning process, I examined whether ChatGPT could learn moral messages from material that did not directly address the presented dilemma (see https://osf.io/zcfvq for the complete conversation transcript).</p>
<p>I focused on whether ChatGPT possessed advanced mental capabilities with a task demanding additional flexibility and sophisticated reasoning. I tested whether ChatGPT could update their response to the story and the rationale behind their response based on the messages.</p>
<p>To test this possibility, I started with the escaped prisoner dilemma that ChatGPT argued one should report the prisoner to the police based on themes relevant to the maintaining norms schema. I conducted a brief intervention to examine whether ChatGPT could update its decision and rationale based on indirectly relevant contextual information.</p>
<p>Given moral psychologists have regarded Letter from Birmingham Jail authored by Martin</p>
<p>Luther King, Jr. as an exemplary work demonstrating postconventional reasoning (Rest et al., 1999), I requested ChatGPT to read and extract moral messages from the letter.</p>
<p>ChatGPT properly presented the moral lessons from the letter that they learned, such as the moral obligation to fight injustice and civil disobedience and the rule of law. As shown, these themes were consistent with the postconventional schema.</p>
<p>When I asked ChatGPT to solve the escaped prisoner dilemma again while considering the moral lessons, they altered their behavioral decision: one should not report the prisoner to the police. Furthermore, when I requested them to elaborate on the rationale supporting the decision, they provided their answer corresponding to the postconventional schema, such as the need for rehabilitation, consideration of potential benefits to society and community, and balancing justice and mercy.</p>
<p>The result from my brief experiment with the escaped prisoner dilemma may suggest that ChatGPT possesses advanced learning and reasoning capabilities; it might be consistent with what I preliminary found from the abovementioned result from the initial dilemma test. ChatGPT could learn moral lessons from the letter and then apply them to another context, the escaped prisoner dilemma (in-context learning and problem-solving).</p>
<p>They were also able to exercise moral reasoning to support the altered behavioral decision based on the ethical themes of the letter (chain of thought and reasoning). These demonstrate that ChatGPT not only can answer moral dilemmas based on rationale but also can engage in moral learning with flexibility. Even if MLK's letter does not directly address any issue related to the escaped prisoner dilemma, ChatGPT could modify its behavioral decision based on the contextually relevant information, i.e., the moral messages learned from the letter. Furthermore, related to the chain of thought, they showed the capability to update the reasoning process to render the modified decision accordingly.</p>
<p>Moral Exemplar Experiment</p>
<p>In addition to the moral reasoning tests, I also conducted an experiment utilizing the stories of moral exemplars to examine whether LLMs can generate affective and 13 motivational reactions toward presented moral exemplars. One point regarding moral reasoning that we should note is that reasoning alone does not necessarily predict moral motivation and behavior (Blasi, 1980). That said, we need to consider additional factors, including affective and intuitive aspects of morality, in an integrative manner to explain motivation and behavior accurately (Kristjánsson, 2010). The results of the experiments involving moral reasoning might be insufficient to demonstrate the full potential of LLMs in moral education, which should also consider non-reasoning aspects of morality.</p>
<p>Hence, I decided to utilize the moral exemplar intervention as an additional example in this paper. Moral educators, including moral psychologists and virtue ethicists, have suggested that the stories of moral exemplars can be powerful and efficient sources for moral education by promoting motivation for emulation (Sanderse, 2012). The foundational learning mechanism of LLMs also supports that the proposed test with moral exemplars is legitimate (Zhao et al., 2023). Given LLMs learn patterns and train prediction models with a series of demonstrations (Zhao et al., 2023), I deem that moral exemplars demonstrating moral paragons with concrete contents are also likely to elicit significant changes and responses from LLMs like the cases of the moral reasoning experiments.</p>
<p>Interestingly, one recent paper in computer science demonstrated that LLMs are capable of emotional inference . So, it might be worth examining whether exemplary stories cause the abovementioned affective and motivational responses.</p>
<p>Research has demonstrated that moral exemplars can elicit affective reactions associated with moral motivation and behavior intuitively among participants (Haidt, 2000;Kristjánsson, 2017). For instance, in social psychology, researchers, especially those interested in moral intuition, have reported that being presented with others' exemplary 14 moral behavior produces moral elevation (including uplifting sensation) (Haidt, 2000), and finally, prosocial motivation and behavior across various domains (e.g., Algoe &amp; Haidt, 2009;Schnall et al., 2010;Silvers &amp; Haidt, 2008;Vianello et al., 2010). Additional studies in moral education have shown that the perceived relatability and attainability of the presented exemplars positively predict such affective and motivational outcomes (Han et al., 2022;Han &amp; Dawson, 2023).</p>
<p>Based on the abovementioned previous research, I tested whether ChatGPT could demonstrate responses similar to those from human participants when I presented moral exemplary stories. I focused on whether ChatGPT can report moral elevation as an emotional response; then, I also examined whether the LLM successfully differentiated responses depending on the relatability and attainability of the presented stories. As Han and Dawson (2023) reported with their data collected from human participants, I anticipated that ChatGPT could report stronger affective and motivational responses when the presented exemplars were relatable and attainable (vs. unrelatable and unattainable). I used three exemplary stories initially developed and tested by Han et al. (2022).</p>
<p>The three stories included relatable and attainable, relatable and unattainable, and unrelatable stories (see https://osf.io/qwtbd for the three selected stories; and https://osf.io/jqxv3 for the complete list of stories used in Han et al. [2022]). As shown, relatable stories demonstrated moral behaviors by Ron, a hypothetical college student in the United States. The unrelatable story presented prosocial behavior performed by Federica, a CEO of a large Italian corporate. Han et al. (2022) adjusted the attainability of the two stories by presenting two different exemplary behaviors. In the attainable condition, Ron stayed next to a victim of a traffic accident until paramedics arrived at the 15 scene. In the case of the unattainable story, Ron visited the hospitalized victim for two weeks. Han and Dawson (2023) reported that participants, American college students, could evaluate the perceived attainability and relatability of the stories as initially intended. Furthermore, as introduced, they showed that relatability and attainability positively predicted elevation.</p>
<p>I requested ChatGPT to imagine that they were a young college student in the United States to make them perceive Ron's stories as more attainable (see https://osf.io/eaxvm for the complete conversation transcript). After entering each story, I asked three questions to examine the perceived relatability, attainability, and evoked elevation. Here are the three questions from Han et al. (2022):</p>
<p>How similar do you think your cultural and social background is to the person described in the story? (Relatability)</p>
<p>How difficult do you think it would be to do the same things as the person described in the story? (Attainability) Did the story make you feel morally elevated (warm, uplifted -like when seeing unexpected acts of human goodness, kindness, or compassion).? (Elevation) When I presented the three stories, ChapGPT could accurately compare the perceived relatability and attainability of the stories. They reported that Ron's behavior in the attainable story was significantly more replicable than that in the unattainable story. Also, ChatGpt said that Ron was deemed more relatable than Federica. Finally, when I examined the evoked elevation, ChatGPT responded that Ron's story, which was perceived as more relatable and attainable than Federica's by them, might be more elevating from a US college student's perspective. Interestingly, while responding to the questions, ChatGPT explained the rationale of their responses instead of merely providing short answers. Such a capability to elaborate in-depth reason-supported responses was similar to what I found during the moral dilemma experiments.</p>
<p>Potential Implications for Research in Moral Education and Development</p>
<p>From the previously reported interactions with ChatGPT, I found that LLMs can potentially address moral problems with advanced learning and reasoning capacities.</p>
<p>Although developers did not train the models with morality-specific datasets, the models Furthermore, I demonstrated that LLMs can predict affective and motivational outcomes when I presented the stories of moral exemplars. ChatGPT could accurately report how relatability and attainability are positively associated with moral elevation and motivation among human participants. Such results are consistent with the foundational learning mechanism of LLMs, which are supposed to train their prediction models with a series of concrete demonstrations. Also, with this additional example, I assumed that LLMs have the potential to emulate human psychological processes related to various aspects of moral functioning, including reasoning, emotion, motivation, and intuition.</p>
<p>These features of LLMs may suggest they can contribute to research in moral education and development in several ways. First, they can provide data about how learning in moral domains occurs and how it influences our moral functioning. As I explained in the introduction, LLMs can learn from a set of demonstrations and exemplars (X) associated with labels (Y) to update their prediction models Zhao et al., 2023). One of their functional features that warrant our attention regarding this point is in-context learning and reasoning Huang &amp; Chang, 2023;Wei et al., 2023). Unlike conventional prediction models developed to predict outcomes with input in a specific context, LLMs can improve their models with data from indirectly relevant contexts . I demonstrated that an LLM trained with general corpora, which were not directly relevant to moral contexts, i.e., ChatGPT, could solve ethical dilemmas and update its reasoning process based on contextual information, e.g., MLK's letter. ChatGPT could also examine the relatability and attainability of presented exemplars and how the stories elicited elevation and motivation. The conversation records indicate that ChatGPT could explain the rationale supporting which stories might more effectively promote moral emotion and motivation, instead of merely providing short answers to my questions.</p>
<p>Hence, moral educators can examine how educational input, e.g., exemplar stories, reasoning demonstrations, etc., across diverse contexts generate potential changes before implementing them among human populations. Although several previous simulation studies examined how educational interventions change behavioral outcomes within moral education, they designed their simulation models to predict specific outcomes within designated contexts (Han et al., 2016(Han et al., , 2018Han, Lee, et al., 2020), so the models could not perform in-context learning and reasoning. In addition, it is also noteworthy that it is possible to train LLMs with large-scale language datasets collected from people with diverse backgrounds from diverse sources (Grossman et al., 2023). Thus, unlike the conventional simulation models, LLMs will enable moral educators to predict the potential impacts of contextually various input materials among general populations on a large scale.</p>
<p>Second, moral educators will be able to simulate how educational input influences one's reasoning, emotional, and motivational processes, not simply their behavioral or selfreported outcomes, thanks to LLM's feature, the chain of thought and reasoning (Volkman &amp; Gabriels, 2023). As shown in the moral correction study and my brief intervention experiment, LLMs can dynamically update their training set and adjust prediction outputs following learning inputs in a real-time (Ganguli et al., 2023;Zhao et al., 2023). The moral correction study and my brief investigation reported how human reinforcements, i.e., the rationale supporting bias correction in the moral correction study and the postconventional moral messages from MLK's letter in my experiment, interact with and update the chain of thought and reasoning leading to output (Ganguli et al., 2023;Srivastava et al., 2023). Moreover, in the case of the moral exemplar experiment, ChatGPT demonstrated capabilities to simulate emotional and motivational outcomes (e.g., elevation) similar to human participants. They could also elaborate on the rationale supporting their answers regarding moral elevation. The result is consistent with what Li et al. (2023) reported in their study, LLMs' capabilities to conduct emotional inference.</p>
<p>Although several previous studies in moral education have developed and tested simulation models predicting intervention outcomes, they could only demonstrate simulated behavioral or group-level results (Han et al., 2016(Han et al., , 2018Han, Lee, et al., 2020).</p>
<p>Unlike LLMs, they did not have any ability to emulate the reasoning process inside individuals and how external demonstrations and exemplars as input influence such a 19 process. Because LLMs possess the emerged chain of thought and reasoning feature, their unique benefit, the models will allow moral educators to examine internal reasoning, emotional, and motivational processes within moral domains. By doing so, moral educators will gain additional information and insights about how their educational input, such as the presentations of sophisticated reasoning exemplars as originally suggested by Blatt and Kohlberg (1975), impacts the internal processes among diverse populations before testing them with human subjects; and, in the domains of moral emotion and motivation, educators can test how different types of exemplary stories differently influence students' psychological processes associated with emotion and motivation.</p>
<p>In conclusion, thanks to the newly emerged features, i.e., in-context learning and reasoning, the chain of thought and reason, LLMs will provide practical benefits to researchers in moral education and development interested in the potential impacts of educational input. Even if they cannot perfectly emulate human behavior and cognition, they can still be versatile simulation tools for improving education with the functional features unavailable in prior ones (Volkman &amp; Gabriels, 2023). For example, in biological science and engineering, large-scale simulation models based on deep learning, which also constitutes the computational basis of LLMs, such as the AlphaFold, are changing ways to develop materials in the whole field (Chan et al., 2019). Previously, biological scientists and engineers were mandated to spend more than months to years exploring protein materials that will become candidates for potential pharmaceutical development. Now, with AlphaFold, they can identify the best candidate materials within less than a few days (Chan et al., 2019). Although the tool per se can only simulate structural aspects of the candidates and cannot perfectly emulate how they work in living organs, extracted information can effectively inform follow-up in-vitro and in-vivo experiments (Samorodnitsky, 2022).</p>
<p>Likewise, in the case of LLMs and moral education, moral educators can conduct preliminary simulations to predict how their educational materials and activities influence students' cognitive, motivational, and behavioral processes. Then, as outputs from AlphaFold inform further in-vitro and in-vivo investigations, the simulation outputs can provide researchers and educators with insights on how to conduct their experiments and how to implement their educational activities with students. It might be a significant advantage because even a brief educational intervention might produce non-trivial longterm impacts (Yeager &amp; Walton, 2011). Thus, gathering information to predict potential outcomes in advance would be helpful for researchers and educators (Han et al., 2016(Han et al., , 2018.</p>
<p>Conclusion Remarks</p>
<p>I reviewed recent updates in LLM research and considered how LLMs might assist research in moral education and development in this paper. In the process, I tested a widely-used LLM, ChatGPT, with ethical dilemmas presented by the bDIT. I also examined whether ChatGPT possessed the chain of thought and reasoning capabilities to update its moral decision with an alternative moral philosophical rationale suggested by MLK's letter.</p>
<p>Interestingly, ChatGPT demonstrated moral reasoning and the capacity to modify its reasoning process while solving the presented dilemmas. Additionally, I also tested ChatGPT's emotional and motivational capabilities by presenting different types of moral exemplars. They could report the perceived relatability, attainability, and elevation similar to human participants. Also, ChatGPT provided the rationale of their responses regarding 21 moral emotion and motivation in addition to short answers. Although the resultant outputs might only support the presence of rudimentary reasoning abilities and emotional and motivational capabilities, ChatGPT demonstrated its potential in simulating moral functioning and its improvement via interventions. Based on the outcomes, I briefly discussed how LLMs might help moral educators better conduct research in moral education and development, particularly those related to simulating moral psychological processes and educational outcomes.</p>
<p>Although LLMs possess the abovementioned practical benefits, several limitations warrant our attention. First, at this point, we cannot ensure that LLMs can perfectly simulate human cognition and behavior. Some scholars argue that even if LLMs might perform rudimentary philosophical reasoning and ToM tasks (Kosinski, 2023;Schwitzgebel et al., 2023), they could be philosophical zombies that conduct their behavior according to what they learned from large corpora (Chalmers, 2023). According to the critique, their human-like behaviors are mere products of prediction models trained by linguistic data, so whether they emulate human cognition or sentience is not ensured (Arcas &amp; Agüera, 2022). Instead of relying on LLMs without reservation, until the further development of technology, we may utilize LLMs as testbeds for moral psychology and education without assuming that they are perfectly emulating humans. Similar to the case of biotechnology, in which scientists use AlphaFold before in-vivo experiments (Samorodnitsky, 2022), researchers may use LLMs before conducting experiments with human subjects to gather additional information.</p>
<p>Second, we need to be aware of the issue of hallucinations. Because developers trained LLMs primarily with large-scale general corpora, which may include false 22 information, LLMs might produce untrustworthy output when we enter inquiries into them. They developed LLMs as generative prediction models based on trained corpora while paying less attention to their veracity, so people now consider hallucination one of their severe limitations (McKenna et al., 2023). That said, moral educators should carefully check the quality of products generated by LLMs, such as the chain of thought and reasoning outputs, so that the products do not contain significant false or unreliable information. One alleviating fact is that users can address such problematic products, including biased responses, by providing human reinforcements and chain of thought and reasoning inputs (Ganguli et al., 2023;Srivastava et al., 2023;Zhao et al., 2023). In the long run, how LLMs correct the falsehood and incredibility in their products, particularly those in moral domains, through interactive feedback processes, can also be an interesting research topic for moral educators. Insights from such correction processes can further inform moral education intending to address biases and misinformation, which has become a crucial topic after the COVID-19 pandemic (Blackburn et al., 2023;Gover et al., 2020).</p>
<p>Third, most current LLMs can only utilize language corpora as input, so they might not be able to perform simulations with diverse modalities of input, e.g., visual, tactical, and visceral information, unlike human beings (Chalmers, 2023). Thus, limited linguistic information constitutes input for simulations, so the models might only imperfectly emulate moral functioning that uses diverse modalities of inputs and involves embodiment (Narvaez, 2016). One good news is that engineers are now developing devices supporting multiple modalities for data input and output (Chalmers, 2023;Mu et al., 2023).</p>
<p>Researchers can simulate moral functioning with multimodal data obtained beyond human 23 language once such devices become available. The enhanced simulation models will allow researchers to examine human moral functioning more realistically.</p>
<p>Due to the same reason, I could only investigate the limited domains of moral psychology and education, e.g., moral reasoning and moral exemplar intervention, in this paper. Although moral reasoning is one fundamental factor predicting moral motivation and behavior (May, 2018), it could not be a sufficient condition for them (Darnell et al., 2022). Also, moral educators utilize various educational methods other than moral exemplar intervention, such as service learning. Once multimodal input and output are supported, we will be able to examine various functional components, such as moral identity and empathy, which constitute the complex network of moral functioning (Darnell et al., 2022;Han, 2023b), and educational methods.</p>
<p>Despite the limitations of LLMs at this point, I suggest LLMs are noteworthy in research on moral education and development in the long run. Recent developments in computer science have enabled LLMs to possess emerging features central to simulating human psychological processes, such as in-context learning and reasoning, the chain of thought and reasoning, reasoning-based correction, and ToM capabilities, which were not available previously. Given the abovementioned novel capabilities constitute the basis for moral functioning, it must be interesting to see how LLMs evolve. Once they acquire additional functionalities to simulate human cognition more accurately (Arcas &amp; Agüera, 2022), moral educators will get more insights into their research. Until then, we should pay keen attention to novel findings and updates regarding LLMs, particularly those closely related to human morality.</p>
<p>Following the bDIT scoring guidelines, I calculated the postconventional reasoning (P) score indicating the likelihood of employing the postconventional schema in all 24 questions. For instance, if ChapGPT selected the postconventional options for 12 questions out of 24, the resultant P-score became 12 / 24 x 100 = 50. The resultant P-score from the trial with ChatGPT was 45.83 (see https://osf.io/ryq5w for the complete conversation transcript). I compared this score with the P-scores calculated from a large dataset collected from undergraduate participants. I reanalyzed the dataset primarily collected by Han (2023b). Han (2023b) collected responses from 1,596 participants (85.37% women; mean age = 21.85 years, SD = 5.88 years) via Qualtrics. The Institutional Review Board at the University of Alabama reviewed and approved the original study (IRB #: 18-12-1842). I used a customized R code to calculate their P-scores (all data and source code files are available via the Open Science Framework: https://osf.io/j98p4/).</p>
<p>(
mean = 52.61, SD = 21.74). 45.83 was equivalent to the 40 th percentile of the whole undergraduate student group. Despite potential caveats, the result might suggest that ChatGPT possibly demonstrates moral judgment and reasoning compatible with those among undergraduate students. Second, I collected more qualitative responses to examine further details about the reasoning process. I again presented the three dilemmas and asked ChatGPT to elaborate on their supporting rationale. First, I asked about the rationale supporting ChatGPT's response to Heinz and the drug: Heinz should steal the drug. Consistent with their answers to the bDIT items, ChatGPT presented several points corresponding to postconventional reasoning, such as the principle of preserving life, moral duty, compassion, and consequentialist perspective. Second, in the case of the Newspaper Dilemma, ChatGPT argued that the principal should not stop the newspaper. Like the case of the Heinz dilemma, when I asked for a rationale, ChatGPT provided several points relevant to the postconventional schema: freedom of expression, student engagement, and education.</p>
<p>trained by general corpora could successfully answer ethical questions with contextual information and flexibility. Moreover, while solving the problems, instead of merely providing determined answers, they could elaborate on the chain of reasoning constituting the basis for their decisions. Finally, models could demonstrate an ability to update their reasoning process and eventual decision based on indirectly relevant sources of information via in-context learning.</p>
<p>Witnessing excellence in action: The 'otherpraising'emotions of elevation, gratitude, and admiration. S B Algoe, J Haidt, The Journal of Positive Psychology. 42Algoe, S. B., &amp; Haidt, J. (2009). Witnessing excellence in action: The 'other- praising'emotions of elevation, gratitude, and admiration. The Journal of Positive Psychology, 4(2), 105-127.</p>
<p>Do Large Language Models Understand Us?. Y Arcas, B Agüera, 10.1162/daed_a_01909Daedalus. 1512Arcas, Y., &amp; Agüera, B. (2022). Do Large Language Models Understand Us? Daedalus, 151(2), 183-197. https://doi.org/10.1162/daed_a_01909</p>
<p>. A M Blackburn, H Han, R A Gelpí, S Stöckli, A Jeftić, B Ch&apos;ng, K Koszałkowska, D Lacko, T L Milfont, Y Lee, Covidistress Ii Consortium, S Vestergren, Blackburn, A. M., Han, H., Gelpí, R. A., Stöckli, S., Jeftić, A., Ch'ng, B., Koszałkowska, K., Lacko, D., Milfont, T. L., Lee, Y., COVIDiSTRESS Ii Consortium, &amp; Vestergren, S. (2023).</p>
<p>Mediation analysis of conspiratorial thinking and anti-expert sentiments on vaccine willingness. Health Psychology. 424Mediation analysis of conspiratorial thinking and anti-expert sentiments on vaccine willingness. Health Psychology, 42(4), 235-246.</p>
<p>. 10.1037/hea0001268https://doi.org/10.1037/hea0001268</p>
<p>Bridging moral cognition and moral action: A critical review of the literature. A Blasi, 10.1037/0033-2909.88.1.1Psychological Bulletin. 88Blasi, A. (1980). Bridging moral cognition and moral action: A critical review of the literature. Psychological Bulletin, 88, 1-45. https://doi.org/10.1037/0033- 2909.88.1.1</p>
<p>The Effects of Classroom Moral Discussion upon Children's Level of Moral Judgment. M M Blatt, L Kohlberg, Journal of Moral Education. 42Blatt, M. M., &amp; Kohlberg, L. (1975). The Effects of Classroom Moral Discussion upon Children's Level of Moral Judgment. Journal of Moral Education, 4(2), 129-161.</p>
<p>. 10.1080/0305724750040207https://doi.org/10.1080/0305724750040207</p>
<p>D J Chalmers, arXiv:2303.07103Could a Large Language Model be Conscious?. Chalmers, D. J. (2023). Could a Large Language Model be Conscious? (arXiv:2303.07103). arXiv. http://arxiv.org/abs/2303.07103</p>
<p>Advancing Drug Discovery via Artificial Intelligence. H C S Chan, H Shan, T Dahoun, H Vogel, S Yuan, Trends in Pharmacological Sciences. 408Chan, H. C. S., Shan, H., Dahoun, T., Vogel, H., &amp; Yuan, S. (2019). Advancing Drug Discovery via Artificial Intelligence. Trends in Pharmacological Sciences, 40(8), 592-604.</p>
<p>. 10.1016/j.tips.2019.06.004https://doi.org/10.1016/j.tips.2019.06.004</p>
<p>Measuring moral 25 reasoning using moral dilemmas: Evaluating reliability, validity, and differential item functioning of the behavioural defining issues test (bDIT). Y.-J Choi, H Han, K J Dawson, S J Thoma, A L Glenn, European Journal of Developmental Psychology. 165Choi, Y.-J., Han, H., Dawson, K. J., Thoma, S. J., &amp; Glenn, A. L. (2019). Measuring moral 25 reasoning using moral dilemmas: Evaluating reliability, validity, and differential item functioning of the behavioural defining issues test (bDIT). European Journal of Developmental Psychology, 16(5), 622-631.</p>
<p>. 10.1080/17405629.2019.1614907https://doi.org/10.1080/17405629.2019.1614907</p>
<p>Empirical evidence for moral Bayesianism. Philosophical Psychology, 1-30. H Cohen, I Nissan-Rozen, A Maril, 10.1080/09515089.2022.2096430Cohen, H., Nissan-Rozen, I., &amp; Maril, A. (2022). Empirical evidence for moral Bayesianism. Philosophical Psychology, 1-30. https://doi.org/10.1080/09515089.2022.2096430</p>
<p>A multifunction approach to assessing Aristotelian phronesis (practical wisdom). C Darnell, B J Fowers, K Kristjánsson, 10.1016/j.paid.2022.111684Personality and Individual Differences. 111684Darnell, C., Fowers, B. J., &amp; Kristjánsson, K. (2022). A multifunction approach to assessing Aristotelian phronesis (practical wisdom). Personality and Individual Differences, 196, 111684. https://doi.org/10.1016/j.paid.2022.111684</p>
<p>. Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, L Li, Z Sui, arXiv:2301.00234A Survey on In-context Learning. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., &amp; Sui, Z. (2023). A Survey on In-context Learning (arXiv:2301.00234). arXiv. http://arxiv.org/abs/2301.00234</p>
<p>Opinion Paper: "So what if ChatGPT wrote it?" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. Y K Dwivedi, N Kshetri, L Hughes, E L Slade, A Jeyaraj, A K Kar, A M Baabdullah, A Koohang, V Raghavan, M Ahuja, H Albanna, M A Albashrawi, A S Al-Busaidi, J Balakrishnan, Y Barlette, S Basu, I Bose, L Brooks, D Buhalis, R Wright, 10.1016/j.ijinfomgt.2023.102642International Journal of Information Management. 71102642Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Baabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Albashrawi, M. A., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., Buhalis, D., … Wright, R. (2023). Opinion Paper: "So what if ChatGPT wrote it?" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy. International Journal of Information Management, 71, 102642. https://doi.org/10.1016/j.ijinfomgt.2023.102642</p>
<p>Learning and inference in the brain. K Friston, 10.1016/j.neunet.2003.06.005Neural Networks. 169Friston, K. (2003). Learning and inference in the brain. Neural Networks, 16(9), 1325-1352. https://doi.org/10.1016/j.neunet.2003.06.005</p>
<p>. D Ganguli, A Askell, N Schiefer, T I Liao, K Lukošiūtė, A Chen, A Goldie, A Mirhoseini, 26Ganguli, D., Askell, A., Schiefer, N., Liao, T. I., Lukošiūtė, K., Chen, A., Goldie, A., Mirhoseini, A., 26</p>
<p>. C Olsson, D Hernandez, D Drain, D Li, E Tran-Johnson, E Perez, J Kernion, Olsson, C., Hernandez, D., Drain, D., Li, D., Tran-Johnson, E., Perez, E., Kernion, J.,</p>
<p>J Kerr, J Mueller, J Landau, K Ndousse, J Kaplan, arXiv:2302.07459The Capacity for Moral Self-Correction in Large Language Models. Kerr, J., Mueller, J., Landau, J., Ndousse, K., … Kaplan, J. (2023). The Capacity for Moral Self-Correction in Large Language Models (arXiv:2302.07459). arXiv. http://arxiv.org/abs/2302.07459</p>
<p>Anti-Asian Hate Crime During the COVID-19 Pandemic: Exploring the Reproduction of Inequality. A R Gover, S B Harper, L Langton, 10.1007/s12103-020-09545-1American Journal of Criminal Justice. 454Gover, A. R., Harper, S. B., &amp; Langton, L. (2020). Anti-Asian Hate Crime During the COVID-19 Pandemic: Exploring the Reproduction of Inequality. American Journal of Criminal Justice, 45(4), 647-667. https://doi.org/10.1007/s12103-020-09545-1</p>
<p>AI and the transformation of social science research. I Grossmann, M Feinberg, D C Parker, N A Christakis, P E Tetlock, W A Cunningham, 10.1126/science.adi1778Science. 3806650Grossmann, I., Feinberg, M., Parker, D. C., Christakis, N. A., Tetlock, P. E., &amp; Cunningham, W. A. (2023). AI and the transformation of social science research. Science, 380(6650), 1108-1109. https://doi.org/10.1126/science.adi1778</p>
<p>How Close is ChatGPT to Human Experts?. B Guo, X Zhang, Z Wang, M Jiang, J Nie, Y Ding, J Yue, Y Wu, arXiv:2301.07597Comparison Corpus, Evaluation, and Detection. Guo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y., Yue, J., &amp; Wu, Y. (2023). How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection (arXiv:2301.07597). arXiv. http://arxiv.org/abs/2301.07597</p>
<p>The positive emotion of elevation. J Haidt, Prevention and Treatment. 33Haidt, J. (2000). The positive emotion of elevation. Prevention and Treatment, 3(3), 1-5.</p>
<p>Considering the Purposes of Moral Education with Evidence in Neuroscience: Emphasis on Habituation of Virtues and Cultivation of Phronesis. H Han, Han, H. (2023a). Considering the Purposes of Moral Education with Evidence in Neuroscience: Emphasis on Habituation of Virtues and Cultivation of Phronesis.</p>
<p>. 10.1007/s10677-023-10369-1Ethical Theory and Moral Practice. Ethical Theory and Moral Practice. https://doi.org/10.1007/s10677-023-10369-1</p>
<p>Examining the Network Structure among Moral Functioning Components with Network Analysis. H Han, 10.31234/osf.io/ufg7ePreprintHan, H. (2023b). Examining the Network Structure among Moral Functioning Components with Network Analysis [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/ufg7e</p>
<p>Relatable and attainable moral exemplars as sources for moral elevation and pleasantness. H Han, K J Dawson, Journal of Moral Education. Han, H., &amp; Dawson, K. J. (2023). Relatable and attainable moral exemplars as sources for moral elevation and pleasantness. Journal of Moral Education.</p>
<p>. 10.1080/03057240.2023.2173158https://doi.org/10.1080/03057240.2023.2173158</p>
<p>. H Han, K J Dawson, S J Thoma, A L Glenn, Developmental Level of Moral. 27Han, H., Dawson, K. J., Thoma, S. J., &amp; Glenn, A. L. (2020). Developmental Level of Moral 27</p>
<p>Judgment Influences Behavioral Patterns During Moral Decision-Making. The Journal of Experimental Education. 884Judgment Influences Behavioral Patterns During Moral Decision-Making. The Journal of Experimental Education, 88(4), 660-675.</p>
<p>. 10.1080/00220973.2019.1574701https://doi.org/10.1080/00220973.2019.1574701</p>
<p>Predicting long-term outcomes of educational interventions using the Evolutionary Causal Matrices and Markov Chain based on educational neuroscience. H Han, K Lee, F Soylu, Trends in Neuroscience and Education. 54Han, H., Lee, K., &amp; Soylu, F. (2016). Predicting long-term outcomes of educational interventions using the Evolutionary Causal Matrices and Markov Chain based on educational neuroscience. Trends in Neuroscience and Education, 5(4), 157-165.</p>
<p>. 10.1016/j.tine.2016.11.003https://doi.org/10.1016/j.tine.2016.11.003</p>
<p>Simulating outcomes of interventions using a multipurpose simulation program based on the evolutionary causal matrices and Markov chain. H Han, K Lee, F Soylu, Knowledge and Information Systems. Han, H., Lee, K., &amp; Soylu, F. (2018). Simulating outcomes of interventions using a multipurpose simulation program based on the evolutionary causal matrices and Markov chain. Knowledge and Information Systems.</p>
<p>. 10.1007/s10115-017-1151-0https://doi.org/10.1007/s10115-017-1151-0</p>
<p>Applying the Deep Learning Method for Simulating Outcomes of Educational Interventions. H Han, K Lee, F Soylu, 10.1007/s42979-020-0075-zSN Computer Science. 1270Han, H., Lee, K., &amp; Soylu, F. (2020). Applying the Deep Learning Method for Simulating Outcomes of Educational Interventions. SN Computer Science, 1(2), 70. https://doi.org/10.1007/s42979-020-0075-z</p>
<p>Which moral exemplars inspire prosociality?. H Han, C I Workman, J May, P Scholtens, K J Dawson, A L Glenn, P Meindl, 10.1080/09515089.2022.2035343Philosophical Psychology. 357Han, H., Workman, C. I., May, J., Scholtens, P., Dawson, K. J., Glenn, A. L., &amp; Meindl, P. (2022). Which moral exemplars inspire prosociality? Philosophical Psychology, 35(7), 943- 970. https://doi.org/10.1080/09515089.2022.2035343</p>
<p>The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts. M Hosseini, D B Resnik, K Holmes, 10.1177/17470161231180449Research Ethics. 17470161231180449Hosseini, M., Resnik, D. B., &amp; Holmes, K. (2023). The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts. Research Ethics, 17470161231180449. https://doi.org/10.1177/17470161231180449</p>
<p>J Huang, K C Chang, .-C , arXiv:2212.10403Towards Reasoning in Large Language Models: A Survey. Huang, J., &amp; Chang, K. C.-C. (2023). Towards Reasoning in Large Language Models: A Survey (arXiv:2212.10403). arXiv. http://arxiv.org/abs/2212.10403</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, S Krusche, G Kutyniok, T Michaeli, C Nerdel, J Pfeffer, O Poquet, M Sailer, A Schmidt, T Seidel, G Kasneci, 10.1016/j.lindif.2023.102274Learning and Individual Differences. 103102274Kasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., … Kasneci, G. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103, 102274. https://doi.org/10.1016/j.lindif.2023.102274</p>
<p>M Kosinski, arXiv:2302.02083Theory of Mind May Have Spontaneously Emerged in Large Language Models. Kosinski, M. (2023). Theory of Mind May Have Spontaneously Emerged in Large Language Models (arXiv:2302.02083). arXiv. http://arxiv.org/abs/2302.02083</p>
<p>Educating Moral Emotions or Moral Selves: A false dichotomy?. K Kristjánsson, Educational Philosophy and Theory. 424Kristjánsson, K. (2010). Educating Moral Emotions or Moral Selves: A false dichotomy? Educational Philosophy and Theory, 42(4), 397-409.</p>
<p>. 10.1111/j.1469-5812.2008.00489.xhttps://doi.org/10.1111/j.1469-5812.2008.00489.x</p>
<p>Emotions targeting moral exemplarity: Making sense of the logical geography of admiration, emulation and elevation. Theory and Research in Education. K Kristjánsson, 10.1177/147787851769567915Kristjánsson, K. (2017). Emotions targeting moral exemplarity: Making sense of the logical geography of admiration, emulation and elevation. Theory and Research in Education, 15(1), 20-37. https://doi.org/10.1177/1477878517695679</p>
<p>Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. M Li, Y Su, H.-Y Huang, J Cheng, X Hu, X Zhang, H Wang, Y Qin, X Wang, Z Liu, D Zhang, arXiv:2302.09582Li, M., Su, Y., Huang, H.-Y., Cheng, J., Hu, X., Zhang, X., Wang, H., Qin, Y., Wang, X., Liu, Z., &amp; Zhang, D. (2023). Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference (arXiv:2302.09582). arXiv. http://arxiv.org/abs/2302.09582</p>
<p>A Bayesian foundation for individual learning under uncertainty. C Mathys, 10.3389/fnhum.2011.00039Frontiers in Human Neuroscience. Mathys, C. (2011). A Bayesian foundation for individual learning under uncertainty. Frontiers in Human Neuroscience, 5. https://doi.org/10.3389/fnhum.2011.00039</p>
<p>Regard for Reason in the Moral Mind. J May, Oxford University PressMay, J. (2018). Regard for Reason in the Moral Mind. Oxford University Press.</p>
<p>Psychologists update their beliefs about effect sizes after replication studies. A D Mcdiarmid, A M Tullett, C M Whitt, S Vazire, P E Smaldino, J E Stephens, 10.1038/s41562-021-01220-7Nature Human Behaviour. 512McDiarmid, A. D., Tullett, A. M., Whitt, C. M., Vazire, S., Smaldino, P. E., &amp; Stephens, J. E. (2021). Psychologists update their beliefs about effect sizes after replication studies. Nature Human Behaviour, 5(12), 1663-1673. https://doi.org/10.1038/s41562-021- 01220-7</p>
<p>N Mckenna, T Li, L Cheng, M J Hosseini, M Johnson, M Steedman, arXiv:2305.14552Sources of Hallucination by Large Language Models on Inference Tasks. McKenna, N., Li, T., Cheng, L., Hosseini, M. J., Johnson, M., &amp; Steedman, M. (2023). Sources of Hallucination by Large Language Models on Inference Tasks (arXiv:2305.14552).</p>
<p>Large language models challenge the future of higher education. S Milano, J A Mcgrane, S Leonelli, Nature Machine Intelligence. 54Milano, S., McGrane, J. A., &amp; Leonelli, S. (2023). Large language models challenge the future of higher education. Nature Machine Intelligence, 5(4), 333-334.</p>
<p>. 10.1038/s42256-023-00644-2https://doi.org/10.1038/s42256-023-00644-2</p>
<p>R H Mogavi, C Deng, J J Kim, P Zhou, Y D Kwon, A H S Metwally, A Tlili, S Bassanelli, A Bucchiarone, S Gujar, L E Nacke, P Hui, arXiv:2305.13114Exploring User Perspectives on ChatGPT: Applications, Perceptions, and Implications for AI-Integrated Education. Mogavi, R. H., Deng, C., Kim, J. J., Zhou, P., Kwon, Y. D., Metwally, A. H. S., Tlili, A., Bassanelli, S., Bucchiarone, A., Gujar, S., Nacke, L. E., &amp; Hui, P. (2023). Exploring User Perspectives on ChatGPT: Applications, Perceptions, and Implications for AI-Integrated Education (arXiv:2305.13114). arXiv. http://arxiv.org/abs/2305.13114</p>
<p>Foundation models for generalist medical artificial intelligence. M Moor, O Banerjee, Z S H Abad, H M Krumholz, J Leskovec, E J Topol, P Rajpurkar, 10.1038/s41586-023-05881-4Nature. 7956Moor, M., Banerjee, O., Abad, Z. S. H., Krumholz, H. M., Leskovec, J., Topol, E. J., &amp; Rajpurkar, P. (2023). Foundation models for generalist medical artificial intelligence. Nature, 616(7956), 259-265. https://doi.org/10.1038/s41586-023-05881-4</p>
<p>Y Mu, Q Zhang, M Hu, W Wang, M Ding, J Jin, B Wang, J Dai, Y Qiao, P Luo, arXiv:2305.15021EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. Mu, Y., Zhang, Q., Hu, M., Wang, W., Ding, M., Jin, J., Wang, B., Dai, J., Qiao, Y., &amp; Luo, P. (2023). EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought (arXiv:2305.15021). arXiv. http://arxiv.org/abs/2305.15021</p>
<p>Embodied Morality: Protectionism, Engagement and Imagination. D Narvaez, Narvaez, D. (2016). Embodied Morality: Protectionism, Engagement and Imagination.</p>
<p>. Palgrave Macmillan, U K , 10.1057/978-1-137-55399-7_1Palgrave Macmillan UK. https://doi.org/10.1057/978-1-137-55399-7_1</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 35Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., &amp; others. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 27730-27744.</p>
<p>P Railton, Moral Learning: Conceptual foundations and normative relevance. Railton, P. (2017). Moral Learning: Conceptual foundations and normative relevance.</p>
<p>. 10.1016/j.cognition.2016.08.015Cognition. 167Cognition, 167, 172-190. https://doi.org/10.1016/j.cognition.2016.08.015</p>
<p>Postconventional moral thinking: A Neo-Kohlbergian approach. J R Rest, D Narvaez, M J Bebeau, S J Thoma, Lawrence Erlbaum Associates, PublishersRest, J. R., Narvaez, D., Bebeau, M. J., &amp; Thoma, S. J. (1999). Postconventional moral thinking: A Neo-Kohlbergian approach. Lawrence Erlbaum Associates, Publishers.</p>
<p>The Future of Biotech in an Artificially Intelligent World: Biotech hopes to benefit from protein structure prediction, pattern recognition, and support for iterative development. D Samorodnitsky, 10.1089/gen.42.01.09Genetic Engineering &amp; Biotechnology News. 421Samorodnitsky, D. (2022). The Future of Biotech in an Artificially Intelligent World: Biotech hopes to benefit from protein structure prediction, pattern recognition, and support for iterative development. Genetic Engineering &amp; Biotechnology News, 42(1), 26-27, 29. https://doi.org/10.1089/gen.42.01.09</p>
<p>The meaning of role modelling in moral and character education. W Sanderse, Journal of Moral Education. 421Sanderse, W. (2012). The meaning of role modelling in moral and character education. Journal of Moral Education, 42(1), 28-42.</p>
<p>. 10.1080/03057240.2012.690727https://doi.org/10.1080/03057240.2012.690727</p>
<p>Elevation leads to altruistic behavior. S Schnall, J Roper, D M T Fessler, 10.1177/0956797609359882Psychological Science. 21Schnall, S., Roper, J., &amp; Fessler, D. M. T. (2010). Elevation leads to altruistic behavior. Psychological Science, 21, 315-320. https://doi.org/10.1177/0956797609359882</p>
<p>Creating a Large Language Model of a Philosopher. E Schwitzgebel, D Schwitzgebel, A Strasser, 10.1111/mila.12466Schwitzgebel, E., Schwitzgebel, D., &amp; Strasser, A. (2023). Creating a Large Language Model of a Philosopher. Mind &amp; Language. https://doi.org/10.1111/mila.12466</p>
<p>N Shapira, M Levy, S H Alavi, X Zhou, Y Choi, Y Goldberg, M Sap, V Shwartz, arXiv:2305.14763Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models. Shapira, N., Levy, M., Alavi, S. H., Zhou, X., Choi, Y., Goldberg, Y., Sap, M., &amp; Shwartz, V. (2023). Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models (arXiv:2305.14763). arXiv. http://arxiv.org/abs/2305.14763</p>
<p>Moral elevation can induce nursing. J A Silvers, J Haidt, Emotion. 82Silvers, J. A., &amp; Haidt, J. (2008). Moral elevation can induce nursing. Emotion, 8(2), 291-295.</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, Wu, arXiv:2206.04615Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., … Wu, Z. (2023). Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (arXiv:2206.04615). arXiv. http://arxiv.org/abs/2206.04615</p>
<p>Elevation at work: The effects of leaders' moral excellence. M Vianello, E M Galliani, J Haidt, The Journal of Positive Psychology. 55Vianello, M., Galliani, E. M., &amp; Haidt, J. (2010). Elevation at work: The effects of leaders' moral excellence. The Journal of Positive Psychology, 5(5), 390-411.</p>
<p>AI Moral Enhancement: Upgrading the Socio-Technical System of Moral Engagement. R Volkman, K Gabriels, 10.1007/s11948-023-00428-2Science and Engineering Ethics. 29211Volkman, R., &amp; Gabriels, K. (2023). AI Moral Enhancement: Upgrading the Socio-Technical System of Moral Engagement. Science and Engineering Ethics, 29(2), 11. https://doi.org/10.1007/s11948-023-00428-2</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., &amp; Zhou, D. (2023). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arXiv:2201.11903). arXiv. http://arxiv.org/abs/2201.11903</p>
<p>Y Wu, S Prabhumoye, S Y Min, Y Bisk, R Salakhutdinov, A Azaria, T Mitchell, Y Li, arXiv:2305.15486SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. Wu, Y., Prabhumoye, S., Min, S. Y., Bisk, Y., Salakhutdinov, R., Azaria, A., Mitchell, T., &amp; Li, Y. (2023). SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning (arXiv:2305.15486). arXiv. http://arxiv.org/abs/2305.15486</p>
<p>Social-psychological interventions in education: They're not magic. D S Yeager, G M Walton, Review of Educational Research. 812Yeager, D. S., &amp; Walton, G. M. (2011). Social-psychological interventions in education: They're not magic. Review of Educational Research, 81(2), 267-301.</p>
<p>. 10.3102/0034654311405999https://doi.org/10.3102/0034654311405999</p>
<p>The neural basis of the interaction between theory of mind and moral judgment. L Young, F Cushman, M Hauser, R Saxe, Proceedings of the National Academy of Sciences of the United States of America. the National Academy of Sciences of the United States of America104Young, L., Cushman, F., Hauser, M., &amp; Saxe, R. (2007). The neural basis of the interaction between theory of mind and moral judgment. Proceedings of the National Academy of Sciences of the United States of America, 104, 8235-8240.</p>
<p>. 10.1073/pnas.0701408104https://doi.org/10.1073/pnas.0701408104</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, J.-R Wen, arXiv:2303.18223arXiv. 32A Survey of Large Language Models. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). A Survey of Large Language Models (arXiv:2303.18223). arXiv. 32</p>            </div>
        </div>

    </div>
</body>
</html>
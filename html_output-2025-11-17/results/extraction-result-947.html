<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-947 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-947</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-947</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-f94c040b02bdd6cf1b85f374e3912630c66861c3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f94c040b02bdd6cf1b85f374e3912630c66861c3" target="_blank">InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> InterCode is a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations, that is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation.</p>
                <p><strong>Paper Abstract:</strong> Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan&Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. Project site with code and data: https://intercode-benchmark.github.io</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e947.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e947.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pre-trained transformer language model from OpenAI evaluated via API; used as an off-the-shelf LLM in interactive coding (InterCode) experiments with prompting-only inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained large transformer-based language model accessed via API for zero-shot and prompting experiments; no architectural changes or additional training performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash (also InterCode-Python dataset available)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / sequential decision-making (SQL query construction); multi-step procedural tool use (Bash file-system commands)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SQL: Single Turn (All) 9.1% success -> Try Again (n=10) 73.7% success; Bash: Single Turn (All) 34.0% -> Try Again (n=10) 48.5%</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>None introduced in this paper (pretrained LLM; standard transformer architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / inference-time evaluation (no fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; interactive environment</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Compared Single-Turn (static seq2seq) generation against multi-turn interactive protocols (Try Again, ReAct, Plan & Solve). Models receive execution feedback (observations) from Dockerized execution environments and can iteratively edit code/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Very large improvement on SQL when moving from Single Turn to interactive Try Again (success rate from 9.1% -> 73.7%). Moderate improvement on Bash (34.0% -> 48.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Static single-turn generation cannot iteratively discover context, correct errors, or compose multi-step procedures; interactive feedback permits context discovery and error correction which yields large gains, especially for tasks requiring sequential decision-making (SQL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e947.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI chat-style LLM used via API for prompting experiments; evaluated on InterCode interactive code-generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer LLM accessed via API with temperature=0 for deterministic generation; evaluated only with prompting/inference (no training changes).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / sequential decision-making (SQL); multi-step procedural tool use (Bash)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SQL: Single Turn (All) 10.5% -> Try Again (n=10) 47.3% success; Bash: Single Turn (All) 34.5% -> Try Again (n=10) 46.5% success. With schema provided in prompt: Single Turn (All) 67.9% -> Try Again 72.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>None changed; standard pretrained transformer LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / inference-time evaluation (no additional fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; prompt augmentation (provide DB schema)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated multiple prompting strategies: Single Turn (zero-shot), Try Again (iterative interactive loop with execution feedback), ReAct, and Plan & Solve. Also ran ablation adding the full SQL database schema to the Single Turn prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Try Again improves SQL from 10.5% -> 47.3% and Bash from 34.5% -> 46.5%. ReAct further improves SQL (Try Again 47.3% -> ReAct 58.7% for gpt-3.5-turbo) while effects differ by task. Providing full DB schema in prompt dramatically boosts Single Turn SQL (10.5% -> 67.9%), but Try Again still outperforms slightly (72.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Interactive tasks require context discovery, conditional action construction, and iterative error correction; single-turn prompts lack the ability to explore state, causing a gap. Also models struggle to use long interaction history effectively as turns accumulate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e947.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI completion-style LLM used as a baseline for static vs interactive code generation experiments in InterCode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer model accessed for single-turn and multi-turn prompting experiments; no finetuning in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / sequential decision-making; multi-step procedural tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SQL: Single Turn (All) 7.4% -> Try Again (n=10) 15.6%; Bash: Single Turn (All) 24.6% -> Try Again 38.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>None introduced in this paper (standard pretrained transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / inference</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (Try Again multi-turn, ReAct, Plan & Solve tested)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same multi-turn prompting strategies as other models; interactive execution feedback used to enable iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Moderate gains from single-turn to interactive: SQL 7.4% -> 15.6%; Bash 24.6% -> 38.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Single-turn generation cannot iteratively inspect or correct actions; the interactive setting allows small models to recover from initial generation errors but gains are smaller than for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e947.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-bison-001</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-2 text-bison-001</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM-2 family model (text bison) evaluated in InterCode for single-turn vs interactive code tasks; used via API in inference-only setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>text-bison-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family text model used for zero-shot and multi-turn prompting experiments; no architecture/training changes in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning (SQL); multi-step procedural (Bash)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SQL: Single Turn (All) 11.5% -> Try Again 12.9%; Bash: Single Turn (All) 17.0% -> Try Again 22.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No changes (pretrained PaLM-2).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / inference</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategies (Try Again, ReAct, Plan & Solve)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated prompting strategies and interactive feedback; no model retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Small improvements with interaction on Bash; minimal effect on SQL for text-bison in these runs.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Similar reasoning: static generation lacks iterative correction and context exploration capabilities required for interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e947.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>chat-bison-001</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-2 chat-bison-001</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM-2 chat model used via API for InterCode evaluations; included in single-turn and interactive comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>chat-bison-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-oriented variant of PaLM-2 used with prompting; inference-only evaluation without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning; procedural tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SQL: Single Turn (All) 7.9% -> Try Again 9.9%; Bash: Single Turn (All) 17.7% -> Try Again 19.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No changes in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / inference</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Assessed multi-turn prompting and execution feedback's impact; used same interactive protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Small improvements with interactive Try Again on both tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Interactive tasks demand iterative exploration and correction not available in static single-turn prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e947.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 13B chat LLM (Vicuna) evaluated in InterCode on static vs interactive code tasks; inference-only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source chat model (13B) used as baseline for interactive coding; no extra training in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / procedural tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SQL: Single Turn (All) 2.6% -> Try Again 6.3%; Bash: Single Turn (All) 16.0% -> Try Again 24.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Standard transformer-based chat model; no architecture changes.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / inference</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; interactive environment</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated with Single Turn and Try Again interactive protocols; observed gains from interaction though smaller absolute performance than larger closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improvement from Single Turn to Try Again on both SQL and Bash, but absolute success rates remain low (e.g., SQL 2.6% -> 6.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Smaller models have less capacity to plan and utilize multi-turn feedback effectively; interactive correction helps but does not close the gap to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e947.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StarChat-16B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StarChat-16B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 16B LLM used as an evaluated baseline in InterCode for static vs interactive code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>StarChat-16B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source large transformer model evaluated via prompting; no training changes in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>16B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning; multi-step procedural</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SQL: Single Turn (All) 8.9% -> Try Again 9.7%; Bash: Single Turn (All) 17.7% -> Try Again 23.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No architecture interventions in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only / inference</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; interactive environment</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Assessed single-turn vs interactive multi-turn prompting; execution feedback used to iteratively refine actions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Small improvements from interaction on Bash; minimal change for SQL in these reported runs.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Same general causes: need for exploration, context discovery, and iterative correction that single-turn prompts cannot provide.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e947.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Try Again</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Try Again (iterative feedback baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple iterative prompting protocol where the agent repeatedly issues actions, receives execution feedback, and can retry until success or budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Try Again</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting/workflow intervention (not a model): agent informed of interactive setting and allowed up to n turns to act and receive execution feedback; termination upon success or turn limit.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>iterative multi-step procedural / sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Used as baseline: e.g. gpt-3.5-turbo SQL SR 47.3% (avg turns 7.25); Bash SR 46.5% (avg turns 6.15).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting-only workflow</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy / interaction protocol</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Provides a generic iterative loop that feeds execution outputs back as observations and allows multiple attempts; designed to mimic human write-execute-test loop.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial improvements over Single Turn for many models (e.g., GPT-4 SQL 9.1% -> Try Again 73.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Enables error correction and context discovery unavailable to static generation, explaining much of the interactive vs single-turn performance gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e947.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reason + Act prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that interleaves reasoning traces ('thoughts') with external actions to encourage models to both think and act during multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting/workflow intervention that elicits interleaved natural-language reasoning and actions (commands) from the LLM, allowing the model to plan, query environment, and react to observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning and sequential decision-making with tool-use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>For gpt-3.5-turbo aggregated: SQL SR 58.7% (avg turns 5.30, Error% 6.94) vs Try Again 47.3%; Bash SR 20.5% (avg turns 4.40, Error% 20.4) vs Try Again 46.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No architecture change  a prompting strategy that elicits reasoning and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context workflow</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (reason+act)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Prompts models to produce interleaved chains of thought and actions, encouraging exploration and explicit reasoning steps tied to environment actions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>On SQL (gpt-3.5-turbo) ReAct improved success rate vs Try Again (47.3% -> 58.7%) and reduced non-admissible actions; on Bash ReAct underperformed relative to Try Again (20.5% vs 46.5%)  demonstrating task-dependent effects.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>ReAct helps tasks that benefit from exploratory reasoning and conditional action selection (SQL), but its unconstrained reasoning pattern can hurt more 'imperative' multi-step planning tasks (some Bash tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e947.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan & Solve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan & Solve prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach that first elicits an explicit high-level plan (decomposition) and then generates solutions for plan steps; designed to improve chain-of-thought style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Plan & Solve</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting workflow that encourages decomposition of the problem into an explicit plan then solving subproblems; no architectural change.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL; InterCode-Bash</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step decomposition for procedural tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Using gpt-3.5-turbo: SQL SR 49.1% (avg turns 4.29, Error% 16.2) vs Try Again 47.3%; Bash SR 28.0% (avg turns 6.65, Error% 53.3) vs Try Again 46.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>None (prompting-only decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy (plan then solve)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Explicitly instructs the model to produce a plan (decomposition) first and then solve each subtask, encouraging structured multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved or comparable performance on SQL relative to Try Again (49.1% vs 47.3%) but underperformed on Bash (28.0% vs 46.5%), indicating planning helps some task types (declarative/multi-subquery SQL) but can be too rigid for tasks requiring on-the-fly adaptation to execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Rigid decomposition may fail to incorporate execution feedback fluidly; interactive tasks that require frequent adjustment favor more flexible reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e947.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e947.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schema-provision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt augmentation with full database schema</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-level intervention that provides the entire SQL database schema in the Single-Turn question message to supply more context to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Schema-provision (provide DB schema in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model  a prompt augmentation intervention where the full SQL schema is appended to the prompt to reduce the need for interaction for context discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>InterCode-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>context-augmentation for multi-step reasoning (SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>For gpt-3.5-turbo: Single Turn (All) jump from 10.5% -> 67.9% when schema provided; Try Again (n=10) performance with schema 72.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompt augmentation / in-context information</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompt augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Includes the complete database schema in the Single Turn prompt to supply the model with the contextual information it would otherwise need to discover via interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Dramatically improves Single Turn SQL performance (gpt-3.5-turbo Single Turn All 10.5% -> with schema 67.9%), narrowing the gap with interactive methods though Try Again still slightly outperforms (72.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>A large component of the interactive advantage is that interaction enables selective context discovery; providing that context up-front substitutes for interaction and closes much of the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models <em>(Rating: 2)</em></li>
                <li>Execution-guided neural program synthesis <em>(Rating: 2)</em></li>
                <li>Executionbased evaluation for data science code generation models <em>(Rating: 1)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-947",
    "paper_id": "paper-f94c040b02bdd6cf1b85f374e3912630c66861c3",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "Large pre-trained transformer language model from OpenAI evaluated via API; used as an off-the-shelf LLM in interactive coding (InterCode) experiments with prompting-only inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "gpt-4",
            "model_description": "Pretrained large transformer-based language model accessed via API for zero-shot and prompting experiments; no architectural changes or additional training performed in this paper.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash (also InterCode-Python dataset available)",
            "interactive_task_type": "multi-step reasoning / sequential decision-making (SQL query construction); multi-step procedural tool use (Bash file-system commands)",
            "interactive_performance": "SQL: Single Turn (All) 9.1% success -&gt; Try Again (n=10) 73.7% success; Bash: Single Turn (All) 34.0% -&gt; Try Again (n=10) 48.5%",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "None introduced in this paper (pretrained LLM; standard transformer architecture).",
            "training_method": "prompting only / inference-time evaluation (no fine-tuning reported).",
            "intervention_type": "prompting strategy; interactive environment",
            "intervention_description": "Compared Single-Turn (static seq2seq) generation against multi-turn interactive protocols (Try Again, ReAct, Plan & Solve). Models receive execution feedback (observations) from Dockerized execution environments and can iteratively edit code/actions.",
            "intervention_effect": "Very large improvement on SQL when moving from Single Turn to interactive Try Again (success rate from 9.1% -&gt; 73.7%). Moderate improvement on Bash (34.0% -&gt; 48.5%).",
            "hypothesized_cause_of_gap": "Static single-turn generation cannot iteratively discover context, correct errors, or compose multi-step procedures; interactive feedback permits context discovery and error correction which yields large gains, especially for tasks requiring sequential decision-making (SQL).",
            "uuid": "e947.0",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "gpt-3.5-turbo",
            "brief_description": "OpenAI chat-style LLM used via API for prompting experiments; evaluated on InterCode interactive code-generation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "gpt-3.5-turbo",
            "model_description": "Pretrained transformer LLM accessed via API with temperature=0 for deterministic generation; evaluated only with prompting/inference (no training changes).",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "multi-step reasoning / sequential decision-making (SQL); multi-step procedural tool use (Bash)",
            "interactive_performance": "SQL: Single Turn (All) 10.5% -&gt; Try Again (n=10) 47.3% success; Bash: Single Turn (All) 34.5% -&gt; Try Again (n=10) 46.5% success. With schema provided in prompt: Single Turn (All) 67.9% -&gt; Try Again 72.8%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "None changed; standard pretrained transformer LLM.",
            "training_method": "prompting only / inference-time evaluation (no additional fine-tuning).",
            "intervention_type": "prompting strategy; prompt augmentation (provide DB schema)",
            "intervention_description": "Evaluated multiple prompting strategies: Single Turn (zero-shot), Try Again (iterative interactive loop with execution feedback), ReAct, and Plan & Solve. Also ran ablation adding the full SQL database schema to the Single Turn prompt.",
            "intervention_effect": "Try Again improves SQL from 10.5% -&gt; 47.3% and Bash from 34.5% -&gt; 46.5%. ReAct further improves SQL (Try Again 47.3% -&gt; ReAct 58.7% for gpt-3.5-turbo) while effects differ by task. Providing full DB schema in prompt dramatically boosts Single Turn SQL (10.5% -&gt; 67.9%), but Try Again still outperforms slightly (72.8%).",
            "hypothesized_cause_of_gap": "Interactive tasks require context discovery, conditional action construction, and iterative error correction; single-turn prompts lack the ability to explore state, causing a gap. Also models struggle to use long interaction history effectively as turns accumulate.",
            "uuid": "e947.1",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003",
            "brief_description": "OpenAI completion-style LLM used as a baseline for static vs interactive code generation experiments in InterCode.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "text-davinci-003",
            "model_description": "Pretrained autoregressive transformer model accessed for single-turn and multi-turn prompting experiments; no finetuning in this work.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "multi-step reasoning / sequential decision-making; multi-step procedural tool use",
            "interactive_performance": "SQL: Single Turn (All) 7.4% -&gt; Try Again (n=10) 15.6%; Bash: Single Turn (All) 24.6% -&gt; Try Again 38.7%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "None introduced in this paper (standard pretrained transformer).",
            "training_method": "prompting only / inference",
            "intervention_type": "prompting strategy (Try Again multi-turn, ReAct, Plan & Solve tested)",
            "intervention_description": "Same multi-turn prompting strategies as other models; interactive execution feedback used to enable iterative correction.",
            "intervention_effect": "Moderate gains from single-turn to interactive: SQL 7.4% -&gt; 15.6%; Bash 24.6% -&gt; 38.7%.",
            "hypothesized_cause_of_gap": "Single-turn generation cannot iteratively inspect or correct actions; the interactive setting allows small models to recover from initial generation errors but gains are smaller than for larger models.",
            "uuid": "e947.2",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "text-bison-001",
            "name_full": "PaLM-2 text-bison-001",
            "brief_description": "PaLM-2 family model (text bison) evaluated in InterCode for single-turn vs interactive code tasks; used via API in inference-only setting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "text-bison-001",
            "model_description": "PaLM-2 family text model used for zero-shot and multi-turn prompting experiments; no architecture/training changes in this work.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "multi-step reasoning (SQL); multi-step procedural (Bash)",
            "interactive_performance": "SQL: Single Turn (All) 11.5% -&gt; Try Again 12.9%; Bash: Single Turn (All) 17.0% -&gt; Try Again 22.5%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No changes (pretrained PaLM-2).",
            "training_method": "prompting only / inference",
            "intervention_type": "prompting strategies (Try Again, ReAct, Plan & Solve)",
            "intervention_description": "Evaluated prompting strategies and interactive feedback; no model retraining.",
            "intervention_effect": "Small improvements with interaction on Bash; minimal effect on SQL for text-bison in these runs.",
            "hypothesized_cause_of_gap": "Similar reasoning: static generation lacks iterative correction and context exploration capabilities required for interactive tasks.",
            "uuid": "e947.3",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "chat-bison-001",
            "name_full": "PaLM-2 chat-bison-001",
            "brief_description": "PaLM-2 chat model used via API for InterCode evaluations; included in single-turn and interactive comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "chat-bison-001",
            "model_description": "Chat-oriented variant of PaLM-2 used with prompting; inference-only evaluation without fine-tuning.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "multi-step reasoning; procedural tool use",
            "interactive_performance": "SQL: Single Turn (All) 7.9% -&gt; Try Again 9.9%; Bash: Single Turn (All) 17.7% -&gt; Try Again 19.2%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No changes in this work.",
            "training_method": "prompting only / inference",
            "intervention_type": "prompting strategy",
            "intervention_description": "Assessed multi-turn prompting and execution feedback's impact; used same interactive protocols.",
            "intervention_effect": "Small improvements with interactive Try Again on both tasks.",
            "hypothesized_cause_of_gap": "Interactive tasks demand iterative exploration and correction not available in static single-turn prompting.",
            "uuid": "e947.4",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Vicuna-13B",
            "name_full": "Vicuna-13B",
            "brief_description": "Open-source 13B chat LLM (Vicuna) evaluated in InterCode on static vs interactive code tasks; inference-only.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Vicuna-13B",
            "model_description": "Open-source chat model (13B) used as baseline for interactive coding; no extra training in this paper.",
            "model_size": "13B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "multi-step reasoning / procedural tool use",
            "interactive_performance": "SQL: Single Turn (All) 2.6% -&gt; Try Again 6.3%; Bash: Single Turn (All) 16.0% -&gt; Try Again 24.5%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Standard transformer-based chat model; no architecture changes.",
            "training_method": "prompting only / inference",
            "intervention_type": "prompting strategy; interactive environment",
            "intervention_description": "Evaluated with Single Turn and Try Again interactive protocols; observed gains from interaction though smaller absolute performance than larger closed models.",
            "intervention_effect": "Improvement from Single Turn to Try Again on both SQL and Bash, but absolute success rates remain low (e.g., SQL 2.6% -&gt; 6.3%).",
            "hypothesized_cause_of_gap": "Smaller models have less capacity to plan and utilize multi-turn feedback effectively; interactive correction helps but does not close the gap to larger models.",
            "uuid": "e947.5",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "StarChat-16B",
            "name_full": "StarChat-16B",
            "brief_description": "Open-source 16B LLM used as an evaluated baseline in InterCode for static vs interactive code tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "StarChat-16B",
            "model_description": "Open-source large transformer model evaluated via prompting; no training changes in experiments.",
            "model_size": "16B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "multi-step reasoning; multi-step procedural",
            "interactive_performance": "SQL: Single Turn (All) 8.9% -&gt; Try Again 9.7%; Bash: Single Turn (All) 17.7% -&gt; Try Again 23.7%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No architecture interventions in this work.",
            "training_method": "prompting only / inference",
            "intervention_type": "prompting strategy; interactive environment",
            "intervention_description": "Assessed single-turn vs interactive multi-turn prompting; execution feedback used to iteratively refine actions.",
            "intervention_effect": "Small improvements from interaction on Bash; minimal change for SQL in these reported runs.",
            "hypothesized_cause_of_gap": "Same general causes: need for exploration, context discovery, and iterative correction that single-turn prompts cannot provide.",
            "uuid": "e947.6",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Try Again",
            "name_full": "Try Again (iterative feedback baseline)",
            "brief_description": "A simple iterative prompting protocol where the agent repeatedly issues actions, receives execution feedback, and can retry until success or budget exhausted.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Try Again",
            "model_description": "Prompting/workflow intervention (not a model): agent informed of interactive setting and allowed up to n turns to act and receive execution feedback; termination upon success or turn limit.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "iterative multi-step procedural / sequential decision-making",
            "interactive_performance": "Used as baseline: e.g. gpt-3.5-turbo SQL SR 47.3% (avg turns 7.25); Bash SR 46.5% (avg turns 6.15).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "prompting-only workflow",
            "intervention_type": "prompting strategy / interaction protocol",
            "intervention_description": "Provides a generic iterative loop that feeds execution outputs back as observations and allows multiple attempts; designed to mimic human write-execute-test loop.",
            "intervention_effect": "Substantial improvements over Single Turn for many models (e.g., GPT-4 SQL 9.1% -&gt; Try Again 73.7%).",
            "hypothesized_cause_of_gap": "Enables error correction and context discovery unavailable to static generation, explaining much of the interactive vs single-turn performance gap.",
            "uuid": "e947.7",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reason + Act prompting)",
            "brief_description": "A prompting framework that interleaves reasoning traces ('thoughts') with external actions to encourage models to both think and act during multi-step tasks.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct",
            "model_description": "Prompting/workflow intervention that elicits interleaved natural-language reasoning and actions (commands) from the LLM, allowing the model to plan, query environment, and react to observations.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "multi-step reasoning and sequential decision-making with tool-use",
            "interactive_performance": "For gpt-3.5-turbo aggregated: SQL SR 58.7% (avg turns 5.30, Error% 6.94) vs Try Again 47.3%; Bash SR 20.5% (avg turns 4.40, Error% 20.4) vs Try Again 46.5%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No architecture change  a prompting strategy that elicits reasoning and actions.",
            "training_method": "prompting / in-context workflow",
            "intervention_type": "prompting strategy (reason+act)",
            "intervention_description": "Prompts models to produce interleaved chains of thought and actions, encouraging exploration and explicit reasoning steps tied to environment actions.",
            "intervention_effect": "On SQL (gpt-3.5-turbo) ReAct improved success rate vs Try Again (47.3% -&gt; 58.7%) and reduced non-admissible actions; on Bash ReAct underperformed relative to Try Again (20.5% vs 46.5%)  demonstrating task-dependent effects.",
            "hypothesized_cause_of_gap": "ReAct helps tasks that benefit from exploratory reasoning and conditional action selection (SQL), but its unconstrained reasoning pattern can hurt more 'imperative' multi-step planning tasks (some Bash tasks).",
            "uuid": "e947.8",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Plan & Solve",
            "name_full": "Plan & Solve prompting",
            "brief_description": "A prompting approach that first elicits an explicit high-level plan (decomposition) and then generates solutions for plan steps; designed to improve chain-of-thought style reasoning.",
            "citation_title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
            "mention_or_use": "use",
            "model_or_agent_name": "Plan & Solve",
            "model_description": "Prompting workflow that encourages decomposition of the problem into an explicit plan then solving subproblems; no architectural change.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL; InterCode-Bash",
            "interactive_task_type": "planning / multi-step decomposition for procedural tasks",
            "interactive_performance": "Using gpt-3.5-turbo: SQL SR 49.1% (avg turns 4.29, Error% 16.2) vs Try Again 47.3%; Bash SR 28.0% (avg turns 6.65, Error% 53.3) vs Try Again 46.5%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "None (prompting-only decomposition)",
            "training_method": "prompting / in-context decomposition",
            "intervention_type": "prompting strategy (plan then solve)",
            "intervention_description": "Explicitly instructs the model to produce a plan (decomposition) first and then solve each subtask, encouraging structured multi-step problem solving.",
            "intervention_effect": "Improved or comparable performance on SQL relative to Try Again (49.1% vs 47.3%) but underperformed on Bash (28.0% vs 46.5%), indicating planning helps some task types (declarative/multi-subquery SQL) but can be too rigid for tasks requiring on-the-fly adaptation to execution feedback.",
            "hypothesized_cause_of_gap": "Rigid decomposition may fail to incorporate execution feedback fluidly; interactive tasks that require frequent adjustment favor more flexible reasoning strategies.",
            "uuid": "e947.9",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Schema-provision",
            "name_full": "Prompt augmentation with full database schema",
            "brief_description": "A prompt-level intervention that provides the entire SQL database schema in the Single-Turn question message to supply more context to the model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Schema-provision (provide DB schema in prompt)",
            "model_description": "Not a model  a prompt augmentation intervention where the full SQL schema is appended to the prompt to reduce the need for interaction for context discovery.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "InterCode-SQL",
            "interactive_task_type": "context-augmentation for multi-step reasoning (SQL)",
            "interactive_performance": "For gpt-3.5-turbo: Single Turn (All) jump from 10.5% -&gt; 67.9% when schema provided; Try Again (n=10) performance with schema 72.8%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "prompt augmentation / in-context information",
            "intervention_type": "prompt augmentation",
            "intervention_description": "Includes the complete database schema in the Single Turn prompt to supply the model with the contextual information it would otherwise need to discover via interaction.",
            "intervention_effect": "Dramatically improves Single Turn SQL performance (gpt-3.5-turbo Single Turn All 10.5% -&gt; with schema 67.9%), narrowing the gap with interactive methods though Try Again still slightly outperforms (72.8%).",
            "hypothesized_cause_of_gap": "A large component of the interactive advantage is that interaction enables selective context discovery; providing that context up-front substitutes for interaction and closes much of the gap.",
            "uuid": "e947.10",
            "source_info": {
                "paper_title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
            "rating": 2
        },
        {
            "paper_title": "Execution-guided neural program synthesis",
            "rating": 2
        },
        {
            "paper_title": "Executionbased evaluation for data science code generation models",
            "rating": 1
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 1
        }
    ],
    "cost": 0.02027725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback</h1>
<p>John Yang Akshara Prabhakar Karthik Narasimhan Shunyu Yao<br>Department of Computer Science, Princeton University<br>{jy1682, ap5697, karthikn, shunyuy}@princeton.edu</p>
<h4>Abstract</h4>
<p>Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash [32], Spider [55], and MBPP [4] datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct [51] and Plan \&amp; Solve [43]. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. ${ }^{*}$</p>
<h2>1 Introduction</h2>
<p>The art of computer programming is naturally an interactive process. When a human programmer writes code, she relies on several iterations of a 'write-execute-test' loop in order to iteratively refine solutions, plan changes, test sub-modules, and solve ambiguities by checking execution behavior. While this is reminiscent of other human endeavors like writing, code compilation and execution produce exact results that provide a deterministic form of feedback to make the refinement process more straightforward. Depending on the observed results, programmers perform various levels of debugging and rewriting, and continue the process until their code satisfies the requirements.</p>
<p>There has been increasing interest in recent years around the development of models that can automatically generate code given a specification in natural language [18, 46, 14, 29, 25]. Powered by large-scale pre-training over thousands of codebases [2, 22, 19], these models have shown solid performance on static benchmarks like HumanEval [9], APPS [20], MBPP [4], CodeXGLUE [33]. However, generating code in a static, sequence-to-sequence or auto-regressive fashion has several drawbacks: 1) simple errors (even typos) can propagate and there is no chance for recovery or</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of InterCode. Setting up an interactive code environment with InterCode requires a Dockerfile, dataset, reward function definition, and a small amount of subclass implementation. The interactive loop between agent and environment closely mirrors real world software development processes. While InterCode task performance is generally quantified as a binary 0/1 completion score, InterCode allows for the design of more complex evaluation criteria that can incorporate execution output and the effects of interaction on the state space.
revision, 2) there is a disconnect between the code generation process and its downstream execution on the desired software and hardware environment, and 3) there is little room for human intervention or collaboration in the code generation process.
Recently, some works have proposed the use of execution feedback or interaction [47] to benefit code generation models [24, 21, 48, 20]. However, these papers consider their own individual setup and are difficult to compare with one other due to the use of different compilers, execution environments, feedback signals, and assumptions on the interactive process such as human participation to create task descriptions or provide natural language feedback. This makes it difficult to compare existing methods for code generation and to clearly understand the benefits of interactive generation.
To address these issues, we propose InterCode, the first standard coding benchmark designed natively with an interactive execution environment. Closely mimicking the human decision-making process, InterCode allows a coding agent to interactively receive feedback from compilers/interpreters that execute its code, and to submit further refinements. We design InterCode to be like a standard reinforcement learning (RL) environment that requires minimal human intervention and one in which generated code is treated as actions, which are executed to reveal observations. Our framework is (1) language and platform agnostic and can easily be used for new coding problems, (2) uses self-contained Docker environments to provide safe execution, and (3) compatible out-of-the-box with traditional seq2seq generation methods, while also enabling and empowering the development of new interactive techniques.
We demonstrate the power of the framework by implementing Bash, SQL, and Python tasks within InterCode, building on pre-existing static datasets [62, 32, 4]. We perform experiments across diverse models and prompting methods, including ReAct [51] and Plan \&amp; Solve [43]. Our findings concretely showcase the benefits of interaction towards solving coding tasks, discuss the distribution of distinct code understanding challenges across different task settings, and explore the ease with which new tasks and datasets can be defined using InterCode.</p>
<p>To summarize, our paper makes the following contributions:</p>
<ul>
<li>We develop InterCode, a new, universal framework for interactive code generation, which provides ease of use, extensibility, and safety.</li>
<li>Using InterCode, we perform a comprehensive evaluation of state-of-the-art models and identify several avenues for improvements.</li>
<li>We release our framework as a new benchmark along with useful empirical tools to customize any new static code datasets into interactive tasks.</li>
</ul>
<h1>2 Related Work</h1>
<p>Interactive environments for coding. Most coding benchmarks (e.g. SQL - Spider [55], KaggleDBQA [26]; Bash - NLC2CMD [1], NL2Bash [32]; Python - HumanEval [9], APPS [20], MBPP [4], CodeXGLUE [33], CodeNet [38]) frame the coding problem as a sequence transduction problem (from instruction to code), rather than an interactive decision making problem with an execution environment. Attempts have been made to simulate interaction by developing conversational, dialoguestyle [57, 56], multi-step problem solving [36] datasets, which involve pre-annotated human-designed queries. The work closest to InterCode has been recent explorations of Python Jupyter Notebooks as a natural choice for interactive coding [21, 24, 54]. However, task data and settings often constrain allowed actions to a closed domain of code and libraries [24, 54], use evaluation procedures or metrics that may not generalize [21], require human-in-the-loop participation (i.e. create task contexts, write problems, evaluate execution per task instance) [24], or are Python-exclusive [21, 24, 54, 48]. InterCode provides a more general purpose foundation defining interactive coding tasks that enables easy construction of diverse task settings, can have any programming language(s) as the action space, and has automatic, execution-based evaluation.
Execution-based evaluation for coding. Evaluation for NL-to-code generation models has recently shifted away from surface form similarity metrics (BLEU [37, 2], ROUGE [31], Exact Match) towards execution oriented ratings (unit tests [4, 9, 21, 24, 20], output matching [16, 21, 62]). The rigidity of surface form analysis overlooks code syntax features, ignores execution effect, or over-penalizes alternative solutions [63], On the contrary, execution-based assessment is a more thorough and comprehensive score of code functionality [20] and is a more natural fit for open-domain program usage that does not constrain code generation to a subset of the language space [48]. However, for newer benchmarks and datasets that put forth task definitions incorporating execution-based evaluation (APPS [20], ExeDS [21], ODEX [48]), the fundamental code generation task (Context + Code $\rightarrow$ Execution $\rightarrow$ Score) is still devoid of interaction. InterCode combines execution-based evaluation with flexible task construction, enabling more diverse problem-solving paradigms within a unified coding task formulation. InterCode's use of virtual containers as execution sandboxes protect against harmful actions and allow for advanced evaluation criteria beyond the aforementioned ones.
Methods for interactive or execution-based coding. The value of generative code models and interactive problem solving has motivated a recent proliferation of work to augment reasoning capabilities' of existing language models [51, 40, 43, 50, 60, 12] or propose new modeling techniques to tackle coding as a sequential decision making and reasoning tasks [6, 11, 17, 29, 8, 25], of which evaluation is unit test based. Approaches that leverage execution typically use re-ranking [61, 35, 53, 58] or majority vote [11, 29, 39] to decide on a final prediction. Additional work also explores incorporating human-in-the-loop [7, 23], compilers [44], and text [45, 59] feedback. A common thread among these contributions is that 1) the task setting can only provide the investigated form of feedback and 2) sought-after capabilities are exemplified by strong performance on favorably curated tasks and datasets, rendering comparisons across benchmarks tedious. InterCode has the potential to standardize the evaluation of these methods because 1) the interactive coding task is a conglomeration of many interesting interaction, reasoning, and decision-making challenges and 2) InterCode's task construction makes it possible to incorporate a wide variety of sources of feedback.</p>
<h2>3 The InterCode Benchmark</h2>
<h3>3.1 Formulation</h3>
<p>The InterCode benchmark formalizes interactive coding with execution feedback as a partially observable Markov decision process (POMDP) $(\mathcal{U}, \mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{R})$ with instruction space $\mathcal{U}$, state</p>
<table>
<thead>
<tr>
<th>Action Space</th>
<th>Environment</th>
<th>Dataset</th>
<th>Reward Function</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bash</td>
<td>Ubuntu Terminal</td>
<td>NL2Bash [32] (200)</td>
<td>Latest Std. Output + File System $\Delta$</td>
</tr>
<tr>
<td>SQL</td>
<td>MySQL Database</td>
<td>Spider 1.0 [55] (1034)</td>
<td>Latest Std. Output</td>
</tr>
<tr>
<td>Python</td>
<td>Python Interpreter</td>
<td>MBPP [4] (117)</td>
<td>Submitted Function</td>
</tr>
</tbody>
</table>
<p>Table 1: Rundown of the two environments with Bash and SQL as action spaces developed using the InterCode framework. The numbers in parentheses refer to the number of task instances adopted from each dataset. Each environment is defined in under 200 lines of code total. Specific discussion of the environment construction and reward function can be found in $\S$ A. 2 and $\S$ A. 3
space $\mathcal{S}$, action space $\mathcal{A}$, observation space $\mathcal{O}$, transition function $\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, and reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$. Given a coding instruction $u \in \mathcal{U}$ in natural language, an agent issues code or a special submit keyword as an action $a_{t} \in \mathcal{A}$. An action is admissible [49] if it can be parsed and executed in the compiler/interpreter environment, and an admissible action incurs a change in the latent state space $s_{t+1} \in \mathcal{S}$, and an execution feedback as observation $o_{t+1} \in \mathcal{O}$. The interaction loop repeats until the submit action is issued, wherein the task episode ends and a reward $r=\mathcal{R}\left(s_{T}\right.$, submit $) \in[0,1]$ is computed, with 1 representing task completion. We use the Success Rate (SR) metric, defined as the proportion of task episodes where $r=1$. We also define the Error \% metric, which is the percentage of non admissible actions across task episodes.</p>
<h1>3.2 Construction pipeline</h1>
<p>At a high level, InterCode decomposes the construction of an interactive coding task into three modular parts: (1) environment construction, (2) data collection, and (3) reward design. This workflow allows for the safe execution of transition functions, flexible reward design, and convenient adaptation of existing instructions to an interactive setting.
Docker-based environments. InterCode uses Docker [34] virtual containers as a general-purpose execution sandbox. Given a Dockerfile that defines a system and execution entrypoint, InterCode creates a corresponding, stateful virtual container that hosts the desired state space and transition function. We choose Docker as the basis of InterCode's environment construction for its safe execution in virtual containers, reproducibility of a Dockerfile across any Docker-equipped machine, and excellent coverage of application code, libraries, and dependencies offered by the Dockerfile DSL.
Data collection. InterCode requires that a dataset has at minimum two fields: query, a natural language instruction $u \in \mathcal{U}$, and gold, an answer or code block that is a procedure for generating the correct answer. We define these conditions to make it easy to adapt existing text-to-code datasets to an interactive setting while also leaving plenty of bandwidth for constructing new tasks and datasets.
Reward design. Across a single task episode, the action, observation, and state modification (if any) per interaction loop are implicitly logged by InterCode. InterCode's default reward function determines task completion via an exact match of the agent's execution output (observation and state modifications) against the gold command, where 1 is awarded only if all components match. Since Exact Match is usually too stringent of an evaluation criteria, InterCode exposes a reward function endpoint that has access to both the interaction history and the execution container, allowing for custom reward function definitions that can incorporate multiple signals.</p>
<h3>3.3 Implementations</h3>
<p>Following the procedure discussed in Section 3.2, we create two separate InterCode based environments where Bash and SQL are the action spaces respectively. Table 1 summarizes them.
InterCode-Bash. We define a bash shell within an Ubuntu Operating System as the task setting. To evaluate an agent's ability to adapt generations to different situations, we architect four distinct file systems that can be swapped into the Bash environment by changing a single line in the Dockerfile.
We bootstrap the NL2Bash [32] dataset (which lacks specificity in queries and grounding to any underlying file system, preventing it from being used directly for interactive evaluations) to create an interactive coding task where an agent completes an instruction via bash actions. Transferring NL2Bash to the interactive task setting requires simple transformations to ground instructions and gold code blocks in the file system. First, we consider a subset of 1000 commands with each</p>
<p>having $\geq 4$ utilities. We then filter out commands that are non-UNIX, non-Linux, or use utilities we currently do not support (eg. "ssh", "sudo", time, and GUI-dependent utilities). Finally, we enhance under-specified commands with specific file names/directory names/paths and update deprecated utilities/flags. The resulting 200 commands are grouped into 4 disjoint sets, 3 of which were grounded to custom-designed file systems, while one set is file-system agnostic. This categorization allows for a comprehensive evaluation of different command-grounding scenarios.</p>
<p>The InterCode-Bash dataset instructions typically make one or both of the following two types of requests. It either 1. Requests information that can be answered via execution output (i.e. "How many files...", "What is the size of...", "Where is <file> stored?") or 2. Requests a change to the location/configuration/content of a file or folder (i.e. "Move dir1 folder...", "Set permissions of...", "Append a line to..."). Therefore, we define a custom reward function that evaluates an agent's performance against file system modifications and the latest execution output. Execution output is graded with a simple lexical similarity function. File system assessment is done in two parts. First, a comparison of the agent's and gold command's list of file system changes (list of [path, modification type $\in$ [added, changed, deleted]] entries) reveals any extraneous or missing changes. Second, md5sum hashes of each commonly edited file path are compared to determine if an added or changed file was altered correctly. A max score of 1 is achieved only if the correct file paths are changed, the changes are correct, and the latest execution output matches the gold command output exactly. Additional Bash statistics and design details are discussed in  A.2.</p>
<p>InterCode-SQL. We write a Dockerfile that defines a SQL interpreter within a MySQL database as the task setting. To create the databases and tables necessary for the task dataset, we write type resolution scripts and perform database conversions using the sqlite3mysql [41] Python library to adapt the Spider [55] database and table schema to a MySQL format. We then consolidate all setup code into a single, unified MySQL . sql dump that contains the complete set of schemas for all tables across 20 different databases. On container start-up, this file is invoked automatically, creating and populating databases with tables and tables with records.</p>
<p>The Spider [55] dataset is a large-scale cross-domain dataset originally meant for evaluating SQL query generations from natural language questions. We adapt the development set, which contains 1034 task instances, and remove all extraneous columns aside from the natural language questions and gold SQL command. The instruction and gold values do not require any additional pre-processing to be compatible with the MySQL task environment.</p>
<p>Finally, we employ Intersection over Union (IoU), or more formally the Jaccard Index, to quantify the correctness of the latest execution output generated by the agent against the gold output, where both outputs are a list of records. A non-tabular execution output receives a reward of 0 by default. Among the items that lie in the intersection of the agent and gold execution outputs, we also apply a penalty if the records are in the incorrect order. To quantify how sorted the agent output is relative to the gold output, we lean on Kendall's $\tau$ and adjust the output range to $[0,1]$. The IoU score is then directly scaled by this coefficient. All in all, only a correctly ordered list with the exact set of records found in the gold output receives a score of 1 . Visualizations like Figure 1 for SQL along with a more extensive implementation discussion for this environment are in  A. 3</p>
<p>InterCode-Python. In this setting, we define a Python interpreter running within an Ubuntu operating System as the task setting. The Dockerfile can be configured to run any Python version. The interpreter is not initialized with any dependencies, but PyPI packages can be installed and used by the agent.</p>
<p>We use the MBPP [4] dataset which presents the code completion task of synthesizing Python code from a method header and docstring. Evaluation of correctness is performed with an associated set of unit tests given by MBPP. The MBPP dataset is straightforward to adapt to the interactive setting, requiring no modifications to the query or evaluation components. Finally, we directly inherit MBPP's evaluation procedure of proportion of unit tests passed. With InterCode, it is easy to use existing datasets to evaluate how well models can use different programming languages as actions.</p>
<p>Validations. To verify the functionality of action execution in the task environment and the correctness of custom reward functions, we write testing scripts for both Bash and SQL that pass the gold command in as a dummy agent's action to ensure that the command is admissible and executes without error, and to verify that the reward received by the command is 1 . To confirm that InterCode's dataset specification is enforced across multiple accepted file formats, we define a custom InterCode data loader class which is then rigorously unit tested.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of Prompting Strategies adjusted for evaluation on InterCode. The "Try Again" termination constraint is conditioned on reward = 1, while ReAct [51] and Plan \&amp; Solve [43] are determined by the agent itself. This is because the purpose of the "Try Again" method is to explore how capable agents are at error correction from feedback, while the other two are more concerned with the overall success of general problem-solving strategies.</p>
<h1>4 Methods</h1>
<p>We perform preliminary experiments to gauge the proficiency and behavior of current large language models on interactive coding tasks with Bash and SQL. To observe and elicit relevant reasoning skills, we draw on several existing prompting strategies that have been put forth to augment language models' reasoning and problem-solving skills. We apply these prompting strategies to models across the following three families: OpenAI (text-davinci-003, gpt-3.5-turbo, gpt-4), PaLM-2 (text-bison-001, chat-bison-001) [3], and Open Source (Vicuna-13B [13], StarChat-16B [28]).
Figure 2 visualizes the four adjusted prompting strategies we evaluate on InterCode.
Single Turn is a zero-shot attempt. A model is given a simple description of the task setting and asked to generate code in a specific programming language that would address the query. The first generation in response to the user's question is then evaluated in the InterCode environment.
"Try Again" is an iterative feedback set up. In the initial message, the agent is informed of the task setting and its interactive nature; an agent has multiple turns to interact with the system, wherein each turn, upon generating an action, the execution output of the action is fed back as an observation. This continues until a reward of 1 (task completion) is achieved or the number of turns ( $n$ ) is exhausted. The agent's position in this approach is meant to mirror human software development as closely as possible. The goal of this method is to probe language models' raw interactive coding abilities in addition to illustrating the benefits and different challenges that arise in interactive coding tasks.</p>
<p>ReAct and Plan \&amp; Solve. We write prompts and design workflows that follow the text and task configurations described in ReAct [51] and Plan \&amp; Solve [43] as faithfully as possible. For these two approaches, the termination of a task episode is conditioned upon the agent's own judgment, as our goal with these methods is to gauge the transferability to and efficacy of existing reasoning frameworks with respect to the interactive coding task. Full prompt templates are included in $\S$ B.7.</p>
<h2>5 Experiments</h2>
<h3>5.1 Base models comparison</h3>
<p>Task performances. We first compare the success rate of models in the Single Turn and Try Again settings for both the InterCode-Bash and SQL datasets. From Table 2 and Table 3, we observe</p>
<p>| InterCode-SQL | Single Turn |  |  |  |  | Try Again $(n=10)$ |  |  |  |  |
| Model / Hardness | Easy | Med | Hard | Extra | All | Easy | Med | Hard | Extra | All |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| text-davinci-003 | 20.6 | 4.9 | 1.7 | 0.0 | 7.4 | 32.4 | 14.6 | 5.2 | 4.2 | 15.6 |
| gpt-3.5-turbo | 22.6 | 8.3 | 5.7 | 3.6 | 10.5 | 72.5 | 44.3 | 43.7 | 21.1 | 47.3 |
| gpt-4 | 19.8 | 7.2 | 4.6 | 3.0 | 9.1 | 87.5 | 76.7 | 66.7 | 52.4 | 73.7 |
| text-bison-001 | 23.8 | 10.9 | 5.7 | 0.6 | 11.5 | 27.0 | 12.3 | 5.7 | 0.6 | 12.9 |
| chat-bison-001 | 18.5 | 6.5 | 4.0 | 0.0 | 7.9 | 22.2 | 7.8 | 6.9 | 0.0 | 9.9 |
| Vicuna-13B | 8.1 | 1.3 | 0.6 | 0.0 | 2.6 | 18.9 | 3.4 | 1.7 | 0.0 | 6.3 |
| StarChat-16B | 21.8 | 7.4 | 2.9 | 0.0 | 8.9 | 22.3 | 8.5 | 2.9 | 1.2 | 9.7 |</p>
<p>Table 2: Success Rate for single vs. multi turn evaluation on InterCode-SQL (refer A.3). Query difficulty is adopted from Spider [55]. Best metrics are in bold.</p>
<p>| InterCode-Bash | Single Turn |  |  |  |  |  | Try Again $(n=10)$ |  |  |  |
| Model / File System | 1 | 2 | 3 | 4 | All | 1 | 2 | 3 | 4 | All |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| text-davinci-003 | 10.0 | 32.1 | 28.8 | 33.3 | 24.6 | 30.0 | 52.8 | 32.2 | 44.4 | 38.7 |
| gpt-3.5-turbo | 30.0 | 39.6 | 33.3 | 37.0 | 34.5 | 45.0 | 49.1 | 45.0 | 48.1 | 46.5 |
| gpt-4 | 25.0 | 37.7 | 36.7 | 40.7 | 34.0 | 41.7 | 47.2 | 51.7 | 59.2 | 48.5 |
| text-bison-001 | 15.0 | 22.6 | 11.7 | 22.2 | 17.0 | 23.3 | 28.3 | 16.7 | 22.2 | 22.5 |
| chat-bison-001 | 12.1 | 22.5 | 16.7 | 22.2 | 17.7 | 13.8 | 24.5 | 18.3 | 22.2 | 19.2 |
| Vicuna-13B | 10.0 | 24.5 | 18.3 | 7.4 | 16.0 | 15.0 | 35.8 | 25.0 | 22.2 | 24.5 |
| StarChat-16B | 15.5 | 22.6 | 13.3 | 22.2 | 17.7 | 17.2 | 30.2 | 21.7 | 29.6 | 23.7 |</p>
<p>Table 3: Success Rate across file systems for single vs. multi-turn evaluation on InterCode-Bash (refer A.2). To evaluate models' ability to interact with different task settings, we evaluate disjoint sets of Bash instructions across four different file systems. Best metrics are in bold.
that performance across different levels of task difficulty (SQL) and different file systems (Bash) is superior in the interactive setting for all models, with a notable multi-fold increase for GPT-4 $(9.1 \% \rightarrow 73.7 \%)$ on the InterCode-SQL task.</p>
<p>Analysis of interactions. Manual inspection of trajectory logs indicates that models actively exercise later turns for discovering relevant context, correcting errors via execution feedback as observations, and solving problems via iteratively constructing and editing actions as affirmed by Figure 3. In addition, models also demonstrate a level of planning and modular problem solving; for instructions with gold commands that chain multiple commands together (i.e. with $1,&gt;$, or ; in bash) or consist of multiple sub-problems (i.e. subqueries in SQL), models will use observations from solving smaller sub-problems in earlier turns to compose the higher-order action. Trajectories that exhibit these phenomena are in  B. 4
Failure cases. With that said, both Figure 3 exhibits a plateauing in Success Rate and and Error $\%$. This suggests that as the amount of context and feedback builds up, models are less capable of discerning relevant past history toward future actions. In late-turn scenarios, task episode trajectories often reveal repetition of earlier actions, a failure to effectively use recent observations towards deciding an appropriate next action, or an inability to recognize that a current problem-solving chain of thought is inconclusive or futile. This is particularly evident for hard and extra level InterCodeSQL task instructions that require context spanning across several tables and actions that incorporate multiple clauses. We note that even when the full schema of all tables and their descriptions are offered in addition to the original instructions, models still benefit greatly from using interaction to experiment with different JOIS and filtering operators across multiple turns, as demonstrated in  B.2. A larger context window size, retrieval of useful memory, and more adaptive reasoning paradigms are just a handful of potential solutions to overcoming such challenges.</p>
<h1>5.2 Prompting strategy comparison</h1>
<p>Initiating language agents with prompting strategies that encourage different forms of reasoning toward problem-solving improves performance on the interactive coding task to varying degrees. Table 4 presents side-by-side comparisons of the success rate, number of turns, and error rate per strategy. Compared to Try Again, which lacks specific guidance on leveraging multiple turns, more</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Success rate vs. turns for InterCode-Bash
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Success rate vs. turns for InterCode-SQL</p>
<p>Figure 3: Growth in Success Rate with increase in number of interaction turns across models configured with Try Again prompting strategy for InterCode-Bash and SQL tasks.</p>
<table>
<thead>
<tr>
<th></th>
<th>Try Again $(n=10)$</th>
<th></th>
<th></th>
<th>ReAct $(n=10)$</th>
<th></th>
<th></th>
<th>Plan \&amp; Solve</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>SR</td>
<td>Turns</td>
<td>Error \%</td>
<td>SR</td>
<td>Turns</td>
<td>Error \%</td>
<td>SR</td>
<td>Turns</td>
<td>Error \%</td>
</tr>
<tr>
<td>SQL</td>
<td>47.3</td>
<td>7.25</td>
<td>46.4</td>
<td>58.7</td>
<td>5.30</td>
<td>6.94</td>
<td>49.1</td>
<td>4.29</td>
<td>16.2</td>
</tr>
<tr>
<td>Bash</td>
<td>46.5</td>
<td>6.15</td>
<td>24.9</td>
<td>20.5</td>
<td>4.40</td>
<td>20.4</td>
<td>28.0</td>
<td>6.65</td>
<td>53.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of different prompting strategies across the entire InterCode-SQL and InterCodeBash datasets using gpt-3.5-turbo as the base model. Turns refers to the average number of turns taken for a single task episode. For Try Again and ReAct, the max number of turns $n=10$. The highest Success Rate, fewest Turns, and lowest Error \% are highlighted per dataset since they reflect more accuracy and efficient task solving. Best metrics are in bold.
explicit reasoning frameworks such as ReAct and Plan \&amp; Solve policies generally achieve higher success rates (SQL: $47.3 \% \rightarrow 58.7 \%$ ) with fewer turns and a higher rate of admissible commands.
Different tasks present different learning challenges. An important skill to solving the InterCodeSQL task is the ability to discover context and construct actions conditionally based on information revealed in prior observations. Given that InterCode-SQL task instructions are phrased most commonly as questions, adapting to the task setting and new information discovered along the way puts more emphasis on error correction and context discovery. On the other hand, the more declarative and multi-step nature of the InterCode-Bash task instructions is more aptly solved by planning and modular task completion. These distinctions manifest in the Plan \&amp; Solve strategy's performance gap between the InterCode-SQL and InterCode-Bash tasks; while Plan \&amp; Solve encourages a model to decompose problems into more manageable steps, the strategy is less favorable towards adjusting on the fly in response to execution feedback. Example trajectories supporting these claims are in  B.4.
More adaptive reasoning is favorable. Compared to "imperative" reasoning paradigms such as Plan \&amp; Solve which prescribe a relatively rigid procedure, more flexible frameworks like ReAct, which do not enforce any particular logical formula or roadmap, are more conducive to eliciting a broader set of reasoning capabilities. However, while ReAct's performance is generally superior to Plan \&amp; Solve, tasks solved by both strategies with gpt-3.5-turbo make up $57 \%(407 / 708)$ and $27.6 \%(21 / 76)$ of the union of all successfully solved InterCode-SQL and InterCode-Bash tasks respectively. This discrepancy highlights a trade-off between the guidance and structural constraints that are inherent to prompting strategies; schemes that draw out specific reasoning patterns often overlook other equally useful capabilities. InterCode's interactive coding task can serve as a strong litmus test toward more adaptable, variegated model reasoning.</p>
<h1>5.3 New tasks \&amp; datasets opportunities</h1>
<p>InterCode's task formulation, modular design, flexible task construction, and use of virtual containers enable task designers to manifest new, complex, code-driven tasks, where completion is much more</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: GPT-4's interaction trajectory for a binary exploitation CTF task. This requires proficiency in Bash and Python, among additional knowledge and reasoning. Orange text and arrows highlight the feedback that the model attends to in generating the next action. In last step, agent submits flag.
attainable through interaction. We draw inspiration from Capture the Flag (CTF) [15], a competitive cybersecurity game that requires expertise in coding, cryptography (i.e. binary exploitation, forensics), reverse engineering, and recognizing security vulnerabilities to accomplish the primary objective of discovering encrypted "flags" concealed within code snippets or file systems. Compared to InterCodeBash \&amp; -SQL, CTF is much more complicated, requiring an agent to exercise knowledge of multiple coding languages, modularize a higher-order objective into sub-problems, construct multi-step plans towards solving each problem, and adjust strategy when a plan fails to yield any useful insights.</p>
<p>We establish InterCode-CTF, a new dataset consisting of 100 CTF objectives from picoCTF [42]. Following the interactive coding task formulation, each task instance in InterCode-CTF is given as a <instruction, assets, hidden flag> tuple. We first construct a Bourne Shell within an Ubuntu OS as the task environment. Here, InterCode's use of virtual containers is crucial, as necessary actions can be irreversibly damaging on real systems (i.e. rm -rf, sudo access). Per task instance, the associated assets (e.g., images, executables, code), necessary for task completion, are copied into the OS file system. Given this setting, a task worker must understand the given material and investigate the assets to develop potential solutions. Executing a successful approach must be done across multiple steps with various conditionals, where the execution feedback of a prior step could have a significant effect on the next step. Figure 4 spotlights the diverse skills needed for CTF.</p>
<h1>6 Discussion</h1>
<p>Conclusion. We have developed InterCode, a novel lightweight framework that facilitates interaction between Language Models and the underlying environment, enabling them to mimic the human approach to language-to-code generation. Our framework has shown promising results when applied to state-of-the-art models using different prompting styles. It effectively leverages the capabilities of LMs to break down complex tasks and recover from errors within a secure and isolated environment. The ability to seamlessly convert existing datasets into the interactive format using InterCodeEnv API, and furthermore, the Bash and SQL environments, empowers task designers to construct new tasks to unlock the plethora of challenges that await in the space of interactive coding.</p>
<p>Limitations and future directions. We point out several current limitations of InterCode. At this time, the number of InterCode based environments is limited to Bash, SQL, and Python action spaces and datasets; within the near future, we plan to expand the number of offerings to cover a wider set of programming languages and datasets that should further deliver on InterCode's purported promises of efficient and expressive task construction. Second, the CTF dataset is limited to just four task instances due to our manual curation procedure. We hope to release more formal work soon that provides a more thorough analysis of the reasoning and collaboration challenges of the CTF task along with a more extensive dataset for evaluation purposes.</p>
<h1>Acknowledgements</h1>
<p>We thank Xiao Liu for the Vicuna/Alpaca APIs, Carlos Jimenez and Yuhan Liu for trying our code, and Princeton NLP Group for helpful discussion and feedback in general. We acknowledge support from the National Science Foundation under Grant No. 2107048. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<h2>References</h2>
<p>[1] M. Agarwal, T. Chakraborti, Q. Fu, D. Gros, X. V. Lin, J. Maene, K. Talamadupula, Z. Teng, and J. White. Neurips 2020 nlc2cmd competition: Translating natural language to bash commands. In H. J. Escalante and K. Hofmann, editors, Proceedings of the NeurIPS 2020 Competition and Demonstration Track, volume 133 of Proceedings of Machine Learning Research, pages 302-324. PMLR, 06-12 Dec 2021. URL https://proceedings.mlr. press/v133/agarwal21b.html.
[2] R. Agashe, S. Iyer, and L. Zettlemoyer. Juice: A large scale distantly supervised dataset for open domain context-based code generation. ArXiv, abs/1910.02216, 2019.
[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, and et al. Palm 2 technical report, 2023.
[4] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton. Program synthesis with large language models, 2021.
[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.
[6] R. Bunel, M. Hausknecht, J. Devlin, R. Singh, and P. Kohli. Leveraging grammar and reinforcement learning for neural program synthesis, 2018.
[7] A. Chen, J. Scheurer, T. Korbak, J. A. Campos, J. S. Chan, S. R. Bowman, K. Cho, and E. Perez. Improving code generation by training with natural language feedback, 2023.
[8] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.
[9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021.
[10] W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023.
[11] X. Chen, C. Liu, and D. X. Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2018.
[12] X. Chen, M. Lin, N. Schrli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.
[13] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
[14] C. B. Clement, D. Drain, J. Timcheck, A. Svyatkovskiy, and N. Sundaresan. Pymt5: multi-mode translation of natural language and python code with transformers, 2020.</p>
<p>[15] C. Cowan, S. Arnold, S. Beattie, C. Wright, and J. Viega. Defcon capture the flag: defending vulnerable code from intense attack. In Proceedings DARPA Information Survivability Conference and Exposition, volume 1, pages 120-129 vol.1, 2003. doi: 10.1109/DISCEX.2003.1194878.
[16] L. Dong and M. Lapata. Language to logical form with neural attention, 2016.
[17] K. Ellis, M. Nye, Y. Pu, F. Sosa, J. Tenenbaum, and A. Solar-Lezama. Write, execute, assess: Program synthesis with a repl, 2019.
[18] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou. Codebert: A pre-trained model for programming and natural languages, 2020.
[19] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.
[20] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021.
[21] J. Huang, C. Wang, J. Zhang, C. Yan, H. Cui, J. P. Inala, C. Clement, and N. Duan. Executionbased evaluation for data science code generation models. In Proceedings of the Fourth Workshop on Data Science with Human-in-the-Loop (Language Advances), pages 28-36, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.dash-1.5.
[22] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search, 2020.
[23] S. K. Lahiri, A. Naik, G. Sakkas, P. Choudhury, C. von Veh, M. Musuvathi, J. P. Inala, C. Wang, and J. Gao. Interactive code generation via test-driven user-intent formalization, 2022.
[24] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, S. W. tau Yih, D. Fried, S. Wang, and T. Yu. Ds-1000: A natural and reliable benchmark for data science code generation. ArXiv, abs/2211.11501, 2022.
[25] H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314-21328, 2022.
[26] C.-H. Lee, O. Polozov, and M. Richardson. KaggleDBQA: Realistic evaluation of text-to-SQL parsers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2261-2273, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.176. URL https://aclanthology.org/2021. acl-long. 176 .
[27] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao, R. Geng, N. Huo, C. Ma, K. C. C. Chang, F. Huang, R. Cheng, and Y. Li. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls, 2023.
[28] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, and et al. Starcoder: may the source be with you!, 2023.
[29] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy, C. de Masson d'Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals. Competition-level code generation with AlphaCode. Science, 378(6624):1092-1097, dec 2022. doi: 10.1126/science.abq1158. URL https://doi.org/10.1126\%2Fscience.abq1158.
[30] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control, 2023.</p>
<p>[31] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.
[32] X. V. Lin, C. Wang, L. Zettlemoyer, and M. D. Ernst. NL2Bash: A corpus and semantic parser for natural language interface to the linux operating system. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL https : //aclanthology.org/L18-1491.
[33] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664, 2021.
[34] D. Merkel. Docker: lightweight linux containers for consistent development and deployment. Linux journal, 2014(239):2, 2014.
[35] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W. tau Yih, S. I. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution, 2023.
[36] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen: An open large language model for code with multi-turn program synthesis. ICLR, 2023.
[37] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics, 2002.
[38] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, V. Zolotov, J. Dolby, J. Chen, M. Choudhury, L. Decker, V. Thost, L. Buratti, S. Pujar, S. Ramji, U. Finkler, S. Malaika, and F. Reiss. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks, 2021.
[39] F. Shi, D. Fried, M. Ghazvininejad, L. Zettlemoyer, and S. I. Wang. Natural language to code translation with execution, 2022.
[40] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
[41] K. Tusar. sqlite3mysql, 2018. URL https://github.com/techouse/sqlite3-to-mysql.
[42] C. M. University. picoCTF, 2013. URL https://picoctf.org/.
[43] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023.
[44] X. Wang, Y. Wang, Y. Wan, F. Mi, Y. Li, P. Zhou, J. Liu, H. Wu, X. Jiang, and Q. Liu. Compilable neural code generation with compiler feedback, 2022.
[45] X. Wang, H. Peng, R. Jabbarvand, and H. Ji. Leti: Learning to generate from textual interactions, 2023.
[46] Y. Wang, W. Wang, S. Joty, and S. C. H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation, 2021.
[47] Z. Wang, G. Zhang, K. Yang, N. Shi, W. Zhou, S. Hao, G. Xiong, Y. Li, M. Y. Sim, X. Chen, Q. Zhu, Z. Yang, A. Nik, Q. Liu, C. Lin, S. Wang, R. Liu, W. Chen, K. Xu, D. Liu, Y. Guo, and J. Fu. Interactive natural language processing, 2023.
[48] Z. Wang, S. Zhou, D. Fried, and G. Neubig. Execution-based evaluation for open-domain code generation, 2023.
[49] S. Yao, R. Rao, M. Hausknecht, and K. Narasimhan. Keep calm and explore: Language models for action generation in text-based games. In Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>[50] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.
[51] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models, 2023.
[52] S. Yao, H. Chen, J. Yang, and K. Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In ArXiv, preprint.
[53] P. Yin and G. Neubig. Reranking for neural semantic parsing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4553-4559, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1447. URL https://aclanthology.org/P19-1447.
[54] P. Yin, W.-D. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland, P. Bailey, M. Catasta, H. Michalewski, A. Polozov, and C. Sutton. Natural language to code generation in interactive data science notebooks, 2022.
[55] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and D. Radev. Spider: A large-scale human-labeled dataset for complex and crossdomain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911-3921, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425.
[56] T. Yu, R. Zhang, H. Y. Er, S. Li, E. Xue, B. Pang, X. V. Lin, Y. C. Tan, T. Shi, Z. Li, Y. Jiang, M. Yasunaga, S. Shim, T. Chen, A. Fabbri, Z. Li, L. Chen, Y. Zhang, S. Dixit, V. Zhang, C. Xiong, R. Socher, W. S. Lasecki, and D. Radev. Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases, 2019.
[57] T. Yu, R. Zhang, M. Yasunaga, Y. C. Tan, X. V. Lin, S. Li, H. Er, I. Li, B. Pang, T. Chen, E. Ji, S. Dixit, D. Proctor, S. Shim, J. Kraft, V. Zhang, C. Xiong, R. Socher, and D. Radev. SParC: Cross-domain semantic parsing in context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511-4523, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1443. URL https: //aclanthology.org/P19-1443.
[58] L. Zeng, S. H. K. Parthasarathi, and D. Hakkani-Tur. N-best hypotheses reranking for text-to-sql systems, 2022.
[59] K. Zhang, Z. Li, J. Li, G. Li, and Z. Jin. Self-edit: Fault-aware code editor for code generation, 2023.
[60] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large language models for code generation, 2023.
[61] T. Zhang, T. Yu, T. B. Hashimoto, M. Lewis, W. tau Yih, D. Fried, and S. I. Wang. Coder reviewer reranking for code generation, 2022.
[62] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning, 2017.
[63] S. Zhou, U. Alon, S. Agarwal, and G. Neubig. Codebertscore: Evaluating code generation with pretrained models of code, 2023.</p>
<h1>Appendix</h1>
<p>In this appendix, we provide additional details about the implementation and usage of the InterCode framework and the InterCodeEnv interface. We also provide visualizations and analyses of additional experiments to demonstrate InterCode's utility and garner further insight into the extent of current models' performance on the interactive coding task. The full template for each prompting strategy is also included. Finally, we also discuss some of the impacts, risks, and limitations of our work. The webpage for InterCode is https://intercode-benchmark.github.io/. The code for InterCode is https://github.com/princeton-nlp/intercode; the link is also included on the InterCode webpage.</p>
<h2>A Environment Details</h2>
<h2>A. 1 InterCode Interface</h2>
<p>The InterCode interface inherits the OpenAI gym [5] environment API definition. Specifically, InterCodeEnv is written as an abstract class that primarily handles the main execution logic for processing code interactions, in addition to logging, data management, and sand-boxed execution, along with both environment-level and task-level customization.</p>
<p>InterCodeEnv exposes the following API. Creating an interactive coding environment requires defining a subclass of InterCodeEnv. The methods denoted with an asterisk can be overridden for the purposes of customization.
<strong>init</strong>(self, data_path: str, image_name: str, **kwargs)</p>
<ul>
<li>Validates that the dataset specified by data_path is formatted correctly and can be used in an interactive setting.</li>
<li>Uses the Docker image specified by image_name to create and connect with a Docker container instance of the image.</li>
<li>Initializes Logging Handler</li>
<li>Keyword arguments:</li>
<li>verbose (bool): If true, logging is enabled and environment interactions are shown to standard output</li>
<li>traj_dir (str): If a valid path is provided, task episode summaries are saved to the given directory (generated by save_trajectory)</li>
<li>preprocess (callable): If provided, this function is run before every task episode. It is a way to provide task instance-specific customization of the execution environment.
reset(self, index: int = None) -&gt; Tuple[str, Dict]</li>
<li>Retrieves task record from data loader</li>
<li>Calls reset_container</li>
<li>Reset task level logger, instance variables
step(self, action: str) -&gt; Tuple[str, int, bool, Dict]</li>
<li>Log (action, observation)</li>
<li>Invoke exec_action on action argument</li>
<li>If action=submit, invoke get_reward, save_trajectory
save_trajectory(self)</li>
<li>Saves task metadata, (action, obs.) sequence, and reward info to .json in traj_dir close(self)</li>
<li>Safely exit or stop any resources (i.e. docker container) used by the environment</li>
<li>execute_action(self, action: str)</li>
<li>Defines how the action is executed within the context of the docker container.</li>
<li>Requires impl. because the Dockerfile definition, particularly its entrypoint, affects how an action would be invoked within the container.</li>
</ul>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Visualization demonstrating the intended invocations and usage of the InterCodeEnv interface, along with how the functions requiring implementation (get_reward(), execute_action(), reset_container() are called by the methods of the main interactive loop.</p>
<ul>
<li>Default impl. passes the action string directly into a self.container.exec(action) call, which invokes the action in the environment and returns execution output. A timeout is imposed on execution duration.</li>
<li>get_reward(self) -&gt; Tuple[float, Dict]</li>
<li>Handles reward calculation of actions with respect to the gold command(s) for a task episode.</li>
<li>Requires impl. because the concept and scoring for task completion varies across datasets and environments.</li>
<li>reset_container(self)</li>
<li>Handles resetting of execution container (i.e. resetting file system to original state).</li>
<li>Requires impl. because the approach to restoring a setting to its initial state varies.</li>
</ul>
<p>Figure 5 conveys how each of these methods are invoked and how they related to one another. In summary, the technicalities for setting up an interactive coding task for a specific system with one or more programming languages as the action space involve:</p>
<ul>
<li>Defining a Dockerfile</li>
<li>Providing a dataset with the query and gold fields</li>
<li>(Optional) Defining a reward (get_reward) function to define task completion.</li>
<li>(Optional) Creating an InterCodeEnv subclass that overrides the execute_action and get_reward methods</li>
</ul>
<h1>A. 2 Bash Environment</h1>
<p>Environment definition. The Dockerfile defining the Bash-based environment is founded on the LTS version of the Ubuntu operating system. Several Linux dependencies that can potentially be used by an agent to address instructions in the InterCode-Bash Dataset are then installed via the Advanced Package Tool (apt) interface. Next, a shell script is invoked within the Dockerfile to initialize one of the three file systems displayed in Figure 6. The shell script consists of a simple sequence of mkdir, touch, and echo commands to deterministically create and populate the content of multiple files and folders. Finally, git is configured for the purposes of determining file diffs per task episode (git status -s) and resetting an environment to its original state (git reset -hard; git clean -fd;) before the beginning of a new task episode. The original code for the Dockerfile along with the file system creation scripts can be found on the project GitHub repository.
Dataset details. The log-frequency distribution of the top-50 utilities is displayed in Figure 7. The NL2Bash [32] dataset is made available for use under the GPLv3 License. To assess the</p>
<p>generalizability of our approach, we designed three distinct file systems to accommodate the bash commands we collected. A key consideration during the construction of these file systems was to ensure that a significant portion of the executed commands would not result in operations that yield no changes. This deliberate design choice aimed to provide a more comprehensive assessment of our approach's adaptability and effectiveness across various scenarios and command executions. The file systems encompass a wide range of file types, including text files (.txt), program files (.c, .java, .py), compressed files (.gz), shell scripts (.sh), PHP scripts (.php), JSON files (.json), documents (.doc), spreadsheets (.csv), webpages (.html), database schemas (.sql), hidden files, and files with special characters in their names, convoluted folder hierarchies. Their directory structures are illustrated in Figure 6. For simplicity, we consider the top-level folder created within the root directory (testbed, system, workspace) as the root of each file system. This root folder contains files and sub-folders that necessitate access and manipulation, while changes are monitored throughout the entire container to accurately evaluate the models' actions. Notably, we intentionally designed file system 1 to be more intricate and encompass relatively challenging bash tasks compared to the other two file systems. Thereby, the models' performance is relatively lower for file system 1.
Reward function. Evaluation of an agent's trajectory across a single task episode towards carrying out the given instruction is determined by modifications to the file system and the latest execution output. The instructions found in the InterCode-Bash dataset fall under one of two buckets: it either 1. Requests information about the file system that can be answered via execution output generated from a correct sequence of Bash actions (i.e. "How many files...", "What is the size of...", "Where is the .png image stored?") or 2. Requests a change to the location, configuration, or content of a file or folder (i.e. "Move the dir1 folder from...", "Set the permissions to...", "Append a line to..."). Any relevant correct changes are therefore captured by considering both execution output and file system modifications during evaluation.
We define $A$ and $G$ as the outputs of the agent and gold commands respectively, where $A_{\text {out }}$ and $G_{\text {out }}$ refer to the execution output, and $A_{f s}$ and $G_{f s}$ refer to a list of entries reflecting file system modifications, where each entry is [file path, modification type $\in$ [added, changed, deleted]]. We then formally define the reward function as follows:</p>
<p>$$
\begin{gathered}
\mathcal{R}=0.34 * \operatorname{similarity}\left(A_{\text {out }}, G_{\text {out }}\right) \
+0.33 *\left(1-\operatorname{erf}\left(\left|A_{f s} \cup G_{f s}-A_{f s} \cap G_{f s}\right|\right)\right)+ \
+0.33 * \frac{\text { is_correct }\left(A_{f s} \cap G_{f s}\right)}{A_{f s} \cap G_{f s}}
\end{gathered}
$$</p>
<p>Where similarity refers to lexical similarity, which is determined by the cosine similarity score between TF-IDF vectors (calculated with TfidfVectorizer from scikit-learn) of the two execution outputs. The second component of the reward function reflects the number of file system modifications that were either not completed or not necessary; the error associated with the total number of misses is constrained to the range $[0,1]$ using the Gauss error function (erf), where 0 corresponds to no file system modification mistakes. The third component checks what proportion of paths altered by both agent and gold were modified correctly. The is_correct function returns the number of file paths that were changed correctly, determined by checking whether the md5sum hashes of each file path are identical for agent and gold. If $A_{f s} \cap G_{f s}=\emptyset$, this reward is automatically 1 . The scalar weights for each component are arbitrarily assigned.
A max score of 1 is achieved only if the correct file paths are changed, the changes are correct, and the latest execution output matches the gold command output exactly. Figure 1 visualizes the reward function. While an exact match comparison would have been a simpler choice to satisfy the Success Rate metric put forth in the main paper, we design this reward function to 1. Demonstrate that InterCode can support complex reward functions that account for multiple forms of execution output, and 2. Provide practitioners who use the InterCode-Bash environment with a scalar reward that reflects how "similar" the given output is to the expected, rather than a flat $0 / 1$ reward value that may over-penalize and discount the efforts of more capable reasoning abilities. These reasons also motivate the SQL-based environment's reward function, discussed in the following section.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) File System 1</p>
<p>Figure 6: File System structures designed for InterCode-Bash.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Top 30 most frequently occurring bash utilities out of the 66 in InterCode-Bash with their frequencies in log scale.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: Distribution of gold command difficult for InterCode-SQL task data adapted from the Spider SQL dataset.</p>
<h1>A. 3 SQL Environment</h1>
<p>Environment Definition. The Dockerfile defining the SQL-based environment inherits from the MySQL image and adds a .sql file setup script to the /docker-entrypoint-initdb.d directory within the Docker image; this is a special directory made for container initialization. On container start-up, the added .sql file, which creates and populates databases with tables and tables with records, is automatically invoked. Since the InterCode-SQL dataset does not feature any queries that involve modifying the database in any manner (i.e. no INSERT, UPDATE, or DELETE commands), there is no reset mechanism written into the Dockerfile definition that is invoked before each task episode; with that said, adding a reset script or version control to the Dockerfile is simple.
InterCode-SQL dataset. InterCode-SQL is adopted from the development set of the Spider dataset [55]. Spider 1.0 is a large-scale cross-domain dataset on generating SQL queries from natural language questions whose development set contains 1034 pairs of <instruction, gold> task instances spanning 20 databases. The distribution of queries according to their hardness criterion is shown in Figure 8. As discussed in Section 3.3, a filtering criterion narrows down the Spider dataset's information to only the necessary components. We do not add anything to the Spider dataset that was not originally available. The Spider 1.0 dataset is available for use under the CC BY-SA 4.0 license.
MySQL databases. We first resolve data types for primary, foreign key pairs across the provided table schemas in Spider for conflicting instances and generate the corresponding SQLite databases. Next, to align with our Docker-supported environment, we convert the SQLite databases to MySQL format using sqlite3mysql [41], a Python library, and then generate a unified MySQL dump having schemas for all the tables. To handle case-sensitive table name discrepancies between the queries and the underlying schema in the original Spider dataset, we activate the lower_case_table_names setting in our evaluation environment. Additionally, for proper access controls, we create a test user and grant them all privileges for all the tables.
Reward function. The completion evaluation mechanism compares the output of the gold SQL command with the latest execution output (i.e. latest observation) from the agent's interaction trajectory. The execution output of all gold SQL queries is a list of records. Each record is a tuple of one or more values that may be different types. For any single execution output, the order of types for every record is identical. Given the agent command(s)' latest execution output $A$ and the gold command's execution output $G$, we formulate the reward function as follows:</p>
<p>$$
\mathcal{R}=\frac{A \cap G}{A \cup G} *(\text { kendalltau }((A \cap(A \cap G)),(G \cap(A \cap G)))+1) / 2
$$</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Evaluation of the results of agent interactions with the SQL Environment against the gold command associated with the task. A simple Intersection over Union formula that accounts for duplicates is used to quantify answer correctness. Task completion is a reward of 1.</p>
<p>We employ Intersection over Union (IoU), or more formally the Jaccard Index, to quantify the correctness of the latest execution output generated by the agent against the gold output. If the latest execution output of the SQL query is not in the form of a list of records (i.e. a string error message), the reward is 0 by default. Among the items that lie in the intersection of the agent and gold execution outputs, we also apply a penalty if the records are in the incorrect order. Since achieving the correct order of fields in a record is of non-trivial importance to addressing many SQL queries correctly, we do not do any re-ordering or pre-processing of the list of records. Therefore, a record formatted as ("Ross", 29) is not awarded any credit against a gold output that includes (29, "Ross"). To quantify how sorted the agent output is relative to the gold output, we lean on Kendall's $\tau$ and adjust the output range to $[0,1]$. The IoU score is then directly scaled by this coefficient.</p>
<p>All in all, only a correctly ordered list with the exact set of records found in the gold output would receive a max score of 1 , which corresponds to task completion. Figure 10 visualizes the reward function for an example set of outputs. Note that in the main paper, the Success Rate metric is used; the scalar $3 / 7$ output shown in the figure is treated as a 0 when quantifying whether the task was completed via the $0 / 1$ Success Rate metric. As mentioned in the discussion of the Bash reward function, this reward function also aims to be a richer and fairer continuous evaluation metric of a model's reasoning abilities compared to a binary $0 / 1$ task completion score.</p>
<h1>A. 4 Python Environment</h1>
<p>Environment definition. The InterCode-Python task environment inherits from a bare-minimum Python 3.9 image that provides the basic essentials for initializing a Python interpreter. We were unable to determine how to initialize a Python interpreter within a Dockerfile such that the container would then be capable of automatically executing Python commands sent to it while continuous logging every action/observation per turn. To overcome this, we create and define a backend application that runs within the Docker container, simulates a Python interpreter, and is responsible for handling input/output. By having the application sit between the agent's actions and the interpreter, we are able to log every episode faithfully in addition to providing an environment that is agentfriendly and faithful to the experience of a real Python interpreter.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Example of interactions between an agent and the InterCode Python Environment</p>
<p>Figure 12: In this setting, an agent interacts with a Python Interpreter to 1. implement the requested method and 2. write test cases to determine function correctness. Upon submission, the reward function then evaluates the agent's implementation with a set of unit tests.</p>
<p>InterCode-Python dataset. A large majority of code datasets popular within the NLP community are based on Python and present code completion as the primary task [9, 4, 20]. In the original problem setting, a task worker is asked to synthesize code in a zero, one, or few-shot setting with little to no access to an execution environment. In the interactive setting, task workers are asked to accomplish the same objective, but informed that they have a Python environment to do whatever may help them write the function correctly, such as prototype different implementations and write/execute their own unit tests. Therefore, datasets such as HumanEval, APPS, and MBPP require little to no revisions to be usable within the InterCode environment, with the only necessary processing for all three being renaming of dataset attributes to InterCode-compatible names. A visualization of an example trajectory of interactions between an agent and the Python interpreter is presented in Figure 11.</p>
<p>Reward function. We preserve the original metric of proportion of unit tests passed to evaluate agent implementations, with all tests passing being equivalent to task completion. Complementary to the visualization of interactions, we also show how InterCode-Python performs automatic evaluation of an agent's implementation of the desired function in Figure 12.</p>
<h1>B Experiment Details</h1>
<h2>B. 1 Model Details</h2>
<p>We do not perform any model training for configuring the methods or running the experiments discussed in this project. Our evaluations use inference call requests to OpenAI, PaLM, and HuggingFace API endpoints to run the baseline models on the InterCode tasks. For OpenAI models, we set temperature to 0 , top_p to 1 , max_tokens to 512 , and n (number of completions) to 1 . For PaLM models, we set temperature to 0 , top_p to 1 , and candidate_count (number of completions) to 1 . For open source models, we set max_new_tokens (maximum number of tokens to generate) to 100 and temperature to 0.01 . Due to constraints in the context window size, we limit the length of each observation to a maximum of 1000 tokens across all inference calls. The code for configuring API calls can be found in the linked repository.</p>
<h1>B. 2 Additional Experiments \&amp; Analysis</h1>
<p>SQL schema ablation. To confirm that the benefits of interaction exceed a simple disparity in information between the Single Turn and Try Again settings, we add the full SQL database schema, providing holistic details of tables necessary to the given instruction, to the Question message of both prompts, then re-run the comparison for several. Table 5 indicates that while Single Turn performance improves drastically, a non-trivial difference in favor of Try Again remains. Manual inspection of task episode trajectories shows that selective and fine-grained context discovery (i.e. inspecting specific table records and file content that affect query construction) is still critical to solving tasks efficiently.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">InterCode-SQL <br> + Schema</th>
<th style="text-align: center;">Single Turn</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Try Again (max 10 turns)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model / Hardness</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Med</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Extra</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Med</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Extra</td>
<td style="text-align: center;">All</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: center;">text-bison-001</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">63.9</td>
</tr>
<tr>
<td style="text-align: center;">chat-bison-001</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">54.9</td>
</tr>
</tbody>
</table>
<p>Table 5: Success Rate across difficulty for single vs. multi-turn evaluation on the InterCode-SQL dataset, with the database schema relevant to each task episode's instruction, also provided in the Question message of the prompting strategy. Best metrics are in bold.</p>
<p>Trends of admissible actions. Table 6 shows that for the SQL task, models generate admissible actions with increasingly higher rates early on; in initial turns, models will tend to hallucinate a query with fabricated table and column names at a high frequency. The drop in error rate between the first and second turns can largely be attributed to the model's decision to begin exploring context; $60.3 \%$ of second turn actions contain either the SHOW TABLES or DESC keywords. Prompting strategies (i.e. ReAct, Plan \&amp; Solve), explicit phrasing that encourages exploration, and demonstrations diminish a model's default tendency to hallucinate a query in the first turn. This trend is not found in Bash. This can likely be attributed to the nature of the instructions; unlike the SQL instructions which simply pose a question and do not have any explicit references to SQL commands or clauses, Bash instructions will typically include keywords that correspond directly to useful Linux commands or give insight into the file system's internal structure. These signals reduce the need for context discovery. Therefore, successful task completion in Bash tends to lean towards 1) Figuring out which flags, options, and arguments to configure a command with and 2) How to string together commands or pass outputs from one command to the next correctly.
For both Bash and SQL, in later turns, the rate of admissible actions does not improve consistently. The actions in these later turns are usually attempts to answer the original instruction. At these stages, a model will tend to make small, cursory adjustments to the prior action based on execution feedback, often resulting in both a repetition of the same types of mistakes and hallucinations that introduce new issues. In these moments, compared to such minor perturbations, alternative reasoning capabilities such as context discovery and modularized problem solving are often more efficient ways to get the relevant insights needed to better decide how to fix the prior turns' issues. As corroborated by Figure 3, models struggle to take advantage of additional context in longer task episodes or horizons. Making the most of multiple queries is an open challenge with exciting implications for solving more difficult coding tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Turn</th>
<th style="text-align: left;">1</th>
<th style="text-align: left;">2</th>
<th style="text-align: left;">3</th>
<th style="text-align: left;">4</th>
<th style="text-align: left;">5</th>
<th style="text-align: left;">6</th>
<th style="text-align: left;">7</th>
<th style="text-align: left;">8</th>
<th style="text-align: left;">9</th>
<th style="text-align: left;">10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SQL</td>
<td style="text-align: left;">90.2</td>
<td style="text-align: left;">46.4</td>
<td style="text-align: left;">34.4</td>
<td style="text-align: left;">39.7</td>
<td style="text-align: left;">31.1</td>
<td style="text-align: left;">42.9</td>
<td style="text-align: left;">51.5</td>
<td style="text-align: left;">47.4</td>
<td style="text-align: left;">48.4</td>
<td style="text-align: left;">46.6</td>
</tr>
<tr>
<td style="text-align: left;">Bash</td>
<td style="text-align: left;">23.1</td>
<td style="text-align: left;">28.6</td>
<td style="text-align: left;">34.7</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">37.6</td>
<td style="text-align: left;">42.9</td>
<td style="text-align: left;">39.3</td>
<td style="text-align: left;">37.1</td>
<td style="text-align: left;">33.7</td>
<td style="text-align: left;">38.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Error \% (Average ratio of non-admissible actions) per turn for the Try Again prompting scheme using a GPT 3.5 model on the Bash and SQL InterCode datasets.</p>
<p>Robustness results. We conducted an evaluation to assess the robustness of the reported accuracy metrics for the models. In order to maintain consistency in the evaluation, we focused on the performance across file systems 2, 3, and 4 (shown in Figure 6), which were designed to have similar difficulty levels. File system 1, intentionally made harder, was not included in this analysis. The</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Code and data available at https://intercode-benchmark.github.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
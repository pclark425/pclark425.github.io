<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1538 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1538</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1538</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-237571819</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2109.09478v1.pdf" target="_blank">A Survey of Text Games for Reinforcement Learning Informed by Natural Language</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Reinforcement Learning has shown success in a number of complex virtual environments. However, many challenges still exist towards solving problems with natural language as a core component. Interactive Fiction Games (or Text Games) are one such problem type that offer a set of safe, partially observable environments where natural language is required as part of the Reinforcement Learning solution. Therefore, this survey’s aim is to assist in the development of new Text Game problem settings and solutions for Reinforcement Learning informed by natural language. Specifically, this survey: 1) introduces the challenges in Text Game Reinforcement Learning problems, 2) outlines the generation tools for rendering Text Games and the subsequent environments generated, and 3) compares the agent architectures currently applied to provide a systematic review of benchmark methodologies and opportunities for future researchers.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1538.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1538.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-specific pre-training (food items)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific pre-training on food items (Adolphs & Hofmann 2020, Ledeepchef)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-style pre-training approach where a submodule is pretrained on a collection of food items (auxiliary dataset) prior to full RL training, intended to give priors that improve generalization to unseen objects in cooking/text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ledeepchef: Deep reinforcement learning agent for families of text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ledeepchef</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL agent family for text-based games that incorporates task-specific pre-training of submodules (e.g., object/item priors) to improve generalisation across related games.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CookingWorld / TextWorld (cooking-themed generated games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>TextWorld-generated kitchen environments where agents navigate rooms, inspect and manipulate objects, and apply actions (e.g., take, chop, combine, cook) to complete recipe-based quests.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / household tasks (cooking)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Gathering ingredients, chopping, combining ingredients, using stove/oven, following multi-step recipes to produce a meal.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Hierarchical: cooking tasks decompose into subtasks (locate ingredient → fetch → prepare/process → combine → cook); recipes form multi-step sequences of primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Task-specific pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>A submodule (for example, an item/ingredient representation or perception module) is pretrained on an auxiliary corpus of food items and their relations before the agent trains on full game episodes; this provides priors about affordances and object categories to speed and stabilise downstream RL.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>prerequisite skills / task-specific pretraining (task similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>From simple single-step ingredient collection to multi-step recipes requiring multiple preparatory and combining actions (exact step counts not specified in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Survey reports that pretraining on food items is intended to improve generalization to unseen objects (qualitative claim in survey; no numerical results reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task-specific pretraining (on related object categories such as food items) is advocated as a means to imbue agents with useful priors and to improve generalisation to unseen objects in procedurally-generated cooking tasks, but the survey does not report quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Text Games for Reinforcement Learning Informed by Natural Language', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1538.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1538.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG constructor pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining knowledge-graph constructor on trajectories of similar games (Adhikari et al. 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-like strategy where the module that constructs dynamic belief/knowledge graphs is pretrained on trajectories from related games so it learns better entity and relation extraction before joint RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Pretrained KG-constructor (module)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A module that builds and updates a belief/knowledge graph from textual observations; pretrained on trajectories from games similar to the target distribution to provide structural priors for downstream decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld / families of text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially-observable text-based environments where observations are textual descriptions of the world and agents perform language actions (navigation, object manipulation, using objects) to complete quests; dynamic world state is representable as a graph of entities/relations.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / object-affordance extraction and tracking</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Tracking that 'key is in drawer', 'lamp provides light', using items as prerequisites (e.g., pick up lantern before exploring dark area), detecting that an object is out-of-place.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Compositional via multi-step chaining of relations in the KG (observations incrementally build edges/nodes that allow reasoning over multi-step procedures).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Pretraining on trajectories from similar games</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The KG constructor is first trained offline on trajectories drawn from games that are similar to the target tasks; this provides the graph-building component with priors about entity types, relations and affordances, which the agent then uses during RL on the target environment.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task similarity / prerequisite representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Survey indicates pretraining the KG constructor improves generalization of belief graph construction across games (qualitative statement, no quantitative details given).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining KG construction modules on trajectories from related games is recommended to provide representational priors and to improve the agent's ability to build belief graphs and generalize; the survey provides no numeric performance measures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Text Games for Reinforcement Learning Informed by Natural Language', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1538.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1538.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Treasure Hunter difficulty levels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Treasure Hunter difficulty curriculum in TextWorld (Côté et al. 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An environment-provided discrete difficulty curriculum (levels 1–30) that varies number of rooms, quest length and the presence of locked doors/containers to scale task difficulty from simple retrieval to complex multi-step quests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Textworld: A learning environment for text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BYU agent / Golovin agent / Choice-based random agent (baselines reported for zero-shot eval)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baselines evaluated in the survey include BYU and Golovin agents (learning/heuristic agents) and a choice-based random agent; the random agent used a choice-based handicap that provides valid actions to choose from.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Treasure Hunter (TextWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Randomly-generated map where an agent must locate and collect a specified coloured object; interactions include navigation, object pickup, opening/unlocking containers; episodes have limited turns.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household retrieval procedures / navigation and object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Navigate rooms to find the target object, open containers, unlock doors with keys, follow multi-step retrieval quests.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Compositional by quest length and prerequisites: higher-level quests require sequences of primitive actions (e.g., find key → unlock door → enter room → pick object); complexity increases with more rooms and nested containers.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Discrete difficulty levels (Levels 1–30)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Environment defines 30 difficulty levels: Levels 1–10: 5 rooms, no doors, quest length 1–5; Levels 11–20: 10 rooms, include doors/containers, quest length 2–10; Levels 21–30: 20 rooms with locked doors/containers, quest length 3–20. These can be used to order training from simple to complex.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task difficulty (rooms / quest length / presence of locks/containers)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Levels 1–30: room counts {5,10,20}; quest lengths from 1 up to 20; locked doors/containers introduced at higher levels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Survey reports zero-shot evaluations on up to 100 generated games (max 1,000 steps) comparing BYU, Golovin and choice-based random agent; the choice-based random agent performed best in those zero-shot settings due to the action-space handicap, but specific numeric scores are not provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Survey used zero-shot evaluation to assess generalization across generated games; agents struggled on generalization and the random choice-based agent (which removes compositional language complexity) outperformed learned agents in that zero-shot setting (qualitative report only).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TextWorld's structured difficulty levels provide a natural curriculum (simple→complex) for training/testing; in surveyed zero-shot experiments the choice-based action-space handicap can invert expected performance (a random choice-based agent beat learning agents), highlighting how action-space handicaps can confound evaluation of curriculum/learning benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Text Games for Reinforcement Learning Informed by Natural Language', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1538.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1538.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CookingWorld / First TextWorld Problems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First TextWorld Problems competition / CookingWorld (Trischler et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large suite of TextWorld-generated cooking games (thousands of instances) intended to stress planning and memory by having agents cook meals from gathered ingredients; the dataset provides fixed train/validation/test splits and supports joint and zero-shot evaluation to study generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>First textworld problems, the competition: Using text-based games to advance capabilities of ai agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Various baseline RL agents and competition entrants</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Participants and baselines vary; typical agents are deep-RL models with text encoders and action scorers, sometimes augmented with knowledge graphs or pretrained components.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CookingWorld (TextWorld family)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Kitchen-themed, procedurally-generated games where agents must gather, process and combine ingredients to prepare recipes; interactions include navigation, object interaction (take, cut, cook), and combining items to form dishes.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / household tasks (multi-step cooking procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Find/carry ingredients, cut/chop items, combine ingredients, cook on stove/oven to complete a recipe-driven quest.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Hierarchical multi-step procedures where successful completion requires sequencing subtasks (gather → prepare → combine → cook) and memory of prior steps; tasks are compositional by recipe structure and ingredient affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>The public dataset contains 4,400 training, 222 validation and 514 test games across multiple difficulty settings; difficulty is described by variables rather than a single discrete scale (exact step counts not specified in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>The dataset is explicitly used to evaluate joint training and zero-shot generalization; the survey notes it as a benchmark for planning/memory and generalization but does not report numeric transfer metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CookingWorld frames procedural (cooking) tasks as hierarchical problems requiring planning and memory; providing a large, varied dataset with fixed splits enables evaluation of generalization (joint vs zero-shot), but the survey does not report concrete curriculum-driven performance improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Text Games for Reinforcement Learning Informed by Natural Language', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1538.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1538.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClubFloyd pretraining (human playthroughs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ClubFloyd human playthroughs dataset used for pretraining action generators (Yao et al. 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large collection of human playthroughs of many interactive fiction games used to pretrain language models for action generation (providing priors for valid commands), functioning as a data-driven pretraining stage in an overall training pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep calm and explore: Language models for action generation in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM (GPT-2 fine-tuned) / language-model-based action generator</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A fine-tuned GPT-2 style language model (CALM) used to generate candidate actions/commands; pretrained on the ClubFloyd human gameplay corpus to learn likely valid commands and action patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Various interactive fiction games (ClubFloyd corpus covers many titles) / Text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Dataset entries are transcripts of human playthroughs across many IF games; environments themselves vary but generally include textual observations and parser-based/choice-based action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / general interactive actions and affordance-driven commands</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Navigation commands, object manipulations, multi-word parser commands consistent with human play traces (e.g., 'take lamp', 'open chest', 'put apple in pot').</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Pretraining on human playthroughs</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>A language model is pretrained/fine-tuned on a large corpus of human playthrough transcripts to provide an action generator with priors about valid, high-quality commands; this acts as an initial curriculum phase before or alongside RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>human demonstrations / task similarity (data-driven pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Survey notes that the CALM generative model (GPT-2 based, trained on human playthroughs) is competitive against models that rely on valid-action handicaps, but the survey does not report numeric performance values in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Using human-play pretraining improves generation of valid actions and helps in settings where action-space is large and combinatorial; survey reports competitiveness qualitatively without numeric transfer rates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining action-generation language models on human playthrough corpora (ClubFloyd) provides strong priors for valid command generation and can compete with methods that depend on explicit valid-action handicaps, suggesting human-play pretraining is a practical curriculum component for large action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Text Games for Reinforcement Learning Informed by Natural Language', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ledeepchef: Deep reinforcement learning agent for families of text-based games. <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games. <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games. <em>(Rating: 2)</em></li>
                <li>First textworld problems, the competition: Using text-based games to advance capabilities of ai agents. <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games. <em>(Rating: 2)</em></li>
                <li>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. <em>(Rating: 1)</em></li>
                <li>TW-Commonsense (TW-Commonsense Murugesan et al. 2020a) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1538",
    "paper_id": "paper-237571819",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Task-specific pre-training (food items)",
            "name_full": "Task-specific pre-training on food items (Adolphs & Hofmann 2020, Ledeepchef)",
            "brief_description": "A curriculum-style pre-training approach where a submodule is pretrained on a collection of food items (auxiliary dataset) prior to full RL training, intended to give priors that improve generalization to unseen objects in cooking/text-based games.",
            "citation_title": "Ledeepchef: Deep reinforcement learning agent for families of text-based games.",
            "mention_or_use": "mention",
            "agent_name": "Ledeepchef",
            "agent_description": "A deep RL agent family for text-based games that incorporates task-specific pre-training of submodules (e.g., object/item priors) to improve generalisation across related games.",
            "agent_size": null,
            "environment_name": "CookingWorld / TextWorld (cooking-themed generated games)",
            "environment_description": "TextWorld-generated kitchen environments where agents navigate rooms, inspect and manipulate objects, and apply actions (e.g., take, chop, combine, cook) to complete recipe-based quests.",
            "procedure_type": "commonsense procedures / household tasks (cooking)",
            "procedure_examples": "Gathering ingredients, chopping, combining ingredients, using stove/oven, following multi-step recipes to produce a meal.",
            "compositional_structure": "Hierarchical: cooking tasks decompose into subtasks (locate ingredient → fetch → prepare/process → combine → cook); recipes form multi-step sequences of primitives.",
            "uses_curriculum": true,
            "curriculum_name": "Task-specific pre-training",
            "curriculum_description": "A submodule (for example, an item/ingredient representation or perception module) is pretrained on an auxiliary corpus of food items and their relations before the agent trains on full game episodes; this provides priors about affordances and object categories to speed and stabilise downstream RL.",
            "curriculum_ordering_principle": "prerequisite skills / task-specific pretraining (task similarity)",
            "task_complexity_range": "From simple single-step ingredient collection to multi-step recipes requiring multiple preparatory and combining actions (exact step counts not specified in survey).",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Survey reports that pretraining on food items is intended to improve generalization to unseen objects (qualitative claim in survey; no numerical results reported here).",
            "key_findings": "Task-specific pretraining (on related object categories such as food items) is advocated as a means to imbue agents with useful priors and to improve generalisation to unseen objects in procedurally-generated cooking tasks, but the survey does not report quantitative comparisons.",
            "uuid": "e1538.0",
            "source_info": {
                "paper_title": "A Survey of Text Games for Reinforcement Learning Informed by Natural Language",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "KG constructor pretraining",
            "name_full": "Pretraining knowledge-graph constructor on trajectories of similar games (Adhikari et al. 2020)",
            "brief_description": "A curriculum-like strategy where the module that constructs dynamic belief/knowledge graphs is pretrained on trajectories from related games so it learns better entity and relation extraction before joint RL training.",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "mention_or_use": "mention",
            "agent_name": "Pretrained KG-constructor (module)",
            "agent_description": "A module that builds and updates a belief/knowledge graph from textual observations; pretrained on trajectories from games similar to the target distribution to provide structural priors for downstream decision-making.",
            "agent_size": null,
            "environment_name": "TextWorld / families of text-based games",
            "environment_description": "Partially-observable text-based environments where observations are textual descriptions of the world and agents perform language actions (navigation, object manipulation, using objects) to complete quests; dynamic world state is representable as a graph of entities/relations.",
            "procedure_type": "commonsense procedures / object-affordance extraction and tracking",
            "procedure_examples": "Tracking that 'key is in drawer', 'lamp provides light', using items as prerequisites (e.g., pick up lantern before exploring dark area), detecting that an object is out-of-place.",
            "compositional_structure": "Compositional via multi-step chaining of relations in the KG (observations incrementally build edges/nodes that allow reasoning over multi-step procedures).",
            "uses_curriculum": true,
            "curriculum_name": "Pretraining on trajectories from similar games",
            "curriculum_description": "The KG constructor is first trained offline on trajectories drawn from games that are similar to the target tasks; this provides the graph-building component with priors about entity types, relations and affordances, which the agent then uses during RL on the target environment.",
            "curriculum_ordering_principle": "task similarity / prerequisite representation learning",
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Survey indicates pretraining the KG constructor improves generalization of belief graph construction across games (qualitative statement, no quantitative details given).",
            "key_findings": "Pretraining KG construction modules on trajectories from related games is recommended to provide representational priors and to improve the agent's ability to build belief graphs and generalize; the survey provides no numeric performance measures.",
            "uuid": "e1538.1",
            "source_info": {
                "paper_title": "A Survey of Text Games for Reinforcement Learning Informed by Natural Language",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Treasure Hunter difficulty levels",
            "name_full": "Treasure Hunter difficulty curriculum in TextWorld (Côté et al. 2018)",
            "brief_description": "An environment-provided discrete difficulty curriculum (levels 1–30) that varies number of rooms, quest length and the presence of locked doors/containers to scale task difficulty from simple retrieval to complex multi-step quests.",
            "citation_title": "Textworld: A learning environment for text-based games.",
            "mention_or_use": "mention",
            "agent_name": "BYU agent / Golovin agent / Choice-based random agent (baselines reported for zero-shot eval)",
            "agent_description": "Baselines evaluated in the survey include BYU and Golovin agents (learning/heuristic agents) and a choice-based random agent; the random agent used a choice-based handicap that provides valid actions to choose from.",
            "agent_size": null,
            "environment_name": "Treasure Hunter (TextWorld)",
            "environment_description": "Randomly-generated map where an agent must locate and collect a specified coloured object; interactions include navigation, object pickup, opening/unlocking containers; episodes have limited turns.",
            "procedure_type": "household retrieval procedures / navigation and object manipulation",
            "procedure_examples": "Navigate rooms to find the target object, open containers, unlock doors with keys, follow multi-step retrieval quests.",
            "compositional_structure": "Compositional by quest length and prerequisites: higher-level quests require sequences of primitive actions (e.g., find key → unlock door → enter room → pick object); complexity increases with more rooms and nested containers.",
            "uses_curriculum": true,
            "curriculum_name": "Discrete difficulty levels (Levels 1–30)",
            "curriculum_description": "Environment defines 30 difficulty levels: Levels 1–10: 5 rooms, no doors, quest length 1–5; Levels 11–20: 10 rooms, include doors/containers, quest length 2–10; Levels 21–30: 20 rooms with locked doors/containers, quest length 3–20. These can be used to order training from simple to complex.",
            "curriculum_ordering_principle": "task difficulty (rooms / quest length / presence of locks/containers)",
            "task_complexity_range": "Levels 1–30: room counts {5,10,20}; quest lengths from 1 up to 20; locked doors/containers introduced at higher levels.",
            "performance_with_curriculum": "Survey reports zero-shot evaluations on up to 100 generated games (max 1,000 steps) comparing BYU, Golovin and choice-based random agent; the choice-based random agent performed best in those zero-shot settings due to the action-space handicap, but specific numeric scores are not provided in the survey text.",
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Survey used zero-shot evaluation to assess generalization across generated games; agents struggled on generalization and the random choice-based agent (which removes compositional language complexity) outperformed learned agents in that zero-shot setting (qualitative report only).",
            "key_findings": "TextWorld's structured difficulty levels provide a natural curriculum (simple→complex) for training/testing; in surveyed zero-shot experiments the choice-based action-space handicap can invert expected performance (a random choice-based agent beat learning agents), highlighting how action-space handicaps can confound evaluation of curriculum/learning benefits.",
            "uuid": "e1538.2",
            "source_info": {
                "paper_title": "A Survey of Text Games for Reinforcement Learning Informed by Natural Language",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "CookingWorld / First TextWorld Problems",
            "name_full": "First TextWorld Problems competition / CookingWorld (Trischler et al. 2019)",
            "brief_description": "A large suite of TextWorld-generated cooking games (thousands of instances) intended to stress planning and memory by having agents cook meals from gathered ingredients; the dataset provides fixed train/validation/test splits and supports joint and zero-shot evaluation to study generalization.",
            "citation_title": "First textworld problems, the competition: Using text-based games to advance capabilities of ai agents.",
            "mention_or_use": "mention",
            "agent_name": "Various baseline RL agents and competition entrants",
            "agent_description": "Participants and baselines vary; typical agents are deep-RL models with text encoders and action scorers, sometimes augmented with knowledge graphs or pretrained components.",
            "agent_size": null,
            "environment_name": "CookingWorld (TextWorld family)",
            "environment_description": "Kitchen-themed, procedurally-generated games where agents must gather, process and combine ingredients to prepare recipes; interactions include navigation, object interaction (take, cut, cook), and combining items to form dishes.",
            "procedure_type": "commonsense procedures / household tasks (multi-step cooking procedures)",
            "procedure_examples": "Find/carry ingredients, cut/chop items, combine ingredients, cook on stove/oven to complete a recipe-driven quest.",
            "compositional_structure": "Hierarchical multi-step procedures where successful completion requires sequencing subtasks (gather → prepare → combine → cook) and memory of prior steps; tasks are compositional by recipe structure and ingredient affordances.",
            "uses_curriculum": null,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "The public dataset contains 4,400 training, 222 validation and 514 test games across multiple difficulty settings; difficulty is described by variables rather than a single discrete scale (exact step counts not specified in survey).",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "The dataset is explicitly used to evaluate joint training and zero-shot generalization; the survey notes it as a benchmark for planning/memory and generalization but does not report numeric transfer metrics.",
            "key_findings": "CookingWorld frames procedural (cooking) tasks as hierarchical problems requiring planning and memory; providing a large, varied dataset with fixed splits enables evaluation of generalization (joint vs zero-shot), but the survey does not report concrete curriculum-driven performance improvements.",
            "uuid": "e1538.3",
            "source_info": {
                "paper_title": "A Survey of Text Games for Reinforcement Learning Informed by Natural Language",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "ClubFloyd pretraining (human playthroughs)",
            "name_full": "ClubFloyd human playthroughs dataset used for pretraining action generators (Yao et al. 2020)",
            "brief_description": "A large collection of human playthroughs of many interactive fiction games used to pretrain language models for action generation (providing priors for valid commands), functioning as a data-driven pretraining stage in an overall training pipeline.",
            "citation_title": "Keep calm and explore: Language models for action generation in text-based games.",
            "mention_or_use": "mention",
            "agent_name": "CALM (GPT-2 fine-tuned) / language-model-based action generator",
            "agent_description": "A fine-tuned GPT-2 style language model (CALM) used to generate candidate actions/commands; pretrained on the ClubFloyd human gameplay corpus to learn likely valid commands and action patterns.",
            "agent_size": null,
            "environment_name": "Various interactive fiction games (ClubFloyd corpus covers many titles) / Text-based games",
            "environment_description": "Dataset entries are transcripts of human playthroughs across many IF games; environments themselves vary but generally include textual observations and parser-based/choice-based action spaces.",
            "procedure_type": "commonsense procedures / general interactive actions and affordance-driven commands",
            "procedure_examples": "Navigation commands, object manipulations, multi-word parser commands consistent with human play traces (e.g., 'take lamp', 'open chest', 'put apple in pot').",
            "compositional_structure": null,
            "uses_curriculum": true,
            "curriculum_name": "Pretraining on human playthroughs",
            "curriculum_description": "A language model is pretrained/fine-tuned on a large corpus of human playthrough transcripts to provide an action generator with priors about valid, high-quality commands; this acts as an initial curriculum phase before or alongside RL training.",
            "curriculum_ordering_principle": "human demonstrations / task similarity (data-driven pretraining)",
            "task_complexity_range": null,
            "performance_with_curriculum": "Survey notes that the CALM generative model (GPT-2 based, trained on human playthroughs) is competitive against models that rely on valid-action handicaps, but the survey does not report numeric performance values in the text.",
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Using human-play pretraining improves generation of valid actions and helps in settings where action-space is large and combinatorial; survey reports competitiveness qualitatively without numeric transfer rates.",
            "key_findings": "Pretraining action-generation language models on human playthrough corpora (ClubFloyd) provides strong priors for valid command generation and can compete with methods that depend on explicit valid-action handicaps, suggesting human-play pretraining is a practical curriculum component for large action spaces.",
            "uuid": "e1538.4",
            "source_info": {
                "paper_title": "A Survey of Text Games for Reinforcement Learning Informed by Natural Language",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ledeepchef: Deep reinforcement learning agent for families of text-based games.",
            "rating": 2,
            "sanitized_title": "ledeepchef_deep_reinforcement_learning_agent_for_families_of_textbased_games"
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games.",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "First textworld problems, the competition: Using text-based games to advance capabilities of ai agents.",
            "rating": 2,
            "sanitized_title": "first_textworld_problems_the_competition_using_textbased_games_to_advance_capabilities_of_ai_agents"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games.",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Text-based rl agents with commonsense knowledge: New challenges, environments and baselines.",
            "rating": 1,
            "sanitized_title": "textbased_rl_agents_with_commonsense_knowledge_new_challenges_environments_and_baselines"
        },
        {
            "paper_title": "TW-Commonsense (TW-Commonsense Murugesan et al. 2020a)",
            "rating": 1,
            "sanitized_title": "twcommonsense_twcommonsense_murugesan_et_al_2020a"
        }
    ],
    "cost": 0.0189935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A SURVEY OF TEXT GAMES FOR REINFORCEMENT LEARNING INFORMED BY NATURAL LANGUAGE A PREPRINT
September 2021</p>
<p>Philip Osborne philip.osborne@manchester.ac.uk 
Heido Nõmm heido.nomm@manchester.ac.uk 
Andre Freitas andre.freitas@manchester.ac.uk </p>
<p>University of Manchester
UK</p>
<p>Kilburn Building University of Manchester</p>
<p>Oxford Rd
M13 9PLManchester</p>
<p>A SURVEY OF TEXT GAMES FOR REINFORCEMENT LEARNING INFORMED BY NATURAL LANGUAGE A PREPRINT
September 202179EC43AFD35B6AB3851B5B7D5F3DC87BarXiv:2109.09478v1[cs.AI]
Reinforcement Learning has shown success in a number of complex virtual environments.However, many challenges still exist towards solving problems with natural language as a core component.Interactive Fiction Games (or Text Games) are one such problem type that offer a set of partially observable environments where natural language is required as part of the reinforcement learning solutions.Therefore, this survey's aim is to assist in the development of new Text Game problem settings and solutions for Reinforcement Learning informed by natural language.Specifically, this survey summarises: 1) the challenges introduced in Text Game Reinforcement Learning problems, 2) the generation tools for evaluating Text Games and the subsequent environments generated and, 3) the agent architectures currently applied are compared to provide a systematic review of benchmark methodologies and opportunities for future researchers.</p>
<p>Introduction</p>
<p>given by Narasimhan et al. [2015] where the player takes the action 'Go East' to cross the bridge.</p>
<p>Reinforcement Learning (RL) has shown human-level performance in solving complex, single setting virtual environments Mnih et al. [2013] &amp; Silver et al. [2016].However, applications and theory in RL problems have been far less developed and it has been posed that this is due to a wide divide between the empirical methodology associated with virtual environments in RL research and the challenges associated with reality Dulac-Arnold et al. [2019].Simply put, Text Games provide a safe and data efficient way to learn from environments that mimic language found in real-world scenarios Shridhar et al. [2020].</p>
<p>Natural language (NL) has been introduced as a solution to many of the challenges in RL Luketina et al. [2019], as NL can facilitate the transfer of abstract knowledge to downstream tasks.However, RL approaches on these language driven environments are still limited in their development and therefore a call has been made for an improvement on the evaluation settings where language is a first-class component.</p>
<p>Text Games gained wider acceptance as a testbed for NL research following work from Narasimhan et al. [2015] who leveraged the Deep Q Network (DQN) framework for policy learning on a set of synthetic textual games.Text Games are both partially observable (as shown in Figure 1) and include outcomes that make reward signals simple to define, making them a suitable problem for Reinforcement Learning to solve.However, research so far has been performed independently, with many authors generating their own environments to evaluate their proposed architectures.This lack of uniformity in the environments makes comparisons between authors challenging and a need for structuring recent work is essential for systematic comparability.</p>
<p>Formally, this survey provides the first systematic review of the challenges posed by the generation tools designed for Text Game evaluation.Furthermore, we also provide details on the generated environments and resultant RL agent architectures used to solved them.This acts as a complement to prior studies in the challenges of real-world RL and RL informed by NL by Dulac-Arnold et al. [2019] &amp; Luketina et al. [2019] respectively.</p>
<p>Text Game Generation</p>
<p>Thus far, there are two main generation tools for applying agents to interactive fiction games: TextWorld and Jericho.</p>
<p>TextWorld Côté et al. [2018] is a logic engine to create new game worlds, populating them with objects and generating quests that define the goal states (and subsequent reward signals).It has since been used as the generation tool for Treasure Hunter Côté et al. [2018], Coin Collector Yuan et al. [2018], CookingWorld Trischler et al. [2019] and the QAit Dataset Yuan et al. [2019].</p>
<p>Jericho Hausknecht et al. [2019a] was more recently developed as a tool for supporting a set of human-made interactive fiction games that cover a range of genres.These include titles such as Zork and Hitchhiker's Guide to the Galaxy.Unsupported games can also be played through Jericho but will not have the point-based scoring system that defines the reward signals.Jericho has been as the generator tool for CALM Yao et al. [2020] and Jericho QA Ammanabrolu et al.</p>
<p>[2020a].</p>
<p>Environment Model</p>
<p>Reinforcement Learning is a framework that enables agents to reason about sequential decision making problems as an optimization process Sutton and Barto [1998].Reinforcement Learning requires a problem to be formulated as a Markov Decision Process (MDP) defined by a tuple &lt; S, A, T, R, γ &gt; where S is the set of states, A is the set of actions, T is the transition probability function, R the reward signal and γ the discount factor.</p>
<p>Given an environment defined by an MDP, the goal of an agent is to determine a policy π(a|s) specifying the action to be taken in any state that maximizes the expected discounted cumulative return ∞ k=0 γ k r k+1 .In text games however, the environment states are never directly observed but rather textual feedback is provided after entering a command.As specified by Côté et al. [2018], a text game is a discrete-time Partially Observed Markov Decision Process (POMDP) defined by &lt; S, A, T, Ω, O, R, γ &gt; where we now have the addition of the set of observations (Ω) and a set of conditional observed probabilities O. Specifically, the function O selects from the environment state what information to show to the agent given the command entered to produce each observation o t ∈ Ω.</p>
<p>Handicaps</p>
<p>Before introducing the challenges of the environments, it is important to understand the possible limitations that can be imposed by the generation tools to reduce the complexity of each problem.These handicaps are typically used to limit the scope of the challenges being faced at any one time for more rigorous comparisons and for simplicity.</p>
<p>It has been noted that TextWorld's Côté et al. [2018] generative functionalities explicit advantage in that it can be used to focus on desired subset of challenges.For example, the size of the state space can be controlled and how many commands are required in order to reach the goal.Evaluation of specific generalisability measures can also be improved by controlling the training vs testing variations.</p>
<p>The partial observability of the state can also be controlled by augmenting the agent's observations.It is possible for the environment to provide all information about the current game state and therefore reduce the amount an agent must explore to determine the world, relationships and the objects contained within.</p>
<p>Furthermore, the complexity of the language itself can be reduced by restricting the agent's vocabulary to in-game words only or the verbs to only those understood by the parser.The grammar can be further simplified by replacing object names with symbolic tokens.It is even possible for the language generation to be avoided completely by converting every generated game into a choice-based game where actions at each timestep are defined by a list of pre-defined commands to choose from.</p>
<p>Rewards can simplified with more immediate rewards during training based on the environment state transitions and the known ground truth winning policy rather than simply a sparse reward at the end of a quest as normally provided.</p>
<p>Actions are defined by text commands of at least one word.The interpreter can accept any sequence of characters but will only recognize a tiny subset and moreso only a fraction of these will change the state of the world.The action space is therefore enormous and so two simplifying assumptions are made:</p>
<p>-Word-level Commands are sequences of at most L words taken from a fixed vocabulary V.</p>
<p>-Syntax Commands have the following structure -verb[noun phrase [adverb phrase]] where [...] indicates that the sub-string is optional.</p>
<p>Jericho Hausknecht et al. [2019a] similarly has a set of possible simplifying steps to reduce the environment's complexity.Most notably, each environment provides agents with the set of valid actions in each game's state.It achieves this by executing a candidate action and looking for the resulting changes to the world-object-tree.To further reduce the difficulty of the games, optional handicaps can be used:</p>
<p>• Fixed random seed to enforce determinism • Use of load, save functionality • Use of game-specific templates and vocabulary • Use of world object tree as an auxiliary state representation or method for detection player location and objects • Use of world-change-detection to identify valid actions Jericho also introduces a set of possible restrictions to the action-space.</p>
<p>• Template-based action spaces separate the problem of picking actions into two (i) picking a template (e.g."take ___ from ___"); (ii) filling the template with objects (e.g."apple", "fridge").Essentially this reduces the issue to verb selection and contextualised entity extraction.</p>
<p>• Parser-Based action spaces require agent to generate a command word-per-word, sometimes following a pre-specified structures similar to (verb, object_1, modifier, object_2).</p>
<p>• Choice-based requires agents to rank predefined set of actions without any option for "creativity" from the model itself.</p>
<p>Lastly, the observation space may be enchanced with the outputs of bonus commands such as "look" and "inventory".These are commands agent can issue on its own but are not considered an actual step in the exploration process that could be costly and produce risks in the real-world to ask for.</p>
<p>Challenges and Posed Solutions</p>
<p>Environments built from the TextWorld generative system bounds the complexity by the set number of objects available.For example, Côté et al. [2018] introduce 10 objects including the logic rules for doors, containers and keys, where complexity can be increased by introducing more objects and rules into the generation process.The most challenging environments are defined in Jericho as these contain 57 real Interactive Fiction games which have been designed by humans, for humans.Specifically, these environments contain more complexity in forms of stochasticity, unnatural interactions, unknown objectives -difficulties originally created to trick and hamper players.</p>
<p>The design and partially observed representation of text games creates a set of natural challenges related to reinforcement learning.Furthermore, a set of challenges specific to language understanding and noted by both Côté et al. [2018] &amp; Hausknecht et al. [2019a] are specified in detail in this section.</p>
<p>Partial Observability</p>
<p>The main challenge for agents solving Textual Games is the environment's partial observability; when observed information is only representative of a small part of the underlying truth.The difference between the two is often unknown and can require extensive exploration and failures for an agent to understand the game's connections between what is observed and how this relates to its actions.</p>
<p>A related additional challenge is that of causality which is when an agent moves away from a state to the next without completing pre-requisites of future states.For example, an agent is required to use a lantern necessary to light its way but may have to backtrack to previous states if this has not been obtained yet.An operation which becomes more complex with time and increases as the length of the agent's trajectory increases.</p>
<p>Handcrafted reward functions have been proved to work for easier settings, like CoinCollector Yuan et al. [2018], but real-world text games can require more nuanced approaches.Go-Explore has been used to find high-reward trajectories and discover under-explored states Madotto et al. [2020], Ammanabrolu et al. [2020a] where more advanced states are given higher priority over states seen early on in the game by a weighted random exploration strategy.Ammanabrolu et al. [2020a] have expanded on this with a modular policy approach aimed at noticing bottleneck states with a combination of a patience parameter and intrinsic motivation for new knowledge.The agent would learn a chain of policies and backtrack to previous states upon getting stuck.</p>
<p>Heuristic-based approaches have been used by Hausknecht et al. [2019b] to restrict navigational commands to only after all other interactive commands have been exhausted.</p>
<p>Leveraging past information has been proven to improve model performance as it allows to limit the partial observability aspect of the games.Ammanabrolu and Hausknecht [2020] propose using a dynamically learned Knowledge Graph (KG) with a novel graph mask to only fill out templates with entities already in the learned KG.</p>
<p>Large State Space</p>
<p>Whilst Textual Games, like all RL problems, require some form of exploration to find better solutions, some papers focus specifically on countering that natural overfitting of RL by actively encouraging exploration to unobserved states in new environments.For example, Yuan et al. [2018] achieved this by setting a reward signal with a bonus for encountering a new state for the first time.This removes the agents capability for high-level contextual knowledge of the environment in favor of simply searching for unseen states.</p>
<p>Subsequent work by Côté et al. [2018] -Treasure Hunter -has expanded on this by increasing the complexity of the environment with additional obstacles such as locked doors that require colour matching keys requiring basic object affordance and generalisation ability.</p>
<p>In a similar vein to Treasure Hunter, where in the worst case agents have to traverse all states to achieve the objective, the location and existence settings of QAit Yuan et al. [2019] require the same with addition of stating the location or existence of an object in the generated game.</p>
<p>These solutions are also related to the challenge of Exploration vs Exploitation that is commonly referenced in all RL literature Sutton and Barto [1998].</p>
<p>Large, Combinatorial and Sparse Action Spaces</p>
<p>Without any restrictions on length or semantics, RL agents aiming to solve games in this domain face the problem of an unbounded action space.Early works limited the action phrases to two words sentences for a verb-object, more recently combinatory action spaces are considered that include action phrases with multiple verbs and objects.A commonly used method for handling combinatory action spaces has been to limit the agent to picking a template T and then filling in the blanks with entity extraction Elimination Network for approximation of the admissibility function: whether the action taken changes the underlying game state or not.The second has been used in limited scope with pointer softmax models generating commands over a fixed vocabulary and the recent textual observation.The CALM generative model, leveraging a fine-tuned GPT-2 for textual games, has proved to be competitive against models using valid action handicaps.</p>
<p>Long-Term Credit Assignment</p>
<p>Assigning rewards to actions can be difficult in situations when the reward signals that are sparse.Specifically, positive rewards might only obtained at the successful completion of the game.However, environments where an agent is unlikely to reach the positive end game through random exploration, provide rewards for specific subtasks such as in Murugesan et al. [2020a], Trischler et al. [2019], Hausknecht et al. [2019a].The reward signal structured in this way also aligns with hierarchical approaches such as in Adolphs and Hofmann [2020].Lastly, to overcome the challenges presented with reward-sparsity, various hand-crafted reward signals have been experimented with Yuan et al. [2018], Ammanabrolu et al. [2020a].</p>
<p>Understanding Parser Feedback &amp; Language Acquisition</p>
<p>LIGHT Urbanek et al. [2019], a crowdsourced platform for the experimentation of grounded dialogue in fantasy settings that differs from the previous action-oriented environments by requiring dialogue with humans, embodied agents and the world itself as part of the quest completion.The authors design LIGHT to investigate how 'a model can both speak and act grounded in perception of its environment and dialogue from other speakers'.Ammanabrolu et al. [2020b] extended this by providing a system that incorporates '1) large-scale language modelling based commonsense reasoning pre-training to imbue the agent with relevant priors and 2) a factorized action space of commands and dialogue'.Furthermore, evaluation can be performed against a dataset collected of held-out human demonstrations.</p>
<p>Commonsense Reasoning &amp; Affordance Extraction</p>
<p>As part of their semantic interpretation, Textual Games require some form of commonsense knowledge to be solved.For example, modeling the association between actions and associated objects (opening doors instead of cutting them, or the fact that taking items allows to use them later on in the game).Various environments have been proposed for testing procedural knowledge in more distinct domains and to asses the agent's generalisation abilities.</p>
<p>For example, Trischler et al. [2019] proposed the 'First TextWorld Problems' competition with the intention of setting a challenge requiring more planning and memory than previous benchmarks.To achieve this, the competition featured 'thousands of unique game instances generated using the TextWorld framework to share the same overarching themean agent is hungry in a house and has a goal of cooking a meal from gathered ingredients'.The agents therefore face a task that is more hierarchical in nature as cooking requires abstract instructions that entail a sequence of high-level actions on objects that are solved as sub-problems.</p>
<p>Furthermore, TW-Commonsense Murugesan et al. [2020a] is explicitly built around agents leveraging prior commonsense knowledge for object-affordance and detection of out of place objects.</p>
<p>Two pre-training datasets have been proposed that form the evaluation goal for specialised modules of RL agents.</p>
<p>The ClubFloyd dataset1 provides human playthroughs of 590 different text based games, allowing to build priors and pre-train generative action generators.Likewise, the Jericho-QA Ammanabrolu et al. [2020a] dataset provides context at a specific timestep in various classical IF games supported by Jericho, and a list of questions, enabling pre-training of QA systems in the domain of Textual Games.They also used the dataset for fine-tuning a pre-trained LM for building a question-answering-based Knowledge Graph.</p>
<p>Lastly, ALFWorld Shridhar et al. [2020] offers a new dimension by enabling the learning of a general policy in a textual environment and then test and enhance it in an embodied environment with a common latent structure.The general tasks also require commonsense reasoning for the agent to make connections between items and attributes, e.g.(sink, "clean"), (lamp, "light").Likewise, the attribute setting of QAit Yuan et al. [2019] environment demands agents to understand attribute affordances (cuttable, edible, cookable) to find and alter (cut, eat, cook) objects in the environment.</p>
<p>Knowledge Representation</p>
<p>It can be specified that at any given time step, the game's state can be represented as a graph that captures observation entities (player, objects, locations, etc) as vertices and the relationship between them as edges.As Text Games are partially observable, an agent can track its belief of the environment into a knowledge graph as it discovers it, eventually converging to an accurate representation of the entire game state Ammanabrolu and Riedl [2019].</p>
<p>In contrast to methods which encode their entire KG into a single vector (as shown in Ammanabrolu and Hausknecht [2020], Ammanabrolu et al. [2020a]) Xu et al. [2020] suggest an intuitive approach of using multiple sub-graphs with different semantical meanings for multi-step reasoning.Previous approaches have relied on predefined rules and Stanford's Open Information Extraction Angeli et al. [2015] for deriving information from observations for KG construction, Adhikari et al. [2020] have instead built an agent, which is capable of building and updating it's belief graph without supervision.</p>
<p>Reframing the domain of Textual-Games, Guo et al.</p>
<p>[2020] consider observations as passages in a multi-passage RC task.They use an object-centric past-observation retrieval to enhance current state representations with relevant past information and then apply attention to draw focus on correlations for action-value prediction.</p>
<p>Benchmark Environments and Agents</p>
<p>Thus far, the majority of researchers have independently generated their own environments with TextWorld Côté et al. [2018].As the field moves towards more uniformity in evaluation, a clear overview of which environments are already generated, their design goals and the benchmark approach is needed.</p>
<p>Table 1 shows the publicly available environments and datasets.Much of the recent research has been published within the past two years (2018-2020).Jericho's Suite of Games (SoG) Hausknecht et al. [2019a] is a collection of 52 games and therefore has its results averaged across all games included in evaluation.</p>
<p>We find that many of the environments focus on exploring increasingly complex environments to 'collect' an item of some kind ranging from a simple coin to household objects.This is due to the TextWorld generator well defined logic rules for such objects but can also be a limitation on the possible scope of the environment.</p>
<p>When evaluating an agent's performance, three types of evaluation settings are typically considered:</p>
<p>• Single Game evaluate agents in the same game under the same conditions,</p>
<p>• Joint settings evaluate agents trained the same set of games that typically share some similarities in the states seen,</p>
<p>• Zero-Shot settings evaluate agents on games completely unseen in training.</p>
<p>The difficulty settings of the environments are defined by the complexity of the challenges that the agents are required to overcome.In most cases, these have been limited to just a few levels.However, CookingWorld defines the challenge by a set of key variables that can be changed seperately or in combination and therefore does not offer clear discrete difficulty settings.</p>
<p>The max number of rooms &amp; objects depends heavily on the type of task.Coin Collecter for example has only 1 object to find as the complexity comes from traversing a number of rooms.Alternatively, CookingWorld has a limited number of rooms and instead focuses on a large number of objects to consider.Zork is naturally the most complex in this regards as an adaptation of a game designed for human players.Likewise, the complexity of the vocabulary depends heavily on the task but is clear to see that the environments limit the number of Action-Verbs for simplicity.length of optimal trajectory.</p>
<p>Agent Architectures</p>
<p>A Deep-RL agent's architecture (see Figure 2) consists of two core components (i) state encoder and (ii) and action scorer Mnih et al. [2013].The first is used to encode game information such as observations, game feedback and KG representations into state approximations.The encoded information is then used by an action selection agent to estimate the value of actions in each state.</p>
<p>Table 2 provides an overview of recent architectural trends for comparison.We find that the initial papers in 2015 used the standard approaches of LSTM or Bag of Words for the encoder and a Deep Q-Network (DQN) for the action selector.More recent developments have been experimenting with both parts towards improved results.Notably, a range of approaches for the encoding have been introduced with Gated Recurrent Unit's (GRUs) have become the most common in 2020.Whereas, there have been fewer variations in the choice of action selector where either Actor-Critic (A2C) or a DQN is typically used.Furthermore, the use of Knowledge Graphs and Pre-trained Tranformers is limited and many of the works that use these were published in 2020.</p>
<p>Most of the agents were applied to either TextWorld/Cookingworld or Jericho.We typically find that alternative environments align to consistent setups; either the authors create a new game that mimics simple real-world rules (similar to TextWorld) or they apply their methods to well know pre-existing games (similar to Jericho).Specifically, Narasimhan et al. [2015] used a self generated two games themselves: 'Home World' to mimic the environment of a typical house and 'Fantasy World' that is more challenging and akin to a role-playing game.Alternatively, He et al.</p>
<p>[2016] used a deterministic text game 'Saving John' and and a larger-scale stochastic text game 'Machine of Death', both pre-existing from a public libary.Encoders used include both simplistic state encodings in the form of Bag of Words (BoW), but also recurrent modules like: Gated Recurrent Unit (GRU) Cho et al.Task Specific Pre-Training entails heuristically establishing a setting in which a submodule learns priors before interacting with training data.For example, Adolphs and Hofmann [2020] pretrain on a collection of food items to improve generalizability to unseen objects and Adhikari et al. [2020] pretrain a KG constructor on trajectories of similar games the agent is trained on.Chaudhury et al. [2020] showed that training an agent on pruned observation space, where the semantically least relevant tokens are removed in each episode to improve generalizability to unseen domains whilst also improving the sample efficieny due to requiring less training games.Furthermore, Jain et al. [2020] propose learning different action-value functions for all possible scores in a game, thus effectively learning separate value functions for each subtask of a whole.</p>
<p>Lastly, the action-space typically varies between template or choice based depending on the type of task.Only two papers have considered a parser based approach: Narasimhan et al. [2015] and Madotto et al. [2020].</p>
<p>Benchmark Results</p>
<p>The following section summarises some of the results published thus far as a means to review the performance of the baselines as they are presented by the original papers to be used for future comparisons.</p>
<p>Treasure Hunter was introduced by Côté et al. [2018] as part of the TextWorld formalisation and inspired by a classic problem to navigate a maze to find a specific object.The agent and objects are placed in a randomly generated map.A coloured object near the agent's start location provides an indicator of which object to obtain provided in the welcome message.A straight forward reward signal is defined as positive for obtaining the correct object and negative for an incorrect object with a limited number of turns available.</p>
<p>Increasing difficulties are defined by the number of rooms, quest length and number of locked doors and containers.Levels 1 to 10 have only 5 rooms, no doors and an increasing quest length from 1 to 5. Levels 11 to 20 have 10 rooms, include doors and containers that may need to be opened and quest length increasing from 2 to 10. Lastly, levels 21 to 30 have 20 rooms, locked doors and containers that may need to be unlocked and opened and quest length increasing from 3 to 20.</p>
<p>For the evaluation, two state-of-the-art agents (BYU Fulda et al. [2017] and Golovin Kostka et al. [2017]) were compared to a choice-based random agent in a zero-shot evaluation setting.For completeness, each was applied to 100 generated games at varying difficulty levels up to a maximum of 1,000 steps.The results of this are shown in table 3 where we note that the random agent performs best but this is due to the choice-based method removing the complexity of the compositional properties of language.The authors noted that may not be directly comparable to the other agents but provides an indication of the difficulty of the tasks.4. However there are some variances in the split of training and test games, making it challenging to compare the results.Most models were trained using the games split of 4,400/514 specified before, but there are variances as some split it as 3,960/440 and some do not state the split at all.Table 4 shows the results of this comparison with a percentage score relative to the maximum reward.Recent research developments for interactive fiction games have generated a set of environments and tested their architectures for generalization and overfitting capabilities.With the comparisons in this work and a uniform set of environments and baselines, new architectures can be systematically contrasted.Currently, most methodologies focus on either an agent's ability to learn efficiently or generalise knowledge.A continued development in this direction is the primary motive for much of the recent research and could lead to solutions that work sufficiently well for the primary real-world challenge of limited and expensive data.</p>
<p>Random</p>
<p>The most recent advances the agent's performance to reach a goal given the linguistic challenge have come from utilising pre-trained transformer models.This has been supported by work published in the NLP community with methods such as BERT Yin et al. [2020], GPT-2 Yao et al. [2020] and ALBERTAmmanabrolu et al. [2020a] and continued developments from this community will support the advancements of future agents architectures.</p>
<p>Similarly, the use of dynamically learned knowledge graphs have become common in recent works and further developments from the NLP community (such as Das et al. [2019]) could enable the use of pre-training knowledge graphs on readily available text corpora before training on the environment itself.</p>
<p>However, there are no methods that currently consider interpretability with language as a primary goal.This is not a challenge unique to Text Games or Reinforcement Learning as calls for research have been made in similar machine learning domains Chu et al. [2020].A notable methodology that is worth considering is the Local Interpretable Model-Agnostic Explanations (LIME) introduced by Ribeiro et al. [2016].</p>
<p>Furthermore, as noted by Luketina et al. [2019], grounding language into the environments is still an open problem that has yet to be addressed fully.Urbanek et al. [2019] introduces a Text Game environment specifically for grounding dialogue as a first step but this is still an open problem.</p>
<p>Lastly, from the contributions analysed in this survey only 5 papers report the amount of time and resources their methods needed for training.Continuing the trend of publishing these specifications is essential in making results more reproducible and applicable in applied settings.Reducing the amount of resources and training time required as a primary motive allows for the domain to be practical as a solution and also accessible given that not all problems allow for unlimited training samples on high performance machines.</p>
<p>Figure 1 :
1
Figure 1: Sample gameplay from a fantasy Text Game as given by Narasimhan et al.[2015]  where the player takes the action 'Go East' to cross the bridge.</p>
<p>Figure 2 :
2
Figure 2: Overview of the Architecture Structure of Agents Applied to a Simple Text Game Example.</p>
<p>[2014], Long Short-Term Memory (LSTM)Hochreiter and Schmidhuber [1997], Transformer (TF)Vaswani et al. [2017], Relational Graph Convolutional Network (R-GCN)Schlichtkrull et al. [2018].Recently, Advantage Actor Critic (A2C)Mnih et al. [2016] has gained popularity for the action selection method, with variants of the Deep Q-NetworkMnih et al. [2013], such as the Deep Reinforcement Relevance Network (DRRN)He et al. [2016], Double DQN (DDQN)Hasselt et al. [2016], Deep Siamese Q-Network (DSQN)Yin and May [2020].</p>
<p>Table 1 :
1
Environments
NameTask descriptionEvalGEN #Diffic. max #rooms max #objects |AV S|len(ot)|V |max |quest|Zork I Anderson et al. [1980]Collect the Twenty Treasures of ZorkSN1110251237NS697396Treasure Hunter Côté et al. [2018]Collect varying items in a HomeZSY30202∼ 4NSNSNSCoin Collector Yuan et al. [2018]Collect a coin in a HomeS,J,ZSY3901264 ± 9NS30FTWP/CookingWorld Trischler et al. [2019] Select &amp; Combine varying items in a Kitchen J, ZSNVarious12561897 ± 49 20,00072Jericho's SoG Hausknecht et al. [2019a]A set of classical TGsSN3NS221avgNS42, 2avg 762avg98avgQAit Yuan et al. [2019]Three QA type settingsZSY112271793.11,647NSTW-Home Ammanabrolu and Riedl [2019]Find varying objects in a homeZSY22040NS9481910TW-Commonsense Murugesan et al. [2020a] Collect and Move varying items in a HomeZSY327∼ 4NSNS17TW-Cook Adhikari et al. [2020]Gather and process cooking ingredientsJ, ZSY5934.128.4NSNS3TextWorld KG Zelinka et al. [2019]Constructing KGs from TG observations-N1N/AN/AN/A29.3NSN/AClubFloyd Yao et al. [2020]Human playthroughs of various TGs-N1N/AN/ANSNS39,670360avgJericho-QA Ammanabrolu et al. [2020a]QA from context strings-N1N/AN/AN/A223.2avgNSN/A
for Textual-Games; Eval: (S)Single, (J) Joint, (ZS) Zero-Shot; GEN: engine support for generation of new games, #Diffic.: number of difficulty settings, #rooms &amp; #objects: number of rooms and objects per game, AVS: size of Action-Verb Space (NS=Not Specified), len(ot): mean number of tokens in the observation ot, |V |: Vocabulary size and, |quest|:</p>
<p>Table 2 :
2
ENC</p>
<p>-state/action encoder, KG -knowledge graph (DL: dynamically learned, CS: commonsense), PTF -pretrained Transformer, PreTr -pretraining (TS: task specific), AS -Action space (TB:template-based, PB: parser based, CB: choice-based)</p>
<p>Table 3 :
3
Trischler et al. [2019]ed to Treasure Hunter's one-life tasks.CookingWorld is the second well-established environment developed using TextWorld byTrischler et al. [2019], is used in joint and zero-shot settings, which both enable testing for generalization.The former entails training and evaluating the agent on the same set of games, while the latter uses an unseen test set upon evaluation.The fixed dataset provides 4, 400 training, 222 validation and 514 test games with 222 different types of games in various game difficulty settings and the current best results are stated in Table
BYUGolovin</p>
<p>Table 4 :
4
Results on the public CookingWorld dataset; OoD -out of domain Jericho's Suite of Games was defined byHausknecht et al. [2019a]and has been used for testing agent's in a single game setting.As the suite provides 57 games, of which a variation of 32 are used due to excessive complexity, then the agents are initialized, trained and evaluated on each of them separately.Agent's are assessed over the final score, which is by convention averaged over the last 100 episodes (which in turn is usually averaged over 5 agents with different initialization seeds).The current state of the art results on the Jericho gameset can be seen in Table5where numeric results are shown relative to the Maximum Reward with the aggregated average percentage shown in the final row 2 .
MaxR CALM-DRRN SHA-KG Trans-v-DRRNMPRC-DQNKG-A2CTDQNDRRNNAIL|T ||V |90510-00000082296acorncourt3001.610100.31.6100151 343advent35036--63.9363620.636189 786advland1000-25.642.20020.60156 398affilicted75--2.08.0-1.42.60146 762anchor1000--00000260 2257awaken500--00000159 505balances519.110.0-10104.81010156 452deephome3001--111113.3173 760detective360289.7208.0288.8317.7207.9169197.8136.9197 344dragon250.10.2-0.040-5.3-3.50.6177 1049enchanter40019.12020201.18.6200290 722gold100---0-4.103200 728inhumane9025.75.4-030.700.6141 409jewel900.31.8-4.461.801.61.6161 657karn1702.3--1000.72.11.2178 615library309.015.81717.714.36.3170.9173 510ludicorp15010.117.81619.717.8613.88.4187 503moonlit10--00000166 669omniquest506.9--10316.855.6207 460pentari70051.334.544.450.717.427.20155 472reverb50-10.610.72.0-0.38.20183 526snacktime5019.4--009.700201 468sorcerer4006.229.4-38.65.8520.85288 1013spellbrkr6004040402521.318.737.840333 844spirit2501.43.8-3.81.30.60.81169 1112temple3507.97.98.07.67.97.47.3175 622tryst205350-6.99.610-09.62197 871yomomma35 ----1-00.40141 619zenon2003.9-03.9000149 401zork135030.434.536.438.3349.932.610.3237 697zork370.50.70.193.630.100.51.8214 564ztuu1003.725.24.885.49.24.921.60186 607Winning % / count21.2%/ 718.2%/ 615.2%/ 569.7%/ 2318.2%/ 618.2%/ 621.2%/ 721.2%/ 7Avg. norm / # games9.4% / 2819.6% / 2022.3% / 1517% / 3310.8% / 28 6% / 33 10.7% / 32 4.8% / 33Handicaps{1}{1, 2, 4}NS{1, 2, 4}{1, 2, 4}{1, 2, 4}{1, 4}N/ATrain steps10 610 610 510 51.6 × 10 610 61.6 × 10 6N/ATrain timeNSNSNS8h-30h per gameNSNSNSN/A</p>
<p>Table 5 :
5
Blue: likely to be solved in near future.Orange: progress is likely, significant scientific progress necessary to solve.Red: very difficult even for humans, unthinkable for current RL.MaxR -Maximum possible reward ;|T | -number of templates; |V | -size of vocabulary set 4 Conclusion and Future Work</p>
<p>http://www.allthingsjacq.com/interactive_fiction.html#clubfloyd
NAIL is a rule-based agent and is noted to emphasize the capabilities of learning based models</p>
<p>Learning dynamic belief graphs to generalize on text-based games. Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, Will Hamilton, Advances in Neural Information Processing Systems. 202033</p>
<p>Ledeepchef: Deep reinforcement learning agent for families of text-based games. Leonard Adolphs, T Hofmann, AAAI. 2020</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark O Riedl, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, 2006.07409. 2020a</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, International Conference on Learning Representations. 2020</p>
<p>How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktaschel, J Weston, ArXiv, abs/2010.006852020b</p>
<p>Tim Anderson, Marc Blank, Bruce Daniels, Dave Lebling, Zork, The great underground empire -part i. 1980</p>
<p>Leveraging linguistic structure for open domain information extraction. Gabor Angeli, Melvin Jose , Johnson Premkumar, Christopher D Manning, 10.3115/v1/P15-1034Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational LinguisticsJuly 20151</p>
<p>Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games. Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Asim Munawar, Ryuki Tachibana, 2020</p>
<p>Learning phrase representations using rnn encoder-decoder for statistical machine translation. Kyunghyun Cho, B V Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, abs/1406.1078Empirical Methods in Natural Language Processing. 2014</p>
<p>Games for fairness and interpretability. Eric Chu, Nabeel Gillani, Priscilla Sneha, Makini, 10.1145/3366424.3384374Companion Proceedings of the Web Conference 2020, WWW '20. New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, J Moore, Matthew J Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, Workshop on Computer Games. 2018</p>
<p>Building dynamic knowledge graphs from text using machine reading comprehension. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, Andrew Mccallum, 2019ICLR</p>
<p>Challenges of real-world reinforcement learning. Gabriel Dulac-Arnold, Daniel Mankowitz, Todd Hester, Proceedings of the 36 th International Conference on Machine Learning. the 36 th International Conference on Machine Learning2019</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, arXiv:1703.034292017arXiv preprint</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. M Xiaoxiao Guo, Yupeng Yu, Chuang Gao, Murray Gan, S Campbell, Chang, 2020EMNLP</p>
<p>Deep reinforcement learning with double q-learning. H V Hasselt, A Guez, D Silver, CoRR, abs/1909.05398Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan2016. 2019aAAAIInteractive fiction games: A colossal adventure</p>
<p>Nail: A general interactive fiction agent. Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, Jason D Williams, arXiv:1902.042592019barXiv preprint</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 2016Association for Computational Linguistics</p>
<p>Algorithmic improvements for deep reinforcement learning applied to interactive fiction. Sepp Hochreiter, Jürgen Schmidhuber, ; Vishal Jain, William Fedus, Hugo Larochelle, Doina Precup, Marc G Bellemare, AAAI. 1997. 20209Long short-term memory</p>
<p>Text-based adventures of the golovin ai agent. Bartosz Kostka, Jaroslaw Kwiecieli, Jakub Kowalski, Pawel Rychlikowski, Computational Intelligence and Games (CIG). 2017</p>
<p>Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, Tim Rocktäschel, A survey of reinforcement learning informed by natural language. 2019arXiv e-prints</p>
<p>Exploration based language learning for text-based games. Andrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Alexandros Papangelis, Dian Yu, Chandra Khatri, Gokhan Tur, IJCAI. 2020</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A Riedmiller, CoRR, abs/1312.56022013</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, 2016</p>
<p>Text-based rl agents with commonsense knowledge: New challenges, environments and baselines. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, CoRR, abs/2010.037902020a</p>
<p>Enhancing text-based reinforcement learning agents with commonsense knowledge. Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, Kartik Talamadupula, arXiv:2005.008112020barXiv preprint</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 2015EMNLP</p>
<p>why should I trust you?": Explaining the predictions of any classifier. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningSan Francisco, CA, USAAugust 13-17, 2016. 2016</p>
<p>Modeling relational data with graph convolutional networks. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den, Ivan Berg, Max Titov, Welling, 2018</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, ArXiv, abs/2010.037682020</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, Aja Huang, Chris J Maddison, A Guez, L Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, S Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, Demis Hassabis, Nature. 5292016</p>
<p>Introduction to Reinforcement Learning. Richard S Sutton, Andrew G Barto, 1998MIT PressCambridge, MA, USA1st edition</p>
<p>Towards solving text-based games by producing adaptive action spaces. Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, Layla El Asri, ArXiv, abs/1812.008552018</p>
<p>First textworld problems, the competition: Using text-based games to advance capabilities of ai agents. Adam Trischler, Marc-Alexandre Côté, Pedro Lima, 2019</p>
<p>Learning to speak and act in a fantasy text adventure game. Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, J Weston, ArXiv, abs/1903.030942019</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems. 202033</p>
<p>Deep reinforcement learning with transformers for text adventure games. Y Xu, L Chen, M Fang, Y Wang, C Zhang, 10.1109/CoG47356.2020.92316222020 IEEE Conference on Games (CoG). 2020</p>
<p>Keep calm and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>Zero-shot learning of text adventure games with sentence-level semantics. Xusen Yin, Jonathan May, arXiv:2004.029862020arXiv preprint</p>
<p>Learning to generalize for sequential decision making. R Xusen Yin, Jonathan Weischedel, May, 2020EMNLP</p>
<p>Counting to explore and generalize in text-based games. Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet Des Combes, Matthew Hausknecht, Adam Trischler, 35th International Conference on Machine Learning, Exploration in Reinforcement Learning Workshop. 2018</p>
<p>Interactive language learning by question answering. Xingdi Yuan, Marc-Alexandre Côté, Jie Fu, Zhouhan Lin, Christopher Pal, Yoshua Bengio, Adam Trischler, 2019</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, 2018</p>
<p>Building dynamic knowledge graphs from text-based games. Mikulá Zelinka, Xingdi Yuan, Marc-Alexandre Côté, R Laroche, Adam Trischler, ArXiv, abs/1910.095322019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1394 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1394</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1394</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-8eeccf1033fd759c868b29ccb6aa70f5ffd1887c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8eeccf1033fd759c868b29ccb6aa70f5ffd1887c" target="_blank">TD-MPC2: Scalable, Robust World Models for Continuous Control</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters.</p>
                <p><strong>Paper Abstract:</strong> TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://tdmpc2.com</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1394.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1394.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TD-MPC2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TD-MPC2 (Temporal Difference Model Predictive Control 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-free (implicit), control-centric latent world model trained jointly with reward and value predictors, used for online MPC (MPPI) planning; designed for robust, scalable multitask continuous control across many embodiments and action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TD-MPC2 implicit world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implicit (decoder-free) latent world model: observations encoded by an encoder h(s,e) into a normalized latent z (SimNorm), a latent dynamics model d(z,a,e) predicts future latent embeddings, and auxiliary heads predict reward R(z,a,e), terminal value Q(z,a,e), and a policy prior p(z,e). The model is trained with joint-embedding prediction (L2 to stop-grad encoded next-latent), discrete (log-space) reward and value regression (cross-entropy), and TD targets bootstrapped with an EMA ensemble of Q-functions. No decoder is used to reconstruct observations; planning is performed in latent space with a sample-based MPPI optimizer guided by the learned policy prior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (implicit, decoder-free)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Large-scale continuous control (DMControl, Meta-World, ManiSkill2, MyoSuite) — single-task and massively multi-task robotic/control domains</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Joint-embedding L2 prediction error for next-latent; cross-entropy (discrete regression) for reward and value in a log-transformed space; TD loss against bootstrapped Q-targets (EMA ensemble); downstream task return/success used as ultimate fidelity proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper does not report standard generic prediction metrics (e.g., MSE on raw states); fidelity is reported indirectly via task performance. TD-MPC2 achieves substantially higher data-efficiency and asymptotic performance than baselines across 104 tasks; normalized aggregate scores (multi-task) improve with size (examples from Table 1): 1M params -> score 16.0, 5M -> 49.5, 19M -> 57.1, 48M -> 68.0, 317M -> 70.6 (normalized score across 80 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Predominantly a black-box neural latent model, but offers limited interpretability via learned task embeddings (t-SNE visualization shows semantically/dynamically similar tasks cluster) and the simplicial SimNorm latent which biases sparsity; latent dimensions are not claimed to correspond to explicit physical variables.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>t-SNE visualization of learned task embeddings; SimNorm (simplicial normalization) imposes structure on latent representations; ensemble Q-functions and ablations examine design-effect interpretability/robustness. No explicit state-variable disentanglement or symbolic extraction is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Default single-task model 5M parameters; multitask experiments scale models from 1M to 317M parameters. Training cost on the 80-task dataset (single RTX 3090): 1M -> 3.7 GPU-days, 5M -> 4.2 GPU-days, 19M -> 5.3 GPU-days, 48M -> 12 GPU-days, 317M -> 33 GPU-days (Table 1). Uses UTD=1 (low update-to-data), default batch 256 for single-task, larger multitask batch 1024. Planning uses MPPI (sample-based) with code optimizations giving ~2x planning throughput improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Designed to be computationally efficient relative to reconstructive generative models: no decoder reduces per-step prediction cost and memory. Compared to DreamerV3 baseline (approx 20M params) TD-MPC2 uses lower UTD (1 vs DreamerV3's 512) and smaller default batch while achieving higher data-efficiency; code-level optimizations and policy-prior warm-start reduce planning compute. Empirically TD-MPC2 trains and plans with modest compute (see GPU-day table) and scales to 317M parameters with practical resource use.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Outperforms state-of-the-art model-free (SAC) and model-based (DreamerV3, TD-MPC) baselines across 104 continuous-control tasks, including challenging high-dimensional locomotion and multi-object manipulation. Demonstrated few-shot benefits: finetuning a pretrained 19M TD-MPC2 improves low-data learning (~2x over learning from scratch on some held-out tasks after 20k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>By prioritizing predictions of task-relevant quantities (rewards and returns) and joint-embedding dynamics rather than pixel-perfect observation reconstruction, TD-MPC2 translates model learning into improved planning and control; i.e., high utility for control despite not modeling raw observations explicitly. The paper argues that task-relevant fidelity (reward/value accuracy and latent predictive consistency) is more important for downstream control than raw observation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off: avoiding decoders reduces computational cost and focuses capacity on control-relevant signals but sacrifices the ability to reconstruct raw observations (so less useful for tasks needing detailed image prediction). The discrete regression on rewards/values improves robustness across varying reward scales at modest compute cost; SimNorm enforces latent structure and stability but potentially limits representational freedom. Larger models improve performance (scale benefits) but require increased GPU-days (e.g., 33 GPU-days for 317M).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: implicit (decoder-free) objective (joint-embedding prediction + reward/value CE), SimNorm simplicial normalization for latent z, discrete log-space regression for reward/value, ensemble of 5 Q-functions with EMA targets, maximum-entropy policy prior p, action-masking and zero-padding to accommodate heterogeneous action spaces, MPPI planning in latent space with policy-prior warm start, dropout in Q heads and LayerNorm/Mish activations, UTD=1 default, and normalized learnable task embeddings (L2 <=1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Versus DreamerV3 (generative): TD-MPC2 focuses on implicit control-centric modeling (no decoder) and is more stable and data-efficient on continuous control tasks in this paper; DreamerV3 sometimes has numerical instabilities (e.g., Dog tasks) and struggles with fine manipulation. Versus TD-MPC (original): TD-MPC2 addresses TD-MPC's training instabilities (exploding gradients), scales far better (5M->317M), and uses additional architectural and objective changes (SimNorm, discrete CE rewards/values, larger Q ensemble). Versus model-free SAC: TD-MPC2 is more data-efficient across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends an implicit, control-centric world model (no decoder) with SimNorm latent normalization, discrete log-space reward/value regression, ensemble Q-functions and EMA targets, policy prior trained with maximum entropy, action masking for heterogeneous action spaces, and planning with MPPI guided by the policy prior. Scaling model and data improves capabilities; these design choices jointly balance fidelity (task-relevant), interpretability (structured task embeddings), and computational efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TD-MPC2: Scalable, Robust World Models for Continuous Control', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1394.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1394.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TD-MPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TD-MPC (Temporal Difference Model Predictive Control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predecessor to TD-MPC2: an implicit latent world model that performs local trajectory optimization (planning) in latent space using TD-learning to bootstrap beyond planning horizon; used as a baseline and earlier algorithmic foundation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Temporal difference learning for model predictive control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TD-MPC implicit world model (original)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implicit (decoder-free) latent model similar in spirit to TD-MPC2: encodes observations to latent, recurrent latent dynamics predicts next-latent and reward/value estimates and plans in latent space using MPPI. Earlier architecture used ELU activations, no LayerNorm, no SimNorm latent normalization, fewer Q-functions (2), and continuous regression for rewards/values.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (implicit, decoder-free)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control single-task RL benchmarks (earlier DMControl experiments and other continuous control settings); used as baseline comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Joint-embedding prediction (L2 to stop-grad encoded next-latent) and continuous regression for reward/value (prior formulation); TD-target bootstrapping with EMA Q used for value training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Performed well in prior single-task settings but exhibited training instabilities (exploding gradients) on certain tasks in this paper; naive scaling of TD-MPC (increasing model size/data) often led to decreased performance unlike TD-MPC2.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Same black-box characteristics as TD-MPC2; no decoder so limited direct observation-level interpretability. Paper reports less stable latent behavior without SimNorm, making latent structure harder to analyze.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specifically used in the original TD-MPC in this paper; the current paper analyzes gradients and stability differences between TD-MPC and TD-MPC2 (Appendix G) to explain divergences.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Smaller default model (approx 1M parameters in prior work); lower nominal parameter count but suffered from instability when scaled. The paper reports TD-MPC's default used larger batch size in some baselines (512) whereas TD-MPC2 uses 256; TD-MPC2 added ~4M parameters relative to TD-MPC for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>TD-MPC was less robust when scaled; required careful per-task tuning and was more sensitive to exploding gradients; overall TD-MPC2 shows improved computational stability and scaling despite being larger.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Prior TD-MPC performs competitively on some single-task problems but in this paper TD-MPC often underperforms or diverges on difficult tasks (e.g., Walker variants) where TD-MPC2 remains stable and higher-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>TD-MPC's implicit modeling and planning provide utility for control but its training instabilities and continuous regression objective for rewards/values limit multi-task scalability and robustness; shows utility on some single-task settings but less suited for large heterogeneous multitask datasets without the TD-MPC2 improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Lower parameter count but more fragile training: continuous reward/value regression and lack of latent normalization made TD-MPC prone to exploding gradients; stabilizing required careful per-task tuning (tradeoff between simplicity and robust scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Implicit decoder-free model, continuous regression for rewards/values, ELU activations, no SimNorm latent normalization, 2 Q-functions (no dropout), deterministic policy prior with Gaussian noise schedule, prioritized replay in original implementation (later removed in TD-MPC2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to TD-MPC2: TD-MPC2 introduces architectural (LayerNorm, Mish), latent SimNorm, discrete log-space reward/value CE, Q-ensemble and dropout, and other improvements that yield far better robustness and scaling. Compared to DreamerV3 (generative), both are model-based but TD-MPC original shares the implicit/decode-free approach (like TD-MPC2) rather than observation reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies TD-MPC alone is insufficient for large-scale multitask settings; suggests adopting TD-MPC2 design changes (SimNorm, discrete reward/value regression, Q-ensemble, LayerNorm/Mish) to achieve optimal robustness and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TD-MPC2: Scalable, Robust World Models for Continuous Control', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1394.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1394.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL approach that learns a generative (reconstruction-based) world model and optimizes policies via rollouts from that generative model; used as a primary baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3 generative world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative latent world model that reconstructs observations (decoder-based), learns latent dynamics and value/policy components, and uses model rollouts to train a model-free policy; typically larger (baseline ~20M parameters in paper's comparisons) and optimized with high update-to-data ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent generative world model (decoder-based, reconstruction objective)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Diverse domains (reported strong on discrete domains like Atari and Minecraft in original work); used here as a baseline for continuous control visual/non-visual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (decoder likelihood / pixel or feature reconstruction), predictive likelihood of future observations, and downstream policy performance via model rollouts; in general Dreamer-family uses reconstruction objectives as fidelity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported as explicit numeric reconstruction fidelity in this paper; empirically in this paper DreamerV3 is less stable on certain high-dimensional continuous-control tasks (e.g., Dog locomotion), and struggles with fine-grained object manipulation compared to TD-MPC2.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generative models can be inspected via reconstructions, but internal latent dynamics remain largely black-box; no interpretability methods are reported in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None described in this paper; Dreamer-style models typically allow inspection of reconstructed observations but no explicit interpretable latent disentanglement is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>DreamerV3 baseline cited here is approx 20M learnable parameters and uses a high update-to-data ratio (UTD ~512) in its training regimen, implying heavier compute per environment step compared to TD-MPC2 (which uses UTD=1 by default).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to TD-MPC2: DreamerV3 uses heavier training schedules (high UTD), is less data-efficient on many continuous control tasks in this paper, and experiences occasional numerical instabilities on some tasks; however Dreamer-family can excel on large-scale discrete domains in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>In this paper DreamerV3 is a strong baseline for visual RL but is generally outperformed by TD-MPC2 on the continuous control suite (104 tasks); DreamerV3 sometimes shows numerical instabilities (Dog tasks) and struggles on fine object manipulation (ManiSkill2).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Reconstruction-focused fidelity (predicting raw observations) does not necessarily translate to superior control performance in these continuous-control tasks; the paper argues that control-centric implicit models (like TD-MPC2) can be more useful for diverse continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off: generative (decoder) models provide reconstruction capability and may capture rich observation statistics but come with higher compute and difficulty in long-horizon accurate observation prediction; such fidelity does not always improve control performance and can reduce robustness across heterogeneous multitask datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>DreamerV3 emphasizes reconstruction-based latent modeling with rollouts to train policies and uses discrete regression techniques to handle reward magnitude issues (cited as related work); training uses high UTD and larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to TD-MPC2: DreamerV3 is reconstructive and typically requires more aggressive training schedules; TD-MPC2's implicit approach leads to better stability and performance on the continuous-control benchmarks in this paper. DreamerV3 has prior success on different domains (Atari, Minecraft) where discrete action spaces and large datasets are used.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper beyond noting that DreamerV3 uses large models and high UTD; the paper suggests implicit, control-centric world models may be preferred for broad continuous-control multitask settings, while Dreamer architectures may be better suited to their original discrete-domain settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TD-MPC2: Scalable, Robust World Models for Continuous Control', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1394.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1394.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reconstruction-based world models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reconstruction-based (generative/decoder) world models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of world models that learn to reconstruct raw observations (images, proprioceptive features) via decoder-based generative objectives (e.g., VAEs), providing dense learning signals but often requiring larger capacity and struggling with long-horizon accurate prediction for control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reconstruction-based generative world models (decoder-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that explicitly model environment dynamics by decoding predicted latent states back to raw observations, trained with reconstruction losses (pixel-wise or perceptual), often combined with latent dynamics and reward/value heads; used to generate rollouts for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent generative world model (decoder-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used broadly in model-based RL and visual RL (e.g., Dreamer family) across simulated and real-world tasks; mentioned here in context of visual and continuous-control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (pixel MSE or likelihood), perceptual similarity metrics, and next-observation prediction error over horizons; often measured via reconstruction MSE or log-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper argues that accurate long-horizon observation reconstruction is difficult and does not necessarily lead to effective control (citing Lambert et al., 2020); no specific numeric fidelity values provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Reconstruction enables qualitative inspection (decoded frames) which can aid interpretability of what the model 'believes' the world looks like, but internal latent dynamics are still neural and largely opaque without additional disentangling techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Observation reconstructions and rollout visualizations; not specifically used in this paper beyond discussion and prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher per-step compute and memory due to decoding high-dimensional observations, potentially larger models and training overhead; discrete regression over high-dimensional latents can be computationally expensive (requires many bins per latent dimension).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper claims reconstruction-based approaches are less parameter-efficient for control and harder to scale for diverse datasets compared to implicit control-centric models; TD-MPC2 favors implicit modeling to reduce compute and focus capacity on task-relevant predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Mixed: reconstruction-based models can enable strong performance in some domains (e.g., Dreamer successes in prior work) but in this paper are reported to be less effective on the heterogeneous continuous-control suite compared to TD-MPC2.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity reconstruction does not guarantee good control performance; focusing model capacity on task-relevant predictions (returns/rewards and latent dynamics predictive of those) can yield better policy performance with lower compute.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Reconstruction yields richer learning signals but increases computational cost and may learn aspects of observations irrelevant for control; implicit models reduce this waste but lose full-reconstruction interpretability and some downstream uses that require decoded imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Decoder architecture, reconstruction loss type (pixel MSE, perceptual), latent dimensionality and bottleneck strength, whether to use discrete regression for rewards/values, and whether to use rollouts for policy learning (as Dreamer does).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to implicit TD-MPC2, reconstruction models are heavier and can be less robust on heterogeneous continuous-control datasets; however, they may be preferred for tasks where accurate visual predictions (reconstructions) are necessary or in discrete-domain successes reported in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that for scalable multitask continuous control, emphasizing control-centric implicit objectives (predict returns and joint-embeddings) rather than full observation reconstruction is a more practical configuration; reconstruction models remain relevant where reconstruction fidelity is required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TD-MPC2: Scalable, Robust World Models for Continuous Control', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Temporal difference learning for model predictive control <em>(Rating: 2)</em></li>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Objective mismatch in model-based reinforcement learning <em>(Rating: 1)</em></li>
                <li>Bootstrap your own latent: A new approach to self-supervised learning <em>(Rating: 1)</em></li>
                <li>Finetuning offline world models in the real world <em>(Rating: 1)</em></li>
                <li>Modem: Accelerating visual model-based reinforcement learning with demonstrations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1394",
    "paper_id": "paper-8eeccf1033fd759c868b29ccb6aa70f5ffd1887c",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "TD-MPC2",
            "name_full": "TD-MPC2 (Temporal Difference Model Predictive Control 2)",
            "brief_description": "A decoder-free (implicit), control-centric latent world model trained jointly with reward and value predictors, used for online MPC (MPPI) planning; designed for robust, scalable multitask continuous control across many embodiments and action spaces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TD-MPC2 implicit world model",
            "model_description": "Implicit (decoder-free) latent world model: observations encoded by an encoder h(s,e) into a normalized latent z (SimNorm), a latent dynamics model d(z,a,e) predicts future latent embeddings, and auxiliary heads predict reward R(z,a,e), terminal value Q(z,a,e), and a policy prior p(z,e). The model is trained with joint-embedding prediction (L2 to stop-grad encoded next-latent), discrete (log-space) reward and value regression (cross-entropy), and TD targets bootstrapped with an EMA ensemble of Q-functions. No decoder is used to reconstruct observations; planning is performed in latent space with a sample-based MPPI optimizer guided by the learned policy prior.",
            "model_type": "latent world model (implicit, decoder-free)",
            "task_domain": "Large-scale continuous control (DMControl, Meta-World, ManiSkill2, MyoSuite) — single-task and massively multi-task robotic/control domains",
            "fidelity_metric": "Joint-embedding L2 prediction error for next-latent; cross-entropy (discrete regression) for reward and value in a log-transformed space; TD loss against bootstrapped Q-targets (EMA ensemble); downstream task return/success used as ultimate fidelity proxy.",
            "fidelity_performance": "Paper does not report standard generic prediction metrics (e.g., MSE on raw states); fidelity is reported indirectly via task performance. TD-MPC2 achieves substantially higher data-efficiency and asymptotic performance than baselines across 104 tasks; normalized aggregate scores (multi-task) improve with size (examples from Table 1): 1M params -&gt; score 16.0, 5M -&gt; 49.5, 19M -&gt; 57.1, 48M -&gt; 68.0, 317M -&gt; 70.6 (normalized score across 80 tasks).",
            "interpretability_assessment": "Predominantly a black-box neural latent model, but offers limited interpretability via learned task embeddings (t-SNE visualization shows semantically/dynamically similar tasks cluster) and the simplicial SimNorm latent which biases sparsity; latent dimensions are not claimed to correspond to explicit physical variables.",
            "interpretability_method": "t-SNE visualization of learned task embeddings; SimNorm (simplicial normalization) imposes structure on latent representations; ensemble Q-functions and ablations examine design-effect interpretability/robustness. No explicit state-variable disentanglement or symbolic extraction is presented.",
            "computational_cost": "Default single-task model 5M parameters; multitask experiments scale models from 1M to 317M parameters. Training cost on the 80-task dataset (single RTX 3090): 1M -&gt; 3.7 GPU-days, 5M -&gt; 4.2 GPU-days, 19M -&gt; 5.3 GPU-days, 48M -&gt; 12 GPU-days, 317M -&gt; 33 GPU-days (Table 1). Uses UTD=1 (low update-to-data), default batch 256 for single-task, larger multitask batch 1024. Planning uses MPPI (sample-based) with code optimizations giving ~2x planning throughput improvement.",
            "efficiency_comparison": "Designed to be computationally efficient relative to reconstructive generative models: no decoder reduces per-step prediction cost and memory. Compared to DreamerV3 baseline (approx 20M params) TD-MPC2 uses lower UTD (1 vs DreamerV3's 512) and smaller default batch while achieving higher data-efficiency; code-level optimizations and policy-prior warm-start reduce planning compute. Empirically TD-MPC2 trains and plans with modest compute (see GPU-day table) and scales to 317M parameters with practical resource use.",
            "task_performance": "Outperforms state-of-the-art model-free (SAC) and model-based (DreamerV3, TD-MPC) baselines across 104 continuous-control tasks, including challenging high-dimensional locomotion and multi-object manipulation. Demonstrated few-shot benefits: finetuning a pretrained 19M TD-MPC2 improves low-data learning (~2x over learning from scratch on some held-out tasks after 20k steps).",
            "task_utility_analysis": "By prioritizing predictions of task-relevant quantities (rewards and returns) and joint-embedding dynamics rather than pixel-perfect observation reconstruction, TD-MPC2 translates model learning into improved planning and control; i.e., high utility for control despite not modeling raw observations explicitly. The paper argues that task-relevant fidelity (reward/value accuracy and latent predictive consistency) is more important for downstream control than raw observation fidelity.",
            "tradeoffs_observed": "Trade-off: avoiding decoders reduces computational cost and focuses capacity on control-relevant signals but sacrifices the ability to reconstruct raw observations (so less useful for tasks needing detailed image prediction). The discrete regression on rewards/values improves robustness across varying reward scales at modest compute cost; SimNorm enforces latent structure and stability but potentially limits representational freedom. Larger models improve performance (scale benefits) but require increased GPU-days (e.g., 33 GPU-days for 317M).",
            "design_choices": "Key choices: implicit (decoder-free) objective (joint-embedding prediction + reward/value CE), SimNorm simplicial normalization for latent z, discrete log-space regression for reward/value, ensemble of 5 Q-functions with EMA targets, maximum-entropy policy prior p, action-masking and zero-padding to accommodate heterogeneous action spaces, MPPI planning in latent space with policy-prior warm start, dropout in Q heads and LayerNorm/Mish activations, UTD=1 default, and normalized learnable task embeddings (L2 &lt;=1).",
            "comparison_to_alternatives": "Versus DreamerV3 (generative): TD-MPC2 focuses on implicit control-centric modeling (no decoder) and is more stable and data-efficient on continuous control tasks in this paper; DreamerV3 sometimes has numerical instabilities (e.g., Dog tasks) and struggles with fine manipulation. Versus TD-MPC (original): TD-MPC2 addresses TD-MPC's training instabilities (exploding gradients), scales far better (5M-&gt;317M), and uses additional architectural and objective changes (SimNorm, discrete CE rewards/values, larger Q ensemble). Versus model-free SAC: TD-MPC2 is more data-efficient across many tasks.",
            "optimal_configuration": "Paper recommends an implicit, control-centric world model (no decoder) with SimNorm latent normalization, discrete log-space reward/value regression, ensemble Q-functions and EMA targets, policy prior trained with maximum entropy, action masking for heterogeneous action spaces, and planning with MPPI guided by the policy prior. Scaling model and data improves capabilities; these design choices jointly balance fidelity (task-relevant), interpretability (structured task embeddings), and computational efficiency.",
            "uuid": "e1394.0",
            "source_info": {
                "paper_title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "TD-MPC",
            "name_full": "TD-MPC (Temporal Difference Model Predictive Control)",
            "brief_description": "Predecessor to TD-MPC2: an implicit latent world model that performs local trajectory optimization (planning) in latent space using TD-learning to bootstrap beyond planning horizon; used as a baseline and earlier algorithmic foundation.",
            "citation_title": "Temporal difference learning for model predictive control",
            "mention_or_use": "use",
            "model_name": "TD-MPC implicit world model (original)",
            "model_description": "Implicit (decoder-free) latent model similar in spirit to TD-MPC2: encodes observations to latent, recurrent latent dynamics predicts next-latent and reward/value estimates and plans in latent space using MPPI. Earlier architecture used ELU activations, no LayerNorm, no SimNorm latent normalization, fewer Q-functions (2), and continuous regression for rewards/values.",
            "model_type": "latent world model (implicit, decoder-free)",
            "task_domain": "Continuous control single-task RL benchmarks (earlier DMControl experiments and other continuous control settings); used as baseline comparison in this paper.",
            "fidelity_metric": "Joint-embedding prediction (L2 to stop-grad encoded next-latent) and continuous regression for reward/value (prior formulation); TD-target bootstrapping with EMA Q used for value training.",
            "fidelity_performance": "Performed well in prior single-task settings but exhibited training instabilities (exploding gradients) on certain tasks in this paper; naive scaling of TD-MPC (increasing model size/data) often led to decreased performance unlike TD-MPC2.",
            "interpretability_assessment": "Same black-box characteristics as TD-MPC2; no decoder so limited direct observation-level interpretability. Paper reports less stable latent behavior without SimNorm, making latent structure harder to analyze.",
            "interpretability_method": "None specifically used in the original TD-MPC in this paper; the current paper analyzes gradients and stability differences between TD-MPC and TD-MPC2 (Appendix G) to explain divergences.",
            "computational_cost": "Smaller default model (approx 1M parameters in prior work); lower nominal parameter count but suffered from instability when scaled. The paper reports TD-MPC's default used larger batch size in some baselines (512) whereas TD-MPC2 uses 256; TD-MPC2 added ~4M parameters relative to TD-MPC for stability.",
            "efficiency_comparison": "TD-MPC was less robust when scaled; required careful per-task tuning and was more sensitive to exploding gradients; overall TD-MPC2 shows improved computational stability and scaling despite being larger.",
            "task_performance": "Prior TD-MPC performs competitively on some single-task problems but in this paper TD-MPC often underperforms or diverges on difficult tasks (e.g., Walker variants) where TD-MPC2 remains stable and higher-performing.",
            "task_utility_analysis": "TD-MPC's implicit modeling and planning provide utility for control but its training instabilities and continuous regression objective for rewards/values limit multi-task scalability and robustness; shows utility on some single-task settings but less suited for large heterogeneous multitask datasets without the TD-MPC2 improvements.",
            "tradeoffs_observed": "Lower parameter count but more fragile training: continuous reward/value regression and lack of latent normalization made TD-MPC prone to exploding gradients; stabilizing required careful per-task tuning (tradeoff between simplicity and robust scaling).",
            "design_choices": "Implicit decoder-free model, continuous regression for rewards/values, ELU activations, no SimNorm latent normalization, 2 Q-functions (no dropout), deterministic policy prior with Gaussian noise schedule, prioritized replay in original implementation (later removed in TD-MPC2).",
            "comparison_to_alternatives": "Compared to TD-MPC2: TD-MPC2 introduces architectural (LayerNorm, Mish), latent SimNorm, discrete log-space reward/value CE, Q-ensemble and dropout, and other improvements that yield far better robustness and scaling. Compared to DreamerV3 (generative), both are model-based but TD-MPC original shares the implicit/decode-free approach (like TD-MPC2) rather than observation reconstruction.",
            "optimal_configuration": "Paper implies TD-MPC alone is insufficient for large-scale multitask settings; suggests adopting TD-MPC2 design changes (SimNorm, discrete reward/value regression, Q-ensemble, LayerNorm/Mish) to achieve optimal robustness and scalability.",
            "uuid": "e1394.1",
            "source_info": {
                "paper_title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DreamerV3",
            "name_full": "DreamerV3",
            "brief_description": "A model-based RL approach that learns a generative (reconstruction-based) world model and optimizes policies via rollouts from that generative model; used as a primary baseline in comparisons.",
            "citation_title": "Mastering diverse domains through world models",
            "mention_or_use": "use",
            "model_name": "DreamerV3 generative world model",
            "model_description": "Generative latent world model that reconstructs observations (decoder-based), learns latent dynamics and value/policy components, and uses model rollouts to train a model-free policy; typically larger (baseline ~20M parameters in paper's comparisons) and optimized with high update-to-data ratios.",
            "model_type": "latent generative world model (decoder-based, reconstruction objective)",
            "task_domain": "Diverse domains (reported strong on discrete domains like Atari and Minecraft in original work); used here as a baseline for continuous control visual/non-visual tasks.",
            "fidelity_metric": "Reconstruction loss (decoder likelihood / pixel or feature reconstruction), predictive likelihood of future observations, and downstream policy performance via model rollouts; in general Dreamer-family uses reconstruction objectives as fidelity metric.",
            "fidelity_performance": "Not reported as explicit numeric reconstruction fidelity in this paper; empirically in this paper DreamerV3 is less stable on certain high-dimensional continuous-control tasks (e.g., Dog locomotion), and struggles with fine-grained object manipulation compared to TD-MPC2.",
            "interpretability_assessment": "Generative models can be inspected via reconstructions, but internal latent dynamics remain largely black-box; no interpretability methods are reported in this paper's comparisons.",
            "interpretability_method": "None described in this paper; Dreamer-style models typically allow inspection of reconstructed observations but no explicit interpretable latent disentanglement is reported here.",
            "computational_cost": "DreamerV3 baseline cited here is approx 20M learnable parameters and uses a high update-to-data ratio (UTD ~512) in its training regimen, implying heavier compute per environment step compared to TD-MPC2 (which uses UTD=1 by default).",
            "efficiency_comparison": "Compared to TD-MPC2: DreamerV3 uses heavier training schedules (high UTD), is less data-efficient on many continuous control tasks in this paper, and experiences occasional numerical instabilities on some tasks; however Dreamer-family can excel on large-scale discrete domains in prior work.",
            "task_performance": "In this paper DreamerV3 is a strong baseline for visual RL but is generally outperformed by TD-MPC2 on the continuous control suite (104 tasks); DreamerV3 sometimes shows numerical instabilities (Dog tasks) and struggles on fine object manipulation (ManiSkill2).",
            "task_utility_analysis": "Reconstruction-focused fidelity (predicting raw observations) does not necessarily translate to superior control performance in these continuous-control tasks; the paper argues that control-centric implicit models (like TD-MPC2) can be more useful for diverse continuous control.",
            "tradeoffs_observed": "Trade-off: generative (decoder) models provide reconstruction capability and may capture rich observation statistics but come with higher compute and difficulty in long-horizon accurate observation prediction; such fidelity does not always improve control performance and can reduce robustness across heterogeneous multitask datasets.",
            "design_choices": "DreamerV3 emphasizes reconstruction-based latent modeling with rollouts to train policies and uses discrete regression techniques to handle reward magnitude issues (cited as related work); training uses high UTD and larger models.",
            "comparison_to_alternatives": "Compared to TD-MPC2: DreamerV3 is reconstructive and typically requires more aggressive training schedules; TD-MPC2's implicit approach leads to better stability and performance on the continuous-control benchmarks in this paper. DreamerV3 has prior success on different domains (Atari, Minecraft) where discrete action spaces and large datasets are used.",
            "optimal_configuration": "Not specified in this paper beyond noting that DreamerV3 uses large models and high UTD; the paper suggests implicit, control-centric world models may be preferred for broad continuous-control multitask settings, while Dreamer architectures may be better suited to their original discrete-domain settings.",
            "uuid": "e1394.2",
            "source_info": {
                "paper_title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reconstruction-based world models",
            "name_full": "Reconstruction-based (generative/decoder) world models",
            "brief_description": "Class of world models that learn to reconstruct raw observations (images, proprioceptive features) via decoder-based generative objectives (e.g., VAEs), providing dense learning signals but often requiring larger capacity and struggling with long-horizon accurate prediction for control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Reconstruction-based generative world models (decoder-based)",
            "model_description": "Models that explicitly model environment dynamics by decoding predicted latent states back to raw observations, trained with reconstruction losses (pixel-wise or perceptual), often combined with latent dynamics and reward/value heads; used to generate rollouts for policy learning.",
            "model_type": "latent generative world model (decoder-based)",
            "task_domain": "Used broadly in model-based RL and visual RL (e.g., Dreamer family) across simulated and real-world tasks; mentioned here in context of visual and continuous-control tasks.",
            "fidelity_metric": "Reconstruction loss (pixel MSE or likelihood), perceptual similarity metrics, and next-observation prediction error over horizons; often measured via reconstruction MSE or log-likelihood.",
            "fidelity_performance": "Paper argues that accurate long-horizon observation reconstruction is difficult and does not necessarily lead to effective control (citing Lambert et al., 2020); no specific numeric fidelity values provided in this paper.",
            "interpretability_assessment": "Reconstruction enables qualitative inspection (decoded frames) which can aid interpretability of what the model 'believes' the world looks like, but internal latent dynamics are still neural and largely opaque without additional disentangling techniques.",
            "interpretability_method": "Observation reconstructions and rollout visualizations; not specifically used in this paper beyond discussion and prior literature.",
            "computational_cost": "Higher per-step compute and memory due to decoding high-dimensional observations, potentially larger models and training overhead; discrete regression over high-dimensional latents can be computationally expensive (requires many bins per latent dimension).",
            "efficiency_comparison": "Paper claims reconstruction-based approaches are less parameter-efficient for control and harder to scale for diverse datasets compared to implicit control-centric models; TD-MPC2 favors implicit modeling to reduce compute and focus capacity on task-relevant predictions.",
            "task_performance": "Mixed: reconstruction-based models can enable strong performance in some domains (e.g., Dreamer successes in prior work) but in this paper are reported to be less effective on the heterogeneous continuous-control suite compared to TD-MPC2.",
            "task_utility_analysis": "High-fidelity reconstruction does not guarantee good control performance; focusing model capacity on task-relevant predictions (returns/rewards and latent dynamics predictive of those) can yield better policy performance with lower compute.",
            "tradeoffs_observed": "Reconstruction yields richer learning signals but increases computational cost and may learn aspects of observations irrelevant for control; implicit models reduce this waste but lose full-reconstruction interpretability and some downstream uses that require decoded imagery.",
            "design_choices": "Decoder architecture, reconstruction loss type (pixel MSE, perceptual), latent dimensionality and bottleneck strength, whether to use discrete regression for rewards/values, and whether to use rollouts for policy learning (as Dreamer does).",
            "comparison_to_alternatives": "Compared to implicit TD-MPC2, reconstruction models are heavier and can be less robust on heterogeneous continuous-control datasets; however, they may be preferred for tasks where accurate visual predictions (reconstructions) are necessary or in discrete-domain successes reported in prior literature.",
            "optimal_configuration": "Paper suggests that for scalable multitask continuous control, emphasizing control-centric implicit objectives (predict returns and joint-embeddings) rather than full observation reconstruction is a more practical configuration; reconstruction models remain relevant where reconstruction fidelity is required.",
            "uuid": "e1394.3",
            "source_info": {
                "paper_title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Temporal difference learning for model predictive control",
            "rating": 2
        },
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2
        },
        {
            "paper_title": "Objective mismatch in model-based reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "rating": 1
        },
        {
            "paper_title": "Finetuning offline world models in the real world",
            "rating": 1
        },
        {
            "paper_title": "Modem: Accelerating visual model-based reinforcement learning with demonstrations",
            "rating": 1
        }
    ],
    "cost": 0.017615,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TD-MPC2:</h1>
<h2>Scalable, Robust World Models for Continuous Control</h2>
<p>Nicklas Hansen ${ }^{+}$, Hao Su ${ }^{+ \dagger}$, Xiaolong Wang ${ }^{+ \dagger}$<br>${ }^{+}$University of California San Diego, ${ }^{\dagger}$ Equal advising<br>{nihansen, haosu, xiw012}@ucsd.edu</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview. TD-MPC2 compares favorably to existing model-free and model-based RL methods across $\mathbf{1 0 4}$ continuous control tasks spanning multiple domains, with a single set of hyperparameters (right). We further demonstrate the scalability of TD-MPC2 by training a single 317M parameter agent to perform $\mathbf{8 0}$ tasks across multiple domains, embodiments, and action spaces (left).</p>
<h4>Abstract</h4>
<p>TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoderfree) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across $\mathbf{1 0 4}$ online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317 M parameter agent to perform $\mathbf{8 0}$ tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.</p>
<p>Explore videos, models, data, code, and more at https://tdmpc2.com</p>
<h2>1 INTRODUCTION</h2>
<p>Training large models on internet-scale datasets has led to generalist models that perform a wide variety of language and vision tasks (Brown et al., 2020; He et al., 2022; Kirillov et al., 2023). The success of these models can largely be attributed to the availability of enormous datasets, and carefully designed architectures that reliably scale with model and data size. While researchers have recently extended this paradigm to robotics (Reed et al., 2022; Brohan et al., 2023), a generalist embodied agent that learns to perform diverse control tasks via low-level actions, across multiple embodiments, from large uncurated (i.e., mixed-quality) datasets remains an elusive goal. We argue that current approaches to generalist embodied agents suffer from (a) the assumption of near-expert trajectories for behavior cloning which severely limits the amount of available data (Reed et al., 2022; Lee et al., 2022; Kumar et al., 2022; Schubert et al., 2023; Driess et al., 2023; Brohan et al., 2023), and (b) a lack of scalable continuous control algorithms that are able to consume large uncurated datasets.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Tasks. TD-MPC2 performs 104 diverse tasks from (left to right) DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al., 2022), with a single set of hyperparameters. See Appendix B for visualization of all tasks.</p>
<p>Reinforcement Learning (RL) is an ideal framework for extracting expert behavior from uncurated datasets. However, most existing RL algorithms (Lillicrap et al., 2016; Haarnoja et al., 2018) are designed for single-task learning and rely on per-task hyperparameters, with no principled method for selecting those hyperparameters (Zhang et al., 2021). An algorithm that can consume large multitask datasets will invariably need to be robust to variation between different tasks (e.g., action space dimensionality, difficulty of exploration, and reward distribution). In this work, we present TDMPC2: a significant step towards achieving this goal. TD-MPC2 is a model-based RL algorithm designed for learning generalist world models on large uncurated datasets composed of multiple task domains, embodiments, and action spaces, with data sourced from behavior policies that cover a wide range of skill levels, and without the need for hyperparameter-tuning.
Our algorithm, which builds upon TD-MPC (Hansen et al., 2022), performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. While the TD-MPC family of algorithms has demonstrated strong empirical performance in prior work (Hansen et al., 2022; 2023; Yuan et al., 2022; Yang et al., 2023; Feng et al., 2023; Chitnis et al., 2023; Zhu et al., 2023; Lancaster et al., 2023), most successes have been limited to single-task learning with little emphasis on scaling. As shown in Figure 1, naïvely increasing model and data size of TD-MPC often leads to a net decrease in agent performance, as is commonly observed in RL literature (Kumar et al., 2023). In contrast, scaling TD-MPC2 leads to consistently improved capabilities. Our algorithmic contributions, which have been key to achieving this milestone, are two-fold: (1) improved algorithmic robustness by revisiting core design choices, and (2) careful design of an architecture that can accommodate datasets with multiple embodiments and action spaces without relying on domain knowledge. The resulting algorithm, TD-MPC2, is scalable, robust, and can be applied to a variety of single-task and multi-task continuous control problems using a single set of hyperparameters.
We evaluate TD-MPC2 across a total of 104 diverse continuous control tasks spanning 4 task domains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al., 2022). We summarize our results in Figure 1, and visualize task domains in Figure 2. Tasks include high-dimensional state and action spaces (up to $\mathcal{A} \in \mathbb{R}^{39}$ ), image observations, sparse rewards, multi-object manipulation, physiologically accurate musculoskeletal motor control, complex locomotion (e.g. Dog and Humanoid embodiments), and cover a wide range of task difficulties. Our results demonstrate that TD-MPC2 consistently outperforms existing model-based and model-free methods, using the same hyperparameters across all tasks (Figure 1, right). Here, "Locomotion" and "Pick YCB" are particularly challenging subsets of DMControl and ManiSkill2, respectively. We further show that agent capabilities increase with model and data size, and successfully train a single 317 M parameter world model to perform 80 tasks across multiple task domains, embodiments, and action spaces (Figure 1, left). In support of open-source science, we publicly release 300+ model checkpoints, datasets, and code for training and evaluating TD-MPC2 agents, which is available at https://t-dmpc2.com. We conclude the paper with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.</p>
<h1>2 BACKGROUND</h1>
<p>Reinforcement Learning (RL) aims to learn a policy from interaction with an environment, formulated as a Markov Decision Process (MDP) (Bellman, 1957). We focus on infinite-horizon MDPs with continuous action spaces, which can be formalized as a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T}, R, \gamma)$ where $\mathbf{s} \in \mathcal{S}$ are states, $\mathbf{a} \in \mathcal{A}$ are actions, $\mathcal{T}: \mathcal{S} \times \mathcal{A} \mapsto \mathcal{S}$ is the transition function, $\mathcal{R}: \mathcal{S} \times \mathcal{A} \mapsto \mathbb{R}$ is a reward function associated with a particular task, and $\gamma$ is a discount factor. The goal is to</p>
<p>$\operatorname{derive}$ a control policy $\pi: \mathcal{S} \mapsto \mathcal{A}$ such that the expected discounted sum of rewards (return) $\mathbb{E}<em t="0">{\pi}\left[\sum</em>}^{\infty} \gamma^{t} r_{t}\right], r_{t}=R\left(\mathbf{s<em t="t">{t}, \pi\left(\mathbf{s}</em>\right)\right)$ is maximized. In this work, we obtain $\pi$ by learning a world model (model of the environment) and then select actions by planning with the learned model.</p>
<p>Model Predictive Control (MPC) is a general framework for model-based control that optimizes action sequences $\mathbf{a}_{t: t+H}$ of finite length such that return is maximized (or cost is minimized) over the time horizon $H$, which corresponds to solving the following optimization problem:</p>
<p>$$
\pi\left(\mathbf{s}<em _mathbf_a="\mathbf{a">{t}\right)=\arg \max </em><em i="0">{t: t+H}} \mathbb{E}\left[\sum</em>}^{H} \gamma^{t+i} R\left(\mathbf{s<em t_i="t+i">{t+i}, \mathbf{a}</em>\right)\right]
$$</p>
<p>The return of a candidate trajectory is estimated by simulating it with the learned model (Negenborn et al., 2005). Thus, a policy obtained by Equation 1 will invariably be a (temporally) locally optimal policy and is not guaranteed (nor likely) to be a solution to the general reinforcement learning problem outlined above. As we discuss in the following, TD-MPC2 addresses this shortcoming of local trajectory optimization by bootstrapping return estimates beyond horizon $H$ with a learned terminal value function.</p>
<h1>3 TD-MPC2</h1>
<p>Our work builds upon TD-MPC (Hansen et al., 2022), a model-based RL algorithm that performs local trajectory optimization (planning) in the latent space of a learned implicit world model. TDMPC2 is a practical algorithm for training massively multitask world models. Specifically, we propose a series of improvements to the TD-MPC algorithm, which have been key to achieving strong algorithmic robustness (can use the same hyperparameters across all tasks) and scaling its world model to $\mathbf{3 0 0} \times$ more parameters than previously. We introduce the TD-MPC2 algorithm in the following, and provide a full list of algorithmic improvements in Appendix A.</p>
<h3>3.1 LEARNING AN IMPLICIT WORLD MODEL</h3>
<p>Learning a generative model of the environment using a reconstruction (decoder) objective is tempting due to its rich learning signal. However, accurately predicting raw future observations (e.g., images or proprioceptive features) over long time horizons is a difficult problem, and does not necessarily lead to effective control (Lambert et al., 2020). Rather than explicitly modeling dynamics using reconstruction, TD-MPC2 aims to learn a maximally useful model: a model that accurately predicts outcomes (returns) conditioned on a sequence of actions. Specifically, TD-MPC2 learns an implicit, control-centric world model from environment interaction using a combination of joint-embedding prediction (Grill et al., 2020), reward prediction, and TD-learning (Sutton, 1998), without decoding observations. We argue that this alternative formulation of model-based RL is key to modeling large datasets with modest model sizes. The world model can subsequently be used for decision-making by performing local trajectory optimization (planning) following the MPC framework.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. The TD-MPC2 architecture. Observations $\mathbf{s}$ are encoded into their (normalized) latent representation $\mathbf{z}$. The model then recurrently predicts actions $\hat{\mathbf{a}}$, rewards $\hat{r}$, and terminal values $\hat{q}$, without decoding future observations.</p>
<p>Components. The TD-MPC2 architecture is shown in Figure 3 and consists of five components:
Encoder $\quad \mathbf{z}=h(\mathbf{s}, \mathbf{e}) \quad \triangleright$ Maps observations to their latent representations
Latent dynamics $\quad \mathbf{z}^{\prime}=d(\mathbf{z}, \mathbf{a}, \mathbf{e}) \quad \triangleright$ Models (latent) forward dynamics
Reward $\quad \hat{r}=R(\mathbf{z}, \mathbf{a}, \mathbf{e}) \quad \triangleright$ Predicts reward $r$ of a transition
Terminal value $\quad \hat{q}=Q(\mathbf{z}, \mathbf{a}, \mathbf{e}) \quad \triangleright$ Predicts discounted sum of rewards (return)
Policy prior $\quad \hat{\mathbf{a}}=p(\mathbf{z}, \mathbf{e}) \quad \triangleright$ Predicts action $\mathbf{a}^{*}$ that maximizes $Q$
where $\mathbf{s}$ and $\mathbf{a}$ are states and actions, $\mathbf{z}$ is the latent representation, and $\mathbf{e}$ is a learnable task embedding for use in multitask world models. For visual clarity, we will omit $\mathbf{e}$ in the following unless it is</p>
<p>particularly relevant. The policy prior $p$ serves to guide the sample-based trajectory optimizer (planner), and to reduce the computational cost of TD-learning. During online interaction, TD-MPC2 maintains a replay buffer $\mathcal{B}$ with trajectories, and iteratively (i) updates the world model using data sampled from $\mathcal{B}$, and (ii) collects new environment data by planning with the learned model.
Model objective. The $h, d, R, Q$ components are jointly optimized to minimize the objective</p>
<p>$$
\mathcal{L}(\theta) \doteq \underset{\left(\mathbf{s}, \mathbf{a}, r, \mathbf{s}^{\prime}\right)<em t="0">{0: H} \sim \mathcal{B}}{\mathbb{E}}\left[\sum</em>}^{H} \lambda^{t}\left(\underbrace{\left|\mathbf{z<em t="t">{t}^{\prime}-\operatorname{sg}\left(h\left(\mathbf{s}</em>\right)\right)\right|}^{\prime<em _Joint="{Joint" _text="\text" embedding="embedding" prediction="prediction">{2}^{2}}</em>}}+\underbrace{\mathrm{CE}\left(\hat{r<em t="t">{t}, r</em>}\right)<em t="t">{\text {Reward prediction }}+\underbrace{\mathrm{CE}\left(\hat{q}</em>\right)\right]
$$}, q_{t}\right)}_{\text {Value prediction }</p>
<p>where sg is the stop-grad operator, $\left(\mathbf{z}<em t="t">{t}^{\prime}, \hat{r}</em>}, \hat{q<em t="t">{t}\right)$ are defined in Equation 2, $q</em>} \doteq r_{t}+\gamma \bar{Q}\left(\mathbf{z<em t="t">{t}^{\prime}, p\left(\mathbf{z}</em>$ as soft targets (Bellemare et al., 2017; Kumar et al., 2023; Hafner et al., 2023).
Policy objective. The policy prior $p$ is a stochastic maximum entropy (Ziebart et al., 2008; Haarnoja et al., 2018) policy that learns to maximize the objective}^{\prime}\right)\right)$ is the TD-target at step $t, \lambda \in(0,1]$ is a constant coefficient that weighs temporally farther time steps less, and CE is the cross-entropy. $\bar{Q}$ used to compute the TD-target is an exponential moving average (EMA) of $Q$ (Lillicrap et al., 2016). As the magnitude of rewards may differ drastically between tasks, TD-MPC2 formulates reward and value prediction as a discrete regression (multiclass classification) problem in a log-transformed space, which is optimized by minimizing crossentropy with $r_{t}, q_{t</p>
<p>$$
\mathcal{L}<em 0:="0:" H="H">{p}(\theta) \doteq \underset{\left(\mathbf{s}, \mathbf{a}\right)</em>} \sim \mathcal{B}}{\mathbb{E}}\left[\sum_{t=0}^{H} \lambda^{t}\left[\alpha Q\left(\mathbf{z<em t="t">{t}, p\left(\mathbf{z}</em>}\right)\right)-\beta \mathcal{H}\left(p\left(\cdot \mid \mathbf{z<em t_1="t+1">{t}\right)\right)\right]\right], \mathbf{z}</em>}=d\left(\mathbf{z<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a}</em>}}\right), \mathbf{z<em 0="0">{0}=h\left(\mathbf{s}</em>\right)
$$</p>
<p>where $\mathcal{H}$ is the entropy of $p$ which can be computed in closed form. Gradients of $\mathcal{L}<em t="t">{p}(\theta)$ are taken wrt. $p$ only. As magnitude of the value estimate $Q\left(\mathbf{z}</em>$ can vary greatly between datasets and different stages of training, it is necessary to balance the two losses to prevent premature entropy collapse (Yarats et al., 2021). A common choice for automatically tuning $\alpha, \beta$ is to keep one of them constant, and adjusting the other based on an entropy target (Haarnoja et al., 2018) or moving statistics (Hafner et al., 2023). In practice, we opt for tuning $\alpha$ via moving statistics, but empirically did not observe any significant difference in results between these two options.
Architecture. All components of TD-MPC2 are implemented as MLPs with intermediate linear layers followed by LayerNorm (Ba et al., 2016) and Mish (Misra, 2019) activations. To mitigate exploding gradients, we normalize the latent representation by projecting $\mathbf{z}$ into $L$ fixed-dimensional simplices using a softmax operation (Lavoie et al., 2022). A key benefit of embedding $\mathbf{z}$ as simplices (as opposed to e.g. a discrete representation or squashing) is that it naturally biases the representation towards sparsity without enforcing hard constraints (see Appendix H for motivation and implementation). We dub this normalization scheme SimNorm. Let $V$ be the dimensionality of each simplex $\mathbf{g}$ constructed from $L$ partitions (groups) of $\mathbf{z}$. SimNorm then applies the following transformation:}, p\left(\mathbf{z}_{t}\right)\right)$ and entropy $\mathcal{H</p>
<p>$$
\mathbf{z}^{\circ} \doteq\left[\mathbf{g}<em L="L">{i}, \ldots, \mathbf{g}</em>}\right], \mathbf{g<em i:="i:" i_V="i+V">{i}=\frac{e^{\mathbf{z}</em>
$$} / \tau}}{\sum_{j=1}^{V} e^{\mathbf{z}_{i: i+V} / \tau}</p>
<p>where $\mathbf{z}^{\circ}$ is the simplicial embedding of $\mathbf{z},[\cdot]$ denotes concatenation, and $\tau&gt;0$ is a temperature parameter that modulates the "sparsity" of the representation. As we will demonstrate in our experiments, SimNorm is essential to the training stability of TD-MPC2. Finally, to reduce bias in TD-targets generated by $\bar{Q}$, we learn an ensemble of $Q$-functions using the objective from Equation 3 and maintain $\bar{Q}$ as an EMA of each $Q$-function. We use $5 Q$-functions in practice. Targets are then computed as the minimum of two randomly sub-sampled $\bar{Q}$-functions (Chen et al., 2021).</p>
<h1>3.2 Model Predictive Control with a Policy Prior</h1>
<p>TD-MPC2 derives its closed-loop control policy by planning with the learned world model. Specifically, our approach leverages the MPC framework for local trajectory optimization using Model Predictive Path Integral (MPPI) (Williams et al., 2015) as a derivative-free optimizer with sampled action sequences $\left(\mathbf{a}<em t_1="t+1">{t}, \mathbf{a}</em>\right)$ of length $H$ evaluated by rolling out latent trajectories with the model. At each decision step, we estimate parameters $\mu^{}, \ldots, \mathbf{a}_{t+H<em>}, \sigma^{</em>}$ of a time-dependent multivariate Gaussian with diagonal covariance such that expected return is maximized, i.e.,</p>
<p>$$
\mu^{<em>}, \sigma^{</em>}=\arg \max <em t="t">{(\mu, \sigma)} \underset{\left(\mathbf{a}</em>}, \mathbf{a<em t_H="t+H">{t+1}, \ldots, \mathbf{a}</em>}\right) \sim \mathcal{N}\left(\mu, \sigma^{2}\right)}{\mathbb{E}}\left[\gamma^{H} Q\left(\mathbf{z<em t_H="t+H">{t+H}, \mathbf{a}</em>}\right)+\sum_{h=t}^{H-1} \gamma^{h} R\left(\mathbf{z<em h="h">{h}, \mathbf{a}</em>\right)\right]
$$</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Single-task RL. Episode return (DMControl) and success rate (others) as a function of environment steps across $\mathbf{1 0 4}$ continuous control tasks spanning 4 diverse task domains. TD-MPC2 achieves higher data-efficiency and asymptotic performance than existing methods, while using the same hyperparameters across all tasks. Mean and $95 \%$ CIs over 3 seeds.
where $\mu, \sigma \in \mathbb{R}^{H \times m}, \mathcal{A} \in \mathbb{R}^{m}$. Equation 6 is solved by iteratively sampling action sequences from $\mathcal{N}\left(\mu, \sigma^{2}\right)$, evaluating their expected return, and updating $\mu, \sigma$ based on a weighted average. Notably, Equation 6 estimates the full RL objective introduced in Section 2 by bootstrapping with the learned terminal value function beyond horizon $H$. TD-MPC2 repeats this iterative planning process for a fixed number of iterations and executes the first action $\mathbf{a}<em t="t">{t} \sim \mathcal{N}\left(\mu</em>^{<em>}, \sigma_{t}^{</em>}\right)$ in the environment. To accelerate convergence of planning, a fraction of action sequences originate from the policy prior $p$, and we warm-start planning by initializing $(\mu, \sigma)$ as the solution to the previous decision step shifted by 1. Refer to Hansen et al. (2022) for more details about the planning procedure.</p>
<h1>3.3 Training Generalist TD-MPC2 Agents</h1>
<p>The success of TD-MPC2 in diverse single-task problems can be attributed to the algorithm outlined above. However, learning a large generalist TD-MPC2 agent that performs a variety of tasks across multiple task domains, embodiments, and action spaces poses several unique challenges: (i) how to learn and represent task semantics? (ii) how to accommodate multiple observation and action spaces without specific domain knowledge? (iii) how to leverage the learned model for few-shot learning of new tasks? We describe our approach to multitask model learning in the following.
Learnable task embeddings. To succeed in a multitask setting, an agent needs to learn a common representation that takes advantage of task similarities, while still retaining the ability to differentiate between tasks at test-time. When task or domain knowledge is available, e.g. in the form of natural language instructions, the task embedding e from Equation 2 may encode such information. However, in the general case where domain knowledge cannot be assumed, we may instead choose to learn the task embeddings (and, implicitly, task relations) from data. TD-MPC2 conditions all of its five components with a learnable, fixed-dimensional task embedding e, which is jointly trained together with other components of the model. To improve training stability, we constrain the $\ell_{2}$-norm of e to be $\leq 1$; this also leads to more semantically coherent task embeddings in our experiments. When finetuning a multitask TD-MPC2 agent to a new task, we can choose to either initialize e as the embedding of a semantically similar task, or simply as a random vector.
Action masking. TD-MPC2 learns to perform tasks with a variety of observation and action spaces, without any domain knowledge. To do so, we zero-pad all model inputs and outputs to their largest respective dimensions, and mask out invalid action dimensions in predictions made by the policy prior $p$ during both training and inference. This ensures that prediction errors in invalid dimensions do not influence TD-target estimation, and prevents $p$ from falsely inflating its entropy for tasks with small action spaces. We similarly only sample actions along valid dimensions during planning.</p>
<h2>4 EXPERIMENTS</h2>
<p>We evaluate TD-MPC2 across a total of $\mathbf{1 0 4}$ diverse continuous control tasks spanning 4 task domains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al., 2022). Tasks include high-dimensional state and action spaces (up to $\mathcal{A} \in \mathbb{R}^{39}$ ), sparse rewards, multi-object manipulation, physiologically accurate musculoskeletal motor control, complex locomotion (e.g. Dog and Humanoid embodiments), and cover a wide range of task difficulties. We also include $\mathbf{1 0}$ DMControl tasks with visual observations. In support of open-source science, we publicly release 300+ model checkpoints, datasets, and code for training and evaluating TD-MPC2 agents, which is available at https://t dmpc2 . com.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. High-dimensional locomotion. Episode return as a function of environment steps in Humanoid ( $\mathcal{A} \in \mathbb{R}^{21}$ ) and Dog ( $\mathcal{A} \in \mathbb{R}^{38}$ ) locomotion tasks from DMControl. SAC and DreamerV3 are prone to numerical instabilities in Dog tasks, and are significantly less data-efficient than TDMPC2 in Humanoid tasks. Mean and $95 \%$ CIs over 3 seeds. See Appendix D for more tasks.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Object manipulation. Success rate (\%) as a function of environment steps on 5 object manipulation tasks from ManiSkill2. Pick YCB considers manipulation of all 74 objects from the YCB (Calli et al., 2015) dataset. TD-MPC2 excels at hard tasks. Mean and $95 \%$ CIs over 3 seeds.</p>
<p>We seek to answer three core research questions through experimentation:</p>
<ul>
<li>Comparison to existing methods. How does TD-MPC2 compare to state-of-the-art model-free (SAC) and model-based (DreamerV3, TD-MPC) methods for data-efficient continuous control?</li>
<li>Scaling. Do the algorithmic innovations of TD-MPC2 lead to improved agent capabilities as model and data size increases? Can a single agent learn to perform diverse skills across multiple task domains, embodiments, and action spaces?</li>
<li>Analysis. How do the specific design choices introduced in TD-MPC2 influence downstream task performance? How much does planning contribute to its success? Are the learned task embeddings semantically meaningful? Can large multi-task agents be adapted to unseen tasks?</li>
</ul>
<p>Baselines. Our primary baselines represent the state-of-the-art in data-efficient RL, and include (1) Soft Actor-Critic (SAC) (Haarnoja et al., 2018), a model-free actor-critic algorithm based on maximum entropy RL, (2) DreamerV3 (Hafner et al., 2023), a model-based method that optimizes a model-free policy with rollouts from a learned generative model of the environment, and (3) the original version of TD-MPC (Hansen et al., 2022), a model-based RL algorithm that performs local trajectory optimization (planning) in the latent space of a learned implicit (non-generative) world model. Additionally, we also compare against current state-of-the-art visual RL methods (4) CURL (Srinivas et al., 2020), an extension of SAC that uses a contrastive auxiliary objective, and (5) DrQv2, a model-free RL algorithm that uses data augmentation. SAC and TD-MPC use task-specific hyperparameters, whereas TD-MPC2 uses the same hyperparameters across all tasks. Additionally, it is worth noting that both SAC and TD-MPC use a larger batch size of 512 , while 256 is sufficient for stable learning with TD-MPC2. Similarly, DreamerV3 uses a high update-to-data (UTD) ratio of 512 , whereas TD-MPC2 uses a UTD of 1 by default. We use a 5 M parameter TD-MPC2 agent in all experiments (unless stated otherwise). For reference, the DreamerV3 baseline has approx. 20M learnable parameters. See Appendix H for more details.</p>
<h1>4.1 RESULTS</h1>
<p>Comparison to existing methods. We first compare the data-efficiency of TD-MPC2 to a set of strong baselines on 104 diverse tasks in an online RL setting. Aggregate results are shown in Figure 4. We find that TD-MPC2 outperforms prior methods across all task domains. The MyoSuite results are particularly noteworthy, as we did not run any TD-MPC2 experiments on this benchmark prior to the reported results. Individual task performances on some of the most difficult tasks (high-dimensional locomotion and multi-object manipulation) are shown in Figure 5 and Figure 6. TD-MPC2 outperforms baselines by a large margin on these tasks, despite using the same hyperparameters across all tasks. Notably, TD-MPC sometimes diverges due to exploding gradients,</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Massively multi-task world models. (Left) Normalized score as a function of model size on the two 80 -task and 30 -task datasets. TD-MPC2 capabilities scale with model size. (Right) T-SNE (van der Maaten \&amp; Hinton, 2008) visualization of task embeddings learned by a TD-MPC2 agent trained on 80 tasks from DMControl and Meta-World. A subset of labels are shown for clarity.
whereas TD-MPC2 remains stable. We provide per-task visualization of gradients in Appendix G. Similarly, we observe that DreamerV3 experiences occasional numerical instabilities (Dog) and generally struggles with tasks that require fine-grained object manipulation (lift, pick, stack). See Appendix D for the full single-task RL results.</p>
<p>Massively multitask world models. To demonstrate that our proposed improvements facilitate scaling of world models, we evaluate the performance of 5 multitask models ranging from 1 M to 317 M parameters on a collection of 80 diverse tasks that span multiple task domains and vary greatly in objective, embodiment, and action space. Models are trained on a dataset of 545 M transitions obtained from the replay buffers of 240 single-task TD-MPC2 agents, and thus contain a wide variety of behaviors ranging from random to expert policies. The task set consists of all 50 Meta-World tasks, as well as 30 DMControl tasks. The DMControl task set includes 19 original DMControl tasks, as well as 11 new tasks. For completeness, we include a separate set of scaling results on the 30 -task DMControl subset ( 345 M transitions) as well. Due to our careful design of the TD-MPC2 algorithm, scaling up is straightforward: to improve rate of convergence we use a $4 \times$ larger batch size (1024) compared to the single-task experiments, but make no other changes to hyperparameters.</p>
<p>Scaling TD-MPC2 to 317M parameters. Our scaling results are shown in Figure 7. To summarize agent performance with a single metric, we produce a normalized score that is an average of all individual task success rates (Meta-World) and episode returns normalized to the $[0,100]$ range (DMControl). We observe that agent capabilities consistently increase with model size on both task sets. Notably, performance does not appear to have saturated for our largest models (317M parameters) on either dataset, and we can thus expect results to continue improving beyond our considered model sizes. We refrain from formulating a scaling law, but note that normalized score appears to scale linearly with the log of model parameters (gray line in Figure 7). We also report approximate training costs in Table 1. The 317M parameter model can be trained with limited computational resources. To better understand why multitask model learning is successful, we explore the task embeddings learned by TD-MPC2 (Figure 7, right). Intriguingly, tasks that are semantically similar (e.g., Door Open and Door Close) are close in the learned task embedding space. However, embedding similarity appears to align more closely with task dynamics (embodiment, objects) than objective (walk, run). This makes intuitive sense, as dynamics are tightly coupled with control.</p>
<p>Few-shot learning. While our work mainly focuses on the scaling and robustness of world models, we also explore the efficacy of finetuning pretrained world models for few-shot learning of unseen tasks. Specifically, we pretrain a 19M parameter TD-MPC2 agent on 70 tasks from DMControl and</p>
<p>Table 1. Training cost. Approximate TD-MPC2 training cost on the 80 -task dataset, reported in GPU days on a single NVIDIA GeForce RTX 3090 GPU. We also list the normalized score achieved by each model at end of training.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Params (M)</th>
<th style="text-align: center;">GPU days</th>
<th style="text-align: center;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">16.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{3 1 7}$</td>
<td style="text-align: center;">$\mathbf{3 3}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 6}$</td>
</tr>
</tbody>
</table>
<p>Figure 8. Finetuning. Score of a 19M parameter TD-MPC2 agent trained on 70 tasks and finetuned online to each of 10 heldout tasks for 20 k environment steps. 3 seeds.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9. Ablations. (Curves) Normalized score as a function of environment steps, averaged across three of the most difficult tasks: Dog Run, Humanoid Walk (DMControl), and Pick YCB (ManiSkill2). Mean and $95 \%$ CIs over 3 random seeds. (Bars) Normalized score of 19M parameter multitask ( 80 tasks) TD-MPC2 agents. Our ablations highlight the relative importance of each design choice; red is the default formulation of TD-MPC2. See Appendix D for more ablations.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10. Visual RL. Episode return as a function of environment steps on 10 image-based DMControl tasks. Mean and $95 \%$ CIs over 3 random seeds. TD-MPC2 is comparable to state-of-the-art.</p>
<p>Meta-World, and naïvely finetune the full model to each of 10 held-out tasks ( 5 from each domain) via online RL with an initially empty replay buffer and no changes to hyperparameters. Aggregate results are shown in Figure 8. We find that TD-MPC2 improves $\mathbf{2} \times$ over learning from scratch on new tasks in the low-data regime ( 20 k environment steps $^{1}$ ). Although finetuning world models to new tasks is very much an open research problem, our exploratory results are promising. See Appendix E for experiment details and individual task curves.</p>
<p>Ablations. We ablate most of our design choices for TD-MPC2, including choice of actor, various normalization techniques, regression objective, and number of $Q$-functions. Our main ablations, shown in Figure 9, are conducted on three of the most difficult online RL tasks, as well as largescale multitask training ( 80 tasks). We observe that all of our proposed improvements contribute meaningfully to the robustness and strong performance of TD-MPC2 in both single-task RL and multi-task RL. Interestingly, we find that the relative importance of each design choice is consistent across both settings. Lastly, we also ablate normalization of the learned task embeddings, shown in Appendix F. The results indicate that maintaining a normalized task embedding space ( $\ell_{2}$-norm of 1) is moderately important for stable multitask training, and results in more meaningful task relations.</p>
<p>Visual RL. We mainly consider high-dimensional continuous control tasks with proprioceptive state observations in this work. However, TD-MPC2 can be readily applied to tasks with other input</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>modalities as well. To demonstrate this, we replace the encoder of TD-MPC2 with a shallow convolutional encoder, and benchmark it against current state-of-the-art methods for visual RL on 10 DMControl tasks of varying difficulty. Results are shown in Figure 10. TD-MPC2 performs comparably to the two best baselines, DrQ-v2 and DreamerV3, without any changes to hyperparameters.</p>
<h1>5 Lessons, Opportunities, and Risks</h1>
<p>Lessons. Historically, RL algorithms have been notoriously sensitive to architecture, hyperparameters, characteristics of the task, and even random seed (Henderson et al., 2018), with no principled method for tuning the algorithms. As a result, successful application of deep RL often requires large teams of experts with significant computational resources (Berner et al., 2019; Schrittwieser et al., 2020; Ouyang et al., 2022). TD-MPC2 - along with several other contemporary RL methods (Yarats et al., 2021; Ye et al., 2021; Hafner et al., 2023) - seek to democratize use of RL (i.e., lowering the barrier of entry for smaller teams of academics, practitioners, and individuals with fewer resources) by improving robustness of existing open-source algorithms. We firmly believe that improving algorithmic robustness will continue to have profound impact on the field. A key lesson from the development of TD-MPC2 is that the community has yet to discover an algorithm that truly masters everything out-of-the-box. While e.g. DreamerV3 (Hafner et al., 2023) has delivered strong results on challenging tasks with discrete action spaces (such as Atari games and Minecraft), we find that TD-MPC2 produces significantly better results on difficult continuous control tasks. At the same time, extending TD-MPC2 to discrete action spaces remains an open problem.</p>
<p>Opportunities. Our scaling results demonstrate a path for model-based RL in which massively multitask world models are leveraged as generalist world models. While multi-task world models remain relatively underexplored in literature, prior work suggests that the implicit world model of TD-MPC2 may be better suited than reconstruction-based approaches for tasks with large visual variation (Zhu et al., 2023). We envision a future in which implicit world models are used zero-shot to perform diverse tasks on seen embodiments (Xu et al., 2023; Yang et al., 2023), finetuned to quickly perform tasks on new embodiments, and combined with existing vision-language models to perform higher-level cognitive tasks in conjunction with low-level physical interaction. Our results are promising, but such level of generalization will likely require several orders of magnitude more tasks than currently available. Lastly, we want to remark that, while TD-MPC2 relies on rewards for task learning, it is useful to adopt a generalized notion of reward as simply a metric for task completion. Such metrics already exist in the wild, e.g., success labels, human preferences or interventions (Ouyang et al., 2022), or the embedding distance between a current observation and a goal (Eysenbach et al., 2022; Ma et al., 2022) within a pre-existing learned representation. However, leveraging such rewards for large-scale pretraining is an open problem. To accelerate research in this area, we are releasing 300+ TD-MPC2 models, including 12 multitask models, as well as datasets and code, and we are beyond excited to see what the community will do with these resources.</p>
<p>Risks. While we are excited by the potential of generalist world models, several challenges remain: (i) misspecification of task rewards can lead to unintended outcomes (Clark \&amp; Amodei, 2016) that may be difficult to anticipate, (ii) handing over unconstrained autonomy of physical robots to a learned model can result in catastrophic failures if no additional safety checks are in place (Lancaster et al., 2023), and (iii) data for certain applications may be prohibitively expensive for small teams to obtain at the scale required for generalist behavior to emerge, leading to a concentration of power. Mitigating each of these challenges will require new research innovations, and we invite the community to join us in these efforts.</p>
<h2>6 Related Work</h2>
<p>Multiple prior works have sought to build RL algorithms that are robust to hyperparameters, architecture, as well as variation in tasks and data. For example, (1) Double $Q$-learning (Hasselt et al., 2016), RED-Q (Chen et al., 2021), SVEA (Hansen et al., 2021), and SR-SPR (D’Oro et al., 2023) each improve the stability of $Q$-learning algorithms by adjusting the bias-variance trade-off in TDtarget estimation, (2) C51 (Bellemare et al., 2017) and DreamerV3 (Hafner et al., 2023) improve robustness to the magnitude of rewards by performing discrete regression in a transformed space, and (3) model-free algorithms DrQ (Kostrikov et al., 2020) and DrQ-v2 (Yarats et al., 2021) improve training stability and exploration, respectively, through use of data augmentation and several other</p>
<p>minor but important implementation details. However, all of the aforementioned works strictly focus on improving data-efficiency and robustness in single-task online RL.</p>
<p>Existing literature that studies scaling of neural architectures for decision-making typically assume access to large datasets of near-expert demonstrations for behavior cloning (Reed et al., 2022; Lee et al., 2022; Kumar et al., 2022; Schubert et al., 2023; Driess et al., 2023; Brohan et al., 2023). Gato (Reed et al., 2022) learns to perform tasks across multiple domains by training a large Transformerbased sequence model (Vaswani et al., 2017) on an enormous dataset of expert demonstrations, and RT-1 (Brohan et al., 2023) similarly learns a sequence model for object manipulation on a single (real) robot embodiment by training on a large dataset collected by human teleoperation. While the empirical results of this line of work are impressive, the assumption of large demonstration datasets is impractical. Additionally, current sequence models rely on discretization of the action space (tokenization), which makes scaling to high-dimensional continuous control tasks difficult.</p>
<p>Most recently, researchers have explored scaling of RL algorithms as a solution to the aforementioned challenges (Baker et al., 2022; Jia et al., 2022; Xu et al., 2023; Kumar et al., 2023; Hafner et al., 2023). For example, VPT (Baker et al., 2022) learns to play Minecraft by first pretraining a behavior cloning policy on a large human play dataset, and then finetuning the policy with RL. GSL (Jia et al., 2022) requires no pre-existing data. Instead, GSL iteratively trains a population of "specialist" agents on individual task variations, distills them into a "generalist" policy via behavior cloning, and then uses the generalist as initialization for the next population of specialists. However, this work considers strictly single-task RL and assumes full control over the initial state in each episode. Lastly, DreamerV3 (Hafner et al., 2023) successfully scales its world model in terms of parameters and shows that larger models generally are more data-efficient in an online RL setting, but does not consider multitask RL.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>This project was supported, in part, by grants from NSF CAREER Award (2240160), NSF TILOS AI Institute (2112665), NSF CCF-2112665 (TILOS), NSF 1730158 CI-New: Cognitive Hardware and Software Ecosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349 CC*DNI Pacific Research Platform, and gifts from Qualcomm.</p>
<h2>REFERENCES</h2>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. Advances in Neural Information Processing Systems, 2016.</p>
<p>Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639-24654, 2022.</p>
<p>Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pp. 449-458. PMLR, 2017.</p>
<p>Richard Bellman. A markovian decision process. Indiana Univ. Math. J., 6:679-684, 1957. ISSN 0022-2518.</p>
<p>Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.</p>
<p>Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901, 2020.</p>
<p>Vittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, and Vikash Kumar. Myosuite - a contact-rich simulation suite for musculoskeletal motor control, 2022.</p>
<p>Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M. Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 International Conference on Advanced Robotics, pp. 510-517, 2015. doi: 10.1109/ICAR.2015. 7251504 .</p>
<p>Xinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. Randomized ensembled double q-learning: Learning fast without a model. International Conference on Learning Representations, 2021.</p>
<p>Rohan Chitnis, Yingchen Xu, Bobak Hashemi, Lucas Lehnert, Urun Dogan, Zheqing Zhu, and Olivier Delalleau. Iql-td-mpc: Implicit q-learning for hierarchical model predictive control. arXiv preprint arXiv:2306.00867, 2023.</p>
<p>Jack Clark and Dario Amodei. Faulty reward functions in the wild. OpenAI Blog, 2016.
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.</p>
<p>Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In H. Jaap van den Herik, Paolo Ciancarini, and H. H. L. M. (Jeroen) Donkers (eds.), Computers and Games, pp. 72-83, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.</p>
<p>Pierluca D’Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G Bellemare, and Aaron Courville. Sample-efficient reinforcement learning by breaking the replay ratio barrier. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learning as goal-conditioned reinforcement learning. Advances in Neural Information Processing Systems, 35:35603-35620, 2022.</p>
<p>Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, and Xiaolong Wang. Finetuning offline world models in the real world. Conference on Robot Learning, 2023.</p>
<p>Jean-Bastien Grill, Florian Strub, Florent Altch'e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. Advances in Neural Information Processing Systems, 2020.</p>
<p>Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiaing Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, and Hao Su. Maniskill2: A unified benchmark for generalizable manipulation skills. In International Conference on Learning Representations, 2023.</p>
<p>Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, G. Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, P. Abbeel, and Sergey Levine. Soft actor-critic algorithms and applications. ArXiv, abs/1812.05905, 2018.</p>
<p>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.</p>
<p>Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision transformers under data augmentation. In Annual Conference on Neural Information Processing Systems, 2021.</p>
<p>Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control. In ICML, 2022.</p>
<p>Nicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash Kumar, and Aravind Rajeswaran. Modem: Accelerating visual model-based reinforcement learning with demonstrations. 2023.
H. V. Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In Aaai, 2016.</p>
<p>Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000-16009, 2022.</p>
<p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'18/IAAI'18/EAAI'18. AAAI Press, 2018. ISBN 978-1-57735-800-8.</p>
<p>Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces, 2021.</p>
<p>Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, and Hao Su. Improving Policy Optimization with Generalist-Specialist Learning. In International Conference on Machine Learning, 2022.</p>
<p>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.</p>
<p>Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. International Conference on Learning Representations, 2020.</p>
<p>Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. Should i run offline reinforcement learning or behavioral cloning? International Conference on Learning Representations, 2022.</p>
<p>Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline qlearning on diverse multi-task data both scales and generalizes. International Conference on Learning Representations, 2023.</p>
<p>Nathan Lambert, Brandon Amos, Omry Yadan, and Roberto Calandra. Objective mismatch in model-based reinforcement learning. Conference on Learning for Decision and Control, 2020.</p>
<p>Patrick Lancaster, Nicklas Hansen, Aravind Rajeswaran, and Vikash Kumar. Modem-v2: Visuomotor world models for real-world robot manipulation. arXiv preprint, 2023.</p>
<p>Samuel Lavoie, Christos Tsirigotis, Max Schwarzer, Ankit Vani, Michael Noukhovitch, Kenji Kawaguchi, and Aaron Courville. Simplicial embeddings in self-supervised learning and downstream classification. arXiv preprint arXiv:2204.00616, 2022.</p>
<p>Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. Advances in Neural Information Processing Systems, 35:27921-27936, 2022.
T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2016.</p>
<p>Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022.</p>
<p>Diganta Misra. Mish: A self regularized non-monotonic neural activation function. arXiv preprint arXiv:1908.08681, 2019.</p>
<p>Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online finetuning, 2023.</p>
<p>Rudy R. Negenborn, Bart De Schutter, Marco A. Wiering, and Hans Hellendoorn. Learning-based model predictive control for markov decision processes. IFAC Proceedings Volumes, 38(1):354359, 2005. 16th IFAC World Congress.</p>
<p>Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. arXiv preprint arXiv:1711.00937, 2017.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>Ingmar Schubert, Jingwei Zhang, Jake Bruce, Sarah Bechtle, Emilio Parisotto, Martin Riedmiller, Jost Tobias Springenberg, Arunkumar Byravan, Leonard Hasenclever, and Nicolas Heess. A generalist dynamics model for control. arXiv preprint arXiv:2305.10912, 2023.</p>
<p>Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):1929-1958, 2014.
R. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9-44, 1998.</p>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, et al. Deepmind control suite. Technical report, DeepMind, 2018.</p>
<p>Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579-2605, 2008.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model predictive path integral control using covariance variable importance sampling. ArXiv, abs/1509.01149, 2015.</p>
<p>Yifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan, Hao Su, and Zhuowen Tu. On the feasibility of cross-task transfer with model-based reinforcement learning. 2023.</p>
<p>Sizhe Yang, Yanjie Ze, and Huazhe Xu. Movie: Visual model-based policy adaptation for view generalization. Advances in Neural Information Processing Systems, 2023.</p>
<p>Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. International Conference on Learning Representations, 2021.</p>
<p>Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in Neural Information Processing Systems, 34:25476-25488, 2021.</p>
<p>Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, 2019.</p>
<p>Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing Hu, Jinyi Liu, Yingfeng Chen, and Changjie Fan. Euclid: Towards efficient unsupervised reinforcement learning with multi-choice dynamics model. arXiv preprint arXiv:2210.00498, 2022.</p>
<p>Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, André Biedenkapp, Kurtland Chua, Frank Hutter, and Roberto Calandra. On the importance of hyperparameter optimization for model-based reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp. 4015-4023. PMLR, 2021.</p>
<p>Chuning Zhu, Max Simchowitz, Siri Gadipudi, and Abhishek Gupta. Repo: Resilient model-based reinforcement learning by regularizing posterior predictability. arXiv preprint arXiv:2309.00082, 2023.</p>
<p>Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd National Conference on Artificial Intelligence, volume 3, 2008.</p>
<h1>APPENDICES</h1>
<p>A Summary of Improvements ..... 16
B Task Visualizations ..... 17
C Task Domains ..... 18
D Single-task Experimental Results ..... 21
E Few-shot Experimental Results ..... 24
F Additional Ablations ..... 25
G Gradient Norm and Training Stability ..... 26
H Implementation Details ..... 26
I Extending TD-MPC2 to Discrete Action Spaces ..... 29
J Test-Time Regularization for Offline RL ..... 30
K Additional Multi-task Results ..... 31</p>
<h1>A SUMMARY OF IMPROVEMENTS</h1>
<p>We summarize the main differences between TD-MPC and TD-MPC2 as follows:</p>
<ul>
<li>Architectural design. All components of TD-MPC2 are MLPs with LayerNorm (Ba et al., 2016) and Mish (Misra, 2019) activations after each layer. We apply SimNorm normalization to the latent state $\mathbf{z}$ which biases the representation towards sparsity and maintaining a small $\ell_{2}$-norm. We train an ensemble of $Q$-functions ( 5 by default) and additionally apply $1 \%$ Dropout (Srivastava et al., 2014) after the first linear layer in each $Q$-function. TD-targets are computed as the mininum of two randomly subsampled $Q$-functions (Chen et al., 2021). In contrast, TD-MPC is implemented as MLPs without LayerNorm, and instead uses ELU (Clevert et al., 2015) activations. TD-MPC does not constrain the latent state at all, which in some instances leads to exploding gradients (see Appendix G for experimental results). Lastly, TD-MPC learns only $2 Q$-functions and does not use Dropout. The architectural differences in TD-MPC2 result in a 4 M net increase in learnable parameters ( 5 M total) for our default single-task model size compared to the 1 M parameters of TD-MPC. However, as shown in Figure 7, naïvely increasing the model size of TD-MPC does not lead to consistently better performance, whereas it does for TD-MPC2.</li>
<li>Policy prior. The policy prior of TD-MPC2 is trained with maximum entropy RL (Ziebart et al., 2008; Haarnoja et al., 2018), whereas the policy prior of TD-MPC is trained as a deterministic policy with Gaussian noise applied to actions. We find that a carefully tuned Gaussian noise schedule is comparable to a policy prior trained with maximum entropy. However, maximum entropy RL can more easily be applied with task-agnostic hyperparameters. We only compute policy entropy over valid action dimensions in multi-task learning with multiple action spaces.</li>
<li>Planning. The planning procedure of TD-MPC2 closely follows that of TD-MPC. However, we simplify planning marginally by not leveraging momentum between iteration, as we find it to produce comparable results. We also improve the throughput of planning by approx. $2 \times$ through a series of code-level optimizations.</li>
<li>Model objective. We revisit the training objective of TD-MPC and improve its robustness to variation in tasks, such as the magnitude of rewards. TD-MPC2 uses discrete regression (soft cross-entropy) of rewards and values in a log-transformed space, which makes the magnitude of the two loss terms independent of the magnitude of the task rewards. TDMPC uses continuous regression which leads to training instabilities in tasks where rewards are large. While this issue can be alleviated by, e.g., normalizing task rewards based on moving statistics, in the single-task case, it is difficult to design robust reward normalization schemes for multi-task learning. TD-MPC2 retains the continuous regression term for jointembedding prediction as the latent representation is already normalized by SimNorm, and discrete regression is computationally expensive for high-dimensional spaces (requires $N$ bins for each dimension of $\mathbf{z}$ ).</li>
<li>Multi-task model. TD-MPC2 introduces a framework for learning multi-task world models across multiple domains, embodiments, and action spaces. We introduce a normalized learnable task embedding space which all components of TD-MPC are conditioned on, and we accommodate multiple observation and action spaces by applying zero-padding and action masking during both training and inference. We train multi-task models on a large number of tasks, and finetune the model to held-out tasks (across embodiments) using online RL. TD-MPC only considers multi-task learning on a small number of tasks with shared observation and action space, and does not consider finetuning of the learned multi-task model.</li>
<li>Simplified algorithm and implementation. TD-MPC2 removes momentum in MPPI (Williams et al., 2015), and replaces prioritized experience replay sampling from the replay buffer with uniform sampling, both of which simplify the implementation with no significant change in experimental results. Finally, we also use a faster replay buffer implementation that uses multiple workers for sampling, and we increase training and planning throughput through code-level optimizations such as $Q$-function ensemble vectorization, which makes the wall-time of TD-MPC2 comparable to that of TD-MPC despite a larger architecture ( 5 M vs. 1 M ).</li>
</ul>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11. Task visualizations. Visualization of a random initial state for each of the 104 tasks that we consider. Tasks vary greatly in objective, embodiment, and action space. Visit https: //tdmpc2.com for videos of TD-MPC2 performing each task. See Appendix C for task details.</p>
<h1>C Task Domains</h1>
<p>We consider a total of 104 continuous control tasks from 4 task domains: DMControl (Tassa et al., 2018), Meta-World (Yu et al., 2019), ManiSkill2 (Gu et al., 2023), and MyoSuite (Caggiano et al., 2022). This section provides an exhaustive list of all tasks considered, as well as their observation and action dimensions. Environment details are listed at the end of the section. We provide (static) task visualizations in Appendix B and videos of TD-MPC2 agents performing each task at https: //www.tdmpc2.com.</p>
<p>Table 2. DMControl. We consider a total of 39 continuous control tasks in the DMControl domain, including 19 original DMControl tasks and 11 new (custom) tasks created specifically for TD-MPC2 benchmarking and multitask training. We list all considered DMControl tasks below. The Locomotion task set shown in Figure 1 corresponds to the Humanoid and Dog embodiments of DMControl, with performance reported at 14 M environment steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Observation dim</th>
<th style="text-align: center;">Action dim</th>
<th style="text-align: center;">Sparse?</th>
<th style="text-align: center;">New?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Acrobot Swingup</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Cartpole Balance</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Cartpole Balance Sparse</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Cartpole Swingup</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Cartpole Swingup Sparse</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Cheetah Jump</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Cheetah Run</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Cheetah Run Back</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Cheetah Run Backwards</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Cheetah Run Front</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Cup Catch</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Cup Spin</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Dog Run</td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Dog Trot</td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Dog Stand</td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Dog Walk</td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Finger Spin</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Finger Turn Easy</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Finger Turn Hard</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Fish Swim</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Hopper Hop</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Hopper Hop Backwards</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Hopper Stand</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Humanoid Run</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Humanoid Stand</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Humanoid Walk</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Pendulum Spin</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Pendulum Swingup</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Quadruped Run</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Quadruped Walk</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Reacher Easy</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Reacher Hard</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Reacher Three Easy</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Reacher Three Hard</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Walker Run</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Walker Run Backwards</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
<tr>
<td style="text-align: left;">Walker Stand</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Walker Walk</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: left;">Walker Walk Backwards</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">Y</td>
</tr>
</tbody>
</table>
<p>Table 3. Meta-World. We consider a total of 50 continuous control tasks from the Meta-World domain. The Meta-World benchmark is designed for multitask and meta-learning research and all tasks thus share embodiment, observation space, and action space.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Observation dim</th>
<th style="text-align: center;">Action dim</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Assembly</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Basketball</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Bin Picking</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">$\ldots$</td>
<td style="text-align: center;">$\ldots$</td>
<td style="text-align: center;">$\ldots$</td>
</tr>
<tr>
<td style="text-align: left;">Window Open</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 4. ManiSkill2. We consider a total of 5 continuous control tasks from the ManiSkill2 domain. The ManiSkill2 benchmark is designed for large-scale robot learning and contains a high degree of randomization and task variations. The Pick YCB task shown in Figure 1 corresponds to the ManiSkill2 task of the same name, with performance reported at 14 M environment steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Observation dim</th>
<th style="text-align: center;">Action dim</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lift Cube</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Pick Cube</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Pick YCB</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">Stack Cube</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Turn Faucet</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">7</td>
</tr>
</tbody>
</table>
<p>Table 5. MyoSuite. We consider a total of 10 continuous control tasks from the MyoSuite domain. The MyoSuite benchmark is designed for high-dimensional physiologically accurate muscoloskeletal motor control and involves particularly complex object manipulation with a dexterous hand. The MyoSuite domain consists of tasks with and without goal randomization. We consider both settings, and refer to them as Easy (fixed goal) and Hard (random goal), respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Observation dim</th>
<th style="text-align: center;">Action dim</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reach Easy</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Reach Hard</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Pose Easy</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Pose Hard</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Pen Twirl Easy</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Pen Twirl Hard</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Object Hold Easy</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Object Hold Hard</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Key Turn Easy</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: left;">Key Turn Hard</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">39</td>
</tr>
</tbody>
</table>
<p>Environment details. We benchmark algorithms on DMControl, Meta-World, ManiSkill2, and MyoSuite without modification. All four domains are infinite-horizon continuous control environments for which we use a fixed episode length and no termination conditions. We list episode lengths, action repeats, total number of environment steps, and the performance metric used for each domain in Table 6. In all experiments, we only consider an episode successful if the final step of an episode is successful. This is a stricter definition of success than used in some of the related literature, which e.g. may consider an episode successful if success is achieved at any step within a given episode. In tasks that require manipulation of objects, such as picking up an object, our definition of success ensures that an episode in which an object is picked up but then dropped again is not considered successful.</p>
<p>Table 6. Environment details. We list the episode length and action repeat used for each task domain, as well as the total number of environment steps and performance metrics that we use for benchmarking methods. All methods use the same values for all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">DMControl</th>
<th style="text-align: center;">Meta-World</th>
<th style="text-align: center;">ManiSkill2</th>
<th style="text-align: center;">MyoSuite</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Episode length</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Action repeat</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Effective length</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Total env. steps</td>
<td style="text-align: center;">$4 \mathrm{M}-14 \mathrm{M}$</td>
<td style="text-align: center;">2 M</td>
<td style="text-align: center;">$4 \mathrm{M}-14 \mathrm{M}$</td>
<td style="text-align: center;">2 M</td>
</tr>
<tr>
<td style="text-align: left;">Performance metric</td>
<td style="text-align: center;">Reward</td>
<td style="text-align: center;">Success</td>
<td style="text-align: center;">Success</td>
<td style="text-align: center;">Success</td>
</tr>
</tbody>
</table>
<ul>
<li>Appendices continue on next page -</li>
</ul>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12. Single-task DMControl results. Episode return as a function of environment steps. The first 4 M environment steps are shown for each task, although the Humanoid and Dog tasks are run for 14 M environment steps; we provide those curves in Figure 15 as part of the "Locomotion" benchmark. Note that TD-MPC diverges on tasks like Walker Stand and Walker Walk whereas TDMPC2 remains stable. We visualize gradients on these tasks in Appendix G. Mean and $95 \%$ CIs over 3 seeds.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1} 20 \mathrm{k}$ environment steps corresponds to 20 episodes in DMControl and 100 episodes in Meta-World.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4757 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4757</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4757</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-270370750</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.06461v3.pdf" target="_blank">Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies</a></p>
                <p><strong>Paper Abstract:</strong> A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don’t surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4757.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4757.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT+SC (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (using GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Independent parallel sampling of Chain-of-Thought (CoT) answers followed by majority vote (self-consistency); increases answer diversity via independent draws and aggregates to improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (turbo-0301 / 0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (decoder-only transformer, used with temperature=1 in experiments reported; model id 0301 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT+SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate many independent chain-of-thought samples (parallel sampling) for each problem, then aggregate answers via majority vote; diversity arises from independent sampling (stochastic decoding).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24 (100-sample subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math reasoning (GSM8K, MATH, TheoremQA), commonsense QA (CSQA), multi-hop QA (HotpotQA), and the Game of 24 puzzle; each experiment used 100 randomly sampled test instances per dataset (Game of 24 has 100 instances total).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: CoT+SC with matched budget consistently outperforms Multi-Agent Debate and Reflexion across most datasets in the paper's budget-aware comparisons; performance increases smoothly with increased query/token budget. (Paper reports CoT+SC as the dominant baseline for GPT-3.5 under equal token/query budgets.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Multi-Agent Debate (MAD) and Reflexion performed worse than CoT+SC under equivalent budgets; Tree-of-Thoughts under GPT-3.5 underperforms CoT+SC on Game of 24.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Independent (diverse) sampling via CoT+SC is highly budget-efficient: when given the same token/query budget as more complex methods, CoT+SC often matches or exceeds their performance; diversity from independent sampling is a primary driver of CoT+SC's gains and its performance increases smoothly with budget.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For problem subsets where the model's per-sample correctness probability p_i < 0.5, majority voting (SC) can reduce performance (increasing samples can worsen results). On some tasks (e.g., HotpotQA in the study) CoT+SC did not clearly dominate; Tree-of-Thoughts with a stronger model (GPT-4) can outperform CoT+SC on Game of 24.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4757.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4757.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT+SC (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (using GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same self-consistency aggregation of independent CoT samples, applied with GPT-4; benefits from higher single-sample quality and still shows smooth budget-driven gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (sizable, multi-modal family, used version 0613 in paper) used with temperature=1 for sampling in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT+SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate many independent CoT samples from GPT-4 and aggregate answers by majority vote; higher per-sample correctness probability accelerates majority convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same set: GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse reasoning benchmarks including math, commonsense, multi-hop, and puzzle tasks; experiments run with matched budgets for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: CoT+SC remains competitive and often best under matched budgets; however, Tree-of-Thoughts with GPT-4 notably outperforms other methods on Game of 24 even when accounting for budget (ToT requires substantially more compute).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Tree-of-Thoughts (with GPT-4 proposer/evaluator) outperforms CoT+SC on Game of 24; MAD and Reflexion underperform CoT+SC under equivalent budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>With a stronger base model (GPT-4), CoT+SC continues to be a strong, budget-efficient baseline; but some compute-intensive structured search strategies (Tree-of-Thoughts) can surpass it if model capacity and budget permit.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Tree-of-Thoughts on GPT-4 beats CoT+SC on Game of 24 (despite token-aware accounting), showing that diversity induced by structured search can outperform independent sampling when both model quality and budget are sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4757.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4757.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent debating protocol where multiple model 'agents' propose answers and critique each other across rounds; later rounds condition on prior dialog (dependent sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Encouraging divergent thinking in large language models through multi-agent debate.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (experiments primarily reported on GPT-3.5; other models also tested in appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-agent debate runs multiple agent instances (paper used 6 agents, 3 rounds for budget-matched experiments) of the language model; agents see previous rounds and condition their outputs on conversation history.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Multiple agents generate and critique answers in rounds; sampling is dependent across rounds because agents condition on past debate transcripts, which can reduce diversity over rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same diverse reasoning benchmarks used to compare reasoning strategies under matched budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: MAD initially produces gains similar to CoT+SC up to the point where both methods are effectively the same (same number of independent samples), but as budget increases MAD's gains plateau and it often underperforms CoT+SC when token/query budgets are matched (paper reports MAD underperforming across most datasets except not strongly on HotpotQA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT+SC generally achieves higher performance under equal budgets; Reflexion also often underperforms; Tree-of-Thoughts with GPT-4 can outperform MAD on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Although MAD is designed to encourage divergent thinking, the dependent sampling (conditioning on previous rounds) leads to decreased entropy/diversity across rounds (measured entropy declines), causing tunneling effects and an eventual performance plateau as budget increases; thus MAD does not benefit as reliably from increased inference budget as independent-sampling methods.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MAD's early rounds (up to the point where it matches the number of independent samples) perform similarly to CoT+SC, but subsequent rounds produce diminishing returns; the paper finds entropy of answers drops round-by-round, empirically supporting the diversity-loss hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4757.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4757.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (iterative reflection and proposal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-and-reflect loop that alternates proposals and reflections (self-evaluation) to refine answers; may optionally rely on an oracle and internal evaluator prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 and GPT-4 (experiments reported for both; comparisons highlighted for GPT-3.5 and GPT-4 evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reflexion runs sequences of proposals and reflections (paper configured up to 10 proposals/9 reflections for a 19-query budget in some experiments); different evaluator configurations (random, GPT-4, oracle) were studied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Alternates generating answers and reflecting on failures to produce revised proposals; reflections depend on prior proposals (dependent/iterative sampling), and an internal self-evaluator may be used to detect correctness and stop early.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks; Reflexion settings evaluated under matched query/token budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Reflexion consistently performed worse than CoT+SC in the paper's budget-aware comparisons; Reflexion with an oracle evaluator can substantially outperform SC, but realistic LLM evaluators (even GPT-4) fall short of oracle-level performance and thus Reflexion underperforms in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT+SC outperforms Reflexion under matched budgets; Reflexion+oracle > CoT+SC, but Reflexion+GPT-4 evaluator still underperforms Reflexion+SC baseline (paper's ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative dependent-reflection does not reliably benefit from larger budgets; Reflexion is highly sensitive to evaluator quality — a perfect (oracle) evaluator yields large gains, but current LLM self-evaluation is insufficient, making Reflexion often worse than cheaper independent-sampling baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Reflexion with an oracle evaluator improves performance substantially; conversely, Reflexion with GPT-4 evaluator still underperforms Reflexion+SC (and CoT+SC), highlighting a large gap between oracle and LLM-based self-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4757.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4757.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured search method that builds a tree of intermediate 'thoughts' (partial chains-of-thought) with proposer and evaluator components to explore multiple reasoning branches and prune unlikely branches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 and GPT-4 (ablation studies used combinations for proposer/evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ToT uses a proposer to generate candidate thoughts and an evaluator to score/prune them; experiments include configurations like GPT-4 proposer + GPT-4 evaluator, GPT-4 proposer + GPT-3.5 evaluator, etc.; beam width and evaluator budget vary.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Perform branching search in thought-space: at each node proposer generates multiple candidate thoughts, evaluator rates/prunes branches; yields structured diversity across branches but requires extra evaluation budget and context tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Game of 24 (primary detailed ablations), other reasoning datasets in broader experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Game of 24: arithmetic puzzle using 4 numbers and operations to make 24; ToT ablations focused on this benchmark to examine proposer/evaluator budgets and model pairing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative & partial quantitative: With GPT-4 as both proposer and evaluator, ToT substantially outperforms other strategies on Game of 24 even when accounting for token/query budgets (paper reports ToT top performance on Game of 24 with GPT-4); with weaker models (e.g., GPT-3.5 proposer/evaluator) ToT performance lags and CoT+SC is preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT+SC outperforms ToT on weaker models (GPT-3.5); ToT (GPT-4 proposer/evaluator) outperforms CoT+SC on Game of 24 when budget is sufficiently large.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ToT's gains hinge on strong proposer/evaluator quality and sufficient budget: evaluator accuracy materially impacts performance (random or weak evaluator causes steep drop); ToT can beat SC but at higher token/compute cost and when the base model is strong enough (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Using a random evaluator or using GPT-3.5 as the proposer/evaluator produces large performance drops; ToT with weaker models is outperformed by the cheaper CoT+SC baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4757.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4757.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC^2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Confident Self-Consistency (SC^2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A weighting variant of self-consistency that multiplies answer counts by model-derived self-evaluation confidence (binary Yes/No or probability) to pick the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (primarily reported), also evaluated with GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SC^2 uses LLM-generated self-evaluations (binary Yes/No sampled multiple times, or numeric probability) to reweight the votes from CoT samples; experiments used caching and small token overhead for evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Confident Self-Consistency (SC^2)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Collect many independent CoT samples, then for each candidate answer run a small self-evaluation probe multiple times to estimate confidence; weight majority counts by this confidence to choose final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH, TheoremQA (integer subset / random subset), HotpotQA (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math and QA benchmarks used to test whether adding self-evaluation confidence to SC improves budget-adjusted accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: SC^2 improves over plain SC on some math datasets (GSM8K, MATH) in the paper's experiments, but falls behind on other datasets (three of five reported) — gains are dataset-dependent and tied to self-evaluation calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Plain CoT+SC (unweighted majority) — SC^2 outperforms SC on math reasoning datasets where model's self-evaluation is calibrated; SC often superior when self-evaluation is uncalibrated or near-random (e.g., TheoremQA).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Weighting self-consistency by LLM self-evaluation can yield non-trivial gains when the self-evaluator is well-calibrated (notably on math datasets), but because LLM self-evaluation quality varies by dataset and difficulty, SC^2's benefits are uneven; self-evaluation is promising but currently unreliable on hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On TheoremQA (hard tasks) GPT-4's self-evaluation accuracy is near random and SC^2 yields only small or no improvements; SC^2 also increases token costs, raising budget-efficiency trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Encouraging divergent thinking in large language models through multi-agent debate. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 1)</em></li>
                <li>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4757",
    "paper_id": "paper-270370750",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "CoT+SC (GPT-3.5)",
            "name_full": "Chain-of-Thought with Self-Consistency (using GPT-3.5)",
            "brief_description": "Independent parallel sampling of Chain-of-Thought (CoT) answers followed by majority vote (self-consistency); increases answer diversity via independent draws and aggregates to improve accuracy.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (turbo-0301 / 0301)",
            "model_description": "GPT-3.5 (decoder-only transformer, used with temperature=1 in experiments reported; model id 0301 in paper).",
            "reasoning_method_name": "Chain-of-Thought with Self-Consistency (CoT+SC)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate many independent chain-of-thought samples (parallel sampling) for each problem, then aggregate answers via majority vote; diversity arises from independent sampling (stochastic decoding).",
            "task_name": "GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24 (100-sample subsets)",
            "task_description": "Math reasoning (GSM8K, MATH, TheoremQA), commonsense QA (CSQA), multi-hop QA (HotpotQA), and the Game of 24 puzzle; each experiment used 100 randomly sampled test instances per dataset (Game of 24 has 100 instances total).",
            "performance": "Qualitative: CoT+SC with matched budget consistently outperforms Multi-Agent Debate and Reflexion across most datasets in the paper's budget-aware comparisons; performance increases smoothly with increased query/token budget. (Paper reports CoT+SC as the dominant baseline for GPT-3.5 under equal token/query budgets.)",
            "comparison_with_other_method": true,
            "performance_other_method": "Multi-Agent Debate (MAD) and Reflexion performed worse than CoT+SC under equivalent budgets; Tree-of-Thoughts under GPT-3.5 underperforms CoT+SC on Game of 24.",
            "key_findings": "Independent (diverse) sampling via CoT+SC is highly budget-efficient: when given the same token/query budget as more complex methods, CoT+SC often matches or exceeds their performance; diversity from independent sampling is a primary driver of CoT+SC's gains and its performance increases smoothly with budget.",
            "counter_examples_or_negative_results": "For problem subsets where the model's per-sample correctness probability p_i &lt; 0.5, majority voting (SC) can reduce performance (increasing samples can worsen results). On some tasks (e.g., HotpotQA in the study) CoT+SC did not clearly dominate; Tree-of-Thoughts with a stronger model (GPT-4) can outperform CoT+SC on Game of 24.",
            "uuid": "e4757.0",
            "source_info": {
                "paper_title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CoT+SC (GPT-4)",
            "name_full": "Chain-of-Thought with Self-Consistency (using GPT-4)",
            "brief_description": "Same self-consistency aggregation of independent CoT samples, applied with GPT-4; benefits from higher single-sample quality and still shows smooth budget-driven gains.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-4 (0613)",
            "model_description": "GPT-4 (sizable, multi-modal family, used version 0613 in paper) used with temperature=1 for sampling in reported experiments.",
            "reasoning_method_name": "Chain-of-Thought with Self-Consistency (CoT+SC)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate many independent CoT samples from GPT-4 and aggregate answers by majority vote; higher per-sample correctness probability accelerates majority convergence.",
            "task_name": "Same set: GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24",
            "task_description": "Diverse reasoning benchmarks including math, commonsense, multi-hop, and puzzle tasks; experiments run with matched budgets for fair comparison.",
            "performance": "Qualitative: CoT+SC remains competitive and often best under matched budgets; however, Tree-of-Thoughts with GPT-4 notably outperforms other methods on Game of 24 even when accounting for budget (ToT requires substantially more compute).",
            "comparison_with_other_method": true,
            "performance_other_method": "Tree-of-Thoughts (with GPT-4 proposer/evaluator) outperforms CoT+SC on Game of 24; MAD and Reflexion underperform CoT+SC under equivalent budgets.",
            "key_findings": "With a stronger base model (GPT-4), CoT+SC continues to be a strong, budget-efficient baseline; but some compute-intensive structured search strategies (Tree-of-Thoughts) can surpass it if model capacity and budget permit.",
            "counter_examples_or_negative_results": "Tree-of-Thoughts on GPT-4 beats CoT+SC on Game of 24 (despite token-aware accounting), showing that diversity induced by structured search can outperform independent sampling when both model quality and budget are sufficient.",
            "uuid": "e4757.1",
            "source_info": {
                "paper_title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MAD",
            "name_full": "Multi-Agent Debate (MAD)",
            "brief_description": "A multi-agent debating protocol where multiple model 'agents' propose answers and critique each other across rounds; later rounds condition on prior dialog (dependent sampling).",
            "citation_title": "Encouraging divergent thinking in large language models through multi-agent debate.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (experiments primarily reported on GPT-3.5; other models also tested in appendix)",
            "model_description": "Multi-agent debate runs multiple agent instances (paper used 6 agents, 3 rounds for budget-matched experiments) of the language model; agents see previous rounds and condition their outputs on conversation history.",
            "reasoning_method_name": "Multi-Agent Debate (MAD)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Multiple agents generate and critique answers in rounds; sampling is dependent across rounds because agents condition on past debate transcripts, which can reduce diversity over rounds.",
            "task_name": "GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24",
            "task_description": "Same diverse reasoning benchmarks used to compare reasoning strategies under matched budgets.",
            "performance": "Qualitative: MAD initially produces gains similar to CoT+SC up to the point where both methods are effectively the same (same number of independent samples), but as budget increases MAD's gains plateau and it often underperforms CoT+SC when token/query budgets are matched (paper reports MAD underperforming across most datasets except not strongly on HotpotQA).",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT+SC generally achieves higher performance under equal budgets; Reflexion also often underperforms; Tree-of-Thoughts with GPT-4 can outperform MAD on some tasks.",
            "key_findings": "Although MAD is designed to encourage divergent thinking, the dependent sampling (conditioning on previous rounds) leads to decreased entropy/diversity across rounds (measured entropy declines), causing tunneling effects and an eventual performance plateau as budget increases; thus MAD does not benefit as reliably from increased inference budget as independent-sampling methods.",
            "counter_examples_or_negative_results": "MAD's early rounds (up to the point where it matches the number of independent samples) perform similarly to CoT+SC, but subsequent rounds produce diminishing returns; the paper finds entropy of answers drops round-by-round, empirically supporting the diversity-loss hypothesis.",
            "uuid": "e4757.2",
            "source_info": {
                "paper_title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (iterative reflection and proposal)",
            "brief_description": "An iterative generate-and-reflect loop that alternates proposals and reflections (self-evaluation) to refine answers; may optionally rely on an oracle and internal evaluator prompts.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 and GPT-4 (experiments reported for both; comparisons highlighted for GPT-3.5 and GPT-4 evaluators)",
            "model_description": "Reflexion runs sequences of proposals and reflections (paper configured up to 10 proposals/9 reflections for a 19-query budget in some experiments); different evaluator configurations (random, GPT-4, oracle) were studied.",
            "reasoning_method_name": "Reflexion",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Alternates generating answers and reflecting on failures to produce revised proposals; reflections depend on prior proposals (dependent/iterative sampling), and an internal self-evaluator may be used to detect correctness and stop early.",
            "task_name": "GSM8K, MATH, TheoremQA, CSQA, HotpotQA, Game of 24",
            "task_description": "Same benchmarks; Reflexion settings evaluated under matched query/token budgets.",
            "performance": "Qualitative: Reflexion consistently performed worse than CoT+SC in the paper's budget-aware comparisons; Reflexion with an oracle evaluator can substantially outperform SC, but realistic LLM evaluators (even GPT-4) fall short of oracle-level performance and thus Reflexion underperforms in practice.",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT+SC outperforms Reflexion under matched budgets; Reflexion+oracle &gt; CoT+SC, but Reflexion+GPT-4 evaluator still underperforms Reflexion+SC baseline (paper's ablation).",
            "key_findings": "Iterative dependent-reflection does not reliably benefit from larger budgets; Reflexion is highly sensitive to evaluator quality — a perfect (oracle) evaluator yields large gains, but current LLM self-evaluation is insufficient, making Reflexion often worse than cheaper independent-sampling baselines.",
            "counter_examples_or_negative_results": "Reflexion with an oracle evaluator improves performance substantially; conversely, Reflexion with GPT-4 evaluator still underperforms Reflexion+SC (and CoT+SC), highlighting a large gap between oracle and LLM-based self-evaluation.",
            "uuid": "e4757.3",
            "source_info": {
                "paper_title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "A structured search method that builds a tree of intermediate 'thoughts' (partial chains-of-thought) with proposer and evaluator components to explore multiple reasoning branches and prune unlikely branches.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 and GPT-4 (ablation studies used combinations for proposer/evaluator)",
            "model_description": "ToT uses a proposer to generate candidate thoughts and an evaluator to score/prune them; experiments include configurations like GPT-4 proposer + GPT-4 evaluator, GPT-4 proposer + GPT-3.5 evaluator, etc.; beam width and evaluator budget vary.",
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Perform branching search in thought-space: at each node proposer generates multiple candidate thoughts, evaluator rates/prunes branches; yields structured diversity across branches but requires extra evaluation budget and context tokens.",
            "task_name": "Game of 24 (primary detailed ablations), other reasoning datasets in broader experiments",
            "task_description": "Game of 24: arithmetic puzzle using 4 numbers and operations to make 24; ToT ablations focused on this benchmark to examine proposer/evaluator budgets and model pairing.",
            "performance": "Qualitative & partial quantitative: With GPT-4 as both proposer and evaluator, ToT substantially outperforms other strategies on Game of 24 even when accounting for token/query budgets (paper reports ToT top performance on Game of 24 with GPT-4); with weaker models (e.g., GPT-3.5 proposer/evaluator) ToT performance lags and CoT+SC is preferable.",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT+SC outperforms ToT on weaker models (GPT-3.5); ToT (GPT-4 proposer/evaluator) outperforms CoT+SC on Game of 24 when budget is sufficiently large.",
            "key_findings": "ToT's gains hinge on strong proposer/evaluator quality and sufficient budget: evaluator accuracy materially impacts performance (random or weak evaluator causes steep drop); ToT can beat SC but at higher token/compute cost and when the base model is strong enough (GPT-4).",
            "counter_examples_or_negative_results": "Using a random evaluator or using GPT-3.5 as the proposer/evaluator produces large performance drops; ToT with weaker models is outperformed by the cheaper CoT+SC baseline.",
            "uuid": "e4757.4",
            "source_info": {
                "paper_title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SC^2",
            "name_full": "Self-Confident Self-Consistency (SC^2)",
            "brief_description": "A weighting variant of self-consistency that multiplies answer counts by model-derived self-evaluation confidence (binary Yes/No or probability) to pick the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (primarily reported), also evaluated with GPT-3.5",
            "model_description": "SC^2 uses LLM-generated self-evaluations (binary Yes/No sampled multiple times, or numeric probability) to reweight the votes from CoT samples; experiments used caching and small token overhead for evaluations.",
            "reasoning_method_name": "Self-Confident Self-Consistency (SC^2)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Collect many independent CoT samples, then for each candidate answer run a small self-evaluation probe multiple times to estimate confidence; weight majority counts by this confidence to choose final answer.",
            "task_name": "GSM8K, MATH, TheoremQA (integer subset / random subset), HotpotQA (subset)",
            "task_description": "Math and QA benchmarks used to test whether adding self-evaluation confidence to SC improves budget-adjusted accuracy.",
            "performance": "Qualitative: SC^2 improves over plain SC on some math datasets (GSM8K, MATH) in the paper's experiments, but falls behind on other datasets (three of five reported) — gains are dataset-dependent and tied to self-evaluation calibration.",
            "comparison_with_other_method": true,
            "performance_other_method": "Plain CoT+SC (unweighted majority) — SC^2 outperforms SC on math reasoning datasets where model's self-evaluation is calibrated; SC often superior when self-evaluation is uncalibrated or near-random (e.g., TheoremQA).",
            "key_findings": "Weighting self-consistency by LLM self-evaluation can yield non-trivial gains when the self-evaluator is well-calibrated (notably on math datasets), but because LLM self-evaluation quality varies by dataset and difficulty, SC^2's benefits are uneven; self-evaluation is promising but currently unreliable on hard tasks.",
            "counter_examples_or_negative_results": "On TheoremQA (hard tasks) GPT-4's self-evaluation accuracy is near random and SC^2 yields only small or no improvements; SC^2 also increases token costs, raising budget-efficiency trade-offs.",
            "uuid": "e4757.5",
            "source_info": {
                "paper_title": "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Encouraging divergent thinking in large language models through multi-agent debate.",
            "rating": 2,
            "sanitized_title": "encouraging_divergent_thinking_in_large_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.",
            "rating": 1,
            "sanitized_title": "planandsolve_prompting_improving_zeroshot_chainofthought_reasoning_by_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0172715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies
15 Jun 2024</p>
<p>Junlin Wang 
Siddhartha Jain 
Dejiao Zhang 
Baishakhi Ray 
Varun Kumar 
Ben Athiwaratkun 
Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies
15 Jun 20240CEB243A8C0CBB092331CA6DF7F0ACFAarXiv:2406.06461v3[cs.CL]
A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models.However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute.By overlooking this aspect, a skewed view of strategy efficiency is often presented.This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost.In this budget-aware perspective, we find that complex reasoning strategies often don't surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated.When we provide a simple baseline like chain-of-thought selfconsistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature.In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.</p>
<p>Introduction</p>
<p>The arena of large language models (LLMs) such as GPT-4 (OpenAI, 2023;Touvron et al., 2023;Team, 2023;Jiang et al., 2023a) has seen a proliferation of diverse reasoning strategies.However, comparing these strategies fairly and comprehensively has proven to be a challenging task due to their varied computational requirements.For instance, strategies like tree of thoughts (ToT) necessitate * Work conducted during an internship at Amazon.§Work done while at Amazon. 1 Duke University 2 Nvidia 3 AWS AI Labs 4 Together AI.Correspondence to: Junlin Wang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#117;&#110;&#108;&#105;&#110;&#46;&#119;&#97;&#110;&#103;&#50;&#64;&#100;&#117;&#107;&#101;&#46;&#101;&#100;&#117;">&#106;&#117;&#110;&#108;&#105;&#110;&#46;&#119;&#97;&#110;&#103;&#50;&#64;&#100;&#117;&#107;&#101;&#46;&#101;&#100;&#117;</a>,Siddhartha Jain <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#116;&#109;&#102;&#115;&#49;&#48;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#116;&#109;&#102;&#115;&#49;&#48;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>.</p>
<p>branching out into multiple sequences and incorporating self-evaluation, making them more compute-intensive than others.Therefore, an evaluation framework that only accounts for performance metrics may miss crucial practical factors such as computational cost.</p>
<p>In this paper, we propose the inclusion of the compute budget into the performance measurement of different reasoning strategies.This budget-aware comparison yields a more balanced perspective on the effectiveness of reasoning strategies, accounting for both the quality of the output and the computational resources expended.</p>
<p>Our empirical research uncovers a significant correlation between the performance and the compute budget.We find that a straightforward baseline strategy, chain-of-thought reasoning coupled with self-consistency, can be remarkably competitive.When scaled to match the compute resources of more sophisticated methods such as Multi-Agent Debate (MAD) (Liang et al., 2023), Reflexion (Shinn et al., 2023), Plan and Solve (Wang et al., 2023), Least to Most Prompting (Zhou et al., 2022), Progressive Hint Prompting (Zheng et al., 2023), this baseline strategy often outperforms them in achieving the best trade-off between performance and budget.We further investigate the reasons behind the gap from simple CoT SC and other reasoning strategies by providing both empirical and theoretical evidence.</p>
<p>Then we scrutinize the influence of two specific types of budgets on performance: (1) the answer generation budget, and (2) the evaluation budget.Our findings indicate that selfevaluation performance is really dependent on models and datasets.Moreover, we identify a strong correlation between the calibration via a correctness prediction proxy and the success of reasoning strategies that leverage self-evaluation.</p>
<p>This work provides a robust framework for comparing a wide array of reasoning strategies and illuminates the significance of self-evaluation in these models.We hope this sets the stage for more focused research on efficient budget utilization and paves the way for the development of even more effective reasoning strategies.(1) Comparison of reasoning approaches multi-agent debate (MAD) against the SC baseline, considering both scale-agnostic and scale-aware evaluation, with published scores and our reproductions on the GSM8K and MATH dataset.The scale-aware evaluation furnishes more comprehensive insights into the influence of scale on reasoning strategies and offers a fairer method of comparison.(2) The scale-aware comparison between Reflexion and SC also illustrates the artifact of scale on performance.For both datasets, we show both budgets, the number of total tokens, and the number of queries.All results were obtained from GPT-3.5.spanning three dimensions: queries, tokens, and monetary cost, advocating for the token-based metric as the most holistic.This metric adeptly captures both the latency and financial implications of computational tasks.</p>
<p>• We present a comprehensive evaluation of seven LLM reasoning strategies across five datasets using five models including GPT-4.Our analysis reveals that traditional evaluation metrics often overlook a critical aspect: the performance gains achievable through additional computational resources.This observation is strongly supported by CoT SC matches or even exceeds more complex strategies in effectiveness.</p>
<p>• We explore the dynamics of reasoning strategies, highlighting that MAD underperforms as diversity diminishes with each round.Conversely, Self-Consistency excels due to the independence of samples boosting diversity and its effectiveness in scenarios where the likelihood of being correct exceeds 50%.</p>
<p>• We conduct ablation studies on ToT and Reflexion by segregating the budget into answer generation and evaluation budgets.We found that self-evaluation is promising at increasing performance while being costeffective but currently LLM can't self-evaluate well.</p>
<p>Related Work</p>
<p>Reasoning strategies for LLMs</p>
<p>There has been a flurry of activity to use language models for generating effective reasoning and planning strategies.An early work in the area was to prompt the language model to generate its Chain-of-Thought (CoT) (Wei et al., 2022) when solving a problem which led to significant improvements in the model's problem-solving abilities.Later work has involved prompting the language model to come up with its plan for solving the problem before trying to solve it (Jiang et al., 2023b), using chain-of-thought to solve a problem and then asking the model to critique and revise its solution (feedback) (Madaan et al., 2023;Scheurer et al., 2023;Chen et al., 2023a;Bai et al., 2022;Kim et al., 2023;Shinn et al., 2023), generating multiple chain-of-thoughts and combining them using LLM (Yoran et al., 2023), setting up a tree search for chain-of-thought instead of sampling a single linear chain-of-thought (Tree of Thoughts -ToT) (Yao et al., 2023), aggregating LLM generated feedback for multiple prompts and their solutions into guidelines that can improve future generation (Chen et al., 2023b), and using multiple LLMs as debating agents to refine feedback for a solution (Du et al., 2023;Liang et al., 2023).However, they are all evaluated on different datasets and whether the baselines are computed</p>
<p>Chain of Thought Reflexion</p>
<p>Multi-Agent Debate Thens elect the answer with the most occurrences.</p>
<p>Self-Consistency</p>
<p>Each agent provides critique based on history of debate with other agents.Optionally, we use summarization between rounds.</p>
<p>Alternate generating answer and re ecting on the answer.In re exion, green = diagnose reason for failture and provide high level plan.</p>
<p>Generating answer given previous generated answers as hints.</p>
<p>At each depth, pick top K branches to continue and stop the rest.or cost-matched is rarely considered.Notable exceptions are Shinn et al. (2023) where they consider performance as a function of the number of queries to the language model and Olausson et al. (2023) which evaluates the performance of a self-debug strategy for code generation as a function of the number of tokens generated.</p>
<p>LLM output evaluation</p>
<p>There has been considerable work on evaluating the output of LLMs -both via training custom models as well as using the LLMs themselves for self-evaluation.For trained verifiers/rerankers, in Cobbe et al. (2021), they train a verifier to rerank outputs of language models for math word problems and show strong improvements.In Inala et al. (2022), they do the same except for code generation.In Uesato et al. (2022); Yang et al. (2022), they train an evaluator for each step in a chain-of-thought and rerank using the combined score for each step in the chain.In Li et al. (2023), they weight the self-consistency by the trained verifier confidence.There has also been work recently on using the LLMs themselves to evaluate their own generations.In Bai et al. (2022), they use LLMs to do pairwise comparisons between generations achieving high accuracy.In Ling et al. (2023), self-consistency for every step is used to evaluate how correct a deductive step is.While they can obtain high accuracy as to whether a step is valid or not, they are unable to improve the overall accuracy of answer generation using that.Tian et al. (2023) examine multiple strategies for eliciting LLM self-evaluation that is as calibrated as possible.</p>
<p>The self-refine (Madaan et al., 2023) approach uses LLMs to get detailed self-evaluation which is used to improve the next round of generation.The Tree-of-Thoughts (Yao et al., 2023) paper uses LLM self-evaluation to rank which node to explore next.</p>
<p>Inference Budget of Reasoning Strategies</p>
<p>While the raw performance of different prompting or reasoning strategies for LLMs is a common topic, how different strategies perform when budget-aware is less well-studied (with the notable exception of Olausson et al. (2023)).However, taking budget into account can be critical when using LLMs.In this section, we describe different usage scenarios that a user could be interested in and what budgetary metrics would be relevant to those scenarios.We furthermore describe how different reasoning strategies can scale in terms of each budget.</p>
<p>Budget</p>
<p>We examine various budgetary metrics for LLMs.Given that the number of input and output tokens often feature prominently across these metrics, we designate them as n I and n O respectively.This section aims to explore key components that can make reasoning strategies successful from the scale-aware perspective.First, we show that the inference budget is often overlooked but is one of the primary indicators of the success of a reasoning strategy.We show that the scale-aware evaluation perspective, CoT (or variants of it like Plan and Solve, Least to Most) self-consistency, for instance, is a strong baseline that can outperform or match many proposed reasoning strategies in the literature given the same level of budget.</p>
<p>We use existing reasoning strategies in literature to perform this study, namely Multi-Agent Debate (MAD) (Liang et al., 2023), Reflexion (Shinn et al., 2023), Plan and Solve (Wang et al., 2023), Least to Most Prompting (Zhou et al., 2022), Progressive Hint Prompting (Zheng et al., 2023), and Tree-of-Thoughts (Yao et al., 2023).We conducted our experiments across a diverse range of reasoning tasks, utilizing math reasoning datasets such as GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), and Theo-remQA (Chen et al., 2023c), along with the commonsense reasoning task CSQA (Talmor et al., 2019), and the multihop reasoning task HotpotQA (Yang et al., 2018) (see Appendix A.1). Additionally, we performed an in-depth analysis of the puzzle game Game24 (Yao et al., 2023) to further our investigation on budget-aware evaluation.We use GPT-3.5 and GPT-4 for our experiments.(See Appendix A.1 for more details about model hyperparameters)</p>
<p>Inference budget unveils superiority of self-consistency baseline over MAD &amp; Reflexion</p>
<p>We present that the observed improvements in performance for various reasoning methods may be strongly influenced by the use of a higher inference budget, rather than the intrinsic merit of the techniques themselves.</p>
<p>Results in Figure 1 and 3 elucidate the efficacy of reasoning techniques, including MAD and Reflexion, in contrast with the SC baseline.We keep the budget for each question to be a maximum of 20 queries or 10k tokens.This means for CoT SC, we would sample 20 times.For MAD, we set the number of agents to 6 and the number of rounds to 3 which resulted in exactly 18 queries.For Reflxion, we set it to reflect a maximum of 10 trials (10 proposals 9 reflections for a total of 19 queries).We demonstrate two budgetary metrics which were discussed in (Section 3.1): a) Performance@Number of Queries b) Performance@Number of Tokens.As illustrated in Figure 1 and 3, when the inference budget of the baseline is aligned with that of each reasoning approach, the perceived benefits of the innovative strategies no longer apply.The SC baseline regularly outperforms more complex strategies when given equivalent budgets across all datasets except HotpotQA.However, even for HotpotQA, the SC baseline still performs as well as other reasoning strategies.Reflexion consistently performs the worst out of the three strategies analyzed and we will discuss this more in detail later.Relying solely on scale-independent assessments, as is sometimes done in prior works, might lead to incomplete or potentially misleading interpretations.Conversely, Progressive Hints, which leverages sequential answers as cues for subsequent questions, exhibits the least effective performance.This comparative analysis underscores that the observed improvements in performance are primarily attributable to increased budget allocations rather than the inherent advantages of the methodologies.Some complex reasoning strategies perform seemly perform better, however, when accounting for budget, fail to beat basic CoT with self-conssitency.Evaluations on more datasets and types of budget as well as details about each strategy can be found in Appendix G.2.</p>
<p>Tree-of-Thoughts is competitive with a caveat</p>
<p>Tree-of-Thoughts can outperform baselines We in addition evaluated the well-known Tree-of-Thoughts strategy in a scale-aware manner on the logical game Game of 24.Notable discrepancies emerged in the behavior of the model when transitioning to GPT-4 and we modified our budgetaware metric slightly to further scrutinize GPT-4.</p>
<p>A strong model is needed to perform better than baseline In Figure 5, we show the performance of GPT-3.5 with the All experiments here are run until at least 10k tokens.CoT with SC consistently beat other reasoning strategies across all 5 datasets with significantly less budget.The budget difference is even more drastic when counting the number of tokens.The MAD result is shown non-round-wise.</p>
<p>Tree-of-Thoughts reasoning strategy on Game of 24 1 .The performance of Tree-of-Thoughts lags that of a simple SC by a considerable margin.This is in stark contrast to the GPT-4 results with Tree-of-thoughts where all other strategies plateau very early and Tree-of-Thoughts beats all of them by a big margin.This remains the case even when we account for the budget (query or token budget) as the other strategies have a low performance ceiling.However, note that Treeof-Thoughts requires a significant budget commitment to deliver such a performance.On weaker models than GPT-4, it is still better to use simpler strategies like SC which 1 We used a modified thought evaluation prompt for GPT-3.5 that gave much better results than the default one outperforms ToT by a considerable margin (Figure 5).</p>
<p>What Makes Reasoning Strategies Work</p>
<p>We conduct a more in-depth analysis into reasoning strategies.Specifically, we investigate what causes the performance gap evidenced by our budget-aware evaluations among reasoning strategies like MAD or Reflexion versus self-consistency in Section 5.1.In Section 5.2, we delve deeper into different components of tree-of-thoughts.Finally in Section 5.4 we examine the effectiveness of selfevaluation in the reasoning loop.CoT SC for both GPT-3.5 and GPT-4 on Game of 24.The dotted lines represent the performance of ToT.For ToT using GPT-4 (red), results for three settings are included.All CoT results are computed using self-consistency on 100 samples.</p>
<p>Reasoning strategies do not benefit equally from higher inference budget</p>
<p>The scale-aware perspective offers clear a guideline for what reasoning strategies make sense.That is, a proposed reasoning strategy should be considered effective only if its performance is better compared to a baseline of equivalent budget; otherwise, the incurred cost could not be justified since the baseline with comparable cost performs better (in terms of FLOPs, latency, monetary cost, latency, or any other type of budget we may care about).A question arises whether we can keep increasing the budget to obtain the best possible abilities.</p>
<p>As seen in Figure 3, we find that the self-consistency baseline exhibits a smooth increase in scores with respect to scale.However, such a trend does not always hold for other reasoning strategies.For instance, in multi-agent debate, an augmented inference budget eventually experiences a performance plateau (potentially due to diversity decrease which we analyze later).For the MAD setting with 6 agents, the graph for MAD and SC overlaps up to 6 queries since they correspond to the exact same strategy up to that point.After 6 queries, the MAD strategy switches to the second round where the performance gain noticeably lessens compared to self-consistency.The amount of tokens required for each subsequent round also increases drastically since each agent needs to look at rounds' conversations.The lowered performance compared to the SC baseline may arise because subsequent rounds of MAD may incite a cascading effect of cumulative mistakes, or snowballed hallucinations suggested in Zhang et al. (2023).Below, we offer some further explanations behind the performance gap.</p>
<p>DEPENDENT SAMPLING CAN HURT RESPONSE</p>
<p>DIVERSITY</p>
<p>Multi-agent debate conditions on the previous round's answers to sample new answers.We posit one of the reasons multi-agent debate performs worse as the budget increases is because it reduces response diversity (through this dependent sampling) and, hence is more likely to tunnel on the wrong answer.To show this, we compared the entropy of the solutions generated at each round for MAD vs. SC.The results are in Figure 6.Here, the entropy corresponds to the diversity of answers within each round.The entropy consistently declines for multi-agent debate as we move to the next round suggesting exactly the kind of cascading effect we hypothesized.By contrast, self-consistency does not suffer such negative consequences and even increases its solution diversity since the responses are generated independently without affecting one another.</p>
<p>EFFECTIVENESS OF INDEPENDENT SAMPLING WITH CHAIN-OF-THOUGHT PROMPTING</p>
<p>Next, we outline a framework that helps explain what makes self-consistency successful.We first empirically verified that the higher the occurrence of an answer, the more likely it is the correct answer (Figure 7).Self-consistency is able to capitalize on this and hence improves performance as the budget increases.</p>
<p>We model the answer generation process by language models as a binomial distribution where each problem has an inherent probability p i of being answered correctly.This analysis reveals several insights:</p>
<ol>
<li>Convergence: The probability of a correct majority vote converges to 0 or 1 as the number of trials increases, depending on whether the probability of a correct answer p i is less or greater than 0.5.</li>
</ol>
<p>Speed of Convergence:</p>
<p>Convergence is fast for extreme values of p i (closer to 1 or 0), but slow if p i is near 0.5.</p>
<p>Distribution of Correctness:</p>
<p>By placing a prior on p i (for instance, with a beta distribution), the aggregate score over the entire dataset converge to non-extreme values, resembling the behavior observed in our results.</p>
<p>That is, self-consistency performance increases smoothly over time is due to the artifact of a model consistently answering plausible answers that tend to be more correct than not.In Appendix C, we detail the analysis with extension to a multinomial setting with Dirichlet priors.</p>
<p>Tree-of-thoughts</p>
<p>In this section, we investigate the factors that contribute to the enhanced performance of the tree-of-thoughts strategy when compared to the self-consistency baseline.Tree-ofthoughts strategy mainly has two components: a proposer and a self-evaluator.The proposer proposes intermediate steps or answers and the evaluator decides whether to prune or continue on current branches.Hence we further divide the budget into the proposer budget and the evaluator budget.</p>
<p>We aim to answer questions like how much of the performance can be attributed to self-evaluation ability.</p>
<p>For the ablation study, we compare four setups for tree-ofthoughts on the Game of 24.</p>
<p>1.The standard tree-of-thoughts strategy where we use GPT-4 to evaluate the new thoughts.</p>
<ol>
<li>
<p>The standard tree-of-thoughts strategy except we now do an evaluation of a thought only once as opposed to 3 times.</p>
</li>
<li>
<p>Using a weaker model (GPT-3.5)as the evaluator while using GPT-4 as the thought generator.</p>
</li>
<li>
<p>Random evaluator, where we randomly select the subset of thoughts to prune.</p>
</li>
</ol>
<p>Evaluator quality has a non-trivial impact As observed in Figure 8, a random evaluator leads to a very steep performance drop for ToT for both best@k as well as total accuracy.Results imply that an evaluator has a non-trivial impact.Evaluation is done only once per thought as opposed to multiple times also leads to significant performance drops.</p>
<p>Cost-efficiency of evaluator However, if we use a weaker evaluator like GPT-3.5, we can maintain most of the performance while being very cost-efficient.For example, running Tree-of-Thoughts (with beam width set to 5) with GPT-4 as thought proposer and GPT-3.5 as evaluator on 100 Game of 24 instances costs $33.53 while getting an accuracy of 72%.</p>
<p>Using GPT-4 as the evaluator on the other hand increases the cost almost 5x to $159.87 while only improving accuracy to 76%.Even if we restrict ourselves to b = 1 with GPT-4 as an evaluator, we still get a higher cost of $49.9 while getting a worse performance of 65%.</p>
<p>More effective use of budget for proposer We found that the proposer plays a significant role in performance, in addition to the evaluator.For further ablation results, please refer to Appendix Table 3.Using a GPT-3.5 proposer with a GPT-4 evaluator yielded an accuracy of only 0.38, which is considerably lower than when using GPT-4 for both .SC making things worse on QA problems.We selected a subset of problems where the correct answer is not the majority.For this subset, the performance decreases with more samples.This is empirical evidence that SC may cause worse performance if the individual sample's accuracy is low.roles.We do not delve deeply into this topic here, as most reasoning strategies require some form of proposer.Our primary focus lies in assessing the extent of the advantages conferred by the unique self-evaluator component, though we wanted to highlight the importance of the proposer in this context.</p>
<p>Reflexion ablation study</p>
<p>We conducted a similar experiment on Reflexion shown in Figure 10.We compared standard Reflexion with oracle where an oracle indicates the correct answer, Reflexion with self-consistency, Reflexion with random evaluator, and Reflexion with GPT-4 as evaluator.We found that although Reflexion with oracle improves performance over SC by a large margin, Reflexion with GPT-4 Evaluator still underperforms Reflexion with SC.This highlights the substantial gap between a perfect evaluator and a GPT-4 evaluator for this dataset, indicating that the self-evaluation capability of LLMs has considerable room for improvement.In addition, evidence suggests that doing Reflection is only slightly better than random guessing.LLMs as a better evaluator could unlock significantly more performance.</p>
<p>Self-evaluation is a promising budget-efficient improvement but is currently lacking</p>
<p>Self-evaluation usually involves very few tokens generated since evaluation is short.This can be potentially very costeffective since prefilling is cheaper and faster.In this section, we investigate further how a self-evaluator can benefit the reasoning process in a budget-aware setting and demonstrate why there may still be a long way to go.In this section, we investigate further how a self-evaluator can benefit the reasoning process in a budget-aware setting.Self-evaluation ability on different datasets Table 1 shows the self-evaluation accuracy for GPT-4 for multiple datasets.The self-evaluation accuracy turns out to be heavily dependent on the dataset.It is possible that for problems that are too hard for the model, it ends up weighing the writing style of the answer much more heavily than the correctness of all the intermediate steps when doing the evaluation.We saw empirical evidence in Figure 15 where easier datasets are better calibrated and vice versa.On harder tasks like TheoremQA, GPT-4's accuracy is close to random.This means LLMs have a long way to go before they are reliable evaluators.We examine this in more detail in the appendix D.</p>
<p>Accuracy and Calibration of Self-Evaluation</p>
<p>Self-Confident Self-Consistency (SC 2 ) As an investigation of using self-evaluation to improve reasoning procedure, we propose to weigh the SC by the confidence the model has in its answer, derived from self-evaluation.We call this score the Self-Confident Self-Consistency (SC 2 ) score.We showed that SC 2 beats self-consistency on GSM8k and MATH while fall behind on the other three as shown in Figure 11. 2 This shows that although theoretically, selfevaluation is promising (shown with oracle results in Figure 10), it is still lacking in practice due to low accuracy.</p>
<p>2 More details can be found at Appendix D.3.</p>
<p>Conclusion</p>
<p>In this paper, we examined the performance of seven reasoning strategies on the often overlooked metric of budget.We used budget metrics of queries and tokens to reflect various ways LLMs are used (LLM APIs or self-host).We identified self-evaluation as an important aspect of many reasoning strategies and analyzed different prompting strategies to have the model evaluate its generations.We then evaluated self-evaluation and found that although self-evaluation could be promising at improving performance while being cost-effective, current LLMs are mostly incapable of doing that.With the current popularity of reasoning strategies, we think this more balanced budget-aware metric is beneficial for the community and helps set the correct trajectory for future LLM research.</p>
<p>Limitations</p>
<p>Our goal in the paper was to highlight the importance of different aspects of the generation budget for LLMs that are often ignored in the recent spate of reasoning strategies for LLMs.To that end, we chose some representative reasoning strategies and evaluated them on some common reasoning tasks.However, due to both monetary and time constraints, we could not include even more reasoning strategies or tasks.A more exhaustive evaluation might reveal additional nuances which would be interesting to explore.</p>
<p>A. Model/Dataset Details</p>
<p>A.1.Datasets</p>
<p>Here we describe the datasets we used in our experiments.</p>
<p>GSM8K GSM8K consists of 8.5K grade school math problems.There are 7.5K examples in the training set and 1K in the testing set.Each problem is expressed in natural language and usually involves multi-hop reasoning.</p>
<p>MATH MATH dataset collects 12.5K (7.5K training, 5K testing) high-school level competitive math problems in natural languages.This dataset is considerably harder than GSM8K.</p>
<p>TheoremQA Theorem QA annotated 800 QA pairs covering over 300 theorems spanning across Math, EE&amp;CS, Physics and Finance.We focus on math reasoning hence we only used the subset that covers math problems which contains 442 questions.This dataset is even harder than GSM8K since these questions are college-level and involve using theorems.</p>
<p>CSQA CSQA sourced commonsense reasoning questions from crowd workers based on ConceptNet.It has a total of 12,247 examples (9741, 1140,1140 for the size of train, dev, and test set respectively).</p>
<p>HotpotQA HotpotQA collects 113K question-answer pairs that require multi-hop reasoning.There are 7,405 pairs in the test set.</p>
<p>Game of 24 Game of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and 4 arithmetic operations (+-*/) to obtain 24.(Yao et al., 2023) collects 100 problems from 4num.com which are ranked 901-1000 (it is ranked from easy to hard, so these 100 are relatively hard).</p>
<p>For each dataset above, we randomly sampled 100 samples from the test set for all of our experiments.For Game of 24, since there are exactly 100 problems, we just use the same 100 problems as in (Yao et al., 2023).</p>
<p>A.2. Model Hyperparameters</p>
<p>Since we want to maintain the diversity of reasoning processes, most of the results are obtained with a temperature of 1 for GPT-3.5 and GPT-4.In our preliminary study, we also tested with a temperature of 0.7 and 0.5 and observed the same conclusion.The GPT-3.5 version we used is 0301.</p>
<p>The GPT-4 version we used is 0613.</p>
<p>B. Additional Result for Budget-aware Performance Metrics</p>
<p>B.1. Budget Metrics on All Datasets</p>
<p>Budget metrics on all datasets are shown in Figure 3 and Appendix G.</p>
<p>B.2. Detailed description of Reasoning strategies</p>
<ol>
<li>Tree of thoughts generates a search tree to search through possible chains of thought.It maintains a chain of thought.At each node in the tree, it generates a list of candidate thoughts to be added to the chain and does an evaluation to select the next thought to add.It concludes by generating an answer at a leaf node of the tree.The path in the tree from the root to the leaf node forms a single chain of thought, with each node corresponding to a single thought.If the answer is deemed incorrect (as per another evaluator), it backtracks to a previous node of the tree (unwinding the chain of thought along the way) and selects the next thought out of the candidate list of thoughts to add to the chain of thought.</li>
</ol>
<p>B.3. Self-Evaluation with CoT</p>
<p>All of our self-evaluations are done without CoT.For both evaluation calibration and weighted confidence selfconsistency, we only generated one token "yes" or "no" or one number.One may be interested in whether CoT can improve the self-evaluation performance and further boost the results.We tested this by extracting 160 CoT answers from 80 questions from GPT-3.5, where each question we extract 1 correct CoT answer and 1 incorrect CoT answer.We then compared the performance of direct evaluation versus CoT then evaluation.For GPT-3.5-turbo-0301, the accuracy increased from 50.625% to 54.375%.For GPT-4-0613, the accuracy increased from 78.75% to 79.375%.For GPT-4 the benefit from CoT is very mariginal and we concluded that it is not worth the extract cost from CoT. Hence we use the direct evaluation for all of our self-evaluations.</p>
<p>Figure 10 that investigates the Reflexion technique (Shinn et al., 2023) reveals a similar trend compared to the multiagent debate with respect to inference scale.We find that Reflexion relies heavily on the oracle that helps the model determine when the correct answer is encountered and stops the generation early and returns that answer.This is in contrast to strategies like SC.We demonstrate the performance of Reflexion including baselines that have access to oracle and without.For direct comparison, it is more fair to compare strategies within the group with access to an oracle, or without.We find that in each group, inference scale is a strong prediction on the performance.</p>
<p>C. Mathematical Framework for Self-Consistency</p>
<p>In many real-world reasoning tasks and decision-making processes, the use of SC has emerged as a powerful and often robust technique.Whether it's human experts forming a consensus or ensemble methods in machine learning, the idea of aggregating multiple opinions to reach a final decision has proven to be effective.The empirical success of SC in various domains, such as classification, regression, and human-driven decision-making, motivates a deeper examination into the underlying principles that make it work so well.</p>
<p>For instance, in complex reasoning tasks where individual models or experts might be uncertain, the wisdom of the crowd often leads to improved accuracy.SC can act as a regularization method, mitigating the effects of overfitting or biases that might be present in individual models.By combining multiple models or opinions, SC captures the common patterns among them, enhancing generalization to unseen data.</p>
<p>In this work, we seek to understand what makes SC an effective strategy, especially in the context of reasoning tasks.We aim to analyze the mathematical properties and probabilistic behavior that underlie this mechanism, considering various scenarios such as binary choices or multi-choice problems.Through rigorous analysis, simulations, and realworld datasets, we hope to derive insights that explain why SC often leads to consistent improvement and under what conditions it might fail.</p>
<p>The following section explores the mathematical explanation of SC, beginning with a simple binomial distribution model and gradually extending to more complex multinomial and Dirichlet distributions.By understanding the mathematical characteristics of these distributions, we hope to explain the empirical results observed in real-world reasoning tasks, thereby contributing to the ongoing efforts to harness the power of SC in a wide range of applications.</p>
<p>C.1. Self-Consistency Results on Reasoning Tasks</p>
<p>In our exploration of SC strategies applied to reasoning tasks, we conducted several experiments to analyze the effectiveness and behavior of different approaches.Figure 3 and Appendix G illustrate our findings, including the results for different tasks.</p>
<p>The convergence patterns and the improvement as the number of trials increases are shown for each task, highlighting the impact of SC.</p>
<p>These visualizations demonstrate the potential of SC in enhancing reasoning tasks, leading to more robust and accurate solutions.In this section, we will provide a theoretical framework that could explain the gains from SC.Note that we use Self-Consistency (SC) and Majority-Vote (MV) interchangeably.</p>
<p>C.2. Binomial</p>
<p>We seek to analyze the behavior of parallel sampling with n trials with self-consistency or SC.In this setup, given a set of problems {x i }, each problem's answer prediction (whether it is correct or not) can be modeled as a binomial distribution, assuming two choices (yes or no).Mathematically, the probability mass function for each problem's answer is given by:
f (X i = k) = n k p k i (1 − p i ) n−k ,(1)
where X i corresponds to the correct answer of the binomial distribution and p i represents the probability of a correct answer for the i-th problem.</p>
<p>We can calculate the probability that SC yields the correct solution over n trials by calculating the probability that X i yields a value that is at least n/2.This is expressed as:
P (MV correct|x i ) = n k=⌈n/2⌉ n k p k i (1 − p i ) n−k . (2)
By plotting the probability of MV being correct as a function of n, we observe that as n increases, P (MV correct|x i ) either goes to 0 or 1, depending on whether p i &gt; 0.5 or p i &lt; 0.5 for this particular problem.This is evident in the synthetic experiment shown in Figure 13.</p>
<p>If p i is extreme (closer to 1 or 0), then the convergence is fast, and the probability function can be described as:
lim n→∞ P (MV correct|x i ) = 1 if p i &gt; 0.5, 0 if p i &lt; 0.5.(3)
On the other hand, if p i is close to 0.5, the convergence is slow, reflecting the uncertainty associated with an answer that is nearly equally likely to be correct or incorrect.</p>
<p>Over the set of all problems we consider, we place a beta distribution over p i and integrate P (MV correct|x i ) over the set of all problems to obtain P (MV correct).This can be expressed mathematically as:
P (MV correct) = 1 0 P (MV correct|p i ) • f (p i |α, β) dp i ,(4)
where f (p i |α, β) is the probability density function of the beta distribution with parameters α and β.</p>
<p>If we select a beta distribution where the mode peaks beyond 0.5, then we find that P (MV correct) increases as a function  of n, albeit to a value less than 1 as you can see in Figure 12.This behavior explains our observation in real datasets directly.</p>
<p>This also implies that for datasets where majority vote leads to consistent improvement, the distribution of p i needs to be peaked greater than 0.5.There would also exist a set of problems where self-consistency leads to lowered performance, specifically for the set of problems where p i &lt; 0.5.</p>
<p>By carefully selecting the parameters of the beta distribution, we can control the characteristics of the majority voting process and gain insights into the behavior of parallel sampling across various datasets.This mathematical framework provides a powerful tool for understanding and optimizing the majority vote process in practical applications.</p>
<p>C.3. Generalization to multinomial</p>
<p>We can further generalize this setup by considering each problem as being modeled by a multinomial distribution with K choices.In this more generalized scenario, the distribution of probabilities over problems can also be modeled by a Dirichlet distribution.</p>
<p>Let p = (p 1 , p 2 , . . ., p K ) be the probabilities associated with the K choices, and let α = (α 1 , α 2 , . . ., α K ) be the parameters of the corresponding Dirichlet distribution.The probability of obtaining a correct majority vote for a given problem is then:
P (MV correct|p) = n k=⌈n/2⌉ multinomial(k; n, p),(5)
where the sum is taken over all combinations of k votes that would result in a majority for the correct choice.</p>
<p>The overall probability of obtaining a correct majority vote, integrating over all problems, can be expressed as:
P (MV correct) = P (MV correct|p) • f (p|α) dp,(6)
where f (p|α) is the probability density function, which can be modeled by the Dirichlet distribution.</p>
<p>Following a similar simulation to the binary case, we find that the conclusions hold (see Figure 14).Specifically, if the mode of the Dirichlet distribution is biased towards the correct choices, the probability of the majority vote being correct increases with n, and the set of problems where self-consistency leads to lowered performance can be characterized by the subset where the correct choice probabilities are below certain thresholds.This generalization to multinomial and Dirichlet distributions adds complexity but also additional flexibility in modeling the majority voting process, making it applicable to a broader range of practical scenarios.Given an answer, there are multiple ways we can prompt the LLM to evaluate that answer.Here we examine 3 possibilities for self-evaluation 1. Binary3 -we ask the model to output Yes/No as to whether the answer is correct.We do this multiple times and take the fraction of times the model answers Yes as the confidence of the model in the answer.</p>
<ol>
<li>
<p>Numerical confidence -we ask the model to output a score between 1 and 10 to indicate its confidence in the answer.We do this multiple times and take the average as the confidence of the model in the answer.</p>
</li>
<li>
<p>Confidence probability -similar to the previous strategy except now we prompt the model to output a confidence between 0.0 and 1.0 and average it.</p>
</li>
</ol>
<p>The evaluation result is shown in Figure 15 and Table 2.The binary Yes or No is the most well calibrated.</p>
<p>D.2. Self-evaluation is correlated with problem difficulty</p>
<p>To get an understanding of whether models found it easier to evaluate answers to easier problems, we computed the following metric for a 100 problem subset of the GSM8K dataset.For each problem i, let a ij be the jth answer.We had 20 sampled answers per problem.We computed the fraction c i of answers that were correct.Our assumption was that c i indicates the difficulty of the problem -the higher the value, the easier the problem.For each answer a ij , we obtained the binary self-evaluation confidence as described in the beginning of this section (we sampled the evaluation 5 times).We then computed the correlation ρ i between the self-evaluation confidence for the answers a ij and the binary vector indicating whether the answers were correct or not.We then computed the correlation between ρ i and c i .We obtained a correlation of 0.347 with a p-value of 0.00026 -a clear indication that an increase in the problem difficulty results in the self-evaluation becoming more noisy.</p>
<p>We repeated this experiment for MATH and TheoremQA and obtained correlations of 0.31 and 0.42 with p-values of 0.02 and 0.0025 respectively.</p>
<p>D.3.Self-Confident Self-Consistency (SC 2 ) Details</p>
<p>We take the answer which has the highest SC score as the predicted answer.Formally the definition is
SC 2 a = ai=a confidence(a i )(7)
where confidence(a i ) = v j</p>
<p>I(vj =Yes) m</p>
<p>where m denotes the number of Binary evaluations v j sampled.We apply this strategy to the MATH, TheoremQA (integer answer subset), TheoremQA (random subset), and HotpotQA datasets.SC 2 is consistently on par or better than a simple majority vote.The results are in Figure 16.SC 2 achieves non-trivial gain for math reasoning tasks but the overall costs increase quite a bit.This prompts us to inquire whether the achieved performance boost justifies the additional costs incurred.However, if we have the option to cache, then during selfevaluation, previous questions and answers can be cached and don't need to be encoded again.This can save a lot of budget and the new results would look like Figure 11.We see non-trivial gains for the math reasoning datasets.However, for TheoremQA we see markedly smaller gains.We hypothesize that the reason for this is that TheoremQA is a harder dataset for the model.As we showed in the previous section, self-evaluation ability decreases as problem difficulty increases.GPT-4 shows a self-evaluation ability of no better than random for TheoremQA and thus we observe very small improvement.Overall, a budget-aware comparison of reasoning methods is a healthy approach to compare among vastly different methods.</p>
<p>D.3.1. BUDGET-EFFICIENCY</p>
<p>The strategy requires only a handful of extra tokens (m additional tokens per answer corresponding to the Yes/No) to execute (Figure 17).   .We evaluated this custom reasoning strategy on MATH with GPT-3.5-Turbo-0125 for 15 queries, so in theory it should generate 15*4=60 responses.Here is the result based on the number of queries metric (we name the custom reasoning strategy Aggre-gateCoT).We can find that it never outperforms Chain-of-Thought Self-Consistency with same amount of tokens.The "improvement" previously was an unfair comparision because the custome reasoning strategy will use much more tokens per query.</p>
<p>Practical aspects</p>
<p>The above is not just a theoretical consideration.In Figure 18a we demonstrate, a custom reasoning strategy that at first glance, seems to outperform self-consistency -based on the number of queries metric.However, when we properly take the holistic compute budget into account via the number of tokens metric, we can see that self-consistency is more token-efficient (Figure 18b).That is, the number of tokens as a metric of budget captures the nuances of resources required for LLM reasoning more properly.</p>
<p>E. More Ablation Results for Tree of Thought</p>
<p>In Table 3 you can see more ToT results on the task of Game of 24.Most of the results are shown in the Figure 8.The table mainly shows the ablation for when using GPT-3.5 as the proposer and GPT-4 as the evaluator.We see that the performance is better than using a GPT-3.5 as the evaluator but far below the performance of using GPT-4 as the proposer.</p>
<p>F. Terms and Licenses</p>
<p>GSM8K, MATH, TheoremQA, CSQA are under the MIT license.HotpotQA is under the CC BY-SA 4.0 License.All the datasets and models are used for their intended use.</p>
<p>G. Results From More Models</p>
<p>G.1. MAD &amp; Reflexion</p>
<p>Here we extend the results to a variety of models: GPT-3.5-Tubo-0125,Mistral-7B-Instruct-v0.2, LLaMA-2-70b-chat, and Mixtral-8x7B-Instruct-v0.1.Overall, we find similar trends that self-consistency is extremely competitive compared to multi-agent debate and reflexion, when evaluated in a budget-aware manner.</p>
<p>We observed that it is very consistent that CoT with self-consistency beat other reasoning strategies across models with various sizes/training procedures.Multi-agent debate and Reflexion often decrease performances with more budget.This is not surprising considering our analysis in Section 5. Note that for LLaMA-2-70b-chat, we can't run Mad and Reflexion to the same amount of budget as CoT with self-consistency due to the context limit of around 4k.But the trend stays similar.</p>
<p>Figure 1.(1)Comparison of reasoning approaches multi-agent debate (MAD) against the SC baseline, considering both scale-agnostic and scale-aware evaluation, with published scores and our reproductions on the GSM8K and MATH dataset.The scale-aware evaluation furnishes more comprehensive insights into the influence of scale on reasoning strategies and offers a fairer method of comparison.(2) The scale-aware comparison between Reflexion and SC also illustrates the artifact of scale on performance.For both datasets, we show both budgets, the number of total tokens, and the number of queries.All results were obtained from GPT-3.5.</p>
<p>Figure 2 .
2
Figure 2. Overview of reasoning strategies.Green cell indicates question prompt, including system prompt and few-shot prompting.The orange cell indicates the answer.Blue cell indicates evaluation or critique.</p>
<p>Figure 3 .
3
Figure3.Performance@Number of Queries and Performance@Number of Tokens Plots for all 5 datasets.All three methods CoT SC, MAD, and Reflexion are plotted on two models (more models in Appendix G).All experiments here are run until at least 10k tokens.CoT with SC consistently beat other reasoning strategies across all 5 datasets with significantly less budget.The budget difference is even more drastic when counting the number of tokens.The MAD result is shown non-round-wise.</p>
<p>Figure 4 .Figure 5 .
45
Figure 4. GPT-3.5-0301Performance@Number of Tokens for all 5 datasets using three other strategies: Plan and Solve, Least to Most, Progressive Hints.</p>
<p>Figure 6 .
6
Figure6.The diversity measure of the answers proposed by GPT-3.5.The strategy is MAD with 6 agents and 3 rounds.</p>
<p>Figure 7 .
7
Figure7.Calibration result binned by answer percentages.If an answer appears more times within all the samples, that answer is more likely to be the correct answer.</p>
<p>Figure 8 .
8
Figure8.Thought proposer and and thought evaluator budget on the dataset of Game of 24.For the cost computation, we assumed the prices OpenAI had as of Aug 14, 2023 which were $0.002 per 1K tokens for GPT-3.5 (encoding or decoding), $0.03 per 1K encoded tokens and $0.06 per 1K decoded tokens for GPT-4.</p>
<p>Figure 10 .
10
Figure 10.Ablation Study of the effect of evaluator on Reflexion with GPT-4.Having an oracle evaluator outperforms SC.</p>
<p>Figure 12 .
12
Figure 12.Convergence of self consistency under different beta distributions.Here, a Beta distribution that peaks at high p indicates that there are a lot of data examples where the model can solve with high probabilities, which leads to higher average self-consistency scores.</p>
<p>Figure 13 .
13
Figure 13.Probability of self consistency being correct for a given problem with varying p.</p>
<p>Figure 14 .
14
Figure 14.Convergence of self consistency under different Dirichlet distributions with K = 3</p>
<p>Figure18.We evaluated this custom reasoning strategy on MATH with GPT-3.5-Turbo-0125 for 15 queries, so in theory it should generate 15*4=60 responses.Here is the result based on the number of queries metric (we name the custom reasoning strategy Aggre-gateCoT).We can find that it never outperforms Chain-of-Thought Self-Consistency with same amount of tokens.The "improvement" previously was an unfair comparision because the custome reasoning strategy will use much more tokens per query.</p>
<p>Figure 19 .Figure 20 .Figure 21 .Figure 22 .Figure 24 .
1920212224
Figure 19.GPT-3.5-0125:(a) Performance@Number of Queries Plots for all 5 datasets.(b) Performance@Number of Tokens for all 5 datasets.</p>
<p>Figure 25 .
25
Figure 25.Mistral-7B-Instruct-v0.2: (a) Performance@Number of Queries Plots for all 5 datasets.(b) Performance@Number of Tokens for all 5 datasets.</p>
<p>Figure 26 .
26
Figure 26.LLaMA-2-70b-chat: (a) Performance@Number of Queries Plots for all 5 datasets.(b) Performance@Number of Tokens for all 5 datasets.</p>
<p>Figure 27 .
27
Figure 27.Mixtral-8x7B-Instruct-v0.1: (a) Performance@Number of Queries Plots for all 5 datasets.(b) Performance@Number of Tokens for all 5 datasets.</p>
<p>Table 1 .
1
Figure 11.SC 2 with total tokens being the budget if caching is enabled.Probability between 0 to 1. Through our benchmark we found Yes or No to be the most calibrated.More details and calibration results can be found in Appendix D Self
Accuracy0.93 0.94 0.9505k GSM8k10k0.55 0.60 0.6505k MATH10k0.35 0.40 0.4505k Number of Tokens 10k TheoremQA0.66 0.67 0.6805k HotpotQA10k0.90 0.91 0.9205k CSQA10kGPT-4 SCGPT-4 SC^2DatasetCorrect Accuracy Incorrect Accuracy Total AccuracyGSM8K0.9920.1560.937MATH0.9110.4610.707TheoremQA0.9450.2320.547HotpotQA0.9940.0290.675CSQA0.9870.060.901
To arrive at the final format of doing self-evaluation, we benchmarked three types of evaluations: Yes or No, Score 1-10, and -evaluation accuracy on 5 datasets.Correct accuracy denotes self-evaluation accuracy for answers that turn out to be correct.Incorrect accuracy is the self-evaluation accuracy of incorrect answers.All numbers are done with GPT-4-0613</p>
<p>Table 2 .
2
Self-evaluation accuracy on MATH with three methods
0.8 1.0Yes or No Confidence Score ProbabilityAccuracy0.4 0.60.20.0Confidence 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Figure 15. Calibration result for the math reasoning datasets. Threedifferent self-evaluation methods are calibrated here.MethodCorrect Accuracy Incorrect Accuracy Total AccuracyYes or NO0.9110.4610.707Score 1-100.9950.1490.613Probability 0.0-1.00.8860.1150.537D. Self-EvaluationD.1. Self-Evaluation Method</p>
<p>However, it does require more encoded tokens (We can sample all of the m additional tokens as part Figure16.SC 2 with total tokens being the budget.There are sizable improvements in using our method SC 2 on math reasoning tasks.Figure 17.Separate proposer budget and evaluation budget on the dataset of MATH.
Accuracy0.93 0.94 0.9505k GSM8k10k0.55 0.60 0.6505k MATH10k0.35 0.40 0.4505k Number of Tokens 10k TheoremQA0.66 0.67 0.6805k HotpotQA10k0.90 0.91 0.9205k CSQA10kGPT-4 SCGPT-4 SC^2GPT4 CoT SCGPT4 SC^20.65Accuracy0.55 0.6010 Proposer Budget ($) 20 304002 Evaluator Budget ($) 46020 Total Budget ($)40MethodTop1 Best out of all Total AccuracyToT b=5 (GPT-4,GPT-4)0.740.760.4ToT b=3 (GPT-4,GPT-4)0.770.770.49ToT b=1 (GPT-4,GPT-4)0.650.650.65ToT eval once (GPT-4,GPT-4) 0.730.750.352CoT 100 times (GPT-4)0.170.560.0756ToT Random Eval (GPT-4)0.00.040.008ToT b=5 (GPT-3.5,GPT-3.5)0.250.350.11CoT 100 times (GPT-3.5)0.040.460.0252ToT b=5 (GPT-4,GPT-3.5)0.680.720.302ToT b=5 (GPT-3.5,GPT-4)0.30.380.156</p>
<p>Table 3 .
3
Various results on Game of 24.ToT refers to Tree-of-Thoughts.For ToT, the first model name in the parenthesis refers to the model used to generate the candidate thoughts, while the second model name refers to the model used to evaluate the candidate thoughts.
of a single query). Thus if one is self-hosting the model,&amp; Sabharwal (2023) shows that without any bound in thethis strategy has only marginal additional cost.number of steps, an encoder-decoder architecture with onlyone encoder and three decoder layers can simulate a TuringD.3.2. QUERY VS TOKEN BUDGETMachine and thus a single query to such a TransformerWhile we have discussed both query and token budget in this paper, token budget has some notable advantages as a metric.can perform computations with arbitrarily large amount of compute. Pérez et al. (2021) shows that even for decoder-only transformers, allowing for polynomial-sized chains of thought makes it powerful enough to do, in a single query,any computation a Turing Machine can do in polynomialTheoretical aspects Equivalence in the number of queriestime. While the number of queries metric fails to capturecan be arbitrarily far from the equivalence in the amount ofthis, by contrast, the number of tokens metric which is novelcompute. Merrill &amp; Sabharwal (2023) and Pérez et al. (2021)to our paper, does capture this aspect as it by definitionboth show that the expressive power of transformers canincludes the length of the generated thought as part of thebe greatly enhanced by generating intermediate steps in thecompute.computation (colloquially called chain of thought). Merrill
We also investigate a variant where we ask the model to think step by step before evaluating. While we see a small increase in performance for such a strategy, it also necessitates a big increase in the token budget. Further analysis is in the Supplement.
G.2. Three Other Reasoning StrategiesIn this section, we will evaluate on three other reasoning strategies in the self-consistency family: Plan and Solve(Wang et al., 2023), Least to Most Prompting(Zhou et al., 2022), and Progressive Hint Prompting(Zheng et al., 2023).Plan and Solve It asks LLMs to do some planning before solving a question.It is like an extension to CoT.Least to Most PromptingThis strategy prompts the model to decompose a question first and then answer each subquestion before aggregating them to the final answer.Progressive Hint Prompting This strategy uses previous answers as hints to generate next answer.All three new strategies here can be integrated with self-consistency seamlessly, since they are mostly just variants of chain-of-thought.Based on the plots, it seems that normal self-consistency is still very competitive, but different prompting styles can make a big difference.For some models and some datasets, a strategy other than CoT converges to a higher performance.This is strong evidence that self-consistency is a really budget-effective strategy.
Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Improving code generation by training with natural language feedback. A Chen, J Scheurer, T Korbak, J A Campos, J S Chan, S R Bowman, K Cho, E Perez, arXiv:2303.167492023aarXiv preprint</p>
<p>Introspective tips: Large language model for in-context decision making. L Chen, L Wang, H Dong, Y Du, J Yan, F Yang, S Li, P Zhao, S Qin, S Rajmohan, arXiv:2305.115982023barXiv preprint</p>
<p>W Chen, M Yin, M Ku, Y Wan, X Ma, J Xu, T Xia, X Wang, P Lu, Theoremqa, 10.48550/arXiv.2305.12524A theorem-driven question answering dataset. Conference on Empirical Methods in Natural Language Processing. 2023c</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, NeurIPS Datasets and Benchmarks. 2021</p>
<p>Fault-aware neural code rankers. J P Inala, C Wang, M Yang, A Codas, M Encarnación, S Lahiri, M Musuvathi, J Gao, Advances in Neural Information Processing Systems. 202235</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, arXiv:2310.06825Mistral 7b. 2023aarXiv preprint</p>
<p>Selfplanning code generation with large language model. X Jiang, Y Dong, L Wang, Q Shang, G Li, arXiv:2303.066892023barXiv preprint</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Making language models better reasoners with step-aware verifier. Y Li, Z Lin, S Zhang, Q Fu, B Chen, J.-G Lou, W Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. T Liang, Z He, W Jiao, X Wang, Y Wang, R Wang, Y Yang, Z Tu, S Shi, arXiv:2305.191182023arXiv preprint</p>
<p>Z Ling, Y Fang, X Li, Z Huang, M Lee, R Memisevic, H Su, arXiv:2306.03872Deductive verification of chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>The expresssive power of transformers with chain of thought. W Merrill, A Sabharwal, arXiv:2310.079232023arXiv preprint</p>
<p>Demystifying gpt self-repair for code generation. T X Olausson, J P Inala, C Wang, J Gao, A Solar-Lezama, arXiv:2306.098962023arXiv preprint</p>
<p>10.48550/arXiv.2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Attention is turingcomplete. J Pérez, P Barceló, J Marinkovic, Journal of Machine Learning Research. 22752021</p>
<p>Training language models with language feedback at scale. J Scheurer, J A Campos, T Korbak, J S Chan, A Chen, K Cho, E Perez, arXiv:2303.167552023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023arXiv preprint</p>
<p>Com-monsenseQA: A question answering challenge targeting commonsense knowledge. A Talmor, J Herzig, N Lourie, J Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>G Team, Gemini, arXiv:2312.11805A family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. K Tian, E Mitchell, A Zhou, A Sharma, R Rafailov, H Yao, C Finn, C D Manning, arXiv:2305.149752023arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Solving math word problems with process-and outcome-based feedback. J Uesato, N Kushman, R Kumar, F Song, N Siegel, L Wang, A Creswell, G Irving, I Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. L Wang, W Xu, Y Lan, Z Hu, Y Lan, R Lee, E.-P Lim, 10.48550/arXiv.2305.04091Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Generating natural language proofs with verifier-guided search. K Yang, J Deng, D Chen, arXiv:2205.124432022arXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. E Riloff, D Chiang, J Hockenmaier, J Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumOctober-November 2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Answering questions by metareasoning over multiple chains of thought. O Yoran, T Wolfson, B Bogin, U Katz, D Deutch, J Berant, arXiv:2304.130072023arXiv preprint</p>
<p>How language model hallucinations can snowball. M Zhang, O Press, W Merrill, A Liu, N A Smith, 10.48550/arXiv.2305.135342023</p>
<p>C Zheng, Z Liu, E Xie, Z Li, Y Li, arXiv:2304.09797Progressivehint prompting improves reasoning in large language models. 2023arXiv preprint</p>
<p>Leastto-most prompting enables complex reasoning in large language models. D Zhou, N Scharli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, O Bousquet, Q Le, Chi , E , 10.48550/arXiv.2205.10625International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-595 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-595</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-595</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-952af139e6a49c5b6490663be967d312c438334d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/952af139e6a49c5b6490663be967d312c438334d" target="_blank">Grammar Induction with Neural Language Models: An Unusual Replication</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is found that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.</p>
                <p><strong>Paper Abstract:</strong> A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e595.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e595.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRPN-replication</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replication experiments of the Parsing-Reading-Predict Network (PRPN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A replication and analysis of Shen et al. (2018)'s PRPN models (PRPN-LM and PRPN-UP) focusing on grammar induction via language modeling; the paper examines sources of variability and tests reproducibility across seeds, hyperparameter/configuration choices, early-stopping criteria, and training data splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PRPN (Parsing-Reading-Predict Network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / Grammar induction</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Unsupervised grammar induction evaluated via constituency parsing F1 and language modeling perplexity (LM objective) using PRPN variants (PRPN-LM tuned for LM, PRPN-UP tuned for parsing) trained on WSJ and AllNLI corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random initialization (random seeds); different model configurations (PRPN-LM vs PRPN-UP with different embedding sizes and hidden units); hyperparameters (embedding dimension, hidden layer sizes, maximum sentence length); vocabulary size (examples: 10k, 15.8k, 76k); early-stopping criterion (parsing objective 'UP' vs language modeling objective 'LM'); dataset / domain differences (WSJ vs AllNLI); use or absence of train/validation/test split (training on full WSJ including test set in some runs, i.e. data leakage); number of training epochs (WSJ: 100 epochs vs AllNLI: 15 epochs in their runs); duplicate sentences retained in AllNLI; evaluation-code differences (original code did not support PRPN-LM and was modified); greedy parsing decisions and model-internal stochasticity (implicit in neural training).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Reported per-metric sample mean and standard deviation for parsing F1 (notation mu(sigma) in tables), median perplexity for language modeling, and max values across runs; also report epoch at which early stopping occurred (approx. 13 vs 65).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Examples reported in the paper: parsing F1 (WSJ10) for PRPN-UP trained on AllNLI with UP stopping: 67.5 (0.6) (mu (sigma)); PRPN-LM trained on WSJ Train with LM stopping (WSJ10): 70.5 (0.4) (mu (sigma)). Language-modeling perplexity (WSJ test set, median): PRPN-LM (WSJ Train, LM, 10k vocab) = 61.4 (median); PRPN-LM (WSJ Train, UP, 10k vocab) = 81.6 (median); PRPN-UP (WSJ Train, LM, 10k vocab) = 92.8 (median); PRPN-UP (WSJ Train, LM, 15.8k vocab) = 112.1 (median). Early-stopping behavior: stopping on parsing objective led models to stop ~13th epoch, while stopping on LM led to stopping around ~65th epoch, with corresponding worse LM perplexity when stopped early. Number of independent runs used for reported averages: 5 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Reproduction attempted by re-running authors' public code; metrics used include unlabeled constituency parsing F1 (on WSJ10 and WSJ test), per-constituent-type accuracy (ADJP, NP, PP, INTJ), median perplexity (PPL) on WSJ test, and reporting of mean/std across 5 random-seed runs; comparisons made to original reported results, to multiple baselines (random, balanced, symbolic grammar induction baselines), and to alternative training splits (full WSJ vs WSJ train/valid/test split).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>They reproduce the main parsing result on full WSJ10 when training without a data split (consistent with Shen et al. (2018)'s reported result). Training on the (unlabeled) WSJ test set (i.e., not splitting) did not give a significant improvement in performance. PRPN-LM often achieves higher parsing F1 than PRPN-UP even though PRPN-UP had been tuned for parsing. PRPN models outperform other latent-tree baselines by a large margin on the reported parsing metrics. Quantitative examples: PRPN-LM (WSJ Train, LM, 10k) WSJ10 F1 = 70.5 (0.4) mu(sigma); PRPN-UP (AllNLI, UP) WSJ10 F1 = 67.5 (0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Identified problems include: tuning and (effectively) training on the test set in the original work (data leakage); differing hyperparameter configurations for different reported task variants (PRPN-LM vs PRPN-UP) making comparisons difficult; vocabulary-size mismatch across configurations; domain mismatch (AllNLI vs WSJ) and differing numbers of training epochs causing non-comparability; original evaluation code lacking support for PRPN-LM (required modification); duplicate sentences in AllNLI not removed; early stopping on the parsing objective causing incomplete LM training; and general sensitivity to random initialization (necessitating multiple runs).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use of multiple random initializations (5 different random seeds) and reporting mean and standard deviation; creation of proper train/validation/test splits for fairer evaluation (they reran experiments with WSJ train/valid/test splits); construction of a validation set for AllNLI by removing 10k random sentences from MultiNLI training and combining with SNLI dev set; adjusting vocabulary size to make fair comparisons (e.g., reducing PRPN-UP vocab from 15.8k to 10k); modifying evaluation code to support PRPN-LM; training for more epochs on datasets where possible; reporting median PPL across runs and max/min as relevant; explicitly comparing early-stopping on LM vs parsing objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative examples demonstrating effectiveness of controls: reducing vocabulary size improved perplexity for PRPN-UP from 112.1 (15.8k vocab) to 92.8 (10k vocab) on WSJ test (median PPL). Early-stopping on parsing vs LM produced worse LM performance: PRPN-LM (WSJ Train) median PPL = 61.4 when stopped on LM, but 81.6 when stopped on UP (parsing), indicating that using LM objective for early stopping yields substantially better LM results. Multiple-run reporting yields low reported standard deviations for parsing F1 in many settings (e.g., sigma ~0.4–0.8 in several reported rows), indicating limited run-to-run variance for those metrics in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>5 runs</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The replication shows variability in results arises from random initialization, hyperparameter/configuration differences (especially vocabulary size and model-size variants), stopping criterion, and dataset choices; the authors mitigated these by running 5 seeds and reporting mu(sigma), using fair train/validation/test splits, and adjusting vocabulary, and demonstrated that (a) early stopping on parsing harms LM performance (models stopped ~13 vs ~65 epochs), (b) reducing vocabulary improved PPL substantially (e.g., PRPN-UP PPL 112.1 -> 92.8), and (c) PRPN-LM is robust and often outperforms PRPN-UP and prior latent-tree baselines even under fairer experimental settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grammar Induction with Neural Language Models: An Unusual Replication', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural language modeling by jointly learning syntax and lexicon <em>(Rating: 2)</em></li>
                <li>Do latent tree learning models identify meaningful structure in sentences? <em>(Rating: 2)</em></li>
                <li>Learning to Compose Words into Sentences with Reinforcement Learning <em>(Rating: 1)</em></li>
                <li>Learning to compose task-specific tree structures <em>(Rating: 1)</em></li>
                <li>Assessing the ability of LSTMs to learn syntax-sensitive dependencies <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-595",
    "paper_id": "paper-952af139e6a49c5b6490663be967d312c438334d",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "PRPN-replication",
            "name_full": "Replication experiments of the Parsing-Reading-Predict Network (PRPN)",
            "brief_description": "A replication and analysis of Shen et al. (2018)'s PRPN models (PRPN-LM and PRPN-UP) focusing on grammar induction via language modeling; the paper examines sources of variability and tests reproducibility across seeds, hyperparameter/configuration choices, early-stopping criteria, and training data splits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PRPN (Parsing-Reading-Predict Network)",
            "model_size": null,
            "scientific_domain": "Natural language processing / Grammar induction",
            "experimental_task": "Unsupervised grammar induction evaluated via constituency parsing F1 and language modeling perplexity (LM objective) using PRPN variants (PRPN-LM tuned for LM, PRPN-UP tuned for parsing) trained on WSJ and AllNLI corpora.",
            "variability_sources": "Random initialization (random seeds); different model configurations (PRPN-LM vs PRPN-UP with different embedding sizes and hidden units); hyperparameters (embedding dimension, hidden layer sizes, maximum sentence length); vocabulary size (examples: 10k, 15.8k, 76k); early-stopping criterion (parsing objective 'UP' vs language modeling objective 'LM'); dataset / domain differences (WSJ vs AllNLI); use or absence of train/validation/test split (training on full WSJ including test set in some runs, i.e. data leakage); number of training epochs (WSJ: 100 epochs vs AllNLI: 15 epochs in their runs); duplicate sentences retained in AllNLI; evaluation-code differences (original code did not support PRPN-LM and was modified); greedy parsing decisions and model-internal stochasticity (implicit in neural training).",
            "variability_measured": true,
            "variability_metrics": "Reported per-metric sample mean and standard deviation for parsing F1 (notation mu(sigma) in tables), median perplexity for language modeling, and max values across runs; also report epoch at which early stopping occurred (approx. 13 vs 65).",
            "variability_results": "Examples reported in the paper: parsing F1 (WSJ10) for PRPN-UP trained on AllNLI with UP stopping: 67.5 (0.6) (mu (sigma)); PRPN-LM trained on WSJ Train with LM stopping (WSJ10): 70.5 (0.4) (mu (sigma)). Language-modeling perplexity (WSJ test set, median): PRPN-LM (WSJ Train, LM, 10k vocab) = 61.4 (median); PRPN-LM (WSJ Train, UP, 10k vocab) = 81.6 (median); PRPN-UP (WSJ Train, LM, 10k vocab) = 92.8 (median); PRPN-UP (WSJ Train, LM, 15.8k vocab) = 112.1 (median). Early-stopping behavior: stopping on parsing objective led models to stop ~13th epoch, while stopping on LM led to stopping around ~65th epoch, with corresponding worse LM perplexity when stopped early. Number of independent runs used for reported averages: 5 runs.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Reproduction attempted by re-running authors' public code; metrics used include unlabeled constituency parsing F1 (on WSJ10 and WSJ test), per-constituent-type accuracy (ADJP, NP, PP, INTJ), median perplexity (PPL) on WSJ test, and reporting of mean/std across 5 random-seed runs; comparisons made to original reported results, to multiple baselines (random, balanced, symbolic grammar induction baselines), and to alternative training splits (full WSJ vs WSJ train/valid/test split).",
            "reproducibility_results": "They reproduce the main parsing result on full WSJ10 when training without a data split (consistent with Shen et al. (2018)'s reported result). Training on the (unlabeled) WSJ test set (i.e., not splitting) did not give a significant improvement in performance. PRPN-LM often achieves higher parsing F1 than PRPN-UP even though PRPN-UP had been tuned for parsing. PRPN models outperform other latent-tree baselines by a large margin on the reported parsing metrics. Quantitative examples: PRPN-LM (WSJ Train, LM, 10k) WSJ10 F1 = 70.5 (0.4) mu(sigma); PRPN-UP (AllNLI, UP) WSJ10 F1 = 67.5 (0.6).",
            "reproducibility_challenges": "Identified problems include: tuning and (effectively) training on the test set in the original work (data leakage); differing hyperparameter configurations for different reported task variants (PRPN-LM vs PRPN-UP) making comparisons difficult; vocabulary-size mismatch across configurations; domain mismatch (AllNLI vs WSJ) and differing numbers of training epochs causing non-comparability; original evaluation code lacking support for PRPN-LM (required modification); duplicate sentences in AllNLI not removed; early stopping on the parsing objective causing incomplete LM training; and general sensitivity to random initialization (necessitating multiple runs).",
            "mitigation_methods": "Use of multiple random initializations (5 different random seeds) and reporting mean and standard deviation; creation of proper train/validation/test splits for fairer evaluation (they reran experiments with WSJ train/valid/test splits); construction of a validation set for AllNLI by removing 10k random sentences from MultiNLI training and combining with SNLI dev set; adjusting vocabulary size to make fair comparisons (e.g., reducing PRPN-UP vocab from 15.8k to 10k); modifying evaluation code to support PRPN-LM; training for more epochs on datasets where possible; reporting median PPL across runs and max/min as relevant; explicitly comparing early-stopping on LM vs parsing objectives.",
            "mitigation_effectiveness": "Quantitative examples demonstrating effectiveness of controls: reducing vocabulary size improved perplexity for PRPN-UP from 112.1 (15.8k vocab) to 92.8 (10k vocab) on WSJ test (median PPL). Early-stopping on parsing vs LM produced worse LM performance: PRPN-LM (WSJ Train) median PPL = 61.4 when stopped on LM, but 81.6 when stopped on UP (parsing), indicating that using LM objective for early stopping yields substantially better LM results. Multiple-run reporting yields low reported standard deviations for parsing F1 in many settings (e.g., sigma ~0.4–0.8 in several reported rows), indicating limited run-to-run variance for those metrics in this work.",
            "comparison_with_without_controls": true,
            "number_of_runs": "5 runs",
            "key_findings": "The replication shows variability in results arises from random initialization, hyperparameter/configuration differences (especially vocabulary size and model-size variants), stopping criterion, and dataset choices; the authors mitigated these by running 5 seeds and reporting mu(sigma), using fair train/validation/test splits, and adjusting vocabulary, and demonstrated that (a) early stopping on parsing harms LM performance (models stopped ~13 vs ~65 epochs), (b) reducing vocabulary improved PPL substantially (e.g., PRPN-UP PPL 112.1 -&gt; 92.8), and (c) PRPN-LM is robust and often outperforms PRPN-UP and prior latent-tree baselines even under fairer experimental settings.",
            "uuid": "e595.0",
            "source_info": {
                "paper_title": "Grammar Induction with Neural Language Models: An Unusual Replication",
                "publication_date_yy_mm": "2018-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural language modeling by jointly learning syntax and lexicon",
            "rating": 2
        },
        {
            "paper_title": "Do latent tree learning models identify meaningful structure in sentences?",
            "rating": 2
        },
        {
            "paper_title": "Learning to Compose Words into Sentences with Reinforcement Learning",
            "rating": 1
        },
        {
            "paper_title": "Learning to compose task-specific tree structures",
            "rating": 1
        },
        {
            "paper_title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
            "rating": 1
        }
    ],
    "cost": 0.010093,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grammar Induction with Neural Language Models: An Unusual Replication</h1>
<p>Phu Mon Htut ${ }^{1}$<br>AdeptMind Scholar<br>pmh330@nyu.edu<br>${ }^{1}$ Center for Data Science New York University 60 Fifth Avenue New York, NY 10011</p>
<p>Kyunghyun Cho ${ }^{1,2}$<br>CIFAR Global Scholar<br>kyunghyun.cho@nyu.edu<br>${ }^{2}$ Dept. of Computer Science<br>New York University<br>60 Fifth Avenue<br>New York, NY 10011</p>
<p>Samuel R. Bowman ${ }^{1,2,3}$<br>bowman@nyu.edu<br>${ }^{3}$ Dept. of Linguistics<br>New York University<br>10 Washington Place<br>New York, NY 10003</p>
<h4>Abstract</h4>
<p>A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.</p>
<h2>1 Introduction and Background</h2>
<p>Work on grammar induction attempts to find methods for syntactic parsing that do not require expensive and difficult-to-design expertlabeled treebanks for training (Charniak and Carroll, 1992; Klein and Manning, 2002; Smith and Eisner, 2005). Recent work on latent tree learning offers a new family of approaches to the problem (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). Latent tree learning models attempt to induce syntactic structure using the supervision from a downstream NLP task such as textual entailment. Though these models tend to show good task performance, they are often not evaluated using standard parsing metrics, and Williams et al.
(2018a) report that the parses they produce tend to be no better than random trees in a standard evaluation on the full Wall Street Journal section of the Penn Treebank (WSJ; Marcus et al., 1993).</p>
<p>This paper addresses the Parsing-ReadingPredict Network (PRPN; Shen et al., 2018), which was recently published at ICLR, and which reports near-state-of-the-art results on language modeling and strong results on grammar induction, a first for latent tree models (though they do not use that term). PRPN is built around a substantially novel architecture, and uses convolutional networks with a form of structured attention (Kim et al., 2017) rather than recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011) to evaluate and learn trees while performing straightforward backpropagation training on a language modeling objective. In this work, we aim to understand what the PRPN model learns that allows it to succeed, and to identify the conditions under which this success is possible.</p>
<p>Their experiments on language modeling and parsing are carried out using different configurations of the PRPN model, which were claimed to be optimized for the corresponding tasks. PRPNLM is tuned for language modeling performance, and PRPN-UP for (unsupervised) parsing performance. In the parsing experiments, we also observe that the WSJ data is not split, such that the test data is used without parse information for training. This approach follows the previous works on grammar induction using non-neural models where the entire dataset is used for training (Klein and Manning, 2002). However, this implies that the parsing results of PRPN-UP may not be generalizable in the way usually expected of machine learning evaluation results. Additionally, it is not obvious that the model should be able to learn to parse reliably: (1) Since the parser is trained as part of a language model, it makes pars-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left Parses from PRPN-LM trained on AllNLI. Right Parses from PRPN-UP trained on AllNLI (stopping criterion: parsing). We can observe that both sets of parses tend to have roughly reasonable high-level structure and tend to identify noun phrases correctly.
ing decisions greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning.</p>
<p>In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of textual entailment, and which we include for comparison. We then evaluate the constituency trees produced by these models on the WSJ test set, full WSJ10, ${ }^{1}$ and the MultiNLI development set.</p>
<p>Our results indicate that PRPN-LM achieves better parsing performance than PRPN-UP on both WSJ and WSJ10 even though PRPN-UP was tuned-at least to some extent-for parsing. Surprisingly, a PRPN-LM model trained on the large out-of-domain AllNLI dataset achieves the best parsing performance on WSJ despite not being tuned for parsing. We also notice that vocabulary size affects the language modeling significantlythe perplexity gets higher as the vocabulary size increases.</p>
<p>Overall, despite the relatively uninformative experimental design used in Shen et al. (2018), we find that PRPN is an effective model. It outperforms all latent tree learning baselines by large</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>margins on both WSJ and MultiNLI, and performs competitively with symbolic grammar induction systems on WSJ10, suggesting that PRPN in particular and language modeling in general are a viable setting for latent tree learning.</p>
<h2>2 Methods</h2>
<p>PRPN consists of three components: (i) a parsing network that uses a two-layer convolution kernel to calculate the syntactic distance between successive pairs of words, which can form an indirect representation of the constituency structure of the sentence, (ii) a recurrent reading network that summarizes the current memory state based on all previous memory states and the implicit constituent structure, and (iii) a predict network that uses the memory state to predict the next token. We refer readers to the appendix and the original work for details.</p>
<p>We do not re-implement or re-tune PRPN, but rather attempt to replicate and understand the results of the work using the author's publicly available code. ${ }^{2}$ The experiments on language modeling and parsing are carried out using different configurations of the model, with substantially different hyperparameter values including the size of the word embeddings, the maximum sentence length, the vocabulary size, and the sizes of hidden layers. PRPN-LM is larger than PRPN-UP, with embedding layer that is 4 times larger and the number of units per layer that is 3 times larger. We use both versions of the model in all our experiments.</p>
<p>We use the 49 k -sentence WSJ corpus in two settings. To replicate the original results, we re-run an experiment with no train/test split, and for a clearer picture of the model's performance, we run it again with the train (Section 0-21 of WSJ), validation (Section 22 of WSJ), and test (Section 23</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Training <br> Data</th>
<th style="text-align: center;">Stopping <br> Criterion</th>
<th style="text-align: center;">Vocab <br> Size</th>
<th style="text-align: center;">Parsing F1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Depth <br> WSJ</th>
<th style="text-align: center;">Accuracy on WSJ by Tag</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WSJ10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WSJ</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mu(\sigma)$</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">$\mu(\sigma)$</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ADJP</td>
<td style="text-align: center;">NP</td>
<td style="text-align: center;">PP</td>
<td style="text-align: center;">INTJ</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">76k</td>
<td style="text-align: center;">67.5 (0.6)</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">36.9 (0.6)</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">76k</td>
<td style="text-align: center;">66.3 (0.8)</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">38.3 (0.5)</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-LM</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">76k</td>
<td style="text-align: center;">52.4 (4.9)</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">35.0 (5.4)</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">WSJ Full</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">15.8 k</td>
<td style="text-align: center;">64.7 (3.2)</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">26.4 (1.7)</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">WSJ Full</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">15.8 k</td>
<td style="text-align: center;">64.3 (3.3)</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">26.3 (1.8)</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">15.8 k</td>
<td style="text-align: center;">63.5 (3.5)</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">26.2 (2.3)</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">15.8 k</td>
<td style="text-align: center;">62.2 (3.9)</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">26.0 (2.3)</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-LM</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">70.5 (0.4)</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">37.4 (0.3)</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-LM</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">66.1 (0.5)</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">33.4 (0.8)</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">300D ST-Gumbel</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">19.0 (1.0)</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: center;">w/o Leaf GRU</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.8 (1.6)</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">300D RL-SPINN</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">13.2 (0.0)</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">50.6</td>
</tr>
<tr>
<td style="text-align: center;">w/o Leaf GRU</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">13.1 (0.1)</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">CCM</td>
<td style="text-align: center;">WSJ10 Full</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">DMV+CCM</td>
<td style="text-align: center;">WSJ10 Full</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">UML-DOP</td>
<td style="text-align: center;">WSJ10 Full</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Random Trees</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">21.3 (0.0)</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: center;">Balanced Trees</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.3 (0.0)</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">55.9</td>
</tr>
<tr>
<td style="text-align: center;">Left Branching</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">13.1 (0.0)</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Right Branching</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">16.5 (0.0)</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: Unlabeled parsing F1 results evaluated on full WSJ10 and WSJ test set broken down by training data and by early stopping criterion. The Accuracy columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses. Italics mark results that are worse than the random baseline. Underlining marks the best results from our runs. Results with RL-SPINN and ST-Gumbel are from Williams et al. (2018a), and are evaluated on the full WSJ. We run the model with 5 different random seeds to calculate the average F1. We use the model with the best F1 score to report ADJP, NP, PP, and INTJ. WSJ10 baselines are from Klein and Manning (2002, CCM), Klein and Manning (2005, DMV+CCM), and Bod (2006, UML-DOP). As the WSJ10 baselines are trained using additional information such as POS tags and dependency parser, they are not strictly comparable with the latent tree learning results.
of WSJ) splits. To compare PRPN to the models studied in Williams et al. (2018a), we also retrain it on AllNLI. As the MultiNLI test set is not publicly available, we follow Williams et al. (2018a) and use the development set for testing. The parsing evaluation code in the original codebase does not support PRPN-LM, and we modify it in our experiments only to add this support.</p>
<p>For early stopping, we remove 10k random sentences from the MultiNLI training set and combine them with the SNLI development set to create a validation set. Our AllNLI training set contains 280.5 K unique sentences ( 1.8 M sentences in total including duplicate premise sentences), and covers six distinct genres of spoken and written English. We do not remove the duplicate sentences. We train the model for 100 epochs for WSJ and 15 epochs for AllNLI. We run the model five times with random initializations and average the results
from the five runs. The generated parses from the trained models with the best F1 scores and the pre-trained model that provides the highest F1 are available online. ${ }^{3}$</p>
<h2>3 Experimental Results</h2>
<p>Table 2 shows our results for language modeling. PRPN-UP, configured as-is with parsing criterion and language modeling criterion, performs dramatically worse than the standard PRPN-LM (a vs. d and e). However, this is not a fair comparison as the larger vocabulary gives PRPN-UP a harder task to solve. Adjusting the vocabulary of PRPNUP down to 10 k to make a fairer comparison possible, the PPL of PRPN-UP improves significantly (c vs. d), but not enough to match PRPN-LM (a vs. c). We also observe that early stopping on</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">Stopping</th>
<th style="text-align: center;">Vocab</th>
<th style="text-align: center;">PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Data</td>
<td style="text-align: center;">Criterion</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">Median</td>
</tr>
<tr>
<td style="text-align: left;">(a) PRPN-LM</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">10 k</td>
<td style="text-align: center;">$\mathbf{6 1 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">(b) PRPN-LM</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">10 k</td>
<td style="text-align: center;">81.6</td>
</tr>
<tr>
<td style="text-align: left;">(c) PRPN-UP</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">10 k</td>
<td style="text-align: center;">92.8</td>
</tr>
<tr>
<td style="text-align: left;">(d) PRPN-UP</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">15.8 k</td>
<td style="text-align: center;">112.1</td>
</tr>
<tr>
<td style="text-align: left;">(e) PRPN-UP</td>
<td style="text-align: center;">WSJ Train</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">15.8 k</td>
<td style="text-align: center;">112.8</td>
</tr>
<tr>
<td style="text-align: left;">(f) PRPN-UP</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">76 k</td>
<td style="text-align: center;">797.5</td>
</tr>
<tr>
<td style="text-align: left;">(g) PRPN-UP</td>
<td style="text-align: center;">AllNLI Train</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">76 k</td>
<td style="text-align: center;">848.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Language modeling performance (perplexity) on the WSJ test set, broken down by training data used and by whether early stopping is done using the parsing objective (UP) or the language modeling objective (LM).
parsing leads to incomplete training and a substantial decrease in perplexity (a vs. b and d vs. e). The models stop training at around the 13th epoch when we early-stop on parsing objective, while they stop training around the 65th epoch when we early-stop on language modeling objective. Both PRPN models trained on AllNLI do even worse (f and g), though the mismatch in vocabulary and domain may explain this effect. In addition, since it takes much longer to train PRPN on the larger AllNLI dataset, we train PRPN on AllNLI for only 15 epochs while we train the PRPN on WSJ for 100 epochs. Although the parsing objective converges within 15 epochs, we notice that language modeling perplexity is still improving. We expect that the perplexity of the PRPN models trained on AllNLI could be lower if we increase the number of training epochs.</p>
<p>Turning toward parsing performance, Table 1 shows results with all the models under study, plus several baselines, on WSJ test set and full WSJ10. On full WSJ10, we reproduce the main parsing result of Shen et al. (2018) with their UP model trained on WSJ without a data split. We also find the choice of parse quality as an early stopping criterion does not have a substantial effect and that training on the (unlabeled) test set does not give a significant improvement in performance. In addition and unexpectedly, we observe that PRPN-LM models achieve higher parsing performance than PRPN-UP. This shows that any tuning done to separate PRPN-UP from PRPN-LM was not necessary, and more importantly, that the results described in the paper can be largely reproduced by a unified model in a fair setting. Moreover, the PRPN models trained on WSJ achieves</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Stopping <br> Criterion</th>
<th style="text-align: center;">F1 wrt.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LB</td>
<td style="text-align: center;">RB</td>
<td style="text-align: center;">SP</td>
<td style="text-align: center;">Depth</td>
</tr>
<tr>
<td style="text-align: center;">300D SPINN</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;">w/o Leaf GRU</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;">300D SPINN-NC</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">6.1</td>
</tr>
<tr>
<td style="text-align: center;">w/o Leaf GRU</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">6.3</td>
</tr>
<tr>
<td style="text-align: center;">300D ST-Gumbel</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">4.1</td>
</tr>
<tr>
<td style="text-align: center;">w/o Leaf GRU</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">4.6</td>
</tr>
<tr>
<td style="text-align: center;">300D RL-SPINN</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: center;">w/o Leaf GRU</td>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-LM</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">UP</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">PRPN-UP</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">Random Trees</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">4.4</td>
</tr>
<tr>
<td style="text-align: center;">Balanced Trees</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">3.9</td>
</tr>
</tbody>
</table>
<p>Table 3: Unlabeled parsing F1 on the MultiNLI development set for models trained on AllNLI. F1 wrt. shows F1 with respect to strictly right- and left-branching ( $\mathrm{LB} / \mathrm{RB}$ ) trees and with respect to the Stanford Parser (SP) trees supplied with the corpus; The evaluations of SPINN, RL-SPINN, and ST-Gumbel are from Williams et al. (2018a). SPINN is a supervised parsing model, and the others are latent tree models. Median F1 of each model trained with 5 different random seeds is reported.
comparable results with CCM (Klein and Manning, 2002). The PRPN models are outperformed by DMV+CCM(Klein and Manning, 2005), and UML-DOP(Bod, 2006). However, these models use additional information such as POS and dependency parser so they are not strictly comparable with the PRPN models.</p>
<p>Turning to the WSJ test set, the results look somewhat different: Although the differences in WSJ10 performance across models are small, the same is not true for the WSJ in terms of average F1. PRPN-LM outperforms all the other models on WSJ test set, even the potentially-overfit PRPN-UP model. Moreover, the PRPN models trained on the larger, out-of-domain AllNLI perform better than those trained on WSJ. Surprisingly, PRPN-LM tained on out-of-domain AllNLI achieves the best F1 score on WSJ test set among all the models we experimented, even though its performance on WSJ10 is the lowest of all. This mean that PRPN-LM trained on AllNLI is strikingly good at parsing longer sentences though its performance on shorter sentences is worse than other models. Under all the configurations we tested, the PRPN model yields much better per-</p>
<p>formance than the baselines from Yogatama et al. (2017, called RL-SPINN) and Choi et al. (2018, called ST-Gumbel), despite the fact that the model was tuned exclusively for WSJ10 parsing. This suggests that PRPN is consistently effective at latent tree learning.</p>
<p>We also show detailed results for several specific constituent types, following Williams et al. (2018a). We observe that the accuracy for NP (noun phrases) on the WSJ test set is above $46 \%$ (Table 1) for all PRPN models, much higher than any of the baseline models. These runs also perform substantially better than the random baseline in the two other categories Williams et al. (2018a) report: ADJP (adjective phrases) and PP (prepositional phrases). However, as WSJ test set contains only one INTJ (interjection phrases), the results on INTJ are either $0.0 \%$ or $100 \%$.</p>
<p>In addition, Table 3 shows that the PRPN-UP models achieve the median parsing F1 scores of 46.3 and 48.6 respectively on the MultiNLI dev set while PRPN-LM performs the median F1 of 45.7; setting the state of the art in parsing performance on this dataset among latent tree models by a large margin. We conclude that PRPN does acquire some substantial knowledge of syntax, and that this knowledge agrees with Penn Treebank (PTB) grammar significantly better than chance.</p>
<p>Qualitatively, the parses produced by most of the best performing PRPN models are relatively balanced (F1 score of 36.5 w.r.t balanced trees) and tend toward right branching (F1 score of 42.0 with respect to balanced trees). They are also shallower than average ground truth PTB parsed trees. These models can parse short sentences relatively well, as shown by their high WSJ10 performance.</p>
<p>For a large proportion of long sentences, most of the best performing models can produce reasonable constituents (Table 1). The best performing model, PRPN-LM trained on AllNLI, achieves the best accuracy at identifying ADJP (adjective phrases), PP (prepositional phrases), and INTJ (interjection phrases) constituents, and a high accuracy on NP (noun phrases). In a more informal inspection, we also observe that our best PRPNLM and PRPN-UP runs are fairly good at pairing determiners with NPs as we can observe in Figure 1). Although lower level tree constituents appear random in many cases for both PRPN-LM and PRPN-UP, the intermediate and higher-level constituents are generally reasonable. For exam-
ple, in Figure 1, although the parse for lower level constituents like The entire Minoan seem random, the higher-level constituents, such as The entire Minoan civilization and nothing worth seeing in the tourist offices, are reasonable.</p>
<h2>4 Conclusion</h2>
<p>In our attempt to replicate the grammar induction results reported in Shen et al. (2018), we find several experimental design problems that make the results difficult to interpret. However, in experiments and analyses going well beyond the scope of the original paper, we find that the PRPN model presented in that work is nonetheless robust. It represents a viable method for grammar induction and the first clear success for latent tree learning with neural networks, and we expect that it heralds further work on language modeling as a tool for grammar induction research.</p>
<h2>Acknowledgments</h2>
<p>This project has benefited from financial support to SB by Google and Tencent Holdings, and was partly supported by Samsung Electronics (Improving Deep Learning using Latent Structure). We thank Adina Williams, Katharina Kann, Ryan Cotterell, and the anonymous reviewers for their helpful comments and suggestions, and NVIDIA for their support.</p>
<h2>References</h2>
<p>Rens Bod. 2006. An All-Subtrees Approach to Unsupervised Parsing. Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 865-872.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.</p>
<p>Eugene Charniak and Glen Carroll. 1992. Two experiments on learning probabilistic dependency grammars from corpora. In Proceedings of the AAAI Workshop on Statistically-Based NLP Techniques, page 113 .</p>
<p>Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-specific tree structures. In Proceedings of the Thirty-Second Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence (AAAI-18), volume 2.</p>
<p>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, 21-26 July, 2004, Barcelona, Spain., pages 111-118.</p>
<p>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Proceedings of International Conference on Neural Networks (ICNN'96).</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1996. Long Short Term Memory. Memory, (1993):1-28.</p>
<p>Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. 2017. Structured attention networks.</p>
<p>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL '02, page 128.</p>
<p>Dan Klein and Christopher D. Manning. 2005. Natural language grammar induction with a generative constituent-context model. Pattern Recognition, 38(9):1407-1419.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of lstms to learn syntaxsensitive dependencies. TACL, 4:521-535.</p>
<p>Jean Maillard, Stephen Clark, and Dani Yogatama. 2017. Jointly learning sentence embeddings and syntax with unsupervised Tree-LSTMs. arXiv preprint 1705.09189.</p>
<p>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313-330.</p>
<p>Yikang Shen, Zhouhan Lin, Chin wei Huang, and Aaron Courville. 2018. Neural language modeling by jointly learning syntax and lexicon. In International Conference on Learning Representations.</p>
<p>Noah A. Smith and Jason Eisner. 2005. Guiding unsupervised grammar induction using contrastive estimation. In Proceedings of IJCAI Workshop on Grammatical Inference Applications, pages 73-82.</p>
<p>Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and Chris Manning. 2011. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In Proceedings of the 28th International Conference on Machine Learning, pages 129-136.</p>
<p>Adina Williams, Andrew Drozdov, and Samuel R. Bowman. 2018a. Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association for Computational Linguistics (TACL).</p>
<p>Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018b. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2017. Learning to Compose Words into Setences with Reinforcement Learning. Proceedings of the International Conference on Learning Representations, pages 1-17.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/nyu-mll/
PRPN-Analysis&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://github.com/yikangshen/PRPN&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
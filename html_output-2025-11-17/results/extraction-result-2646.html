<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2646 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2646</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2646</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-de28ec1d7bd38c8fc4e8ac59b6133800818b4e29</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29" target="_blank">ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing</a></p>
                <p><strong>Paper Venue:</strong> BioNLP@ACL</p>
                <p><strong>Paper TL;DR:</strong> ScispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library is described, which detail the performance of two packages of models released in scispa Cy and demonstrate their robustness on several tasks and datasets.</p>
                <p><strong>Paper Abstract:</strong> Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2646.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2646.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>scispaCy candidate generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>scispaCy UMLS Candidate Generator (approximate nearest-neighbors over TF-IDF character 3-grams)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based candidate generation module in scispaCy that maps mention surface forms to UMLS concepts by performing approximate nearest-neighbors search over TF-IDF vectors of character 3-grams, with alias expansion and abbreviation handling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>scispaCy UMLS Candidate Generator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encodes each UMLS concept and its aliases as TF-IDF vectors over character 3-grams (kept if they appear in >=10 entity names/aliases). Builds an index over these vectors for approximate nearest-neighbor (ANN) retrieval. Given a mention, it retrieves the top-K nearest neighbor strings, expands those strings to all linked UMLS concept identifiers (handling ambiguous aliases), and returns the candidate concept set. To reduce failure on abbreviations, the pipeline substitutes detected abbreviations with their long-form definitions using an unsupervised abbreviation detection step (Schwartz & Hearst) before querying the index. Cached vectors for 2.78M concepts occupy ~1.1 GB on disk.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented / vector-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical / medicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Intrinsic evaluation via gold candidate generation recall on annotated corpora (MedMentions) by varying K (number of retrieved neighbors). Compared recall curves against Murty et al. (2018) and measured average candidate counts and recall improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Released code and candidate-generation data (cached vectors) and specification of UMLS subset (sections 0,1,2,9) used; reported dataset sizes and average candidate counts (e.g., for K=100 average retrieved candidates 54.26 ± 12.45).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>MedMentions (used to evaluate gold candidate generation recall); UMLS 2017 AA reduced subset (2.78M concepts) used as KB; evaluations reported relative to Murty et al. (2018).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported ~5 percentage point absolute improvement in gold candidate generation recall over Murty et al. (2018) while generating ~46% fewer candidates on average; for K=100 the system retrieved 54.26 ± 12.45 candidate entities on average (max 164). (Exact recall values per K are shown in Figure 3 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Direct comparison to Murty et al. (2018): +5% absolute recall with 46% fewer candidates; compared against gold mentions in MedMentions.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Coverage limited to the publicly distributable subset of UMLS (sections 0,1,2,9) used; long-tail aliases can produce many candidate concepts (max reported 164 candidates for K=100); abbreviation variants were a notable failure mode and required explicit unsupervised abbreviation detection to improve recall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2646.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2646.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SchwartzHearst</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Schwartz and Hearst abbreviation detection algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised algorithm to identify abbreviation–definition pairs in biomedical text, used to replace short-form mentions with their long forms before downstream candidate generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A simple algorithm for identifying abbreviation definitions in biomedical text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Schwartz & Hearst abbreviation detection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An unsupervised pattern-based algorithm that locates parenthetical abbreviation definitions (and other patterns) and pairs short forms with their likely long-form expansions in text; in this paper it is applied as a preprocessing step to expand abbreviations before performing ANN-based concept candidate lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>rule-based / unsupervised</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical / NLP preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Used as an ablation/augmentation for candidate generation; Figure 3 in the paper shows that applying abbreviation expansion improves gold candidate recall.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Algorithm is a published, well-known unsupervised method (Schwartz & Hearst 2002) and was implemented in the scispaCy pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>MedMentions (used to evaluate impact on candidate recall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improves gold candidate generation recall (exact improvement per-K shown in paper's Figure 3); no numeric percentage reported in-text beyond the plotted curves.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>When incorporated into the candidate generation pipeline, recall improves relative to the pipeline without abbreviation expansion; compared against Murty et al. (2018) baseline in overall candidate generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only handles abbreviations that can be linked by its pattern-based heuristics; not all abbreviations are captured.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2646.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2646.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Literome</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literome: PubMed-scale genomic knowledge base</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale information-extraction pipeline / search engine that aggregates genic pathway interactions and genotype–phenotype interactions mined from PubMed; created by Poon et al. and cited here as large-scale automated extraction used to assist curation and discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Literome: PubMed-scale genomic knowledge base in the cloud</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Literome (PubMed-scale genomic knowledge base / extraction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A large-scale extraction system that processes PubMed abstracts to extract genic pathway interactions and genotype–phenotype relationships; developed from large-scale IE/distant supervision workflows and used to provide a searchable knowledge base of extracted interactions across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>distant-supervision-augmented information extraction / knowledge-base construction</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genomics / biomedical literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Implicit: produces large collections of extracted interactions/events from literature that can be used to generate hypotheses about gene/pathway relationships; extraction was performed via distant supervision and event extraction rules/triggers operating over dependency representations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Large-scale IE pipeline validated by producing an indexable knowledge base and by comparing extracted interactions to literature; the scispaCy paper cites Poon et al. (2015) and Poon et al. (2014) as producing 1.5M cancer pathway interactions (Poon et al. 2015) which enabled Literome.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Relies on distant supervision and pattern-based extraction using dependency paths and hand-written triggers—these constrain extractions to learned or curated patterns (mitigates arbitrary hallucination but does not quantify false positive rates in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>PubMed abstracts at scale; compared extractions to curated resources (as reported in originating works).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported outcome in related work: extraction of ~1.5 million cancer pathway interactions from PubMed abstracts (Poon et al., 2015); exact precision/recall figures not reported in this scispaCy paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cited reliance on handwritten rules and triggers for event extraction based on dependency paths; such rule-based approaches may limit recall and generalization and require careful engineering and maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2646.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2646.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Poon2015 distant supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distant supervision for cancer pathway extraction from text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distant-supervision-based information extraction pipeline (Poon et al., 2015) that extracts cancer pathway interactions at large scale from PubMed abstracts to populate knowledge bases such as Literome.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distant supervision for cancer pathway extraction from text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Distant-supervision cancer pathway extraction (Poon et al. 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline employing distant supervision to map known pathway/interaction patterns from structured resources onto text, then extract candidate interactions/events from PubMed abstracts; uses pattern matching and event extraction heuristics operating on dependency tree paths to produce large-scale interaction datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>distant-supervision information extraction / rule-augmented IE</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cancer biology / biomedical literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate interaction assertions by extracting relation/event mentions across large corpora; aggregated extractions can be used to surface novel or under-supported pathway hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Aggregated extractions validated indirectly by scale (e.g., number of extracted interactions) and by downstream use in resources such as Literome; scispaCy paper cites Poon et al. (2015) for the extraction counts but does not report detailed validation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Extraction constrained by dependency-path based rules/triggers and distant supervision mappings to KBs, which reduce unconstrained generation.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>PubMed abstracts (large-scale processing); evaluation reported in originating work (Poon et al. 2015).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported extraction scale: ~1.5 million cancer pathway interactions (cited in scispaCy paper referencing Poon et al. 2015).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reliance on distant supervision and hand-written dependency-path triggers; potential for systematic extraction errors and coverage gaps due to rule reliance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2646.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2646.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Valenzuela-Escarcega2018</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale automated machine reading (Valenzuela-Escarcega et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A machine-reading system reported to discover new cancer-driving mechanisms by large-scale automated reading of the literature; cited as an example of automated discovery in biomedical NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large-scale automated machine reading discovers new cancer-driving mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large-scale automated machine reading (Valenzuela-Escarcega et al. 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A pipeline for automated machine reading of the biomedical literature that extracts biological events and relations at scale; the scispaCy paper cites it as producing discoveries (new cancer-driving mechanisms) and notes use of handwritten rules and triggers based on dependency tree paths in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large-scale information extraction / machine reading (rule- and pattern-based event extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cancer biology / biomedical literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate mechanistic assertions by extracting and aggregating events/relations from literature using machine-reading pipelines; candidate assertions can be interpreted as hypotheses about mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Cited as having produced discoveries; specific validation procedures (e.g., experimental follow-up or expert curation) are not detailed within the scispaCy paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Uses constrained extraction via rules/triggers on dependency paths (as do similar IE systems), which limits unconstrained generation of spurious assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied over PubMed-scale corpora; original paper contains experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The scispaCy paper cites Valenzuela-Escarcega et al. (2018) as demonstrating automated discovery of cancer-driving mechanisms but does not report numeric performance metrics within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Cited as discovering new cancer-driving mechanisms (details in the original Valenzuela-Escarcega et al. 2018 paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>In scispaCy's discussion, such systems typically rely on hand-written rules and dependency-path based triggers, which implies limited generalization and a need for manual engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing', 'publication_date_yy_mm': '2019-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Literome: PubMed-scale genomic knowledge base in the cloud <em>(Rating: 2)</em></li>
                <li>Distant supervision for cancer pathway extraction from text <em>(Rating: 2)</em></li>
                <li>Large-scale automated machine reading discovers new cancer-driving mechanisms <em>(Rating: 2)</em></li>
                <li>Hierarchical losses and new resources for fine-grained entity typing and linking <em>(Rating: 1)</em></li>
                <li>A simple algorithm for identifying abbreviation definitions in biomedical text <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2646",
    "paper_id": "paper-de28ec1d7bd38c8fc4e8ac59b6133800818b4e29",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "scispaCy candidate generator",
            "name_full": "scispaCy UMLS Candidate Generator (approximate nearest-neighbors over TF-IDF character 3-grams)",
            "brief_description": "A retrieval-based candidate generation module in scispaCy that maps mention surface forms to UMLS concepts by performing approximate nearest-neighbors search over TF-IDF vectors of character 3-grams, with alias expansion and abbreviation handling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "scispaCy UMLS Candidate Generator",
            "system_description": "Encodes each UMLS concept and its aliases as TF-IDF vectors over character 3-grams (kept if they appear in &gt;=10 entity names/aliases). Builds an index over these vectors for approximate nearest-neighbor (ANN) retrieval. Given a mention, it retrieves the top-K nearest neighbor strings, expands those strings to all linked UMLS concept identifiers (handling ambiguous aliases), and returns the candidate concept set. To reduce failure on abbreviations, the pipeline substitutes detected abbreviations with their long-form definitions using an unsupervised abbreviation detection step (Schwartz & Hearst) before querying the index. Cached vectors for 2.78M concepts occupy ~1.1 GB on disk.",
            "system_type": "retrieval-augmented / vector-based retrieval",
            "scientific_domain": "biomedical / medicine",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Intrinsic evaluation via gold candidate generation recall on annotated corpora (MedMentions) by varying K (number of retrieved neighbors). Compared recall curves against Murty et al. (2018) and measured average candidate counts and recall improvements.",
            "reproducibility_measures": "Released code and candidate-generation data (cached vectors) and specification of UMLS subset (sections 0,1,2,9) used; reported dataset sizes and average candidate counts (e.g., for K=100 average retrieved candidates 54.26 ± 12.45).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "MedMentions (used to evaluate gold candidate generation recall); UMLS 2017 AA reduced subset (2.78M concepts) used as KB; evaluations reported relative to Murty et al. (2018).",
            "performance_metrics": "Reported ~5 percentage point absolute improvement in gold candidate generation recall over Murty et al. (2018) while generating ~46% fewer candidates on average; for K=100 the system retrieved 54.26 ± 12.45 candidate entities on average (max 164). (Exact recall values per K are shown in Figure 3 of the paper.)",
            "comparison_with_baseline": "Direct comparison to Murty et al. (2018): +5% absolute recall with 46% fewer candidates; compared against gold mentions in MedMentions.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Coverage limited to the publicly distributable subset of UMLS (sections 0,1,2,9) used; long-tail aliases can produce many candidate concepts (max reported 164 candidates for K=100); abbreviation variants were a notable failure mode and required explicit unsupervised abbreviation detection to improve recall.",
            "uuid": "e2646.0",
            "source_info": {
                "paper_title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "SchwartzHearst",
            "name_full": "Schwartz and Hearst abbreviation detection algorithm",
            "brief_description": "An unsupervised algorithm to identify abbreviation–definition pairs in biomedical text, used to replace short-form mentions with their long forms before downstream candidate generation.",
            "citation_title": "A simple algorithm for identifying abbreviation definitions in biomedical text",
            "mention_or_use": "use",
            "system_name": "Schwartz & Hearst abbreviation detection",
            "system_description": "An unsupervised pattern-based algorithm that locates parenthetical abbreviation definitions (and other patterns) and pairs short forms with their likely long-form expansions in text; in this paper it is applied as a preprocessing step to expand abbreviations before performing ANN-based concept candidate lookup.",
            "system_type": "rule-based / unsupervised",
            "scientific_domain": "biomedical / NLP preprocessing",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Used as an ablation/augmentation for candidate generation; Figure 3 in the paper shows that applying abbreviation expansion improves gold candidate recall.",
            "reproducibility_measures": "Algorithm is a published, well-known unsupervised method (Schwartz & Hearst 2002) and was implemented in the scispaCy pipeline.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "MedMentions (used to evaluate impact on candidate recall)",
            "performance_metrics": "Improves gold candidate generation recall (exact improvement per-K shown in paper's Figure 3); no numeric percentage reported in-text beyond the plotted curves.",
            "comparison_with_baseline": "When incorporated into the candidate generation pipeline, recall improves relative to the pipeline without abbreviation expansion; compared against Murty et al. (2018) baseline in overall candidate generation performance.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Only handles abbreviations that can be linked by its pattern-based heuristics; not all abbreviations are captured.",
            "uuid": "e2646.1",
            "source_info": {
                "paper_title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Literome",
            "name_full": "Literome: PubMed-scale genomic knowledge base",
            "brief_description": "A large-scale information-extraction pipeline / search engine that aggregates genic pathway interactions and genotype–phenotype interactions mined from PubMed; created by Poon et al. and cited here as large-scale automated extraction used to assist curation and discovery.",
            "citation_title": "Literome: PubMed-scale genomic knowledge base in the cloud",
            "mention_or_use": "mention",
            "system_name": "Literome (PubMed-scale genomic knowledge base / extraction pipeline)",
            "system_description": "A large-scale extraction system that processes PubMed abstracts to extract genic pathway interactions and genotype–phenotype relationships; developed from large-scale IE/distant supervision workflows and used to provide a searchable knowledge base of extracted interactions across the literature.",
            "system_type": "distant-supervision-augmented information extraction / knowledge-base construction",
            "scientific_domain": "genomics / biomedical literature mining",
            "hypothesis_generation_method": "Implicit: produces large collections of extracted interactions/events from literature that can be used to generate hypotheses about gene/pathway relationships; extraction was performed via distant supervision and event extraction rules/triggers operating over dependency representations.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Large-scale IE pipeline validated by producing an indexable knowledge base and by comparing extracted interactions to literature; the scispaCy paper cites Poon et al. (2015) and Poon et al. (2014) as producing 1.5M cancer pathway interactions (Poon et al. 2015) which enabled Literome.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Relies on distant supervision and pattern-based extraction using dependency paths and hand-written triggers—these constrain extractions to learned or curated patterns (mitigates arbitrary hallucination but does not quantify false positive rates in this paper).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "PubMed abstracts at scale; compared extractions to curated resources (as reported in originating works).",
            "performance_metrics": "Reported outcome in related work: extraction of ~1.5 million cancer pathway interactions from PubMed abstracts (Poon et al., 2015); exact precision/recall figures not reported in this scispaCy paper.",
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Cited reliance on handwritten rules and triggers for event extraction based on dependency paths; such rule-based approaches may limit recall and generalization and require careful engineering and maintenance.",
            "uuid": "e2646.2",
            "source_info": {
                "paper_title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Poon2015 distant supervision",
            "name_full": "Distant supervision for cancer pathway extraction from text",
            "brief_description": "A distant-supervision-based information extraction pipeline (Poon et al., 2015) that extracts cancer pathway interactions at large scale from PubMed abstracts to populate knowledge bases such as Literome.",
            "citation_title": "Distant supervision for cancer pathway extraction from text",
            "mention_or_use": "mention",
            "system_name": "Distant-supervision cancer pathway extraction (Poon et al. 2015)",
            "system_description": "A pipeline employing distant supervision to map known pathway/interaction patterns from structured resources onto text, then extract candidate interactions/events from PubMed abstracts; uses pattern matching and event extraction heuristics operating on dependency tree paths to produce large-scale interaction datasets.",
            "system_type": "distant-supervision information extraction / rule-augmented IE",
            "scientific_domain": "cancer biology / biomedical literature mining",
            "hypothesis_generation_method": "Generates candidate interaction assertions by extracting relation/event mentions across large corpora; aggregated extractions can be used to surface novel or under-supported pathway hypotheses.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Aggregated extractions validated indirectly by scale (e.g., number of extracted interactions) and by downstream use in resources such as Literome; scispaCy paper cites Poon et al. (2015) for the extraction counts but does not report detailed validation protocols.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Extraction constrained by dependency-path based rules/triggers and distant supervision mappings to KBs, which reduce unconstrained generation.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "PubMed abstracts (large-scale processing); evaluation reported in originating work (Poon et al. 2015).",
            "performance_metrics": "Reported extraction scale: ~1.5 million cancer pathway interactions (cited in scispaCy paper referencing Poon et al. 2015).",
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Reliance on distant supervision and hand-written dependency-path triggers; potential for systematic extraction errors and coverage gaps due to rule reliance.",
            "uuid": "e2646.3",
            "source_info": {
                "paper_title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
                "publication_date_yy_mm": "2019-02"
            }
        },
        {
            "name_short": "Valenzuela-Escarcega2018",
            "name_full": "Large-scale automated machine reading (Valenzuela-Escarcega et al., 2018)",
            "brief_description": "A machine-reading system reported to discover new cancer-driving mechanisms by large-scale automated reading of the literature; cited as an example of automated discovery in biomedical NLP.",
            "citation_title": "Large-scale automated machine reading discovers new cancer-driving mechanisms",
            "mention_or_use": "mention",
            "system_name": "Large-scale automated machine reading (Valenzuela-Escarcega et al. 2018)",
            "system_description": "A pipeline for automated machine reading of the biomedical literature that extracts biological events and relations at scale; the scispaCy paper cites it as producing discoveries (new cancer-driving mechanisms) and notes use of handwritten rules and triggers based on dependency tree paths in related work.",
            "system_type": "large-scale information extraction / machine reading (rule- and pattern-based event extraction)",
            "scientific_domain": "cancer biology / biomedical literature mining",
            "hypothesis_generation_method": "Generates candidate mechanistic assertions by extracting and aggregating events/relations from literature using machine-reading pipelines; candidate assertions can be interpreted as hypotheses about mechanisms.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Cited as having produced discoveries; specific validation procedures (e.g., experimental follow-up or expert curation) are not detailed within the scispaCy paper.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Uses constrained extraction via rules/triggers on dependency paths (as do similar IE systems), which limits unconstrained generation of spurious assertions.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied over PubMed-scale corpora; original paper contains experimental details.",
            "performance_metrics": "The scispaCy paper cites Valenzuela-Escarcega et al. (2018) as demonstrating automated discovery of cancer-driving mechanisms but does not report numeric performance metrics within this paper.",
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": "Cited as discovering new cancer-driving mechanisms (details in the original Valenzuela-Escarcega et al. 2018 paper).",
            "limitations": "In scispaCy's discussion, such systems typically rely on hand-written rules and dependency-path based triggers, which implies limited generalization and a need for manual engineering.",
            "uuid": "e2646.4",
            "source_info": {
                "paper_title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
                "publication_date_yy_mm": "2019-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Literome: PubMed-scale genomic knowledge base in the cloud",
            "rating": 2
        },
        {
            "paper_title": "Distant supervision for cancer pathway extraction from text",
            "rating": 2
        },
        {
            "paper_title": "Large-scale automated machine reading discovers new cancer-driving mechanisms",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical losses and new resources for fine-grained entity typing and linking",
            "rating": 1
        },
        {
            "paper_title": "A simple algorithm for identifying abbreviation definitions in biomedical text",
            "rating": 2
        }
    ],
    "cost": 0.01536675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing</h1>
<p>Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar<br>Allen Institute for Artificial Intelligence, Seattle, WA, USA<br>{markn,daniel,beltagy,waleeda}@allenai.org</p>
<h4>Abstract</h4>
<p>Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https:// allenai.github.io/scispacy/.</p>
<h2>1 Introduction</h2>
<p>The publication rate in the medical and biomedical sciences is growing at an exponential rate (Bornmann and Mutz, 2014). The information overload problem is widespread across academia, but is particularly apparent in the biomedical sciences, where individual papers may contain specific discoveries relating to a dizzying variety of genes, drugs, and proteins. In order to cope with the sheer volume of new scientific knowledge, there have been many attempts to automate the process of extracting entities, relations, protein interactions and other structured knowledge from scientific papers (Wei et al., 2016; Ammar et al., 2018; Poon et al., 2014).</p>
<p>Although there exists a wealth of tools for processing biomedical text, many focus primarily on named entity recognition and disambiguation. MetaMap and MetaMapLite (Aronson, 2001; Demner-Fushman et al., 2017), the two most widely used and supported tools for biomedical text processing, support entity linking with negation detection and acronym resolution. However,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Growth of the annual number of cited references from 1650 to 2012 in the medical and health sciences (citing publications from 1980 to 2012). Figure from (Bornmann and Mutz, 2014).
tools which cover more classical natural language processing (NLP) tasks such as the GENIA tagger (Tsuruoka et al., 2005; Tsuruoka and Tsujii, 2005), or phrase structure parsers such as those presented in McClosky and Charniak (2008) typically do not make use of new research innovations such as word representations or neural networks.</p>
<p>In this paper, we introduce scispaCy, a specialized NLP library for processing biomedical texts which builds on the robust spaCy library, ${ }^{1}$ and document its performance relative to state of the art models for part of speech (POS) tagging, dependency parsing, named entity recognition (NER) and sentence segmentation. Specifically, we:</p>
<ul>
<li>Release a reformatted version of the GENIA 1.0 (Kim et al., 2003) corpus converted into Universal Dependencies v1.0 and aligned with the original text from the PubMed abstracts.
${ }^{1}$ spacy.io</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Vocab Size</th>
<th style="text-align: left;">Vector <br> Count</th>
<th style="text-align: left;">Min <br> Word <br> Freq</th>
<th style="text-align: left;">Min <br> Doc <br> Freq</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">en_core_sci_sm</td>
<td style="text-align: left;">58,338</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">en_core_sci_md</td>
<td style="text-align: left;">101,678</td>
<td style="text-align: left;">98,131</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">5</td>
</tr>
</tbody>
</table>
<p>Table 1: Vocabulary statistics for the two core packages in scispaCy.</p>
<ul>
<li>Benchmark 9 named entity recognition models for more specific entity extraction applications demonstrating competitive performance when compared to strong baselines.</li>
<li>Release and evaluate two fast and convenient pipelines for biomedical text, which include tokenization, part of speech tagging, dependency parsing and named entity recognition.</li>
</ul>
<h2>2 Overview of (sci)spaCy</h2>
<p>In this section, we briefly describe the models used in the spaCy library and describe how we build on them in scispaCy.
spaCy. The Python-based spaCy library (Honnibal and Montani, 2017) ${ }^{2}$ provides a variety of practical tools for text processing in multiple languages. Their models have emerged as the defacto standard for practical NLP due to their speed, robustness and close to state of the art performance. As the spaCy models are popular and the spaCy API is widely known to many potential users, we choose to build upon the spaCy library for creating a biomedical text processing pipeline.
scispaCy. Our goal is to develop scispaCy as a robust, efficient and performant NLP library to satisfy the primary text processing needs in the biomedical domain. In this release of scispaCy, we retrain spaCy ${ }^{3}$ models for POS tagging, dependency parsing, and NER using datasets relevant to biomedical text, and enhance the tokenization module with additional rules. scispaCy contains two core released packages: en_core_sci_sm and en_core_sci_md. Models in the en_core_sci_md package have a larger vocabulary and include word vectors, while those in en_core_sci_sm have a smaller vocabulary and do not include word vectors, as shown in Table 1.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Wall clock comparison of different publicly available biomedical NLP pipelines. All experiments run on a single machine with 12 Intel(R) Core(TM) i7-6850K CPU @ 3.60GHz and 62GB RAM. For the Biaffine Parser, a pre-compiled Tensorflow binary with support for AVX2 instructions was used in a good faith attempt to optimize the implementation. Dynet does support the Intel MKL, but requires compilation from scratch and as such, does not represent an "off the shelf" system. TF is short for Tensorflow.</p>
<p>Processing Speed. To emphasize the efficiency and practical utility of the end-to-end pipeline provided by scispaCy packages, we perform a speed comparison with several other publicly available processing pipelines for biomedical text using 10k randomly selected PubMed abstracts. We report results with and without segmenting the abstracts into sentences since some of the libraries (e.g., GENIA tagger) are designed to operate on sentences.</p>
<p>As shown in Table 2, both models released in scispaCy demonstrate competitive speed to pipelines written in C++ and Java, languages designed for production settings.</p>
<p>Whilst scispaCy is not as fast as pipelines designed for purely production use-cases (e.g., NLP4J), it has the benefit of straightforward integration with the large ecosystem of Python libraries for machine learning and text processing. Although the comparison in Table 2 is not an apples to apples comparison with other frameworks (different tasks, implementation languages etc), it is useful to understand scispaCy's runtime in the context of other pipeline components. Running scispaCy models in addition to standard Entity Linking software such as MetaMap would result in only a marginal increase in overall runtime.</p>
<p>In the following section, we describe the POS taggers and dependency parsers in scispaCy.</p>
<h2>3 POS Tagging and Dependency Parsing</h2>
<p>The joint POS tagging and dependency parsing model in spaCy is an arc-eager transition-based parser trained with a dynamic oracle, similar to Goldberg and Nivre (2012). Features are CNN representations of token features and shared across all pipeline models (Kiperwasser and Goldberg, 2016; Zhang and Weiss, 2016). Next, we describe the data we used to train it in scispaCy.</p>
<h3>3.1 Datasets</h3>
<p>GENIA 1.0 Dependencies. To train the dependency parser and part of speech tagger in both released models, we convert the treebank of McClosky and Charniak (2008), ${ }^{4}$ which is based on the GENIA 1.0 corpus (Kim et al., 2003), to Universal Dependencies v1.0 using the Stanford Dependency Converter (Schuster and Manning, 2016). As this dataset has POS tags annotated, we use it to train the POS tagger jointly with the dependency parser in both released models.</p>
<p>As we believe the Universal Dependencies converted from the original GENIA 1.0 corpus are generally useful, we have released them as a separate contribution of this paper. ${ }^{5}$ In this data release, we also align the converted dependency parses to their original text spans in the raw, untokenized abstracts from the original release, ${ }^{6}$ and include the PubMed metadata for the abstracts which was discarded in the GENIA corpus released by McClosky and Charniak (2008). We hope that this raw format can emerge as a resource for practical evaluation in the biomedical domain of core NLP tasks such as tokenization, sentence segmentation and joint models of syntax.</p>
<p>Finally, we also retrieve from PubMed the original metadata associated with each abstract. This includes relevant named entities linked to their Medical Subject Headings (MeSH terms) as well as chemicals and drugs linked to a variety of ontologies, as well as author metadata, publication dates, citation statistics and journal metadata. We hope that the community can find interesting problems for which such natural supervision can be used.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Part of Speech tagging results on the GENIA Test set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Package/Model</th>
<th style="text-align: center;">UAS</th>
<th style="text-align: center;">LAS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Stanford-NNdep</td>
<td style="text-align: center;">89.02</td>
<td style="text-align: center;">87.56</td>
</tr>
<tr>
<td style="text-align: left;">NLP4J-dep</td>
<td style="text-align: center;">90.25</td>
<td style="text-align: center;">88.87</td>
</tr>
<tr>
<td style="text-align: left;">jPTDP-v1</td>
<td style="text-align: center;">91.89</td>
<td style="text-align: center;">90.27</td>
</tr>
<tr>
<td style="text-align: left;">Stanford-Biaffine-v2</td>
<td style="text-align: center;">92.64</td>
<td style="text-align: center;">91.23</td>
</tr>
<tr>
<td style="text-align: left;">Stanford-Biaffine-v2(Gold POS)</td>
<td style="text-align: center;">92.84</td>
<td style="text-align: center;">91.92</td>
</tr>
<tr>
<td style="text-align: left;">en_core_sci_sm - SD</td>
<td style="text-align: center;">90.31</td>
<td style="text-align: center;">88.65</td>
</tr>
<tr>
<td style="text-align: left;">en_core_sci_md - SD</td>
<td style="text-align: center;">90.66</td>
<td style="text-align: center;">88.98</td>
</tr>
<tr>
<td style="text-align: left;">en_core_sci_sm</td>
<td style="text-align: center;">89.69</td>
<td style="text-align: center;">87.67</td>
</tr>
<tr>
<td style="text-align: left;">en_core_sci_md</td>
<td style="text-align: center;">90.60</td>
<td style="text-align: center;">88.79</td>
</tr>
</tbody>
</table>
<p>Table 4: Dependency Parsing results on the GENIA 1.0 corpus converted to dependencies using the Stanford Universal Dependency Converter. We additionally provide evaluations using Stanford Dependencies(SD) in order for comparison relative to the results reported in (Nguyen and Verspoor, 2018).</p>
<p>OntoNotes 5.0. To increase the robustness of the dependency parser and POS tagger to generic text, we make use of the OntoNotes 5.0 corpus ${ }^{7}$ when training the dependency parser and part of speech tagger (Weischedel et al., 2011; Hovy et al., 2006). The OntoNotes corpus consists of multiple genres of text, annotated with syntactic and semantic information, but we only use POS and dependency parsing annotations in this work.</p>
<h3>3.2 Experiments</h3>
<p>We compare our models to the recent survey study of dependency parsing and POS tagging for biomedical data (Nguyen and Verspoor, 2018) in Tables 3 and 4. POS tagging results show that both models released in scispaCy are competitive with state of the art systems, and can be considered of</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>equivalent practical value. In the case of dependency parsing, we find that the Biaffine parser of Dozat and Manning (2016) outperforms the scispaCy models by a margin of $2-3 \%$. However, as demonstrated in Table 2, the scispaCy models are approximately 9 x faster due to the speed optimizations in spaCy. ${ }^{8}$</p>
<p>Robustness to Web Data. A core principle of the scispaCy models is that they are useful on a wide variety of types of text with a biomedical focus, such as clinical notes, academic papers, clinical trials reports and medical records. In order to make our models robust across a wider range of domains more generally, we experiment with incorporating training data from the OntoNotes 5.0 corpus when training the dependency parser and POS tagger. Figure 2 demonstrates the effectiveness of adding increasing percentages of web data, showing substantially improved performance on OntoNotes, at no reduction in performance on biomedical text. Note that mixing in web text during training has been applied to previous systems - the GENIA Tagger (Tsuruoka et al., 2005) also employs this technique.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Unlabeled attachment score (UAS) performance for an en_core_sci_md model trained with increasing amounts of web data incorporated. Table shows mean of 3 random seeds.</p>
<h2>4 Named Entity Recognition</h2>
<p>The NER model in spaCy is a transition-based system based on the chunking model from Lample et al. (2016). Tokens are represented as hashed, embedded representations of the prefix, suffix, shape and lemmatized features of individ-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ual words. Next, we describe the data we used to train NER models in scispaCy.</p>
<h3>4.1 Datasets</h3>
<p>The main NER model in both released packages in scispaCy is trained on the mention spans in the MedMentions dataset (Murty et al., 2018). Since the MedMentions dataset was originally designed for entity linking, this model recognizes a wide variety of entity types, as well as non-standard syntactic phrases such as verbs and modifiers, but the model does not predict the entity type. In order to provide for users with more specific requirements around entity types, we release four additional packages en_ner ${\mathbf{b c 5 c d r} \mid$ craft $\mid \mathbf{j n l p b a} \mid$ bionlp13cg $} . \mathbf{m d}$ with finer-grained NER models trained on BC5CDR (for chemicals and diseases; Li et al., 2016), CRAFT (for cell types, chemicals, proteins, genes; Bada et al., 2011), JNLPBA (for cell lines, cell types, DNAs, RNAs, proteins; Collier and Kim, 2004) and BioNLP13CG (for cancer genetics; Pyysalo et al., 2015), respectively.</p>
<h3>4.2 Experiments</h3>
<p>As NER is a key task for other biomedical text processing tasks, we conduct a through evaluation of the suitability of scispaCy to provide baseline performance across a wide variety of datasets. In particular, we retrain the spaCy NER model on each of the four datasets mentioned earlier (BC5CDR, CRAFT, JNLPBA, BioNLP13CG) as well as five more datasets in Crichton et al. (2017): AnatEM, BC2GM, BC4CHEMD, Linnaeus, NCBI-Disease. These datasets cover a wide variety of entity types required by different biomedical domains, including cancer genetics, disease-drug interactions, pathway analysis and trial population extraction. Additionally, they vary considerably in size and number of entities. For example, BC4CHEMD (Krallinger et al., 2015) has 84,310 annotations while Linnaeus (Gerner et al., 2009) only has 4,263. BioNLP13CG (Pyysalo et al., 2015) annotates 16 entity types while five of the datasets only annotate a single entity type. ${ }^{9}$</p>
<p>Table 5 provides a thorough comparison of the scispaCy NER models compared to a variety of models. In particular, we compare the models to</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>strong baselines which do not consider the use of 1) multi-task learning across multiple datasets and 2) semi-supervised learning via large pretrained language models. Overall, we find that the scispaCy models are competitive baselines for 5 of the 9 datasets.</p>
<p>Additionally, in Table 6 we evaluate the recall of the pipeline mention detector available in both scispaCy models (trained on the MedMentions dataset) against all 9 specialised NER datasets. Overall, we observe a modest drop in average recall when compared directly to the MedMentions results in Table 7, but considering the diverse domains of the 9 specialised NER datasets, achieving this level of recall across datasets is already nontrivial.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">sci_sm</th>
<th style="text-align: left;">sci_md</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BC5CDR</td>
<td style="text-align: left;">75.62</td>
<td style="text-align: left;">78.79</td>
</tr>
<tr>
<td style="text-align: left;">CRAFT</td>
<td style="text-align: left;">58.28</td>
<td style="text-align: left;">58.03</td>
</tr>
<tr>
<td style="text-align: left;">JNLPBA</td>
<td style="text-align: left;">67.33</td>
<td style="text-align: left;">70.36</td>
</tr>
<tr>
<td style="text-align: left;">BioNLP13CG</td>
<td style="text-align: left;">58.93</td>
<td style="text-align: left;">60.25</td>
</tr>
<tr>
<td style="text-align: left;">AnatEM</td>
<td style="text-align: left;">56.55</td>
<td style="text-align: left;">57.94</td>
</tr>
<tr>
<td style="text-align: left;">BC2GM</td>
<td style="text-align: left;">54.87</td>
<td style="text-align: left;">56.89</td>
</tr>
<tr>
<td style="text-align: left;">BC4CHEMD</td>
<td style="text-align: left;">60.60</td>
<td style="text-align: left;">60.75</td>
</tr>
<tr>
<td style="text-align: left;">Linnaeus</td>
<td style="text-align: left;">67.48</td>
<td style="text-align: left;">68.61</td>
</tr>
<tr>
<td style="text-align: left;">NCBI-Disease</td>
<td style="text-align: left;">65.76</td>
<td style="text-align: left;">65.65</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">62.81</td>
<td style="text-align: left;">64.14</td>
</tr>
</tbody>
</table>
<p>Table 6: Recall on the test sets of 9 specialist NER datasets, when the base mention detector is trained on MedMentions. The base mention detector is available in both en_core_sci_sm and en_core_sci_md models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">en_core_sci_sm</td>
<td style="text-align: center;">69.22</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">68.19</td>
</tr>
<tr>
<td style="text-align: left;">en_core_sci_md</td>
<td style="text-align: center;">70.44</td>
<td style="text-align: center;">67.56</td>
<td style="text-align: center;">68.97</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of the base mention detector on the MedMentions Corpus.</p>
<h2>5 Candidate Generation for Entity Linking</h2>
<p>In addition to Named Entity Recognition, scispaCy contains some initial groundwork needed to build an Entity Linking model designed to link to a subset of the Unified Medical Language System (UMLS; Bodenreider, 2004). This reduced subset is comprised of sections $0,1,2$ and 9 (SNOMED) of the UMLS 2017 AA release, which are publicly
distributable. It contains 2.78 M unique concepts and covers $99 \%$ of the mention concepts present in the MedMentions dataset (Murty et al., 2018).</p>
<h3>5.1 Candidate Generation</h3>
<p>To generate candidate entities for linking a given mention, we use an approximate nearest neighbours search over our subset of UMLS concepts and concept aliases and output the entities associated with the nearest K. Concepts and aliases are encoded using the vector of TF-IDF scores of character 3-grams which appears in 10 or more entity names or aliases (i.e., document frequency $\geq$ 10). In total, all data associated with the candidate generator including cached vectors for 2.78 M concepts occupies 1.1 GB of space on disk.</p>
<p>Aliases. Canonical concepts in UMLS have aliases - common names of drugs, alternative spellings, and otherwise words or phrases that are often linked to a given concept. Importantly, aliases may be shared across concepts, such as "cancer" for the canonical concepts of both "Lung Cancer" and "Breast Cancer". Since the nearest neighbor search is based on the surface forms, it returns K string values. However, because a given string may be an alias for multiple concepts, the list of K nearest neighbor strings may not translate to a list of K candidate entities. This is the correct implementation in practice, because given a possibly ambiguous alias, it is beneficial to score all plausible concepts, but it does mean that we cannot determine the exact number of candidate entities that will be generated for a given value of K . In practice, the number of retrieved candidates for a given K is much lower than K itself, with the exception of a few long tail aliases, which are aliases for a large number of concepts. For example, for $\mathrm{K}=100$, we retrieve $54.26 \pm 12.45$ candidates, with the max number of candidates for a single mention being 164.</p>
<p>Abbreviations. During development of the candidate generator, we noticed that abbreviated mentions account for a substantial proportion of the failure cases where none of the generated candidates match the correct entity. To partially remedy this, we implement the unsupervised abbreviation detection algorithm of Schwartz and Hearst (2002), substituting mention candidates marked as abbreviations for their long form definitions before searching for their nearest neighbours. Figure 3 demonstrates the improved recall of gold concepts</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Baseline</th>
<th style="text-align: left;">SOTA</th>
<th style="text-align: left;">+ Resources</th>
<th style="text-align: left;">sci.sm</th>
<th style="text-align: left;">sci.md</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BC5CDR (Li et al., 2016)</td>
<td style="text-align: left;">83.87</td>
<td style="text-align: left;">86.92 b</td>
<td style="text-align: left;">$89.69^{\text {bb }}$</td>
<td style="text-align: left;">78.83</td>
<td style="text-align: left;">83.92</td>
</tr>
<tr>
<td style="text-align: left;">CRAFT (Bada et al., 2011)</td>
<td style="text-align: left;">79.55</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">72.31</td>
<td style="text-align: left;">76.17</td>
</tr>
<tr>
<td style="text-align: left;">JNLPBA (Collier and Kim, 2004)</td>
<td style="text-align: left;">68.95</td>
<td style="text-align: left;">$73.48^{\text {b }}$</td>
<td style="text-align: left;">$75.50^{\text {bb }}$</td>
<td style="text-align: left;">71.78</td>
<td style="text-align: left;">73.21</td>
</tr>
<tr>
<td style="text-align: left;">BioNLP13CG (Pyysalo et al., 2015)</td>
<td style="text-align: left;">76.74</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">72.98</td>
<td style="text-align: left;">77.60</td>
</tr>
<tr>
<td style="text-align: left;">AnatEM (Pyysalo and Ananiadou, 2014)</td>
<td style="text-align: left;">88.55</td>
<td style="text-align: left;">$91.61^{<em> </em>}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">80.13</td>
<td style="text-align: left;">84.14</td>
</tr>
<tr>
<td style="text-align: left;">BC2GM (Smith et al., 2008)</td>
<td style="text-align: left;">84.41</td>
<td style="text-align: left;">$80.51^{\text {b }}$</td>
<td style="text-align: left;">$81.69^{\text {bb }}$</td>
<td style="text-align: left;">75.77</td>
<td style="text-align: left;">78.30</td>
</tr>
<tr>
<td style="text-align: left;">BC4CHEMD (Krallinger et al., 2015)</td>
<td style="text-align: left;">82.32</td>
<td style="text-align: left;">$88.75^{\text {a }}$</td>
<td style="text-align: left;">$89.37^{\text {aa }}$</td>
<td style="text-align: left;">82.24</td>
<td style="text-align: left;">84.55</td>
</tr>
<tr>
<td style="text-align: left;">Linnaeus (Gerner et al., 2009)</td>
<td style="text-align: left;">79.33</td>
<td style="text-align: left;">$95.68^{<em> </em>}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">79.20</td>
<td style="text-align: left;">81.74</td>
</tr>
<tr>
<td style="text-align: left;">NCBI-Disease (Dogan et al., 2014)</td>
<td style="text-align: left;">77.82</td>
<td style="text-align: left;">$85.80^{\text {b }}$</td>
<td style="text-align: left;">$87.34^{\text {bb }}$</td>
<td style="text-align: left;">79.50</td>
<td style="text-align: left;">81.65</td>
</tr>
</tbody>
</table>
<p>bb: LM model from Sachan et al. (2017) b: LSTM model from Sachan et al. (2017)
a: Single Task model from Wang et al. (2018) aa: Multi-task model from Wang et al. (2018)
** Evaluations use dictionaries developed without a clear train/test split.
Table 5: Test F1 Measure on NER for the small and medium scispaCy models compared to a variety of strong baselines and state of the art models. The Baseline and SOTA (State of the Art) columns include only single models which do not use additional resources, such as language models, or additional sources of supervision, such as multi-task learning. + Resources allows any type of supervision or pretraining. All scispaCy results are the mean of 5 random seeds.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Gold Candidate Generation Recall for different values of K. Note that K refers to the number of nearest neighbour queries, and not the number of considered candidates. Murty et al. (2018) do not report this distinction, but for a given K the same amount of work is done (retrieving K neighbours from the index), so results are comparable. For all K, the actual number of candidates is considerably lower on average.
for various values of K nearest neighbours. Our candidate generator provides a $5 \%$ absolute improvement over Murty et al. (2018) despite generating $46 \%$ fewer candidates per mention on average.</p>
<h2>6 Sentence Segmentation and Citation Handling</h2>
<p>Accurate sentence segmentation is required for many practical applications of natural language processing. Biomedical data presents many dif-
ficulties for standard sentence segmentation algorithms: abbreviated names and noun compounds containing punctuation are more common, whilst the wide range of citation styles can easily be misidentified as sentence boundaries.</p>
<p>We evaluate sentence segmentation using both sentence and full-abstract accuracy when segmenting PubMed abstracts from the raw, untokenized GENIA development set (the Sent/Abstract columns in Table 8).</p>
<p>Additionally, we examine the ability of the segmentation learned by our model to generalise to the body text of PubMed articles. Body text is typically more complex than abstract text, but in particular, it contains citations, which are considerably less frequent in abstract text. In order to examine the effectiveness of our models in this scenario, we design the following synthetic experiment. Given sentences from Cohan et al. (2019) which were originally designed for citation intent prediction, we run these sentences individually through our models. As we know that these sentences should be single sentences, we can simply count the frequency with which our models segment the individual sentences containing citations into multiple sentences (the Citation column in Table 8).</p>
<p>As demonstrated by Table 8, training the dependency parser on in-domain data (both the scispaCy models) completely obviates the need for rule-based sentence segmentation. This is a positive result - rule based sentence segmentation is</p>
<p>a brittle, time consuming process, which we have replaced with a domain specific version of an existing pipeline component.</p>
<p>Both scispaCy models are released with the custom tokeniser, but without a custom sentence segmenter by default.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Sent</th>
<th style="text-align: left;">Abstract</th>
<th style="text-align: left;">Citation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">web-small</td>
<td style="text-align: left;">$88.2 \%$</td>
<td style="text-align: left;">$67.5 \%$</td>
<td style="text-align: left;">$74.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">web-small + ct</td>
<td style="text-align: left;">$86.6 \%$</td>
<td style="text-align: left;">$62.1 \%$</td>
<td style="text-align: left;">$88.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">web-small + cs</td>
<td style="text-align: left;">$91.9 \%$</td>
<td style="text-align: left;">$77.0 \%$</td>
<td style="text-align: left;">$87.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">web-small + cs + ct</td>
<td style="text-align: left;">$92.1 \%$</td>
<td style="text-align: left;">$78.3 \%$</td>
<td style="text-align: left;">$94.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">sci-small + ct</td>
<td style="text-align: left;">$97.2 \%$</td>
<td style="text-align: left;">$81.7 \%$</td>
<td style="text-align: left;">$97.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">sci-small + cs + ct</td>
<td style="text-align: left;">$97.2 \%$</td>
<td style="text-align: left;">$81.7 \%$</td>
<td style="text-align: left;">$98.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">sci-med + ct</td>
<td style="text-align: left;">$97.3 \%$</td>
<td style="text-align: left;">$81.7 \%$</td>
<td style="text-align: left;">$98.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">sci-med + cs + ct</td>
<td style="text-align: left;">$97.4 \%$</td>
<td style="text-align: left;">$81.7 \%$</td>
<td style="text-align: left;">$98.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: Sentence segmentation performance for the core spaCy and scispaCy models. $\mathbf{c s}=$ custom rule based sentence segmenter and $\mathbf{c t}=$ custom rule based tokenizer, both designed explicitly to handle citations and common patterns in biomedical text.</p>
<h2>7 Related Work</h2>
<p>Apache cTakes (Savova et al., 2010) was designed specifically for clinical notes rather than the broader biomedical domain. MetaMap and MetaMapLite (Aronson, 2001; Demner-Fushman et al., 2017) from the National Library of Medicine focus specifically on entity linking using the Unified Medical Language System (UMLS) (Bodenreider, 2004) as a knowledge base. Buyko et al. adapt Apache OpenNLP using the GENIA corpus, but their system is not openly available and is less suitable for modern, Python-based workflows. The GENIA Tagger (Tsuruoka et al., 2005) provides the closest comparison to scispaCy due to it's multi-stage pipeline, integrated research contributions and production quality runtime. We improve on the GENIA Tagger by adding a full dependency parser rather than just noun chunking, as well as improved results for NER without compromising significantly on speed.</p>
<p>In more fundamental NLP research, the GENIA corpus (Kim et al., 2003) has been widely used to evaluate transfer learning and domain adaptation. McClosky et al. (2006) demonstrate the effectiveness of self-training and parse re-ranking for domain adaptation. Rimell and Clark (2008) adapt a CCG parser using only POS and lexical categories, while Joshi et al. (2018) extend a neu-
ral phrase structure parser trained on web text to the biomedical domain with a small number of partially annotated examples. These papers focus mainly of the problem of domain adaptation itself, rather than the objective of obtaining a robust, high-performance parser using existing resources.</p>
<p>NLP techniques, and in particular, distant supervision have been employed to assist the curation of large, structured biomedical resources. Poon et al. (2015) extract 1.5 million cancer pathway interactions from PubMed abstracts, leading to the development of Literome (Poon et al., 2014), a search engine for genic pathway interactions and genotype-phenotype interactions. A fundamental aspect of Valenzuela-Escarcega et al. (2018) and Poon et al. (2014) is the use of handwritten rules and triggers for events based on dependency tree paths; the connection to the application of scispaCy is quite apparent.</p>
<h2>8 Conclusion</h2>
<p>In this paper we presented several robust model pipelines for a variety of natural language processing tasks focused on biomedical text. The scispaCy models are fast, easy to use, scalable, and achieve close to state of the art performance. We hope that the release of these models enables new applications in biomedical information extraction whilst making it easy to leverage high quality syntactic annotation for downstream tasks. Additionally, we released a reformatted GENIA 1.0 corpus augmented with automatically produced Universal Dependency annotations and recovered and aligned original abstract metadata. Future work on scispaCy will include a more fully featured entity linker built from the current candidate generation work, as well as other pipeline components such as negation detection commonly used in the clinical and biomedical natural language processing communities.</p>
<h2>References</h2>
<p>Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew E. Peters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang, Chris Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. 2018. Construction of the literature graph in semantic scholar. In NAACLHLT.</p>
<p>Alan R. Aronson. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. Proceedings. AMIA Symposium, pages $17-21$.</p>
<p>Michael Bada, Miriam Eckert, Donald Evans, Kristin Garcia, Krista Shipley, Dmitry Sitnikov, William A. Baumgartner, K. Bretonnel Cohen, Karin M. Verspoor, Judith A. Blake, and Lawrence Hunter. 2011. Concept annotation in the CRAFT corpus. In BMC Bioinformatics.</p>
<p>Olivier Bodenreider. 2004. The unified medical language system (UMLS): integrating biomedical terminology. Nucleic acids research, 32 Database issue:D267-70.</p>
<p>Lutz Bornmann and Rüdiger Mutz. 2014. Growth rates of modern science: A bibliometric analysis. CoRR, abs/1402.4578.</p>
<p>Ekaterina Buyko, Joachim Wermter, Michael Poprat, and Udo Hahn. Automatically adapting an NLP core engine to the biology domain. In Proceedings of the ISMB 2006 Joint Linking Literature, Information and Knowledge for Biology and the 9th BioOntologies Meeting.</p>
<p>Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural scaffolds for citation intent classification in scientific publications. CoRR, abs/1904.01608.</p>
<p>Nigel Collier and Jin-Dong Kim. 2004. Introduction to the bio-entity recognition task at JNLPBA. In NLP$B A / B i o N L P$.</p>
<p>Gamal K. O. Crichton, Sampo Pyysalo, Billy Chiu, and Anna Korhonen. 2017. A neural network multi-task learning approach to biomedical named entity recognition. In BMC Bioinformatics.</p>
<p>Dina Demner-Fushman, Willie J. Rogers, and Alan R. Aronson. 2017. MetaMap Lite: an evaluation of a new Java implementation of MetaMap. Journal of the American Medical Informatics Association : JAMIA, 24 4:841-844.</p>
<p>Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. 2014. NCBI disease corpus: A resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1-10.</p>
<p>Timothy Dozat and Christopher D. Manning. 2016. Deep biaffine attention for neural dependency parsing. CoRR, abs/1611.01734.</p>
<p>Martin Gerner, Goran Nenadic, and Casey M. Bergman. 2009. LINNAEUS: A species name identification system for biomedical literature. In BMC Bioinformatics.</p>
<p>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In Coling 2012.</p>
<p>Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. To appear.</p>
<p>Eduard H. Hovy, Mitchell P. Marcus, Martha Palmer, Lance A. Ramshaw, and Ralph M. Weischedel. 2006. OntoNotes: The $90 \%$ solution. In HLT$N A A C L$.</p>
<p>Vidur Joshi, Matthew Peters, and Mark Hopkins. 2018. Extending a parser to distant domains using a few dozen partially annotated examples. In $A C L$.</p>
<p>Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun'ichi Tsujii. 2003. GENIA corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19 Suppl 1:i180-2.</p>
<p>Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional lstm feature representations. Transactions of the Association for Computational Linguistics, 4:313-327.</p>
<p>Martin Krallinger, Florian Leitner, Obdulia Rabal, Miguel Vazquez, Julen Oyarzábal, and Alfonso Valencia. 2015. CHEMDNER: The drugs and chemical names extraction challenge. In J. Cheminformatics.</p>
<p>Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In HLT-NAACL.</p>
<p>Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016. BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database : the journal of biological databases and curation, 2016.</p>
<p>David McClosky and Eugene Charniak. 2008. Selftraining for biomedical parsing. In $A C L$.</p>
<p>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In $A C L$.</p>
<p>Shikhar Murty, Patrick Verga, Luke Vilnis, Irena Radovanovic, and Andrew McCallum. 2018. Hierarchical losses and new resources for fine-grained entity typing and linking. In $A C L$.</p>
<p>Dat Quoc Nguyen and Karin Verspoor. 2018. From POS tagging to dependency parsing for biomedical event extraction. arXiv preprint arXiv:1808.03731.</p>
<p>Hoifung Poon, Chris Quirk, Charlie DeZiel, and David Heckerman. 2014. Literome: PubMed-scale genomic knowledge base in the cloud. Bioinformatics, 30 19:2840-2.</p>
<p>Hoifung Poon, Kristina Toutanova, and Chris Quirk. 2015. Distant supervision for cancer pathway extraction from text. Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing, pages $120-31$.</p>
<p>Sampo Pyysalo and Sophia Ananiadou. 2014. Anatomical entity mention recognition at literature scale. In Bioinformatics.</p>
<p>Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Andrew Rowley, Hong-Woo Chun, Sung-Jae Jung, Sung-Pil Choi, Jun'ichi Tsujii, and Sophia Ananiadou. 2015. Overview of the cancer genetics and pathway curation tasks of BioNLP shared task 2013. In BMC Bioinformatics.</p>
<p>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In EMNLP.</p>
<p>Devendra Singh Sachan, Pengtao Xie, Mrinmaya Sachan, and Eric P. Xing. 2017. Effective use of bidirectional language modeling for transfer learning in biomedical named entity recognition. In MLHC.</p>
<p>Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin Kipper Schuler, and Christopher G. Chute. 2010. Mayo clinical text analysis and knowledge extraction system (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association : JAMIA, 17 5:507-13.</p>
<p>Sebastian Schuster and Christopher D. Manning. 2016. Enhanced english universal dependencies: An improved representation for natural language understanding tasks. In LREC.</p>
<p>Ariel S. Schwartz and Marti A. Hearst. 2002. A simple algorithm for identifying abbreviation definitions in biomedical text. Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing, pages $451-62$.</p>
<p>Larry Smith, Lorraine K. Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Y Lin, Roman Klinger, Christoph M. Friedrich, Kuzman Ganchev, Manabu Torii, Hongfang Liu, Barry Haddow, Craig A. Struble, Richard J. Povinelli, Andreas Vlachos, William A. Baumgartner, Lawrence E. Hunter, Bob Carpenter, Richard Tzong-Han Tsai, Hong-Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun, Sophia Katrenko, Pieter Adriaans, Christian Blaschke, Rafael Torres, Mariana Neves, Preslav Nakov, Anna Divoli, Manuel MañaLópez, Jacinto Mata, and W. John Wilbur. 2008. Overview of BioCreative II gene mention recognition. Genome Biology, 9:S2 - S2.</p>
<p>Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, Tomoko Ohta, John McNaught, Sophia Ananiadou, and Jun'ichi Tsujii. 2005. Developing a robust part-of-speech tagger for biomedical text. In Panhellenic Conference on Informatics.</p>
<p>Yoshimasa Tsuruoka and Jun'ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In HLT/EMNLP.</p>
<p>Marco Antonio Valenzuela-Escarcega, Ozgun Babur, Gus Hahn-Powell, Dane Bell, Thomas Hicks, Enrique Noriega-Atala, Xia Wang, Mihai Surdeanu, Emek Demir, and Clayton T. Morrison. 2018. Large-scale automated machine reading discovers new cancer-driving mechanisms. In Database.</p>
<p>Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis P. Langlotz, and Jiawei Han. 2018. Cross-type biomedical named entity recognition with deep multi-task learning. Bioinformatics.</p>
<p>Chih-Hsuan Wei, Yifan Peng, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Jiao Li, Thomas C. Wiegers, and Zhiyong Lu. 2016. Assessing the state of the art in biomedical relation extraction: overview of the BioCreative V chemicaldisease relation (CDR) task. Database : the journal of biological databases and curation, 2016.</p>
<p>Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch Marcus, Robert Belvin adn Sameer Pradhan, Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A large training corpus for enhanced processing. In Joseph Olive, Caitlin Christianson, and John McCary, editors, Handbook of Natural Language Processing and Machine Translation. Springer.</p>
<p>Yuan Zhang and David I Weiss. 2016. Stackpropagation: Improved representation learning for syntax. CoRR, abs/1603.06598.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ We refer the interested reader to Nguyen and Verspoor (2018) for a comprehensive description of model architectures considered in this evaluation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ For a detailed discussion of the datasets and their creation, we refer the reader to https://github.com/ cambridgeit1/MTL-Bioinformatics-2016/ blob/master/Additional\%20file\%201.pdf&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
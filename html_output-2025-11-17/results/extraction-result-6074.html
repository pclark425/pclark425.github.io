<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6074 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6074</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6074</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-9e5b3d3988da7a294f816c93a09be8f3a84c4ac7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9e5b3d3988da7a294f816c93a09be8f3a84c4ac7" target="_blank">Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT</a></p>
                <p><strong>Paper Venue:</strong> REFSQ Workshops</p>
                <p><strong>Paper TL;DR:</strong> An extensive empirical evaluation of two ChatGPT models, specifically gpt-3.5-turbo, and gpt-4 in both zero-shot and few-shot settings for requirements classification shows that there is no single best technique for all types of requirement classes.</p>
                <p><strong>Paper Abstract:</strong> Recently, Large Language Models like ChatGPT have demonstrated remarkable proficiency in various Natural Language Processing tasks. Their application in Requirements Engineering, especially in requirements classification, has gained increasing interest. This paper reports an extensive empirical evaluation of two ChatGPT models, specifically gpt-3.5-turbo, and gpt-4 in both zero-shot and few-shot settings for requirements classification. The question arises as to how these models compare to traditional classification methods, specifically Support Vector Machine and Long Short-Term Memory. Based on five different datasets, our results show that there is no single best technique for all types of requirement classes. Interestingly, the few-shot setting has been found to be beneficial primarily in scenarios where zero-shot results are significantly low.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6074",
    "paper_id": "paper-9e5b3d3988da7a294f816c93a09be8f3a84c4ac7",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0030649999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT</h1>
<p>Abdelkarim El-Hajjami ${ }^{1}$, Nicolas Fafin ${ }^{1}$ and Camille Salinesi ${ }^{1}$<br>${ }^{1}$ Paris 1 Panthéon-Sorbonne University, Paris, France</p>
<h4>Abstract</h4>
<p>Recently, Large Language Models like ChatGPT have demonstrated remarkable proficiency in various Natural Language Processing tasks. Their application in Requirements Engineering, especially in requirements classification, has gained increasing interest. This paper reports an extensive empirical evaluation of two ChatGPT models, specifically gpt-3.5-turbo, and gpt-4 in both zero-shot and few-shot settings for requirements classification. The question arises as to how these models compare to traditional classification methods, specifically Support Vector Machine and Long Short-Term Memory. Based on five different datasets, our results show that there is no single best technique for all types of requirement classes. Interestingly, the few-shot setting has been found to be beneficial primarily in scenarios where zero-shot results are significantly low.</p>
<h2>Keywords</h2>
<p>Large Language Models, ChatGPT, Requirements Classification</p>
<h2>1. Introduction</h2>
<p>Artificial Intelligence (AI) presents an exciting opportunity for Requirements Engineering (RE). As many publications have already shown, AI is enhancing the precision and efficiency of many RE tasks from elicitation to negotiation, and formalization. This paper focuses on requirements classification, particularly the challenge of differentiating between Functional (FR) and Non-Functional Requirements (NFR), a significant matter in both academic research and practical industry applications.</p>
<p>The distinction between FR, and NFR is subject to ongoing debate within the community, and there is no consensus on their definitions [1] [2] [3]. We believe that employing Large Language Models (LLMs) as requirements classifiers will shed further light on this scientific discussion.</p>
<p>Software development projects often involve managing a multitude of requirements, making it essential for software professionals to effectively organize and prioritize them. One key aspect of this process is the classification of requirements into FR and NFR. However, manually categorizing each requirement is a labor-intensive process. Therefore, there is a compelling need for an automated method that can accurately and efficiently identify FR and NFR.</p>
<p>Our research is grounded upon Dalpiaz et al. version. [4] of Kurtanović and Maalej's requirements classification approach [5]. Since the original classifier by Kurtanović and Maalej is not available online, we rely on Dalpiaz et al.'s reconstruction, emphasizing the differentiation of FR and NFR using high-dimensional (500) word level features.</p>
<p>Building upon Dalpiaz et al.'s work with SVM, our research explores the Long Short-Term Memory (LSTM) model and two ChatGPT versions, gpt-3.5-turbo and gpt-4 by OpenAI, to align with our specific research objectives. Based on this, our research aims to address the following research questions:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>RQ1: What is the best technique for requirements classification between SVM, LSTM and ChatGPT?</li>
<li>RQ2: What are the performance differences between GPT-4 and 3.5?</li>
<li>RQ3: What are the performance differences between Zero-Shot and Few-Shot settings?</li>
</ul>
<p>The rest of the paper is organized as follows: Section II outlines the methodology, providing insights into our research approach. Section III presents the analysis of our experimental results. Section IV comprises the threats to validity of our research, while Section VI presents the conclusion, synthesizing our findings and suggesting future directions in the field.</p>
<h1>2. Methodology</h1>
<h3>2.1. The classification problem</h3>
<p>In our methodology, we follow the modeling paradigm of Li et al. [3] as further refined by Dalpiaz et al. The approach categorizes requirements considering two aspects in any requirements: the functional aspect and the quality aspect. A requirement has a functional aspect when it specifies a functional goal or a functional constraint. In parallel, the quality aspects of a requirement include quality goals and quality constraints.</p>
<p>Acknowledging the possibility that a requirement may encompass both functional and quality aspects, Dalpiaz et al. subsequently formulated four distinct binary classification problems:</p>
<ul>
<li>IsFunctional: does a requirement possess functional aspects?</li>
<li>IsQuality: does a requirement possess quality aspects?</li>
<li>OnlyFunctional: does a requirement possess only functional aspects?</li>
<li>OnlyQuality: does a requirement possess only quality aspects?</li>
</ul>
<h3>2.2. The Datasets</h3>
<p>While our initial goal was to utilize the exact same datasets as the ones curated by Dalpiaz et al., we had access to only five public requirements datasets out of the eight used by Dalpiaz et al. in their experiments: PROMISE, Dronology, ReqView, Leeds Library and WASP [6].</p>
<p>Table 1 indicates the number of requirements and their distribution among the four classes.</p>
<p>Table 1
Datasets overview</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Rows</th>
<th style="text-align: center;">IsFunctional</th>
<th style="text-align: center;">IsQuality</th>
<th style="text-align: center;">OnlyFunctional</th>
<th style="text-align: center;">OnlyQuality</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PROMISE</td>
<td style="text-align: center;">625</td>
<td style="text-align: center;">310</td>
<td style="text-align: center;">382</td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">302</td>
</tr>
<tr>
<td style="text-align: left;">Dronology</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">ReqView</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: left;">Leeds Library</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">WASP</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Totals</td>
<td style="text-align: center;">956</td>
<td style="text-align: center;">578</td>
<td style="text-align: center;">522</td>
<td style="text-align: center;">417</td>
<td style="text-align: center;">361</td>
</tr>
</tbody>
</table>
<p>Table 1 reveals a clear imbalance in the datasets across the different classes. The "IsFunctional" class has 578 instances compared to "OnlyQuality" with only 361 instances. This pattern of imbalance extends across all datasets, with "IsFunctional" and "IsQuality" classes being more common than "OnlyFunctional" and "OnlyQuality" classes.</p>
<h1>2.3. The Evaluated Models</h1>
<h3>2.3.1. SVM</h3>
<p>We directly used the datasets from the work of Dalpiaz et al. for their SVM-based classification approach [6], which employs 500 word-level features such as text n-grams or Part-of-Speech (POS) n-grams. We trained the model on $75 \%$ of the PROMISE dataset and evaluated its performance on the global evaluation dataset, which was formed by combining the individual test sets-specifically, the remaining $25 \%$ of the PROMISE dataset, along with the datasets from Dronology, ReqView, Leeds Library, and WASP. This configuration allowed us to assess the model's performance on diverse and unseen data.</p>
<h3>2.3.2. LSTM</h3>
<p>LSTM, a type of Recurrent Neural Network (RNN), is especially effective for handling sequential data, making it relevant for requirements classification problems. Before evaluating the capabilities of ChatGPT in requirements classification, it was imperative to benchmark its performance against a wellestablished model known for its sequential data processing capabilities. The LSTM model architecture in this study includes an embedding layer that converts words into numerical vectors, a spatial dropout layer for regularization to prevent overfitting, an LSTM layer that processes text sequences and captures word dependencies, and a dense layer with a sigmoid activation for outputting a probability score indicating the likelihood of a requirement being in the positive class.</p>
<p>Mirroring the approach adopted for the SVM model, we trained the LSTM on $75 \%$ of the PROMISE dataset and assessed its performance on the global evaluation dataset.</p>
<h3>2.3.3. ChatGPT</h3>
<p>ChatGPT, developed by OpenAI, is a Generative Pre-trained Transformer model designed to generate human-like text based on the prompts it receives.</p>
<p>In our study, we used:</p>
<ol>
<li>gpt-3.5-turbo: As of the date of our experimentation, October 20, 2023, this is the latest and most advanced model in the GPT-3.5 series. Its training data extends up to September 2021.</li>
<li>gpt-4: OpenAI's newest model, trained until September 2021.</li>
</ol>
<p>Prompt Engineering Prompt engineering is a critical aspect of interacting with ChatGPT models. Crafting effective prompts can significantly influence the model's responses, particularly when precise or specific outputs are desired.</p>
<p>In the context of our study, two major strategies were employed: zero-shot prompting and few-shot prompting [7]. Both of these strategies aimed to guide the model in correctly classifying a requirement, but they approach the task differently.</p>
<p>Zero-Shot Prompting In Zero-shot prompting, models are given the task without the benefit of specific contextual examples in the prompt. As such, the model draws purely from its vast pre-trained knowledge and the immediate context provided in the prompt. Zero-shot scenarios offer a unique insight into the model's innate understanding of the task and its ability to generalize from its training data without needing explicit examples to guide its response.</p>
<p>For the zero-shot setting in our study, our prompts were engineered following these three basic principles:</p>
<ol>
<li>Each prompt should include all necessary details to get more relevant answers;</li>
<li>Each prompt should ask the model to adopt a software requirements expert persona.</li>
<li>The words used in each prompt should come from the Dalpiaz et al. reference paper.</li>
</ol>
<p>For example, the designed zero-shot prompt for the "IsFunctional" classification is:</p>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">software</span><span class="w"> </span><span class="n">requirements</span><span class="w"> </span><span class="n">expert</span><span class="w"> </span><span class="n">tasked</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">categorizing</span><span class="w"> </span><span class="n">software</span><span class="w"> </span><span class="n">requirements</span><span class="w"> </span><span class="k">into</span><span class="err">:</span>
<span class="nl">IsFunctional</span><span class="p">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirement</span><span class="w"> </span><span class="n">possesses</span><span class="w"> </span><span class="n">functional</span><span class="w"> </span><span class="n">aspects</span><span class="w"> </span><span class="p">(</span><span class="n">functional</span><span class="w"> </span><span class="n">goals</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">functional</span><span class="w"> </span><span class="k">constraints</span><span class="p">).</span>
<span class="nl">IsQuality</span><span class="p">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirement</span><span class="w"> </span><span class="n">possesses</span><span class="w"> </span><span class="n">quality</span><span class="w"> </span><span class="n">aspects</span><span class="w"> </span><span class="p">(</span><span class="n">quality</span><span class="w"> </span><span class="n">goals</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">quality</span><span class="w"> </span><span class="k">constraints</span><span class="p">).</span>
<span class="nl">OnlyFunctional</span><span class="p">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirement</span><span class="w"> </span><span class="n">possesses</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">functional</span><span class="w"> </span><span class="n">aspects</span><span class="p">,</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="n">quality</span><span class="w"> </span><span class="n">aspects</span><span class="p">.</span>
<span class="nl">OnlyQuality</span><span class="p">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirement</span><span class="w"> </span><span class="n">possesses</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">quality</span><span class="w"> </span><span class="n">aspects</span><span class="p">,</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="n">functional</span><span class="w"> </span><span class="n">aspects</span><span class="p">.</span>
<span class="n">Given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">requirement</span><span class="w"> </span><span class="n">below</span><span class="p">,</span><span class="w"> </span><span class="n">determine</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">falls</span><span class="w"> </span><span class="k">under</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="ss">&quot;IsFunctional&quot;</span>
<span class="nl">Requirement</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;[requirement_text]&quot;</span>
<span class="n">Respond</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="ss">&quot;yes&quot;</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="ss">&quot;no&quot;</span><span class="p">.</span>
</code></pre></div>

<p>Few-Shot Prompting Few-shot prompting is another approach to guide the model towards desired outputs by providing a limited number of example inputs and their corresponding outputs. By providing these examples, the intention is to give the model a clear context of what is expected from it, thus aiding it in generating more accurate and task-specific responses.</p>
<p>In our study, few-shot prompting was applied with adherence to a set of structured guidelines. Firstly, we adopted the same principles as those used in the zero-shot setting. Additionally, for the selection of examples, we ensured that all were sourced from the PROMISE training set. A balanced representation was a key focus; for each class - "IsFunctional", "IsQuality", "OnlyFunctional", and "OnlyQuality" we curated two examples each of the positive and negative cases. This approach was designed to ensure that the model was exposed to a diverse range of scenarios, thereby providing a comprehensive understanding of the classification nuances. For instance, in the case of "IsFunctional", the positive instances included one scenario that combined both "IsFunctional" and "IsQuality", and another that incorporated "IsFunctional" without "IsQuality".</p>
<p>It is important to note that ChatGPT was also evaluated on the same global evaluation dataset as used for SVM and LSTM, which includes the $25 \%$ of the PROMISE dataset and the entirety of the datasets from Dronology, ReqView, Leeds Library, and WASP. The examples for few-shot prompting were specifically chosen from the PROMISE training set to maintain consistency.</p>
<p>Querying ChatGPT The OpenAI API was used to query ChatGPT. Every individual instance of each test set was provided to ChatGPT in one API call. The parameters of ChatGPT API calls were set as follows:</p>
<ul>
<li>Temperature: We set the temperature parameter to 0 in order to minimize randomness in outputs and enhance the reproducibility of our evaluation results. However, due to the non-deterministic nature of ChatGPT, some variability may still occur.</li>
<li>Other parameters: Beyond the temperature setting, all other parameters were set at their default values.</li>
</ul>
<p>While running the experiments, we encountered multiple server-side errors (e.g., BadGateway HTTP 502, ServiceUnavailableError - HTTP 500). We incorporated exception handling in our approach in order to handle them.</p>
<p>To provide a clear understanding of the implementation details and ensure transparency and reproducibility, the code used in this study has been made publicly available [8].</p>
<h1>3. Experimental Results Analysis</h1>
<h3>3.1. The Evaluation Metric</h3>
<p>In the context of our requirements classification task, which is characterized by a pronounced imbalance in the dataset as shown in Table 1, the selection of an appropriate evaluation metric is important.</p>
<p>Dalpiaz et al. have previously employed a range of metrics including precision, recall, F1-score, and AUC to assess model performance. Since ChatGPT does not offer probability distribution or score confidence which are essential for the calculation of the AUC metric, we cannot employ this metric in our evaluation. The F1-score, while commonly used, assumes that precision and recall are of equal importance by attributing them equal weight. This assumption does not hold in our context, especially considering that achieving high recall is more challenging and crucial in manual processes. Typically, it is more feasible to manually reject a false positive identified by a tool than to manually find a true positive within the tool's input. Consequently, we propose the use of the $F_{\beta}$ score, which allows us to weight recall over precision to reflect its greater importance. The value of $\beta$ should be empirically determined based on the inverse frequency of the target class within the dataset [9]. As explained by Berry [9], the inverse frequency indicates the average number of items that must be examined in the search space to find one true positive, providing a lower bound estimate for $F_{\beta}$.</p>
<p>Table 2 is constructed based on the inverse frequencies derived from the datasets, illustrating the $\beta$ values for each class.</p>
<p>Table 2
Inverse Frequency of Classes as $\beta$ Values</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test set</th>
<th style="text-align: center;">$\beta_{\text {IsFunctional }}$</th>
<th style="text-align: center;">$\beta_{\text {IsQuality }}$</th>
<th style="text-align: center;">$\beta_{\text {OnlyFunctional }}$</th>
<th style="text-align: center;">$\beta_{\text {OnlyQuality }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PROMISE Test</td>
<td style="text-align: center;">2.09</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">2.87</td>
<td style="text-align: center;">1.91</td>
</tr>
<tr>
<td style="text-align: left;">Dronology</td>
<td style="text-align: center;">1.03</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">48.50</td>
</tr>
<tr>
<td style="text-align: left;">ReqView</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">2.72</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;">7.91</td>
</tr>
<tr>
<td style="text-align: left;">Leeds Library</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">2.13</td>
</tr>
<tr>
<td style="text-align: left;">WASP</td>
<td style="text-align: center;">1.13</td>
<td style="text-align: center;">3.26</td>
<td style="text-align: center;">1.48</td>
<td style="text-align: center;">10.33</td>
</tr>
<tr>
<td style="text-align: left;">Global</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">2.02</td>
<td style="text-align: center;">2.02</td>
<td style="text-align: center;">3.49</td>
</tr>
</tbody>
</table>
<h1>3.2. RQ1: What is the best technique for requirements classification between SVM, LSTM and ChatGPT?</h1>
<p>Table 3 provides a comprehensive evaluation of the global $F_{\beta}$ scores for SVM, LSTM, GPT-3.5, and GPT-4 models, including both Zero-Shot and Few-Shot configurations, across four binary requirements classifications: "IsFunctional", "IsQuality", "OnlyFunctional", and "OnlyQuality".</p>
<p>Table 3
The global $F_{\beta}$ score of SVM, LSTM, and GPT configurations</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Classification</th>
<th style="text-align: center;">SVM</th>
<th style="text-align: center;">LSTM</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
</tr>
<tr>
<td style="text-align: left;">IsFunctional</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 9}$</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.888</td>
</tr>
<tr>
<td style="text-align: left;">IsQuality</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 8}$</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.493</td>
</tr>
<tr>
<td style="text-align: left;">OnlyFunctional</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 2}$</td>
<td style="text-align: center;">0.843</td>
</tr>
<tr>
<td style="text-align: left;">OnlyQuality</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 2}$</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.497</td>
</tr>
</tbody>
</table>
<p>The $F_{\beta}$ scores in Table 3 reveal that no single technique consistently outperforms the others across all four classifications. Rather, the optimal classification technique is contingent upon the specific requirements class pursued. For instance, in the "IsFunctional" classification, GPT-3.5 and GPT-4 are highly competitive in both Zero-Shot and Few-Shot settings, with GPT-3.5 Few-Shot reaching an $F_{\beta}$ score of 0.899 . Meanwhile, LSTM leads in the "IsQuality" classification with a score of 0.708 . The "OnlyFunctional" classification is best addressed by the GPT-4 Zero-Shot model, which scores 0.872 . Lastly, for requirements that are solely quality-centric ("OnlyQuality"), the GPT-3.5 Few-Shot setting also shows a commendable $F_{\beta}$ score of 0.732 .</p>
<p>An additional nuance to consider regarding the performance of the LSTM model is that although it is the best performer in the "IsQuality" classification, its effectiveness is not consistent across all the individual test sets. Indeed, its performance is particularly good on the PROMISE test set, likely due</p>
<p>to being trained on similar data. However, its effectiveness drops when applied to other test sets, as illustrated in Table 4, indicating a limitation in its ability to generalize.</p>
<p>Table 4
LSTM evaluation metrics for "IsQuality" classification</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">$\mathbf{F}_{\boldsymbol{\beta}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PROMISE Test</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.925</td>
</tr>
<tr>
<td style="text-align: left;">Dronology</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.584</td>
</tr>
<tr>
<td style="text-align: left;">ReqView</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.670</td>
</tr>
<tr>
<td style="text-align: left;">Leeds Library</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.566</td>
</tr>
<tr>
<td style="text-align: left;">WASP</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.659</td>
</tr>
</tbody>
</table>
<p>A further observation from Table 5 is that none of the models were able to effectively classify "OnlyQuality" requirements in the Dronology test set except for the LSTM model. This issue likely results from the dataset's significant imbalance, with only two "OnlyQuality" samples present. The absence of a substantial number of such samples means that failing to identify these few instances translates directly to a score of zero. While the $F_{\beta}$ score is designed to provide a more nuanced evaluation in the presence of class imbalance by emphasizing recall, this example shows that even a well-chosen metric may not be sufficient in the context of highly imbalanced datasets.</p>
<p>Table 5
The "OnlyQuality" $F_{\beta}$ performance metrics comparison</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test set</th>
<th style="text-align: center;">SVM</th>
<th style="text-align: center;">LSTM</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
</tr>
<tr>
<td style="text-align: left;">PROMISE Test</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.593</td>
</tr>
<tr>
<td style="text-align: left;">Dronology</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0}$</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ReqView</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.368</td>
</tr>
<tr>
<td style="text-align: left;">Leeds Library</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.500</td>
</tr>
<tr>
<td style="text-align: left;">WASP</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.168</td>
</tr>
</tbody>
</table>
<p>In summarizing our findings, it is clear that there is no single best technique for all requirements classifications. The best technique varies depending on the specific class of requirements that you need to identify.</p>
<h1>3.3. RQ2: What are the performance differences between GPT-4 and 3.5?</h1>
<p>Table 6
The global $F_{\beta}$ score of GPT configurations</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Classification</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
</tr>
<tr>
<td style="text-align: left;">IsFunctional</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 9}$</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.888</td>
</tr>
<tr>
<td style="text-align: left;">IsQuality</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 7}$</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.493</td>
</tr>
<tr>
<td style="text-align: left;">OnlyFunctional</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 2}$</td>
<td style="text-align: center;">0.843</td>
</tr>
<tr>
<td style="text-align: left;">OnlyQuality</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 2}$</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.497</td>
</tr>
</tbody>
</table>
<p>In an analytical comparison of GPT-3.5 and GPT-4 based on the given performance metrics as presented in Table 6, one can observe that GPT-3.5 outperforms GPT-4 in three out of four scenarios. This is particularly notable in the "IsQuality" and "OnlyQuality" classifications, where GPT-3.5 has higher $F_{\beta}$ scores. GPT-4 only stands out is in the "OnlyFunctional" classification, demonstrating a significant improvement over GPT-3.5 in both zero-shot and few-shot settings. Therefore, if the classification of purely functional requirements is critical and the budget allows for the higher cost of GPT-4, it would be the recommended choice.</p>
<h1>3.4. RQ3: What are the performance differences between Zero-Shot and Few-Shot settings?</h1>
<p>Table 7 presents the score deltas, computed as the few-shot global $F_{\beta}$ score minus the zero-shot global $F_{\beta}$ score, for GPT-3.5 and GPT-4 across our four classifications. This metric serves as an indicator of the added value from few-shot learning over an initial zero-shot baseline.</p>
<p>Table 7
The $F_{\beta}$ score delta between Few-Shot and Zero-Shot for ChatGPT</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Classification</th>
<th style="text-align: center;">GPT-3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GPT-4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">Delta</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">Delta</td>
</tr>
<tr>
<td style="text-align: left;">IsFunctional</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">-0.007</td>
</tr>
<tr>
<td style="text-align: left;">IsQuality</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">-0.117</td>
</tr>
<tr>
<td style="text-align: left;">OnlyFunctional</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 2}$</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">-0.029</td>
</tr>
<tr>
<td style="text-align: left;">OnlyQuality</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 2}$</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">-0.057</td>
</tr>
</tbody>
</table>
<p>The deltas show that few-shot setting does not consistently improve upon the zero-shot baseline, with marginal declines in performance across most classifications. Conversely, GPT-3.5 displays significant positive deltas in "OnlyFunctional" and "OnlyQuality" classifications, indicating that few-shot learning has a pronounced beneficial impact when the zero-shot performance is weak. These insights suggest that few-shot learning can be particularly valuable in scenarios where a model struggles to perform adequately without prior examples, as it can lead to marked performance gains and help models like GPT-3.5 overcome initial deficiencies in understanding and classifying requirements.</p>
<h2>4. Threats to Validity</h2>
<p>In our study, we identified several threats to validity across different dimensions. Internally, we noted biases such as the sensitivity of ChatGPT's responses to prompt variations and the non-deterministic nature of its output, which could vary slightly even with identical prompts. The choice and representation of few-shot examples and the potential impact of server errors also posed challenges to the reliability of our findings. Externally, the use of specific datasets and the subjectivity of tagger annotations could introduce biases. Regarding construct validity, we acknowledged that traditional metrics might not fully reflect the model's capabilities in classifying requirements, accentuating the importance of interpretability. Finally, our conclusion validity faced threats from unbalanced datasets, which could bias our results.</p>
<h2>5. Related Work</h2>
<p>The work of Rashwan et al. [10] introduced a novel approach by developing a new corpus with annotations for different types of NFR based on a requirements ontology and employing a SVM classifier to categorize requirements sentences into different ontology classes automatically. The proposed approach showed promising results in two different software requirements specification corpora, underlining the importance of semantic analysis and ontological representation.</p>
<p>Ray et al. [11] introduced the aeroBERT-Classifier, a specialized model designed for the aerospace domain. The model's architecture, leveraging a domain-adapted BERT, underscores the merits of industry-specific adaptations. Their comparative analysis against models like GPT-2 and Bi-LSTM further delineates the robustness of transformer architectures in similar tasks.</p>
<p>In a distinct contribution, Rahimi et al. [12] proposed an ensemble approach combining a suite of machine learning classifiers, including Naïve Bayes, SVM, Decision Tree, Logistic Regression, and SVC. Their ensemble method achieved a remarkable $99.45 \%$ accuracy in classifying Functional Requirements, affirming the efficacy of harnessing multiple models to optimize classification outcomes.</p>
<p>Lastly, adding a linguistic dimension, Yucalar [13] introduced BERTurk, a model specifically finetuned for classifying software requirements in the Turkish language. Through rigorous empirical validation, Yucalar established that BERTurk achieves a commendable 95\% F1-score in differentiating between functional and non-functional requirements. This research accentuates the value of integrating linguistic nuances with state-of-the-art NLP techniques within the sphere of RE.</p>
<h1>6. Conclusion and Future Work</h1>
<p>In this study, we conducted a comprehensive assessment of two ChatGPT models: gpt-3.5-turbo, and gpt-4. We evaluated these models in both zero-shot and few-shot settings, comparing them against established methods such as SVM and LSTM in requirements classification. Our findings show that there is no single best technique for all requirements classifications. The best technique varies depending on the specific requirement classification. For instance, GPT-3.5 Few-Shot configuration leads in the "IsFunctional" and "OnlyQuality" classifications, LSTM model performs best in the "IsQuality" classification, and GPT-4 Zero-Shot setting stands out in the "OnlyFunctional" classification. Our results also indicate that GPT-3.5 is generally more effective than GPT-4, except when it comes to "OnlyFunctional" requirements classification, where GPT-4's higher cost may be justified by its enhanced performance. Interestingly, the few-shot setting has been found to be beneficial primarily in scenarios where zero-shot performance is notably weak. From a practical perspective, these findings suggest that for "IsFunctional" and "OnlyQuality" classifications, the default choice should be GPT-3.5 Few-Shot. In scenarios where the classification of "OnlyFunctional" requirements is paramount and resources are available, GPT-4 emerges as a reasonably good cold start. If budget constraints are a factor, GPT-3.5 Few-Shot remains a viable alternative. For "IsQuality" classification, LSTM stands out as the most effective tool, however it needs to be trained.</p>
<p>In the future, we intend to undertake a series of rigorous comparisons of ChatGPT with other LLMs, such as Llama-2 and Mistral, in performing RE tasks. Such analyses could offer valuable insights into the relative advantages and limitations of each model for various RE tasks. To complement this research direction, it is also essential to build high-quality benchmark requirements datasets for training and comprehensively evaluating LLMs on a wider range of RE tasks.</p>
<h2>References</h2>
<p>[1] L. Chung, B. Nixon, E. Yu, J. Mylopoulos, Non-functional requirements in software engineering, in: A. Borgida, V. Chaudhri, P. Giorgini, E. Yu (Eds.), Conceptual Modeling: Foundations and Applications, volume 5 of Lecture Notes in Computer Science, Springer, 2012.
[2] J. Eckhardt, A. Vogelsang, D. M. Fernández, Are "non-functional" requirements really nonfunctional? an investigation of non-functional requirements in practice, in: Proceedings of the IEEE/ACM International Conference on Software Engineering, IEEE, 2016.
[3] F.-L. Li, J. Horkoff, J. Mylopoulos, R. S. Guizzardi, G. Guizzardi, A. Borgida, L. Liu, Non-functional requirements as qualities, with a spice of ontology, in: Proceedings of the IEEE International Requirements Engineering Conference, IEEE, 2014.
[4] F. Dalpiaz, D. Dell'Anna, F. B. Aydemir, S. Çevikol, Requirements classification with interpretable machine learning and dependency parsing, in: Proceedings of the 2019 IEEE 27th International Requirements Engineering Conference (RE), IEEE, 2019.
[5] Z. Kurtanović, W. Maalej, Automatically classifying functional and non-functional requirements using supervised machine learning, in: Proceedings of the 2017 IEEE 25th International Requirements Engineering Conference (RE), IEEE, 2017.
[6] F. Dalpiaz, D. Dell'Anna, F. B. Aydemir, S. Çevikol, explainable-re/re-2019-materials, 2019. URL: https://doi.org/10.5281/zenodo. 3309669.
[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,</p>
<p>D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei, Language models are few-shot learners, in: H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.), Advances in Neural Information Processing Systems, Curran Associates, Inc., 2020.
[8] A. El-Hajjami, N. Fafin, C. Salinesi, Requirements-classifiers-evaluation-materials, 2024. URL: https://doi.org/10.5281/zenodo. 10802076.
[9] D. M. Berry, Empirical evaluation of tools for hairy requirements engineering tasks, Empirical Software Engineering 26 (2021) 111.
[10] A. Rashwan, O. Ormandjieva, R. Witte, Ontology-based classification of non-functional requirements in software specifications: A new corpus and svm-based classifier, in: Proceedings of the 2013 IEEE 37th Annual Computer Software and Applications Conference (COMPSAC), IEEE, 2013.
[11] A. T. Ray, B. F. Cole, O. J. P. Fischer, D. N. Mavris, aerobert-classifier: Classification of aerospace requirements using bert, Aerospace 10 (2023) 279.
[12] N. Rahimi, F. Eassa, L. Elrefaei, An ensemble machine learning technique for functional requirement classification, Symmetry 12 (2020) 1601.
[13] F. Yucalar, Developing an advanced software requirements classification model using bert: An empirical evaluation study on newly generated turkish data, Applied Sciences 13 (2023) 11127.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>In: D. Mendez, A. Moreira, J. Horkoff, T. Weyer, M. Daneva, M. Unterkalmsteiner, S. Bühne, J. Hehn, B. Penzenstadler, N. CondoriFernández, O. Dieste, R. Guizzardi, K. M. Habibullah, A. Perini, A. Susi, S. Abualhaija, C. Arora, D. Dell'Anna, A. Ferrari, S. Ghanavati, F. Dalpiaz, J. Steghöfer, A. Rachmann, J. Gulden, A. Müller, M. Beck, D. Birkmeier, A. Herrmann, P. Mennig, K. Schneider. Joint Proceedings of REFSQ-2024 Workshops, Doctoral Symposium, Posters \&amp; Tools Track, and Education and Training Track. Co-located with REFSQ 2024. Winterthur, Switzerland, April 8, 2024.
${ }^{a}$ Corresponding author.
abdelkarim.el-hajjami@univ-paris1.fr (A. El-Hajjami); nicolas.fafin@etu.univ-paris1.fr (N. Fafin); camille.salinesi@univ-paris1.fr (C. Salinesi)
0009-0004-7053-3264 (A. El-Hajjami); 0000-0002-1957-0519 (C. Salinesi)
(c) 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-490 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-490</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-490</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-265506856</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.18064v1.pdf" target="_blank">GELDA: A generative language annotation framework to reveal visual biases in datasets</a></p>
                <p><strong>Paper Abstract:</strong> Bias analysis is a crucial step in the process of creating fair datasets for training and evaluating computer vision models. The bottleneck in dataset analysis is annotation, which typically requires: (1) specifying a list of attributes relevant to the dataset domain, and (2) classifying each image-attribute pair. While the second step has made rapid progress in automation, the first has remained human-centered, requiring an experimenter to compile lists of in-domain attributes. However, an experimenter may have limited foresight leading to annotation"blind spots,"which in turn can lead to flawed downstream dataset analyses. To combat this, we propose GELDA, a nearly automatic framework that leverages large generative language models (LLMs) to propose and label various attributes for a domain. GELDA takes a user-defined domain caption (e.g.,"a photo of a bird,""a photo of a living room") and uses an LLM to hierarchically generate attributes. In addition, GELDA uses the LLM to decide which of a set of vision-language models (VLMs) to use to classify each attribute in images. Results on real datasets show that GELDA can generate accurate and diverse visual attribute suggestions, and uncover biases such as confounding between class labels and background features. Results on synthetic datasets demonstrate that GELDA can be used to evaluate the biases of text-to-image diffusion models and generative adversarial networks. Overall, we show that while GELDA is not accurate enough to replace human annotators, it can serve as a complementary tool to help humans analyze datasets in a cheap, low-effort, and flexible manner.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e490.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e490.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoregressive large language model used to hierarchically generate attribute categories and attribute examples from a user-provided domain caption in the GELDA pipeline; configured with temperature τ=0.3 and prompted for structured list outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer vision dataset bias analysis / dataset annotation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>hierarchical generation of visual attribute categories and attribute labels given a domain caption (automatic attribute proposal for dataset analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Auto-regressive sampling nondeterminism (stochastic outputs given same prompt), sampling temperature (they used τ = 0.3), prompt phrasing / formatting, API/output formatting variability, and implicit random seeds inside the model/API.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Non-deterministic (stochastic) outputs from the LLM; prompt sensitivity; incomplete recall of some ground-truth attributes; cost and latency trade-offs when re-querying the LLM to reduce stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Set temperature low (τ = 0.3); enforce structured output formatting (request a single Python list); hierarchical (chain) prompting (categories then attributes then object/image-level decision); require yes/no answers with explanations to improve answer quality; perform multiple queries per prompt and select most-frequent outputs (majority/frequency voting).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The authors note that auto-regressive LLM outputs are stochastic and that determinism aids reproducibility; they mitigate this by low temperature, structured prompts, and repeated-sampling + frequency voting to pick the most common categories/attributes, but they do not provide a quantitative measurement of residual variability or a formal reproducibility guarantee.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GELDA: A generative language annotation framework to reveal visual biases in datasets', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e490.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e490.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM stochasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochasticity of auto-regressive large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual observation in the paper that auto-regressive LLMs can produce different outputs for the same prompt; while this captures distributional diversity it undermines deterministic reproducibility for downstream annotation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP methods applied to dataset annotation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>n/a (conceptual discussion related to LLM-driven attribute generation)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling randomness inherent to auto-regressive generation, temperature parameter, prompt phrasing/format, API nondeterminism.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Non-deterministic sampling and prompt sensitivity make exact reproducibility difficult without controls.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Recommend increasing determinism (lower temperature), repeated sampling and majority-selection, structured/hierarchical prompting, and requesting explanations to improve answer consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper explicitly acknowledges LLM stochasticity and prescribes repeated sampling plus prompt-engineering and lower temperature to improve consistency, but does not report quantitative variability/reproducibility measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GELDA: A generative language annotation framework to reveal visual biases in datasets', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e490.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e490.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GELDA reproducibility protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GELDA's reproducibility protocol: repeated queries, frequency voting, temperature control, and structured prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's implemented set of practical steps intended to reduce variability in LLM-generated attribute proposals: use τ=0.3, request consistent list formats, run queries multiple times and select the most frequent categories/attributes, hierarchical queries, and require explanations for binary decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-1106</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer vision dataset bias analysis / dataset annotation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>reduce output variability in hierarchical attribute generation for automated dataset annotation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>stochastic sampling from the LLM, prompt phrasing variability, and LLM internal nondeterminism; also cost/latency constraints that limit number of repeated queries.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Frequency counts of generated categories/attributes used for selection (heuristic aggregation) rather than formal statistical variability metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Aggregation-by-frequency (pick top-N categories/attributes across multiple queries) used as a practical reproducibility heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Trade-off between query cost/latency and number of repeats needed to stabilize outputs; residual LLM/VLM systematic errors and missing attribute recall despite aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Repeated queries per prompt + majority/frequency voting; low temperature (τ = 0.3); enforce structured output (Python list); hierarchical prompting (categories → attributes → object/image-level decision); require short explanations for binary labels to improve answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>several (authors state they perform queries several times per prompt but do not specify exact count)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Practical controls (low temperature, repeated sampling with majority selection, structured prompts, and requiring explanations) improve consistency of generated attribute lists for downstream annotation, but the paper does not quantify variance reduction and cautions that GELDA cannot replace human annotation due to residual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GELDA: A generative language annotation framework to reveal visual biases in datasets', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e490.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e490.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM annotation variability (BLIP / OWLv2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variability and reproducibility issues in vision-language model (VLM) annotation using BLIP (ITM) and OWLv2 (OVOD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical evaluation of two VLMs for zero-shot annotation of LLM-generated attributes: BLIP (image-text matching) and OWLv2 (open-vocabulary object detection), reporting average AUC performance and human–VLM agreement analyses that reveal attribute-dependent annotation variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLIP (image-text matching) and OWLv2 (open-vocabulary object detection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer vision / dataset annotation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>zero-shot annotation of attributes generated by an LLM for dataset bias analysis (multilabel classification per category; OVOD outputs thresholded detections)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>VLM model errors and biases, subjective/ambiguous attributes (e.g., 'style', 'color scheme') causing high human annotator disagreement, thresholding behavior (OVOD threshold α = 0.3), and ITM score behavior (hard negatives) making fixed thresholds difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average AUC across attributes (for ground-truth attribute labels), human–VLM agreement/confusion matrices, inter-annotator agreement (consensus by majority), counts of 'no consensus' labels.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>BLIP achieves good average AUC (> 0.7) across DeepFashion, CelebA, and CUB-200; OWLv2 generally underperforms BLIP on these image-level attributes. Human–VLM agreement is high for background/habitat attributes (CUB-200, Stanford Cars) but lower for subjective attributes such as DeepFashion 'style' and SD Living Rooms 'color scheme' (authors report higher fractions of images with no human consensus for these attributes).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>AUC for VLM vs. ground truth; confusion matrices and percentage agreement between BLIP output and aggregated human annotations; inter-annotator consensus fraction.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>BLIP: average AUC > 0.7 (across evaluated datasets). Human evaluation (200–400 images per dataset, 1–3 annotations per image, 12 annotators) showed strong agreement with BLIP for background/habitat attributes but weaker agreement for subjective image-level attributes; no precise numeric agreement rates provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>High inter-annotator disagreement for subjective attributes limiting both human ground-truth reliability and VLM assessment; ITM scoring behavior complicates thresholding decisions; VLM systematic biases/errors limit annotation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Select VLM appropriate for attribute type (ITM for image-level, OVOD for object-level); set detection threshold α = 0.3 for OVOD; compare ITM attribute scores to a base caption score and label the highest-scoring attribute if it exceeds the base (multiclass selection) rather than fixed absolute thresholding; human inspection and consensus labeling in evaluation (majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Human evaluation: 200–400 images per dataset, 1–3 human annotations per image, 12 unique annotators</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BLIP performs well overall (average AUC > 0.7) and matches humans for objective/background attributes, but subjective/image-level attributes (e.g., style, color scheme) show high human and VLM disagreement, indicating annotation variability driven by attribute ambiguity and VLM limitations; therefore GELDA's annotations require visual inspection and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GELDA: A generative language annotation framework to reveal visual biases in datasets', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>On the opportunities and risks of foundation models <em>(Rating: 1)</em></li>
                <li>Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-490",
    "paper_id": "paper-265506856",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-1106)",
            "brief_description": "Autoregressive large language model used to hierarchically generate attribute categories and attribute examples from a user-provided domain caption in the GELDA pipeline; configured with temperature τ=0.3 and prompted for structured list outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_size": null,
            "scientific_domain": "computer vision dataset bias analysis / dataset annotation",
            "experimental_task": "hierarchical generation of visual attribute categories and attribute labels given a domain caption (automatic attribute proposal for dataset analysis)",
            "variability_sources": "Auto-regressive sampling nondeterminism (stochastic outputs given same prompt), sampling temperature (they used τ = 0.3), prompt phrasing / formatting, API/output formatting variability, and implicit random seeds inside the model/API.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Non-deterministic (stochastic) outputs from the LLM; prompt sensitivity; incomplete recall of some ground-truth attributes; cost and latency trade-offs when re-querying the LLM to reduce stochasticity.",
            "mitigation_methods": "Set temperature low (τ = 0.3); enforce structured output formatting (request a single Python list); hierarchical (chain) prompting (categories then attributes then object/image-level decision); require yes/no answers with explanations to improve answer quality; perform multiple queries per prompt and select most-frequent outputs (majority/frequency voting).",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The authors note that auto-regressive LLM outputs are stochastic and that determinism aids reproducibility; they mitigate this by low temperature, structured prompts, and repeated-sampling + frequency voting to pick the most common categories/attributes, but they do not provide a quantitative measurement of residual variability or a formal reproducibility guarantee.",
            "uuid": "e490.0",
            "source_info": {
                "paper_title": "GELDA: A generative language annotation framework to reveal visual biases in datasets",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM stochasticity",
            "name_full": "Stochasticity of auto-regressive large language models",
            "brief_description": "Conceptual observation in the paper that auto-regressive LLMs can produce different outputs for the same prompt; while this captures distributional diversity it undermines deterministic reproducibility for downstream annotation tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP methods applied to dataset annotation",
            "experimental_task": "n/a (conceptual discussion related to LLM-driven attribute generation)",
            "variability_sources": "Sampling randomness inherent to auto-regressive generation, temperature parameter, prompt phrasing/format, API nondeterminism.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Non-deterministic sampling and prompt sensitivity make exact reproducibility difficult without controls.",
            "mitigation_methods": "Recommend increasing determinism (lower temperature), repeated sampling and majority-selection, structured/hierarchical prompting, and requesting explanations to improve answer consistency.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "The paper explicitly acknowledges LLM stochasticity and prescribes repeated sampling plus prompt-engineering and lower temperature to improve consistency, but does not report quantitative variability/reproducibility measurements.",
            "uuid": "e490.1",
            "source_info": {
                "paper_title": "GELDA: A generative language annotation framework to reveal visual biases in datasets",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GELDA reproducibility protocol",
            "name_full": "GELDA's reproducibility protocol: repeated queries, frequency voting, temperature control, and structured prompting",
            "brief_description": "The paper's implemented set of practical steps intended to reduce variability in LLM-generated attribute proposals: use τ=0.3, request consistent list formats, run queries multiple times and select the most frequent categories/attributes, hierarchical queries, and require explanations for binary decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-1106",
            "model_size": null,
            "scientific_domain": "computer vision dataset bias analysis / dataset annotation",
            "experimental_task": "reduce output variability in hierarchical attribute generation for automated dataset annotation",
            "variability_sources": "stochastic sampling from the LLM, prompt phrasing variability, and LLM internal nondeterminism; also cost/latency constraints that limit number of repeated queries.",
            "variability_measured": false,
            "variability_metrics": "Frequency counts of generated categories/attributes used for selection (heuristic aggregation) rather than formal statistical variability metrics.",
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": "Aggregation-by-frequency (pick top-N categories/attributes across multiple queries) used as a practical reproducibility heuristic.",
            "reproducibility_results": null,
            "reproducibility_challenges": "Trade-off between query cost/latency and number of repeats needed to stabilize outputs; residual LLM/VLM systematic errors and missing attribute recall despite aggregation.",
            "mitigation_methods": "Repeated queries per prompt + majority/frequency voting; low temperature (τ = 0.3); enforce structured output (Python list); hierarchical prompting (categories → attributes → object/image-level decision); require short explanations for binary labels to improve answer quality.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "several (authors state they perform queries several times per prompt but do not specify exact count)",
            "key_findings": "Practical controls (low temperature, repeated sampling with majority selection, structured prompts, and requiring explanations) improve consistency of generated attribute lists for downstream annotation, but the paper does not quantify variance reduction and cautions that GELDA cannot replace human annotation due to residual errors.",
            "uuid": "e490.2",
            "source_info": {
                "paper_title": "GELDA: A generative language annotation framework to reveal visual biases in datasets",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "VLM annotation variability (BLIP / OWLv2)",
            "name_full": "Variability and reproducibility issues in vision-language model (VLM) annotation using BLIP (ITM) and OWLv2 (OVOD)",
            "brief_description": "Empirical evaluation of two VLMs for zero-shot annotation of LLM-generated attributes: BLIP (image-text matching) and OWLv2 (open-vocabulary object detection), reporting average AUC performance and human–VLM agreement analyses that reveal attribute-dependent annotation variability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BLIP (image-text matching) and OWLv2 (open-vocabulary object detection)",
            "model_size": null,
            "scientific_domain": "computer vision / dataset annotation",
            "experimental_task": "zero-shot annotation of attributes generated by an LLM for dataset bias analysis (multilabel classification per category; OVOD outputs thresholded detections)",
            "variability_sources": "VLM model errors and biases, subjective/ambiguous attributes (e.g., 'style', 'color scheme') causing high human annotator disagreement, thresholding behavior (OVOD threshold α = 0.3), and ITM score behavior (hard negatives) making fixed thresholds difficult.",
            "variability_measured": true,
            "variability_metrics": "Average AUC across attributes (for ground-truth attribute labels), human–VLM agreement/confusion matrices, inter-annotator agreement (consensus by majority), counts of 'no consensus' labels.",
            "variability_results": "BLIP achieves good average AUC (&gt; 0.7) across DeepFashion, CelebA, and CUB-200; OWLv2 generally underperforms BLIP on these image-level attributes. Human–VLM agreement is high for background/habitat attributes (CUB-200, Stanford Cars) but lower for subjective attributes such as DeepFashion 'style' and SD Living Rooms 'color scheme' (authors report higher fractions of images with no human consensus for these attributes).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "AUC for VLM vs. ground truth; confusion matrices and percentage agreement between BLIP output and aggregated human annotations; inter-annotator consensus fraction.",
            "reproducibility_results": "BLIP: average AUC &gt; 0.7 (across evaluated datasets). Human evaluation (200–400 images per dataset, 1–3 annotations per image, 12 annotators) showed strong agreement with BLIP for background/habitat attributes but weaker agreement for subjective image-level attributes; no precise numeric agreement rates provided in text.",
            "reproducibility_challenges": "High inter-annotator disagreement for subjective attributes limiting both human ground-truth reliability and VLM assessment; ITM scoring behavior complicates thresholding decisions; VLM systematic biases/errors limit annotation fidelity.",
            "mitigation_methods": "Select VLM appropriate for attribute type (ITM for image-level, OVOD for object-level); set detection threshold α = 0.3 for OVOD; compare ITM attribute scores to a base caption score and label the highest-scoring attribute if it exceeds the base (multiclass selection) rather than fixed absolute thresholding; human inspection and consensus labeling in evaluation (majority vote).",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Human evaluation: 200–400 images per dataset, 1–3 human annotations per image, 12 unique annotators",
            "key_findings": "BLIP performs well overall (average AUC &gt; 0.7) and matches humans for objective/background attributes, but subjective/image-level attributes (e.g., style, color scheme) show high human and VLM disagreement, indicating annotation variability driven by attribute ambiguity and VLM limitations; therefore GELDA's annotations require visual inspection and human oversight.",
            "uuid": "e490.3",
            "source_info": {
                "paper_title": "GELDA: A generative language annotation framework to reveal visual biases in datasets",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "On the opportunities and risks of foundation models",
            "rating": 1,
            "sanitized_title": "on_the_opportunities_and_risks_of_foundation_models"
        },
        {
            "paper_title": "Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners",
            "rating": 1,
            "sanitized_title": "prompt_generate_then_cache_cascade_of_foundation_models_makes_strong_fewshot_learners"
        }
    ],
    "cost": 0.016184999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GELDA: A generative language annotation framework to reveal visual biases in datasets
29 Nov 2023</p>
<p>Krish Kabra 
Rice University</p>
<p>Kathleen M Lewis kmlewis@mit.edu 
Massachusetts Institute of Technology</p>
<p>Guha Balakrishnan 
Rice University</p>
<p>GELDA: A generative language annotation framework to reveal visual biases in datasets
29 Nov 20233415DAFA04E85AD6A1F5E6B4C25AED61arXiv:2311.18064v1[cs.CV]
Bias analysis is a crucial step in the process of creating fair datasets for training and evaluating computer vision models.The bottleneck in dataset analysis is annotation, which typically requires: (1) specifying a list of attributes relevant to the dataset domain, and (2) classifying each image-attribute pair.While the second step has made rapid progress in automation, the first has remained human-centered, requiring an experimenter to compile lists of in-domain attributes.However, an experimenter may have limited foresight leading to annotation "blind spots," which in turn can lead to flawed downstream dataset analyses.To combat this, we propose GELDA, a nearly automatic framework that leverages large generative language models (LLMs) to propose and label various attributes for a domain.GELDA takes a user-defined domain caption (e.g., "a photo of a bird," "a photo of a living room") and uses an LLM to hierarchically generate attributes.In addition, GELDA uses the LLM to decide which of a set of visionlanguage models (VLMs) to use to classify each attribute in images.Results on real datasets show that GELDA can generate accurate and diverse visual attribute suggestions, and uncover biases such as confounding between class labels and background features.Results on synthetic datasets demonstrate that GELDA can be used to evaluate the biases of text-to-image diffusion models and generative adversarial networks.Overall, we show that while GELDA is not accurate enough to replace human annotators, it can serve as a complementary tool to help humans analyze datasets in a cheap, low-effort, and flexible manner.</p>
<p>Introduction</p>
<p>Dataset bias analysis is a crucial step in the machine learning model development process and typically occurs when certain attribute combinations are over-or underrepresented.Dataset bias virtually always exists in observational data sampled "from-the-wild."For example, the popular face dataset CelebA has a low percentage of dark-skinned faces, and a significantly higher fraction of young women compared to young men [5], ImageNet is known to have inequalities of visual concepts across its 1000 classes [51], and public chest radiograph datasets are surprisingly predictive of race [13].Measuring dataset bias is the first step towards mitigating bias, which is important for two reasons.First, machine learning models can inherit biases from training data [16,38,55], resulting in potentially unfair behavior when deployed.Second, evaluation data with spurious correlations between visual attributes (e.g., age with gender in CelebA) prevent experimenters from causally linking model performance to specific visual phenomena [5].</p>
<p>Sampling biases may be measured by annotating each image in a dataset with a list of labels, and computing frequency statistics over these labels.The typical annotation workflow involves two steps: (1) compiling a list of attributes to annotate, and (2) annotating those attributes for each example.In the first step, a human dataset designer/engineer typically decides upon a set of key attributes keeping in mind specific downstream use cases of the dataset.For example, face images may be labeled with various attributes that are key for face analysis systems, such as facial expression, skin tone, or perceived age.In the second step, the designer may employ crowdsourced annotators or automated algorithms to annotate the presence/absence of each attribute in each image.</p>
<p>While the second step (annotation) is clearly moving rapidly towards automation with the various advances in object recognition and foundation models [19,26,54], the first step (attribute selection) remains largely human-centered.This raises a subtle issue: the process is only as good as the attributes decided upon by human experimenters, which can leave attribute blind spots that they might not even foresee.For example, while species, color, and size may be annotated for birds in CUB-200 [46], backgrounds and perching behavior are not, though they clearly have imbalances (as we show in our experiments, see Table 2 and Fig. 4).While there is no substitute for human ground truth, an an-notation method that trades off accuracy for flexibility and automation would enable practitioners to quickly and effortlessly gather insights about their dataset.We propose such a method.</p>
<p>The key insight behind our method, called GELDA (for GEnerative Language-based Dataset Annotation), is that generative large language models (LLMs) like GPT [9,35] capture a significant amount of world knowledge [36] and can serve as priors [52] for linking domains to their related attributes.In addition, recent work has demonstrated the effectiveness of using LLMs to select downstream models for given tasks [14].Therefore, we posit that LLMs may be used to automatically curate a rich set of relevant, domainspecific attributes and select vision models suited to the "type" of each attribute (for example, attributes related to objects are suited for object detectors, whereas holistic image attributes, like "color scheme" or "style", are suited for image-text matching models).</p>
<p>Provided a user-specified domain, GELDA queries an LLM (GPT in our experiments) for semantic categories (e.g., living room furniture and color scheme) and attributes per category (e.g., couch and coffee table for the furniture category) that can visually distinguish images from that domain.Second, we use vision-language models (VLMs) to annotate the generated attributes for the images conditioned on the attribute labels.We use a zero-shot captioning model to annotate attributes related to image-level concepts (e.g., background setting, style), and a text-guided object grounding algorithm to annotate attributes related to object-level concepts (e.g., object and part detection).GELDA is automatic with the exception of a few low-cost user inputs (e.g., domain caption, number of desired categories/attributes).</p>
<p>We evaluate our work with both popular computer vision datasets and synthetic image data produced by stateof-the-art text-to-image models (Stable Diffusion [37]) and generative adversarial networks (StyleGAN2 [18]).First, we demonstrate that GPT is capable of recovering a high percentage of labels already annotated in several vision datasets, while also suggesting other relevant concepts.Second, we demonstrate that GELDA can discover previously known and unknown biases in real datasets.Examples include waterbird species in CUB-200 [46] appearing more often in "coastal" or "wetland" backgrounds than land habitats, and luxury brands in Stanford Cars [24] appearing less often in "parking lots" or "gas stations" compared to other brands.Third, we use GELDA to show that living rooms generated by Stable Diffusion almost always have neutral or monochromatic color schemes and contain coffee tables, sofas, area rugs, and throw pillows, and that StyleGAN2 amplifies biases from its training set.Finally, we present some of GELDA's limitations and draw conclusions regarding the safe use of this new data analysis framework.</p>
<p>Related Works</p>
<p>Dataset bias measurement in computer vision</p>
<p>Computer vision datasets are known to have biases [38,44,45,47,48,51], and human-related domains such as faces are particularly scrutinized [2,11,20,22,23,33] because models trained on these data can inherit biases along attributes like race and gender that are protected by the law [21,40,57].Biased benchmarking data can inhibit causal analysis of algorithmic performance due to specific visual factors, due to confounding variables [5,28].Approaches to mitigating dataset bias include collecting more thorough examples [33], using image synthesis to fill distribution gaps [23,40], and resampling [27].</p>
<p>Our work is most closely related to and inspired by REVISE [48], a recent dataset bias analysis tool that also computes visual attribute frequencies.The main distinction between REVISE and GELDA is that REVISE relies on ground truth dataset annotations and focuses on three axes of analysis (object, person, and geography) on natural scenes, while GELDA uses LLM/VLMs to generate and label attributes, and is best suited for closed domain datasets.As a consequence, GELDA sacrifices some accuracy for flexibility and automation.</p>
<p>Several other tools have also been developed to diagnose the weaknesses of machine learning models such as object detectors and action recognizers [3,15,43].Facebook's Fairness Flow [1] and IBM's AI Fairness 360 [6] focus on assessing machine learning model biases as opposed to dataset biases.Amazon SageMaker Clarify [4] also works to detect bias in training data, but along predefined axes.Google's Know Your Data [41] also aims to help mitigate bias issues in image datasets, but their tool currently only works on TensorFlow image datasets.In contrast, GELDA uses LLMs and VLMs to automatically generate annotations for any dataset from a specific domain.</p>
<p>Foundation models and zero-shot learning</p>
<p>Foundation models are large-scale machine learning models pre-trained on vast amounts of data to learn general patterns [7,54].These models serve as fundamental building blocks for various AI applications.Large language models (LLMs), like GPT-3 [9] and its successors [35], have made significant advancements in natural language understanding and generation.They are widely used in various applications, including chatbots, content generation, and language translation.Vision-language models (VLMs), trained on large-scale multimodal datasets, have been shown to perform strongly on downstream tasks such as zero-shot classification, image captioning, and object detection [26,34,39,49,53].Recent works demonstrate the power of combining multiple foundation models to perform tasks.Several works have shown improved classi- fication performance by first prompting LLMs to generate class-specific text descriptions and then using a VLM to combine images and the generated text for classification [25,32,52,56].Another work shows that LLMs can be used to augment text in image-text datasets to assist in zero-shot classification [12].GELDA uses a foundation model composition, but for a new purpose: identifying and labeling domain-specific attributes in image datasets using LLMs and VLMs for bias analysis.</p>
<p>Methods</p>
<p>Our goal is to take a user-specified domain along with a set of images S from that domain, and automatically produce attribute annotations for each image in S from a variety of in-domain categories.Using these attributes, we can then perform bias analyses of S.There are two key challenges to this task: (1) automatically obtaining a list of relevant categories and attributes for the specified domain, and</p>
<p>(2) automatically choosing the appropriate model for evaluating each image-attribute pair.We propose a framework (see Fig. 1) that addresses both of these challenges.</p>
<p>Our insight for the first challenge is that large language models (LLMs) are adept at linking concepts to one another [36,52].We therefore query an LLM for a list of domain categories along with their associated attributes with careful prompting.To address the second challenge, we observe that vision-language models (VLMs) offer a powerful means of performing such evaluations like zero-shot image classification [39] and object grounding [34] from text input alone.The key challenge is determining which VLM to use for a given attribute.Certain image-level attributes like style or color scheme are better suited for image-text matching (ITM) models, whereas determining the presence of an object like a couch is better suited for open-vocabulary object detectors (OVODs).We again use the LLM, this time to provide a decision into the attribute type, and automatically choose the appropriate VLM based on a pre-specified list of VLMs for each attribute type.We describe our method further in the following sections.</p>
<p>Attribute generation with an LLM</p>
<p>We use an LLM to generate attributes in a hierarchical fashion by querying the LLM for categories, followed by querying attribute examples per category.We use this hierarchical form for several reasons.First, we empirically find that querying the LLM directly for attributes results in poor coverage of visual concepts.Second, breaking up the prediction as a "chain" is known to be a successful strategy for controlling LLMs towards more human-like reasoning [50].Third, this approach allows the user control over the number of categories and attributes per category that they desire.First, the user provides a prompt query Q1 of the form: Q1 : "What are N attribute categories that can be used to visually distinguish images described by the caption caption?",where N is a number chosen by the user and caption is a word or phrase describing the data domain (e.g., "birds" or "a headshot photo of a person").Second, for each of the categories {category1, ..., categoryN} returned by Q1, we obtain attribute labels with query Q2: Q2 : "What are M different examples of the category category that can be used to distinguish images described by the caption caption?",where M is again chosen by the user.Lastly, we determine whether each of the N attribute categories relates to imagelevel or object-level concepts with query Q3: Q3: "Are {att1, ..., attM} examples of objects or items?Answer with a yes or no.Explain your answer.",where {att1, att2, ...attM} is the list of M generated attributes for a category.We require a binary yes or no answer in order to automatically filter the response into one of the two appropriate downstream models.Requiring an explanation pushes the model to provide more accurate answers, as demonstrated in prior work [50].</p>
<p>Dealing with stochasticity: Auto-regressive LLMs are stochastic in that they can produce different outputs given the same prompt.While stochasticity helps capture the full output distribution, determinism is helpful for reproducibility.To obtain high-quality attribute labels that are mostly consistent across experiments, we perform the queries in the previous section several times per prompt, and pick the N and M most frequently labeled categories and attributes.</p>
<p>Zero-shot annotation with VLMs</p>
<p>We assume access to pretrained VLMs that take input images and text captions and can perform annotation.In our experiments, we use two VLMs -one for image-text matching (ITM) and one for open-vocabulary object detection (OVOD).To convert LLM-generated attributes into input captions for the VLMs, we define a set of prompt templates that correspond to noun-attribute relationship phrases, e.g., "a noun has attribute" and "a attribute noun."We let the user assign the correct noun-attribute relationship phrase for each of the N categories, though this can likely also be automated by the LLM in future work.</p>
<p>OVOD models output bounding boxes and detection scores, allowing us to label an attribute if its detection score is simply above a threshold α.Output values of current ITM models are less predictable because they are trained with a hard negative mining strategy [26], making it difficult to set a constant threshold.Instead, we compute ITM scores for the M attribute text captions and a generic "base" reference caption describing the domain (same as the one used in query Q1, see Sec. 3.1).Finally, we select the highestscoring caption among the M attributes, and label that attribute as present if it is greater than the base caption score.This process essentially performs multiclass classification.2. Analysis of GPT attribute generation performance on three real datasets.We plot the fraction of significant attribute dimensions (the fraction of principal components that explain 95% of the cumulative variance) versus recall (fraction of real dataset attributes that match with a generated attribute, see Eqn. 1).We plot separate curves for each number of generated categories (N in Sec.3.1) queried by GPT, and each dot represents a different number of queried attributes per category (M ).</p>
<p>Experiments and Results</p>
<p>We evaluate GELDA in several ways.Sec.4.1 and Sec.4.2 quantitatively analyze the performances of the LLM and VLM components.Sec.4.3 demonstrates visual biases discovered by GELDA in real datasets, and Sec.4.4 demonstrates biases discovered in deep generative model outputs.We use the following publicly available models: GPT-3.5 for chat completion, BLIP for ITM, and OWLv2 for OVOD using a threshold of α = 0.3.</p>
<p>Datasets: We used 7 real and synthetic datasets spanning a range of domains to show the generality of our method.We use the test sets of four popular real image datasets: (1) DeepFashion (clothing items) [31], (2) CelebA (human faces) [30], (3) CUB-200 (birds) [46], and (4) Stanford Cars (cars) [24].We use the public Stable Diffusion XL model [37] to generate (5) SD Living Rooms, consisting of 1,024 synthetic images using the caption "a photo of a living room."We use the public StyleGAN2 (SG2) models [18] on the FFHQ [17] and AFHQ [10] datasets to generate 10,000 images each of (6) SG2 Faces and (7) SG2 Dogs (with truncation ψ = 0.7 [8]).</p>
<p>Analysis of attribute generation (LLM)</p>
<p>GPT-3.5 virtually always generates attributes relevant (i.e., related in some manner) to a given data domain (see Supplementary for a list of generated attributes per domain).However, simply generating a huge number of attributes (large M and N ) is a poor strategy for several reasons.First, during data analysis, we want a compact set of features to avoid multi-hypothesis testing.Second, each new added attribute will eventually yield marginal information gain, leading to many redundant attributes.Third, though relatively minor, there is a cost (monetary and time) associated with each query to GPT-3.5 ($0.0020/1K output tokens, average response time of ∼30 seconds/1K output tokens).</p>
<p>With this in mind, we first explore the quality of the attributes generated by GPT with varying values to M and Table 1.VLM performances on ground-truth attribute labels from real datasets.Numbers are average AUC scores across all attributes in each dataset.BLIP achieves good performance across all three datasets, and consistently outperforms OWLv2 likely due to more image-level attributes being labeled.</p>
<p>Model</p>
<p>Dataset N .We consider two figures of merit: recall and effective attribute dimension.Recall is the fraction of real labels that are annotated by GPT, measuring the ability to recover known relevant attributes.We estimate it as:
Recall = 1 |A R | ar∈A R max ag∈A G cos T (a r ), T (a g ) &gt; β , (1)
where A R is the set of the real dataset attributes, A G is the set of generated attributes, T (•) is a text embedding function, cos(•, •) is the cosine similarity between two vectors, and β is a threshold for defining a match in similarity.For our experiments, we set β = 0.8.We define effective attribute dimension as the number of principal components needed to capture 95% of the variance in the embedding space spanned by a set of attributes, {T (a g )} ag∈A G , which intuitively captures the fraction of meaningful attribute dimensions in the set.</p>
<p>We present analyses in Fig. 2 for the three real datasets with rich attribute labels: DeepFashion, CelebA, and CUB-200.We swept N in {3, 5, 10, 15, 20} and M in {3, 5, 7, 10, 15, 20}.As expected, increasing N and M increases recall and decreases the fraction of effective attributes.For DeepFashion and CelebA with N ≥ 10, increasing M leads to minimal gains to recall and large dropoffs in the fraction of effective attributes, indicating that GPT starts to generate redundant attributes.For CUB-200, increasing both N and M results in large increases in recall.However, for large N , increases in M result in sharper decreases in the fraction of effective attributes.For all datasets, N = 10 categories and M = 5 attributes provide a good balance between few attributes and concept coverage.We use these values in subsequent experiments.</p>
<p>Analysis of attribute annotation (VLMs)</p>
<p>Next, we evaluate the performance of BLIP and OWLv2 for annotation.We use the same three real datasets from the previous section (DeepFashion, CelebA, and CUB-200) because they each have ground-truth annotations for many attributes.We predict labels within each category in a multilabel classification scheme.Table 1 presents the average AUC across all dataset attributes for each VLM.BLIP shows good performance (AUC &gt; 0.7) across all datasets Figure 3. Human evaluation of BLIP (VLM) for image-level annotations.We selected one category each from four datasets for human annotation.We find that BLIP annotations match with humans for backgrounds seen in CUB-200 and Stanford Cars.However, BLIP annotations of DeepFashion style and SD Living Rooms color schemes match less strongly.Counts are normalized over rows (i.e., total human annotations) to yield percentages. and generally outperforms OWLv2 likely due to the larger fraction of image-level attributes labeled in the datasets.</p>
<p>We also evaluated BLIP's performance on image-level attributes using human annotators.We selected one GPTgenerated category (not already labeled) from each of four datasets: "style" for DeepFashion, "habitat" for CUB-200, "location" for Stanford Cars, and "color scheme" for SD Living Rooms.We collected human annotations on a subset of each dataset (200-400 images per dataset, 1-3 annotations per image) using 12 unique annotators.Fig. 3 presents confusion matrices reporting the agreement level between human and VLM annotations.There is good agreement between humans and BLIP on background scenery (i.e., bird habitat and car location).However, for DeepFashion there is confusion between pairs of styles ("bohemian" vs. "vintage" and "formal" vs. "minimalist") and for color schemes in SD Living Rooms.We provide visual examples of disagreements between humans and VLMs in Supplementary, as well as inter-annotator disagreements.</p>
<p>Discovering biases in real datasets</p>
<p>We next demonstrate using GELDA to reveal biases in real datasets: DeepFashion, CUB-200, and Stanford Cars.We show a few example attributes with annotations in the top row of Fig. 5.We provide full lists of generated categories Table 2. Categories generated by GELDA along with their associated attributes with the highest frequencies for various datasets.Category corresponds with attributes determined to be object-level, and the majority attribute names have been shortened for brevity.Across all three datasets, several categories contain attributes with frequencies over 30%, which may be cause for concern.More detailed breakdowns for each dataset are shown in Supplementary.appear less often in "parking lots" or at "gas stations" compared to "garages".</p>
<p>and attributes, example annotations, and histograms in Supplementary.Table 2 presents a summary of the generated categories and their associated attributes with the highest frequencies.There are several attributes our method reports as having high occurrences that are not labeled in the orig-inal datasets.For example, over 35% of images in Deep-Fashion contain minimalist-style clothing items, over 35% of images in CUB-200 contain a bird perching on a tree branch, and over 60% of images in Stanford cars contain a brand-new car.There are generally many uneven attribute distributions within categories.</p>
<p>We also demonstrate using GELDA to reveal confounding biases in these datasets, i.e., correlations between attribute pairs.We combined the existing classification labels from CUB-200 and Stanford Cars with annotations of the bird habitats and car locations generated by GELDA, resulting in an analysis presented in Fig. 4. For CUB-200, waterbirds (a bird species known to live on or around water) appear more often in this dataset with water-related environments (e.g."coastal" or "wetland" habitats), consistent with observations from prior work [42].In Stanford Cars, luxury brands (costing more than $70K) such as Bugatti or Ferrari, appear less often in parking lots or gas stations.</p>
<p>Discovering biases of generative image models</p>
<p>We next demonstrate using GELDA to evaluate biases of image generation models using the synthetic datasets SD Living Rooms, SG2 Faces, and SG2 Dogs.We show example attributes and annotations in the bottom row of Fig. 5.We provide a full list of generated categories, attributes, and example annotations in Supplementary.</p>
<p>We plot a histogram of generated attributes for SD Living Rooms in Fig. 6.Several categories have uneven attribute distributions.For example, over 90% of generated living rooms contain a coffee table, sofa, area rug, or throw pillows.Furthermore, less than 10% contain wall sconces, bookshelves, blinds, shutters, or shades.The majority of living rooms also have an "eclectic" layout, a "neutral" color scheme," a "Bohemian" or "Scandinavian" style, and a "cozy and rustic" ambiance.BLIP struggles to annotate generated flooring attributes, with the majority of images receiving a higher score for the base caption.</p>
<p>Next, we analyze differences in attribute distributions  between StyleGAN2 generators and their training distributions (FFHQ and AFHQ-Dogs datasets).We show the differences in attribute frequencies computed by GELDA in Fig. 7.The analysis demonstrates SG2 amplifies bias -for both SG2 Faces and SG2 Dogs, the majority attribute per category in the training dataset almost always has an exacerbated majority in the generated dataset.This is shown in the plot as a negative difference (i.e. higher frequency in the generated dataset) for several of the first attributes in each category (the attributes are sorted in order of descending frequency in the training dataset).For example, in SG2 Faces, over 10% more images contain a smiling facial expression, fair skin tone, brunette hair color, middleaged appearance, hazel eye color, and stubble facial hair in comparison to its corresponding training dataset.For SG2 Dogs, over 20% more images contain a dog with an "alert" posture and over 10% more contain a medium-sized dog in comparison to its training dataset.</p>
<p>Discussion and Conclusion</p>
<p>We propose GELDA, the first semi-automated framework leveraging the power of large language and vision-language models to suggest and annotate attributes for dataset bias analysis.Experimental results demonstrate that GPT can successfully suggest most attributes already labeled in real datasets, while also suggesting new ones that lead to bias discoveries.The VLMs (BLIP and OWLv2) also perform well, though BLIP struggled with certain image-level attributes like styles (e.g., "Bohemian") and color schemes (e.g., "triadic").However, as shown in Supplementary, these attributes also have high levels of inter-annotator disagreement, meaning they are difficult even for humans to judge.These findings lead to the insight that GPT should not just be evaluated in isolation in terms of generated attribute coverage, but also in terms of how well its attributes may be confidently labeled without confusion.A future direction is to develop methods to constrain GPT to do so.GELDA has several limitations.First, it is only as good as its constituent LLM and VLMs, which have their own systematic errors and biases.While VLMs have improved tremendously in the past several years, they are still far from perfect on high-level semantics beyond object recognition.In addition, GPT fails to recall a number of attributes annotated in the real datasets.The combination of these errors indicates that a method like GELDA cannot simply replace humans in an annotation pipeline in terms of attribute coverage or annotation accuracy.Instead, GELDA will be most useful as a fast, flexible, and automated tool to perform coarse dataset analysis, complementing existing annotations.Second, our current implementation selects one image-level attribute per category for an image (multiclass classification), though an image can contain multiple attributes together (e.g.clothing items can both be formal and minimalist, or living rooms can have both monochromatic and neutral color schemes).Third, we evaluated GELDA on datasets with "contained" domains focusing on one type of scene/object.Datasets with complex natural scenes like MS-COCO [29] would pose challenges in attribute generation (a compact prompt cannot describe arbitrary natural scenes) and image-level attribute annotations (object-level annotations should be relatively unharmed).</p>
<p>Ethics and responsible use</p>
<p>GELDA inherits the biases of its LLM/VLM models, which are themselves trained on potentially biased data distributions.Biases of the LLM will mainly result in missed attribute categories which, while undesirable, are not as problematic as VLM biases.VLM biases can result in incorrect annotations, thereby skewing dataset analyses.These inaccuracies may be particularly harmful when dealing with human-centered datasets like faces for which these models are not tuned for.A user should therefore always exercise caution and visually inspect image annotation results to confirm reasonable labels and understand the limitations of the VLMs.We recommend using GELDA not as a replacement to human perceptual ground truth, but as an efficient, flexible, and low-cost method to complement human annotation in dataset bias benchmarking.Material is "silk", "leather", "cotton", "denim", "wool" Fit with "loose fit", "oversized fit", "slim fit", "tailored fit", "athletic fit" Style with "bohemian style", "vintage style", "streetwear style", "formal style", "minimalist style" Pattern with "striped pattern", "floral pattern", "polka dot pattern", "animal print pattern", "plaid pattern" Sleeve length with "sleeveless", "short sleeve", "long sleeve", "cap sleeve", "three-quarter sleeve" Color is "red", "yellow", "blue", "black", "white" Embellishments -"embroidery", "sequins", "beads", "lace", "appliqué" Neckline with "v-neckline", "halter neckline", "boat neckline","off-the-shoulder neckline", "crew neckline" Brand/logo with "gucci logo", "adidas logo", "nike logo", "polo ralph lauren logo", "levi's logo" Type of clothing item -"skirt", "jacket", "pants", "dress", "t-shirt"</p>
<p>CUB-200</p>
<p>"a photo of a bird" Habitat in "forest habitat", "wetland habitat", "desert habitat", "coastal habitat", "urban habitat" Plumage pattern has "solid-colored", "striped", "spotted", "mottled", "barred" Color is "yellow", "blue", "orange", "green", "red" Size is "small", "medium-sized", "large", "tiny", "gigantic" Wing shape has "rounded wings", "pointed wings", "broad wings", "slender wings", "elongated wings" Species is "american robin", "great horned owl", "bald eagle", "blue jay", "sparrow" Perching behavior -"bird perched on a statue", "bird perched on a fence", "bird perched on a rooftop", "bird perched on a tree branch", "bird perched on a power line" Background scenery -"a photo of a bird with a clear blue sky as the background scenery", "a photo of a bird perched on a tree branch with lush green foliage as the background scenery", "a photo of a bird flying above a field of colorful wildflowers as the background scenery", "a photo of a bird standing on a rock with a serene lake as the background scenery", "a photo of a bird standing on a rocky cliff with a vast ocean as the background scenery" Shape has "square", "circle", "triangle", "pentagon", "diamond" Eye color has "yellow eyes", "red eyes", "blue eyes", "green eyes", "brown eyes"
Figure S1
. Visual samples of disagreement between human and BLIP (VLM) annotations.Each row per grid corresponds to three randomly selected images determined by a human to contain an attribute that does not match the BLIP annotated attribute.Rows with less than three images correspond with all images in the dataset that have an annotated attribute disagreement (i.e.rows with no images correspond with no disagreements).The corresponding human annotation is given to the left of each row and BLIP annotation below each image.Attribute names have been shortened for brevity.Each row per grid corresponds to four randomly selected images containing the specified attribute from a category.Rows with less than four images correspond with all images in the dataset determined to contain the attribute (i.e.rows with no images correspond with no annotations).Attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level, and are also shown in images with red bounding box detections.</p>
<p>Figure</p>
<p>Figure2.Analysis of GPT attribute generation performance on three real datasets.We plot the fraction of significant attribute dimensions (the fraction of principal components that explain 95% of the cumulative variance) versus recall (fraction of real dataset attributes that match with a generated attribute, see Eqn. 1).We plot separate curves for each number of generated categories (N in Sec.3.1) queried by GPT, and each dot represents a different number of queried attributes per category (M ).</p>
<p>Figure 4 .
4
Figure 4. Discovered confounding relationships between class labels in CUB-200 and Stanford Cars and environmental attributes generated by GELDA.(a.) Land versus water background bias in CUB-200.Bird species known generally as "waterbirds" (names in red) appear more often with water backgrounds.(b.) Location bias in Stanford Cars.Luxury brands (names in red)appear less often in "parking lots" or at "gas stations" compared to "garages".</p>
<p>Figure 5 .
5
Figure 5. Visual samples with annotations produced GELDA.The attribute names were generated automatically by the LLM (GPT), and the assignment of images to labels was produced by VLMs (BLIP and OWLv2).Attributes determined to be object-level are shown in images with red bounding box detections.Attribute names have been shortened for brevity.</p>
<p>Figure 6 .
6
Figure 6.Distribution of annotated attributes returned by GELDA for the SD Living Room (synthetic) dataset.Bars are grouped by attribute categories in different colors, and attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level.Certain attributes are prominent in the generated images, such as coffee tables and sofas for furniture, throw pillows and area rugs for accessories, neutral and monochromatic hues for color schemes, and Bohemian and Scandinavian styles.</p>
<p>Figure 7 .
7
Figure 7. Comparisons of attribute bias of synthetic Style-GAN2 (SG2) image generators with respect to their training distributions.(a.) SG2 Faces vs. FFHQ.(b.) SG2 Dogs vs. AFHQ.The attributes are ordered from top to bottom in each category by descending frequency in the training dataset.SG2 amplifies bias -the most popular attributes in the training dataset for each category have an even greater majority in the generated dataset, as seen by large negative differences.Category corresponds with attributes determined to be object-level.</p>
<p>Figure S2 .
S2
Figure S2.Distribution of annotated attributes returned by GELDA for real datasets.(a.) DeepFashion, (b.) CUB-200, (c.) Stanford Cars.Bars are grouped by attribute categories in different colors, and attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level.</p>
<p>Figure S3 .
S3
FigureS3.Visual samples with annotations produced by GELDA for DeepFashion.Each row per grid corresponds to four randomly selected images containing the specified attribute from a category.Rows with less than four images correspond with all images in the dataset determined to contain the attribute.Attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level, and are also shown in images with red bounding box detections.</p>
<p>Figure S4 .
S4
Figure S4.Visual samples with annotations produced by GELDA for CUB-200.Each row per grid corresponds to four randomly selected images containing the specified attribute from a category.Rows with less than four images correspond with all images in the dataset determined to contain the attribute.Attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level, and are also shown in images with red bounding box detections.</p>
<p>Figure S5 .
S5
FigureS5.Visual samples with annotations produced by GELDA for Stanford Cars.Each row per grid corresponds to four randomly selected images containing the specified attribute from a category.Rows with less than four images correspond with all images in the dataset determined to contain the attribute (i.e.rows with no images correspond with no annotations).Attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level, and are also shown in images with red bounding box detections.</p>
<p>Figure S6 .
S6
Figure S6.Visual samples with annotations produced by GELDA for SD Living Rooms.Each row per grid corresponds to four randomly selected images containing the specified attribute from a category.Rows with less than four images correspond with all images in the dataset determined to contain the attribute (i.e.rows with no images correspond with no annotations).Attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level, and are also shown in images with red bounding box detections.</p>
<p>Figure S7 .
S7
FigureS7.Visual samples with annotations produced by GELDA for SG2 Faces.Each row per grid corresponds to four randomly selected images containing the specified attribute from a category.Rows with less than four images correspond with all images in the dataset determined to contain the attribute.Attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level, and are also shown in images with red bounding box detections.</p>
<p>Figure S8 .
S8
Figure S8.Visual samples with annotations produced by GELDA for SG2 Dogs.Each row per grid corresponds to four randomly selected images containing the specified attribute from a category.Rows with less than four images correspond with all images in the dataset determined to contain the attribute (i.e.rows with no images correspond with no annotations).Attribute names have been shortened for brevity.Category corresponds with attributes determined to be object-level, and are also shown in images with red bounding box detections.</p>
<p>Figure1.Overview of GELDA.Given a user-specified domain in the form of a caption, GELDA first queries an LLM to generate a set of visual attributes to annotate for an image dataset from that domain.The querying method is hierarchical, in that GELDA prompts the LLM to first generate N attribute categories, then generate M labels per attribute category, and finally describe whether each attribute is object-level or image-level.In the second stage, GELDA uses pre-trained VLMs to automatically annotate the generated attributes for each image.We use the LLM to assign all image-level attributes to a VLM tuned for image-text matching, and all object-level attributes to a VLM for open-vocabulary object detection.Once GELDA has identified and annotated visual attributes, we can then analyze visual biases in the dataset.
Generating Attributes with an LLM (Sec 3.1)Zero-shot Annotation with VLMs (Sec 3.2)Analyzing Visual Biases in Datasets (Sec 4.3-4.4)User InputsGenerated AttributesCaption: "A photo of a living room" N = 4 M = 3Image-levelStyle: 1. Scandinavian 2. Bohemian 3. Industrial Color Scheme: 1. NeutralBookshelfSofaCoffee Table2. Monochromatic 3. AnalogousIndustrialScandinavianBohemian100LLM for Hierarchical attribute generationObject-levelFurniture 1. Sofa 2. Coffee Table 3. Bookshelf Wall Decor 1. Framed artworkVLMs for Image datasetOccurrence (%)20 40 60 802. Wall mirrorImage-text matching3. Wall clocksOpen-set object detection123123123123StyleColorFurniture Wall Decor</p>
<p>Evaluation of image generation algorithms, particularly large text-to-image models, is drawing interest in the vision community.Given that a model like Stable Diffusion can generate any image distribution describable by text, it is desirable to also develop analysis algorithms like GELDA that are equally flexible.Results demonstrate that Stable Dif-
fusion can skew color schemes, accessories, and furniturewhen generating "a photo of a living room." Such insightcan help practitioners engineer their prompts to steer awayfrom unwanted biased attributes. Results also demonstratethat GELDA can measure bias amplifications of a gener-ator with respect to its training distribution, such as withStyleGAN2-produced faces and dogs.</p>
<p>Table S1 .
S1
Prompt templates corresponding to a noun-attribute relationship phrase.Each row details the verb or preposition used to establish the noun-attribute relationship and the corresponding prompt template.noun, attr, and category correspond with the noun describing the image domain, generated attribute, and generated category respectively.
Verb/Preposition Templateisa attr nounhasa noun has attr categorywitha noun with attrina noun in attrfroma noun from attr</p>
<p>Table S2 .
S2
Attributes generated by GPT for real datasets.We detail the generated attributes used in experiments from Sec. 4.3.The template column corresponds with the prompt template from TableS1used to convert the attributes into a text caption.In instances where GPT generates attributes that are already in a caption-like form or the attributes are determined to be object-level, we do not use a prompt template.Category corresponds with attributes determined to be object-level.
DatasetCategoryTemplateAttributesDeepFashion"a photo of aclothing item"
https://github.com/openai/openai-python
https://labelbox.com/product/annotate/
GELDA: A generative language annotation framework to reveal visual biases in datasetsSupplementary MaterialA. Attribute generations with GPTWe access GPT using the OpenAI Python API 1 .For all experiments, we use model version gpt-3.5-turbo-1106and use temperature τ = 0.3.For category and example attribute generations using prompt queries Q1 and Q2, we append the queries with the requests "Output the categories in one Python list" and "Output the examples in one Python list" respectively.We heuristically find that this produces generations with consistent output templates that can be parsed automatically.TableS1contains a list of prompt templates used to describe noun-attribute relationship phrases.TableS2and S3 contain summarized lists of all generated categories, attributes, and image-or object-level decisions, as well as prompt templates used for experiments in Sec.4B.2. Human evaluation of GELDA annotationsWe recruit 12 unique annotators from our work environment and collect annotations using Labelbox Annotate 2 .We curate a subset of images from the DeepFashion, CUB-200, Stanford Cars, and SD Living Rooms datasets.For each image, annotators are asked to select all attributes that best describe the image with respect to the specified category (e.g., "Bohemian" for style in DeepFashion).Annotators are allowed to select more than one attribute, and we provide an "unknown" label for instances in which the annotator believes no attribute adequately describes the image.The final human annotation for an image is determined using the consensus (majority) attribute.For images where the consensus attribute is the "unknown" label, we select the attribute with the next highest annotation count if available (else we leave as "unknown").TableS6contains a summary of the human annotations collected for all the datasets used.We observe that there is a higher percentage of images with no consensus attribute obtained for DeepFashion style and SD Living Rooms color scheme, demonstrating the difficulty for humans to label these attributes.Fig.S1visualizes examples of images where there is a disagreement between human and BLIP annotations.C. Extended results for GELDAStanford Cars"a photo of a car" Body type -"sedan", "convertible", "SUV", "hatchback", "coupe" Color is "blue", "red", "black", "white", "silver" Condition is "vintage", "brand new", "damaged", "restored", "rusty" Year from "1965", "2010", "2021", "2012", "2020" Size -"small car", "compact car", "mid-size car", "SUV", "full-size car" Make/model -"ford mustang", "bmw 3 series", "toyota camry", "honda civic", "chevrolet corvette" Lighting in "natural sunlight", "spotlight", "soft lighting", "dramatic low-key lighting", "neon lighting" Location -"a photo of a car in a parking lot", "a photo of a car on a city street", "a photo of a car in a garage", "a photo of a car on a highway", "a photo of a car at a gas station" Features -"color histogram", "object detection", "texture analysis", "shape features", "edge detection" Surroundings -"a photo of a car parked in a busy city street", "a photo of a car surrounded by palm trees on a tropical beach", "a photo of a car driving on a winding mountain road", "a photo of a car in a crowded parking lot", "a photo of a car parked in a suburban driveway" TableS3.Attributes generated by GPT for synthetic datasets.We detail the generated attributes used in experiments from Sec. 4.4.The template column corresponds with the prompt template from TableS1used to convert the attributes into a text caption.In instances where GPT generates attributes that are already in a caption-like form or the attributes are determined to be object-level, we do not use a prompt template.Category corresponds with attributes determined to be object-level.Dataset Category Template AttributesSD Living Rooms"a photo of a living room" Layout with "symmetrical layout", "eclectic layout", "grid layout", "minimalist layout", "open layout" Wall decor -"framed artwork or paintings", "wall clocks", "gallery wall with a collection of framed photos or prints", "wall-mounted shelves with decorative items or plants", "wall mirrors" Flooring with "hardwood flooring", "laminate flooring", "carpet flooring", "tile flooring", "vinyl flooring" Lighting -"chandelier hanging from the ceiling", "recessed ceiling lights", "table lamp on a side table", "wall sconces on either side of the fireplace", "floor lamp next to a cozy armchair" Furniture -"tv stand", "armchair", "coffee table", "bookshelf", "sofa" Accessories -"wall art", "throw pillows", "area rugs", "table lamps", "decorative vases" Color scheme with "complementary color scheme", "monochromatic color scheme", "analogous color scheme", "triadic color scheme", "neutral color scheme" Window treatments -"curtains", "valances", "shades", "blinds", "shutters" Style with "industrial style", "bohemian style", "traditional style", "scandinavian style", "minimalist style" Overall ambiance is "elegant and luxurious", "modern and minimalist", "bright and airy", "cozy and rustic", "cozy and warm"SG2 Faces"a headshot photo of a person" Glasses -"reading glasses", "safety glasses", "sunglasses", "fashion glasses", "eyeglasses" Gender is "genderqueer", "female", "non-binary", "male", "transgender" Facial expression has "neutral", "smiling", "frowning", "serious", "surprised" Skin tone with "medium skin tone", "dark skin tone", "olive skin tone", "light skin tone", "fair skin tone" Ethnicity is "african American", "caucasian", "asian", "middle eastern", "hispanic/latino" Hair color with "blonde hair", "brunette hair", "red hair", "gray hair", "black hair" Age is "middle-aged", "child", "teenager", "young", "senior" Eye color has "blue eyes", "hazel eyes", "green eyes", "gray eyes", "brown eyes" Hairstyle with "pixie cut", "bald head, "mohawk", "bangs", "long wavy hair" Facial hair with "clean-shaven", "goatee", "full beard", "stubble", "mustache"SG2 Dogs"a photo of a dog" Color is "black", "white", "golden", "brown", "spotted" Size is "giant", "tiny", "medium-sized", "small", "large" Posture with "upright posture", "playful posture", "sitting posture", "alert posture", "lying down posture" Breed is "labrador retriever", "poodle", "golden retriever", "bulldog", "german shepherd" Tail shape has "straight tail", "bushy tail", "docked tail", "sickle-shaped tail", "curled tail" Ear shape with "floppy ears", "folded ears", "pointed ears", "pricked ears", "button ears" Fur length with "long fur", "short fur", "shaggy fur", "medium fur", "curly fur" Environment -"a photo of a dog playing in a lush green park", "a photo of a dog walking on a sandy beach with crashing waves in the background", "a photo of a dog swimming in a crystal-clear lake", "a photo of a dog sitting in a snowy forest during winter", "a photo of a dog exploring a colorful flower garden" Eye color has "green eyes", "hazel eyes", "amber eyes", "brown eyes", "blue eyes" Paw size with "large paw size", "extra-large paw size", "small paw size", "medium paw size", "extra-small paw size"TableS4.VLM performances on ground-truth attribute labels from DeepFashion and CelebA.Numbers are AUC across all attributes in the dataset.We observe good performance (&gt; 0.7) of BLIP and OWLv2 across the majority of attributes.
A I Facebook, Fairness flow. </p>
<p>Analysis of gender inequality in face recognition accuracy. Vítor Albiero, K S Krishnapriya, Kushal Vangara, Kai Zhang, Michael C King, Kevin W Bowyer, Proceedings of the IEEE Winter Conference on Applications of Computer Vision Workshops. the IEEE Winter Conference on Applications of Computer Vision Workshops2020</p>
<p>Diagnosing error in temporal action detectors. Humam Alwassel, Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>. Amazon. Amazon sagemaker clarify. </p>
<p>Towards causal benchmarking of biasin face analysis algorithms. Guha Balakrishnan, Yuanjun Xiong, Wei Xia, Pietro Perona, Deep Learning-Based Face Analytics. Springer20211</p>
<p>Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. Rachel Ke Bellamy, Kuntal Dey, Michael Hind, Stephanie Samuel C Hoffman, Kalapriya Houde, Pranay Kannan, Jacquelyn Lohia, Sameep Martino, Aleksandra Mehta, Mojsilovic, arXiv:1810.019432018arXiv preprint</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, S Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri S Castellon, Annie S Chatterji, Kathleen A Chen, Jared Creel, Dora Davis, Chris Demszky, John Donahue ; Stefano Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren E Gale, Karan Gillespie, Noah D Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas F Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, O Khani, Pang Wei Khattab, Mark S Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Manning, P Suvir, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Benjamin Narayanan, Allen Newman, Juan Carlos Nie, Niebles, J F Hamed Nilforoshan, Giray Nyarko, Armin W Ogut ; Rohan Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Wu, Michihiro Michael Xie, Jiaxuan Yasunaga, You, A Matei, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Kaitlyn Zheng, Percy Zhou, Liang, arXiv:2108.07258On the opportunities and risks of foundation models. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, H Yusuf, Camilo Roohani, Jack Ruiz, Ryan, Dorsa Christopher R'e, Shiori Sadigh, Keshav Sagawa, Andy Santhanam, Krishna Parasuram Shih, Alex Srinivasan, Tamkin, Laurel Orr, Isabel Papadimitriou2021arXiv preprintMoussa Doumbouya, Esin Durmus,</p>
<p>Large scale gan training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, International Conference on Learning Representations. 2018</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, 202033</p>
<p>Stargan v2: Diverse image synthesis for multiple domains. Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Demographic bias in biometrics: A survey on an emerging challenge. Pawel Drozdowski, Christian Rathgeb, Antitza Dantcheva, Naser Damer, Christoph Busch, IEEE Transactions on Technology and Society. 22020</p>
<p>Improving CLIP training with language rewrites. Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Ai recognition of patient race in medical imaging: a modelling study. Judy Wawira Gichoya, Imon Banerjee, Ananth Reddy Bhimireddy, John L Burns, Leo Anthony Celi, Li-Ching Chen, Ramon Correa, Natalie Dullerud, Marzyeh Ghassemi, Shih-Cheng Huang, The Lancet Digital Health. 462022</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Diagnosing error in object detectors. Derek Hoiem, Yodsawalai Chodpathumwan, Qieyun Dai, European conference on computer vision. Springer2012</p>
<p>Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. Kimmo Karkkainen, Jungseock Joo, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2021</p>
<p>A style-based generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Analyzing and improving the image quality of stylegan. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202024</p>
<p>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, arXiv:2304.02643Piotr Dollár, and Ross Girshick. Segment anything. 2023</p>
<p>Face recognition performance: Role of demographic information. Mark J Brendan F Klare, Joshua C Burge, Richard W Klontz, Anil K Vorder Bruegge, Jain, IEEE Transactions on Information Forensics and Security. 762012</p>
<p>Discrimination in the age of algorithms. Jon Kleinberg, Jens Ludwig, Sendhil Mullainathany, Cass R Sunstein, on behalf of The John M. Olin Center for Law, Economics and Business at Harvard Law School. Oxford University Press2019Published by</p>
<p>Empirically analyzing the effect of dataset biases on deep face recognition systems. Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster, Thomas Vetter, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition Workshops2018</p>
<p>Andreas Morel-Forster, and Thomas Vetter. Analyzing and reducing the damage of dataset bias to face recognition with synthetic data. Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition Workshops2019</p>
<p>3d object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, Proceedings of the IEEE international conference on computer vision workshops. the IEEE international conference on computer vision workshops20134</p>
<p>Gist: Generating image-specific text for fine-grained object classification. Kathleen M Lewis, Emily Mu, Adrian V Dalca, John Guttag, arXiv-23072023arXiv e-prints</p>
<p>Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu, Hong Hoi, 2021344</p>
<p>Repair: Removing representation bias by dataset resampling. Yi Li, Nuno Vasconcelos, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019</p>
<p>Benchmarking algorithmic bias in face recognition: An experimental approach using synthetic faces and human evaluation. Hao Liang, Pietro Perona, Guha Balakrishnan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Deep learning face attributes in the wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, Proceedings of International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)2015</p>
<p>Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2016</p>
<p>Enhancing clip with gpt-4: Harnessing visual descriptions as prompts. Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin Mcguinness, E O' Noel, Connor, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Michele Merler, Nalini Ratha, Rogerio S Feris, John R Smith, arXiv:1901.10436Diversity in faces. 2019arXiv preprint</p>
<p>Scaling open-vocabulary object detection. Matthias Minderer, Alexey A Gritsenko, Neil Houlsby, Thirty-seventh Conference on Neural Information Processing Systems. 202323</p>
<p>. OpenAI. Gpt-4 technical report. 22023</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational Linguistics201923</p>
<p>Sdxl: Improving latent diffusion models for high-resolution image synthesis. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach, arXiv:2307.01952202324arXiv preprint</p>
<p>Dataset issues in object recognition. Jean Ponce, Tamara L Berg, Mark Everingham, David A Forsyth, Martial Hebert, Svetlana Lazebnik, Marcin Marszalek, Cordelia Schmid, Bryan C Russell, Antonio Torralba, Toward category-level object recognition. Springer20061</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. 202123</p>
<p>Fair attribute classification through latent space de-biasing. Sunnie Sy Vikram V Ramaswamy, Olga Kim, Russakovsky, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Google People, + Ai Research, Know your data. </p>
<p>Distributionally robust neural networks. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, Percy Liang, In International Conference on Learning Representations. 62020</p>
<p>What actions are needed for understanding human actions in videos?. Olga Gunnar A Sigurdsson, Abhinav Russakovsky, Gupta, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>A deeper look at dataset bias. Domain adaptation in computer vision applications. Tatiana Tommasi, Novi Patricia, Barbara Caputo, Tinne Tuytelaars, 2017</p>
<p>Unbiased look at dataset bias. Antonio Torralba, Alexei A Efros, CVPR 2011. 2011</p>
<p>The caltech-ucsd birds-200-2011 dataset. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, Serge Belongie, 201114California Institute of TechnologyTechnical report</p>
<p>Directional bias amplification. Angelina Wang, Olga Russakovsky, International Conference on Machine Learning. 2021</p>
<p>Revise: A tool for measuring and mitigating bias in visual datasets. Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, Olga Russakovsky, International Journal of Computer Vision. 13072022</p>
<p>GIT: A generative image-to-text transformer for vision and language. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, Transactions on Machine Learning Research. 22022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022354</p>
<p>Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy. Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, Olga Russakovsky, Proceedings of the 2020 conference on fairness, accountability, and transparency. the 2020 conference on fairness, accountability, and transparency20201</p>
<p>Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, Mark Yatskar, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202323</p>
<p>Coca: Contrastive captioners are image-text foundation models. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu, Transactions on Machine Learning Research. 22022</p>
<p>Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, arXiv:2111.11432A new foundation model for computer vision. 20211arXiv preprint</p>
<p>Learning fair representations. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, International conference on machine learning. PMLR2013</p>
<p>Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, Hongsheng Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Leveling down in computer vision: Pareto inefficiencies in fair deep classifiers. Dominik Zietlow, Michael Lohaus, Guha Balakrishnan, Matthäus Kleindessner, Francesco Locatello, Bernhard Schölkopf, Chris Russell, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>            </div>
        </div>

    </div>
</body>
</html>
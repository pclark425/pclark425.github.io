<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4582 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4582</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4582</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-2efae9851606b5f8b16edbbf357b30de5846876e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2efae9851606b5f8b16edbbf357b30de5846876e" target="_blank">Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces an enhanced planning and search methodology designed to boost the creative potential of LLM-based systems, and outperforms the current state-of-the-art, generating at least 2.5 times more top-rated ideas based on 170 seed papers in a Swiss Tournament evaluation.</p>
                <p><strong>Paper Abstract:</strong> Scientific innovation is pivotal for humanity, and harnessing large language models (LLMs) to generate research ideas could transform discovery. However, existing LLMs often produce simplistic and repetitive suggestions due to their limited ability in acquiring external knowledge for innovation. To address this problem, we introduce an enhanced planning and search methodology designed to boost the creative potential of LLM-based systems. Our approach involves an iterative process to purposely plan the retrieval of external knowledge, progressively enriching the idea generation with broader and deeper insights. Validation through automated and human assessments indicates that our framework substantially elevates the quality of generated ideas, particularly in novelty and diversity. The number of unique novel ideas produced by our framework is 3.4 times higher than without it. Moreover, our method outperforms the current state-of-the-art, generating at least 2.5 times more top-rated ideas based on 170 seed papers in a Swiss Tournament evaluation.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4582.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4582.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based pipeline that uses LLMs' internal knowledge to generate search plans, retrieve targeted external papers, iteratively refine seed ideas, and expand them into detailed research proposals—optimizing for novelty and diversity via planning-driven retrieval and self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three-stage pipeline: (1) multi-source initial seed idea generation (LLM uses input paper references, recent influential papers, and 10 scientific discovery methods to produce 15 seed ideas, with self-check/self-critique to reduce hallucination); (2) iterative planning-and-search loop where the LLM produces explicit search plans (key fields/queries) to retrieve external papers (arXiv retriever covering Jan 2022–Aug 2024), uses retrieved papers + prior ideas + input paper to generate and refine new seed ideas (generate 10 -> self-reflect to 3) and replaces prior pool; (3) idea completion where final seed pool is clustered (k-means) and cluster centers used to generate 100 final ideas then expanded into detailed proposals. The pipeline also includes a knowledge-tracking module that monitors references and recent influential papers (social-metric based filtering).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not fully specified for generation in this paper (generic large LLMs); Claude-3.5-Sonnet used as the zero-shot Swiss-tournament ranker for automatic quality evaluation; all-MiniLM-L6-v2 used for embedding similarity in novelty/diversity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-planned retrieval (retrieval-augmented generation): the LLM generates explicit search plans/queries which guide an arXiv paper retriever; embedding-based retrieval (all-MiniLM-L6-v2) and social-metric filtering of recent influential papers; reference-following and topical retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative multi-step synthesis: LLM combines input paper, retrieved papers, and prior seed ideas to produce new ideas; self-reflection/self-critique prunes candidates; clustering (k-means) aggregates idea space and cluster-centers drive final idea generation; hierarchical expansion into proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluation corpus: 170 target papers (from ACL/ICLR/CVPR 2024) selected from an initial ~7,805 corpus (filtered to ~2,000 by LLM-related keywords then to 170 by citation thresholds). Retriever searches arXiv papers from Jan 2022 to Aug 2024.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science (LLM-related research papers across NLP, CV, AI).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas and expanded research proposals (structured final proposals and decomposed method submodules).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Automatic: Swiss-system tournament ranking with Claude-3.5-Sonnet (pairwise comparisons), novelty judged by embedding-similarity against top-10 relevant papers (all-MiniLM-L6-v2, similarity threshold 0.3), diversity measured as proportion non-duplicate (duplication threshold cosine 0.8). Human: expert panel ratings on Novelty, Feasibility, Effectiveness, Overall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Compared to baselines on 170 seed papers: produced 3.4x more unique novel ideas versus a non-planning baseline; produced >=2.5x more top-rated ideas (Swiss Tournament top-rated) than prior state-of-the-art baselines; counts reported in paper: 619 ideas scored at quality level 4 and 2521 scored at level 5 across evaluation (per automatic scoring); >80% of generated ideas were unique (non-duplicate percentage).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to AI-Researcher (Si et al., 2024), AI-Scientist (Lu et al., 2024), and ResearchAgent / Research-Agent (Baek et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Nova consistently outperforms the three baselines: higher Swiss-tournament scores (more top-rated ideas), higher novelty and diversity metrics (3.4x more unique novel ideas; >80% uniqueness vs lower baseline uniqueness), and higher proportion of top-4 human-rated ideas (Nova contributed 37.5% of top-4 ideas in human evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Planning-driven retrieval (using LLMs to produce search plans) plus retrieval significantly improves novelty and diversity compared to naive retrieval or no retrieval; iterative replacement of seed ideas expands search scope; self-reflection and clustering improve final idea quality; automatic and human evaluations exhibit consistent rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper-level: limited number of iterations (performance plateaus after ~3 iterations), planning module lacks explicit reward function, potential for hallucination despite self-checks, computational cost of iterative retrieval and generation, no analysis of model-size scaling for generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Empirically evaluated across iterations: unique novel-idea counts increase across iterations up to iteration 3 then plateau. Evaluated on 170 papers; the paper does not report how performance scales with larger numbers of papers or larger model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4582.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4582.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Researcher: Can LLMs Generate Novel Research Ideas? (Si et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative retrieval-augmented generation approach that uses the Semantic Scholar API to retrieve topic-relevant papers and applies RAG to generate research ideas; demonstrated that LLM-generated ideas can be judged more novel than human experts in a large human study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can LLMs Generate Novel Research Ideas? A LargeScale Human Study with 100+ NLP Researchers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Researcher</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative pipeline that retrieves topic-relevant papers via the Semantic Scholar API then uses retrieval-augmented generation (RAG) with a large language model to produce research ideas; includes evaluation protocols (human study) and an idea-ranking procedure based on pairwise comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this Nova paper summary; original AI-Researcher paper used large LLMs (unspecified here).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Semantic Scholar API retrieval of topic-relevant papers (keyword/topic-based retrieval) used to supply context for RAG-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-augmented generation (RAG): retrieved documents are used as context in prompts to synthesize new research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper's description; retrieval is open-ended via Semantic Scholar (variable depending on query).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>NLP / LLM research topics (evaluated in a large human study across NLP researchers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research idea proposals (short descriptions) and candidate research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human expert judgments on novelty; pairwise comparison ranking method (idea ranking) used to evaluate and rank ideas; reported 71.4% accuracy in distinguishing accepted vs rejected ICLR submissions using their ranking method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in Si et al.: LLM-generated ideas judged more novel than human experts (p < 0.05); ranking method achieved ~71.4% accuracy on distinguishing accepted vs rejected ICLR submissions (reported by Si et al.). In Nova's comparisons, AI-Researcher underperformed Nova on Swiss-tournament and diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to human expert-generated ideas and used as a baseline in Nova's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>In original claims: more novel than human expert-generated ideas (statistically significant). In Nova's re-evaluation: produced fewer top-rated and fewer unique novel ideas than Nova (no exact numeric breakdown in Nova beyond relative multipliers).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrated that RAG-enabled LLM pipelines can produce ideas judged as novel by experts; pairwise ranking is a more reliable automatic evaluator than direct scoring for idea quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>According to Nova: tends to produce repetitive ideas and lacks diversity due to absence of goal-directed planning for retrieval; keyword/topic-based retrieval may not focus acquisition on knowledge that fosters novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail in Nova; original paper includes large human-study scale but does not provide Nova-level iteration-scaling analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4582.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4582.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models (Baek et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative system that builds a knowledge graph / uses co-occurrence entity search to retrieve entities and integrate them into LLM prompts for idea generation over scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative idea-generation agent that constructs a knowledge graph or uses co-occurrence entity extraction from papers; retrieved entities are used as retrieval cues and are incorporated into prompts to an LLM which generates candidate ideas; iteration refines the knowledge graph and idea set.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in Nova; original ResearchAgent uses large LLMs (unspecified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Knowledge-graph / co-occurrence entity extraction from paper text and metadata; entity-based retrieval (entity co-occurrence search).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Prompt-based synthesis where retrieved entities and linked literature are provided as context to an LLM to generate ideas; iterative updating of knowledge graph with newly retrieved information.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in Nova's description of the baseline; intended to run over a literature corpus (variable).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature; used here as a baseline on AI/LLM literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas and candidate directions informed by entities from the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>In Nova: used as baseline; Nova compares Swiss-tournament scores, novelty (embedding-based), and diversity metrics. ResearchAgent's original metrics not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In Nova's experiments, ResearchAgent underperformed Nova on Swiss-tournament, novelty and diversity; exact baseline numbers not enumerated in Nova beyond comparative statements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Nova, AI-Researcher, AI-Scientist in Nova's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ResearchAgent produced fewer top-rated and fewer unique ideas than Nova; Nova attributes ResearchAgent-style methods' limitations to lack of goal-directed planning in retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Entity/co-occurrence knowledge-graph augmentation can provide structured cues for idea generation but may be limited by unguided retrieval scope and produce repetitive ideas without planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Potential limited novelty due to entity/co-occurrence focus and no explicit planning to seek orthogonal/contrasting literature; details and quantitative limitations not fully reported in Nova.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in Nova; original design is iterative and intended to scale with corpus size but Nova does not report scaling trends for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4582.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4582.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery (Lu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system aimed at automated open-ended scientific discovery; includes methods to prevent repetition by treating past generated ideas as negative examples and instructing LLMs on negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agentic discovery framework that automates aspects of the scientific process; includes negative-example based prompt conditioning (past generated ideas used as negative examples) to avoid repeated or low-value suggestions, and agent orchestration for iterative discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in Nova's summary (original AI-Scientist uses large LLMs; details not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Likely retrieval-augmented approaches and agentic tool use (not detailed in Nova); Nova only reports the negative-example technique for diversity control.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agent-driven iterative synthesis with mechanisms to discourage regeneration of prior ideas (negative-example conditioning); full synthesis pipeline not specified in Nova.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in Nova's comparative description.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific discovery/AI-driven research ideation (evaluated on AI research topics as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated discoveries / research ideas / potential experimental directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in Nova beyond being included as a baseline; Nova uses Swiss-tournament, novelty, diversity to compare.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In Nova's experiments AI-Scientist was the second-best baseline behind Nova (performed better than ResearchAgent and AI-Researcher in Nova's ranking), but did not match Nova's novelty/diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Nova, ResearchAgent, AI-Researcher.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperformed some baselines in Nova's experiments but was outperformed by Nova on top-rated idea proportion and novelty/diversity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Negative-example conditioning is an effective strategy to reduce repetition in generated ideas, but without planning-driven retrieval it may still miss breadth/depth required for higher novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Still susceptible to limited diversity if external knowledge acquisition is not goal-directed; details of other limitations not provided in Nova.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in Nova.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4582.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4582.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scimon: Scientific Inspiration Machines Optimized for Novelty (Q. Wang et al., 2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that enriches idea generation by incorporating co-occurrence entities and external knowledge into LLM prompts to optimize for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach that augments LLM idea generation with retrieved external entities (co-occurrence entities) and other knowledge sources, prompting the LLM to generate ideas grounded in retrieved entities to enhance novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in Nova; implied use of large LLMs in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Entity co-occurrence retrieval (entity extraction from literature to identify promising combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Prompt-based synthesis integrating retrieved co-occurrence entities with LLM internal knowledge to propose novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in Nova's description.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature / idea generation (applied to scientific inspiration tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research inspirations / idea suggestions optimized for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Optimized for novelty metrics (not fully specified in Nova); Nova references it as an example of entity-based enrichment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to improve novelty by enriching prompts with co-occurrence entities (no direct numeric comparison in Nova beyond qualitative mention).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work; compared qualitatively to Nova and other baselines within Nova's experimentation (Scimon-style methods are represented by entity/co-occurrence baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>In Nova's evaluation, entity/co-occurrence-based methods underperform Nova in diversity/novelty when not combined with planning-driven retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Co-occurrence entity enrichment can increase novelty but benefits are limited without goal-directed search/planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Risk of constrained search scope and generation of repetitive ideas if retrieval is unguided; exact limitations depend on implementation details in original Scimon paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in Nova.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4582.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4582.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG+LLM (Gu & Krenn)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models (Gu & Krenn, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach combining knowledge-graph construction from literature with LLM-based idea generation and human-expert evaluation to surface interesting research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KG+LLM idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a knowledge graph from scientific literature (entities, relations) and uses graph-derived signals (e.g., novel entity pairings, paths) to condition LLM prompting for generating candidate research ideas, followed by human-expert evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in Nova's description; original paper likely used large LLM(s).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Knowledge-graph construction from parsed paper content (entity/relation extraction) and graph-based retrieval of candidate novel links.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Graph-guided prompting: LLMs synthesize ideas conditioned on graph-discovered novel combinations or paths; human-expert evaluation validates interest.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in Nova's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature across domains; evaluated in their paper with human expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Interesting research ideas (graph-informed proposals) evaluated by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human-expert evaluations (qualitative judgments) and novel-link discovery metrics in original work; not enumerated in Nova beyond mention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported human-expert evaluation results in original paper (not detailed numerically in Nova).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Presented alongside other LLM+KG methods in related work and compared qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in Nova; treated as a promising approach but not directly compared numerically in Nova's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining structured knowledge (KG) with LLM prompting can yield interesting, expert-valued ideas; KG provides explicit structure to guide generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>KG construction quality and coverage limit downstream idea generation; may still require goal-directed planning to find the most innovative KG subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in Nova.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can LLMs Generate Novel Research Ideas? A LargeScale Human Study with 100+ NLP Researchers <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models <em>(Rating: 2)</em></li>
                <li>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions <em>(Rating: 1)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 1)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4582",
    "paper_id": "paper-2efae9851606b5f8b16edbbf357b30de5846876e",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Nova",
            "name_full": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
            "brief_description": "An LLM-based pipeline that uses LLMs' internal knowledge to generate search plans, retrieve targeted external papers, iteratively refine seed ideas, and expand them into detailed research proposals—optimizing for novelty and diversity via planning-driven retrieval and self-reflection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Nova",
            "system_description": "Three-stage pipeline: (1) multi-source initial seed idea generation (LLM uses input paper references, recent influential papers, and 10 scientific discovery methods to produce 15 seed ideas, with self-check/self-critique to reduce hallucination); (2) iterative planning-and-search loop where the LLM produces explicit search plans (key fields/queries) to retrieve external papers (arXiv retriever covering Jan 2022–Aug 2024), uses retrieved papers + prior ideas + input paper to generate and refine new seed ideas (generate 10 -&gt; self-reflect to 3) and replaces prior pool; (3) idea completion where final seed pool is clustered (k-means) and cluster centers used to generate 100 final ideas then expanded into detailed proposals. The pipeline also includes a knowledge-tracking module that monitors references and recent influential papers (social-metric based filtering).",
            "llm_model_used": "Not fully specified for generation in this paper (generic large LLMs); Claude-3.5-Sonnet used as the zero-shot Swiss-tournament ranker for automatic quality evaluation; all-MiniLM-L6-v2 used for embedding similarity in novelty/diversity checks.",
            "extraction_technique": "LLM-planned retrieval (retrieval-augmented generation): the LLM generates explicit search plans/queries which guide an arXiv paper retriever; embedding-based retrieval (all-MiniLM-L6-v2) and social-metric filtering of recent influential papers; reference-following and topical retrieval.",
            "synthesis_technique": "Iterative multi-step synthesis: LLM combines input paper, retrieved papers, and prior seed ideas to produce new ideas; self-reflection/self-critique prunes candidates; clustering (k-means) aggregates idea space and cluster-centers drive final idea generation; hierarchical expansion into proposals.",
            "number_of_papers": "Evaluation corpus: 170 target papers (from ACL/ICLR/CVPR 2024) selected from an initial ~7,805 corpus (filtered to ~2,000 by LLM-related keywords then to 170 by citation thresholds). Retriever searches arXiv papers from Jan 2022 to Aug 2024.",
            "domain_or_topic": "Computer science (LLM-related research papers across NLP, CV, AI).",
            "output_type": "Research ideas and expanded research proposals (structured final proposals and decomposed method submodules).",
            "evaluation_metrics": "Automatic: Swiss-system tournament ranking with Claude-3.5-Sonnet (pairwise comparisons), novelty judged by embedding-similarity against top-10 relevant papers (all-MiniLM-L6-v2, similarity threshold 0.3), diversity measured as proportion non-duplicate (duplication threshold cosine 0.8). Human: expert panel ratings on Novelty, Feasibility, Effectiveness, Overall.",
            "performance_results": "Compared to baselines on 170 seed papers: produced 3.4x more unique novel ideas versus a non-planning baseline; produced &gt;=2.5x more top-rated ideas (Swiss Tournament top-rated) than prior state-of-the-art baselines; counts reported in paper: 619 ideas scored at quality level 4 and 2521 scored at level 5 across evaluation (per automatic scoring); &gt;80% of generated ideas were unique (non-duplicate percentage).",
            "comparison_baseline": "Compared to AI-Researcher (Si et al., 2024), AI-Scientist (Lu et al., 2024), and ResearchAgent / Research-Agent (Baek et al., 2024).",
            "performance_vs_baseline": "Nova consistently outperforms the three baselines: higher Swiss-tournament scores (more top-rated ideas), higher novelty and diversity metrics (3.4x more unique novel ideas; &gt;80% uniqueness vs lower baseline uniqueness), and higher proportion of top-4 human-rated ideas (Nova contributed 37.5% of top-4 ideas in human evaluation).",
            "key_findings": "Planning-driven retrieval (using LLMs to produce search plans) plus retrieval significantly improves novelty and diversity compared to naive retrieval or no retrieval; iterative replacement of seed ideas expands search scope; self-reflection and clustering improve final idea quality; automatic and human evaluations exhibit consistent rankings.",
            "limitations_challenges": "Paper-level: limited number of iterations (performance plateaus after ~3 iterations), planning module lacks explicit reward function, potential for hallucination despite self-checks, computational cost of iterative retrieval and generation, no analysis of model-size scaling for generation quality.",
            "scaling_behavior": "Empirically evaluated across iterations: unique novel-idea counts increase across iterations up to iteration 3 then plateau. Evaluated on 170 papers; the paper does not report how performance scales with larger numbers of papers or larger model sizes.",
            "uuid": "e4582.0",
            "source_info": {
                "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AI-Researcher",
            "name_full": "AI-Researcher: Can LLMs Generate Novel Research Ideas? (Si et al., 2024)",
            "brief_description": "An iterative retrieval-augmented generation approach that uses the Semantic Scholar API to retrieve topic-relevant papers and applies RAG to generate research ideas; demonstrated that LLM-generated ideas can be judged more novel than human experts in a large human study.",
            "citation_title": "Can LLMs Generate Novel Research Ideas? A LargeScale Human Study with 100+ NLP Researchers",
            "mention_or_use": "use",
            "system_name": "AI-Researcher",
            "system_description": "Iterative pipeline that retrieves topic-relevant papers via the Semantic Scholar API then uses retrieval-augmented generation (RAG) with a large language model to produce research ideas; includes evaluation protocols (human study) and an idea-ranking procedure based on pairwise comparisons.",
            "llm_model_used": "Not specified in this Nova paper summary; original AI-Researcher paper used large LLMs (unspecified here).",
            "extraction_technique": "Semantic Scholar API retrieval of topic-relevant papers (keyword/topic-based retrieval) used to supply context for RAG-based generation.",
            "synthesis_technique": "Retrieval-augmented generation (RAG): retrieved documents are used as context in prompts to synthesize new research ideas.",
            "number_of_papers": "Not specified in this paper's description; retrieval is open-ended via Semantic Scholar (variable depending on query).",
            "domain_or_topic": "NLP / LLM research topics (evaluated in a large human study across NLP researchers).",
            "output_type": "Research idea proposals (short descriptions) and candidate research directions.",
            "evaluation_metrics": "Human expert judgments on novelty; pairwise comparison ranking method (idea ranking) used to evaluate and rank ideas; reported 71.4% accuracy in distinguishing accepted vs rejected ICLR submissions using their ranking method.",
            "performance_results": "Reported in Si et al.: LLM-generated ideas judged more novel than human experts (p &lt; 0.05); ranking method achieved ~71.4% accuracy on distinguishing accepted vs rejected ICLR submissions (reported by Si et al.). In Nova's comparisons, AI-Researcher underperformed Nova on Swiss-tournament and diversity metrics.",
            "comparison_baseline": "Compared to human expert-generated ideas and used as a baseline in Nova's experiments.",
            "performance_vs_baseline": "In original claims: more novel than human expert-generated ideas (statistically significant). In Nova's re-evaluation: produced fewer top-rated and fewer unique novel ideas than Nova (no exact numeric breakdown in Nova beyond relative multipliers).",
            "key_findings": "Demonstrated that RAG-enabled LLM pipelines can produce ideas judged as novel by experts; pairwise ranking is a more reliable automatic evaluator than direct scoring for idea quality.",
            "limitations_challenges": "According to Nova: tends to produce repetitive ideas and lacks diversity due to absence of goal-directed planning for retrieval; keyword/topic-based retrieval may not focus acquisition on knowledge that fosters novelty.",
            "scaling_behavior": "Not discussed in detail in Nova; original paper includes large human-study scale but does not provide Nova-level iteration-scaling analysis.",
            "uuid": "e4582.1",
            "source_info": {
                "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models (Baek et al., 2024)",
            "brief_description": "An iterative system that builds a knowledge graph / uses co-occurrence entity search to retrieve entities and integrate them into LLM prompts for idea generation over scientific literature.",
            "citation_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "mention_or_use": "use",
            "system_name": "ResearchAgent",
            "system_description": "Iterative idea-generation agent that constructs a knowledge graph or uses co-occurrence entity extraction from papers; retrieved entities are used as retrieval cues and are incorporated into prompts to an LLM which generates candidate ideas; iteration refines the knowledge graph and idea set.",
            "llm_model_used": "Not specified in Nova; original ResearchAgent uses large LLMs (unspecified in this paper).",
            "extraction_technique": "Knowledge-graph / co-occurrence entity extraction from paper text and metadata; entity-based retrieval (entity co-occurrence search).",
            "synthesis_technique": "Prompt-based synthesis where retrieved entities and linked literature are provided as context to an LLM to generate ideas; iterative updating of knowledge graph with newly retrieved information.",
            "number_of_papers": "Not specified in Nova's description of the baseline; intended to run over a literature corpus (variable).",
            "domain_or_topic": "General scientific literature; used here as a baseline on AI/LLM literature.",
            "output_type": "Research ideas and candidate directions informed by entities from the knowledge graph.",
            "evaluation_metrics": "In Nova: used as baseline; Nova compares Swiss-tournament scores, novelty (embedding-based), and diversity metrics. ResearchAgent's original metrics not detailed here.",
            "performance_results": "In Nova's experiments, ResearchAgent underperformed Nova on Swiss-tournament, novelty and diversity; exact baseline numbers not enumerated in Nova beyond comparative statements.",
            "comparison_baseline": "Compared against Nova, AI-Researcher, AI-Scientist in Nova's experiments.",
            "performance_vs_baseline": "ResearchAgent produced fewer top-rated and fewer unique ideas than Nova; Nova attributes ResearchAgent-style methods' limitations to lack of goal-directed planning in retrieval.",
            "key_findings": "Entity/co-occurrence knowledge-graph augmentation can provide structured cues for idea generation but may be limited by unguided retrieval scope and produce repetitive ideas without planning.",
            "limitations_challenges": "Potential limited novelty due to entity/co-occurrence focus and no explicit planning to seek orthogonal/contrasting literature; details and quantitative limitations not fully reported in Nova.",
            "scaling_behavior": "Not discussed in Nova; original design is iterative and intended to scale with corpus size but Nova does not report scaling trends for this baseline.",
            "uuid": "e4582.2",
            "source_info": {
                "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AI-Scientist",
            "name_full": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery (Lu et al., 2024)",
            "brief_description": "A system aimed at automated open-ended scientific discovery; includes methods to prevent repetition by treating past generated ideas as negative examples and instructing LLMs on negative examples.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "use",
            "system_name": "AI-Scientist",
            "system_description": "Agentic discovery framework that automates aspects of the scientific process; includes negative-example based prompt conditioning (past generated ideas used as negative examples) to avoid repeated or low-value suggestions, and agent orchestration for iterative discovery.",
            "llm_model_used": "Not specified in Nova's summary (original AI-Scientist uses large LLMs; details not reported here).",
            "extraction_technique": "Likely retrieval-augmented approaches and agentic tool use (not detailed in Nova); Nova only reports the negative-example technique for diversity control.",
            "synthesis_technique": "Agent-driven iterative synthesis with mechanisms to discourage regeneration of prior ideas (negative-example conditioning); full synthesis pipeline not specified in Nova.",
            "number_of_papers": "Not specified in Nova's comparative description.",
            "domain_or_topic": "General scientific discovery/AI-driven research ideation (evaluated on AI research topics as baseline).",
            "output_type": "Automated discoveries / research ideas / potential experimental directions.",
            "evaluation_metrics": "Not specified in Nova beyond being included as a baseline; Nova uses Swiss-tournament, novelty, diversity to compare.",
            "performance_results": "In Nova's experiments AI-Scientist was the second-best baseline behind Nova (performed better than ResearchAgent and AI-Researcher in Nova's ranking), but did not match Nova's novelty/diversity.",
            "comparison_baseline": "Compared against Nova, ResearchAgent, AI-Researcher.",
            "performance_vs_baseline": "Outperformed some baselines in Nova's experiments but was outperformed by Nova on top-rated idea proportion and novelty/diversity metrics.",
            "key_findings": "Negative-example conditioning is an effective strategy to reduce repetition in generated ideas, but without planning-driven retrieval it may still miss breadth/depth required for higher novelty.",
            "limitations_challenges": "Still susceptible to limited diversity if external knowledge acquisition is not goal-directed; details of other limitations not provided in Nova.",
            "scaling_behavior": "Not discussed in Nova.",
            "uuid": "e4582.3",
            "source_info": {
                "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SciMon",
            "name_full": "Scimon: Scientific Inspiration Machines Optimized for Novelty (Q. Wang et al., 2024b)",
            "brief_description": "A system that enriches idea generation by incorporating co-occurrence entities and external knowledge into LLM prompts to optimize for novelty.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "system_name": "Scimon",
            "system_description": "Approach that augments LLM idea generation with retrieved external entities (co-occurrence entities) and other knowledge sources, prompting the LLM to generate ideas grounded in retrieved entities to enhance novelty.",
            "llm_model_used": "Not specified in Nova; implied use of large LLMs in original work.",
            "extraction_technique": "Entity co-occurrence retrieval (entity extraction from literature to identify promising combinations).",
            "synthesis_technique": "Prompt-based synthesis integrating retrieved co-occurrence entities with LLM internal knowledge to propose novel ideas.",
            "number_of_papers": "Not specified in Nova's description.",
            "domain_or_topic": "Scientific literature / idea generation (applied to scientific inspiration tasks).",
            "output_type": "Research inspirations / idea suggestions optimized for novelty.",
            "evaluation_metrics": "Optimized for novelty metrics (not fully specified in Nova); Nova references it as an example of entity-based enrichment.",
            "performance_results": "Reported to improve novelty by enriching prompts with co-occurrence entities (no direct numeric comparison in Nova beyond qualitative mention).",
            "comparison_baseline": "Mentioned in related work; compared qualitatively to Nova and other baselines within Nova's experimentation (Scimon-style methods are represented by entity/co-occurrence baselines).",
            "performance_vs_baseline": "In Nova's evaluation, entity/co-occurrence-based methods underperform Nova in diversity/novelty when not combined with planning-driven retrieval.",
            "key_findings": "Co-occurrence entity enrichment can increase novelty but benefits are limited without goal-directed search/planning.",
            "limitations_challenges": "Risk of constrained search scope and generation of repetitive ideas if retrieval is unguided; exact limitations depend on implementation details in original Scimon paper.",
            "scaling_behavior": "Not discussed in Nova.",
            "uuid": "e4582.4",
            "source_info": {
                "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "KG+LLM (Gu & Krenn)",
            "name_full": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models (Gu & Krenn, 2024)",
            "brief_description": "An approach combining knowledge-graph construction from literature with LLM-based idea generation and human-expert evaluation to surface interesting research ideas.",
            "citation_title": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models",
            "mention_or_use": "mention",
            "system_name": "KG+LLM idea generation",
            "system_description": "Constructs a knowledge graph from scientific literature (entities, relations) and uses graph-derived signals (e.g., novel entity pairings, paths) to condition LLM prompting for generating candidate research ideas, followed by human-expert evaluation.",
            "llm_model_used": "Not specified in Nova's description; original paper likely used large LLM(s).",
            "extraction_technique": "Knowledge-graph construction from parsed paper content (entity/relation extraction) and graph-based retrieval of candidate novel links.",
            "synthesis_technique": "Graph-guided prompting: LLMs synthesize ideas conditioned on graph-discovered novel combinations or paths; human-expert evaluation validates interest.",
            "number_of_papers": "Not specified in Nova's summary.",
            "domain_or_topic": "Scientific literature across domains; evaluated in their paper with human expert judgments.",
            "output_type": "Interesting research ideas (graph-informed proposals) evaluated by experts.",
            "evaluation_metrics": "Human-expert evaluations (qualitative judgments) and novel-link discovery metrics in original work; not enumerated in Nova beyond mention.",
            "performance_results": "Reported human-expert evaluation results in original paper (not detailed numerically in Nova).",
            "comparison_baseline": "Presented alongside other LLM+KG methods in related work and compared qualitatively.",
            "performance_vs_baseline": "Not quantified in Nova; treated as a promising approach but not directly compared numerically in Nova's experiments.",
            "key_findings": "Combining structured knowledge (KG) with LLM prompting can yield interesting, expert-valued ideas; KG provides explicit structure to guide generation.",
            "limitations_challenges": "KG construction quality and coverage limit downstream idea generation; may still require goal-directed planning to find the most innovative KG subspaces.",
            "scaling_behavior": "Not discussed in Nova.",
            "uuid": "e4582.5",
            "source_info": {
                "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas? A LargeScale Human Study with 100+ NLP Researchers",
            "rating": 2
        },
        {
            "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2
        },
        {
            "paper_title": "Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models",
            "rating": 2
        },
        {
            "paper_title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
            "rating": 1
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 1
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 1
        }
    ],
    "cost": 0.0201675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas</h1>
<p>Xiang $\mathbf{H u}^{4, <em>}$, Hongyu $\mathbf{F u}^{5, </em>}$, Jinge Wang ${ }^{1}$, Yifeng Wang ${ }^{6}$, Zhikun $\mathbf{L i}^{7}$<br>Renjun $\mathbf{X u}^{2}$, Yu Lu ${ }^{1}$, Yaochu Jin ${ }^{1}$, Lili Pan ${ }^{1,3}$, Zhenzhong Lan ${ }^{1,1}$<br>Westlake University ${ }^{1}$ Zhejiang University ${ }^{2}$<br>University of Electronic Science and Technology of China ${ }^{3}$<br>China Life R\&amp;D Center ${ }^{4}$ Carnegie Mellon University ${ }^{5}$ Southeast University ${ }^{6}$ University of Oxford ${ }^{7}$<br>huxiang2022@e-chinalife.com hongyuf@andrew.cmu.edu wangjinge@westlake.edu.cn<br>lilipan@uestc.edu.cn lanzhenzhong@westlake.edu.cn</p>
<h4>Abstract</h4>
<p>Scientific innovation is pivotal for humanity, and harnessing large language models (LLMs) to generate research ideas could transform discovery. However, existing LLMs often produce simplistic and repetitive suggestions due to their limited ability in acquiring external knowledge for innovation. To address this problem, we introduce an enhanced planning and search methodology designed to boost the creative potential of LLM-based systems. Our approach involves an iterative process to purposely plan the retrieval of external knowledge, progressively enriching the idea generation with broader and deeper insights. Validation through automated and human assessments indicates that our framework substantially elevates the quality of generated ideas, particularly in novelty and diversity. The number of unique novel ideas produced by our framework is 3.4 times higher than without it. Moreover, our method outperforms the current state-of-theart, generating at least 2.5 times more top-rated ideas based on 170 seed papers in a Swiss Tournament evaluation.</p>
<h2>1 Introduction</h2>
<p>In recent years, LLMs have demonstrated remarkable progress across various challenging tasks, including solving mathematical problems (RomeraParedes et al., 2024), proving mathematical theory (Wang et al., 2024a), and generating code to solve analytical or computational tasks (Huang et al., 2024). These progresses have opened up new possibilities to utilize LLMs to accelerate research (Wang et al., 2023a), including generating novel research ideas (Si et al., 2024; Wang et al., 2024b; Baek et al., 2024).</p>
<p>Our work is dedicated to addressing the challenge of employing LLMs to produce high-caliber</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Nova's Performance. The Left: Comparison with the state-of-the-arts. Nova significantly outperforms other agents (Si et al., 2024; Baek et al., 2024; Lu et al., 2024) in generating high-quality ideas (Swiss Tournament Score is 5). The Right: The number of unique novel ideas at each iteration step. The iterative planning framework significantly enhances the generation of unique novel ideas, increasing by 3.4 times from the baseline.
research ideas, with an emphasis on enhancing their novelty and diversity. Existing studies (Wang et al., 2024b; Si et al., 2024) tackle this challenge by integrating additional knowledge into the idea generation process. Wang et al. (2024b) enrich the process by incorporating co-occurrence entities with existing knowledge, prompting LLMs to generate ideas based on these entities. Si et al. (2024) suggest an iterative approach to retrieve topic-relevant papers through the Semantic Scholar API, utilizing retrieval-augmented generation (RAG) for idea generation. They find that "LLM-generated ideas are judged as more novel ( $p&lt;0.05$ ) than human expert". However, they also show that "LLMs lack diversity in idea generation". We argue that this repetitive problem is due to the constrained scope and lack of direction in knowledge acquisition within these methods.</p>
<p>Broadening the search scope, both in terms of breadth and depth, presents a significant challenge. The crux of the issue lies in determining which knowledge to retrieve. Traditional methods of en-</p>
<p>tity and keyword retrieval are not goal-oriented and frequently yield knowledge that is not conducive to fostering innovation.</p>
<p>In order to address the above problem, we introduce an iterative planning framework for LLMbased idea generation that specifically targets the enhancement of the novelty and diversity of the ideas produced. Starting with seed ideas that generated using different scientific discovery methods, our framework undergoes multiple iterations of planning and searching. In each iteration, the model is tasked with devising a search plan aimed at identifying papers that will enhance the novelty and diversity of the current set of ideas.</p>
<p>As depicted in Fig. 1,the proposed iterative planning framework significantly enhances the quality of ideas generated from recent 170 LLM-related papers (from top conferences like ACL, ICLR, and CVPR). The number of high-quality ideas (as measured by the Swiss Tournament Score (Si et al., 2024)) is at least 2.5 times greater than those produced by other state-of-the-art methods. Moreover, the number of unique novel ideas generated by our iterative planning framework is 3.4 times higher compared to approaches that do not incorporate such a framework.</p>
<h2>2 Related work</h2>
<h3>2.1 LLM-based Scientific Innovation</h3>
<p>In the past year, several studies on LLM-based scientific innovation (Yang et al., 2024; Baek et al., 2024; Lu et al., 2024; Wang et al., 2024b; Gu and Krenn, 2024; Li et al., 2024) have been proposed, garnering significant attention from the LLM community. Among these studies, Baek et al. (2024) introduces a research agent that utilizes an external knowledge graph for co-occurrence entity search and integrates retrieved entities into idea generation of LLMs. To avoid generating similar ideas, Lu et al. (2024) treat past generated ideas as negative examples and instruct the LLM on what constitutes a negative example. To explore more external knowledge for innovation, some other works (Wang et al., 2024b; Gu and Krenn, 2024) propose prompting the LLM to generate ideas integrated with external knowledge, such as retrieved external entities or problem-solution pairs.</p>
<p>Concurrent with our research, Si et al. (2024) introduce AI-Researcher, which, for the first time, demonstrates that LLMs can generate ideas deemed more novel than those written by human experts.</p>
<p>In addition, they point out that using LLMs to directly evaluate different dimensions of scientific ideas is unreliable and propose an idea ranking method based on pairwise comparison, achieving an accuracy of $71.4 \%$ in distinguishing accepted and rejected submissions on real ICLR 2024 data.</p>
<p>Although effective, the above approach often generates repetitive ideas (Si et al., 2024) due to the lack of direction in acquiring new knowledge. In contrast, our method provides a plan for searching for new knowledge and suffers less from the repetitive problem.</p>
<h3>2.2 Reasoning and Planning</h3>
<p>Reasoning has been proven to be an effective technique for enhancing the problem-solving capabilities of LLMs, and several studies have been conducted to further promote LLMs' reasoning abilities. Wei et al. (2022) propose chain of thought (CoT), which involves guiding LLMs to solve complex problems by generating a step-by-step reasoning process. Later, Wang et al. (2023c) improve CoT by sampling and comparing diverse reasoning pathways to enhance the consistency of the reasoning process. To solve problems harder than the exemplars shown in prompts, Zhou et al. (2023) propose to break down the complex problem into a series of simpler subproblems and then solve them in sequence. Generalizing from Chain of Thought (CoT), Yao et al. (2023a) propose the Tree of Thought (ToT) framework, enabling LLMs to explore multiple reasoning paths and conduct selfevaluations when determining the next action. To enable more effective exploration of the solution space, Xie et al. (2024) enhance the reasoning capabilities of LLMs by introducing Monte Carlo Tree Search (MCTS) with iterative preference learning.</p>
<p>These methods significantly enhance the reasoning capabilities of LLMs; however, they seldom consider interacting with the external environment. To address this limitation, Trivedi et al. (2023) integrate CoT with knowledge retrieval, interleaving reasoning with searching to acquire additional external knowledge for knowledge-intensive question answering. Yao et al. (2023b) propose a ReAct paradigm combining reasoning and acting for solving language reasoning and decision-making tasks. It creates and adjusts high-level plans for acting while also interacting with the external environments to incorporate additional information into reasoning. Later, Aksitov et al. (2023) develop a ReAct-style LLM agent to reason and act</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Nova Pipeline. The Pipeline includes initial seed idea generation, seed idea iteration, and idea completion. Upon receiving an input paper (<em>i.e.</em>, seed paper), the LLM is prompted to generate initial seed ideas by utilizing related papers (including recent publications) and scientific discovery methods. After that, the generated ideas are revised according to the new knowledge acquired according to iterative planning and search. Finally, each idea is expanded with more detailed methods.</p>
<p>Upon external knowledge, using self-critique for self-improvement. By integrating reasoning and acting, these methods achieve more dynamic and contextually aware problem-solving based on both internal knowledge and external knowledge.</p>
<p>The reasoning capabilities of LLMs can also be applied to planning, such as generating plausible goal-driven action plans that can be enacted in interactive, embodied environments (Huang et al., 2022). Additionally, plan-to-solve prompting (Wang et al., 2023b) can generate a plan that divides complex reasoning tasks into subtasks, enabling LLMs to execute each subtask according to the outlined plan.</p>
<p>Our work marks the inaugural integration of planning methodologies into the complex domain of research tasks.</p>
<h2>3 Nova Pipeline</h2>
<p>The pipeline for Nova is illustrated in Fig. 2. Our pipeline streamlines the research process through three stages: initial idea generation, iterative refinement, and detailed completion. It begins with an input paper, which the LLM uses to generate initial ideas by drawing on related literature and scientific discovery techniques. These ideas are then enhanced through iterative planning and search, incorporating new insights. The final step involves detailing the ideas. An example of the whole process is in Fig. 3.</p>
<h3>3.1 Initial Seed Idea Generation</h3>
<p>To produce high-quality ideas, we design a multi-source seed idea generation module that initiates with diverse and novel concepts. This module activates the LLM to generate ideas using related literature and scientific discovery techniques upon receiving an input paper. The prompt for initial idea generation is in Tab. 1 (details in Tab. 9) and an example of an initial seed idea is in Tab. 2.</p>
<p>To enrich the knowledge base with the most current insights, we utilize the input paper's references and have designed a knowledge tracking module. This module addresses the shortcomings of previous approaches by monitoring the latest publications. We pinpoint influential recent papers based on user engagement metrics such as likes, comments, and reposts across social media, forums, and GitHub. Furthermore, we harness LLMs to distill summaries of prevailing research trends from these papers, extracting valuable knowledge to enrich our target innovation efforts.</p>
<p>To further increase the diversity of the generated ideas, we employ 10 fundamental scientific discovery methods to guide LLMs in generating innovative ideas from an input paper and its as-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example of Planning-Driven Iterative Seed Idea Generation Process. This example highlights the planning-driven iterative seed idea iteration process. Starting from an initial concept, a detailed plan is formulated to guide the search for relevant literature and acquire up-to-date knowledge.</p>
<p>sociated literature. Drawing on Kuhn’s paradigm (Kuhn, 1997) of scientific discovery, these methods help identify new research problems, such as analyzing anomalies in existing approaches. An example theory of scientific discovery can be found in Tab. 3 and all theories (Appendix Tab. 20 and 21.)</p>
<p>To mimic human intuition, we tap into the LLMs' internal knowledge to craft initial seed ideas. This involves prompting the LLM to assess the shortcomings of the input paper and related works, thereby sparking the creation of fresh ideas.</p>
<p>To prevent hallucination and improve the logicality of generated initial seed ideas, we also utilize self-correction mechanics: self-check (Miao et al., 2023), self-critique (Gou et al., 2024), and reflection (Shinn et al., 2023). These methods partly guarantee that the generated seed ideas are logical and reasonable. In the end, we generate 15 seed ideas for each input paper.</p>
<h3>3.2 Iterative Planning and Search for Seed Idea Improvement</h3>
<p>Once an initial seed idea pool is generated, we start to iteratively planning and search new knowledge according to the see idea and generate new idea using the acquired new knowledge.</p>
<h4>3.2.1 Planning and Search</h4>
<p>In planning and search step, we guide the LLM to identify key fields for comprehensive and novel knowledge acquisition to enhance further research and idea generation based on the given ideas.</p>
<p>This approach, demonstrated through an in-context learning example, leverages the LLM's internal knowledge to determine useful knowledge for new ideas, surpassing traditional entity or keyword-based retrieval methods.</p>
<p><strong>New Seed Idea Generation.</strong> Once new knowledge is acquired, the new seed idea is generated based on the retrieved papers, the initial seed idea, and the given input paper. For each idea, our models generate 10 new seed ideas and then use self-reflection to cut the number down to 3.</p>
<p>In each iteration, the old seed ideas are replaced with the newly generated seed ideas. This allows our agent to dive deeper, largely expanding the scope of search. Therefore, in each iteration, we generate 3 times more seed ideas.</p>
<h3>3.3 Output Idea Generation</h3>
<p>After finishing <em>T</em> step iteration, we have a final seed idea pool. We then expand the seed idea into the</p>
<h1>Prompt</h1>
<p>Role: You are an expert researcher in AI. Your goal is to propose some innovative and valuable research ideas based on the target paper.
Skill: Generate subsequent exploration ideas according to the following steps:
Understanding of the target paper and related papers is essential:</p>
<ul>
<li>The target paper is the primary research study you aim to enhance or build upon through future research, serving as the central source and focus for identifying and developing the specific research idea.</li>
<li>
<p>The referenced papers are studies that the target paper has cited, indicating their direct relevance and connection to the primary research topic you are focusing on, and providing additional context and ideas that are essential for understanding and expanding upon the target paper.
Step 1: Combine target paper and referenced paper to answer the following information:</p>
</li>
<li>
<p>What are the tasks, methods, and main innovations of the current paper?</p>
</li>
<li>What are the weaknesses and limitations of the current paper?</li>
</ul>
<p>Step 2: Propose some valuable and new research ideas.
Output Format: {qa_info_with_idea_json_format}
Requirements: ...
Input: ...
Output: ...
Table 1: Prompt for initial seed idea generation.</p>
<p>Thinking: The target paper's reliance on existing literature may limit the generation of truly novel ideas. By incorporating real-time data sources, such as ongoing research developments or recent publications, the ResearchAgent can generate more innovative and timely research ideas.
Idea: Incorporate multi-modal data sources, including experimental data, patents, and industry reports, into the ResearchAgent's knowledge base to generate more comprehensive and interdisciplinary research ideas.</p>
<p>Keywords: multi-modal data sources, experimental data, interdisciplinary research ideas</p>
<p>Table 2: Seed Idea Example.
initial proposal and final proposal as in (Si et al., 2024). Specifically, given an input paper and its corresponding seed idea, we ask LLM to decompose the idea into several sub-modules and utilize LLMs to design these sub-modules separately in a more detailed way(details in Tab. 32 in Appendix). An example of an initial proposal and final proposal are in Tab. 4 and Tab. 5, separately.</p>
<h2>4 Experiment</h2>
<p>To validate our proposed iterative planning framework, we perform comprehensive comparisons with state-of-the-art research idea generation methods and conduct an ablation study.</p>
<h3>4.1 Experimental Setup</h3>
<p>Data. Our dataset is constructed by collecting highquality papers from top conferences. The initial
corpus comprised 7,805 papers from CVPR 2024, ACL 2024, and ICLR 2024. The keywords related to "LLM" are used to filter the initial corpus down to about 2,000 papers. The minimum citation number is used to further cut down the number to 153, with citation thresholds set at 20 for ICLR 2024 and 10 for CVPR 2024 and ACL 2024. We add additional 17 papers from Hugging Face Daily Papers according to their user ratings. At the end, the dataset consists of 170 papers, each of which is used to generate 100 ideas for subsequent evaluation.
Baseline. To compare our proposed approach with the state-of-the-art approaches, we choose three leading approaches as the baselines, including AIResearcher (Si et al., 2024), AI-Scientist (Lu et al., 2024) and Research-Agent (Baek et al., 2024). For AI-researcher and AI-Scientist, we run the original</p>
<table>
<thead>
<tr>
<th>Define New Scientific Problems</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Theoretical Basis: Kuhn’s paradigm theory, Laudan’s problem-solving model, Nichols’s problem-generation theory.</td>
<td></td>
</tr>
<tr>
<td>Method: Identify anomalies in existing theories; explore theoretical boundaries and scope of application; integrate interdisciplinary knowledge and discover new problems; re-examine neglected historical problems.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: An example theory of scientific discovery</p>
<p>Problem: State the problem statement, which should be closely related to the idea description and something that large language models cannot solve well yet.</p>
<p>Existing Methods: Mention some existing benchmarks and baseline methods if there are any.</p>
<p>Motivation: Explain the inspiration of the proposed method and why it would work well.</p>
<p>Proposed Method: Propose your new method and describe it in detail. The proposed method should be maximally different from all existing work and baselines, and be more advanced and effective than the baselines. You should be as creative as possible in proposing new methods; we love unhinged ideas that sound crazy. This should be the most detailed section of the proposal.</p>
<p>Experiment Plan: Specify the experiment steps, baselines, and evaluation metrics.</p>
<p>Table 4: Initial Proposal Template (follow [Si et al. (2024)]).</p>
<p>code on our seed papers. For Research-Agent, we also build a knowledge graph based on the description of the paper.</p>
<p>Automatic Evaluation. In our automatic evaluation, we mainly focus on overall quality evaluation and also concern with the novelty and diversity.</p>
<ol>
<li>Quality. Following [Si et al. (2024)], we employ the Swiss System Tournament with Claude-3.5-Sonnet zero-shot ranker to evaluate the quality of ideas. The ranker makes pairwise comparisons to determine which idea is better. For each idea, there are 5 rounds of comparison, each winning comparison gets 1 score. Such a quality evaluation method has been shown to be better than direct comparison <em>Lu et al. (2024)</em>.</li>
<li>Novelty. Following [Baek et al. (2024)], we use LLMs to judge whether a generated idea is novel by checking the top 10 most relevant papers and if no paper is identified as containing a similar idea, it is considered novel. We use embedding using the all-MiniLM-L6-v2 model and if the cosine similarity threshold is larger than 0.3, we say that they are similar <em>Si et al. (2024)</em>.</li>
<li>Diversity. Similar to [Si et al. (2024)], we use the proportion of unique ideas to measure generation diversity. To be specific, we use the same similarity measurement as in the novelty measurement and the duplication threshold is set to be 0.8.</li>
</ol>
<p>In the automatic evaluation, we utilize Nova alongside three baseline methods to generate 400 ideas from a given input paper. Each method generates 100 ideas separately. The iteration step for</p>
<p>1https://en.wikipedia.org/wiki/Swiss-system_tournament
2
We use the same paper retriever as in the idea generation phase, with data sourced from arXiv. The time range spans from January 1, 2022, to August 2024, and the categories include literature related to AI, NLP, and CV.
3https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</p>
<table>
<thead>
<tr>
<th>Title: A concise statement of the main research question to be used as the paper title.</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Problem Statement: Clearly define the problem your research intends to address. Explain clearly</td>
<td></td>
</tr>
<tr>
<td>why this problem is interesting and important.</td>
<td></td>
</tr>
<tr>
<td>Motivation: Explain why existing methods are not good enough to solve the problem, and explain</td>
<td></td>
</tr>
<tr>
<td>the inspiration behind the new proposed method. You should also motivate why the proposed</td>
<td></td>
</tr>
<tr>
<td>method would work better than existing baselines on the problem.</td>
<td></td>
</tr>
<tr>
<td>Proposed Method: Explain how the proposed method works, describe all the essential steps.</td>
<td></td>
</tr>
<tr>
<td>Step-by-Step Experiment Plan: Break down every single step of the experiments, make sure</td>
<td></td>
</tr>
<tr>
<td>every step is executable. Cover all essential details such as the datasets, models, and metrics to be</td>
<td></td>
</tr>
<tr>
<td>used. If the project involves prompting, give some example prompts for each step.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: Final Proposal Template (follow Si et al. (2024)).</p>
<p>Nova is set to 3, and the initial number of seed ideas is 15. After 3 iterations, we get 405 initial seed ideas, we then cluster the ideas using k-means clustering into 100 clusters and use the cluster center to generate the 100 final ideas. The implementation details for the baseline methods remain consistent with those in the original work.</p>
<p>Human Evaluation. To validate the effectiveness of our automatic evaluation, we have an additional human evaluation. Our goal is to assess how well our automatic evaluation aligns with human expert evaluations. We recruit a panel of 10 experts, all holding a PhD degree or professorship in natural language processing, machine learning, or computer vision, doing research in LLMs-related fields. These experts evaluated ideas based on novelty and overall quality (including feasibility and effectiveness).</p>
<p>We select five ideas generated by each agent based on the same input paper. These ideas correspond to the $1^{\text {st }}, 25^{\text {th }}, 50^{\text {th }}, 75^{\text {th }}$, and $100^{\text {th }}$ percentiles of the automatic evaluation, resulting in a total of 20 ideas per topic. This process is repeated for 20 times. Each expert reviews four groups, ensuring that at least two independent experts evaluate each idea. The final score for each idea is averaged across all ratings from different experts.</p>
<p>We compare the distributions of expert evaluations against those in automatic evaluation. Specifically, we track which methods produce the top 20 percent of ideas, as ranked by the experts. This helps determine which methods outperform the others. Moreover, this approach reveals whether the model evaluation aligns with human evaluation.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Non-Duplicate Percentage Comparison.</p>
<h3>4.2 Experimental Results</h3>
<h3>4.2.1 Automatic Evaluation Results</h3>
<p>The Swiss Tournament score comparison are shown in Fig. 4. The novelty and diversity comparison are shown in Fig. 5.</p>
<p>Clearly, Nova achieves a significantly higher Swiss score. 619 and 2521 of the ideas generated by Nova are scored at 4 and 5, significantly surpassing the performance of other agents. By incorporating iterative planning and search for external knowledge retrieval, Nova engages in more effective exploration for innovation. This may significantly enhance the novelty of the generated ideas. Since novelty is often the most important factor in evaluating idea quality, Nova consistently better than other state-of-the-art methods.</p>
<p>Fig. 5 shows that Nova generates significantly more diverse ideas. As the number of generated ideas increases, Nova can continuously generate new ideas through iterative planning and search. In Non-Duplicate Percentage, Nova significantly</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Human Evaluation for Overall Quality.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Human Evaluation for Novelty.</p>
<p>outperforms others, with over 80% of the ideas being unique.</p>
<h3>4.2.2 Human Evaluation Results</h3>
<p>In our human evaluation, Nova achieves the highest scores for both overall quality and novelty. As shown in Fig. 6, Nova contributes 37.5% of the top 4 ideas, the highest among the four methods. Additionally, Nova has a notably low percentage of the worst 4 ideas, accounting for only 17.53% in terms of overall quality. In Fig. 7, a similar pattern is observed in novelty evaluation.</p>
<p>Our human and automated evaluations show strong consistency in distinguishing between the top-rated and worst-rated ideas. By comparing the distribution of top-rated ideas in both human and automated evaluations (Fig. 4 and 6), it is evident that human reviewers and the LLM evaluate the performance of the four methods in a similar pattern. In both human and automatic evaluations, our method generates the highest proportion of top-rated ideas, followed by AI-Scientist, ResearchAgent, and finally AI-Researcher. This indicates that our automatic review mechanism effectively captures human reviewers' true preferences.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Ablation studies for Nova. We can find that both retrieval and planning significantly enhance the generation of unique novel ideas.</p>
<h3>4.3 Ablation Study</h3>
<p>To assess the effectiveness of planning and search in Nova, we conduct comparisons by gradually removing planning and retrieval components. All methods retrieve the same number of papers, specifically K = 5. Both retrieval and planning are found to significantly enhance the generation of unique and novel ideas. When planning is excluded, the number of unique ideas at step 3 (44.1) no longer increases compared to step 2 (42.4). This suggests that without planning, relying solely on retrieval based on seed ideas limits access to valuable external knowledge for innovation. This limitation may arise from the restricted scope of search when planning is absent. Obviously, when planning and retrieval are both removed, the number of unique novel ideas increases slightly at step 2 (from 25.3 to 30.6) and stagnates at step 3 (from 30.6 to 31.35), due to no external knowledge being introduced.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we propose an LLM-based scientific innovation method, Nova, which introduces iterative planning and search to retrieve external knowledge for innovation. Nova leverages the internal knowledge of LLMs to generate search plans for external knowledge retrieval, significantly enhancing the effectiveness of the retrieval process. The ablation study demonstrates the effect of the iterative planning and search framework on promoting the novelty of generating ideas. The automatic and human evaluations show that Nova significantly and consistently outperforms state-of-the-art scientific innovation methods. In the future, we will explore incorporating a reward function into our iterative planning framework to further enhance external knowledge retrieval.</p>
<h2>6 Limitations</h2>
<p>In this work, we investigate using iterative planning and search, mimicking the manner of our human beings, to enhance the innovation capability of existing LLM-based methods. Despite promising findings, some limitations remain in this work, which we discuss below:
Limited Iterations Steps. Although our approach can significantly enhance the novelty and diversity of generated ideas through iteration, we do not see a continuous increment in generating new ideas after 3 rounds of iteration.
Planning without Rewards. In our planning and search framework, we do not introduce reward functions but only use the internal knowledge of LLMs to generate search plans. This may limit the effectiveness of planning.</p>
<p>We hope these findings inspire future investigations into using LLM to comprehensively integrate both internal and external knowledge for LLMbased scientific innovation. We believe addressing each of these shortcomings will lead to exciting future directions.</p>
<h2>7 Ethics Statement</h2>
<p>Publication Policy. The increasing use of AI to generate research ideas poses significant challenges to academic integrity. The growing accessibility of LLMs and the rising usefulness of LLMs in research may lead to deterioration in the overall quality of scholarly content, as individuals may rely on AI for both creativity and submission reviews. Therefore, there is a legitimate concern that students or researchers would exploit these technologies and present low-quality research proposals. To mitigate these risks, it is crucial to hold accountability for outputs generated through AI tools in scientific submissions.
Intellectual Credit. Generative AI in the research cycle poses great concerns about intellectual credit on the submitted works. While traditional frameworks were more like a tool for human researchers, LLMs are more potent in a way that plays a more significant role in the scientific research process if used. It is still unclear how intellectual credit should be distributed in the case of AI-driven research. To better attribute credit to AI-supported research, researchers should adopt transparent documentation about their research process, including the extent of AI involvement in generating ideas and developing experiments.</p>
<p>Potential for Misuse. AI-generated research ideas, particularly those introducing novel concepts, possess the potential for misuse. This could lead to harmful outcomes. Ideation agents may be exploited to develop adversarial attack strategies or other unethical applications. Therefore, it is important to develop anti-jailbreak mechanisms or safety checks on AI-generated content and the use of generative AI in research.
Idea Homogenization. If AI was widely used in scientific research, this would raise concerns about the potential idea of homogenization. The wide adoption of LLMs in research could reflect a narrower set of perspectives or systematic biases compared to human researchers not using AI assistance. Therefore, it is important to recognize the limitations of current LLM-generated ideas, and future work should focus more on enhancing the generation diversity either by improving the models themselves or by refining the ideation process.
Impact on Human Researchers. The challenge posed by AI's integration into research should be well recognized because research is fundamentally and historically a community-driven and collaborative effort. It is still unclear on the negative consequences of the introduction of AI in the research process. People should be cautious and aware of the potential decline in human thought and a reduction in opportunities for human collaboration after the introduction of AI in research. Future works should explore other methods of human-AI collaboration. Understanding how LLM should be integrated into the research process will be an ongoing problem.</p>
<h2>References</h2>
<p>Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar. 2023. REST MEETS REACT: SELFIMPROVEMENT FOR MULTI-STEP REASONING LLM AGENT. Preprint, arXiv:2312.10003.</p>
<p>Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. Preprint, arXiv:2404.07738.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: Large Language Models Can SelfCorrect with Tool-Interactive Critiquing. Preprint, arXiv:2305.11738.</p>
<p>Xuemei Gu and Mario Krenn. 2024. Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models. Preprint, arXiv:2405.17044.</p>
<p>Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2024. MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. In ICML 2024.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language Models as ZeroShot Planners: Extracting Actionable Knowledge for Embodied Agents. In ICML 2022, volume 162 of Proceedings of Machine Learning Research, pages 9118-9147. PMLR.</p>
<p>Thomas S Kuhn. 1997. The structure of scientific revolutions, volume 962. University of Chicago press Chicago.</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. 2024. Mlr-copilot: Autonomous machine learning research based on large language models agents. Preprint, arXiv:2408.14033.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. Preprint, arXiv:2408.06292.</p>
<p>Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. Preprint, arXiv:2308.00436.</p>
<p>Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. 2024. Mathematical discoveries from program search with large language models. Nature, 625(7995):468-475.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In NeurIPS 2023.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can LLMs Generate Novel Research Ideas? A LargeScale Human Study with 100+ NLP Researchers. Preprint, arXiv:2409.04109.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for KnowledgeIntensive Multi-Step Questions. In ACL 2023, pages 10014-10037, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Haiming Wang, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. 2024a. Lego-prover: Neural theorem proving with growing libraries.</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. 2023a. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023b. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. In ACL 2023, pages 2609-2634, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2024b. Scimon: Scientific inspiration machines optimized for novelty. Preprint, arXiv:2305.14259.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In ICLR 2023. OpenReview.net.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In NeurIPS 2022.</p>
<p>Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning. Preprint, arXiv:2405.00451.</p>
<p>Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. 2024. Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. Preprint, arXiv:2309.02726.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In NeurIPS 2023.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing Reasoning and Acting in Language Models. In ICLR, 2023.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In ICLR 2023. OpenReview.net.</p>
<h2>A Human Annotation</h2>
<p>This section provides details about human annotation in our human evaluation experiment.</p>
<h2>A. 1 Annotation Instructions</h2>
<p>The complete annotation instruction for the idea reviewer is given below:</p>
<p>Please evaluate the given twenty ideas based on four criteria (Novelty, Feasibility, Effectiveness, and Overall), identify the best four and the worst four ideas, and rank them accordingly. The principles include:
Annotation Dimensions: See Table 6 for detailed dimension instructions.
Annotation Method: Annotate the best 4 and the worst 4 ideas in each of the 4 dimensions. For the best ideas, mark them as $1,2,3$, and 4 respectively, while 1 refers to the best idea. For the worst ideas, mark them as $17,18,19$, and 20 , while 20 refers to the worst idea. No need to annotate ideas other than the best 4 and the worst 4 . For an example, see Table 7.</p>
<h2>A. 2 Data Description</h2>
<p>We manually selected 20 different papers from ACL2024, CVPR2024, and ICLR2024. Each paper is carefully selected, and the 20 papers are from various research fields, including Natural Language Processing, Computer Vision, and Large Language Models in general, and have varied academic significance measured by citations to represent a broad scope of research papers. The online pilot study gives the human evaluators a form of twenty rows and five columns, along with a hyperlink to the original paper the ideas are generated. Each row is of an idea generated by one of the four different methods, Nova, AI-Scientist, AI-Researcher, or ResearchAgent. Still, human experts have no information on which method to generate that particular idea. The four columns are summary, novelty, feasibility, effectiveness, and overall. The summary is the research plan generated from one of the four methods, and the remaining four columns are entries for human experts to input their rankings. The four best ideas are labeled as $1,2,3$, or 4 , and the four worst ideas are labeled as $17,18,19$, or 20 . The rest of the entries are left blank.</p>
<h2>A. 3 Risk Statement</h2>
<p>Physical Risk. This study does not involve any activities that may cause physical harm or discomfort.
Psychological Risk. This study does not involve sensitive topics or psychological experiments.
Social Risk. This study does not involve activities
that could affect participants' social relationships or reputations. No personal information will be disclosed.
Economic Risk. This study will not result in any economic loss for the participants.
Privacy and Data Security Risk. All annotation data will be randomly assigned to anonymous experts. No personally sensitive information will be collected.</p>
<h2>B Prompts and Examples</h2>
<p>This section provides a comprehensive overview of various prompts and examples Nova uses in idea generation and research proposal creation. The tables are organized in Tab. 8 to guide the reader through different methods and stages of idea development, research trend exploration, and proposal drafting, with each table focusing on a distinct aspect of the research process.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Criteria</th>
<th style="text-align: center;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Novelty</td>
<td style="text-align: center;">Novelty refers to the originality and innovativeness of the idea. It assesses how new and unique the idea is compared to existing work in the field.</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Overall evaluates the general quality and potential of the idea, taking into account all other criteria (Novelty, Feasibility, and Effectiveness). It provides a holistic assessment of the idea's value.</td>
</tr>
<tr>
<td style="text-align: center;">Feasibility</td>
<td style="text-align: center;">Feasibility assesses the practicality and implementability of the idea. It considers whether the idea can be realistically executed with available resources and within a reasonable timeframe.</td>
</tr>
<tr>
<td style="text-align: center;">Effectiveness</td>
<td style="text-align: center;">Effectiveness evaluates the expected impact and success of the idea in achieving its intended goals. It considers how well the idea is likely to perform in practice.</td>
</tr>
</tbody>
</table>
<p>Table 6: Evaluation Criteria and Definitions for Online Idea Assessment Based on Novelty, Feasibility, Effectiveness, and Overall Quality</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Rank</th>
<th style="text-align: right;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Best</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: right;">Second Best</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: right;">Third Best</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: right;">Fourth Best</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: right;">$\ldots$</td>
<td style="text-align: right;">$\ldots$</td>
</tr>
<tr>
<td style="text-align: right;">$\ldots$</td>
<td style="text-align: right;">$\ldots$</td>
</tr>
<tr>
<td style="text-align: right;">Fourth Worst</td>
<td style="text-align: right;">17</td>
</tr>
<tr>
<td style="text-align: right;">Third Worst</td>
<td style="text-align: right;">18</td>
</tr>
<tr>
<td style="text-align: right;">Second Worst</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: right;">Worst</td>
<td style="text-align: right;">20</td>
</tr>
</tbody>
</table>
<p>Table 7: An example of Ranking Labels for Annotating the Best and Worst Ideas Across Four Evaluation Dimensions</p>
<p>Table 8: Tables in the Appendix</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table Number</th>
<th style="text-align: center;">Table Title</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Table 6</td>
<td style="text-align: center;">Prompt for initial seed idea generation using inner knowledge from LLM</td>
</tr>
<tr>
<td style="text-align: center;">Table 7</td>
<td style="text-align: center;">An example of the initial seed idea generation using inner knowledge from LLM (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 8</td>
<td style="text-align: center;">An example of the initial seed idea generation using inner knowledge from LLM (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 9</td>
<td style="text-align: center;">Prompt for generating research trends</td>
</tr>
<tr>
<td style="text-align: center;">Table 10</td>
<td style="text-align: center;">Current hot research trends in AI (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 11</td>
<td style="text-align: center;">Current hot research trends in AI (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 14</td>
<td style="text-align: center;">Prompt for generating research ideas based on popular research trends</td>
</tr>
<tr>
<td style="text-align: center;">Table 12</td>
<td style="text-align: center;">An example of generating research ideas based on popular research trends (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 13</td>
<td style="text-align: center;">An example of generating research ideas based on popular research trends (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 17</td>
<td style="text-align: center;">General theory of scientific discovery (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 18</td>
<td style="text-align: center;">General theory of scientific discovery (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 19</td>
<td style="text-align: center;">Prompt for idea generation based on the general theory of scientific discovery (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 20</td>
<td style="text-align: center;">Prompt for idea generation based on the general theory of scientific discovery (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 21</td>
<td style="text-align: center;">An example of idea generation based on the general theory of scientific discovery (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 22</td>
<td style="text-align: center;">An example of idea generation based on the general theory of scientific discovery (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 23</td>
<td style="text-align: center;">Prompt for expanding idea generation using retrieved knowledge and reflection iteration</td>
</tr>
<tr>
<td style="text-align: center;">Table 24</td>
<td style="text-align: center;">An example of original idea</td>
</tr>
<tr>
<td style="text-align: center;">Table 25</td>
<td style="text-align: center;">An example of iterative research ideas</td>
</tr>
<tr>
<td style="text-align: center;">Table 26</td>
<td style="text-align: center;">Prompt for initial proposal generation</td>
</tr>
<tr>
<td style="text-align: center;">Table 27</td>
<td style="text-align: center;">An example of initial proposal</td>
</tr>
<tr>
<td style="text-align: center;">Table 28</td>
<td style="text-align: center;">Prompt for method decomposition</td>
</tr>
<tr>
<td style="text-align: center;">Table 29</td>
<td style="text-align: center;">An example of method decomposition (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 30</td>
<td style="text-align: center;">An example of method decomposition (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 31</td>
<td style="text-align: center;">Prompt for final proposal generation (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 32</td>
<td style="text-align: center;">Prompt for final proposal generation (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 33</td>
<td style="text-align: center;">An example of final proposal (Part 1)</td>
</tr>
<tr>
<td style="text-align: center;">Table 34</td>
<td style="text-align: center;">An example of final proposal (Part 2)</td>
</tr>
<tr>
<td style="text-align: center;">Table 35</td>
<td style="text-align: center;">Prompt for search plan generation</td>
</tr>
<tr>
<td style="text-align: center;">Table 36</td>
<td style="text-align: center;">An example of search plan</td>
</tr>
<tr>
<td style="text-align: center;">Table 37</td>
<td style="text-align: center;">An example of search result</td>
</tr>
</tbody>
</table>
<h1>Prompt</h1>
<p>Role: You are an expert researcher in AI. Your goal is to propose some innovative and valuable research ideas based on the target paper.
Skill: Generate subsequent exploration ideas according to the following steps: Understanding of the target paper and related papers is essential:</p>
<ul>
<li>The target paper is the primary research study you aim to enhance or build upon through future research, serving as the central source and focus for identifying and developing the specific research idea.</li>
<li>
<p>The referenced papers are studies that the target paper has cited, indicating their direct relevance and connection to the primary research topic you are focusing on, and providing additional context and ideas that are essential for understanding and expanding upon the target paper.
Step 1: Combine target paper and referenced paper to answer the following information:</p>
</li>
<li>
<p>What are the tasks, methods, and main innovations of the current paper?</p>
</li>
<li>What are the weaknesses and limitations of the current paper?</li>
</ul>
<p>Step 2: Propose some valuable and new research ideas.
Output Format: {qa_info_with_idea_json_format}</p>
<h2>Requirements:</h2>
<ol>
<li>Output about 5 new ideas worth exploring.</li>
<li>Aim for new research ideas that can potentially win best paper awards at top conferences like ACL, NeurIPS, ICLR, and CVPR.</li>
<li>Please output your thought process.</li>
<li>Please think step by step.</li>
</ol>
<p>Input:
target_title: {paper.title}
target_abstract: {paper.abstract}
referenced_paper_title: {paper.related_paper_titles}
referenced_paper_abstract: {paper.related_paper_abstract}</p>
<h2>Output:</h2>
<p>Thinking:</p>
<output your thinking process here, explain why it should have changed to win the best paper awards at top conferences $>$
New research idea that improves target_paper:
$<$ JSON $>$

Table 9: Prompt for initial seed idea generation using inner knowledge from LLM.

# ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models 

## Generated Research Ideas (Part 1)

## 1. Integrate Real-Time Data Feeds

Thinking: The target paper's reliance on existing literature may limit the generation of truly novel ideas. By incorporating real-time data sources, such as ongoing research developments or recent publications, the ResearchAgent can generate more innovative and timely research ideas.
Idea: Integrate real-time data feeds and recent publications into the ResearchAgent's knowledge base to ensure that the generated ideas are aligned with the latest scientific advancements.
Keywords: real-time data integration, recent publications tracking, innovative research ideas

## 2. Develop a Multi-Faceted Evaluation Framework

Thinking: The evaluation metrics used in the target paper may not fully capture the practical applicability and impact of the generated ideas. By incorporating domain-specific expert reviews and practical implementation feedback, the evaluation process can be more comprehensive.
Idea: Develop a multi-faceted evaluation framework that includes domainspecific expert reviews and practical implementation feedback to assess the applicability and impact of the generated research ideas.
Keywords: multi-faceted evaluation, expert reviews, practical implementation feedback

## 3. Optimize Computational Efficiency

Thinking: The iterative refinement process in the target paper might be computationally expensive and time-consuming. By optimizing the computational efficiency of the ReviewingAgents and leveraging parallel processing techniques, the scalability of the ResearchAgent can be improved.
Idea: Optimize the computational efficiency of the ReviewingAgents and leverage parallel processing techniques to enhance the scalability of the ResearchAgent.
Keywords: computational efficiency, parallel processing, scalability

Table 10: An example of seed idea generation using inner knowledge from LLM (Part 1).

# ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models 

## Generated Research Ideas (Part 2)

## 4. Incorporate Multi-Modal Data Sources

Thinking: The target paper focuses on generating research ideas based on scientific literature. By incorporating multi-modal data sources, such as experimental data, patents, and industry reports, the ResearchAgent can generate more comprehensive and interdisciplinary research ideas.
Idea: Incorporate multi-modal data sources, including experimental data, patents, and industry reports, into the ResearchAgent's knowledge base to generate more comprehensive and interdisciplinary research ideas.
Keywords: multi-modal data sources, experimental data, interdisciplinary research ideas

## 5. Incorporate Diverse ReviewingAgents

Thinking: The target paper uses ReviewingAgents instantiated with human preference-aligned LLMs for iterative feedback. By incorporating a diverse set of ReviewingAgents with different expertise and perspectives, the feedback process can be more robust and holistic.
Idea: Incorporate a diverse set of ReviewingAgents with different expertise and perspectives to provide more robust and holistic feedback during the iterative refinement process.
Keywords: diverse ReviewingAgents, expertise diversity, holistic feedback

Table 11: An example of seed idea generation using inner knowledge from LLM (Part 2).

## Prompt

Role: You are an AI expert researcher. You can summarise the current hot research trends from the list of recent AI papers.
Skill: You will analyze the research trending based on the recent popular paper, provide us with the research trending report.

## Requirements:

1. Provide a comprehensive analysis, including the hot research directions, the highlights of the technologies and methods, and discuss whether these technologies can be used in other fields.

I will provide a list of recent popular paper list here: \{popular_paper_list\} Then, Please output the current research trending report here:

Table 12: Prompt for generating popular research trends.

# Hot Research Directions 

1. Long-Context Language Models (LLMs) and Retrieval-Augmented Generation (RAG)

- Key Papers: "RAG in the Era of Long-Context LLMs", "LongCite", "MemLong", "Improved RAG with Self-Reasoning", "LongWriter", "EfficientRAG", "Enhanced RAG with Long-Context LLMs", "GraphReader"
- Highlights: Addressing the challenge of maintaining focus and relevance in long-context LLMs. Combining RAG mechanisms with long-context capabilities to improve performance in tasks like question answering and citation generation. Innovations such as order-preserving RAG, external retrievers, and graph-based systems to enhance context handling.
- Cross-Field Applications: These advancements can be applied in fields requiring extensive document analysis, such as legal research, academic literature review, and medical records analysis.

2. Strategic Chain-of-Thought (CoT) and Self-Improvement Techniques

- Key Papers: "Strategic Chain-of-Thought", "Teaching LLM Agents to SelfImprove", "Self-Taught Evaluators", "Meta-Rewarding LLMs", "SelfGoal"
- Highlights: Incorporating strategic knowledge to guide intermediate reasoning steps. Iterative self-improvement and self-evaluation to enhance model performance over multiple turns. Use of self-generated training data to refine judgment and reasoning capabilities.
- Cross-Field Applications: These methods can be beneficial in educational technologies, autonomous decision-making systems, and any domain requiring iterative problem-solving and learning.

3. Mixture-of-Experts (MoE) and Multi-Agent Systems

- Key Papers: "OLMoE", "Agentic RAG for Time Series Analysis", "Mixture-of-Agents", "MindSearch"
- Highlights: Leveraging sparse Mixture-of-Experts to optimize model performance and efficiency. Multi-agent architectures for specialized task handling, such as time series analysis and complex web-information seeking.
- Cross-Field Applications: These approaches can be utilized in financial forecasting, climate modeling, and complex system simulations where specialized expertise is crucial.

4. Synthetic Data Generation and Utilization

- Key Papers: "Smaller, Weaker, Yet Better", "Scaling Synthetic Data Creation", "Improving Retrieval in LLMs through Synthetic Data", "Model Collapse on Synthetic Data"
- Highlights: Using weaker models to generate high-quality synthetic data for fine-tuning stronger models. Addressing the challenges of model collapse due to recursive training on synthetic data.
- Cross-Field Applications: Synthetic data can be used in privacy-preserving data analysis, training AI models in healthcare, and augmenting datasets in low-resource languages.

# Hot Research Directions 

5. Controllable and Robust Text Generation

- Key Papers: "Controllable Text Generation for LLMs", "Enhancing Robustness in LLMs", "Improving Legibility of LLM Outputs"
- Highlights: Techniques for controlling the style, safety, and consistency of generated text. Methods to enhance robustness by filtering out irrelevant information and improving the clarity of outputs.
- Cross-Field Applications: These advancements are crucial for developing reliable AI assistants, automated content generation, and ensuring the safety of AI-generated outputs in sensitive applications.

6. AI in Scientific Discovery and Evaluation

- Key Papers: "The AI Scientist", "Automate Design of Agentic Systems", "Self-Taught Evaluators"
- Highlights: AI agents capable of conducting independent research and writing scientific papers. Meta-agent frameworks for designing and evaluating agentic systems.
- Cross-Field Applications: These innovations can revolutionize scientific research, enabling faster discovery and validation of new theories across various scientific disciplines.

7. Advanced Prompt Engineering and Personalization

- Key Papers: "Conversational Prompt Engineering", "A Survey of Prompt Engineering Methods in LLMs"
- Highlights: Techniques for creating personalized prompts through iterative user interaction. Comprehensive surveys on prompt engineering methods for various NLP tasks.
- Cross-Field Applications: Personalized prompt engineering can enhance user experience in customer service bots, personalized education platforms, and adaptive learning systems.

8. AI in Code and Software Engineering

- Key Papers: "LLM Compiler", "From LLMs to LLM-based Agents for Software Engineering", "DeepSeek-Coder-V2"
- Highlights: Models designed for code optimization and generation. Surveys on the application of LLMs in software engineering tasks like requirement engineering and test generation.
- Cross-Field Applications: These advancements can improve software development workflows, automate code reviews, and enhance the capabilities of integrated development environments (IDEs).

Table 14: Current hot research trends in AI (Part 2)

# ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models 

## Generated Research Ideas from Popular Knowledge (Part 1)

## 1. Integrate Synthetic Data Generation Techniques

Thinking: The target paper relies heavily on existing literature, which may limit the generation of truly novel ideas. By integrating synthetic data generation techniques, we can create high-quality synthetic research papers and datasets to augment the existing literature. This approach can help overcome the dependency on existing literature and introduce more diversity in the generated ideas. Idea: Integrate synthetic data generation techniques to create high-quality synthetic research papers and datasets, augmenting the existing literature and introducing more diversity in the generated ideas.
Keywords: synthetic data generation, research idea generation, LLMs, data augmentation
Rationale: This idea addresses the limitation of dependency on existing literature by introducing synthetic data, which can diversify the input data and potentially lead to more novel research ideas. The use of synthetic data generation is innovative and has the potential to enhance the robustness and creativity of the ResearchAgent, making it a strong candidate for top conference recognition.

## 2. Incorporate Strategic Chain-of-Thought and Self-Improvement Techniques

Thinking: The evaluation metrics used in the target paper may not fully capture the practical applicability and impact of the generated ideas. By incorporating strategic chain-of-thought and self-improvement techniques, we can enhance the evaluation process by enabling the ReviewingAgents to iteratively refine their feedback and improve their judgment over multiple turns.
Idea: Incorporate strategic chain-of-thought and self-improvement techniques to enhance the evaluation process, enabling ReviewingAgents to iteratively refine their feedback and improve their judgment over multiple turns.
Keywords: strategic chain-of-thought, self-improvement, iterative feedback, evaluation metrics
Rationale: This idea improves the evaluation process by making it more dynamic and iterative, allowing ReviewingAgents to learn and refine their feedback over time. This approach can lead to more accurate and practical evaluations of the generated research ideas, addressing a key limitation of the target paper. The innovative use of self-improvement techniques makes this idea a strong contender for top conference recognition.

Table 15: An example of generating research ideas based on popular research trends (Part 1).

# ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models 

## Generated Research Ideas from Popular Knowledge (Part 2)

## 3. Leverage Mixture-of-Experts and Multi-Agent Systems

Thinking: The iterative refinement process in the target paper might be computationally expensive and time-consuming. By leveraging mixture-of-experts (MoE) and multi-agent systems, we can optimize the performance and efficiency of the ResearchAgent, making the iterative refinement process more scalable.
Idea: Leverage mixture-of-experts (MoE) and multi-agent systems to optimize the performance and efficiency of the ResearchAgent, making the iterative refinement process more scalable.
Keywords: mixture-of-experts, multi-agent systems, scalability, performance optimization
Rationale: This idea addresses the scalability limitation by optimizing the computational efficiency of the ResearchAgent using MoE and multi-agent systems. This approach can significantly reduce the time and resources required for the iterative refinement process, making the system more scalable and practical for large-scale applications. The innovative use of MoE and multiagent systems enhances the feasibility and impact of the ResearchAgent, making it a strong candidate for top conference recognition.

Table 16: An example of generating research ideas based on popular research trends (Part 2).

# Prompt 

Role: You are an expert researcher in AI. Your goal is to propose some innovative and valuable research ideas based on the target paper and some high-quality research trends.
Skills: Propose some innovative and valuable research ideas following these steps:

1. Understand the target paper and target_paper_base_info well.
2. Understand the research_trending_info and high_quality_paper_list well. Analyze the latest innovations, ideas, and methods they have used.
3. List some technologies used in research_trending_info and high_quality_paper_list, analyze the feasibility of combining these technologies with target_paper_base_info, and analyze the advantages and disadvantages of the combination.
4. Come up with some innovative and valuable research ideas.

Input Data Description: It is important to understand target_paper_base_info and research_trending_info:

- target_paper_base_info provides some basic information and limitation information of the target paper.
- research_trending_info and high_quality_paper_list: This data analyzes the current popular research trends and high-quality papers. You can get inspiration from them and draw on their latest methods, ideas, and innovations to come up with new ideas, but you must be different from them and make sure that the ideas you propose are reasonable.
- exist_idea is some ideas that have been proposed before; you need to propose some different ones.


## Requirements:

1. Output about 5 new ideas worth exploring.
2. You should aim for new research ideas that can potentially win best paper awards at top conferences like ACL, NeurIPS, ICLR, and CVPR.
3. The research trending may not well match the target paper; the ideas you make should make sense and be reasonable. Explain which trending or highquality paper your inspiration comes from. What is the original idea, what are the benefits of adopting this idea, and explain why it is feasible.
4. Please output your thought process.
5. Please think step by step.

## Input:

target paper title: \{paper.title\}
target paper abstract: \{paper.abstract\}
target_paper_base_info: \{target_paper_base_info\}
research_trending_info: \{research_trending_info\}
high_quality_paper_list: \{str(topk_high_quality_paper_list)\}
exist_idea: \{exist_idea\}
target_paper_base_info: \{target_paper_base_info\}

## Output:

Thinking:
<output your thinking process here, explaining what new knowledge you used for the new idea and why it makes sense and why it should have changed to win the best paper awards at top conferences>
New research idea that improves target_paper:
$<$ JSON $>$
Table 17: Prompt for generating research ideas Based on recent high-quality articles.


<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Equal contribution.
${ }^{1}$ Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
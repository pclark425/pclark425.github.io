<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-330 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-330</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-330</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-99bd07e888476904c6dd77ca154fd48629ac6dce</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/99bd07e888476904c6dd77ca154fd48629ac6dce" target="_blank">How well do Large Language Models perform in Arithmetic tasks?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatG PT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provides a detailed analysis of the ability of largelanguage models.</p>
                <p><strong>Paper Abstract:</strong> Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \url{https://github.com/GanjinZero/math401-llm}.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e330.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e330.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art instruction-tuned large language model evaluated on a 401-example arithmetic benchmark (MATH 401); shows the best arithmetic performance in the paper across most operators and difficult settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, trigonometric (sin, cos, tan), logarithm, decimals, negative numbers, irrational constants (e, pi), long multi-operator expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>small integers up to millions and beyond (groups include numbers up to 1e12), decimals, irrational constants, big-number multiplications (group 9), long bracketed expressions (group 14), exponentiation with integer and decimal exponents (groups 11-12)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot with targeted prompts (system prompt: "You are an accurate calculator, please calculate provided equation to four decimal places."), Compare prompts including 'Calculate:', LaTeX-style prompts; greedy/default decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~83.5% (Table 3: Acc 83.54, RE 0.07, NNR 0.00). Per-operation highlights (Table 1): addition ~99%, multiplication ~67%, division ~100%, exponentiation ~50%, trigonometry ~68%, logarithm ~76%, handling of irrationals 100%, long expressions accuracy ~96%; handles big numbers better than most but still fails on large-number multiplications (group 9) in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No direct mechanistic probes; paper reports behavioral observations: GPT-4 reliably performs step-by-step calculations (often claims to 'use a calculator' in generated message), correctly interprets irrational constants (e) and radians/degrees, and appears to produce systematic stepwise decomposition for long expressions. Pretraining/instruction tuning and prompt/system prompts affect behavior. No internal attention/head-level mechanism described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Not explicitly known for GPT-4; authors note GPT-4 is slower and likely larger than ChatGPT but no parameter count; generally outperforms smaller models — consistent with scaling trends but no internal scaling law established for GPT-4 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails on some big-number multiplications (close estimations with correct head/tail but wrong middle), limited accuracy on decimal-base exponentiation; otherwise strong. Rare non-number outputs when not given strict system prompt (not observed under tested prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ChatGPT (gpt-3.5-turbo-0301), earlier InstructGPT/davinci models, and multiple open-source LLMs; ablations across prompts and with/without system messages.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-4 attains the highest arithmetic accuracy in MATH 401, excels at division, trig, logarithms and long stepwise calculations, but still shows failure on big-number multiplications and decimal exponentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e330.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned conversational model (gpt-3.5-turbo-0301) evaluated on MATH 401; strong arithmetic ability, second only to GPT-4 in this study, and sensitive to prompts and system messages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, trigonometric, logarithm, decimals, negative numbers, irrational constants, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>groups include small integers up to 1e12, decimals, irrationals, long expressions; performs well on addition/subtraction across ranges, struggles on big-number multiplication and decimal exponentiation</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompts including 'Calculate:', system-level messages (e.g. 'You are an accurate calculator.' / 'please calculate to four decimal places.'), zero-shot chain-of-thought (COT) prompt tested</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~75.06% (Table 3: Acc 75.06, RE 0.14, NNR 0.50). Per-operation (Table 1): addition ~97%, multiplication ~65%, division ~80%, exponentiation ~50% (integer exponents), trig ~44%, logarithm ~56%, irrationals ~64%, long expressions ~68%; performs near-perfectly on easy groups but degrades on hard/extrapolation groups (big numbers, decimals, long/complex expressions). Prompting to output 4 decimal places improves numeric output reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Behavioral evidence suggests ChatGPT does not call an external calculator API (produces close estimations consistent with internal generation). Arithmetic ability is tied to tokenization and pretraining frequency (digit-level tokenization helps), instruction tuning and RLHF improve performance; sometimes ChatGPT's generated explanations can mislead its own computations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Compared across models in the paper, ChatGPT outperforms many larger open models (authors hypothesize ChatGPT may be smaller than 175B but benefits from instruction tuning/RLHF and dataset), shows good extrapolation on hard queries compared to others.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Systematic failures on large-number multiplication (group 9) with near-correct head/tail but wrong middle; decimal-base exponentiation (treats '**' as multiplication sometimes), occasionally outputs non-numeric explanations unless constrained by a system prompt; limited precision on division unless asked for fixed decimal places.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with GPT-4, InstructGPT/davinci, multiple open-source LLMs; ablations on system prompts, 'Calculate:' vs COT vs LaTeX prompts, and effect of requiring four-decimal output.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>ChatGPT delivers strong arithmetic performance (near GPT-4) driven by instruction tuning and prompting, is sensitive to system prompts (which reduce non-number outputs), but still makes systematic numeric estimation errors on large multiplications and decimal exponentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e330.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (InstructGPT / OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-finetuned davinci model (InstructGPT family) evaluated on MATH 401; shows moderate arithmetic ability improved relative to SFT-only variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (InstructGPT family reported in paper as 175B for InstructGPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, trigonometry, logarithm, decimals, negatives, irrationals, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>same MATH 401 groups: integers up to 1e12, decimals, irrationals, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompts; 'Calculate:' prompt used as best prompt in experiments; RLHF fine-tuning (not applied in paper but known for model and discussed as important)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~56.61% (Table 3: Acc 56.61, RE 0.76, NNR 2.99). Outperforms earlier SFT-only davinci (text-davinci-002) in arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Instruction tuning (and RLHF) improves arithmetic behavior compared to SFT-only models; no internal mechanistic explanation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>As an RLHF/instruction-tuned 175B family model, performance is higher than SFT-only variants — authors attribute some gains to RLHF/instruction tuning rather than pure parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower accuracy than GPT series on many hard groups; yields a higher non-number ratio and larger relative errors compared to GPT-3.5-turbo and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to text-davinci-002 (SFT), code-davinci-002, and instruction-tuned vs non-instruction variants to highlight instruction tuning benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Instruction tuning and RLHF (as used in text-davinci-003) substantially improve arithmetic performance compared to SFT models, though still below GPT-3.5-turbo and GPT-4 on MATH 401.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e330.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>code-davinci-002 (Codex family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-pretrained model tested on MATH 401; performs poorly on arithmetic relative to instruction-tuned text models despite code pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (reported as Codex-scale in paper table)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, decimals, etc. (full MATH 401 coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>groups include big numbers, decimals, irrationals, long expressions (same MATH 401)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompts (Eqa prompt used as best in table), greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~21.7% (Table 3: Acc 21.7, RE 2.39, NNR 11.47). Performs worse than many text-trained instruction models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Behavioral conclusion: code pretraining alone does not guarantee strong arithmetic ability; mathematical reasoning appears distinct from simply being trained on code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Despite being a large model, code pretraining did not confer arithmetic advantage in this benchmark compared to instruction-tuned text models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High relative error and high non-number ratio compared to InstructGPT; poor accuracy on many groups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to text-davinci-002/003 and other code/text-pretrained models to argue that code-pretrain != arithmetic proficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Pretraining on code does not automatically yield strong arithmetic performance; instruction tuning and dataset composition matter more for arithmetic ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e330.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica-120B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica (120B, Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large model pre-trained on scientific texts and LaTeX sources; shows relatively strong arithmetic behavior among open models, attributed to LaTeX-rich pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica-120B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>120B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, decimals, LaTeX-style math expressions, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>MATH 401 groups (integers of various sizes, decimals, irrationals, long bracketed expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompts with LaTeX-oriented prompts (Eqa) found to be best for Galactica; greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~45.14% (Table 3: Acc 45.14, RE 1.30, NNR 3.99). Per-table shows better performance than many same- or larger-sized open models on arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Authors hypothesize LaTeX-heavy pretraining corpus improves arithmetic on LaTeX-styled expressions; digit-level tokenization (splitting numbers into digits/tokens) also helps Galactica.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Galactica-120B performed similarly to Galactica-30B on arithmetic, suggesting limited gains beyond ~30B for this capability in this model family.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles on division/exponentiation/trig/log in many cases; non-zero non-number ratio and relative error remain larger than top-performing GPTs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to other open models (LLaMA, OPT, Bloom, GLM) and to instruction-tuned variants; prompt ablations (LaTeX prompts perform best).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Pretraining on LaTeX/scientific text and digit-level tokenization correlate with better arithmetic performance among open models, but instruction/RLHF still yield superior results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e330.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-65B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (65B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Openly released foundation model evaluated on MATH 401; shows modest arithmetic ability, better when digit-level tokenization is used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, decimals, negatives, irrationals, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Same MATH 401 groups</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot with LaTeX-style prompting (Eqa best for LLaMA in table), greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~28.43% (Table 3: Acc 28.43, RE 1.61, NNR 4.74). Performs reasonably on addition/subtraction but weak on division/exponentiation/trig/log.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Digit-level tokenization (splitting numbers into digit tokens) benefits arithmetic; pretraining corpora and token frequencies affect arithmetic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Within LLaMA family, larger sizes improved arithmetic up to a point but not to top levels achieved by instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Poor on hard/extrapolation groups (big numbers, decimals, long expressions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Galactica, OPT, Bloom, GPT-NeoX; prompt ablations show LaTeX prompts help.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>LLaMA achieves modest arithmetic ability; tokenization and prompt format affect outcomes but instruction tuning yields larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e330.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT-175B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open pre-trained transformer family evaluated on arithmetic tasks; shows limited arithmetic ability relative to instruction-tuned counterparts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT-175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, decimals, negatives, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>MATH 401 groups (various sizes and complexities)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompting (best prompt: 'Calculate:' in experiments); some instruction-tuned OPT variants also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~21.70% (Table 3: Acc 21.70, RE 3.18, NNR 21.70). Instruction-tuned variants (opt-iml-max-30b) show some improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Instruction tuning improves arithmetic behavior; plain pretraining does not suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Larger parameter count alone (175B) did not produce strong arithmetic performance compared to instruction-tuned smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High non-number ratio and relative error in naive prompting; struggles on hard/extrapolation groups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared opt base vs instruction-tuned opt variants and other open models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>OPT baseline shows that large parameter count without instruction tuning does not guarantee arithmetic proficiency; instruction tuning improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e330.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-NeoX-20B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-NeoX-20B (EleutherAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open autoregressive model evaluated on MATH 401; attains middle-range arithmetic performance among open models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-NeoX-20B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, decimals, negatives, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>MATH 401 groups</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompts with LaTeX prompt (Eqa found best), greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~35.41% (Table 3: Acc 35.41, RE 1.19, NNR 4.49). Better than many baseline open models on arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No internal mechanism probed; performance consistent with digit-level tokenization and pretraining exposure contributing to ability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>As a mid-sized open model, performance is consistent with trend that size helps but instruction/data matter more.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails primarily on hard groups and big-number/exponentiation/trig/logarithm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with LLaMA, OPT, Galactica and other open LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-NeoX-20B shows reasonable arithmetic ability for an open model; digitization/tokenization and prompt choice influence results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e330.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bilingual pre-trained model evaluated on MATH 401; produces modest arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, decimals, negatives, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>MATH 401 groups</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompting (best prompt: '$' LaTeX-inspired in table), greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~25.94% (Table 3: Acc 25.94, RE 1.27, NNR 2.74).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No explicit mechanistic claims beyond the paper's general observations about tokenization and pretraining term frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>As a large bilingual model, performance is below top instruction-tuned models; indicates architecture/language mix not sufficient alone.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Struggles on hard/extrapolated tasks and produces higher relative error than top models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with other large open models and prompting formats.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GLM-130B performs moderately but is outperformed by instruction-tuned and LaTeX-pretrained models in arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e330.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BloomZ/Bloom-176B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM / BLOOMZ (176B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large multilingual open models evaluated on arithmetic tasks; BLOOMZ (instruction-tuned) performs slightly better than baseline BLOOM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOMZ-176B / BLOOM-176B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>basic arithmetic, multiplication, division, exponentiation, logarithm, decimals, long expressions</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>MATH 401 groups</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot with '$' prompt or plain prompts; instruction-tuned BLOOMZ evaluated vs BLOOM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>BLOOMZ overall accuracy ~22.44% (Table 3), BLOOM baseline ~20.20%. Relative errors larger than instruction-tuned GPT models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Instruction tuning (BLOOMZ) reduces relative error and non-number ratio versus baseline BLOOM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Large parameter count did not match performance of instruction-tuned GPT series; instruction tuning gave modest improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High relative error on many groups, low accuracy on hard groups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared BLOOMZ vs BLOOM and other open models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Multilingual large models require instruction tuning and suitable tokenization to approach arithmetic competence; BLOOMZ improves but remains far from GPT-4/ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e330.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e330.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-XXL (11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned encoder-decoder T5 variant evaluated on arithmetic; performs poorly on numeric tasks in this benchmark, likely due to tokenizer and numeric token coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How well do Large Language Models perform in Arithmetic tasks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder (T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, exponentiation, decimals, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>MATH 401 groups</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>zero-shot prompts (Eqa performed best for some models), in-context learning tested (ICL) with example cases</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall accuracy ~3.74% (Table 3: Acc 3.74, RE 5.78, NNR 43.89). Very high non-number ratio and relative error.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Tokenizer lacks math tokens (π, ×, ÷, °) and T5 tokenizer behavior harms numeric encoding; tokenization identified as a major limiting factor for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Small/medium encoder-decoder models with T5-style tokenizers underperform; instruction-tuning did not rescue arithmetic ability in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Often outputs no numbers (high NNR) or large numeric errors; ICL sometimes makes results worse for small Flan-T5 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared Flan-T5 to similarly-sized open models (OPT, LLaMA); tokenization differences highlighted.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Encoder-decoder T5-style tokenization and missing math tokens lead to near-failure on arithmetic tasks despite instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How well do Large Language Models perform in Arithmetic tasks?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating the limitations of the transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Evaluating transformer language models on arithmetic operations using number decomposition <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-330",
    "paper_id": "paper-99bd07e888476904c6dd77ca154fd48629ac6dce",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "State-of-the-art instruction-tuned large language model evaluated on a 401-example arithmetic benchmark (MATH 401); shows the best arithmetic performance in the paper across most operators and difficult settings.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, trigonometric (sin, cos, tan), logarithm, decimals, negative numbers, irrational constants (e, pi), long multi-operator expressions",
            "number_range_or_complexity": "small integers up to millions and beyond (groups include numbers up to 1e12), decimals, irrational constants, big-number multiplications (group 9), long bracketed expressions (group 14), exponentiation with integer and decimal exponents (groups 11-12)",
            "method_or_intervention": "zero-shot with targeted prompts (system prompt: \"You are an accurate calculator, please calculate provided equation to four decimal places.\"), Compare prompts including 'Calculate:', LaTeX-style prompts; greedy/default decoding",
            "performance_result": "Overall accuracy ~83.5% (Table 3: Acc 83.54, RE 0.07, NNR 0.00). Per-operation highlights (Table 1): addition ~99%, multiplication ~67%, division ~100%, exponentiation ~50%, trigonometry ~68%, logarithm ~76%, handling of irrationals 100%, long expressions accuracy ~96%; handles big numbers better than most but still fails on large-number multiplications (group 9) in the dataset.",
            "mechanistic_insight": "No direct mechanistic probes; paper reports behavioral observations: GPT-4 reliably performs step-by-step calculations (often claims to 'use a calculator' in generated message), correctly interprets irrational constants (e) and radians/degrees, and appears to produce systematic stepwise decomposition for long expressions. Pretraining/instruction tuning and prompt/system prompts affect behavior. No internal attention/head-level mechanism described.",
            "performance_scaling": "Not explicitly known for GPT-4; authors note GPT-4 is slower and likely larger than ChatGPT but no parameter count; generally outperforms smaller models — consistent with scaling trends but no internal scaling law established for GPT-4 specifically.",
            "failure_modes": "Fails on some big-number multiplications (close estimations with correct head/tail but wrong middle), limited accuracy on decimal-base exponentiation; otherwise strong. Rare non-number outputs when not given strict system prompt (not observed under tested prompt).",
            "comparison_baseline": "Compared against ChatGPT (gpt-3.5-turbo-0301), earlier InstructGPT/davinci models, and multiple open-source LLMs; ablations across prompts and with/without system messages.",
            "key_finding": "GPT-4 attains the highest arithmetic accuracy in MATH 401, excels at division, trig, logarithms and long stepwise calculations, but still shows failure on big-number multiplications and decimal exponentiation.",
            "uuid": "e330.0",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ChatGPT (gpt-3.5-turbo-0301)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301, OpenAI)",
            "brief_description": "Instruction-tuned conversational model (gpt-3.5-turbo-0301) evaluated on MATH 401; strong arithmetic ability, second only to GPT-4 in this study, and sensitive to prompts and system messages.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (ChatGPT)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, trigonometric, logarithm, decimals, negative numbers, irrational constants, long expressions",
            "number_range_or_complexity": "groups include small integers up to 1e12, decimals, irrationals, long expressions; performs well on addition/subtraction across ranges, struggles on big-number multiplication and decimal exponentiation",
            "method_or_intervention": "zero-shot prompts including 'Calculate:', system-level messages (e.g. 'You are an accurate calculator.' / 'please calculate to four decimal places.'), zero-shot chain-of-thought (COT) prompt tested",
            "performance_result": "Overall accuracy ~75.06% (Table 3: Acc 75.06, RE 0.14, NNR 0.50). Per-operation (Table 1): addition ~97%, multiplication ~65%, division ~80%, exponentiation ~50% (integer exponents), trig ~44%, logarithm ~56%, irrationals ~64%, long expressions ~68%; performs near-perfectly on easy groups but degrades on hard/extrapolation groups (big numbers, decimals, long/complex expressions). Prompting to output 4 decimal places improves numeric output reliability.",
            "mechanistic_insight": "Behavioral evidence suggests ChatGPT does not call an external calculator API (produces close estimations consistent with internal generation). Arithmetic ability is tied to tokenization and pretraining frequency (digit-level tokenization helps), instruction tuning and RLHF improve performance; sometimes ChatGPT's generated explanations can mislead its own computations.",
            "performance_scaling": "Compared across models in the paper, ChatGPT outperforms many larger open models (authors hypothesize ChatGPT may be smaller than 175B but benefits from instruction tuning/RLHF and dataset), shows good extrapolation on hard queries compared to others.",
            "failure_modes": "Systematic failures on large-number multiplication (group 9) with near-correct head/tail but wrong middle; decimal-base exponentiation (treats '**' as multiplication sometimes), occasionally outputs non-numeric explanations unless constrained by a system prompt; limited precision on division unless asked for fixed decimal places.",
            "comparison_baseline": "Compared with GPT-4, InstructGPT/davinci, multiple open-source LLMs; ablations on system prompts, 'Calculate:' vs COT vs LaTeX prompts, and effect of requiring four-decimal output.",
            "key_finding": "ChatGPT delivers strong arithmetic performance (near GPT-4) driven by instruction tuning and prompting, is sensitive to system prompts (which reduce non-number outputs), but still makes systematic numeric estimation errors on large multiplications and decimal exponentiation.",
            "uuid": "e330.1",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (InstructGPT / OpenAI)",
            "brief_description": "An instruction-finetuned davinci model (InstructGPT family) evaluated on MATH 401; shows moderate arithmetic ability improved relative to SFT-only variants.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_size": "175B (InstructGPT family reported in paper as 175B for InstructGPT variants)",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, trigonometry, logarithm, decimals, negatives, irrationals, long expressions",
            "number_range_or_complexity": "same MATH 401 groups: integers up to 1e12, decimals, irrationals, long expressions",
            "method_or_intervention": "zero-shot prompts; 'Calculate:' prompt used as best prompt in experiments; RLHF fine-tuning (not applied in paper but known for model and discussed as important)",
            "performance_result": "Overall accuracy ~56.61% (Table 3: Acc 56.61, RE 0.76, NNR 2.99). Outperforms earlier SFT-only davinci (text-davinci-002) in arithmetic tasks.",
            "mechanistic_insight": "Instruction tuning (and RLHF) improves arithmetic behavior compared to SFT-only models; no internal mechanistic explanation provided.",
            "performance_scaling": "As an RLHF/instruction-tuned 175B family model, performance is higher than SFT-only variants — authors attribute some gains to RLHF/instruction tuning rather than pure parameter count.",
            "failure_modes": "Lower accuracy than GPT series on many hard groups; yields a higher non-number ratio and larger relative errors compared to GPT-3.5-turbo and GPT-4.",
            "comparison_baseline": "Compared to text-davinci-002 (SFT), code-davinci-002, and instruction-tuned vs non-instruction variants to highlight instruction tuning benefits.",
            "key_finding": "Instruction tuning and RLHF (as used in text-davinci-003) substantially improve arithmetic performance compared to SFT models, though still below GPT-3.5-turbo and GPT-4 on MATH 401.",
            "uuid": "e330.2",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "code-davinci-002",
            "name_full": "code-davinci-002 (Codex family)",
            "brief_description": "A code-pretrained model tested on MATH 401; performs poorly on arithmetic relative to instruction-tuned text models despite code pretraining.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_size": "175B (reported as Codex-scale in paper table)",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, decimals, etc. (full MATH 401 coverage)",
            "number_range_or_complexity": "groups include big numbers, decimals, irrationals, long expressions (same MATH 401)",
            "method_or_intervention": "zero-shot prompts (Eqa prompt used as best in table), greedy decoding",
            "performance_result": "Overall accuracy ~21.7% (Table 3: Acc 21.7, RE 2.39, NNR 11.47). Performs worse than many text-trained instruction models.",
            "mechanistic_insight": "Behavioral conclusion: code pretraining alone does not guarantee strong arithmetic ability; mathematical reasoning appears distinct from simply being trained on code.",
            "performance_scaling": "Despite being a large model, code pretraining did not confer arithmetic advantage in this benchmark compared to instruction-tuned text models.",
            "failure_modes": "High relative error and high non-number ratio compared to InstructGPT; poor accuracy on many groups.",
            "comparison_baseline": "Compared to text-davinci-002/003 and other code/text-pretrained models to argue that code-pretrain != arithmetic proficiency.",
            "key_finding": "Pretraining on code does not automatically yield strong arithmetic performance; instruction tuning and dataset composition matter more for arithmetic ability.",
            "uuid": "e330.3",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Galactica-120B",
            "name_full": "Galactica (120B, Meta)",
            "brief_description": "Large model pre-trained on scientific texts and LaTeX sources; shows relatively strong arithmetic behavior among open models, attributed to LaTeX-rich pretraining.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "Galactica-120B",
            "model_size": "120B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, decimals, LaTeX-style math expressions, long expressions",
            "number_range_or_complexity": "MATH 401 groups (integers of various sizes, decimals, irrationals, long bracketed expressions)",
            "method_or_intervention": "zero-shot prompts with LaTeX-oriented prompts (Eqa) found to be best for Galactica; greedy decoding",
            "performance_result": "Overall accuracy ~45.14% (Table 3: Acc 45.14, RE 1.30, NNR 3.99). Per-table shows better performance than many same- or larger-sized open models on arithmetic.",
            "mechanistic_insight": "Authors hypothesize LaTeX-heavy pretraining corpus improves arithmetic on LaTeX-styled expressions; digit-level tokenization (splitting numbers into digits/tokens) also helps Galactica.",
            "performance_scaling": "Galactica-120B performed similarly to Galactica-30B on arithmetic, suggesting limited gains beyond ~30B for this capability in this model family.",
            "failure_modes": "Struggles on division/exponentiation/trig/log in many cases; non-zero non-number ratio and relative error remain larger than top-performing GPTs.",
            "comparison_baseline": "Compared to other open models (LLaMA, OPT, Bloom, GLM) and to instruction-tuned variants; prompt ablations (LaTeX prompts perform best).",
            "key_finding": "Pretraining on LaTeX/scientific text and digit-level tokenization correlate with better arithmetic performance among open models, but instruction/RLHF still yield superior results.",
            "uuid": "e330.4",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "LLaMA-65B",
            "name_full": "LLaMA (65B)",
            "brief_description": "Openly released foundation model evaluated on MATH 401; shows modest arithmetic ability, better when digit-level tokenization is used.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "LLaMA-65B",
            "model_size": "65B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, decimals, negatives, irrationals, long expressions",
            "number_range_or_complexity": "Same MATH 401 groups",
            "method_or_intervention": "zero-shot with LaTeX-style prompting (Eqa best for LLaMA in table), greedy decoding",
            "performance_result": "Overall accuracy ~28.43% (Table 3: Acc 28.43, RE 1.61, NNR 4.74). Performs reasonably on addition/subtraction but weak on division/exponentiation/trig/log.",
            "mechanistic_insight": "Digit-level tokenization (splitting numbers into digit tokens) benefits arithmetic; pretraining corpora and token frequencies affect arithmetic behavior.",
            "performance_scaling": "Within LLaMA family, larger sizes improved arithmetic up to a point but not to top levels achieved by instruction-tuned models.",
            "failure_modes": "Poor on hard/extrapolation groups (big numbers, decimals, long expressions).",
            "comparison_baseline": "Compared to Galactica, OPT, Bloom, GPT-NeoX; prompt ablations show LaTeX prompts help.",
            "key_finding": "LLaMA achieves modest arithmetic ability; tokenization and prompt format affect outcomes but instruction tuning yields larger gains.",
            "uuid": "e330.5",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "OPT-175B",
            "name_full": "OPT (175B)",
            "brief_description": "Open pre-trained transformer family evaluated on arithmetic tasks; shows limited arithmetic ability relative to instruction-tuned counterparts.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "OPT-175B",
            "model_size": "175B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, decimals, negatives, long expressions",
            "number_range_or_complexity": "MATH 401 groups (various sizes and complexities)",
            "method_or_intervention": "zero-shot prompting (best prompt: 'Calculate:' in experiments); some instruction-tuned OPT variants also evaluated",
            "performance_result": "Overall accuracy ~21.70% (Table 3: Acc 21.70, RE 3.18, NNR 21.70). Instruction-tuned variants (opt-iml-max-30b) show some improvement.",
            "mechanistic_insight": "Instruction tuning improves arithmetic behavior; plain pretraining does not suffice.",
            "performance_scaling": "Larger parameter count alone (175B) did not produce strong arithmetic performance compared to instruction-tuned smaller models.",
            "failure_modes": "High non-number ratio and relative error in naive prompting; struggles on hard/extrapolation groups.",
            "comparison_baseline": "Compared opt base vs instruction-tuned opt variants and other open models.",
            "key_finding": "OPT baseline shows that large parameter count without instruction tuning does not guarantee arithmetic proficiency; instruction tuning improves results.",
            "uuid": "e330.6",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-NeoX-20B",
            "name_full": "GPT-NeoX-20B (EleutherAI)",
            "brief_description": "Open autoregressive model evaluated on MATH 401; attains middle-range arithmetic performance among open models.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "GPT-NeoX-20B",
            "model_size": "20B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, decimals, negatives, long expressions",
            "number_range_or_complexity": "MATH 401 groups",
            "method_or_intervention": "zero-shot prompts with LaTeX prompt (Eqa found best), greedy decoding",
            "performance_result": "Overall accuracy ~35.41% (Table 3: Acc 35.41, RE 1.19, NNR 4.49). Better than many baseline open models on arithmetic.",
            "mechanistic_insight": "No internal mechanism probed; performance consistent with digit-level tokenization and pretraining exposure contributing to ability.",
            "performance_scaling": "As a mid-sized open model, performance is consistent with trend that size helps but instruction/data matter more.",
            "failure_modes": "Fails primarily on hard groups and big-number/exponentiation/trig/logarithm tasks.",
            "comparison_baseline": "Compared with LLaMA, OPT, Galactica and other open LLMs.",
            "key_finding": "GPT-NeoX-20B shows reasonable arithmetic ability for an open model; digitization/tokenization and prompt choice influence results.",
            "uuid": "e330.7",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GLM-130B",
            "name_full": "GLM-130B",
            "brief_description": "Bilingual pre-trained model evaluated on MATH 401; produces modest arithmetic performance.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "GLM-130B",
            "model_size": "130B",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, decimals, negatives, long expressions",
            "number_range_or_complexity": "MATH 401 groups",
            "method_or_intervention": "zero-shot prompting (best prompt: '$' LaTeX-inspired in table), greedy decoding",
            "performance_result": "Overall accuracy ~25.94% (Table 3: Acc 25.94, RE 1.27, NNR 2.74).",
            "mechanistic_insight": "No explicit mechanistic claims beyond the paper's general observations about tokenization and pretraining term frequency.",
            "performance_scaling": "As a large bilingual model, performance is below top instruction-tuned models; indicates architecture/language mix not sufficient alone.",
            "failure_modes": "Struggles on hard/extrapolated tasks and produces higher relative error than top models.",
            "comparison_baseline": "Compared with other large open models and prompting formats.",
            "key_finding": "GLM-130B performs moderately but is outperformed by instruction-tuned and LaTeX-pretrained models in arithmetic tasks.",
            "uuid": "e330.8",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "BloomZ/Bloom-176B",
            "name_full": "BLOOM / BLOOMZ (176B)",
            "brief_description": "Large multilingual open models evaluated on arithmetic tasks; BLOOMZ (instruction-tuned) performs slightly better than baseline BLOOM.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "BLOOMZ-176B / BLOOM-176B",
            "model_size": "176B",
            "model_architecture": null,
            "arithmetic_operation_type": "basic arithmetic, multiplication, division, exponentiation, logarithm, decimals, long expressions",
            "number_range_or_complexity": "MATH 401 groups",
            "method_or_intervention": "zero-shot with '$' prompt or plain prompts; instruction-tuned BLOOMZ evaluated vs BLOOM",
            "performance_result": "BLOOMZ overall accuracy ~22.44% (Table 3), BLOOM baseline ~20.20%. Relative errors larger than instruction-tuned GPT models.",
            "mechanistic_insight": "Instruction tuning (BLOOMZ) reduces relative error and non-number ratio versus baseline BLOOM.",
            "performance_scaling": "Large parameter count did not match performance of instruction-tuned GPT series; instruction tuning gave modest improvements.",
            "failure_modes": "High relative error on many groups, low accuracy on hard groups.",
            "comparison_baseline": "Compared BLOOMZ vs BLOOM and other open models.",
            "key_finding": "Multilingual large models require instruction tuning and suitable tokenization to approach arithmetic competence; BLOOMZ improves but remains far from GPT-4/ChatGPT.",
            "uuid": "e330.9",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Flan-T5-XXL",
            "name_full": "Flan-T5-XXL (11B)",
            "brief_description": "Instruction-tuned encoder-decoder T5 variant evaluated on arithmetic; performs poorly on numeric tasks in this benchmark, likely due to tokenizer and numeric token coverage.",
            "citation_title": "How well do Large Language Models perform in Arithmetic tasks?",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XXL",
            "model_size": "11B",
            "model_architecture": "encoder-decoder (T5 family)",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, exponentiation, decimals, etc.",
            "number_range_or_complexity": "MATH 401 groups",
            "method_or_intervention": "zero-shot prompts (Eqa performed best for some models), in-context learning tested (ICL) with example cases",
            "performance_result": "Overall accuracy ~3.74% (Table 3: Acc 3.74, RE 5.78, NNR 43.89). Very high non-number ratio and relative error.",
            "mechanistic_insight": "Tokenizer lacks math tokens (π, ×, ÷, °) and T5 tokenizer behavior harms numeric encoding; tokenization identified as a major limiting factor for arithmetic.",
            "performance_scaling": "Small/medium encoder-decoder models with T5-style tokenizers underperform; instruction-tuning did not rescue arithmetic ability in this setting.",
            "failure_modes": "Often outputs no numbers (high NNR) or large numeric errors; ICL sometimes makes results worse for small Flan-T5 variants.",
            "comparison_baseline": "Compared Flan-T5 to similarly-sized open models (OPT, LLaMA); tokenization differences highlighted.",
            "key_finding": "Encoder-decoder T5-style tokenization and missing math tokens lead to near-failure on arithmetic tasks despite instruction tuning.",
            "uuid": "e330.10",
            "source_info": {
                "paper_title": "How well do Large Language Models perform in Arithmetic tasks?",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating the limitations of the transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Evaluating transformer language models on arithmetic operations using number decomposition",
            "rating": 2
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1
        }
    ],
    "cost": 0.017641749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How well do Large Language Models perform in Arithmetic tasks?</h1>
<p>Zheng Yuan ${ }^{1}$ Hongyi Yuan ${ }^{12}$ Chuanqi Tan ${ }^{1}$ Wei Wang ${ }^{1}$ Songfang Huang ${ }^{1}$<br>${ }^{1}$ Alibaba Group ${ }^{2}$ Tsinghua University<br>{yuanzheng.yuanzhen, chuanqi.tcq, hebian.ww, songfang.hsf}@alibaba-inc.com yuanhy2@@mails.tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Large language models have emerged abilities including chain-of-thought to answer math word problems step by step (Wei et al., 2022b). Solving math word problems not only requires abilities to disassemble problems via chain-ofthought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at https://github.com/ GanjinZero/math4@1-11m.</p>
<h2>1 Introduction</h2>
<p>Emergent abilities show in sufficiently large language models (LLMs) (Wei et al., 2022a) like chain-of-thought reasoning (COT) (Wei et al., 2022b). Chain-of-thought reasoning requires LLMs to solve a question by thinking questions step by step which performs well in school math word problems (Wei et al., 2022b; Kojima et al., 2022). Recent LLMs are further fine-tuned with instruction tuning (Sanh et al., 2021; Chung et al., 2022; Ouyang et al., 2022) which demonstrates improved COT ability compared to only selfsupervised pre-training. To solve a math word problem, COT disassembles the problem into simple steps. For each step, LLMs have to compute correctly based on arithmetic expressions. Thus, evaluating the arithmetic ability of LLMs is necessary since it is the upper bound of LLMs' ability for solving math word problems.</p>
<p>To this end, we propose an arithmetic dataset named MATH 401. Different difficulties are</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>contained in this dataset including addition $(+)$, subtraction $(-)$, multiplication $(\times)$, division $(\div)$, exponentiation $(\wedge)$, trigonometry functions ( $\sin , \cos , \tan$ ), and logarithm functions ( $\log , \ln$ ) of integers, decimals, and irrational numbers $(\pi, e)$. Long arithmetic expressions with brackets are also included which are common in complex math word problems. Results in Table 1 show detailed evaluations on OpenAI's GPTs including GPT-4 (OpenAI, 2023), ChatGPT ${ }^{2}$, GPT-3.5 (Ouyang et al., 2022) and other open-sourced LLMs. We find that GPT-4 and ChatGPT outperform other models by a large margin in all kinds of arithmetic abilities. InstructGPT (Ouyang et al., 2022) and Galactica (Taylor et al., 2022) do have some arithmetic abilities. We analyze factors affecting LLMs' arithmetic ability systematically including tokenization (§4.2), pre-training (§4.3), prompts (§4.4), interpolation and extrapolation (§4.5), scaling laws (§4.6), COT (§4.7), and ICL (§4.8).</p>
<p>One may say that the ability to solve arithmetic tasks is not necessary for a large language model. LLMs can use the calculator API when they need to decode an answer (Schick et al., 2023). Arithmetic ability evaluation can be a gauge for general intelligence since mastering arithmetic serves as a fundamental requirement for performing intricate mathematical tasks including symbolic math reasoning (Noorbakhsh et al., 2021; Gaur and Saunshi, 2022) and automatic theorem proofing (Polu and Sutskever, 2020; Wu et al., 2022).</p>
<h2>2 Related Works</h2>
<p>Evaluate Math Ability of LLMs To show the math reasoning ability of LLMs, Wang and Komatsuzaki (2021); Chung et al. (2022); Thoppilan et al. (2022) evaluate their models on various math word problems benchmark (Saxton et al., 2019; Hendrycks et al., 2021; Cobbe et al., 2021; Shi</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">E</th>
<th style="text-align: center;">$+$</th>
<th style="text-align: center;">$\times$</th>
<th style="text-align: center;">$\div$</th>
<th style="text-align: center;">$\wedge$</th>
<th style="text-align: center;">Tri</th>
<th style="text-align: center;">$\log$</th>
<th style="text-align: center;">Dec</th>
<th style="text-align: center;">Neg</th>
<th style="text-align: center;">Irr</th>
<th style="text-align: center;">Big</th>
<th style="text-align: center;">Long</th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{9 9}$</td>
<td style="text-align: center;">$\mathbf{6 7}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{5 0}$</td>
<td style="text-align: center;">$\mathbf{6 8}$</td>
<td style="text-align: center;">$\mathbf{7 6}$</td>
<td style="text-align: center;">$\mathbf{6 7}$</td>
<td style="text-align: center;">$\mathbf{6 7}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{4 8}$</td>
<td style="text-align: center;">$\mathbf{9 6}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{6 7}$</td>
<td style="text-align: center;">$\mathbf{8 4}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\underline{97}$</td>
<td style="text-align: center;">$\underline{65}$</td>
<td style="text-align: center;">$\underline{80}$</td>
<td style="text-align: center;">$\underline{\mathbf{5 0}}$</td>
<td style="text-align: center;">$\underline{44}$</td>
<td style="text-align: center;">$\underline{56}$</td>
<td style="text-align: center;">$\mathbf{6 7}$</td>
<td style="text-align: center;">$\mathbf{6 7}$</td>
<td style="text-align: center;">$\underline{64}$</td>
<td style="text-align: center;">$\underline{40}$</td>
<td style="text-align: center;">$\underline{68}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\underline{49}$</td>
<td style="text-align: center;">$\underline{74}$</td>
</tr>
<tr>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\underline{83}$</td>
<td style="text-align: center;">$\underline{59}$</td>
<td style="text-align: center;">$\underline{80}$</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$\underline{64}$</td>
<td style="text-align: center;">$\underline{64}$</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">57</td>
</tr>
<tr>
<td style="text-align: left;">CodeX</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: left;">Galactica</td>
<td style="text-align: center;">120B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: center;">65B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">28</td>
</tr>
<tr>
<td style="text-align: left;">OPT</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neox</td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: left;">GLM</td>
<td style="text-align: center;">130B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: left;">BloomZ</td>
<td style="text-align: center;">176B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: left;">Bloom</td>
<td style="text-align: center;">176B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">T0++</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Flan-T5</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 1: Arithmetic ability for LLMs measured by accuracy, we only list models with largest parameter counts. E $=$ Euler, Dec $=$ Decimal, Neg $=$ Negative, $\operatorname{Irr}=$ Irrational, Big $=$ Big Numbers, Long $=$ Long Expressions.
et al., 2022). For newly released LLM ChatGPT, Shakarian et al. (2023); Frieder et al. (2023) evaluate its mathematical ability independently. To notice, our paper evaluates ChatGPT using gpt-3.5-turbo-0301 version and GPT-4 using chat UI on March 16th which may have different performances compared to their reported results and future analysis.</p>
<h2>Evaluate Arithmetic Ability of LLMs</h2>
<p>Nogueira et al. (2021); Wang et al. (2021) evaluate pretrained language models on simple arithmetic expressions including addition (+) and subtraction (-). Muffo et al. (2022) have further tested the multiplication $(\times)$ of language models. They found tokenization (Nogueira et al., 2021; Kim et al., 2021) and token frequency (Razeghi et al., 2022) are two important factors for language model arithmetic ability. Compared to previous work, we focus on evaluating Large LMs (with instruction fine-tuning) on comprehensive arithmetic abilities with different types of operators and numbers.</p>
<h2>3 Evaluation Settings</h2>
<h3>3.1 Arithmetic Expression Settings</h3>
<p>We construct 401 arithmetic expressions to test large language models which include Euler equation $\left(e^{i \pi}+1=0\right)$ as group 0 and 25 problems each for group $1 \sim 16$. If not otherwise mentioned, used numbers are positive integers.</p>
<ul>
<li>Euler Equation.</li>
<li>Add \&amp; Subtract of two integers within 10.</li>
<li>Add \&amp; Subtract of two integers within 100.</li>
<li>Add \&amp; Subtract of two integers within 1,000.</li>
<li>Add \&amp; Subtract of two integers within $1,000,000,000,000$.</li>
<li>Add \&amp; Subtract of two integers within -$10 \sim 10$.</li>
<li>Add \&amp; Subtract of two decimal numbers within $-100 \sim 100$.</li>
<li>Multiply two integers within 100.</li>
<li>Multiply two decimal numbers within 10.</li>
<li>Multiply two integers within 100,000.</li>
<li>Division of two integers within 100.</li>
<li>Exponentiation of with integer base within 10 and integer exponent within $2 \sim 4$.</li>
<li>Exponentiation of with a decimal number within 10 as the base and a decimal number within $2 \sim 4$ as the exponent.</li>
<li>Add, Subtract \&amp; Multiply with one integer within 10 and a common irrational number (i.e. $e$ or $\pi$ ).</li>
<li>Long arithmetic expressions with brackets, involved integers are all within 100 and operators contain add, subtract, multiply, and division.</li>
<li>Trigonometry functions including $\sin , \cos$, and tan. Inputs can be in the format of degrees and radians ( $\pi$ can also appear in the inputs).</li>
<li>Logarithm of integers within 1000 of different bases: $2, e, 10$.</li>
</ul>
<table>
<thead>
<tr>
<th>Model</th>
<th>Prompt</th>
<th>Acc $\uparrow$</th>
<th>RE $\downarrow$</th>
<th>NNR $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-4</td>
<td>Cal*4</td>
<td>83.54</td>
<td>0.07</td>
<td>0.00</td>
</tr>
<tr>
<td>gpt-3.5-turbo-0301</td>
<td>Cal*</td>
<td>75.06</td>
<td>0.14</td>
<td>0.50</td>
</tr>
<tr>
<td>text-davinci-003</td>
<td>Cal</td>
<td>56.61</td>
<td>0.76</td>
<td>2.99</td>
</tr>
<tr>
<td>code-davinci-002</td>
<td>Eqa</td>
<td>21.7</td>
<td>2.39</td>
<td>11.47</td>
</tr>
<tr>
<td>galactica-120b</td>
<td>Eqa</td>
<td>45.14</td>
<td>1.30</td>
<td>3.99</td>
</tr>
<tr>
<td>galactica-30b</td>
<td>Eqa</td>
<td>45.14</td>
<td>0.69</td>
<td>1.75</td>
</tr>
<tr>
<td>llama-65b</td>
<td>Eqa</td>
<td>28.43</td>
<td>1.61</td>
<td>4.74</td>
</tr>
<tr>
<td>opt-175b</td>
<td>Cal</td>
<td>21.70</td>
<td>3.18</td>
<td>21.70</td>
</tr>
<tr>
<td>gpt-neox-20b</td>
<td>Eqa</td>
<td>35.41</td>
<td>1.19</td>
<td>4.49</td>
</tr>
<tr>
<td>glm-130b</td>
<td>$</td>
<td>25.94</td>
<td>1.27</td>
<td>2.74</td>
</tr>
<tr>
<td>bloomz-176b</td>
<td>$$</td>
<td>22.44</td>
<td>1.50</td>
<td>4.74</td>
</tr>
<tr>
<td>bloom-176b</td>
<td>$</td>
<td>20.20</td>
<td>2.60</td>
<td>18.45</td>
</tr>
<tr>
<td>T0++-11b</td>
<td>Cal</td>
<td>4.24</td>
<td>3.34</td>
<td>9.48</td>
</tr>
<tr>
<td>flan-t5-xxl-11b</td>
<td>Eqa</td>
<td>3.74</td>
<td>5.78</td>
<td>43.89</td>
</tr>
<tr>
<td>flan-t5-xl-3b</td>
<td>$</td>
<td>7.48</td>
<td>3.34</td>
<td>25.19</td>
</tr>
</tbody>
</table>
<p>These groups cover mathematical operators used in elementary mathematics. We consider groups 1,2,3,5,6,7,8,11 as Easy queries and all others as Hard queries. We calculate the results of all arithmetic expressions using built-in functions of Python and round to four decimal places. Examples of expressions are listed in Appendix A.</p>
<h3>3.2 Metrics</h3>
<p>Since LLMs can decode arbitrary contents (which may contain their step-by-step calculation steps), we first ignore decoded numbers in parentheses and preserve the last number decoded by LLMs. If the decoded number is a fraction, we will convert it to decimal for evaluation except for group 10 which requires calculating division. To measure the arithmetic ability of LLMs, we use the following metrics to measure their outputs.</p>
<p>Accuracy If the difference between the decoded number and the target number is less than $1e-3$, we consider it a correct prediction. Accuracy is calculated based on correct prediction counts.</p>
<p>Relative error We denote decoded number is $\hat{y}$ and target is $y$. We calculate relative error by:</p>
<p>$R E=\min(10,\frac{|\hat{y}-y|}{\max(|y|,1)})$ (1)</p>
<p>If LLM does not decode any number, we consider $R E=10$. We truncate the relative error to 10 to prevent that one big mistake dominate the average relative error.</p>
<p>Non-number ratio If decoded content does not contain any numbers, we consider it a failure. We calculate the non-number ratio based on it.</p>
<h3>3.3 Evaluation Details</h3>
<p>We test GPT-4 by their official chat UI${ }^{3}$. Since GPT-4 has limited request counts, we only query GPT-4 with groups that ChatGPT cannot answer correctly. We test GPT-3.5 (including davinci (CodeX, InstructGPT) and turbo (ChatGPT) series models) <em>Ouyang et al. (2022); Chen et al. (2021)</em> via OpenAI APIs. We also test following open-sourced LLMs including Galactica <em>Taylor et al. (2022)</em>, GPT from EleutherAI <em>Wang and Komatsuzaki (2021); Black et al. (2022)</em>, LLaMA <em>Touvron et al. (2023)</em>, OPT (with instruction learning) <em>Zhang et al. (2022)</em>, Bloom (with instruction learning)</p>
<p>Table 2: Evaluation on MATH 401 with different LLMs. Prompts are selected via best accuracy. Cal means "Calculate:" and Eqa means "\begin{equation}*. * means providing an additional system-level message.</p>
<p>[Scao et al., 2022; Muennighoff et al., 2022], T0++ <em>Sanh et al. (2021)</em>, GLM <em>Zeng et al. (2022)</em> and Flan-T5 <em>Chung et al. (2022)</em>. We also test the smaller versions of the above models.</p>
<p>We test following prompts: $\emptyset$ (i.e. no prompt), "Calculate:", "$", "$$", and "\begin{equation}*. The latest three prompts are inspired by that LLMs may be pretrained with LaTeX sources. We provide three versions of input formats: math texts ( $\pi$ ), plain texts (pi), LaTeX texts ( $\backslash$ pi). When we use LaTeX-related prompts, we provide the model with LaTeX texts. When we use other prompts, we will provide math texts if their tokenizers can encode them. Otherwise, we will provide plain text. For ChatGPT (gpt-3.5-turbo-0301), we test different system-level prompts as instructions: $\emptyset$ (i.e. no prompt), "You are an accurate calculator.", and "You are an accurate calculator, please calculate provided equation to four decimal places.". For GPT-4, we only test prompt "You are an accurate calculator, please calculate provided equation to four decimal places.".</p>
<p>We use default decode settings for OpenAI APIs, and we use greedy decoding for all other LLMs.</p>
<h2>4 Results and Analysis</h2>
<h3>4.1 Results</h3>
<p>Overall Results Table 1, 2, and 3 show results of different LLMs on MATH 401. We find GPT-4 and ChatGPT outperform all other models by a</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Acc $\uparrow$</th>
<th style="text-align: center;">RE $\downarrow$</th>
<th style="text-align: center;">NNR $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gpt-4</td>
<td style="text-align: center;">Cal*4</td>
<td style="text-align: center;">83.54</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">gpt-3.5-turbo-0301</td>
<td style="text-align: center;">Cal*</td>
<td style="text-align: center;">75.06</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-003</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">56.61</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">2.99</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">42.89</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">15.96</td>
</tr>
<tr>
<td style="text-align: center;">text-curie-001</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">11.47</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">6.48</td>
</tr>
<tr>
<td style="text-align: center;">text-babbage-001</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">5.24</td>
<td style="text-align: center;">2.59</td>
<td style="text-align: center;">5.74</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">21.70</td>
<td style="text-align: center;">2.39</td>
<td style="text-align: center;">11.47</td>
</tr>
<tr>
<td style="text-align: center;">galactica-120b</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">45.14</td>
<td style="text-align: center;">1.30</td>
<td style="text-align: center;">3.99</td>
</tr>
<tr>
<td style="text-align: center;">galactica-30b</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">45.14</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">1.75</td>
</tr>
<tr>
<td style="text-align: center;">galactica-6.7b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">34.41</td>
<td style="text-align: center;">2.61</td>
<td style="text-align: center;">8.73</td>
</tr>
<tr>
<td style="text-align: center;">llama-65b</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">28.43</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;">4.74</td>
</tr>
<tr>
<td style="text-align: center;">llama-30b</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">30.17</td>
<td style="text-align: center;">1.72</td>
<td style="text-align: center;">3.74</td>
</tr>
<tr>
<td style="text-align: center;">llama-13b</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">27.68</td>
<td style="text-align: center;">2.40</td>
<td style="text-align: center;">9.73</td>
</tr>
<tr>
<td style="text-align: center;">llama-7b</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">21.95</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">7.48</td>
</tr>
<tr>
<td style="text-align: center;">opt-175b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">21.70</td>
<td style="text-align: center;">3.18</td>
<td style="text-align: center;">21.70</td>
</tr>
<tr>
<td style="text-align: center;">opt-66b</td>
<td style="text-align: center;">$\emptyset$</td>
<td style="text-align: center;">20.70</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">18.70</td>
</tr>
<tr>
<td style="text-align: center;">opt-iml-max-30b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">17.46</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">6.23</td>
</tr>
<tr>
<td style="text-align: center;">opt-30b</td>
<td style="text-align: center;">$\emptyset$</td>
<td style="text-align: center;">15.96</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">11.22</td>
</tr>
<tr>
<td style="text-align: center;">opt-13b</td>
<td style="text-align: center;">$\emptyset$</td>
<td style="text-align: center;">15.21</td>
<td style="text-align: center;">2.19</td>
<td style="text-align: center;">10.97</td>
</tr>
<tr>
<td style="text-align: center;">opt-6.7b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">14.46</td>
<td style="text-align: center;">1.46</td>
<td style="text-align: center;">4.24</td>
</tr>
<tr>
<td style="text-align: center;">gpt-neox-20b</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">35.41</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">4.49</td>
</tr>
<tr>
<td style="text-align: center;">gpt-j-6b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">27.18</td>
<td style="text-align: center;">1.55</td>
<td style="text-align: center;">8.98</td>
</tr>
<tr>
<td style="text-align: center;">bloomz-176b</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">22.44</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">4.74</td>
</tr>
<tr>
<td style="text-align: center;">bloom-176b</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">18.45</td>
</tr>
<tr>
<td style="text-align: center;">bloomz-7b1</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">12.72</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">15.46</td>
</tr>
<tr>
<td style="text-align: center;">bloom-7b1</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">7.23</td>
<td style="text-align: center;">2.41</td>
<td style="text-align: center;">6.48</td>
</tr>
<tr>
<td style="text-align: center;">bloomz-3b</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">7.98</td>
<td style="text-align: center;">2.63</td>
<td style="text-align: center;">12.47</td>
</tr>
<tr>
<td style="text-align: center;">bloom-3b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">2.41</td>
<td style="text-align: center;">8.73</td>
</tr>
<tr>
<td style="text-align: center;">bloomz-1b7</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">4.28</td>
<td style="text-align: center;">31.17</td>
</tr>
<tr>
<td style="text-align: center;">bloom-1b7</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">5.24</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">11.22</td>
</tr>
<tr>
<td style="text-align: center;">T0++-11b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">9.48</td>
</tr>
<tr>
<td style="text-align: center;">glm-130b</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">25.94</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">2.74</td>
</tr>
<tr>
<td style="text-align: center;">glm-10b</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">14.96</td>
<td style="text-align: center;">2.30</td>
<td style="text-align: center;">3.74</td>
</tr>
<tr>
<td style="text-align: center;">flan-t5-xxl-11b</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">3.74</td>
<td style="text-align: center;">5.78</td>
<td style="text-align: center;">43.89</td>
</tr>
<tr>
<td style="text-align: center;">flan-t5-xl-3b</td>
<td style="text-align: center;">\$</td>
<td style="text-align: center;">7.48</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">25.19</td>
</tr>
<tr>
<td style="text-align: center;">flan-t5-large-780m</td>
<td style="text-align: center;">Cal</td>
<td style="text-align: center;">3.74</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">2.49</td>
</tr>
<tr>
<td style="text-align: center;">flan-t5-base-250m</td>
<td style="text-align: center;">Eqa</td>
<td style="text-align: center;">2.49</td>
<td style="text-align: center;">3.18</td>
<td style="text-align: center;">14.21</td>
</tr>
</tbody>
</table>
<p>Table 3: Full evaluation on MATH 401 with different LLMs. Prompts are selected via best accuracy.
large margin ${ }^{4}$. GPT-4 surpasses ChatGPT with accuracy of 10 points and reduce relative error half. InstructGPT performs third measured by accuracy and Galactica-30B performs third measured by relative error. Compared to models proposed before InstructGPT (text-davinci-003), GPT-series applies Reinforcement Learning from Human Feedback (RLHF) which may enhance their arithmetic ability significantly. Galactica is pre-trained with massive LaTeX source codes which could be the reason why Galactica performs well in arithmetics.</p>
<p>Grouped Results To clearly understand the arithmetic ability of LLMs, we show grouped accuracy in Table 1. GPT-4 obtains first places and ChatGPT obtains second places for all groups. Most LLMs are only capable of doing addition and subtraction and have some ability for multiplication.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Division, exponentiation, trigonometry functions, and logarithm functions are hard for most LLMs. LLMs have some abilities dealing with decimal, negative, and irrational numbers. Only GPT-4 and ChatGPT have the ability to deal with big numbers ( $&gt;1 e 12$ ) and complex long queries which proves their generalization and reasoning abilities. GPT-4 shows extremely good ability in long arithmetic expressions.</p>
<p>When will ChatGPT fail? Though ChatGPT obtains such a good performance, we will check when ChatGPT fails to answer. For multiplication $(\times)$, ChatGPT passes all queries in Group 7 and 8 and get wrong answers for all queries in Group 9. An example is ChatGPT predicts $71786 \times 21638=1,551,402,068$, while the true answer is $1,553,305,468$. ChatGPT gives a very close estimation with the correct head and tail, which proves that ChatGPT does not use a calculator API for math calculation.</p>
<p>For division in Group 11, ChatGPT sometimes gives correct answers to two decimal places which will be considered incorrect in our metric. We can see in Table 5, requiring ChatGPT to output four decimal places will improve its accuracy in multiplication and division.</p>
<p>For exponentiation $(\wedge)$, ChatGPT correctly answers all queries in Group 10 which contain only integers as bases. It is too hard for any language model (even ChatGPT) correctly estimate the exponentiation of a decimal number as the base and a decimal number as the exponent. It seems that ChatGPT treats $<em> </em>$ as multiplication sometimes, for example, ChatGPT estimates $5.5507 * * 2.0434=$ 10.31554$ which is close to $5.5507 \times 2.0434=$ 11.3423 and far from answer 33.1895 .</p>
<p>For calculating trigonometry functions, ChatGPT understands degrees and radians correctly and generates exact answers for special inputs like $\cos \left(-210^{\circ}\right)=-\frac{\sqrt{3}}{2}$ (we omit explanation generated by ChatGPT here). However, ChatGPT may generate wrong explanations which mislead itself. An example is: "We know that the sine function is periodic with a period of $2 \pi$, which means that $\sin (x+2 \pi)=\sin (x)$ for any value of $x$. Therefore, we can subtract multiples of $2 \pi$ from $-3.75 \pi$ until we get a value between 0 and $2 \pi:-3.75 \pi=-3 \pi-0.75 \pi=$ $-9.42477-2.35619=-11.78096$. Adding $2 \pi$, we get: $-11.78096+2 \pi=-9.42477$ etc." Any mistake in explanations may result in a wrong answer.</p>
<p>For logarithm functions, we find that ChatGPT is capable of using change of base formula and predicting answers within two decimal places.</p>
<p>For long expressions, ChatGPT can understand the operators’ priorities. ChatGPT sometimes generates answers step by step and sometimes generates answers directly. It is very likely to generate wrong answers when it decodes answers directly.</p>
<p>What about GPT-4? For big number multiplication $(\times)$ in group 9, GPT-4 also fails in all cases with similar problems occurring in ChatGPT.</p>
<p>For exponentiation $(\wedge)$, GPT-4 will not consider ** as $\times$ anymore and give better estimations.</p>
<p>For calculating expressions with irrational numbers, GPT-4 will consider $e$ as natural logarithm correctly.</p>
<p>For logarithm functions, GPT-4 calculates logarithm base $e$ and 10 by "using a calculator" (this is a message generated by GPT-4). GPT-4 calculates logarithm base 2 by change of base formula and generates approximate results.</p>
<p>For long equations, GPT-4 solves all equations step by step and obtains a much higher accuracy.</p>
<p>We compare and summarize how GPT-4 outperforms ChatGPT here:</p>
<ul>
<li>Better division ability.</li>
<li>Better trigonometry ability.</li>
<li>Understand irrational numbers properly.</li>
<li>Always calculate long expressions step by step.</li>
</ul>
<h3>4.2 Tokenization</h3>
<p>Arithmetic expressions have special tokens including $\pi, \times, \div,{ }^{\circ}$ which are not within T5 series models (i.e. T0++ and Flan-T5). T0++-11B (Acc 4.24 and RE 3.34) and Flan-T5-xxl-11B (Acc 3.74 and RE 5.78) perform badly on arithmetic tasks compared to other similar-size models: Opt-13B (Acc 15.21 and RE 2.19) and LLaMA-13B (Acc 27.68 and RE 2.4).</p>
<p>We notice that Galactica and LLaMA split numbers into individual tokens. For example 123.456 is converted into 123.456 . Razeghi et al. (2022) show that arithmetic ability is related to pre-training term frequencies. For tokens that appear more in pre-training, LLMs can have better accuracy in answering arithmetic expressions about them. Number tokens with more digits (e.g. 23) apparently appear less than single digit token (e.g.
2 and 3). Splitting numbers into individual tokens neglects all number tokens with more digits and makes all single digit tokens (mainly $0 \sim 9$ ) appear in the pre-training corpus in the same order of magnitude. Galactica-30B and LLaMA-30B obtain 45.14 and 30.17 in terms of accuracy (list in Table 3) that outperforms OPT-30B (15.96), Bloom176B (20.2), and GLM-130B (25.94), which show superiority of digit-level tokenization.</p>
<h3>4.3 Training</h3>
<p>Self-supervised Pre-training While pretraining, code corpus and $\mathrm{LAT}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$-sources are possible to relate to arithmetic ability since they all contain arithmetic operators and numbers. Code-davinci-002 is pretrained with code corpus. Code-davinci-002 performs well on many reasoning-related tasks (Zhou et al., 2022), however, it performs not good compared to other LLMs in arithmetics. This proves that mathematical reasoning ability is different from arithmetic ability which needs to understand numbers deeply. Galactica with numerous $\mathrm{LAT}</em>$ is useful.}} \mathrm{X}$-sources outperforms other LLMs except for InstructGPT and ChatGPT which show $\mathrm{LAT}_{\mathrm{E}} \mathrm{X</p>
<p>Instruction Tuning is also very important in arithmetic ability. Comparing Opt-30B (Acc 15.96 RE 2.28 NNR 11.22) with Opt-Iml-Max-30B (Acc 17.46 RE 1.52 NNR 6.23), Bloom (Acc 20.2 RE 2.6 NNR 18.45) with BloomZ (Acc 22.44 RE 1.5 NNR 4.74), and code-davinci-002 (Acc 21.7) with text-davinci-002 (Acc 42.89) in Table 3 show that instruction tuning can boost the performance in all metrics. Text-davinci-003 (RLHF) outperforms text-davinci-002 (SFT) in arithmetic tasks which shows RLHF is important for building arithmetic ability.</p>
<h3>4.4 Prompts</h3>
<p>Input Prompts We find the best prompts are different across LLMs. We list the best and worst prompts for LLMs in Table 8. We find models are sensitive to input prompts and not using prompts is the worst option for most LLMs. For InstructGPT and ChatGPT, using "Calculate" as a prompt perform best. For other LLMs, using $\mathrm{LAT}_{\mathrm{E}} \mathrm{X}$-related prompts perform best.</p>
<p>System Prompts For ChatGPT, we can also provide system-level messages as instruction prompts. Table 5 shows providing system-level messages improves ChatGPT's accuracy and reduces relative</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Best</th>
<th>Acc</th>
<th>Worst</th>
<th>Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-3.5-turbo-0301</td>
<td>Cal*</td>
<td>$\mathbf{7 5 . 0 6}$</td>
<td>$\$$</td>
<td>64.59</td>
</tr>
<tr>
<td>text-davinci-003</td>
<td>Cal</td>
<td>$\underline{56.61}$</td>
<td>Eqa</td>
<td>43.64</td>
</tr>
<tr>
<td>galactica-120b</td>
<td>Eqa</td>
<td>45.14</td>
<td>$\emptyset$</td>
<td>38.9</td>
</tr>
<tr>
<td>llama-65b</td>
<td>Eqa</td>
<td>28.43</td>
<td>Cal</td>
<td>4.74</td>
</tr>
<tr>
<td>opt-175b</td>
<td>Cal</td>
<td>21.7</td>
<td>$\emptyset$</td>
<td>15.21</td>
</tr>
<tr>
<td>gpt-neox-20b</td>
<td>Eqa</td>
<td>35.41</td>
<td>$\emptyset$</td>
<td>26.93</td>
</tr>
<tr>
<td>glm-130b</td>
<td>$\$$</td>
<td>25.94</td>
<td>$\emptyset$</td>
<td>22.44</td>
</tr>
<tr>
<td>bloomz-176b</td>
<td>$\$$</td>
<td>22.44</td>
<td>$\emptyset$</td>
<td>11.72</td>
</tr>
</tbody>
</table>
<p>Table 4: Best and worst prompts for different LLMs.
error significantly. The most different groups are group 13 irrational numbers and group 16 logarithm functions. Without a system-level message, ChatGPT thinks $e$ can be Euler's number or a variable and cannot give an answer. For logarithm functions, ChatGPT tries to explain how it calculates which may mislead our provided parser. We notice that if we require ChatGPT to output results to four decimal places, it will have a zero nonnumber ratio. To conclude, ChatGPT will try to explain the calculation procedure without a systemlevel prompt and will only provide answers with a system-level prompt.</p>
<table>
<thead>
<tr>
<th>Group</th>
<th>Cal</th>
<th></th>
<th>Cal*</th>
<th></th>
<th>Cal*4</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc</td>
<td>RE</td>
<td>Acc</td>
<td>RE</td>
<td>Acc</td>
<td>RE</td>
</tr>
<tr>
<td>0 Euler</td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{. 0 0}$</td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{. 0 0}$</td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{. 0 0}$</td>
</tr>
<tr>
<td>$1 \sim 6+-$</td>
<td>$\mathbf{9 7}$</td>
<td>$\mathbf{. 0 0}$</td>
<td>96</td>
<td>.00</td>
<td>93</td>
<td>.01</td>
</tr>
<tr>
<td>$7 \sim 10 \times \div$</td>
<td>69</td>
<td>.20</td>
<td>69</td>
<td>$\mathbf{. 0 1}$</td>
<td>$\mathbf{7 1}$</td>
<td>$\mathbf{. 0 1}$</td>
</tr>
<tr>
<td>$11 \sim 12 \wedge$</td>
<td>$\mathbf{5 0}$</td>
<td>$\mathbf{. 2 4}$</td>
<td>$\mathbf{5 0}$</td>
<td>.32</td>
<td>$\mathbf{5 0}$</td>
<td>.27</td>
</tr>
<tr>
<td>13 Irr.</td>
<td>64</td>
<td>1.73</td>
<td>72</td>
<td>.56</td>
<td>$\mathbf{8 4}$</td>
<td>$\mathbf{. 1 1}$</td>
</tr>
<tr>
<td>14 Long</td>
<td>$\mathbf{6 8}$</td>
<td>$\mathbf{. 1 9}$</td>
<td>64</td>
<td>.46</td>
<td>60</td>
<td>.59</td>
</tr>
<tr>
<td>15 Tri.</td>
<td>44</td>
<td>1.21</td>
<td>$\mathbf{4 8}$</td>
<td>$\mathbf{. 9 6}$</td>
<td>44</td>
<td>1.40</td>
</tr>
<tr>
<td>16 Log</td>
<td>56</td>
<td>.80</td>
<td>$\mathbf{6 0}$</td>
<td>.04</td>
<td>56</td>
<td>$\mathbf{. 0 1}$</td>
</tr>
<tr>
<td>Overall</td>
<td>74</td>
<td>.33</td>
<td>$\mathbf{7 5}$</td>
<td>$\mathbf{. 1 4}$</td>
<td>74</td>
<td>$\mathbf{. 1 4}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparing different system prompts in ChatGPT on MATH 401. Cal means no system prompt. * $=$ "You are an accurate calculator." $4=$ "Calculating to four decimal places."</p>
<h3>4.5 Interpolation and Extrapolation</h3>
<p>LLMs have strong abilities to fit on in-domain data. If pretraining corpora contain arithmetic expressions, it is easy for LLMs to memorize them. For out-of-domain data, LLMs need to extrapolate how to calculate them. We do not know what are indomain data and out-of-domain data for models (especially ChatGPT), so it is hard to test their interpolation and extrapolation abilities. We use the easy group and the hard group to estimate the interpolation and extrapolation abilities. The easy group queries have possibilities that appear in the pretraining corpora or instruct-tuning, while the hard group queries contain big numbers / decimal
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performances of MATH 401 on LLMs with different sizes. We do not know the parameter count of ChatGPT. We list InstructGPT results with SFT setting (text-davinci-002) only for a fair comparison.
numbers / long expressions which are very unlikely to be covered by pretraining corpora or instructions. Thus answering easy queries may examine the interpolation ability of models and answering hard queries must examine the extrapolation ability of the models. We find ChatGPT performs best on hard queries, and all other models have limited performance on hard queries which show limited extrapolation.</p>
<h3>4.6 Scaling Laws</h3>
<p>To understand how parameter counts influence arithmetic ability, we plot the results with differentsize LLMs in Figure 1. We do not plot text-davinci003, gpt-3.5-turbo-0301 and gpt-4 since they do not have smaller versions with the same setting. We find that LLMs have better abilities with larger parameter counts. An interesting phenomenon we found is model over 30B does not improve significantly compared with 30B models, especially in Galactica where the 120B model performs the same as the 30B model. We hypothesize that 30B may be enough for arithmetic ability. ChatGPT may be a model smaller than 175B which outperforms other 175B models a lot, thus larger parameter count does not guarantee better arithmetic ability. For GPT-4, we cannot have any possible guess. Considering its much slower inference speed, we guess it has larger</p>
<table>
<thead>
<tr>
<th>Expression</th>
<th>Calculate:</th>
</tr>
</thead>
<tbody>
<tr>
<td>$(88-42) <em>(33</em>$</td>
<td>First, we need to simplify the expression inside the</td>
</tr>
<tr>
<td>$41)-(13-27)=$</td>
<td>parentheses: $88-42=4633 * 41=135313-27=$</td>
</tr>
<tr>
<td>62252</td>
<td>-14 Now we can substitute these values into the original expression: (46) * (1353) - (-14) Multiplying 46 and 1353 gives: 62,238 Remember that subtracting a negative number is the same as adding a positive number, so we can rewrite the expression as: 62,238 + 14 Adding 14 gives: 62,252 Therefore, the answer is: 62,252</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Zero-shot COT</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>First, we simplify the parentheses: (88 - 42) * (33 *</td>
<td></td>
</tr>
<tr>
<td>41) - (13 - 27) = 46 * 1353 - (-14) Next, we simplify</td>
<td></td>
</tr>
<tr>
<td>the second set of parentheses by changing the sub-</td>
<td></td>
</tr>
<tr>
<td>traction into addition of the opposite: = 46 * 1353 +</td>
<td></td>
</tr>
<tr>
<td>14 We can then multiply 46 and 1353: = 62338 + 14</td>
<td></td>
</tr>
<tr>
<td>Finally, we add: = 62352 Therefore, (88 - 42) * (33</td>
<td></td>
</tr>
<tr>
<td>* 41) - (13 - 27) = 62352.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 6: Example of Group 14 decoded by ChatGPT with Calculate and COT prompts. parameter counts than ChatGPT and obtain better reasoning ability (i.e. long arithmetic expression).</p>
<h3>4.7 Chain-of-Thought</h3>
<p>LLMs can leverage chain-of-thought to better answer math word problems [wei2022chain]. We test on ChatGPT whether chain-of-thought will improve arithmetic calculations. We use the prompt "Let us solve this equation step by step" to instruct ChatGPT for zero-shot COT [kojima2022coot]. We compare the results of zero-shot COT using "Calculate:" in Table 7. Surprisingly, we find that COT does not improve the performance of any group even in group 14 with long arithmetic expressions. To understand the reason for this phenomenon, we check decoded results for these two prompts in Table 6. We find using "Calculate:" as the prompt can automatically generate chain-of-thoughts for long arithmetic expressions and generate answers directly for easy questions.</p>
<table>
<thead>
<tr>
<th>Group</th>
<th>Cal</th>
<th></th>
<th>0 COT</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc</td>
<td>RE</td>
<td>Acc</td>
<td>RE</td>
</tr>
<tr>
<td>0 Euler</td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{. 0 0}$</td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{. 0 0}$</td>
</tr>
<tr>
<td>$1 \sim 6+-$</td>
<td>$\mathbf{9 7}$</td>
<td>$\mathbf{. 0 0}$</td>
<td>94</td>
<td>.02</td>
</tr>
<tr>
<td>$7 \sim 10 \times \div$</td>
<td>$\mathbf{6 9}$</td>
<td>$\mathbf{. 2 0}$</td>
<td>61</td>
<td>.66</td>
</tr>
<tr>
<td>$11 \sim 12 \wedge$</td>
<td>$\mathbf{5 0}$</td>
<td>$\mathbf{. 2 4}$</td>
<td>48</td>
<td>.56</td>
</tr>
<tr>
<td>13 Irr.</td>
<td>$\mathbf{6 4}$</td>
<td>$\mathbf{1 . 7 3}$</td>
<td>28</td>
<td>4.89</td>
</tr>
<tr>
<td>14 Long</td>
<td>$\mathbf{6 8}$</td>
<td>$\mathbf{. 1 9}$</td>
<td>64</td>
<td>.46</td>
</tr>
<tr>
<td>15 Tri.</td>
<td>$\mathbf{4 4}$</td>
<td>1.21</td>
<td>40</td>
<td>$\mathbf{1 . 1 4}$</td>
</tr>
<tr>
<td>16 Log</td>
<td>$\mathbf{5 6}$</td>
<td>$\mathbf{. 8 0}$</td>
<td>28</td>
<td>5.37</td>
</tr>
<tr>
<td>Overall</td>
<td>$\mathbf{7 4}$</td>
<td>$\mathbf{. 3 3}$</td>
<td>66</td>
<td>.98</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparing zero-shot COT and Calculate using ChatGPT on MATH 401.</p>
<h3>4.8 In-context Learning</h3>
<p>In-context learning (ICL) provides related questionanswer pairs to improve LLMs [brown2020gpt; wei2022chain]. In our task, we can provide similar arithmetic expressions before the queries to help model understanding the arithmetic operator as done in [smith2022coot]. We provide 8 similar cases (we promise these cases are different from</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Naive</th>
<th></th>
<th>ICL</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc</td>
<td>RE</td>
<td>Acc</td>
<td>RE</td>
</tr>
<tr>
<td>galactica-120b</td>
<td>45.14</td>
<td>1.3</td>
<td>45.14</td>
<td>0.42</td>
</tr>
<tr>
<td>galactica-6.7b</td>
<td>34.41</td>
<td>2.61</td>
<td>32.67</td>
<td>0.65</td>
</tr>
<tr>
<td>flan-t5-xxl</td>
<td>3.74</td>
<td>5.78</td>
<td>0.0</td>
<td>10.0</td>
</tr>
<tr>
<td>flan-t5-base</td>
<td>2.49</td>
<td>3.18</td>
<td>0.0</td>
<td>10.0</td>
</tr>
</tbody>
</table>
<p>Table 8: In-context learning on MATH 401.
the query) for each query. We test whether ICL can improve the well-behaved model (Galactica) and the underperforming model (Flan-T5). For Galactica, it does not improve accuracy but reduces relative error significantly. For small-sized Flan (smaller than 3B) it cannot generate any number under the setting of in-context-learning.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we propose MATH 401 to evaluate the arithmetic ability of LLMs. We find that tokenization, pre-training corpus, prompts, and model parameter counts are important for their arithmetic ability. The reason ChatGPT performs so well in arithmetic still has some mystery, i.e. the parameter counts and instruction datasets of ChatGPT. We hope this paper can help readers improve LLMs with better arithmetic ability. This paper is only focused on arithmetic, testing LLMs on other math topics including symbolic mathematics, solving (ordinary differential, partial differential) equations, calculus, algebra, geometry, probability theory, and graph theory are also interesting topics.</p>
<h2>References</h2>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An opensource autoregressive language model. In Proceedings of the ACL Workshop on Challenges \&amp; Perspectives in Creating Large Language Models.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv, abs/2107.03374.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and J J Berner. 2023. Mathematical capabilities of chatgpt. ArXiv, abs/2301.13867.</p>
<p>Vedant Gaur and Nikunj Saunshi. 2022. Symbolic math reasoning with language models. In 2022 IEEE MIT Undergraduate Research Technology Conference (URTC), pages 1-5.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Jeonghwan Kim, Giwon Hong, Kyung min Kim, Junmo Kang, and Sung-Hyon Myaeng. 2021. Have
you seen that number? investigating extrapolation in question answering models. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning.</p>
<p>Matteo Muffo, Aldo Cocco, and Enrico Bertino. 2022. Evaluating transformer language models on arithmetic operations using number decomposition. In International Conference on Language Resources and Evaluation.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy J. Li. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. ArXiv, abs/2102.13019.</p>
<p>Kimia Noorbakhsh, Modar Sulaiman, Mahdi Sharifi, Kallol Roy, and Pooyan Jamshidi. 2021. Pretrained language models are symbolic mathematics solvers too! arXiv preprint arXiv:2110.03501.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. ArXiv, abs/2009.03393.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. ArXiv, abs/2202.07206.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761.</p>
<p>Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. 2023. An independent evaluation of chatgpt on mathematical word problems (mwp).</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2022. Language models are multilingual chain-of-thought reasoners. ArXiv, abs/2210.03057.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Anand Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. ArXiv, abs/2201.11990.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz Søraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Díaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin HoffmanJohn, Josh Lee, Lora Aroyo, Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray</p>
<p>Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Huai hsin Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. ArXiv, abs/2201.08239.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Cunxiang Wang, Boyuan Zheng, Yuchen Niu, and Yue Zhang. 2021. Exploring generalization ability of pretrained language models on arithmetic and logical reasoning. In Natural Language Processing and Chinese Computing.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. 2022. Autoformalization with large language models. ArXiv, abs/2205.12615.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. Glm130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. ArXiv, abs/2205.10625.</p>
<h2>A Examples from MATH 401</h2>
<p>We list examples for each group from MATH 401.</p>
<ul>
<li>
<p>$e^{i \pi}+1=0$</p>
</li>
<li>
<p>$5+9=14$</p>
</li>
<li>$21+97=118$</li>
<li>$721-847=-126$</li>
<li>$\begin{aligned} &amp; 714637232158-667119914538 \ &amp; 47517317620\end{aligned}=$ $47517317620$</li>
<li>$-1+(-6)=-7$</li>
<li>$-0.038+0.0092=-0.0288$</li>
<li>$78 \times 64=4992$</li>
<li>$5.0 \times 0.09=0.045$</li>
<li>$45960 \times 59693=2743490280$</li>
<li>$70 \div 61=1.1475$</li>
<li>$7^{4}=2401$</li>
<li>$2.242^{3.7342}=20.3865$</li>
<li>$e+\pi=5.8598$</li>
<li>$(4 \times 64) \times(39+12)=13056$</li>
<li>$\sin (-3.75 \pi)=0.7071$</li>
<li>$\log _{10}(797)=2.9015$</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ OpenAI states they improve the math of ChatGPT since version Jan 30, and we cannot evaluate any previous version.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://openai.com/blog/
introducing-chatgpt-and-whisper-apis&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
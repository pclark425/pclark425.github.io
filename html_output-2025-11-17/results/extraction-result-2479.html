<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2479 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2479</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2479</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-231717724</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2106.01309v1.pdf" target="_blank">Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains</a></p>
                <p><strong>Paper Abstract:</strong> In the field of machine learning (ML) for materials optimization, active learning algorithms, such as Bayesian Optimization (BO), have been leveraged for guiding autonomous and high-throughput experimentation systems. However, very few studies have evaluated the efficiency of BO as a general optimization algorithm across a broad range of experimental materials science domains. In this work, we evaluate the performance of BO algorithms with a collection of surrogate model and acquisition function pairs across five diverse experimental materials systems, namely carbon nanotube polymer blends, silver nanoparticles, lead-halide perovskites, as well as additively manufactured polymer structures and shapes. By defining acceleration and enhancement metrics for general materials optimization objectives, we find that for surrogate model selection, Gaussian Process (GP) with anisotropic kernels (automatic relevance detection, ARD) and Random Forests (RF) have comparable performance and both outperform the commonly used GP without ARD. We discuss the implicit distributional assumptions of RF and GP, and the benefits of using GP with anisotropic kernels in detail. We provide practical insights for experimentalists on surrogate model selection of BO during materials optimization campaigns.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2479.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2479.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pool-based BO benchmarking framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pool-based active-learning benchmarking framework for Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation framework used in this paper that models closed-loop materials optimization as pool-based active learning: at each cycle a surrogate model estimates a posterior (mean and std) over a discrete pool of candidate experiments and an acquisition function selects the next experiment; performance is evaluated with Top%, EF and AF metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pool-based active-learning Bayesian optimization framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative closed-loop optimization emulating autonomous experimental campaigns. Starting from a small set of initial experiments (n=2 in most runs), at each cycle the surrogate model (GP, GP+ARD, or RF) is trained on collected data X; the model predicts mean μ(x) and standard deviation σ(x) for every remaining candidate x in a discrete pool D; an acquisition function α(μ,σ) is evaluated for all x∈D and the argmax is selected as the next experiment; the selected sample is revealed (from ground-truth pool) and added to X; the loop repeats until budget exhaustion or pool depletion. Implementation: single-sample-batch updates (batch size = 1), 50 ensemble runs per algorithm, datasets from five experimental materials domains provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science experimental optimization (autonomous/high-throughput materials discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Greedy sequential allocation: at each cycle, select the single candidate x that maximizes the chosen acquisition function α(μ(x),σ(x)) evaluated across the full pool D. Acquisition functions used include LCB_λ (−λμ+σ), Expected Improvement (EI), and Probability of Improvement (PI); LCB with λ=1 (equal exploration/exploitation) is the main comparator. Initial experiments chosen randomly (or via LHS if recommended) to seed the surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured wall-clock running time ratios across surrogate models (empirical ratio t_RF : t_GP : t_GP_ARD = 1 : 1.32 : 1.54) and discussed asymptotic complexities: RF training O(n log n · n_dim · n_tree) vs GP training O(n^3 + n^2 · n_dim) (n = number of training points). Also discussion of absolute retraining latency relative to experiment synthesis time and implications for feedback loop speed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition-function driven expected utility: EI directly uses expected improvement (closed-form using μ,σ); LCB uses a linear trade-off of predicted mean and uncertainty (−λμ+σ); PI uses probability of surpassing current best. Information gain is thus operationalized via these acquisition functions (expected improvement or uncertainty-weighted utility), not via mutual information or explicit information-theoretic objectives in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via acquisition functions. LCB_λ parameter λ trades exploitation (minimize predicted mean) and exploration (maximize predictive std); LCB_1 (λ=1) found to balance exploration and exploitation well and outperform overly exploratory variants. EI tends to be greedier (can over-exploit), PI focuses only on probability of improvement (ignores magnitude) and performed worse here.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No dedicated diversity-promoting acquisition (e.g., determinantal sampling) was implemented; diversity considerations are encoded in the optimization objective and evaluation metrics: the Top% metric rewards discovering multiple top-performing candidates (top 5%), and initial-design strategies (recommendation of Latin Hypercube Sampling for initial experiments) are used to better cover the space and avoid extrapolation. Jaccard similarity between sample sequences is used to measure diversity of search paths between algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments budget (number of cycles / samples). Discussion also considers alternative budget accounting by total experimental time (wall-clock) and computational training time as implicit resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Benchmarks are run under fixed-sample budgets and compared via Top%(i) curves; acceleration and enhancement metrics (AF and EF) quantify how many fewer experimental cycles BO needs relative to random baseline to reach specific discovery targets; computational costs are discussed qualitatively/empirically (model training time ratios) and recommended to influence surrogate selection when experimental throughput causes model latency to matter.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Threshold-based: 'Top% (i)' is fraction of top-M candidates (M defined as top 5% of pool) discovered by cycle i; a practical success goal chosen is Top% = 0.8 (finding 80% of the top-5% candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Top%(i) (dimensionless fraction), Enhancement Factor EF(i)=Top%_BO(i)/Top%_random(i) (unitless), Acceleration Factor AF(a)=i_random / i_BO for reaching Top%=a (unitless). Reported empirical results: EF_max up to 16× on the crossed-barrel dataset; GP+ARD and RF reach Top%=0.8 at ~30 samples vs GP (isotropic) ≈90 samples on crossed-barrel (600 total points); other datasets show EF_max in ~2×–5× range.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random sampling baseline (statistical random search over the same pool). Also inter-algorithm comparisons across surrogate models (RF, GP isotropic, GP+ARD) and acquisition functions (LCB, EI, PI).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BO algorithms outperform random baseline in all datasets tested. Empirically EF (improvement factor over random) peaks up to 16× in best-case (crossed-barrel); for other datasets EF_max typically 2×–5×. GP+ARD and RF achieve similar or superior performance vs GP isotropic; LCB_1 outperforms EI and PI in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported up to 16× enhancement in Top% relative to random (EF_max); acceleration measured as AF up to similar orders depending on Top% target; e.g., GP+ARD and RF reached the Top%=0.8 target roughly 3× faster (30 vs 90 samples) than GP isotropic in the crossed-barrel case (i.e., ~3× reduction in experiments required).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Explicit comparisons performed between surrogate model predictive accuracy, sample-efficiency (information gain per experiment), and computational cost (training time). Key observations: RF trains faster and scales better with n (lower retraining latency) and is robust to noise and model misspecification; GP+ARD provides better feature relevance estimates and can outperform RF at later stages (after sufficient data) but is computationally heavier (O(n^3)). Choice of surrogate should consider experimental sample-generation rate vs model retraining time; LCB_λ tuning shifts the exploration/exploitation balance and strongly affects early vs late performance. Dataset collection methodology (grid sampling vs BO-guided collection) affects apparent EF/AF (intrinsic acceleration).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Practical recommendations: use GP with anisotropic kernels (ARD) or Random Forests as surrogate models (both outperform isotropic GP); use LCB with λ≈1 for a balanced exploration/exploitation trade-off; measure performance with Top% and aim for realistic goals like Top%=0.8 to assess early-stage value; when experimental throughput is high so that retraining latency matters, prefer RF for faster retraining and parallelizability; use Latin Hypercube Sampling for initial design to cover the space and reduce extrapolation risk; select surrogate and acquisition pair depending on available experiment budget and desired stage (RF may excel early/low budget, GP+ARD may excel later/high budget).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2479.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2479.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Acquisition functions (LCB / EI / PI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Acquisition functions used in Bayesian optimization: Lower Confidence Bound (LCB), Expected Improvement (EI), Probability of Improvement (PI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Acquisition functions that convert surrogate-model predictive mean and variance into an acquisition score used to allocate the next experimental resource; each encodes a different trade-off between exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Acquisition-function driven experiment allocation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three acquisition functions are evaluated: LCB_λ(x)=−λμ(x)+σ(x) (user-tunable λ controls trade-off), EI(x) = expected improvement relative to current best (closed-form using μ,σ and normal cdf/pdf), and PI(x)=probability of improvement. These functions are evaluated across the discrete pool and the argmax selects the next experiment. LCB with λ=1 (equal weight to μ and σ) was the principal comparator and performed best across datasets in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning / Bayesian optimization for experimental materials design</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Rank-allocation: compute α(x) for all candidate actions and select the one with maximum acquisition score at each cycle. LCB_λ explicitly balances exploitation and exploration via λ; EI trades expected value of possible improvement (magnitude-aware) whereas PI only considers probability of exceeding current best.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computational cost dominated by surrogate predictions (μ,σ) for all pool candidates; acquisition evaluation cost is marginal relative to surrogate retraining. No explicit FLOP counts given; wall-clock comparisons for end-to-end benchmarking are tied to surrogate model training times.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>EI implements expected improvement (an expected-utility variant); LCB uses a heuristic combining predicted objective and uncertainty as a proxy for expected utility; PI uses probability of improvement (a simpler, magnitude-agnostic utility).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Direct through acquisition design: LCB_λ tunable parameter λ, EI incorporates both mean and variance to estimate expected improvement thereby balancing exploration/exploitation, PI focuses on exploitation of high probability gains and can under-explore.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit: higher uncertainty terms (σ) in acquisition functions can promote sampling in uncertain regions, indirectly increasing hypothesis diversity; no explicit diversity-penalty or batch-diversification algorithm is used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments (per-cycle selection until budget), batch size = 1 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition functions are used until budget exhaustion; performance assessed per cycle via Top% and EF/AF metrics to quantify allocation efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Acquisition functions are evaluated by their ability to cause the system to find top-K candidates quickly (Top% for top 5% candidates); LCB_1 achieved superior practical discovery rates relative to EI and PI in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Top% vs cycle curves for each acquisition function paired with surrogate models; LCB_1 produced faster early identification of multiple top candidates and higher EF/AF than other acquisition choices in these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against random selection baseline and across acquisition alternatives EI and PI paired with each surrogate model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LCB_1 paired with GP+ARD or RF outperformed EI and PI and exceeded random baseline by EF>1; EI sometimes overly greedy and PI underperformed relative to EI and LCB in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>LCB_1 contributed to the EF_max results (up to 16× in best case), by enabling balanced exploration/exploitation and reducing wasted experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper reports that acquisition function choice significantly affects early vs late-stage performance: overly exploratory LCB_λ (large emphasis on σ) underperforms; overly greedy EI can get stuck; PI tends to perform worse because it ignores magnitude of improvement. Thus tuning λ and selecting EI vs LCB should match experimental goals (finding many top candidates vs fastest single optimum).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: use LCB with balanced weighting (λ≈1) for tasks that aim to discover multiple high-performing candidates (Top% objective); EI may be too exploitative in some materials design spaces, and PI is less effective for magnitude-sensitive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2479.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2479.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarking & discovery metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top% (top-candidate discovery), Enhancement Factor (EF), Acceleration Factor (AF), and Jaccard path diversity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of metrics used to quantify efficiency and diversity of experimental resource allocation in BO-guided campaigns: Top% measures fraction of top-K candidates found; EF and AF quantify improvement and speedup relative to random; Jaccard similarity measures overlap/diversity of experiment sequences between methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Discovery and efficiency benchmarking metrics (Top%, EF, AF, Jaccard)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Top%(i) = (number of top-candidates discovered by cycle i) / (total number of top candidates M). EF(i)=Top%_BO(i)/Top%_random(i) measures multiplicative enhancement over random baseline at cycle i. AF(Top%=a)=i_random / i_BO compares cycle counts needed to reach a given Top% target a (acceleration). Jaccard similarity J(i) = |A(i)∩B(i)|/|A(i)∪B(i)| quantifies overlap between two algorithms' chosen sample sequences at cycle i, used to study diversity of search trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evaluation of active learning / BO performance in materials optimization</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Metrics are used to judge allocation strategies: prefer algorithms and acquisition parameters that maximize Top% early (i.e., maximize EF and AF) while maintaining diversity (lower Jaccard overlap between different surrogates may indicate complementary search paths).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Metrics are orthogonal to computational cost but are reported alongside measured training-time ratios; EF and AF are sample-efficiency metrics (unitless) not directly measuring compute.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Top%, EF, AF operationalize information/information gain as practical goal achievement per experiment (i.e., how quickly high-value information—top candidates—is discovered).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Metrics distinguish exploration vs exploitation performance indirectly: early rapid increase in Top% indicates effective exploitation of high-performing regions; continued increases at later cycles indicate exploration discovering remaining top candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Jaccard similarity is used as a diagnostic to quantify diversity and divergence between optimization paths; Top% objective (finding many top candidates) encourages selection strategies that avoid focusing on a single best candidate (thus encouraging diversity in practice).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-number-of-experiments comparisons used to compute EF/AF; also used to examine how performance varies with different allocated budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Performance curves (Top% vs cycle) and EF/AF curves are used to select algorithms appropriate for the available budget (e.g., RF may be superior under tight early budgets while GP+ARD may excel given larger budgets).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Breakthroughs operationalized as inclusion in the top 5% of pool objective values; Top% tracks fraction of these breakthrough candidates found over time. Practical target used: Top%=0.8 (finding 80% of top-5% set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Numerical examples: EF_max up to 16× on crossed-barrel dataset; GP+ARD and RF reached Top%=0.8 at ~30 samples (out of 600) while GP isotropic required ≈90 samples; other datasets showed EF_max ≈2–5×.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random sampling baseline; cross-algorithm comparison (different surrogate/acquisition pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>All BO configurations improved over random baseline (EF>1). EF and AF curves highlight stages where BO gains are large (early-to-mid learning cycles) and diminishing returns later.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Concrete reported gains: up to 16× enhancement in fraction of top candidates discovered at certain cycles; ~3× reduction in samples to reach Top%=0.8 when comparing GP+ARD/RF vs GP isotropic in the crossed-barrel dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Metrics reveal tradeoffs: algorithms that excel early (higher EF at small i) may be different than those best at achieving final Top% target; computationally cheaper surrogates (RF) can enable faster retraining and thus effective real-time allocation in high-throughput settings but may be overtaken by GP+ARD in information efficiency at larger budgets; dataset collection methodology (grid vs BO-guided) biases EF/AF estimates and must be accounted for.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use Top%/EF/AF curves to choose algorithm that matches experiment budget and objectives (e.g., if goal is to find many top candidates quickly, RF:LCB_1 is effective early; for fewer experiments but higher final yield GP+ARD:LCB_1 may be optimal). Employ Jaccard diagnostics to identify complementary algorithms whose combined runs could increase hypothesis diversity and robustness (backup candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Taking the human out of the loop: A review of bayesian optimization <em>(Rating: 2)</em></li>
                <li>A tutorial on bayesian optimization <em>(Rating: 2)</em></li>
                <li>Olympus: a benchmarking framework for noisy optimization and experiment planning <em>(Rating: 2)</em></li>
                <li>Benchmarking the acceleration of materials discovery by sequential learning <em>(Rating: 2)</em></li>
                <li>Entropy search for information-efficient global optimization <em>(Rating: 1)</em></li>
                <li>Self-driving laboratory for accelerated discovery of thin-film materials <em>(Rating: 1)</em></li>
                <li>A bayesian experimental autonomous researcher for mechanical design <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2479",
    "paper_id": "paper-231717724",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Pool-based BO benchmarking framework",
            "name_full": "Pool-based active-learning benchmarking framework for Bayesian Optimization",
            "brief_description": "A simulation framework used in this paper that models closed-loop materials optimization as pool-based active learning: at each cycle a surrogate model estimates a posterior (mean and std) over a discrete pool of candidate experiments and an acquisition function selects the next experiment; performance is evaluated with Top%, EF and AF metrics.",
            "citation_title": "BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS",
            "mention_or_use": "use",
            "system_name": "Pool-based active-learning Bayesian optimization framework",
            "system_description": "Iterative closed-loop optimization emulating autonomous experimental campaigns. Starting from a small set of initial experiments (n=2 in most runs), at each cycle the surrogate model (GP, GP+ARD, or RF) is trained on collected data X; the model predicts mean μ(x) and standard deviation σ(x) for every remaining candidate x in a discrete pool D; an acquisition function α(μ,σ) is evaluated for all x∈D and the argmax is selected as the next experiment; the selected sample is revealed (from ground-truth pool) and added to X; the loop repeats until budget exhaustion or pool depletion. Implementation: single-sample-batch updates (batch size = 1), 50 ensemble runs per algorithm, datasets from five experimental materials domains provided.",
            "application_domain": "Materials science experimental optimization (autonomous/high-throughput materials discovery)",
            "resource_allocation_strategy": "Greedy sequential allocation: at each cycle, select the single candidate x that maximizes the chosen acquisition function α(μ(x),σ(x)) evaluated across the full pool D. Acquisition functions used include LCB_λ (−λμ+σ), Expected Improvement (EI), and Probability of Improvement (PI); LCB with λ=1 (equal exploration/exploitation) is the main comparator. Initial experiments chosen randomly (or via LHS if recommended) to seed the surrogate.",
            "computational_cost_metric": "Measured wall-clock running time ratios across surrogate models (empirical ratio t_RF : t_GP : t_GP_ARD = 1 : 1.32 : 1.54) and discussed asymptotic complexities: RF training O(n log n · n_dim · n_tree) vs GP training O(n^3 + n^2 · n_dim) (n = number of training points). Also discussion of absolute retraining latency relative to experiment synthesis time and implications for feedback loop speed.",
            "information_gain_metric": "Acquisition-function driven expected utility: EI directly uses expected improvement (closed-form using μ,σ); LCB uses a linear trade-off of predicted mean and uncertainty (−λμ+σ); PI uses probability of surpassing current best. Information gain is thus operationalized via these acquisition functions (expected improvement or uncertainty-weighted utility), not via mutual information or explicit information-theoretic objectives in this study.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit via acquisition functions. LCB_λ parameter λ trades exploitation (minimize predicted mean) and exploration (maximize predictive std); LCB_1 (λ=1) found to balance exploration and exploitation well and outperform overly exploratory variants. EI tends to be greedier (can over-exploit), PI focuses only on probability of improvement (ignores magnitude) and performed worse here.",
            "diversity_mechanism": "No dedicated diversity-promoting acquisition (e.g., determinantal sampling) was implemented; diversity considerations are encoded in the optimization objective and evaluation metrics: the Top% metric rewards discovering multiple top-performing candidates (top 5%), and initial-design strategies (recommendation of Latin Hypercube Sampling for initial experiments) are used to better cover the space and avoid extrapolation. Jaccard similarity between sample sequences is used to measure diversity of search paths between algorithms.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-number-of-experiments budget (number of cycles / samples). Discussion also considers alternative budget accounting by total experimental time (wall-clock) and computational training time as implicit resource constraints.",
            "budget_constraint_handling": "Benchmarks are run under fixed-sample budgets and compared via Top%(i) curves; acceleration and enhancement metrics (AF and EF) quantify how many fewer experimental cycles BO needs relative to random baseline to reach specific discovery targets; computational costs are discussed qualitatively/empirically (model training time ratios) and recommended to influence surrogate selection when experimental throughput causes model latency to matter.",
            "breakthrough_discovery_metric": "Threshold-based: 'Top% (i)' is fraction of top-M candidates (M defined as top 5% of pool) discovered by cycle i; a practical success goal chosen is Top% = 0.8 (finding 80% of the top-5% candidates).",
            "performance_metrics": "Top%(i) (dimensionless fraction), Enhancement Factor EF(i)=Top%_BO(i)/Top%_random(i) (unitless), Acceleration Factor AF(a)=i_random / i_BO for reaching Top%=a (unitless). Reported empirical results: EF_max up to 16× on the crossed-barrel dataset; GP+ARD and RF reach Top%=0.8 at ~30 samples vs GP (isotropic) ≈90 samples on crossed-barrel (600 total points); other datasets show EF_max in ~2×–5× range.",
            "comparison_baseline": "Random sampling baseline (statistical random search over the same pool). Also inter-algorithm comparisons across surrogate models (RF, GP isotropic, GP+ARD) and acquisition functions (LCB, EI, PI).",
            "performance_vs_baseline": "BO algorithms outperform random baseline in all datasets tested. Empirically EF (improvement factor over random) peaks up to 16× in best-case (crossed-barrel); for other datasets EF_max typically 2×–5×. GP+ARD and RF achieve similar or superior performance vs GP isotropic; LCB_1 outperforms EI and PI in these experiments.",
            "efficiency_gain": "Reported up to 16× enhancement in Top% relative to random (EF_max); acceleration measured as AF up to similar orders depending on Top% target; e.g., GP+ARD and RF reached the Top%=0.8 target roughly 3× faster (30 vs 90 samples) than GP isotropic in the crossed-barrel case (i.e., ~3× reduction in experiments required).",
            "tradeoff_analysis": "Explicit comparisons performed between surrogate model predictive accuracy, sample-efficiency (information gain per experiment), and computational cost (training time). Key observations: RF trains faster and scales better with n (lower retraining latency) and is robust to noise and model misspecification; GP+ARD provides better feature relevance estimates and can outperform RF at later stages (after sufficient data) but is computationally heavier (O(n^3)). Choice of surrogate should consider experimental sample-generation rate vs model retraining time; LCB_λ tuning shifts the exploration/exploitation balance and strongly affects early vs late performance. Dataset collection methodology (grid sampling vs BO-guided collection) affects apparent EF/AF (intrinsic acceleration).",
            "optimal_allocation_findings": "Practical recommendations: use GP with anisotropic kernels (ARD) or Random Forests as surrogate models (both outperform isotropic GP); use LCB with λ≈1 for a balanced exploration/exploitation trade-off; measure performance with Top% and aim for realistic goals like Top%=0.8 to assess early-stage value; when experimental throughput is high so that retraining latency matters, prefer RF for faster retraining and parallelizability; use Latin Hypercube Sampling for initial design to cover the space and reduce extrapolation risk; select surrogate and acquisition pair depending on available experiment budget and desired stage (RF may excel early/low budget, GP+ARD may excel later/high budget).",
            "uuid": "e2479.0",
            "source_info": {
                "paper_title": "Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Acquisition functions (LCB / EI / PI)",
            "name_full": "Acquisition functions used in Bayesian optimization: Lower Confidence Bound (LCB), Expected Improvement (EI), Probability of Improvement (PI)",
            "brief_description": "Acquisition functions that convert surrogate-model predictive mean and variance into an acquisition score used to allocate the next experimental resource; each encodes a different trade-off between exploration and exploitation.",
            "citation_title": "BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS",
            "mention_or_use": "use",
            "system_name": "Acquisition-function driven experiment allocation",
            "system_description": "Three acquisition functions are evaluated: LCB_λ(x)=−λμ(x)+σ(x) (user-tunable λ controls trade-off), EI(x) = expected improvement relative to current best (closed-form using μ,σ and normal cdf/pdf), and PI(x)=probability of improvement. These functions are evaluated across the discrete pool and the argmax selects the next experiment. LCB with λ=1 (equal weight to μ and σ) was the principal comparator and performed best across datasets in this study.",
            "application_domain": "Active learning / Bayesian optimization for experimental materials design",
            "resource_allocation_strategy": "Rank-allocation: compute α(x) for all candidate actions and select the one with maximum acquisition score at each cycle. LCB_λ explicitly balances exploitation and exploration via λ; EI trades expected value of possible improvement (magnitude-aware) whereas PI only considers probability of exceeding current best.",
            "computational_cost_metric": "Computational cost dominated by surrogate predictions (μ,σ) for all pool candidates; acquisition evaluation cost is marginal relative to surrogate retraining. No explicit FLOP counts given; wall-clock comparisons for end-to-end benchmarking are tied to surrogate model training times.",
            "information_gain_metric": "EI implements expected improvement (an expected-utility variant); LCB uses a heuristic combining predicted objective and uncertainty as a proxy for expected utility; PI uses probability of improvement (a simpler, magnitude-agnostic utility).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Direct through acquisition design: LCB_λ tunable parameter λ, EI incorporates both mean and variance to estimate expected improvement thereby balancing exploration/exploitation, PI focuses on exploitation of high probability gains and can under-explore.",
            "diversity_mechanism": "Implicit: higher uncertainty terms (σ) in acquisition functions can promote sampling in uncertain regions, indirectly increasing hypothesis diversity; no explicit diversity-penalty or batch-diversification algorithm is used.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed-number-of-experiments (per-cycle selection until budget), batch size = 1 in experiments",
            "budget_constraint_handling": "Acquisition functions are used until budget exhaustion; performance assessed per cycle via Top% and EF/AF metrics to quantify allocation efficiency.",
            "breakthrough_discovery_metric": "Acquisition functions are evaluated by their ability to cause the system to find top-K candidates quickly (Top% for top 5% candidates); LCB_1 achieved superior practical discovery rates relative to EI and PI in this study.",
            "performance_metrics": "Top% vs cycle curves for each acquisition function paired with surrogate models; LCB_1 produced faster early identification of multiple top candidates and higher EF/AF than other acquisition choices in these datasets.",
            "comparison_baseline": "Compared against random selection baseline and across acquisition alternatives EI and PI paired with each surrogate model.",
            "performance_vs_baseline": "LCB_1 paired with GP+ARD or RF outperformed EI and PI and exceeded random baseline by EF&gt;1; EI sometimes overly greedy and PI underperformed relative to EI and LCB in these experiments.",
            "efficiency_gain": "LCB_1 contributed to the EF_max results (up to 16× in best case), by enabling balanced exploration/exploitation and reducing wasted experiments.",
            "tradeoff_analysis": "Paper reports that acquisition function choice significantly affects early vs late-stage performance: overly exploratory LCB_λ (large emphasis on σ) underperforms; overly greedy EI can get stuck; PI tends to perform worse because it ignores magnitude of improvement. Thus tuning λ and selecting EI vs LCB should match experimental goals (finding many top candidates vs fastest single optimum).",
            "optimal_allocation_findings": "Recommendation: use LCB with balanced weighting (λ≈1) for tasks that aim to discover multiple high-performing candidates (Top% objective); EI may be too exploitative in some materials design spaces, and PI is less effective for magnitude-sensitive objectives.",
            "uuid": "e2479.1",
            "source_info": {
                "paper_title": "Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Benchmarking & discovery metrics",
            "name_full": "Top% (top-candidate discovery), Enhancement Factor (EF), Acceleration Factor (AF), and Jaccard path diversity",
            "brief_description": "A set of metrics used to quantify efficiency and diversity of experimental resource allocation in BO-guided campaigns: Top% measures fraction of top-K candidates found; EF and AF quantify improvement and speedup relative to random; Jaccard similarity measures overlap/diversity of experiment sequences between methods.",
            "citation_title": "BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS",
            "mention_or_use": "use",
            "system_name": "Discovery and efficiency benchmarking metrics (Top%, EF, AF, Jaccard)",
            "system_description": "Top%(i) = (number of top-candidates discovered by cycle i) / (total number of top candidates M). EF(i)=Top%_BO(i)/Top%_random(i) measures multiplicative enhancement over random baseline at cycle i. AF(Top%=a)=i_random / i_BO compares cycle counts needed to reach a given Top% target a (acceleration). Jaccard similarity J(i) = |A(i)∩B(i)|/|A(i)∪B(i)| quantifies overlap between two algorithms' chosen sample sequences at cycle i, used to study diversity of search trajectories.",
            "application_domain": "Evaluation of active learning / BO performance in materials optimization",
            "resource_allocation_strategy": "Metrics are used to judge allocation strategies: prefer algorithms and acquisition parameters that maximize Top% early (i.e., maximize EF and AF) while maintaining diversity (lower Jaccard overlap between different surrogates may indicate complementary search paths).",
            "computational_cost_metric": "Metrics are orthogonal to computational cost but are reported alongside measured training-time ratios; EF and AF are sample-efficiency metrics (unitless) not directly measuring compute.",
            "information_gain_metric": "Top%, EF, AF operationalize information/information gain as practical goal achievement per experiment (i.e., how quickly high-value information—top candidates—is discovered).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Metrics distinguish exploration vs exploitation performance indirectly: early rapid increase in Top% indicates effective exploitation of high-performing regions; continued increases at later cycles indicate exploration discovering remaining top candidates.",
            "diversity_mechanism": "Jaccard similarity is used as a diagnostic to quantify diversity and divergence between optimization paths; Top% objective (finding many top candidates) encourages selection strategies that avoid focusing on a single best candidate (thus encouraging diversity in practice).",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-number-of-experiments comparisons used to compute EF/AF; also used to examine how performance varies with different allocated budgets.",
            "budget_constraint_handling": "Performance curves (Top% vs cycle) and EF/AF curves are used to select algorithms appropriate for the available budget (e.g., RF may be superior under tight early budgets while GP+ARD may excel given larger budgets).",
            "breakthrough_discovery_metric": "Breakthroughs operationalized as inclusion in the top 5% of pool objective values; Top% tracks fraction of these breakthrough candidates found over time. Practical target used: Top%=0.8 (finding 80% of top-5% set).",
            "performance_metrics": "Numerical examples: EF_max up to 16× on crossed-barrel dataset; GP+ARD and RF reached Top%=0.8 at ~30 samples (out of 600) while GP isotropic required ≈90 samples; other datasets showed EF_max ≈2–5×.",
            "comparison_baseline": "Random sampling baseline; cross-algorithm comparison (different surrogate/acquisition pairs).",
            "performance_vs_baseline": "All BO configurations improved over random baseline (EF&gt;1). EF and AF curves highlight stages where BO gains are large (early-to-mid learning cycles) and diminishing returns later.",
            "efficiency_gain": "Concrete reported gains: up to 16× enhancement in fraction of top candidates discovered at certain cycles; ~3× reduction in samples to reach Top%=0.8 when comparing GP+ARD/RF vs GP isotropic in the crossed-barrel dataset.",
            "tradeoff_analysis": "Metrics reveal tradeoffs: algorithms that excel early (higher EF at small i) may be different than those best at achieving final Top% target; computationally cheaper surrogates (RF) can enable faster retraining and thus effective real-time allocation in high-throughput settings but may be overtaken by GP+ARD in information efficiency at larger budgets; dataset collection methodology (grid vs BO-guided) biases EF/AF estimates and must be accounted for.",
            "optimal_allocation_findings": "Use Top%/EF/AF curves to choose algorithm that matches experiment budget and objectives (e.g., if goal is to find many top candidates quickly, RF:LCB_1 is effective early; for fewer experiments but higher final yield GP+ARD:LCB_1 may be optimal). Employ Jaccard diagnostics to identify complementary algorithms whose combined runs could increase hypothesis diversity and robustness (backup candidates).",
            "uuid": "e2479.2",
            "source_info": {
                "paper_title": "Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Taking the human out of the loop: A review of bayesian optimization",
            "rating": 2,
            "sanitized_title": "taking_the_human_out_of_the_loop_a_review_of_bayesian_optimization"
        },
        {
            "paper_title": "A tutorial on bayesian optimization",
            "rating": 2,
            "sanitized_title": "a_tutorial_on_bayesian_optimization"
        },
        {
            "paper_title": "Olympus: a benchmarking framework for noisy optimization and experiment planning",
            "rating": 2,
            "sanitized_title": "olympus_a_benchmarking_framework_for_noisy_optimization_and_experiment_planning"
        },
        {
            "paper_title": "Benchmarking the acceleration of materials discovery by sequential learning",
            "rating": 2,
            "sanitized_title": "benchmarking_the_acceleration_of_materials_discovery_by_sequential_learning"
        },
        {
            "paper_title": "Entropy search for information-efficient global optimization",
            "rating": 1,
            "sanitized_title": "entropy_search_for_informationefficient_global_optimization"
        },
        {
            "paper_title": "Self-driving laboratory for accelerated discovery of thin-film materials",
            "rating": 1,
            "sanitized_title": "selfdriving_laboratory_for_accelerated_discovery_of_thinfilm_materials"
        },
        {
            "paper_title": "A bayesian experimental autonomous researcher for mechanical design",
            "rating": 1,
            "sanitized_title": "a_bayesian_experimental_autonomous_researcher_for_mechanical_design"
        }
    ],
    "cost": 0.0163395,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS
June 3, 2021</p>
<p>Qiaohao Liang 
Department of Materials Science
Massachusetts Institute of Technology</p>
<p>Aldair E Gongora 
Department of Mechanical Engineering
Boston University</p>
<p>Zekun Ren 
Singapore-MIT Alliance for Research and Technology</p>
<p>Armi Tiihonen 
Department of Mechanical Engineering
Massachusetts Institute of Technology</p>
<p>Zhe Liu 
Department of Mechanical Engineering
Massachusetts Institute of Technology</p>
<p>Shijing Sun 
Department of Mechanical Engineering
Massachusetts Institute of Technology</p>
<p>James R Deneault 
Air Force Research Laboratory</p>
<p>Daniil Bash 
Agency for Science, Technology and Research (A*STAR)</p>
<p>Flore Mekki-Berrada 
National University of Singapore</p>
<p>Saif A Khan 
National University of Singapore</p>
<p>Kedar Hippalgaonkar 
Agency for Science, Technology and Research (A*STAR)</p>
<p>Benji Maruyama 
Air Force Research Laboratory</p>
<p>Keith A Brown 
Department of Mechanical Engineering
Boston University</p>
<p>John Fisher Iii 
Computer Science &amp; Artificial Intelligence Lab
Massachusetts Institute of Technology</p>
<p>Tonio Buonassisi 
Department of Mechanical Engineering
Massachusetts Institute of Technology</p>
<p>BENCHMARKING THE PERFORMANCE OF BAYESIAN OPTIMIZATION ACROSS MULTIPLE EXPERIMENTAL MATERIALS SCIENCE DOMAINS
June 3, 2021
In the field of machine learning (ML) for materials optimization, active learning algorithms, such as Bayesian Optimization (BO), have been leveraged for guiding autonomous and high-throughput experimentation systems. However, very few studies have evaluated the efficiency of BO as a general optimization algorithm across a broad range of experimental materials science domains. In this work, we evaluate the performance of BO algorithms with a collection of surrogate model and acquisition function pairs across five diverse experimental materials systems, namely carbon nanotube polymer blends, silver nanoparticles, lead-halide perovskites, as well as additively manufactured polymer structures and shapes. By defining acceleration and enhancement metrics for general materials optimization objectives, we find that for surrogate model selection, Gaussian Process (GP) with anisotropic kernels (automatic relevance detection, ARD) and Random Forests (RF) have comparable performance and both outperform the commonly used GP without ARD. We discuss the implicit arXiv:2106.01309v1 [cond-mat.mtrl-sci]  23 May 2021 A PREPRINT -JUNE 3, 2021 distributional assumptions of RF and GP, and the benefits of using GP with anisotropic kernels in detail. We provide practical insights for experimentalists on surrogate model selection of BO during materials optimization campaigns.</p>
<p>Introduction</p>
<p>Autonomous experimental systems have recently emerged as the new frontier for accelerated materials research.</p>
<p>These systems excel at optimizing materials objectives, e.g. environmental stability of solar cells or toughness of 3D printed mechanical structures, that are typically costly, slow, or difficult to simulate and experimentally evaluate. While autonomous experimental systems are often associated with high sample synthesis rates via high-throughput experiments (HTE), they may also utilize closed-loop feedback from machine learning (ML) during materials property optimization.</p>
<p>The latter has motivated integration of advanced lab automation components with ML algorithms. Specifically, active learning [1,2] algorithms have traditionally been applied to minimizing total experiment costs while maximizing machine learning model accuracy through hyperparameter tuning. Their primary utility for materials science research, where experiments remain relatively costly, lies in an iterative formulation that proposes targeted experiments with regard to a specific design objective based on prior experimental observations. Bayesian optimization (BO) [3,4,5], one class of active-learning learning methods, utilizes surrogate model to approximate a mapping from experiment parameters to an objective criterion, and provides optimal experiment selection when combined with an acquisition function. BO has been shown to be a data-efficient closed-loop active learning method for navigating complex design spaces [3,6,7,8,9,10]. Consequently, it has become an appealing methodology for accelerated materials research and optimizing material properties [11,12,13,14,15,16,17,18,19,20,21,22] beyond state-of-the-art.</p>
<p>The materials science community has seen successful demonstrations in performing materials optimization via autonomous experiments guided by BO and its variants [17,23,24,25,26,27]. Naturally, previous work emphasized the ability to achieve materials optimization with fewer experimental iterations. There have been very few quantitative analyses of the acceleration or enhancement resulting from applying BO algorithms and discussions on sensitivity of BO performance to surrogate model and acquisition function selection. Rohr et al. [28], Graff et al. [29] and Gongora et al. [24] have evaluated the performance of BO using multiple surrogate models and acquisition functions within specific electrocatalyst, ligand and mechanical structure design spaces respectively. However, comprehensive benchmarking of the performance of BO algorithms across a broad array of experimental materials systems, as we present here, has not been done. Although one could test BO across various analytical functions or emulated materials design spaces [30,25], empirical performance evaluation on a broader collection of experimental materials science data is still necessary to provide practical guidelines. Optimization algorithms need systematic and comprehensive benchmarks to evaluate their performance, and lack of these could significantly slow down advanced algorithm development, eventually posing obstacles for building full autonomous platforms. Presented below, the benchmarking framework, practical performance metrics, datasets collected from realistic noisy experiments, and insights derived from side-by-side comparison of BO algorithms will allow researchers to evaluate and select their optimization algorithm before deploying it on autonomous research platforms. Our work provides comprehensive benchmarks for optimization algorithms specifically developed for autonomous and high-throughput experimental materials research. Ideally, it provides insight for designing and deploying Bayesian optimization algorithms that suit the sample generation rate of future autonomous platforms and tackle materials optimization in more complex design spaces.</p>
<p>In this work, we benchmark the performance of BO across five different experimental materials science datasets, optimizing properties of carbon nanotube polymer blends, silver nanoparticles, lead-halide perovskites, and additively manufactured polymer structures and shapes. We utilize a pool-based active learning framework to approximate experimental materials optimization processes. We also adapt metrics such as enhancement factor and acceleration factor to quantitatively compare performances of BO algorithms against that of a random sampling baseline. We observe that when paired with the same acquisition functions, Random Forest (RF) [31,32,33] as a surrogate model can compete with Gaussian Process (GP) [4] with automatic relevance detection (ARD) [34] that has anisotropic kernels. They also both outperform commonly used GP without ARD. Our discussion on the differences in the implicit distributional assumptions of surrogate models and the benefits of using GP with anisotropic kernels yield deeper insights regarding surrogate model selection for materials optimization campaigns. These discussions suggest guidelines on using BO for general materials optimization. We also offer open source implementation of benchmarking code and datasets to support future development of such algorithms in the field.</p>
<p>Results</p>
<p>Experimental materials datasets</p>
<p>As seen in Table 1, we have assembled a list of five materials datasets with varying sizes, dimensions n dim , and materials systems. These diverse datasets are generated from autonomous experimental studies of collaborators, and facilitate BO performance analysis across a broad range of materials. They contain three to five independent input features, one property as materials optimization objective, and contain from a few tens to hundreds of data points. Based on their optimization objectives, the design space input features in the datasets range from materials compositions to synthesis processing parameters, as seen in Table T1 -T5 in supplementary information (SI).  [36] Silver nanoparticles Flow synthesis 164 5 Absorbance spectrum score Perovskite [23] Thin film perovskite Spin coating 94 3 Stability score Crossed barrel [24] 3D printed structure 3D printing 600 4 Mechanical toughness AutoAM [37] Materials manufacturing 3D printing 100 4 Shape score</p>
<p>The datasets were processed for comparison purposes:</p>
<ol>
<li>For each each dataset, its optimization objective values are independently centered to its mean and scaled to unit variance.</li>
</ol>
<p>3 A PREPRINT -JUNE 3, 2021 2. For each each dataset, its optimization problems is formulated as global minimization for consistency.</p>
<p>It should be noted that while all datasets were gathered from relatively high-throughput experimental systems, P3HT/CNT, AgNP, Perovskite, and AutoAM had BO guiding the selection of subsequent experiments partially through the materials optimization campaigns. Across the datasets, the differences in distribution of normalized objective values can be observed in Figure 1(a); the differences in distribution of sampled data points in its respective materials design space can be seen in Figure 1(b). The five materials datasets in the current study are available in the following GitHub repository [38]. 
High Low PC1 PC1 PC1 PC1 PC1 PC2 PC2 PC2 PC2 PC2 PC3 PC3 PC3 PC3 PC3 (b) (a)</p>
<p>Bayesian Optimization: surrogate models and acquisition functions</p>
<p>Bayesian optimization (BO) [3,4,5] aims to solve the problem of finding a global optimum (min or max) of an unknown objective function g:</p>
<h1>» x * = arg min x g( #» x ) where #» x ∈ X and X is a domain of interest in R n dim . BO</h1>
<p>holds the assumption that this black-box function g can be evaluated at any #» x ∈ X and the responses are noisy
point-wise observations ( #» x , y), where E[y|g( #» x )] = g( #» x ).
The surrogate model f is probabilistic and consists of a prior distribution that approximates the unknown objective function g, and is sequentially updated with collected data to yield a Bayesian posterior belief of g. Decision policies aimed to find the optimum in fewer experiments are implemented in acquisition functions, which can use the mean and variance predicted at any #» x ∈ X in the posterior to select the next observation to be performed.</p>
<p>The BO algorithm is comprised of both a surrogate model and an acquisition function. The surrogate models considered in this study are random forest (RF) [31], Gaussian process (GP) regression [39], and GP with automatic relevance detection (ARD) [39,5,34].</p>
<ol>
<li>
<p>To approximate the experience of a researcher with little prior knowledge of a materials design space, for RF, we have hyperparameters applicable across all five datasets without loss of generality: n tree = 50 and bootstrap = True.</p>
</li>
<li>
<p>For hyperparameters of GP, we choose kernels from Matérn52, Matérn32, Matérn12, radial basis function (RBF), and multilayer perceptron (MLP). The initial lengthscale for each kernel was set to unit length.</p>
</li>
<li>
<p>For hyperparameters of GP ARD, we use ARD, which allows GP to keep anisotropic kernels. The kernel function of GP then has individual characteristic lengthscales l j [5,34] for each of the input feature dimensions j.</p>
</li>
</ol>
<p>As an example, in dimension j, Matérn52 kernel function between two points #» p , #» q in design space would be
k(p j , q j ) = σ 2 0 · (1 + √ 5r l j + 5r 2 3l 2 j ) exp (− √ 5r l j )(1)
where r = (p j − q j ) 2 , σ is standard deviation and l j is the characteristic lengthscale. These characteristic lengthscales can be used to estimate the distance moved along j th dimension from the input values in design space before the change of objective values become uncorrelated with this feature. 1 l j is thus useful in understanding the sensitivity of objective value to input feature j.</p>
<p>We then pair the selected surrogate model with one of three acquisition functions, including expected improvement (EI), probability of improvement (PI), and lower confidence bound (LCB) LCB λ ( #» x ) = −λμ( #» x ) +σ( #» x ), whereμ andσ are the mean and standard deviation estimated by surrogate model while λ is an adjustable ratio between exploitation and exploration.</p>
<p>In addition, these surrogate models, their hyperparameters, and acquisition functions were chosen because they represent the majority of off-the-shelf options accessible, and are ones that have been widely applied to materials optimization campaigns in the field. Our study provides a comprehensive test across the five datasets in order to reflect how each BO algorithm, resulting from the pairing above, performs across many different materials science design spaces. GP and RF were also selected as examples to specifically illustrate how the differences in implicit distributional assumptions of surrogate model could affect the prediction of the mean and standard deviation when selecting subsequent experiments.</p>
<p>Pool-based active learning benchmarking framework</p>
<p>Within each respective experimental dataset, the set of data points form a discrete representation of ground truth in the materials design space. Figure 2 shows the pool-based active learning benchmarking framework we use to simulate materials optimization campaigns guided by BO algorithms in each materials system.</p>
<p>The framework has the following properties:</p>
<ol>
<li>
<p>It has the traits of an active learning study as it contains a machine learning model that is iteratively refined through subsequent experimental observation selection based on information from previously explored data points. The framework is also adapted for BO, and emphasizes optimization of materials objectives over building an accurate regression model in design space.</p>
</li>
<li>
<p>It is derived from pool-based active learning. Besides the randomly selected initial experiments, the subsequent experimental observations are selected from the total pool of undiscovered data points ( #» x , y) ∈ D, whose input features #» x are all made available for evaluation by the acquisition functions. The ground truth in the materials design space was represented with discrete data points over a continuous emulation for the following reasons: Observation of performance through case study on crossed barrel dataset While the five datasets covered a breadth of materials domains, the relative performances of tested BO algorithms were observed to be consistent. Figure 3(a) demonsrates the performances of RF, GP ARD (Matérn52), and GP (Matérn52). For the full combinatorial study including all types of GP kernels and acquisition functions, please refer to supplementary information (SI). We showcase the benchmarking results using the crossed barrel dataset [24], which was collected by grid sampling the design space through a robotic experimental system while optimizing the toughness
Initial experiments = {( ! , ! )} Add next experimental measurement ⃗ * → * to From ̂⃗ , . ⃗ calculate α̂⃗ , . ⃗ , ⃗ ∈ Select next experiment ⃗ * = arg max ⃗ $∈&amp; (̂⃗ , . ⃗ ) Update surrogate model : ⃗ ↦̂⃗ , . ⃗ with ⃗, , ⃗ ∈</p>
</li>
</ol>
<p>Planning Inference</p>
<p>Benchmarking with performance metrics contains ordered list of ⃗, in search for
⃗ '() = arg m ⃗ $∈* Iterate Task Objective
Acquisition functions α: Expected improvement, Lower confidence bound, Probability of improvement Surrogate models : Gaussian Process without ARD, Gaussian Process with ARD, Random Forest Figure 2: Benchmarking framework including a simulation of BO performing closed-loop optimization with alternating inference and planning stages. X is the iteratively collected sequence of experimental data ( #» x , y) during optimization campaign. D is the original pool or total undiscovered set of data from which next experiments are selected. f is the surrogate model used to estimate meanμ and standard deviationσ, which parameterize the acquisition function α to select next experiment #» x * to be evaluated. of additive manufactured crossed barrel structure. As for the performance metric, we use
Top%(i) = number of top candidates discovered number of total top candidates ∈ <a href="2">0, 1</a>
to show the fraction of the crossed barrel structures with top 5% toughness that have been discovered by cycle i = 1, 2, 3, . . . , N . Top% describes how quickly can a BO guided autonomous experimental system could identify multiple top candidates in a materials design space. Keeping multiple well-performing candidates allows one to not only observe regions in design space that frequently yield high-performing samples but also have backup options for further evaluation should the most optimal candidate fail in subsequent evaluations. There are research objectives related to finding any good materials candidate, yet in those cases, random selection could outperform optimization algorithms due to luck in a simple design space. Our objective of finding multiple or all top tier candidates is more applicable to experimental materials optimization scenarios and suitable for demonstrating the true efficacy and impact of BO.   2. Both RF and GP with ARD outclass GP without ARD.
(a) (b) (c) (d)
3. LCB 1 , which has equal weights for exploration and exploitation, outperforms other acquisition functions LCB λ that overly emphasize exploration. LCB 1 also outperformed EI, which is a very popular acquisition function in previous materials optimization studies but has also been known to make excessive greedy decisions [40,41,42].</p>
<p>Because of this observation, when trying to compare BO algorithms with different surrogate models in this work, the same acquisition function LCB 1 was used. We have also evaluated the performance of BO algorithms using probability of improvement (PI) as acquisition function, but its performance was mostly worse than EI and therefore not the focus of discussion; this observation can be partially attributed to PI only focusing on how likely is an improvement occurs, but not considering how much improvement could be made.</p>
<p>To further quantify the relative performance, we set Top% = 0.8 as a realistic goal to indicate we have identified 80% of the structures with top 5% toughness (Figure 3a). For surrogate models paired with LCB 1 , we see that GP with ARD and RF reach that goal by evaluating approximately 30 candidates out of the total of 600, whereas GP without ARD needs about 90 samples. Top% rises initially as slowly as the random baseline because the surrogate models suffer from high variance in prediction, having only been trained with a small datasets; Top% ramps up very quickly as the model learns to become more accurate in identifying general regions of interest to explore; the rate of learning eventually slows down at high learning cycles because the local exploitation for the global optimum has exhausted most if not all top 5% toughness candidates, and the algorithms therefore switch to exploring sub-optimal regions. Therefore, it can be assumed that the most valuable regions to examine performance is before each curve reaches Top% = 0.8 and Top% = 0.8 can be used as a realistic optimization goal.</p>
<p>To quantify the acceleration of discovery from BO, we adapt two other metrics similar to the ones from Rohr et al. [28].</p>
<p>Both compared to a statistical random baseline, Enhancement Factor(EF)
EF(i) = Top% BO (i) Top% random (i)(3)
shows how much improvement in a metric one would receive at cycle i, and Acceleration Factor(AF)
AF(Top% = a) = i BO i random(4)
is the ratio of cycle numbers showing how much faster one could reach a specific value Top%(i BO ) = Top%(i random ) = a ∈ [0, 1]. The aggregated performance of BO algorithms is further quantified via EF and AF curves in Figure 3(b), 3(c): starting off with small EFs or AFs before the surrogate model gains more accuracy; reaching absolute EF max and AF max of up to 16×. Eventually, the learning algorithms show diminishing returns from an information gain perspective as we progress deeper into our optimization campaigns during pool-based active learning. We observe that for the two BO algorithms both with same acquisition function LCB 1 but different surrogate model GP ARD and RF, they reach EF max at different learning cycles and AF max at different Top%, both corresponding to the switch of best performing algorithm around Top% = 0.67. RF: LCB 1 clearly excels at lower learning cycles, yet GP ARD: LCB 1 takes the lead and would reach Top% = 0.8 with fewer experiments. Therefore, these results objectively show that optimal BO algorithm selection varies with assigned experiment budget and specific optimization task [28].</p>
<p>Since we identified two BO algorithms, RF: LCB 1 and GP ARD: LCB 1 , to have similar performance, we wanted to further investigate how similar their optimization paths were in the design space when starting from the same initial experiments. In Figure 3(d), we use Jaccard similarity index to quantify the similarity in optimizations paths. Jaccard similarity, J = |A ∩ B| |A ∪ B| , is the size of the intersection divided by the size of the union of two finite sample sets; specifically in our benchmarking study, using the same 50-ensemble runs that generated Figure 3 In addition, the Jaccard similarity value does not increase monotonically, and a significant drop can be seen in J such as one around i = 30, which coincides with the learning cycles where GP ARD : LCB 1 overtook RF : LCB 1 as best performing algorithm in Figure 3(a). Since the two algorithms used the same acquisition function, this observation shows that while in general the optimization paths of the two algorithms have more overlap overtime, occasional divergence paths still take place because the two algorithms have considerable difference in gathered data used to learn their surrogate models and how their surrogate models predict mean and standard deviation. GP ARD : LCB 1 and RF :</p>
<p>LCB 1 started at the same two initial experiments and use the same acquisition function, and the only difference is the surrogate model used. Thus, the divergence and convergence in optimization paths can be again primarily attributed to GP ARD and RF exploiting underlying physics of crossed barrel structure differently. Figure 3(d) highlights the impact of different surrogate model selection beyond final performance, and to provide better guidelines to future research, inspires us to further investigate the role of surrogate models.</p>
<p>Comparison of performance across datasets</p>
<p>To further assess the performance of BO, optimization campaigns were conducted for the P3HT/CNT, AgNP, AM ARES, and Perovskite datasets. Across most, if not all the investigated datasets, it was quite observed consistently that the performance of BO algorithms using GP with ARD and RF as surrogate models were comparable, and both outperform those using GP without ARD in most datasets. To illustrate, in Figure 4, we show such relative performance using normalized EF max of BO algorithms same acquisition function LCB 1 but with different surrogate models across all five datasets. In each dataset, the BO algorithm with the largest EF max had its EF scaled to 1, and the other two BO algorithms showing lower EF max were correspondingly scaled, resulting in five sets of column plots. In addition to the observation on relative performance, we also observe that BO algorithms with RF and GP ARD as surrogate model also have plenty overlap between their 25th to 75th percentile across five datasets, further indicating their similarity in performance.</p>
<p>Notably, EF max of the other four datasets datasets were in the 2× to 5× range as seen in Figure S1 from SI, which is much lower than the EF max of the crossed barrel dataset in Figure 3(b). The difference in the absolute EF max can be attributed to the data collection methodology of the individual datasets. While the crossed barrel dataset was collected using a grid sampling approach, the other four studies were collected along the path of a BO guided materials optimization campaign. Therefore, these four datasets were smaller in size and possessed an intrinsic enhancement and acceleration within their datasets. As a result, it is reasonable that these datasets demonstrate lower EFs, AFs during benchmarking. Nevertheless, the fact that both EFs and AFs are still larger than 1 indicate that further acceleration and enhancement is still possible when given design spaces as parts of optimization paths. Noticeably, the Perovskite dataset had the most intrinsic acceleration because its next experimental choice was guided by BO infused with probabilistic constraints generated from DFT proxy calculations of the environmental stability [23] of perovskite . As a result, the optimization sequence to be chosen in that study is already narrowed down to a more efficient path from initial experiments to final optimum, making the random baseline to appear arbitrarily much worse. Another interesting Figure 4: Normalized EF max demonstrated by BO algorithms having GP without ARD, GP with ARD, and RF as surrogate models and all using LCB 1 as acquisition function. For each algorithm applied across datasets, the 50 th percentile of EF max is shown by the barplots, and its 25 th and 75 th percentile are shown by respective floating bars.</p>
<p>observation is how the performance of GP without ARD (isotropic kernels) as surrogate model catches up with GP ARD and RF when the design space has an already "easier" path towards the optimum, further supporting our observations on relative performance. The hypothesis that the lower EF max are caused by intrinsic acceleration and enhancement resulting from dataset collection process can be verified by collecting a subset from the uniformed grid sampled crossed barrel dataset. This subset is collected by running BO algorithm GP: EI until all candidates with top 5% toughness are found, representing an "easier" path towards optimums, and therefore carries intrinsic enhancement and acceleration.</p>
<p>We run the same benchmarking framework on this subset, and observe that while EF max is significantly reduced, the relative ranking of the three surrogate models is consistent, as seen again in Figure S1 from SI. Despite the differences described above, all the investigated BO algorithms outperformed the random baseline demonstrating the efficacy of BO in materials optimization campaigns.</p>
<p>Discussion</p>
<p>In this section, we discuss performance of BO algorithms in the context of autonomous and high-throughput materials optimization.</p>
<p>Comparison of RF and GP as surrogate models</p>
<p>While BO algorithms with GP type surrogate models have been extensively used in the field, we observe that the performance of RF as surrogate model is comparable to that of GP. The results heavily suggest that RF is a capable surrogate model to consider besides GP in BO for future HT materials optimization campaigns.</p>
<p>Before more detailed comparison, an important distinction to make between RF and GP as surrogate models is how they predict mean and standard deviation, which can be attributed to the implicit assumptions when using them as surrogates to represent an unknown ground truth within materials domains. A GP in this work, whether with ARD or not, is essentially a distribution over a materials domain such that any finite selection of data points in This important distinction explains the differences of results in Figure 3 and Figure 4. Other factors affecting surrogate model selection are further discussed via the following side-by-side comparisons of GPs and RFs.</p>
<p>We would first like to discuss the time complexity of these surrogate models. Most commonly in autonomous experimental materials optimization studies, time spent on generating samples is much more significant than that of surrogate model training. However, in our study, there was a noticeable difference in time when benchmarking different BO algorithms, which reveals foreseeable challenges to future research. Across five datasets in this study, starting from the same initial experiments and using the same acquisition function LCB 1 , the ratio of average running time to finish benchmarking framework between the three surrogate models is t RF : t GP : t GP ARD = 1 : 1.32 : 1.54. With n as the number of training data, n dim as design space dimension, n tree as number of decisions trees kept in RF model, in terms of general training time complexity, we have t RF = O(nlog(n) · n dim · n tree ) &lt; t GP = O(n 3 + n 2 · n dim ) [44,45,46].</p>
<p>The relatively expensive computational complexity of GP model is mostly due to the process of calculating the inverse of an n by n matrix during its training process, and keeping anisotropic kernels certainly adds extra computational time.</p>
<p>For reasonable choice of n, n dim , and n tree in materials science research, RF not only can be trained even faster via parallel computing of its decision trees, but also suffer less in performance with increasing n than GP. While the O(n 3 )</p>
<p>time complexity of GPs is typically less of a concern when working with smaller datasets, it could quickly become intractable when applied to larger datasets after experimental samples can be generated at unprecedented rate through advancement of automation in materials laboratories. At each learning cycle, time used in synthesis will eventually match with the time used in model retraining and prediction, typically within seconds. Therefore, in the future, if we do not consider the number of experiments as the budget, but instead consider the total experimental time invested, then RF has a potential advantage over GP when we aim to have fast and seamless feedback loop between model and materials experiments.</p>
<p>We next discuss some properties in the RF, GP, and GP ARD models that would explain the observed differences in performance. The benchmarking effort presented here is unique in its use of abundant experimental materials data with built in noise, often unavoidable in physical science research. The results thus provide a realistic performance evaluation for optimization algorithms in the context of materials research. RF having good performance across the five experimental datasets can be partially attributed to predictions by RF being empirical estimates from its ensemble of decision trees and free of distributional assumptions. The multiple decision trees of RF naturally have low bias and high variance; the aggregation process in RF of different decision trees mitigates this issue, resulting in a model that has relatively low bias and medium variance, and thus are likely more robust to noise and applicable for generalized prediction with unknown assumption [43,32]. These properties of RF can be partially observed in Figure 4, where we not only see the performance of RF match that of GP ARD as surrogate model, but also observe the variance of EF max for RF is on average lower than those of GP with ARD and GP without ARD. Generally speaking, on one hand, if the ground truth manifold of a design space indeed satisfies the gaussianity assumption of GP, then arguably GP type surrogate models have an advantage in learning a model with low bias and variance. On the other hand, if there were sharp discontinuities, piece-wise constants or changes in orders of magnitude through local regions of materials design space, The decision trees of RF would be able to capture these points accurately and reflect their influences on future predictions via aggregated result. These points are specifically regions of interest to be further investigated by researchers, whether they are new findings or outliers from experiments, but they typically fall out of distributional assumptions of GP and could be smoothed out in the learned model without considerable effort in kernel hyperparameter tuning.</p>
<p>We last discuss the effort required hyperparameter tuning of surrogate model during optimization. Despite HTE drastically increasing the rate of materials data collection, active learning for optimization in new materials domains still requires data collection in a sequential or batched manner. While RF has potentially more hyperparameters such as n tree , max depth, and max split to select, it is less penalized for sub-optimal choice of hyperparameters compared to GP.</p>
<p>In this study, across five datasets, as long as sufficient n tree were used in RF, its performance as surrogate model in BO algorithm has been consistently comparable to that of GP. Other hyperparameters of RF have had less of an impact.</p>
<p>Meanwhile, besides the implicit distribution assumption of using a GP type surrogate model, a kernel (covariance function) of GP specifies a specific prior on the domain. Choosing a kernel that is incompatible with the domain manifold could significantly slow down optimization conversion due to loss of generalization, as seen in Figure S2 -S6 from SI, where BO algorithms with GP surrogate models are highly sensitive to kernel selection. For example, Matérn52 kernel analytically requires the fitted GP to be 2 times differentiable in the mean-square sense [4], which can be difficult to verify for unknown materials design spaces. Selecting such a kernel could introduce extra domain manifold assumptions to an unfamiliar design space, as we often have limited data to make confident distribution assumptions at optimization onset. Instead of devoting nontrivial experimental budget to optimize the kernels of GP using adaptive kernels [47], automating kernel selection [48] or keeping a library of kernels available via online learning, RF is an easier off-the-shelf option that allows one to make fewer structural assumptions about unfamiliar materials domains. If a GP surrogate model is still preferred, a Multilayer Perceptron (MLP) kernel [49] mimicking neural networks would be suggested as it has comparable performance to other kernels (see Figure S2 -S6 from SI).</p>
<p>Admittedly, our benchmarking framework might have given RF a slight advantage by discretizing the materials domain through actively acquiring a new datapoint at each cycle and limiting the choice of next experiments within the pool of undiscovered datapoints. However, the crossed barrel dataset has a sampling density, size, and range within its design space sufficient to cover its manifold complexity. A drawback of RF is that it performs poorly in extrapolation beyond the search space covered by training data, yet in the context of materials optimization campaigns, this disadvantage can be mitigated by clever design of initial experiments, namely using sampling strategies like latin hypercube sampling (LHS). In this way, we can not only preserve the pseudo random nature of selecting initial experiments but also cover a wider range of data in each dimension so that RF surrogate model would not have to often extrapolate to completely unknown regions. Considering our benchmarking results and the comparisons above, RF's relative ease of use paired with the intuitive tuning of LCB's weights to adjust exploration and exploitation forms a BO algorithm suitable for general materials optimization campaigns at early stages.</p>
<p>Benefits of using GP ARD with anisotropic kernels</p>
<p>As seen from Figure 4, using anisotropic kernels with GP essentially removes the performance gap between RF and GP with isotropic kernels. with the benefit that the one can exploit the underlying modeling assumptions of the GP ARD for more advanced analysis.</p>
<p>As mentioned earlier, ARD allows us to utilize individual lengthscales for each input dimension j in the kernel function of GP, which are subsequently optimized along learning cycles. These lengthscales in an anisotropic kernel provide a "weight" for measuring relative relevancy of each feature to predicting the objective, i.e. understanding the sensitivity of objective value to each input feature dimension. The reason GP without ARD shows worse performance is as follows:</p>
<p>it will have a single lengthscale in an isotropic kernel as scaling parameter controlling GP's kernel function, which is at odds with the fact that each input feature has its distinct contribution to the objective. Depending on how different each feature is in nature, range and units, e.g. solvent composition vs. printing speed, using the same lengthscale in the kernel function for each feature dimension could provide unreliable predictive results. The materials optimization objective naturally has different sensitivities to each input variable, and thus it is rationale then, that the "lengthscale"</p>
<p>parameter inside the GP kernel should be independent. In Figure 4, the noticeable improvements of using an anisotropic kernel can be seen in the relative lower performance of GP without ARD compared to that of GP with ARD. While data normalization can partially alleviate the problem, how it is conducted is highly subject to a researcher's choice, and therefore we would like to raise awareness of the benefits of using GP with anisotropic kernels.</p>
<p>In addition, the lengthscales from the kernels of GP with ARD provides us with more useful information about the input features. These lengthscale values have been used for removing irrelevant inputs [4], where high l j values imply low relevancy input feature j. In the context of materials optimization, we find the following use of ARD especially useful: ARD could identify a few directions in the input space with specially high "relevance." This means that if we train GP with ARD on input data with their original units and without normalization, once we extract the lengthscale of each feature l j , our GP model in theory should not be able to accurately extrapolate more than l j units away from collected observations in j th dimension. Thus, l j suggests the range of next experiments to be performed in the j th dimension of the materials design space. It also infers a suitable sampling density in each dimension in the experimental setting. When a particular input feature dimension has a relative small l j or large 1 l j , it means that for small change in objective value, we would have a relatively large change in the location within this input feature dimension; thus, the the sampling density or resolution in this dimension should be high enough to capture such sensitivity. In addition, people have considered using information extracted from these lengthscales for even more advanced analysis and variable selection [50]. At the expense of computation time tolerable in the context of materials optimization campaigns, an anisotropic kernel provides not only a better generalizable GP model but also useful information in analyzing input feature relevancy at each learning cycle. For the above mentioned reasons, it would be great practice for researchers to emphasize their use of GP with anisotropic kernels as surrogate models during materials optimization campaigns.</p>
<p>In conclusion, we benchmarked the performance of BO algorithms across five different experimental materials science domains. We utilize a pool-based active learning framework to approximate experimental materials optimization processes, and adapted active learning metrics to quantitatively evaluate the enhancement and acceleration of BO for common research objectives. We demonstrate that when paired with the same acquisition functions, RF as surrogate model can compete with GP with ARD, and both outperform GP without ARD. In the context of autonomous and high-throughput experimental materials research, we discuss the differences in implicit distributional assumptions of surrogate models and the benefits of using GP with anisotropic kernels. We provide practical insights on surrogate model selection for materials optimization campaigns, and also offer open source implementation of benchmarking code and datasets to support future algorithmic development.</p>
<p>Establishing benchmarks for active learning algorithms like BO across a broad scope of materials systems is only a starting point. Our observations demonstrate how the choice of active learning algorithms has to adapt to their applications in materials science, motivating more efficient ML guided closed-loop experimentation, and will likely directly result in a larger number of successful optimization of materials with record breaking properties. The impact of this work can be extended to not only other materials systems, but also a broader scope of scientific studies utilizing closed-loop and high-throughput research platforms. Through our benchmarking effort, we hope to share our insights with the field of accelerated materials discovery and motivate a closer collaboration between ML and physical science communities.</p>
<p>Methods Prediction by surrogate models and acquisition functions</p>
<p>In order to estimate the meanμ( #» x * ) and standard deviationσ( #» x * ) of predicted objective value at a previously undiscovered observation #» x * in design space:</p>
<p>For a Gaussian process (GP), it assumes a prior over the design space that is constructed from already collected observations ( #» x i , y i ), i = 1, 2, ..., n. This prior is the source of implicit distributional assumptions, and when an undiscovered new observation ( #» x * , y * ) is being considered during noisy setting (σ = 0.01), the joint distribution between the objective values of collected data #» y ∈ R n and y * is
  #» y y *   ∼ N   0,   K + σ 2 I K T * K * K **     .(5)
K is the covariance matrix of the input features X = { #» x i |i = 1, 2, ..., n}; K * is the covariance between the collected data and new input feature #» x * ; K ** is the covariance between the new data. For each of the covariance matrices, 
K pq = k( #» x p , #» x q ),
The standard deviation valueσ( #» x ) can be obtained from the diagonal elements of this covariance matrix.</p>
<p>For a random forest (RF), letĥ k ( #» x * ) denote the prediction of objective value from the k th decision tree in the forest, k = 1, 2, ..., n tree , thenμ
( #» x * ) = 1 n tree ntree k=1ĥ k ( #» x * )(8)
andσ
( #» x * ) = ntree k=1 (ĥ k ( #» x * ) −μ( #» x * )) 2 n tree(9)
The median or other variations could also be used in future studies to aggregate the predictions for potential improvement in robustness [43].</p>
<p>We tested three acquisition functions in our study, including expected improvement (EI), probability of improvement (PI), and lower confidence bound (LCB).</p>
<p>EI( #» x ) = (y best −μ( #» x ) − ξ) · Φ(Z) +σ( #» x )ϕ(Z) (10)
PI( #» x ) = Φ(Z)(11)
where
Z = y best −μ( #» x ) − ξ σ( #» x )(12)
µ andσ are estimated mean and standard deviation by surrogate model; y best is best discovered objective value within all collected values so far; ξ = 0.01 is jitter value that can slightly control exploration and exploitation; Φ and ϕ are the cumulative density function and probability density function of a normal distribution.
LCB λ ( #» x ) = −λμ( #» x ) +σ( #» x )(13)
where λ is a adjustable ratio between exploitation and exploration.</p>
<p>Pool-based active learning framework</p>
<p>As seen in Figure  and standard deviationσ( #» x ). We then evaluate the acquisition function values α(μ( #» x ),σ( #» x )) for each remaining experimental action #» x ∈ D in parallel. At each cycle, action #» x * = arg max x α( #» x ) will be selected as next experiment.</p>
<p>During inference stage, after selecting action #» x * , the corresponding sample observation y * is obtained, and ( #» x * , y * )</p>
<p>is added to X and removed from set D. The new observation ( #» x * , y * ) is incorporated into the surrogate model. The sequential alternation between planning and inference is repeated until undiscovered data points run out.</p>
<p>Statistical baselines</p>
<p>In Figure 3 amd 4, we have introduced some statistical baselines when benchmarking the performance of BO algorithms with a pool-based active learning framework.</p>
<p>For the random baseline in Figure 3 Then at cycle i = 2, 3, ..., N , there is
P (i) = M − i−1 n=1 P (n) N − i(14)
and
Top%(i) = i n=1 P (n) M(15)
In Figure 3(d), between two optimization paths starting with the same two initial data points:</p>
<ol>
<li>
<p>The statistically most overlap happens when two paths are identical, resulting in J(i) = 1, i = 1, 2, ..., N ;</p>
</li>
<li>
<p>The statistically least overlap happens when the two follow drastically different paths until they run out of data points undiscovered by both algorithms, resulting in
J(i) =              1 1 ≤ x ≤ 2 1 i − 1 3 ≤ i ≤ N 2 + 1 2i − N N N 2 + 2 ≤ i ≤ N(16)</p>
</li>
</ol>
<p>Data availability</p>
<p>The five experimental datasets in the current study is available are the following GitHub repository [38]:</p>
<p>https://github.com/PV-Lab/Benchmarking.</p>
<p>Code availability</p>
<p>The code for pool-based active learning framework and visualization in the current study are available in the following GitHub repository [38]: https://github.com/PV-Lab/Benchmarking.</p>
<p>Figure 1 :
1Experimental materials dataset design space manifold complexity visualization. (a) Histogram of objective values normalized to zero-mean without loss of generality. (b) Input feature space, i.e. design space, visualization after dimension reduction to 3D via principal component analysis (PCA). The colors of each point in the datasets indicate its value. PCA was performed to reduce all datasets dimensions to three for visualization, and the three axes shown are the top three principal component directions of each dataset.</p>
<p>( a )
aMaterials design spaces are often not continuous due to experimental resolution limitation and noise in real research scenarios. (b) Materials datasets do not necessarily capture the domain manifold due to lack of abundant design space sampling density and coverage. (c) To emulate a domain manifold, selecting of models such as GP introduces smoothness assumptions into the design space, and thus during the benchmarking process could give great advantages to BO algorithms with surrogate models sharing similar assumptions. 3. At each learning cycle of the framework, instead of selecting a larger batch, only one new experiment is obtained. Due to the different dataset sizes across five materials studies, keeping a batch size of one allows us to directly compare the performance of BO algorithms across different materials studies.Each BO algorithm is evaluated for 50 ensembles with 50 independent random seeds governing the initialization of experiments. The aggregated performances of the BO algorithms derived from 50 averaged runs resulting from 10 random five-fold splits using the 50 original ensembles, is compared against a statistical random search baseline, and we can quantitatively evaluate its performance via active learning metrics defined in the sections below. A detailed description of the framework and the calculation of statistical random baselines can be seen in the Methods section.The simulated materials optimization campaigns were conducted on the Boston University Shared Computing Cluster (SCC). This enabled the parallel execution of multiple campaigns on individual computing nodes. The computing nodes were configured to have access to a maximum of 28 cores (2 fourteen-core 2.4 GHz Intel Xeon E5-2680v4 processors) with a maximum of 256 GB of memory.</p>
<p>Figure 3 (
3a) illustrates learning rates based on Top% metric for RF and GP with multiple acquisition functions.</p>
<p>Figure 3 :
3The aggregated performance of BO algorithms on the Crossed barrel dataset measured by a) Top% vs. learning cycle i against random baseline, and how b) Enhancement factor EF and c) acceleration factor AF are derived from it. The algorithms with GP as surrogate model are labeled in red, and RF in blue; higher color saturation is correlated with better performance. Variation at each learning cycle is visualized by plotting the median as well as shaded regions representing the 25 th to 75 th percentile of the aggregated 50-run ensembles. The acquisition functions used are EI and LCB λ (d) Jaccard similarity index calculated between the optimization campaign sequences of BO algorithms RF: LCB 1 and GP ARD: LCB 1 . The median, 25 th , 75 th percentile of the 50-run ensemble are shown respectively.</p>
<p>(a), we can calculate Jaccard similarity value J(i) at each learning cycle i, where A(i) is the set of data points sequential collected at each learning cycle during an optimization path guided by BO algorithm GP ARD: LCB 1 , and B(i) is that of using RF: LCB 1 . As baselines, we have also drawn what the Jaccard similarity value would look like between two optimization paths that begin with the same initial experiments and statistically have least overlap or most overlap. When i = 1 or 2, the same initial experiments are given to the two BO algorithms, and J = 1. When 2 &lt; i &lt; 8, we can see that the Jaccard similarity value drops as quickly as the statistically least overlapping paths, indicating that despite the fact that GP with ARD and RF were trained on the same initial experiments at the onset, they follow very different paths in the materials design space. This behavior indicates that, despite achieving comparable performance, they exploit the underlying physics differently by virtue of the choice of experiments.When i ≥ 8, J increases with i, indicating that the paths chosen by the two algorithms gradually start to have some overlap as they move towards finding crossed barrels structures with high toughness. Recall both algorithms reached Top% = 0.8 between 30 to 40 learning cycles inFigure 3(a), and between those learning cycles, we observe that J is approximately between 0.6 -0.65, still considerably far from J = 1. This observation shows that while both algorithms have comparable performance in the task of finding crossed barrel structures with good toughness, due to their different choice of surrogate models, their paths towards discovering optimum can differ considerably.</p>
<p>this design space results in a multivariate Gaussian density over any point of interest in the space. For the selection of a new data point as next experiment, its predicted mean and standard deviation are all part of a gaussian distribution constructed from previous experiments. Therefore, the predicted means and standard deviations of GPs from their posteriors carry gaussianity assumptions and can be interpreted as statistical predictions based on prior information. Meanwhile, a RF is an ensemble of decision trees, which are trained on the experimental data points collected during optimization and have slight variation due to bootstrapping. For RF, prediction of objective value at a new data point is an aggregated result, most likely the mean[43] as used in this study, of all its decision trees' prediction; similarly, prediction of standard deviation at new data point by RF is the standard deviation of all its decision trees' predictions. Compared to those of GPs, the predicted means and standard deviations of RFs do not have strong distributional assumptions, and can be interpreted as empirical estimates. In short, during prediction, GPs rely on heavy distributional assumptions while RF is distribution free. Such difference between two surrogate models carries over to the values of mean and standard deviation, and together affect the selection of next experiment despite being paired with exact same acquisition function.</p>
<p>where k is the kernel function, whether isotropic or anisotropic, used in GP. Then from the posterior, we have estimatesμ( #» x ) = y * = K * [K + σ 2 I] −1 #» y(6)and covariance matrix cov(y * ) = K ** − K * [K + σ 2 I] −1 K T *</p>
<p>2, to approximate early stage exploration during each optimization campaign, n = 2 initial experiments are drawn randomly with no replacement from original pool D = {( #» x i , y i )|i = 1, 2, . . . , N } and add to collection X = {( #» x i , y i )|i = 1, 2, . . . , n}. During planning stage, surrogate model f is used to estimate the meanμ( #» x )</p>
<p>(a), assuming a total pool of N data points and the number of good materials candidates M = 0.05N , at cycle i = 1, expected probability of finding a good candidate is P (1) = 0.05 and expected value of Top%(1) = 1 · P (1) M = 0.0016.</p>
<p>Table 1 :
1Description of experimental materials science datasets.Dataset 
Domain 
Synthesis 
Size 
n dim 
Optimization Objective </p>
<p>P3HT/CNT [35] 
Composite blends 
Drop casting 
178 
5 
Electrical conductivity 
AgNP </p>
<ol>
<li>RF initially excels at lower learning cycles, while GP with ARD takes the lead after Top% = 0.67. Under the same acquisition function, performance of RF as a surrogate model is on par, if not slightly worse, when compared to the performance of GP with ARD.</li>
</ol>
<p>Active learning literature survey. Burr Settles, Burr Settles. Active learning literature survey. 2009.</p>
<p>Active learning with statistical models. Zoubin David A Cohn, Michael I Jordan Ghahramani, Journal of artificial intelligence research. 4David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129-145, 1996.</p>
<p>Taking the human out of the loop: A review of bayesian optimization. Bobak Shahriari, Kevin Swersky, Ziyu Wang, P Ryan, Nando De Adams, Freitas, Proceedings of the IEEE. 1041Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148-175, 2015.</p>
<p>Gaussian processes for machine learning (gpml) toolbox. Carl Edward Rasmussen, Hannes Nickisch, The Journal of Machine Learning Research. 11Carl Edward Rasmussen and Hannes Nickisch. Gaussian processes for machine learning (gpml) toolbox. The Journal of Machine Learning Research, 11:3011-3015, 2010.</p>
<p>I Peter, Frazier, arXiv:1807.02811A tutorial on bayesian optimization. arXiv preprintPeter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.</p>
<p>Bayesian optimization with robust bayesian neural networks. Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, Frank Hutter, Advances in neural information processing systems. 29Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. Advances in neural information processing systems, 29:4134-4142, 2016.</p>
<p>A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. Eric Brochu, M Vlad, Nando De Cora, Freitas, arXiv:1012.2599arXiv preprintEric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010.</p>
<p>Bayesian optimization for materials design. I Peter, Jialei Frazier, Wang, Information Science for Materials Discovery and Design. SpringerPeter I Frazier and Jialei Wang. Bayesian optimization for materials design. In Information Science for Materials Discovery and Design, pages 45-75. Springer, 2016.</p>
<p>Scalable global optimization via local bayesian optimization. David Eriksson, Michael Pearce, R Jacob, Ryan Gardner, Matthias Turner, Poloczek, arXiv:1910.01739arXiv preprintDavid Eriksson, Michael Pearce, Jacob R Gardner, Ryan Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization. arXiv preprint arXiv:1910.01739, 2019.</p>
<p>Batched high-dimensional bayesian optimization via structural kernel learning. Zi Wang, Chengtao Li, Stefanie Jegelka, Pushmeet Kohli, International Conference on Machine Learning. PMLRZi Wang, Chengtao Li, Stefanie Jegelka, and Pushmeet Kohli. Batched high-dimensional bayesian optimization via structural kernel learning. In International Conference on Machine Learning, pages 3656-3664. PMLR, 2017.</p>
<p>Multi-objective bayesian materials discovery: Application on the discovery of precipitation strengthened niti shape memory alloys through micromechanical modeling. Alexandros Solomou, Guang Zhao, Shahin Boluki, K Jobin, Xiaoning Joy, Ibrahim Qian, Raymundo Karaman, Dimitris C Arróyave, Lagoudas, Materials &amp; Design. 160Alexandros Solomou, Guang Zhao, Shahin Boluki, Jobin K Joy, Xiaoning Qian, Ibrahim Karaman, Raymundo Arróyave, and Dimitris C Lagoudas. Multi-objective bayesian materials discovery: Application on the discovery of precipitation strengthened niti shape memory alloys through micromechanical modeling. Materials &amp; Design, 160:810-827, 2018.</p>
<p>Multifunctional structural design of graphene thermoelectrics by bayesian optimization. Masaki Yamawaki, Masato Ohnishi, Shenghong Ju, Junichiro Shiomi, Science advances. 464192Masaki Yamawaki, Masato Ohnishi, Shenghong Ju, and Junichiro Shiomi. Multifunctional structural design of graphene thermoelectrics by bayesian optimization. Science advances, 4(6):eaar4192, 2018.</p>
<p>Active learning for accelerated design of layered materials. Lindsay Bassman, Pankaj Rajak, K Rajiv, Aiichiro Kalia, Fei Nakano, Jifeng Sha, Sun, J David, Muratahan Singh, Patrick Aykol, Kristin Huck, Persson, npj Computational Materials. 41Lindsay Bassman, Pankaj Rajak, Rajiv K Kalia, Aiichiro Nakano, Fei Sha, Jifeng Sun, David J Singh, Muratahan Aykol, Patrick Huck, Kristin Persson, et al. Active learning for accelerated design of layered materials. npj Computational Materials, 4(1):1-9, 2018.</p>
<p>Optimisation of gan leds and the reduction of efficiency droop using active machine learning. Bertrand Rouet-Leduc, Kipton Barros, Turab Lookman, Colin J Humphreys, Scientific reports. 61Bertrand Rouet-Leduc, Kipton Barros, Turab Lookman, and Colin J Humphreys. Optimisation of gan leds and the reduction of efficiency droop using active machine learning. Scientific reports, 6(1):1-6, 2016.</p>
<p>Accelerated search for batio3-based piezoelectrics with vertical morphotropic phase boundary using bayesian learning. Dezhen Xue, V Prasanna, Ruihao Balachandran, Tao Yuan, Xiaoning Hu, Qian, R Edward, Turab Dougherty, Lookman, Proceedings of the National Academy of Sciences. 11347Dezhen Xue, Prasanna V Balachandran, Ruihao Yuan, Tao Hu, Xiaoning Qian, Edward R Dougherty, and Turab Lookman. Accelerated search for batio3-based piezoelectrics with vertical morphotropic phase boundary using bayesian learning. Proceedings of the National Academy of Sciences, 113(47):13301-13306, 2016.</p>
<p>Efficient closed-loop maximization of carbon nanotube growth rate using bayesian optimization. Jorge Chang, Pavel Nikolaev, Jennifer Carpena-Núñez, Rahul Rao, Kevin Decker, Ahmad E Islam, Jiseob Kim, A Mark, Jay I Pitt, Benji Myung, Maruyama, Scientific Reports. 101Jorge Chang, Pavel Nikolaev, Jennifer Carpena-Núñez, Rahul Rao, Kevin Decker, Ahmad E Islam, Jiseob Kim, Mark A Pitt, Jay I Myung, and Benji Maruyama. Efficient closed-loop maximization of carbon nanotube growth rate using bayesian optimization. Scientific Reports, 10(1):1-9, 2020.</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. P Benjamin, Macleod, G L Fraser, Parlane, D Thomas, Florian Morrissey, Häse, M Loïc, Kevan E Roch, Raphaell Dettelbach, Moreira, P E Lars, Yunker, B Michael, Joseph R Rooney, Deeth, Science Advances. 6208867Benjamin P MacLeod, Fraser GL Parlane, Thomas D Morrissey, Florian Häse, Loïc M Roch, Kevan E Dettelbach, Raphaell Moreira, Lars PE Yunker, Michael B Rooney, Joseph R Deeth, et al. Self-driving laboratory for accelerated discovery of thin-film materials. Science Advances, 6(20):eaaz8867, 2020.</p>
<p>Toward machine learning-enhanced high-throughput experimentation. Natalie S Eyke, A Brent, Klavs F Koscher, Jensen, Trends in Chemistry. Natalie S Eyke, Brent A Koscher, and Klavs F Jensen. Toward machine learning-enhanced high-throughput experimentation. Trends in Chemistry, 2021.</p>
<p>Next-generation experimentation with self-driving laboratories. Florian Häse, M Loïc, Alán Roch, Aspuru-Guzik, Trends in Chemistry. 13Florian Häse, Loïc M Roch, and Alán Aspuru-Guzik. Next-generation experimentation with self-driving laborato- ries. Trends in Chemistry, 1(3):282-291, 2019.</p>
<p>Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments. Fang Ren, Logan Ward, Travis Williams, Kevin J Laws, Christopher Wolverton, Jason Hattrick-Simpers, Apurva Mehta, Science advances. 441566Fang Ren, Logan Ward, Travis Williams, Kevin J Laws, Christopher Wolverton, Jason Hattrick-Simpers, and Apurva Mehta. Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments. Science advances, 4(4):eaaq1566, 2018.</p>
<p>Autonomy in materials research: a case study in carbon nanotube growth. Pavel Nikolaev, Daylond Hooper, Frederick Webber, Rahul Rao, Kevin Decker, Michael Krein, Jason Poleski, Rick Barto, Benji Maruyama, npj Computational Materials. 21Pavel Nikolaev, Daylond Hooper, Frederick Webber, Rahul Rao, Kevin Decker, Michael Krein, Jason Poleski, Rick Barto, and Benji Maruyama. Autonomy in materials research: a case study in carbon nanotube growth. npj Computational Materials, 2(1):1-6, 2016.</p>
<p>Efficient search of compositional space for hybrid organic-inorganic perovskites via bayesian optimization. C Henry, Weici Herbol, Peter Hu, Paulette Frazier, Matthias Clancy, Poloczek, npj Computational Materials. 41Henry C Herbol, Weici Hu, Peter Frazier, Paulette Clancy, and Matthias Poloczek. Efficient search of compositional space for hybrid organic-inorganic perovskites via bayesian optimization. npj Computational Materials, 4(1):1-7, 2018.</p>
<p>A data fusion approach to optimize compositional stability of halide perovskites. Shijing Sun, Armi Tiihonen, Felipe Oviedo, Zhe Liu, Janak Thapa, Yicheng Zhao, Noor Titan, P Hartono, Anuj Goyal, Thomas Heumueller, Clio Batali, Matter4Shijing Sun, Armi Tiihonen, Felipe Oviedo, Zhe Liu, Janak Thapa, Yicheng Zhao, Noor Titan P Hartono, Anuj Goyal, Thomas Heumueller, Clio Batali, et al. A data fusion approach to optimize compositional stability of halide perovskites. Matter, 4(4):1305-1322, 2021.</p>
<p>A bayesian experimental autonomous researcher for mechanical design. E Aldair, Bowen Gongora, Wyatt Xu, Chika Perry, Patrick Okoye, Riley, G Kristofer, Elise F Reyes, Keith A Morgan, Brown, Science advances. 6151708Aldair E Gongora, Bowen Xu, Wyatt Perry, Chika Okoye, Patrick Riley, Kristofer G Reyes, Elise F Morgan, and Keith A Brown. A bayesian experimental autonomous researcher for mechanical design. Science advances, 6(15):eaaz1708, 2020.</p>
<p>Phoenics: a bayesian optimizer for chemistry. Florian Häse, M Loïc, Christoph Roch, Alán Kreisbeck, Aspuru-Guzik, ACS central science. 49Florian Häse, Loïc M Roch, Christoph Kreisbeck, and Alán Aspuru-Guzik. Phoenics: a bayesian optimizer for chemistry. ACS central science, 4(9):1134-1145, 2018.</p>
<p>Using simulation to accelerate autonomous experimentation: A case study using mechanics. E Aldair, Gongora, L Kelsey, Emily Snapp, Patrick Whiting, Riley, G Kristofer, Elise F Reyes, Keith A Morgan, Brown, Iscience. 244102262Aldair E Gongora, Kelsey L Snapp, Emily Whiting, Patrick Riley, Kristofer G Reyes, Elise F Morgan, and Keith A Brown. Using simulation to accelerate autonomous experimentation: A case study using mechanics. Iscience, 24(4):102262, 2021.</p>
<p>Beyond ternary opv: high-throughput experimentation and selfdriving laboratories optimize multicomponent systems. Stefan Langner, Florian Häse, José Darío Perea, Tobias Stubhan, Jens Hauch, M Loïc, Thomas Roch, Alán Heumueller, Christoph J Aspuru-Guzik, Brabec, Advanced Materials. 32141907801Stefan Langner, Florian Häse, José Darío Perea, Tobias Stubhan, Jens Hauch, Loïc M Roch, Thomas Heumueller, Alán Aspuru-Guzik, and Christoph J Brabec. Beyond ternary opv: high-throughput experimentation and self- driving laboratories optimize multicomponent systems. Advanced Materials, 32(14):1907801, 2020.</p>
<p>. Brian Rohr, Helge S Stein, Dan Guevarra, Yu Wang, Joel A Haber, Muratahan Aykol, Santosh K Suram, andBrian Rohr, Helge S Stein, Dan Guevarra, Yu Wang, Joel A Haber, Muratahan Aykol, Santosh K Suram, and</p>
<p>Benchmarking the acceleration of materials discovery by sequential learning. M John, Gregoire, Chemical Science. 1110John M Gregoire. Benchmarking the acceleration of materials discovery by sequential learning. Chemical Science, 11(10):2696-2706, 2020.</p>
<p>Accelerating high-throughput virtual screening through molecular pool-based active learning. E David, Eugene I Graff, Connor W Shakhnovich, Coley, Chemical Science. 2021David E Graff, Eugene I Shakhnovich, and Connor W Coley. Accelerating high-throughput virtual screening through molecular pool-based active learning. Chemical Science, 2021.</p>
<p>Olympus: a benchmarking framework for noisy optimization and experiment planning. Florian Hase, Matteo Aldeghi, Riley Hickman, Loic Roch, Elena Liles, Melodie Christensen, Jason Hein, Alán Aspuru-Guzik, Machine Learning: Science and Technology. Florian Hase, Matteo Aldeghi, Riley Hickman, Loic Roch, Elena Liles, Melodie Christensen, Jason Hein, and Alán Aspuru-Guzik. Olympus: a benchmarking framework for noisy optimization and experiment planning. Machine Learning: Science and Technology, 2021.</p>
<p>Scikit-learn: Machine learning in python. the. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Journal of machine Learning research. 12Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.</p>
<p>Random forests. Machine learning. Leo Breiman, 45Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.</p>
<p>Classification and regression by randomforest. Andy Liaw, Matthew Wiener, 2R newsAndy Liaw, Matthew Wiener, et al. Classification and regression by randomforest. R news, 2(3):18-22, 2002.</p>
<p>Bayesian learning for neural networks. M Radford, Neal, Springer Science &amp; Business Media118Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science &amp; Business Media, 2012.</p>
<p>Machine learning and high-throughput robust design of p3ht-cnt composite thin films for high electrical conductivity. Daniil Bash, Yongqiang Cai, Vijila Chellappan, Swee Liang Wong, Yang Xu, Pawan Kumar, Jin Da Tan, Anas Abutaha, Jayce Cheng, Yee Fun Lim, arXiv:2011.10382arXiv preprintDaniil Bash, Yongqiang Cai, Vijila Chellappan, Swee Liang Wong, Yang Xu, Pawan Kumar, Jin Da Tan, Anas Abutaha, Jayce Cheng, Yee Fun Lim, et al. Machine learning and high-throughput robust design of p3ht-cnt composite thin films for high electrical conductivity. arXiv preprint arXiv:2011.10382, 2020.</p>
<p>Daniil Bash, et al. Two-step machine learning enables optimized nanoparticle synthesis. Flore Mekki-Berrada, Zekun Ren, Tan Huang, Wai Kuan Wong, Fang Zheng, Jiaxun Xie, Isaac Parker Siyu Tian, Senthilnath Jayavelu, Zackaria Mahfoud, Computational Materials. 71Flore Mekki-Berrada, Zekun Ren, Tan Huang, Wai Kuan Wong, Fang Zheng, Jiaxun Xie, Isaac Parker Siyu Tian, Senthilnath Jayavelu, Zackaria Mahfoud, Daniil Bash, et al. Two-step machine learning enables optimized nanoparticle synthesis. npj Computational Materials, 7(1):1-10, 2021.</p>
<p>Toward autonomous additive manufacturing: Bayesian optimization on a 3d printer. Jorge James R Deneault, Jay Chang, Daylond Myung, Andrew Hooper, Mark Armstrong, Benji Pitt, Maruyama, MRS Bulletin. James R Deneault, Jorge Chang, Jay Myung, Daylond Hooper, Andrew Armstrong, Mark Pitt, and Benji Maruyama. Toward autonomous additive manufacturing: Bayesian optimization on a 3d printer. MRS Bulletin, pages 1-10, 2021.</p>
<p>Benchmarking the performance of bayesian optimization across multiple experimental materials science domains. Q Liang, 2021Q. Liang. Benchmarking the performance of bayesian optimization across multiple experimental materials science domains. https://github.com/PV-Lab/Benchmarking, 2021.</p>
<p>A gaussian process framework in python. Gpy Gpy, GPy GPy. A gaussian process framework in python, 2012.</p>
<p>On the convergence rates of expected improvement methods. O Ilya, Ryzhov, Operations Research. 646Ilya O Ryzhov. On the convergence rates of expected improvement methods. Operations Research, 64(6):1515- 1528, 2016.</p>
<p>Entropy search for information-efficient global optimization. Philipp Hennig, J Christian, Schuler, Journal of Machine Learning Research. 136Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 13(6), 2012.</p>
<p>Bayesian optimization. I Peter, Frazier, Recent Advances in Optimization and Modeling of Contemporary Problems. INFORMSPeter I Frazier. Bayesian optimization. In Recent Advances in Optimization and Modeling of Contemporary Problems, pages 255-278. INFORMS, 2018.</p>
<p>Robustness of random forests for regression. Marie-Hélène Roy, Denis Larocque, Journal of Nonparametric Statistics. 244Marie-Hélène Roy and Denis Larocque. Robustness of random forests for regression. Journal of Nonparametric Statistics, 24(4):993-1006, 2012.</p>
<p>Sparse gaussian processes using pseudo-inputs. Edward Snelson, Zoubin Ghahramani, Advances in Neural Information Processing Systems. 18Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. Advances in Neural Information Processing Systems, 18:1259-1266, 2006.</p>
<p>Flexible and efficient Gaussian process models for machine learning. Edward Lloyd Snelson, UCL (University College LondonPhD thesisEdward Lloyd Snelson. Flexible and efficient Gaussian process models for machine learning. PhD thesis, UCL (University College London), 2007.</p>
<p>Local and global sparse gaussian process approximations. Edward Snelson, Zoubin Ghahramani, Artificial Intelligence and Statistics. PMLREdward Snelson and Zoubin Ghahramani. Local and global sparse gaussian process approximations. In Artificial Intelligence and Statistics, pages 524-531. PMLR, 2007.</p>
<p>Gaussian process kernels for pattern discovery and extrapolation. Andrew Wilson, Ryan Adams, International conference on machine learning. PMLRAndrew Wilson and Ryan Adams. Gaussian process kernels for pattern discovery and extrapolation. In Interna- tional conference on machine learning, pages 1067-1075. PMLR, 2013.</p>
<p>Automated model search using bayesian optimization and genetic programming. Louis Schlessinger, Gustavo Malkomes, Roman Garnett, 3rd Workshop on Meta-Learning at the Conference and Workshop on Neural Information Processing Systems. Louis Schlessinger, Gustavo Malkomes, and Roman Garnett. Automated model search using bayesian optimiza- tion and genetic programming. 3rd Workshop on Meta-Learning at the Conference and Workshop on Neural Information Processing Systems, 2019.</p>
<p>Kernel methods for deep learning. Youngmin Cho, Lawrence K Saul, Advances in Neural Information Processing Systems 22. Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Advances in Neural Information Processing Systems 22. 2009.</p>
<p>Variable selection for gaussian processes via sensitivity analysis of the posterior predictive distribution. Topi Paananen, Juho Piironen, Aki Michael Riis Andersen, Vehtari, The 22nd International Conference on Artificial Intelligence and Statistics. PMLRTopi Paananen, Juho Piironen, Michael Riis Andersen, and Aki Vehtari. Variable selection for gaussian processes via sensitivity analysis of the posterior predictive distribution. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1743-1752. PMLR, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
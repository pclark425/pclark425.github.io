<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-485 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-485</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-485</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-270619899</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.14325v3.pdf" target="_blank">Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers</a></p>
                <p><strong>Paper Abstract:</strong> Many research fields are currently reckoning with issues of poor levels of reproducibility. Some label it a"crisis", and research employing or building Machine Learning (ML) models is no exception. Issues including lack of transparency, data or code, poor adherence to standards, and the sensitivity of ML training conditions mean that many papers are not even reproducible in principle. Where they are, though, reproducibility experiments have found worryingly low degrees of similarity with original results. Despite previous appeals from ML researchers on this topic and various initiatives from conference reproducibility tracks to the ACM's new Emerging Interest Group on Reproducibility and Replicability, we contend that the general community continues to take this issue too lightly. Poor reproducibility threatens trust in and integrity of research results. Therefore, in this article, we lay out a new perspective on the key barriers and drivers (both procedural and technical) to increased reproducibility at various levels (methods, code, data, and experiments). We then map the drivers to the barriers to give concrete advice for strategies for researchers to mitigate reproducibility issues in their own work, to lay out key areas where further research is needed in specific areas, and to further ignite discussion on the threat presented by these urgent issues.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e485.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e485.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Underspecified methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Underspecified or ambiguous methodological descriptions in papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Natural language descriptions (paper methods sections) often omit critical implementation details (models, preprocessing, exact evaluation metrics, hyperparameter choices), making faithful reproduction of code and experiments impossible or ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML experiment pipeline / published study</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Typical ML research experiment as described in a paper (textual methods, figures, tables) intended to be reimplemented or rerun by others.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section (textual description)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reimplementation derived from paper text or author-provided code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous description</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers frequently fail to specify the exact ML model variant, preprocessing steps, dataset splits, evaluation metric definitions, and selection criteria (e.g., best run vs averaged runs). These omissions force reproducers to make assumptions or implement variants that differ from the original, producing divergent results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing; model specification; evaluation metrics; experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual reading of papers during reproduction attempts; reproducibility studies reporting inability to reproduce due to missing details</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative counts and audits of reporting completeness (e.g., checklist compliance); the paper cites systematic reviews and audits measuring reporting completeness</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to failed or ambiguous reproductions and uncertain comparability; specific quantitative values not provided here but tied to subsequent reproducibility failure rates reported across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread – described as a major barrier; linked to general observations across ML and biomedical literature (no single % given for this gap alone in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Concise/ambiguous natural language, space constraints, incentive structures (publish-or-perish), and lack of standardized reporting templates.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use of guidelines and checklists (e.g., TRIPOD, CLAIM, ROBUST-ML), mandatory reporting policies, model cards, and model info sheets; preregistration to reduce spin and selective reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The paper cites a systematic review where mandatory checklists increased inclusion of main methodological information by 65%; other mitigations are advocated but quantitative effectiveness beyond that is not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; biomedical ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e485.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selective reporting / spin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selective reporting and 'spin' in natural language conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors sometimes present over-generalized or selectively positive interpretations in text (spin), or report only the single best run rather than aggregated statistics, misaligning textual claims with the implemented code/experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML research reporting and evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Publication text (abstract, results, discussion) summarizing experimental outcomes produced by code and analysis scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper conclusions and results description (narrative interpretation)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experimental runs/results aggregation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>selective reporting / mismatch between reported summary and actual experimental distribution</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural language conclusions present inflated claims or recommend deployment without proper external validation; reported numbers may reflect only best-of-many runs rather than mean±variance, hiding variability present in code execution.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>results reporting; evaluation metrics; experimental aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>meta-analyses and systematic reviews comparing reported claims against methodological details and reproduction attempts; audits noting absence of aggregated statistics</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counts/proportions in systematic reviews (e.g., prevalence of spin practices), comparison of reported single-run numbers versus re-run distributions</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Misleads readers about model robustness and reproducibility; can cause unreproducible apparent performance when reproducers run multiple seeds and observe high variance (no single unified quantitative effect given beyond reviewed studies).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common in biomedical ML; the paper cites studies finding widespread spin and selective reporting (e.g., many studies recommend models without external validation; exact overall % not universally given here).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Cultural incentives (novelty/impact), publication bias favoring positive results, lack of enforced reporting standards.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Preregistration, mandatory reporting checklists, requiring aggregated results (multiple seeds/runs with variance), journal policies encouraging null/result-agnostic publication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Preregistration and mandatory checklists are argued to reduce spin; paper cites evidence that mandatory checklists increase reporting completeness by 65%, implying reduction in selective omissions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; biomedical ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e485.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Incomplete/shared code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Incomplete, poorly documented, or missing code releases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Code accompanying papers is often not shared, or when shared is incomplete (pseudocode, skeletons) or lacks environment/dependency specifics, causing misalignment between the paper's descriptions and runnable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Research code repositories / experiment artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Author-provided code repositories intended to implement experiments described in papers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper methods and README documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>source code repository (scripts, notebooks), or absent</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing implementation details / incomplete code / undocumented dependencies</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors frequently omit critical pieces like experiment setup scripts, data preprocessing steps, hyperparameter sweep scripts, hardware specs, or dependency versions; sometimes only pseudocode is provided, forcing reproducers to reimplement and possibly diverge.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experiment scripts; preprocessing; dependency specification; hardware/software environment</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Attempted reproduction and inspection of repositories (manual code review); audits reporting rates of code sharing</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical audits: the paper reports that only one-third of researchers share data and even fewer share source code (citation [58]); radiomics review: 16 of 257 shared data; other studies counted completeness of shared artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial barrier to reproducibility — inability to rerun experiments or reproduce exact results; leads to non-reproducibility or large variance due to reimplementation differences.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High: 'Only one-third share data, even fewer share code' (paper statement). Radiomics: 16/257 shared data (example of data sharing rarity).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Time/effort required to clean/share code, IP concerns, incentives, lack of norms/requirements for comprehensive artifact release.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use hosting services, virtualization/containers (Docker, Code Ocean), reproducible packaging (RO-Crate, ReproZip), and experiment tracking/versioning tools (DVC, MLflow, dToolAI) to bundle code, dependencies, and metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Platforms and containers have been shown to reduce setup friction and improve sharing; Code Ocean integration with Nature journals cited; effectiveness depends on adoption—no single overall quantitative effectiveness number given here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; biomedical ML; recommender systems</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e485.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hidden AutoML procedures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Opaque AutoML optimization and hidden implementation choices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AutoML and no-code/low-code platforms often hide model search, preprocessing and hyperparameter tuning procedures, producing a mismatch between high-level textual description and the actual automated implementation used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML / no-code ML platforms</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Commercial or open AutoML platforms (e.g., H2O Driverless AI, Google Cloud AutoML, DataRobot) that automate end-to-end model development steps.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper methods mentioning use of AutoML platform; vendor documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>proprietary platform pipeline or exported model artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / hidden optimization details / incomplete transparency</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>AutoML platforms can obscure what exact preprocessing, model selection, and hyperparameter optimization were performed; papers may state 'used AutoML' without enumerating the exact pipeline, thus natural language descriptions are insufficient to reproduce the exact implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>preprocessing; model search; hyperparameter optimization; pipeline composition</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Reproducibility studies assessing AutoML outputs, qualitative analyses of reproduction attempts where platform internals are unavailable, and platform evaluations noting lack of out-of-the-box reproducibility [46,94].</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative evaluation and reproduction assessment; referenced assessments concluded current AutoML platforms cannot provide out-of-the-box reproducibility (no numeric metric provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevents exact reproduction of reported pipelines and performance; may also hide methodological errors (e.g., handling of leakage) and impede trustworthy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Rising concern due to increasing use of AutoML and no-code tools; paper cites literature indicating growing prevalence but no precise prevalence figure.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Platform opacity, proprietary internals, lack of detailed exported pipeline descriptions, and domain experts' limited ML expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require AutoML platforms to export full pipeline logs and artifacts, use platforms with automatic documentation, develop standards/guidelines for reporting AutoML runs, and provide training for domain users.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Some AutoML tools provide automatic documentation features; qualitative studies note potential enabler roles but overall platforms currently fail to ensure OOTB reproducibility—quantitative improvement not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; applied domain experts using AutoML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e485.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperparameter & environment mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch in hyperparameters, software dependencies, and environment leading to discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences between described hyperparameters/environment and the actual implementation (or missing specification) cause reproducibility failures; even small hyperparameter changes or framework versions can change outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML training and evaluation environment</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Training code executed across different software frameworks, versions, dependency stacks, and hardware (GPU/CPU).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper methods section and supplementary material</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training scripts; dependency manifests (often missing)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / dependency/version mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers or released code omit or misstate hyperparameter values, optimizer settings, random seeds, or exact software versions; different framework versions (PyTorch vs TensorFlow or different versions of same framework) and compiler settings yield divergent results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>hyperparameters; software dependencies; experiment setup; random seeds</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical comparison of runs across frameworks/versions and reproduction attempts reporting differences; cited comparative studies showing differing performance between PyTorch and TensorFlow even with fixed seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical performance comparisons across frameworks/versions; studies referenced observed measurable performance differences (no single percent quoted here in paper summary).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can lead to different performances and conclusions; example citations show framework and version differences produce differing outcomes and that GPU parallelism increases nondeterminism.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common — the paper emphasizes this as a significant barrier; no single global percentage provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incomplete reporting of environment/dependencies, software evolution, hardware heterogeneity, and implicit assumptions about default settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use virtualization/containers (Docker, VMs), record and share hardware specs and dependency versions, host experiments on standardized hosting services, publish seeds and full experiment manifests (ReproZip, RO-Crate).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Containers and virtualization reduce reproducibility friction and have been adopted successfully in various fields; effectiveness depends on adoption and may add overhead. No single numeric effectiveness provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e485.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inherent nondeterminism / randomness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inherent nondeterminism and stochastic sources in ML training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Randomness in initialization, stochastic optimization, data subsampling, and parallel GPU operations leads to run-to-run variation even with identical code and data, causing misalignment between a single natural language reported result and typical outcomes of the implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML training pipeline (neural networks; RL environments)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Training procedures involving stochastic components (random weight init, SGD, data shuffling, environment randomness) that produce variable results across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper-reported single-run or underspecified run-level reporting</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training scripts with/without seed management</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>stochastic variability / missing seed management</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often report point estimates or best runs without accounting for inherent randomness; even published code rerun multiple times can produce widely varying outcomes and different conclusions, especially in neural networks and reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure; experimental replication; evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Reproducibility experiments running multiple seeds/runs; studies measuring variance across runs and comparing conclusions drawn from single runs versus aggregated results.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reporting performance distribution across multiple random seeds; referenced works advise benchmarking with multiple seeds and report variance. Specific studies (e.g., patching approach) reported success in enforcing determinism for certain models.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large run-to-run variance can change comparative outcomes and conclusions; can cause failures to reproduce reported claims when only single-run results were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Pervasive for stochastic ML methods; paper cites neural networks and RL as particularly affected. No single numeric prevalence provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Stochastic training algorithms, parallel/non-deterministic GPU operations, unrecorded seeds, and failure to report run distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Manage randomness via fixed seeds, deterministic algorithm variants, patching non-deterministic ops (case study achieved reproducibility across six neural nets), evaluate across multiple seeds and report variance, use deterministic RL frameworks where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Fixing seeds can ensure reproducibility when not executing parallel GPU operations; patching achieved reproducible image classification for six networks in a cited case study but incurred higher computational costs; overall mitigations reduce variance but may not fully eliminate it across all setups.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; deep learning; reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e485.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Environmental differences</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differences in hardware, compilers, and framework versions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discrepancies between the described experimental environment and the environment where code is executed can lead to divergent computational outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Computational environment for ML experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Hardware (GPUs/CPUs), compilers, and ML frameworks/versions used to run training/evaluation code.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper environment/specification section (often brief)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>environment-dependent binaries, framework-specific implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>environment mismatch / software/hardware dependency differences</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Different GPUs/CPUs, compiler flags, or framework versions (or different frameworks altogether) have been observed to change results; even identical code and seeds can behave differently across hardware or framework versions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>execution environment; hardware; software stack</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Cross-environment experiments and comparisons reported in literature; empirical studies comparing same algorithm across PyTorch and TensorFlow or across framework versions.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Performance comparisons across environments; cited literature documents measurable differences though no single consolidated metric is given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can produce nontrivial performance differences and nondeterminism; sometimes leads authors to prefer CPU-only runs to reduce randomness (at runtime cost).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common enough to be highlighted as major barrier; no precise prevalence percentage in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Heterogeneous hardware, nondeterministic low-level primitives, framework implementation differences, and incomplete environment reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use containers/virtualization to fix environment, provide hardware and compiler specs, host experiments on reproducible hosting services, or publish RO-Crate/experiment manifests.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Containers and hosting services improve reproducibility of environment; nevertheless, paper notes hosting services do not guarantee OOTB reproducibility and resource limits may apply.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; deep learning; computational sciences</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e485.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data leakage mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between described data handling and implemented data splits/usage (data leakage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Natural language descriptions can fail to accurately document data partitioning or legitimate feature choices, while code or pipelines may inadvertently include leaked information, producing inflated reported performance rarely reproducible in correct setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Data preprocessing and dataset management within ML pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Data collection, preprocessing, feature selection, train/validation/test splits and imputation steps described in text and implemented in code.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper data/methods description; model info sheets (when present)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>preprocessing scripts, feature selection scripts, data split implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>data leakage / missing train/test split specification / non-legitimate features used</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Descriptions may omit or insufficiently specify train/test split procedures or legitimate feature filtering; implementations sometimes use test data during preprocessing/feature selection or include features not available in deployment, causing a misalignment between the described experimental intent and the actual code behavior (target leakage).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing; feature selection; train/test splits; evaluation dataset selection</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Reproduction attempts, manual inspection, and application of model info sheets; systematic audits found leakage in reproduced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical audits: the paper cites an empirical study of 12 ML prediction papers where a third exhibited data leakage (i.e., ~33% in that small sample); radiomics review also documents widespread problems with data sharing and provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Data leakage inflates reported performance and produces irreproducible results when proper splits are used; leads to validity shrinkage and overoptimistic claims.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not rare: cited study found leakage in ~1/3 of a 12-paper sample; broader literature indicates data leakage is a common methodological error.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Lack of ML expertise among domain researchers, insufficient documentation of data splits and preprocessing, and no standard use of model info sheets or rigorous checks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use model info sheets and model cards to document splits and legitimate features, adopt standardized datasets/splits, deploy checklists and peer-review checks focused on leakage, and automated tools to detect leakage patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Model info sheets could have prevented all leakage cases in the cited 12-paper study (authors' claim) but adoption is limited; checklists and standardized datasets help but require community uptake.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; biomedical ML; predictive modeling</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e485.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e485.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model info sheets / detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model info sheets as documentation to detect data/code-description gaps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured documentation (model info sheets) requiring detailed answers about data provenance, splits, and design choices can reveal mismatches between natural language descriptions and actual implementations, especially for data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Documentation and reporting layer for ML experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Supplementary structured forms (model info sheets) published alongside papers to capture granular details about datasets, preprocessing, splits, and intended use.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>model info sheet (structured documentation)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>supporting metadata and explicit answers about implemented procedures</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>documentation-driven detection of mismatches / methodological checklist</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Model info sheets prompt authors to disclose details (train/test splits, leakage risks, feature legitimacy). When completed and compared with code, they can expose gaps where code uses non-legitimate features or improper splitting not mentioned in text.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data provenance; training/testing splits; feature legitimacy</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual review of model info sheets vs code; use in reproducibility audits revealed previously hidden leakage or mis-specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Empirical comparison in a small study: the paper cites that an empirical study of 12 papers would have had leakage preventable by model info sheets (i.e., all leakage cases detectable post-hoc), though verification requires reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Increases transparency and enables quicker detection of methodological flaws leading to irreproducible claims; practical effect limited by adoption and reviewer expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Adoption limited currently; leakage itself is relatively common but model info sheet use remains rare.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Lack of standardized, enforced documentation and limited ML expertise among authors/reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage/require model info sheets with submissions; integrate these into review and publication workflows; pair with automated checks where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Promising but not widely adopted; cited small-sample evidence suggests they could have detected all leakage cases in one study, but broader quantitative evidence not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Machine learning; biomedical ML</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sources of irreproducibility in machine learning: A review <em>(Rating: 2)</em></li>
                <li>Leakage and the reproducibility crisis in machine-learning-based science <em>(Rating: 2)</em></li>
                <li>Do machine learning platforms provide out-of-the-box reproducibility? <em>(Rating: 2)</em></li>
                <li>Reproducible automl: An assessment of research reproducibility of no-code automl tools <em>(Rating: 2)</em></li>
                <li>Managing Randomness to Enable Reproducible Machine Learning <em>(Rating: 1)</em></li>
                <li>Toward training reproducible deep learning models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-485",
    "paper_id": "paper-270619899",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Underspecified methods",
            "name_full": "Underspecified or ambiguous methodological descriptions in papers",
            "brief_description": "Natural language descriptions (paper methods sections) often omit critical implementation details (models, preprocessing, exact evaluation metrics, hyperparameter choices), making faithful reproduction of code and experiments impossible or ambiguous.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML experiment pipeline / published study",
            "system_description": "Typical ML research experiment as described in a paper (textual methods, figures, tables) intended to be reimplemented or rerun by others.",
            "nl_description_type": "research paper methods section (textual description)",
            "code_implementation_type": "reimplementation derived from paper text or author-provided code",
            "gap_type": "incomplete specification / ambiguous description",
            "gap_description": "Papers frequently fail to specify the exact ML model variant, preprocessing steps, dataset splits, evaluation metric definitions, and selection criteria (e.g., best run vs averaged runs). These omissions force reproducers to make assumptions or implement variants that differ from the original, producing divergent results.",
            "gap_location": "data preprocessing; model specification; evaluation metrics; experimental protocol",
            "detection_method": "manual reading of papers during reproduction attempts; reproducibility studies reporting inability to reproduce due to missing details",
            "measurement_method": "qualitative counts and audits of reporting completeness (e.g., checklist compliance); the paper cites systematic reviews and audits measuring reporting completeness",
            "impact_on_results": "Leads to failed or ambiguous reproductions and uncertain comparability; specific quantitative values not provided here but tied to subsequent reproducibility failure rates reported across domains.",
            "frequency_or_prevalence": "Widespread – described as a major barrier; linked to general observations across ML and biomedical literature (no single % given for this gap alone in this paper).",
            "root_cause": "Concise/ambiguous natural language, space constraints, incentive structures (publish-or-perish), and lack of standardized reporting templates.",
            "mitigation_approach": "Use of guidelines and checklists (e.g., TRIPOD, CLAIM, ROBUST-ML), mandatory reporting policies, model cards, and model info sheets; preregistration to reduce spin and selective reporting.",
            "mitigation_effectiveness": "The paper cites a systematic review where mandatory checklists increased inclusion of main methodological information by 65%; other mitigations are advocated but quantitative effectiveness beyond that is not provided here.",
            "domain_or_field": "Machine learning; biomedical ML",
            "reproducibility_impact": true,
            "uuid": "e485.0",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Selective reporting / spin",
            "name_full": "Selective reporting and 'spin' in natural language conclusions",
            "brief_description": "Authors sometimes present over-generalized or selectively positive interpretations in text (spin), or report only the single best run rather than aggregated statistics, misaligning textual claims with the implemented code/experiment.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML research reporting and evaluation pipeline",
            "system_description": "Publication text (abstract, results, discussion) summarizing experimental outcomes produced by code and analysis scripts.",
            "nl_description_type": "paper conclusions and results description (narrative interpretation)",
            "code_implementation_type": "experimental runs/results aggregation scripts",
            "gap_type": "selective reporting / mismatch between reported summary and actual experimental distribution",
            "gap_description": "Natural language conclusions present inflated claims or recommend deployment without proper external validation; reported numbers may reflect only best-of-many runs rather than mean±variance, hiding variability present in code execution.",
            "gap_location": "results reporting; evaluation metrics; experimental aggregation",
            "detection_method": "meta-analyses and systematic reviews comparing reported claims against methodological details and reproduction attempts; audits noting absence of aggregated statistics",
            "measurement_method": "Counts/proportions in systematic reviews (e.g., prevalence of spin practices), comparison of reported single-run numbers versus re-run distributions",
            "impact_on_results": "Misleads readers about model robustness and reproducibility; can cause unreproducible apparent performance when reproducers run multiple seeds and observe high variance (no single unified quantitative effect given beyond reviewed studies).",
            "frequency_or_prevalence": "Common in biomedical ML; the paper cites studies finding widespread spin and selective reporting (e.g., many studies recommend models without external validation; exact overall % not universally given here).",
            "root_cause": "Cultural incentives (novelty/impact), publication bias favoring positive results, lack of enforced reporting standards.",
            "mitigation_approach": "Preregistration, mandatory reporting checklists, requiring aggregated results (multiple seeds/runs with variance), journal policies encouraging null/result-agnostic publication.",
            "mitigation_effectiveness": "Preregistration and mandatory checklists are argued to reduce spin; paper cites evidence that mandatory checklists increase reporting completeness by 65%, implying reduction in selective omissions.",
            "domain_or_field": "Machine learning; biomedical ML",
            "reproducibility_impact": true,
            "uuid": "e485.1",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Incomplete/shared code",
            "name_full": "Incomplete, poorly documented, or missing code releases",
            "brief_description": "Code accompanying papers is often not shared, or when shared is incomplete (pseudocode, skeletons) or lacks environment/dependency specifics, causing misalignment between the paper's descriptions and runnable implementations.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Research code repositories / experiment artifacts",
            "system_description": "Author-provided code repositories intended to implement experiments described in papers.",
            "nl_description_type": "paper methods and README documentation",
            "code_implementation_type": "source code repository (scripts, notebooks), or absent",
            "gap_type": "missing implementation details / incomplete code / undocumented dependencies",
            "gap_description": "Authors frequently omit critical pieces like experiment setup scripts, data preprocessing steps, hyperparameter sweep scripts, hardware specs, or dependency versions; sometimes only pseudocode is provided, forcing reproducers to reimplement and possibly diverge.",
            "gap_location": "experiment scripts; preprocessing; dependency specification; hardware/software environment",
            "detection_method": "Attempted reproduction and inspection of repositories (manual code review); audits reporting rates of code sharing",
            "measurement_method": "Empirical audits: the paper reports that only one-third of researchers share data and even fewer share source code (citation [58]); radiomics review: 16 of 257 shared data; other studies counted completeness of shared artifacts.",
            "impact_on_results": "Substantial barrier to reproducibility — inability to rerun experiments or reproduce exact results; leads to non-reproducibility or large variance due to reimplementation differences.",
            "frequency_or_prevalence": "High: 'Only one-third share data, even fewer share code' (paper statement). Radiomics: 16/257 shared data (example of data sharing rarity).",
            "root_cause": "Time/effort required to clean/share code, IP concerns, incentives, lack of norms/requirements for comprehensive artifact release.",
            "mitigation_approach": "Use hosting services, virtualization/containers (Docker, Code Ocean), reproducible packaging (RO-Crate, ReproZip), and experiment tracking/versioning tools (DVC, MLflow, dToolAI) to bundle code, dependencies, and metadata.",
            "mitigation_effectiveness": "Platforms and containers have been shown to reduce setup friction and improve sharing; Code Ocean integration with Nature journals cited; effectiveness depends on adoption—no single overall quantitative effectiveness number given here.",
            "domain_or_field": "Machine learning; biomedical ML; recommender systems",
            "reproducibility_impact": true,
            "uuid": "e485.2",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Hidden AutoML procedures",
            "name_full": "Opaque AutoML optimization and hidden implementation choices",
            "brief_description": "AutoML and no-code/low-code platforms often hide model search, preprocessing and hyperparameter tuning procedures, producing a mismatch between high-level textual description and the actual automated implementation used.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "AutoML / no-code ML platforms",
            "system_description": "Commercial or open AutoML platforms (e.g., H2O Driverless AI, Google Cloud AutoML, DataRobot) that automate end-to-end model development steps.",
            "nl_description_type": "paper methods mentioning use of AutoML platform; vendor documentation",
            "code_implementation_type": "proprietary platform pipeline or exported model artifacts",
            "gap_type": "different algorithm variant / hidden optimization details / incomplete transparency",
            "gap_description": "AutoML platforms can obscure what exact preprocessing, model selection, and hyperparameter optimization were performed; papers may state 'used AutoML' without enumerating the exact pipeline, thus natural language descriptions are insufficient to reproduce the exact implementation.",
            "gap_location": "preprocessing; model search; hyperparameter optimization; pipeline composition",
            "detection_method": "Reproducibility studies assessing AutoML outputs, qualitative analyses of reproduction attempts where platform internals are unavailable, and platform evaluations noting lack of out-of-the-box reproducibility [46,94].",
            "measurement_method": "Qualitative evaluation and reproduction assessment; referenced assessments concluded current AutoML platforms cannot provide out-of-the-box reproducibility (no numeric metric provided in this paper).",
            "impact_on_results": "Prevents exact reproduction of reported pipelines and performance; may also hide methodological errors (e.g., handling of leakage) and impede trustworthy comparisons.",
            "frequency_or_prevalence": "Rising concern due to increasing use of AutoML and no-code tools; paper cites literature indicating growing prevalence but no precise prevalence figure.",
            "root_cause": "Platform opacity, proprietary internals, lack of detailed exported pipeline descriptions, and domain experts' limited ML expertise.",
            "mitigation_approach": "Require AutoML platforms to export full pipeline logs and artifacts, use platforms with automatic documentation, develop standards/guidelines for reporting AutoML runs, and provide training for domain users.",
            "mitigation_effectiveness": "Some AutoML tools provide automatic documentation features; qualitative studies note potential enabler roles but overall platforms currently fail to ensure OOTB reproducibility—quantitative improvement not provided here.",
            "domain_or_field": "Machine learning; applied domain experts using AutoML",
            "reproducibility_impact": true,
            "uuid": "e485.3",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Hyperparameter & environment mismatch",
            "name_full": "Mismatch in hyperparameters, software dependencies, and environment leading to discrepancy",
            "brief_description": "Differences between described hyperparameters/environment and the actual implementation (or missing specification) cause reproducibility failures; even small hyperparameter changes or framework versions can change outcomes.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML training and evaluation environment",
            "system_description": "Training code executed across different software frameworks, versions, dependency stacks, and hardware (GPU/CPU).",
            "nl_description_type": "paper methods section and supplementary material",
            "code_implementation_type": "training scripts; dependency manifests (often missing)",
            "gap_type": "hyperparameter mismatch / dependency/version mismatch",
            "gap_description": "Papers or released code omit or misstate hyperparameter values, optimizer settings, random seeds, or exact software versions; different framework versions (PyTorch vs TensorFlow or different versions of same framework) and compiler settings yield divergent results.",
            "gap_location": "hyperparameters; software dependencies; experiment setup; random seeds",
            "detection_method": "Empirical comparison of runs across frameworks/versions and reproduction attempts reporting differences; cited comparative studies showing differing performance between PyTorch and TensorFlow even with fixed seeds.",
            "measurement_method": "Empirical performance comparisons across frameworks/versions; studies referenced observed measurable performance differences (no single percent quoted here in paper summary).",
            "impact_on_results": "Can lead to different performances and conclusions; example citations show framework and version differences produce differing outcomes and that GPU parallelism increases nondeterminism.",
            "frequency_or_prevalence": "Common — the paper emphasizes this as a significant barrier; no single global percentage provided.",
            "root_cause": "Incomplete reporting of environment/dependencies, software evolution, hardware heterogeneity, and implicit assumptions about default settings.",
            "mitigation_approach": "Use virtualization/containers (Docker, VMs), record and share hardware specs and dependency versions, host experiments on standardized hosting services, publish seeds and full experiment manifests (ReproZip, RO-Crate).",
            "mitigation_effectiveness": "Containers and virtualization reduce reproducibility friction and have been adopted successfully in various fields; effectiveness depends on adoption and may add overhead. No single numeric effectiveness provided here.",
            "domain_or_field": "Machine learning; deep learning",
            "reproducibility_impact": true,
            "uuid": "e485.4",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Inherent nondeterminism / randomness",
            "name_full": "Inherent nondeterminism and stochastic sources in ML training",
            "brief_description": "Randomness in initialization, stochastic optimization, data subsampling, and parallel GPU operations leads to run-to-run variation even with identical code and data, causing misalignment between a single natural language reported result and typical outcomes of the implementation.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "ML training pipeline (neural networks; RL environments)",
            "system_description": "Training procedures involving stochastic components (random weight init, SGD, data shuffling, environment randomness) that produce variable results across runs.",
            "nl_description_type": "paper-reported single-run or underspecified run-level reporting",
            "code_implementation_type": "training scripts with/without seed management",
            "gap_type": "stochastic variability / missing seed management",
            "gap_description": "Papers often report point estimates or best runs without accounting for inherent randomness; even published code rerun multiple times can produce widely varying outcomes and different conclusions, especially in neural networks and reinforcement learning.",
            "gap_location": "training procedure; experimental replication; evaluation",
            "detection_method": "Reproducibility experiments running multiple seeds/runs; studies measuring variance across runs and comparing conclusions drawn from single runs versus aggregated results.",
            "measurement_method": "Reporting performance distribution across multiple random seeds; referenced works advise benchmarking with multiple seeds and report variance. Specific studies (e.g., patching approach) reported success in enforcing determinism for certain models.",
            "impact_on_results": "Large run-to-run variance can change comparative outcomes and conclusions; can cause failures to reproduce reported claims when only single-run results were reported.",
            "frequency_or_prevalence": "Pervasive for stochastic ML methods; paper cites neural networks and RL as particularly affected. No single numeric prevalence provided.",
            "root_cause": "Stochastic training algorithms, parallel/non-deterministic GPU operations, unrecorded seeds, and failure to report run distributions.",
            "mitigation_approach": "Manage randomness via fixed seeds, deterministic algorithm variants, patching non-deterministic ops (case study achieved reproducibility across six neural nets), evaluate across multiple seeds and report variance, use deterministic RL frameworks where possible.",
            "mitigation_effectiveness": "Fixing seeds can ensure reproducibility when not executing parallel GPU operations; patching achieved reproducible image classification for six networks in a cited case study but incurred higher computational costs; overall mitigations reduce variance but may not fully eliminate it across all setups.",
            "domain_or_field": "Machine learning; deep learning; reinforcement learning",
            "reproducibility_impact": true,
            "uuid": "e485.5",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Environmental differences",
            "name_full": "Differences in hardware, compilers, and framework versions",
            "brief_description": "Discrepancies between the described experimental environment and the environment where code is executed can lead to divergent computational outcomes.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Computational environment for ML experiments",
            "system_description": "Hardware (GPUs/CPUs), compilers, and ML frameworks/versions used to run training/evaluation code.",
            "nl_description_type": "paper environment/specification section (often brief)",
            "code_implementation_type": "environment-dependent binaries, framework-specific implementations",
            "gap_type": "environment mismatch / software/hardware dependency differences",
            "gap_description": "Different GPUs/CPUs, compiler flags, or framework versions (or different frameworks altogether) have been observed to change results; even identical code and seeds can behave differently across hardware or framework versions.",
            "gap_location": "execution environment; hardware; software stack",
            "detection_method": "Cross-environment experiments and comparisons reported in literature; empirical studies comparing same algorithm across PyTorch and TensorFlow or across framework versions.",
            "measurement_method": "Performance comparisons across environments; cited literature documents measurable differences though no single consolidated metric is given in this paper.",
            "impact_on_results": "Can produce nontrivial performance differences and nondeterminism; sometimes leads authors to prefer CPU-only runs to reduce randomness (at runtime cost).",
            "frequency_or_prevalence": "Common enough to be highlighted as major barrier; no precise prevalence percentage in this paper.",
            "root_cause": "Heterogeneous hardware, nondeterministic low-level primitives, framework implementation differences, and incomplete environment reporting.",
            "mitigation_approach": "Use containers/virtualization to fix environment, provide hardware and compiler specs, host experiments on reproducible hosting services, or publish RO-Crate/experiment manifests.",
            "mitigation_effectiveness": "Containers and hosting services improve reproducibility of environment; nevertheless, paper notes hosting services do not guarantee OOTB reproducibility and resource limits may apply.",
            "domain_or_field": "Machine learning; deep learning; computational sciences",
            "reproducibility_impact": true,
            "uuid": "e485.6",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Data leakage mismatch",
            "name_full": "Mismatch between described data handling and implemented data splits/usage (data leakage)",
            "brief_description": "Natural language descriptions can fail to accurately document data partitioning or legitimate feature choices, while code or pipelines may inadvertently include leaked information, producing inflated reported performance rarely reproducible in correct setups.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Data preprocessing and dataset management within ML pipeline",
            "system_description": "Data collection, preprocessing, feature selection, train/validation/test splits and imputation steps described in text and implemented in code.",
            "nl_description_type": "paper data/methods description; model info sheets (when present)",
            "code_implementation_type": "preprocessing scripts, feature selection scripts, data split implementations",
            "gap_type": "data leakage / missing train/test split specification / non-legitimate features used",
            "gap_description": "Descriptions may omit or insufficiently specify train/test split procedures or legitimate feature filtering; implementations sometimes use test data during preprocessing/feature selection or include features not available in deployment, causing a misalignment between the described experimental intent and the actual code behavior (target leakage).",
            "gap_location": "data preprocessing; feature selection; train/test splits; evaluation dataset selection",
            "detection_method": "Reproduction attempts, manual inspection, and application of model info sheets; systematic audits found leakage in reproduced studies.",
            "measurement_method": "Empirical audits: the paper cites an empirical study of 12 ML prediction papers where a third exhibited data leakage (i.e., ~33% in that small sample); radiomics review also documents widespread problems with data sharing and provenance.",
            "impact_on_results": "Data leakage inflates reported performance and produces irreproducible results when proper splits are used; leads to validity shrinkage and overoptimistic claims.",
            "frequency_or_prevalence": "Not rare: cited study found leakage in ~1/3 of a 12-paper sample; broader literature indicates data leakage is a common methodological error.",
            "root_cause": "Lack of ML expertise among domain researchers, insufficient documentation of data splits and preprocessing, and no standard use of model info sheets or rigorous checks.",
            "mitigation_approach": "Use model info sheets and model cards to document splits and legitimate features, adopt standardized datasets/splits, deploy checklists and peer-review checks focused on leakage, and automated tools to detect leakage patterns.",
            "mitigation_effectiveness": "Model info sheets could have prevented all leakage cases in the cited 12-paper study (authors' claim) but adoption is limited; checklists and standardized datasets help but require community uptake.",
            "domain_or_field": "Machine learning; biomedical ML; predictive modeling",
            "reproducibility_impact": true,
            "uuid": "e485.7",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Model info sheets / detection",
            "name_full": "Model info sheets as documentation to detect data/code-description gaps",
            "brief_description": "Structured documentation (model info sheets) requiring detailed answers about data provenance, splits, and design choices can reveal mismatches between natural language descriptions and actual implementations, especially for data leakage.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Documentation and reporting layer for ML experiments",
            "system_description": "Supplementary structured forms (model info sheets) published alongside papers to capture granular details about datasets, preprocessing, splits, and intended use.",
            "nl_description_type": "model info sheet (structured documentation)",
            "code_implementation_type": "supporting metadata and explicit answers about implemented procedures",
            "gap_type": "documentation-driven detection of mismatches / methodological checklist",
            "gap_description": "Model info sheets prompt authors to disclose details (train/test splits, leakage risks, feature legitimacy). When completed and compared with code, they can expose gaps where code uses non-legitimate features or improper splitting not mentioned in text.",
            "gap_location": "data provenance; training/testing splits; feature legitimacy",
            "detection_method": "Manual review of model info sheets vs code; use in reproducibility audits revealed previously hidden leakage or mis-specifications.",
            "measurement_method": "Empirical comparison in a small study: the paper cites that an empirical study of 12 papers would have had leakage preventable by model info sheets (i.e., all leakage cases detectable post-hoc), though verification requires reproduction.",
            "impact_on_results": "Increases transparency and enables quicker detection of methodological flaws leading to irreproducible claims; practical effect limited by adoption and reviewer expertise.",
            "frequency_or_prevalence": "Adoption limited currently; leakage itself is relatively common but model info sheet use remains rare.",
            "root_cause": "Lack of standardized, enforced documentation and limited ML expertise among authors/reviewers.",
            "mitigation_approach": "Encourage/require model info sheets with submissions; integrate these into review and publication workflows; pair with automated checks where possible.",
            "mitigation_effectiveness": "Promising but not widely adopted; cited small-sample evidence suggests they could have detected all leakage cases in one study, but broader quantitative evidence not provided here.",
            "domain_or_field": "Machine learning; biomedical ML",
            "reproducibility_impact": true,
            "uuid": "e485.8",
            "source_info": {
                "paper_title": "Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sources of irreproducibility in machine learning: A review",
            "rating": 2,
            "sanitized_title": "sources_of_irreproducibility_in_machine_learning_a_review"
        },
        {
            "paper_title": "Leakage and the reproducibility crisis in machine-learning-based science",
            "rating": 2,
            "sanitized_title": "leakage_and_the_reproducibility_crisis_in_machinelearningbased_science"
        },
        {
            "paper_title": "Do machine learning platforms provide out-of-the-box reproducibility?",
            "rating": 2,
            "sanitized_title": "do_machine_learning_platforms_provide_outofthebox_reproducibility"
        },
        {
            "paper_title": "Reproducible automl: An assessment of research reproducibility of no-code automl tools",
            "rating": 2,
            "sanitized_title": "reproducible_automl_an_assessment_of_research_reproducibility_of_nocode_automl_tools"
        },
        {
            "paper_title": "Managing Randomness to Enable Reproducible Machine Learning",
            "rating": 1,
            "sanitized_title": "managing_randomness_to_enable_reproducible_machine_learning"
        },
        {
            "paper_title": "Toward training reproducible deep learning models",
            "rating": 1,
            "sanitized_title": "toward_training_reproducible_deep_learning_models"
        }
    ],
    "cost": 0.0179945,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers
26 Feb 2025</p>
<p>Harald Semmelrock h.semmelrock@alumni.tugraz.at 
Graz University of Technology
GrazAustria</p>
<p>Tony Ross-Hellauer 
Know Center Research GmbH
GrazAustria</p>
<p>Simone Kopeinik skopeinik@know-center.at 
Know Center Research GmbH
GrazAustria</p>
<p>Dieter Theiler dtheiler@know-center.at 
Know Center Research GmbH
GrazAustria</p>
<p>Armin Haberl armin.haberl@uni-graz.at 
University of Graz
GrazAustria</p>
<p>Stefan Thalmann stefan.thalmann@uni-graz.at 
University of Graz
GrazAustria</p>
<p>Dominik Kowald dkowald@know-center.at 
Know Center Research GmbH
GrazAustria</p>
<p>Graz University of Technology
GrazAustria</p>
<p>Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers
26 Feb 2025DA07383C54A4B4B45FF56D738941C6EFarXiv:2406.14325v3[cs.SE]Machine LearningArtificial IntelligenceReproducibilityIrreproducibility
Many research fields are currently reckoning with issues of poor levels of reproducibility.Some label it a "crisis", and research employing or building Machine Learning (ML) models is no exception.Issues including lack of transparency, data or code, poor adherence to standards, and the sensitivity of ML training conditions mean that many papers are not even reproducible in principle.Where they are, though, reproducibility experiments have found worryingly low degrees of similarity with original results.Despite previous appeals from ML researchers on this topic and various initiatives from conference reproducibility tracks to the ACM's new Emerging Interest Group on Reproducibility and Replicability, we contend that the general community continues to take this issue too lightly.Poor reproducibility threatens trust in and integrity of research results.Therefore, in this article, we lay out a new perspective on the key barriers and drivers (both procedural and technical) to increased reproducibility at various levels (methods, code, data, and experiments).We then map the drivers to the barriers to give concrete advice for strategies for researchers to mitigate reproducibility issues in their own work, to lay out key areas where further research is needed in specific areas, and to further ignite discussion on the threat presented by these urgent issues.</p>
<p>Introduction</p>
<p>Trustworthy AI requires reproducibility [70].Unreliable results risk hindering scientific progress by wasting resources, reducing trust, slowing discovery, and undermining the foundation for future research [84,45].However, many scientific fields currently face crucial questions over the reproducibility of research findings [10].Concerns of a "reproducibility crisis" have been most prominently raised in biomedical [40,59,60,35] and social [88,27,51] sciences, but research employing Artificial Intelligence (AI) in general, and Machine Learning (ML) in particular, is also under scrutiny [57].ML is becoming ever more deeply integrated into research methods, not just in computer science but across disciplines [32,89].Indeed, recipients of the 2024 Nobel Prizes for both chemistry and physics included ML researchers.Hence, issues regarding the reproducibility of ML raise urgent concerns about the reliability and validity of findings not only for computer scientists but for large swathes of cutting-edge scientific research across disciplines.</p>
<p>The causes of poor reproducibility can be technical, methodological, or cultural.Viewed from a high level, some causes, such as lack of sharing data and code, lack of or poor adherence to standards, suboptimal research design, or poor incentives, may be seen as common to many domains.Apart from the common challenges faced by other disciplines, the use of ML introduces unique obstacles for reproducibility, including sensitivity to ML training conditions, sources of randomness [101], inherent nondeterminism, costs (economic and environmental) of computational resources, and the increasing use of Automated-ML (AutoML) tools [67,48].Among the methodological and cultural aspects, specificities of ML research, like "data leakage", as well as ML-specific issues regarding unobserved bias, lack of transparency, selective reporting of findings, and publishing cultures, each play a role as well.Indeed, this cultural aspect must not be underestimated.The culture of "publish or perish" pervades academia, pushing researchers to publish as many papers in the highest-ranked or most prestigious journals or conferences as possible [95].In turn, this culture distorts incentives towards corner-cutting, giving rise to so-called "questionable research practices" and "design, analytic, or reporting practices that have been questioned because of the potential for the practice to be employed with the purpose of presenting biased evidence in favor of an assertion" [11].</p>
<p>This paper aims to provide a detailed overview of reproducibility and its associated barriers and drivers in ML.This is urgently needed since, despite previous appeals from ML researchers on this topic, various initiatives from conference reproducibility tracks to the ACM's new Emerging Interest Group on Reproducibility and Replicability, and an expanding literature on the topic [78,46,53,64,43,5,108,44], we contend that the general community continues to take this issue too lightly.In addition, despite the growing literature, no such comprehensive overview exists.For example, in [44], the authors identify and categorize sources of irreproducibility in ML and how these sources affect conclusions drawn from ML experiments.However, this study does not investigate the drivers to address these sources of irreproducibility.Thus, our paper provides a contextual categorization of the barriers and drivers to the four types of ML reproducibility (description, code, data, and experiment) proposed by [43], with specific reference to research in both computer science and biomedical fields.We also propose a Drivers-Barriers-Matrix to summarize and visualize the results of the discussion.Such an analysis stands to clarify the current state regarding ML reproducibility, to give concrete advice for strategies for researchers to mitigate reproducibility issues in their own work, to lay out key areas where further research is needed in specific ar-eas, and to further ignite discussion on the threat presented by these urgent issues.The paper is structured as follows: in Section 2, we clarify terms and working definitions.We then analyze the barriers to increased reproducibility of ML-driven research (Section 3), and next, the drivers that support ML reproducibility, including different tools, practices, and interventions (Section 4).Here, we also provide a comparison of the strengths and potential limitations of these drivers.Finally, we map the barriers to the drivers to help determine the feasibility of various options for enhancing ML reproducibility (Section 5).We close the paper with a conclusion and an outlook into our future research in Section 6.</p>
<p>Defining Reproducibility</p>
<p>The concept of reproducibility can have different interpretations across various research fields and even within the same field [39].To avoid confusion, we first specify our terms, broadly defining reproducibility and then further categorizing it into various types and degrees.The first distinction comes from Goodman et al. [42], who specify a fundamental division between whether we (i) mean reproducible in principle (termed "methods" reproducibility) due to sufficient description/sharing of methodologies, materials, etc., or (ii) whether results/conclusions actually prove to be reproducible when experiments or analyses are re-done.In the second category, they distinguish "results" and "inferential" reproducibility, depending on whether the analyses or inferences to broader conclusions are reproduced.</p>
<p>Within ML research, widely accepted definitions that build further on these key distinctions have been proposed by Gundersen et al. [46,43].We follow and build upon these latter definitions, and hence, here outline them at some length.Gundersen et al. [44] define reproducibility in general as "the ability of independent investigators to draw the same conclusions from an experiment by following the documentation shared by the original investigators".Relating to point (ii) of Goodman et al.'s schema, Gundersen and colleagues [46,43] further distinguish the targets of reproducibility, i.e., how closely an experiment can be reproduced:</p>
<p>-Outcome reproducibility requires the reproduced experiment to have the same or adequately similar outcome as the original experiment.Due to this, the same analysis and interpretation follow, and the hypothesis is either supported or rejected by both experiments.-Analysis reproducibility does not require the reproduced experiment to have the same/similar outcome; however, if the same/similar analysis and, therefore, also interpretation can be made, an experiment is analysis reproducible.-Interpretation reproducibility does not require the reproduced experiment to have the same/similar outcome nor analysis but requires the interpretation to be the same as the original one.</p>
<p>Fig. 1: Types of reproducibility.Adapted from Gundersen [43].</p>
<p>This categorization aims to overcome the problem of ambiguity when making specific claims about the reproducibility of an experiment.Often in literature, authors write about reproducing the same "results" of an experiment.It is not apparent, however, in which cases they mean to achieve the same computational outcome, i.e., outputs of the algorithms, or whether they mean to reach the same analysis or interpretation.Therefore, achieving interpretation reproducibility is a more general and often less stringent requirement than achieving outcome reproducibility.This categorization is not specific to ML, but is generally applicable to any research field that conducts data analysis and interpretation.</p>
<p>In addition, relating to point (i) of the Goodman et al. schema ("methods" reproducibility), [43] specifies reproducibility types to which methods can be made transparent through description or sharing.These four types are defined as R1 Description, R2 Code, R3 Data, and R4 Experiment.The lower the level of reproducibility, the less shared information is shared, making the study more difficult to reproduce.As an example, in general, all published research experiments are accompanied by a textual description of the experiment.If this textual description is the only information shared by the authors, the research is categorized as R1 Description, which, according to this scheme, is the minimal kind of reproducibility.In contrast, if all three building blocks, i.e., text, code, and data, are shared, the experiment can be categorized as R4 Experiment, the most expansive kind of reproducibility.Furthermore, the distinction between R2 Code and R3 Data is defined by whether the textual description is accompanied by either code or data, respectively.Also, the different types of ML reproducibility exhibit an interplay between generalizability and reproducibility.R1 Description leads to strong generalizability but to weak reproducibility, while R4 Experiment leads to stronger reproducibility but weaker generalizability.What this means in sum is that rerunning the same code on the same data using the same description will make it likelier to obtain the same results, but those results might still be wrong due to errors or biases in any of those elements.On the other hand, building code from scratch and using alternative datasets for analysis will show that the techniques give similar results across contexts and, hence, higher levels of confidence in the generalizability of findings.These relationships are illustrated in Figure 1.In what follows, unless stated otherwise, we use the definitions proposed by Gundersen and colleagues [43].</p>
<p>Barriers in ML Reproducibility</p>
<p>Next, we discuss nine barriers to ML reproducibility, categorized into the four types of reproducibility mentioned beforehand.Where applicable, we give examples within the research fields of biomedical science and computer science.</p>
<p>R1 Description</p>
<p>Completeness and quality of reporting.Research often lacks reproducibility due to missing or vague methodological details.Mainly, there are three issues in this regard, which often hinder the reproduction of study results [93]:</p>
<p>1.The ML model or training procedure is either incorrectly specified or underspecified.Reports should give clear details on all steps of the procedure, even if data and code are not shared.This includes details about which ML models are used, as well as details on the training data and data preprocessing.2. The evaluation metrics used to report results are not properly specified.</p>
<p>There are many metrics which can be used to evaluate ML models, e.g., accuracy, receiver-operator-curve (ROC), or mean-squared-error (MSE).It is important to define these metrics and also explain why they were used.3. Often results are selectively reported, e.g., researchers may only provide results for the best test run out of many test runs, instead of properly assessing and reporting average values and variances [14].</p>
<p>Generally, it is important for studies to use a robust methodology and provide detailed reports so that other researchers can verify results and understand how analyses were conducted.While ML models have proven highly effective in biomedical fields, studies often fall short in providing comprehensive and highquality reporting.For example, in studies on predicting cardiometabolic risk from dietary patterns [90] or supporting the clinical management of diabetes [63], ML models were observed to be very promising.While this further enhances the promise of applying ML models for various clinical prediction tasks, there is a clear need for thorough reporting and validation of these models to allow for their integration into routine clinical care [63].This is also true for the application of ML models for cancer imaging, where ML models often surpass radiologists in performance, but publications on these models lack the documentation details needed to reproduce the results [97].</p>
<p>Spin practices and publication bias.Another issue commonly observed in ML-based research that negatively affects reproducibility is "spin".It refers to the misuse of language to "intentionally or unintentionally affect the interpretation of study findings".It is also understood as an inconsistency between the study results and the conclusions, in the sense that results are over-generalized or the claimed conclusions are not supported by the scientific method.This has been shown to impact both the interpretations and decision-making by readers [6].In ML-based biomedical research, the most common practice of spin includes recommending models for various applications without providing external validation in the same study.More concretely, the recommendation to use a model either in a clinical setting or for a different population is only validated in approximately 15% of the cases.Other observed instances of spin are invalid comparisons of results to previous studies and the use of leading words and strong statements to make the results sound more significant [6].The prevalence of spin can perhaps be attributed in large part to the academic culture of "publish or perish" and its associated reward systems.Valuing and rewarding perceived novelty and potential impact over basic rigor and responsible reporting can lead researchers to inflate claims in hopes of acceptance in the most prestigious venues.It can also skew the literature in other ways, leading to so-called "publication bias" [105].</p>
<p>Here, in addition to spin and the aforementioned selective reporting of (usually positive) results, the role of the peer review system is also in question, given known biases on the part of reviewers that can lead to preferential treatment for researchers from specific regions, institutions or demographics, or for certain types of research [73].Other kinds of bias, such as "complexity bias" (tendency to prefer complicated over simple results and explanations), are also known to influence acceptance decisions [115].Finally, we note how a core aspect of computer science culture may exacerbate these issues, namely the importance of conference rankings.Within computer science, the prime mode of publication is within conference proceedings, with conferences ranked A* to C by bodies such as ICORE4 .To date, surprisingly little has been said regarding the analogous nature of such conference rankings to other metrics like the Journal Impact Factor, where a rich literature exists critiquing its worth as an indicator of quality or impact for individual pieces of work [72].Although elaboration at length on this issue is outside the scope of this article, we suggest this as an underexplored topic for future research.Such research can build on a rich evidence-base exploring downstream ill-effects of badly designed or misused metrics, including distortion of incentives [110], inviting manipulation or gaming [15], goal displacement and task reduction [102], and influencing core academic values [20].</p>
<p>R2 Code</p>
<p>Limited access to code.Published ML research is often not accompanied by available data and code.Only one-third of researchers share data, and even fewer share their source code [58].This can be attributed to several factors, such as the increasing pressure on researchers to publish quickly, which often leaves insufficient time to refine code and decreases the willingness to share it.Additionally, concerns about intellectual property may further discourage researchers from releasing their code.According to Gundersen and Kjensmo [45], sharing ML code to facilitate reproducibility requires publishing seven pieces of information: hypothesis, prediction method, source code, hardware specifications, software dependencies, experiment setup, and experiment source code.Unfortunately, current research rarely meets these requirements, leading to reproducibility issues due to different software versions, hyperparameter settings, or hardware differences [54,14].For example, research in recommender systems has struggled with the lack of shared code, significantly contributing to a lack of reproducibility.Even when code is shared, it is often incomplete, poorly documented, or limited to pseudocode or skeletal implementations rather than fully executable code [29].To address this, shared code should encompass comprehensive documentation, including scripts for data preprocessing, hyperparameter tuning, management of random seeds, and implementations for comparisons against baseline models.</p>
<p>R3 Data</p>
<p>Limited access to data.The main reproducibility barrier associated with R3 Data is that data is simply not shared or made publicly available most of the time [58]).A review in biomedical research, specifically radiomics, investigated 257 recent ML publications and found that only 16 of them shared data or used publicly accessible datasets [33].This could be due to privacy concerns or a lack of incentives and motivation.Moreover, many benchmark and training datasets encounter challenges related to copyright, licensing, and longevity.These datasets may also raise ethical concerns, such as the unintentional inclusion of privacy-sensitive or harmful content, making it difficult to share the data for ML model training [91].Similar to the sharing of source code, sharing only the datasets is insufficient without sufficiently detailed levels of documentation.For proper use, it is also important to share specific splits, i.e., the training dataset, validation dataset, and test dataset [45].Furthermore, data sharing needs to be accompanied by documentation specifying details about the provenance and preprocessing of data.Significant recent initiatives aiming at improvement here are: Croissant, a unified format for machine learning datasets that integrates metadata, resource descriptions, data structure, and default ML semantics in a single file 5 , and MLCommons, which is working towards open benchmarks and public data 6 .In addition, RO-Crate is a standard for packaging data and other research objects together with data to enable reuse and reproducibility 7 .</p>
<p>Standardising and mainstreaming these practices is essential for validation and checking of methods.We next discuss two common methodological errors related to data, data leakage and bias, and their impact on reproducibility.</p>
<p>Data leakage.In practice, methodological issues such as data leakage (also referred to as target leakage) often hinder the reproducibility of ML-based research [64].This is due to the growing number of non-experts employing machine learning across different research fields [41], which is fueled by the ease of application of ML libraries and no-code off-the-shelf AI tools.In essence, data leakage happens when data on which the ML model should not be trained leaks into the training process.Data leakage can be categorized into 3 subcategories [64]:</p>
<p>1.No clean train/test split.Here, four variants are possible: (1) training data and test data are not split at all, (2) test data is also used to select the best features from the training data (feature selection), ( 3) test data is also used for imputation of missing data during preprocessing, and (4) duplicates occur in the training and test data.2. Use of non-legitimate data.For example, when the use of antihypertensive drugs is used as a feature to predict hypertension.This data is nonlegitimate, since it would not be available in a real-world scenario (people are prescribed those drugs because of a hypertension diagnosis) and would be useless for predicting hypertension in undiagnosed patients.3. Test set is not drawn from the distribution of scientific interest.There are three possible variants: (i) temporal leakage, which is problematic for ML models that attempt to predict future outcomes, i.e., when some training samples have a later timestamp than samples available in the test set, (ii) the training and test data are not independent of each other, e.g., there should not be samples in the training and test data that are drawn from the same person, and (iii) the test set is not chosen selectively; for instance, if the model is solely evaluated on data, on which it performs well.</p>
<p>Bias.Bias in ML refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model, often leading to systematic deviations from the real world.Furthermore, biases can arise when the model contains imbalances or reflects existing societal biases [79].ML models, which are subject to bias, are prone to generalization issues, and are therefore potentially problematic for ML reproducibility.There are eight kinds of bias that can arise during the data handling phase of ML development [104]: (i) selection bias -using data not being representative of the target group, (ii) exclusion bias -excluding particular data samples based on the belief that they are unimportant, (iii) measurement bias -favoring certain measurement results, (iv) recall bias -labeling similar data samples differently, (v) survey bias -introducing data issues stemming from data collection surveys, (vi) confirmation bias -favoring information, which confirms previous beliefs, (vii) prejudice bias -including human-related prejudices in training data, and (viii) algorithmic bias -replicating or amplifying biases by the inner workings of the algorithms.Bias, such as selection bias, often leads to the issue of validity shrinkage in biomedical science research [61].For example, in obesity and nutritional research, ML is used to predict obesity, heart rate, or the risk of a heart attack based on data from an individual.Here, validity shrinkage refers to the issue that a predictive model trained on a subset of data will most probably not perform well on new samples.The difference between predictive performance on known data and new data, however, is most often not accounted for in nutritional science, and therefore also leads to performance claims that cannot be reproduced [61].</p>
<p>R4 Experiment</p>
<p>Inherent nondeterminism.Inherent nondeterminism in ML models means that results can vary between test runs, even with identical code, data, and hyperparameters.This variation arises from sources of randomness during training, such as, e.g., random parameter initialization, stochastic optimization, and random data subsampling (e.g., in k-fold cross-validation) and the complex interactions between them, which are fundamental characteristics of most ML workflows [101,74].Neural networks are especially known for their inherent nondeterminism, leading to varied computational outcomes in multiple reruns due to increased sources of randomness during training [1].In some cases, inherent nondeterminism can cause such large variations that reruns not only yield slightly different outcomes, but also lead to significant fluctuations in the performance of an ML model [1] or to varying conclusions in ML model comparisons [47].This issue is exacerbated when other sources of variation, such as different hyperparameters, are introduced to the ML model.In such cases, the impact can be magnified, and it is often observed that minor changes in hyperparameters can result in significant performance loss [14].Reviews of reproducibility in both NLP research [14] and biomedical research [2] highlight these core issues with nondeterminism that are exemplary for ML research because they reflect challenges that are prevalent across ML-based research and serve as representative examples of the broader reproducibility crisis faced by the ML community.Simply rerunning the original code of an experiment during a reproduction leads to large variances of results and different computational outcomes on each run.Reinforcement learning, a subfield of ML, is particularly susceptible to these reproducibility issues, partially because of additional sources of nondeterminism, such as the reinforcement learning environment or policy [85].</p>
<p>Environmental differences.Various studies have demonstrated that hardware differences, such as different GPUs or CPUs, and compiler settings can lead to different computational outcomes [54].Additionally, a comparison between the same ML algorithm with fixed random seeds executed using PyTorch8 and TensorFlow9 resulted in different performances [96].Furthermore, even different versions of the same framework can lead to different performance results [109].A comparison of the results of experiments performed on different hosted ML platforms also found that out-of-the-box reproducibility is not guaranteed there [46].Another important factor is the use of GPUs, which can increase randomness compared to the use of CPUs.This is due to parallel optimization and the use of optimizers in ML frameworks, such as PyTorch and TensorFlow.As a result, some researchers have resorted to solely using CPUs for executing their experiments.However, this comes at the expense of runtime-efficiency [4].</p>
<p>Limited access to computational resources.The barrier to ML reproducibility posed by limited access to computational resources has recently become evident in the case of transformer-based Large Language Models (LLMs) [13].</p>
<p>These transformer architectures need a vast amount of data and computational resources, to which most researchers have limited access.Estimates have calculated the costs to reproduce one model to be around $1 million to $3.2 million [13].Another study found that the needed computational resources are one of the most significant factors impacting reproducibility [100].Especially ML models, which require computational clusters for training and optimization, are notably hard to reproduce.</p>
<p>Drivers for ML Reproducibility</p>
<p>In this section, we discuss drivers for ML reproducibility, which we subdivide into (i) technology-based drivers, (ii) procedural drivers, and (iii) drivers related to awareness and education.For every driver, we also provide case studies or examples from the literature illustrating the effectiveness of the driver for ML reproducibility.</p>
<p>Technology-based Drivers</p>
<p>Hosting services.Utilizing hosting services offers an efficient way to share code, data, and ML model parameter settings, thus supporting the reproducibility of ML-driven research [114].Examples of hosting services include the runtime environments of ML platforms.If the original author runs the ML experiment in such a runtime environment, e.g., Kaggle Notebooks10 , Google Colab11 , or CodaLab 12 , researchers attempting to reproduce the results should be able to execute the experiment within the same environment.The main advantage of using a hosting service is that the provider takes care of the logistics of code hosting and distribution.However, the main drawbacks are the limits on data size and computational resources.Since these hosting services are run in the cloud, there are restrictions on how many resources a single user can utilize.The limit on resources varies between different hosting services and is limited by users' available funds and sometimes subscription levels.Because of these limits, hosting services may not be suitable for all research purposes, especially considering the compute-intensive nature of novel ML models, such as LLMs.</p>
<p>As said above, the degree to which such services offer out-of-the-box reproducibility remains highly questionable [46].Nevertheless, some hosting services have effectively been used to create end-to-end reproducible AI pipelines, especially in conjunction with standardized datasets such as the National Cancer Institute Imaging Data Commons [37].The effectiveness of hosting services for ML reproducibility has been shown for research in radiology [18] and similar reproducibility experiments have been successfully conducted in pathology research [106].As also noted in these experiments, it is important that results are reported with a quantification of the variance across different test runs, since effects of randomness could still be prevalent.</p>
<p>Virtualization.Reproducing the environment and setup of any ML experiment requires the consideration of existing dependencies and software versions, and is usually a complex task itself.Virtualization can simplify this process by bundling the essential components of ML models and experiments, such as the dependencies and code, into a single package for sharing with other researchers.Thus, if the authors of a paper build the experiment in a virtual environment, issues associated with setup reproduction can be greatly reduced.However, the adoption of virtualization by researchers depends on its user-friendliness and the effort of integration into their current workflows [17].Concerns about virtualization include its limitations in allowing researchers to build upon them in a scalable manner.Traditional virtual machines (VMs) emulate an entire operating system for setting up and running experiments.The use of containerization software like Docker 13 has become more popular in recent years.Containers are more lightweight and flexible than VMs, making it easier to adapt environments for follow-up studies [17].There are also designated platforms for computational research, such as Code Ocean 14 , that offer virtualization via so-called reproducible capsules.Their focus, in particular, is to simplify the virtualization process and allow researchers to focus on the research itself rather than the standardization of environments [25].Additionally, there are many other tools, such as ReproZip [23], one of the recommended tools by the SIGMOD Reproducibility Availability and Reproducibility Initiative15 to streamline reproducibility, and DetTrace [86], which aims to ensure completely deterministic computations.</p>
<p>The use of containers is rapidly gaining in popularity across many research fields, e.g., neuroscience and genomics [82].Notably, the platform Code Ocean has been integrated into the peer reviewing process by Nature journals to support the submission process of experiments [34].This widespread adoption highlights the suitability of containers to enhance experiment sharing and improve reproducibility.Furthermore, a case study has compared ten different containerizationbased approaches for reproducibility [24].The strengths and weaknesses of each approach were analyzed, with results demonstrating the suitability of containers to enhance reproducibility by encapsulating the computational environment and to decrease the effort for publishing reproducible ML-based experiments.</p>
<p>Managing sources of randomness.Many different sources of randomness during ML training lead to the irreproducibility of ML research.Managing these sources of randomness, e.g., via random number seeds, deterministic algorithms, or other methods, could therefore greatly increase reproducibility.Fixed random number seeds should be used and published to make ML experiments more reproducible and control a number of sources of inherent nondeterminism.A seed is a first value used to initialize the pseudo-random number generator.When the same seed is used, the sequence of pseudo-random numbers generated is deterministic, meaning it will be the same every time the code is run.Experiments have shown that fixing random seeds can effectively ensure reproducible results when algorithms are not being executed in parallel on GPUs [1].Additionally, one case study has shown that achieving reproducibility for GPU-trained neural networks [21] is possible through a method known as patching.Patching aims to replace non-deterministic operations with deterministic ones.In the case study, a systematic patching approach was successful in achieving reproducible image classification results for six different neural network.However, this process also leads to higher computational costs and a time overhead, which was also analyzed in the study.Additionally, ML models should be benchmarked and evaluated with multiple random number seeds, such that the variance can be reported and inform about the true performance of an ML model [101].Similarly, the use of uncertainty-aware quantification metrics to evaluate ML models can also help increase reproducibility [96].</p>
<p>Additionally, to counteract inherent nondeterminism in reinforcement learning and achieve reproducible evaluations, there exist frameworks, such as Gym-Ignition [38,19], rl_reach [9], or MinAtar [121], which act as standardized benchmarking environments.Within them, different algorithms designed for the same task can be evaluated and compared against each other in a common environment.Some frameworks counteract the effects of inherent nondeterminism by automatically controlling random seeds and evaluating algorithms over a number of runs.Finally, it is an ongoing field of research to implement fully deterministic reinforcement learning algorithms [85] and make use of them within such frameworks [113].</p>
<p>Privacy-preserving technologies.Privacy-preserving technologies support reproducibility, as they enable the collaborative training of ML models without sharing private or sensitive data.The main benefit of this is, that ML models can make use of larger and more diverse data, thus helping to decrease bias and leading to more reproducible ML models.The main aim of Privacy-Preserving Machine Learning (PPML) is to facilitate the use of privacy-sensitive data to create better ML models, and, to allow data owners to collaboratively train ML models on private data.In that regard, PPML has several requirements.First, protecting the confidentiality of the training data.Second, preventing the leak-age of sensitive information from ML model parameters and outputs, i.e., to hinder the re-identification of individuals.Third, achieving the listed security and privacy aspects while still preserving the utility of the ML model [119].To achieve this, a number of different techniques are being used and developed, mainly Differential Privacy (DP), Homomorphic Encryption (HE), Secure Multi-Party Computation (SMPC), and Federated Learning (FL) [30].These techniques are implemented in software libraries such as TensorFlow Privacy, PySyft, ML Privacy Meter, CryptFlow, or Crypten [8].Furthermore, data can be made anonymous by removing identifiable personal information.However, if too much data is removed, the ML models may perform poorly.If not enough data is removed, it may still be possible to re-identify individuals by combining many different non-unique features [119].</p>
<p>An alternative approach to PPML is to generate synthetic data that captures the same information as the original data.A robust technique for creating such datasets can produce readily available datasets of nearly any size, as demonstrated in biomedical fields.This approach has led to the development of Synthea, a software package designed to generate synthetic patient data and electronic health care records [116].It is, however, important to mention that there is still a gap in efficiency between theoretical advancements and real-world applications when using PPML techniques.To this end, one study conducting reproductions of 26 state-of-the-art applications of PPML has highlighted the challenges of balancing computational efficiency, privacy guarantees, and model utility, while emphasizing the need for improved reproducibility, open-source availability, and practical scalability [65].</p>
<p>Tools and platforms.There are many tools and platforms that assist in the implementation and management of ML models and ML-based applications.A recent study has evaluated 19 ML tools to gain insights into their concepts constituting reproducibility support [99].As a result, five main pillars of ML reproducibility in tools and platforms were identified: (i) code versioning, (ii) data access, (iii) data versioning, (iv) experiment logging, and (v) pipeline creation.Most of these pillars are associated with managing and keeping track of different artifacts created during phases of the ML lifecycle (i.e., design, development, and deployment) as for instance, datasets, labels, code, logs, environment dependencies, random number seeds, or hyperparameters [107].Each of these artifacts influences the final results of the ML model.Consequently, most tools aim to collect, store, and manage these artifacts, ensuring researchers can access and use them during reproduction attempts.Notable are also various tools and platforms for experiment tracking [107], such as:</p>
<p>-DVC 16 : A version control system for ML projects with a command-line interface similar to Git AutoML platforms, such as H2O Driverless AI20 , Google Cloud AutoML21 , DataRobot 22 ) are a novel subcategory of ML tools that aim to aid with every aspect of the ML lifecycle, from data aggregation to model deployment.Thus, AutoML tools could facilitate more standardized ML models and also take care of tasks like hyperparameter optimization.It is, however, questionable how practical these tools are for reproducible ML research, since they often hide ML model optimization procedures.Recent assessments of the reproducibility of AutoML tools also came to the conclusion that current platforms cannot provide out-of-the-box-reproducibility [46,94].In a qualitative analysis of the reproduction experiments, the latter study did identify areas in which such tools can be enablers for reproducibility, e.g., due to their automatic documentation capabilities.However, the authors also identified aspects that need to be addressed, such as the need for simplified tool user interfaces -as many participants were overwhelmed by tool complexity and could not make use of the documentation capabilities -and more built-in reproducibility capabilities, which support the sharing of code and data [94].Furthermore, some AutoML tools, such as H2O Driverless AI, aim to address problems such as model overfitting.In the case of data leakage, this is done by checking for a strong correlation between a feature and the target and then taking action, e.g., warning the user or automatically handling it.This is, however, a very simple solution to the problem and does not address the more complex cases of data leakage that are often present in research, e.g., temporal leakage.</p>
<p>Procedural Drivers</p>
<p>Standardized datasets and evaluation.Due to a lack of shared datasets, many researchers in ML-driven research -most notably in biomedical fieldshave to use individually acquired data [78].The collection of such data is a time-consuming task and bears a significant risk of causing reproducibility issues, e.g., bias or data leakage.Often, the number of individual participants represented within datasets is not very large and, thus, findings might suffer poor generalizability.Creating shared and standardized datasets can, therefore, (i) save researchers time in acquiring new data, (ii) facilitate the collaborative and independent maintenance and verification of data to minimize methodological errors, and (iii) support transferability and generalizability through the use of multi-institutional data [78].In addition to the standardization of datasets, data cards [98] provide a consistent and comparable framework for reporting essential aspects of ML datasets.This includes information, e.g., about access restrictions, risks and limitations associated with the usage of the dataset, or any preprocessing steps, amongst many other contents, which are needed for reproducible ML development.</p>
<p>Another issue is the lack of standardized evaluation methods, which leads to reported performances of ML models often being overly optimistic [29].To ensure the statistical significance of ML model evaluations, it is crucial to report performance as an aggregate of results obtained from multiple random runs, and with different random number seeds [26].Furthermore, ML models should, if possible, be tested and evaluated on multiple different datasets [31].This underscores the need for standardized evaluation methods, which can be supported by checklists or tools to prevent errors in this critical aspect of ML research.For this, similarly to data cards, model cards [80] are aimed to standardize the evaluation and reporting of the performance of ML models for a variety of use cases.Model cards should inform users about the possible applications of the ML model and its limitations.In 2020, Google introduced the Model Card Toolkit for the creation of model cards 23 .In reinforcement learning, the creation of standardized evaluation pipelines is continually being researched to enable reproducible benchmarking of different reinforcement learning algorithms [66].</p>
<p>The National Cancer Institute Imaging Data Commons is an established example of standardized datasets in biomedical research [37].As a cloud-based repository, it contains a collection of cancer imaging data and has been used successfully in reproduction experiments in combination with hosting services.</p>
<p>Other notable examples include the MIMIC [62] database for electronic health records, or OGB [56] for applying ML on graph data.Efforts are also being invested into increasing the reproducibility of language models, e.g., with the Holistic Evaluation of Language Models (HELM) [75], which offers a broad, scenario-diverse, and multi-metric benchmarking suite for language models.As demonstrated by the authors, using HELM, new language models can be evaluated in a more comprehensive way.Furthermore, the Language Model Evaluation Harness (lm-eval) toolkit [16] is a framework designed for language model evaluations and concerned with reproducibility aspects.This tool has already been used by other researchers for more reproducible language model evaluations [36,71].However, it is important to recognize how irreproducible most major models currently are.There exists a live-tracker of model openness 24 , which has reported that many projects, even those claiming to be open source, "inherit undocumented data of dubious legality", that few projects share data or model or human reinforcement learning (RLHF) weights, and that "careful scientific documentation is exceedingly rare" [76].</p>
<p>Guidelines and checklists.There are many guidelines and checklists that outline best practices for increasing the reproducibility of ML.The guidelines are often aimed at specific parts of the ML workflow.For example, the FAIR principles 25 aim to improve the management and stewardship of scientific data by making scientific data findable, accessible, interoperable, and reusable.Other guidelines promote the transparency and openness of scientific reporting in general, such as the TOP guidelines 26 , which target journals.Similarly, checklists provide a simple framework for ensuring certain criteria are met.Checklists have been applied effectively in the past, e.g., in safety-critical systems, where they were used as early as in 1935 to complete pre-flight checks in Boeing airplanes.A promising example is the ML checklist proposed in [93], which has been suggested as best practice by researchers of different fields, e.g., in chemistry [7].The checklist requests information about (i) the models and algorithms being used, (ii) theoretical claims in the research article, (iii) data, (iv) code, and (v) the ML experiment(s).However, one drawback of reproducibility checklists when used for academic conferences and journals is the additional workload they impose on already overburdened reviewers.To mitigate this, one suggestion is to leverage LLMs to assist the review process [77].</p>
<p>Finally, numerous guidelines and checklists for ML reproducibility have been recommended in various research fields [7].Especially in biomedical fields, there has been a considerable adoption of guidelines and checklists, such as the TRI-POD statement [28], the CLAIM checklist [81], the ROBUST-ML checklist [3], or PROBAST [118].A systematic review in biomedical research has shown that the use of checklists is linked to increased reporting quality [49].The review examined 943 articles over two years and found that mandatory checklists increased the inclusion of the main methodological information needed to reproduce the experiments by 65%.</p>
<p>Model cards and model info sheets.Model cards [80] are documentation sheets that provide information about ML models, including their intended use, potential limitations, and ethical considerations.They aim to enhance transparency in AI, by detailing aspects such as data used for training, performance metrics, evaluation methodologies, and possible biases.Model cards help users to understand and evaluate ML models more comprehensively, such that they are not deployed in unsuited contexts, and thus to increase reproducibility.Similarly, model info sheets also provide documentation about ML models, but are specifically designed for the detection and prevention of data leakage in ML models [64].Model info sheets are published alongside research to enable other researchers to quickly verify the validity of the data used to train ML models.They require authors to answer detailed questions about the data and corresponding train/test splits, targeting various types of data leakage [64].</p>
<p>An empirical study investigating twelve papers, making use of ML methods for prediction, found that a third were subject to some type of data leakage [64], and that in all those cases leakage errors could have been prevented by the use of model info sheets.Despite that, model info sheets have two main drawbacks: first, verifying the correctness of info sheets only works after reproducing the results; second, completing these sheets requires a certain level of expertise in ML.In general, model cards and model info sheets represent a promising, loweffort driver for ML reproducibility.They are especially useful in handling some of the methodological issues associated with ML models that could arise [64].</p>
<p>Awareness and Education</p>
<p>Awareness of reproducibility issues and available training/education to support reproducibility can be a powerful driver for ML reproducibility [117].</p>
<p>Publication policies and initiatives.To enhance awareness and establish a minimum of reproducibility standards, the policies of scientific journals are considered an influencing factor.A number of journals already mandate data and/or code availability for publication [93,92,50].However, to address issues such as result manipulation, more extensive journal participation is needed to, for instance, introduce preregistration where researchers register their research intentions for future publication.This approach ensures credibility by separating the research plan from experimental outcomes [112,87], thereby reducing spin practices, HARKing, and p-hacking [46].The ACM TORS (Transactions on Recommender Systems) journal exemplifies this by allowing preregistration and publishing "reproducibility papers" dedicated to reproduction studies and enhancing reproducibility tools.Apart from that, various initiatives have been launched to raise awareness of reproducibility issues.A few examples are the following:</p>
<p>-The ReScience journal publishes peer-reviewed papers discussing attempts to reproduce original publications.These reproductions are published on GitHub 27 and available to other researchers [103].-PapersWithCode.com28 is a resource for (i) ML papers, accompanied by the code, (ii) datasets, and (iii) ML methods.The ML papers include a link to a repository, which features the code and other artifacts for reproducing the results.</p>
<p>-Reproducibility challenges, where several researchers try to reproduce many recent publications in parallel, are being held frequently.These challenges allow for an analysis of the success rate of reproduction and can be used to evaluate progress over multiple years [93].Additionally, conferences such as the European Conference on Information Retrieval (ECIR), provides special reproducibility tracks, in which researchers are encouraged to reproduce existing papers and build upon their results (e.g., [69,83,68]).</p>
<p>-The ACM has convened a new emerging interest group on reproducibility 29 .</p>
<p>The main goals are to (i) contribute to the development of reproducibility standards, practices and policies, (ii) promote the development and evaluation of tools and methodologies, and (iii) encourage best practices.-ReproducedPapers.org is another online repository fostering reproductions.</p>
<p>It further focuses on education by incorporating a reproduction project into a Master's level ML course at TU Delft [120].</p>
<p>As also indicated by research in information retrieval and recommender systems, increased awareness and education in the form of publication policies and initiatives can address reproducibility issues by emphasizing robust experimental practices, methodological rigor, and the development of shared resources among the different actors identified, i.e., students, educators, scholars, practitioners and decision-makers [12].</p>
<p>Mapping Drivers to Barriers</p>
<p>In this section, we map the drivers of reproducibility to the barriers in the form of a Drivers-Barriers Matrix.This will be based on the definition and categorization of reproducibility as a foundation (Section 2), and the identification of the major barriers (Section 3) and drivers (Section 4) of ML reproducibility.The resulting Drivers-Barriers-Matrix is depicted in Figure 2 and categorizes the barriers into the four different types of ML reproducibility, i.e., R1 Description, R2 Code, R3 Data, R4 Experiment [43], and drivers into technology-driven drivers, procedural drivers, and drivers related to awareness and education.Our Drivers-Barriers-Matrix shows that there are often multiple drivers for the same barrier.Consequently, there are also several possible solutions for a barrier or different aspects of a barrier.The mapping allows us to quickly assess which drivers address the different barriers and which barriers have a higher or lower number of drivers associated with them.It underlines the need for contextdependent approaches instead of "one-size-fits-all" solutions, as the proper selection of a suitable driver depends on the specific conditions and existing barriers relevant to any ML application.We describe intersections between drivers and barriers in more detail in the following and close this section with an overview of strengths and potential limitations of the identified drivers.Fig. 2: Drivers-Barriers-Matrix.We map the 9 drivers to the 9 barriers identified in this paper.The colored boxes show that a specific driver is applicable to a specific barrier.We argue that drivers related to awareness and education are, in general, applicable to address all barriers.R1 Description.Completeness and quality of reporting, as well as spin practices and publication bias, present the major barriers associated with R1 Description.These are characterized as missing information in reports and overinflated results that hinder reproducibility.</p>
<p>The major drivers for completeness and report quality are guidelines and checklists.Guidelines provide best practices to adopt in order to achieve reproducible ML research.Furthermore, many checklists exist that comprehensively state the different pitfalls and provide information on how they can be avoided.Researchers can use them to ensure their research meets the desired standards.Furthermore, some of these checklists and guidelines are enforced by journals, such that research will only be published if certain criteria are met.In comparison, spin practices are not as easily identifiable.In this case, the discussion within the research community centers around removing the incentives for inflating research results.A particularly effective driver for this is preregistration as an example for publication policies and initiatives, where researchers submit research objectives and methods for review before conducting the research.If accepted, the research will be published regardless of the outcome (i.e., whether results are positive, negative or null), thereby minimizing spin practices.R2 Code.Code sharing is essential to reproducibility, which makes limited access to code a significant barrier in the field.However, it is often neglected as the process is not trivial.To make shared code useful to the scientific community, it is necessary to share, in addition to the source code, the information about the entire software setup and dependencies, including software versions and hardware configurations.To assist with this, researchers may consider running their code in hosting services or virtualization environments, which we identified as drivers for code sharing.Both have similar advantages, i.e., they can easily be shared and made public for other researchers to use.As a consequence, it will give reproducers immediate access to code, including the complete configuration setup, such as dependencies and versions.Hosting services are a quicker and easier way of achieving this; however, they may be subject to different resource limits.Virtualization (e.g., VMs or containers) is more difficult to set up but offers more flexibility and is not externally (e.g., by a provider) restricted in capabilities and resources.Furthermore, tools and platforms can be drivers for reproducibility.A lot of ML tools provide capabilities for code versioning or other features, which are key to reproducibility.One example is dToolAI [52], which automatically logs the supplemental information of the code, i.e., metadata, hyperparameters, and more, which are essential for ML reproducibility.</p>
<p>R3 Data.Data-related barriers are a severe obstacle to ML reproducibility due to the research fields' data-driven nature, where limited access to data forms a major challenge.Privacy concerns are among the crucial arguments that cause hesitation in sharing data.The need for data privacy is evident, especially in biomedical fields, which deal with patients' electronic health records.Nevertheless, it increases reproducibility issues in ML-based science and, thus, delays technological progress within these domains.However, there are several approaches that aim to meet the requirements of sharing sensitive data: Privacypreserving technologies allow reproducers to train ML models on private data without actually possessing the data.This way, reproduction becomes possible without violating potential privacy regulations.Other than that, the use of standardized datasets and evaluation can support issues in regard to dataset metainformation, including the specification of train-test splits and data provenance.Once again, tools and platforms can assist with data versioning, and numerous guidelines and checklists have been proposed to address the provenance of data.These guidelines and checklists are designed to help researchers to avoid common pitfalls.Current initiatives are supported by journals that more frequently require data to be shared as part of a publication.</p>
<p>Concerning methodological errors associated with the data, data leakage is a major issue, which can, for instance, be mitigated using standardized datasets and evaluation.Other drivers to solve data leakage are model info sheets and model cards, which are provided as supplemental information to a published dataset.Even though there are some limitations to model info sheets, they are capable of detecting all types of data leakage.Bias is another methodological error, leading to irreproducible results.This is because the biased data usually does not generalize well to problems outside the experimental setup of a specific ML study.Bias has been an important source of concern, e.g., in biomedical fields.Effects thereof can again be minimized using standardized datasets and evaluation or specific guidelines and checklists, e.g., ROBUST-ML [3].R4 Experiment.If an ML experiment is shared entirely and code and data are available, i.e., reproducibility type R4 Experiment, there are still three barriers, which can lead to irreproducible results.Inherent nondeterminism arises from the different sources of randomness in ML, and makes it difficult to achieve repeatable results, even on the same machine.There are, however, methods to manage the sources of randomness, such as fixed random seeds and deterministic implementations, while comprehensively mitigating all sources of randomness is still a very challenging endeavor.</p>
<p>Another barrier is described as environmental differences, which has two main issues associated with it, i.e., software differences and hardware differences.Both types of differences can be avoided by using either hosting services or virtualization; constraints can be assumed to be similar to the barrier of limited access to code.Limited access to computational resources constitutes another barrier to ML reproducibility identified in this work.The issue is particularly problematic for research using LLMs because of their need for extensive computational resources in training and reproduction.Hosting services offer a solution, providing access to pre-trained models and allowing researchers to directly access and run respective models on-site.Finally, Table 1 gives an overview of strengths and potential limitations of the identified drivers.As we can see, the choice of using a particular driver strongly depends on the given use and to what extent potential limitations are applicable for the use case.</p>
<p>Conclusion</p>
<p>In this paper, we examined the barriers and drivers associated with the four types of ML reproducibility as outlined by Gundersen et al. (description, data, code, and experiment) [43], specifically in the cases of computer science and biomedical research.We synthesized our findings into a Drivers-Barriers-Matrix to summarize and illustrate which drivers are feasible solutions to the various barriers.We observe that the barriers to ML reproducibility can be addressed through three kinds of drivers: technology-driven solutions, procedural improvements, and enhanced awareness and education.It is important to highlight that, in theory, awareness and education can complement the other drivers and serve as a foundational basis for overcoming reproducibility-related challenges.</p>
<p>One of the main issues hindering reproducibility in research appears to be rooted in the cultural aspects of research communities.As argued by [22,12], the current incentives for conducting reproducible research are limited, and open research is often regarded as an unrewarded additional effort.Consequently, there is a lack of training and insufficient funding to cover the additional time and resources required by researchers.Notably, the lack of funding also impacts the ability to perform quality checks during and after the publication process.Therefore, we strongly believe that the way forward towards ML reproducibility is rooted in better education and more awareness of this topic among all involved stakeholders, e.g., students, educators, researchers, publishers, and policymakers.This, combined with the other tools and drivers described in this paper, could lead to more reproducible ML pipelines and, with this, more robust findings.</p>
<p>From a more technical perspective, the rise of AutoML tools for ML development and ML tasks performed by domain experts, (potentially) not having in-depth computer or data science knowledge, could pose another barrier to reproducibility [48].We thus believe future work should address the increasing use of AutoML tools for AI development in research ( [48]) among non-computer or data science experts.While these easy-to-use tools can standardize ML workflows by default and include documentation features, domain experts often lack the necessary expertise to recognize potential problems associated with ML, such as biased or imbalanced data.Thus, research on reproducibility should emphasize this challenge and aim to establish standards and guidelines for the use of No and Low Code ML tools in research, as well as the training required for their responsible application.</p>
<p>In summary, we hope that our paper provides practical guidance and orientation for researchers employing ML and clarifies the current state of play.Of course, in such a dynamic and fast-paced research area, this discussion opens up a series of further questions and avenues for exploration.We recommend further investigation of the various issues and potential solutions laid out here.We would also encourage further investigation into the potential role of platforms [46] or foundation models [55] in further exacerbating or alleviating these challenges.</p>
<p>17.It integrates with Git, supports cloud storage, and handles large versioning of datasets.DVC ensures full code and data provenance by enabling experiment tracking.-MLflow 18: An open-source tool for supporting ML experiment tracking, ML model deployment, and centralized model storage.Additionally, it provides an easy-to-use Web dashboard.-RO-Crate 19 : A specification, implemented by a number of tools, aimed at aggregating and describing research data and metadata [111].Although not specifically designed for ML, RO-Crate can aggregate and represent any resource, making it applicable for managing ML artifacts as well.-dToolAI [52]: Collects and packages ML models together with supplemental information, such as hyperparameter settings, appropriate metadata, and persistent URIs for model training data.In contrast to the other tools, dToolAI is specifically tailored towards Deep Learning models.</p>
<p>Table 1 :
1
Comparison of strengths and weaknesses of our identified drivers.
DriverStrengthsPotential LimitationsHosting ServicesFacilitates sharing of models,Out-of-the-box reproducibilitycode, and datasets; increases ac-is not yet provided and there arecessibility.limits to the available compute.VirtualizationEnables environment replica-Requires technical expertise;tion (e.g., Docker, virtual ma-may introduce overhead for sim-chines); resolves dependency is-ple experiments.sues.Managing sources ofCritical for deterministic out-Can be hard to implementrandomnesscomes; has the potential to re-consistently across frameworks;duce and even eliminate vari-only leads to point estimates ofances across multiple runs.performance.Privacy-preservingExpands access to sensitiveStill an emerging field; per-technologiesdatasets without compromisingformance trade-offs can makeprivacy.widespread adoption slower.Tools and platforms Can streamline reproducibilityOut-of-the-box reproducibilitypractices and automatically ac-is not yet provided, and frag-quire reproducibility artifacts.mentation of tools can lead tosiloed solutions rather than uni-fied workflows.StandardizedProvides consistency and com-May not generalize well to nichedatasets and evalua-parability for results acrossor domain-specific problems andtionstudies.can be subject to privacy con-cerns.Guidelines, checklists PromotesbestpracticesCompliance can be time-through structured processesconsuming and may not be(e.g., reproducibility checklists).enforced consistently.Model info sheetsImproves transparency aroundAdoption is still limited; re-and model cardsmodel design and intended use.quires effort to standardize andmaintain across the community.Publication policies,Drive cultural change by incen-Impact depends on communityinitiativestivizing openness (e.g., bench-participation and is a slow pro-marks, competitions).cess in general.
https://portal.core.edu.au/conf-ranks/
https://github.com/mlcommons/croissant
https://github.com/mlcommons
https://www.researchobject.org/ro-crate/
https://pytorch.org/
https://www.tensorflow.org/
https://www.kaggle.com/code
https://colab.google/
https://codalab.org/
https://www.docker.com/
https://codeocean.com/
https://reproducibility.sigmod.org/
https://dvc.org/
https://git-scm.com/
https://mlflow.org/
https://www.researchobject.org/ro-crate/
https://h2o.ai
https://cloud.google.com/automl
https://www.datarobot.com/platform
https://research.google/blog/introducing-the-model-card-toolkit-for-easier-model-transparency-repor
https://opening-up-chatgpt.github.io/
https://www.go-fair.org/fair-principles/
https://www.cos.io/initiatives/top-guidelines
https://github.com/
https://paperswithcode.com/
https://reproducibility.acm.org/
Semmelrock, Ross-Hellauer, Kopeinik, Theiler, Haberl, Thalmann, &amp; Kowald
Acknowledgements.This research is supported by the Horizon Europe project TIER2 (GA: 101094817), and the FFG COMET program.
Managing Randomness to Enable Reproducible Machine Learning. H Ahmed, J Lofstead, Proceedings of the 5th International Workshop on Practical Reproducible Evaluation of Computer Systems. the 5th International Workshop on Practical Reproducible Evaluation of Computer SystemsMinneapolis MN USAACM2022</p>
<p>Measuring Reproduciblity of Machine Learning Methods for Medical Diagnosis. H Ahmed, R Tchoua, J Lofstead, 2022 Fourth International Conference on Transdisciplinary AI (TransAI). 2022</p>
<p>A clinician's guide to understanding and critically appraising machine learning studies: a checklist for Ruling Out Bias Using Standard Tools in Machine Learning (ROBUST-ML). S S Al-Zaiti, A A Alghwiri, X Hu, G Clermont, A Peace, P Macfarlane, R Bond, European Heart Journal -Digital Health. 322022</p>
<p>Challenges for the Repeatability of Deep Learning Models. S S Alahmari, D B Goldgof, P R Mouton, L O Hall, IEEE Access. 82020IEEE</p>
<p>R Albertoni, S Colantonio, P Skrzypczyński, J Stefanowski, arXiv:2302.12691Reproducibility of machine learning: Terminology, recommendations and open issues. 2023arXiv preprint</p>
<p>Systematic review finds "spin" practices and poor reporting standards in studies on machine learning-based prediction models. Andaur Navarro, C L Damen, J A Takada, T Nijman, S W Dhiman, P Ma, J Collins, G S Bajpai, R Riley, R D Moons, K G Hooft, L , Journal of Clinical Epidemiology. 1582023</p>
<p>Best practices in machine learning for chemistry. N Artrith, K T Butler, F X Coudert, S Han, O Isayev, A Jain, A Walsh, Nature Chemistry. 1362021</p>
<p>Z Aslanyan, P Vasilikos, Privacy-Preserving Machine Learning. 2020</p>
<p>rl_reach: Reproducible reinforcement learning experiments for robotic reaching tasks. P Aumjaud, D Mcauliffe, F J R Lera, P Cardiff, Software Impacts. 81000612021</p>
<p>1,500 scientists lift the lid on reproducibility. M Baker, Nature. 53376042016</p>
<p>Evidence on questionable research practices: The good, the bad, and the ugly. G C Banks, S G Rogelberg, H M Woznyj, R S Landis, D E Rupp, Journal of Business and Psychology. 312016</p>
<p>Frontiers of information access experimentation for research and education (dagstuhl seminar 23031). C Bauer, B Carterette, N Ferro, N Fuhr, G Faggioli, 2023Schloss Dagstuhl-Leibniz-Zentrum für Informatik13</p>
<p>Challenges to the Reproducibility of Machine Learning Models in Health Care. A L Beam, A K Manrai, M Ghassemi, JAMA. 32342020</p>
<p>A systematic review of reproducibility research in natural language processing. A Belz, S Agarwal, A Shimorina, E Reiter, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics. the 16th Conference of the European Chapter of the Association for Computational Linguistics01 2021</p>
<p>Gaming the metrics: Misconduct and manipulation in academic research. M Biagioli, A Lippman, 2020Mit Press</p>
<p>S Biderman, H Schoelkopf, L Sutawika, L Gao, J Tow, B Abbasi, A F Aji, P S Ammanamanchi, S Black, J Clive, arXiv:2405.14782Lessons from the trenches on reproducible evaluation of language models. 2024arXiv preprint</p>
<p>An introduction to Docker for reproducible research. C Boettiger, ACM SIGOPS Operating Systems Review. 4912015</p>
<p>End-to-end reproducible ai pipelines in radiology using the cloud. D Bontempi, L Nuernberg, S Pai, D Krishnaswamy, V Thiriveedhi, A Hosny, R H Mak, K Farahani, R Kikinis, A Fedorov, Nature Communications. 15169312024</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540Openai gym. 2016arXiv preprint</p>
<p>Living with the h-index? metric assemblages in the contemporary academy. R Burrows, The sociological review. 6022012</p>
<p>Towards training reproducible deep learning models. B Chen, M Wen, Y Shi, D Lin, G K Rajbahadur, Z M J Jiang, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software EngineeringNew York, NY, USAACM202222</p>
<p>The Art of Publishing Reproducible Research Outputs: Supporting emerging practices through cultural and technological innovation. A Chiarelli, L Loffreda, R Johnson, 2021Knowledge Exchange</p>
<p>ReproZip: Computational Reproducibility With Ease. F Chirigati, R Rampin, D Shasha, J Freire, Proceedings of the 2016 International Conference on Management of Data. the 2016 International Conference on Management of DataNew York, NY, USAACM201616</p>
<p>Comparing containerization-based approaches for reproducible computational modeling of environmental systems. Y D Choi, B Roy, J Nguyen, R Ahmad, I Maghami, A Nassar, Z Li, A M Castronova, T Malik, S Wang, Environmental Modelling &amp; Software. 1671057602023</p>
<p>A Clyburne-Sherin, X Fei, S A Green, Computational Reproducibility via Containers in Psychology. 20193</p>
<p>How many random seeds? statistical power analysis in deep reinforcement learning experiments. C Colas, O Sigaud, P Y Oudeyer, arXiv:1806.082952018arXiv preprint</p>
<p>Estimating the reproducibility of psychological science. O S Collaboration, Science. 349625147162015</p>
<p>Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD). G S Collins, J B Reitsma, D G Altman, K G Moons, publisher: American Heart Association. 2015131</p>
<p>P Cremonesi, D Jannach, Progress in Recommender Systems Research: Crisis? What Crisis? AI Magazine. 202142</p>
<p>A critical overview of privacy in machine learning. E De Cristofaro, IEEE Security &amp; Privacy. 1942021</p>
<p>R Dror, G Baumer, M Bogomolov, R Reichart, Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets. 20175</p>
<p>Y K Dwivedi, L Hughes, E Ismagilova, G Aarts, C Coombs, T Crick, Y Duan, R Dwivedi, J Edwards, A Eirug, Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research. 202157101994Artificial intelligence</p>
<p>T A D'antonoli, R Cuocolo, B Baessler, D P Santos, Towards reproducible radiomics research: introduction of a database for radiomics studies. 202434436</p>
<p>Seamless sharing and peer review of code. N C S Editorial, Nature Computational Science. 2127732022</p>
<p>Investigating the replicability of preclinical cancer biology. T M Errington, M Mathur, C K Soderberg, A Denis, N Perfito, E Iorns, B A Nosek, Elife. 10e716012021</p>
<p>M Faysse, P Fernandes, N Guerreiro, A Loison, D Alves, C Corro, N Boizard, J Alves, R Rei, P Martins, arXiv:2402.00786Croissantllm: A truly bilingual frenchenglish language model. 2024arXiv preprint</p>
<p>National cancer institute imaging data commons: toward transparency, reproducibility, and scalability in imaging artificial intelligence. A Fedorov, W J Longabaugh, D Pot, D A Clunie, S D Pieper, D L Gibbs, C Bridge, M D Herrmann, A Homeyer, R Lewis, Radiographics. 4312e2301802023</p>
<p>Gym-Ignition: Reproducible Robotic Simulations for Reinforcement Learning. D Ferigo, S Traversaro, G Metta, D Pucci, 2020 IEEE/SICE International Symposium on System Integration (SII). 2020</p>
<p>F Fidler, J Wilcox, The Stanford Encyclopedia of Philosophy. E N Zalta, 2021Metaphysics Research Lab, Stanford UniversityReproducibility of Scientific Results. summer 2021 edn.</p>
<p>The economics of reproducibility in preclinical research. L P Freedman, I M Cockburn, T S Simcoe, PLoS biology. 136e10021652015</p>
<p>Could machine learning fuel a reproducibility crisis in science?. E Gibney, Nature. 60879222022Nature Publishing Group Subject_term: Machine learning, Publishing, Mathematics and computingbandiera_abtest: a Cg_type: News Number: 7922 Publisher</p>
<p>What does research reproducibility mean?. S N Goodman, D Fanelli, J P Ioannidis, Science translational medicine. 83412016</p>
<p>The fundamental principles of reproducibility. O E Gundersen, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 379202002102197. 2021Royal Society</p>
<p>O E Gundersen, K Coakley, C Kirkpatrick, Y Gil, arXiv:2204.07610Sources of irreproducibility in machine learning: A review. 2023arXiv preprint</p>
<p>State of the Art: Reproducibility in Artificial Intelligence. O E Gundersen, S Kjensmo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Do machine learning platforms provide out-of-the-box reproducibility?. O E Gundersen, S Shamsaliei, R J Isdahl, Future Generation Computer Systems. 1262022</p>
<p>On reporting robust and trustworthy conclusions from model comparison studies involving neural networks and randomness. O E Gundersen, S Shamsaliei, H S Kjaernli, H Langseth, Proceedings of the 2023 ACM Conference on Reproducibility and Replicability. the 2023 ACM Conference on Reproducibility and Replicability2023</p>
<p>Automated machine learning in research -a literature review. A Haberl, S Thalmann, Proceedings of the 58th Hawaii International Conference on System Sciences. the 58th Hawaii International Conference on System Sciences2025</p>
<p>A checklist is associated with increased quality of reporting preclinical biomedical research: a systematic review. S Han, T F Olonisakin, J P Pribis, J Zupetic, J H Yoon, K M Holleran, K Jeong, N Shaikh, D M Rubio, J S Lee, PloS one. 129e01835912017</p>
<p>Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal Cognition. T E Hardwicke, M B Mathur, K Macdonald, G Nilsonne, G C Banks, M C Kidwell, A Hofelich Mohr, E Clayton, E J Yoon, M Henry Tessler, R L Lenne, S Altman, B Long, M C Frank, Royal Society Open Science. 581804482018Royal Societypublisher</p>
<p>T E Hardwicke, J D Wallach, M C Kidwell, T Bendixen, S Crüwell, J P Ioannidis, An empirical assessment of transparency and reproducibility-related research practices in the social sciences. 2014-2017. 20207190806</p>
<p>dtoolAI: Reproducibility for Deep Learning. M Hartley, T S G Olsson, Patterns. 151000732020</p>
<p>Reproducibility standards for machine learning in the life sciences. B J Heil, M M Hoffman, F Markowetz, S I Lee, C S Greene, S C Hicks, Nature Methods. 18102021</p>
<p>An Evaluation of the Software System Dependency of a Global Atmospheric Model. S Y Hong, M S Koo, J Jang, J E E Kim, H Park, M S Joh, J H Kang, T J Oh, Monthly Weather Review. 141112013American Meteorological SocietyMonthly Weather Review</p>
<p>Open science at the generative ai turn: An exploratory analysis of challenges and opportunities. M Hosseini, S P Horbach, K Holmes, T Ross-Hellauer, Quantitative Science Studies. 2024</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, Advances in neural information processing systems. 332020</p>
<p>Artificial intelligence faces reproducibility crisis Unpublished code and sensitivity to training conditions make many claims hard to verify. M Hutson, Science. 35963772018</p>
<p>Missing data hinder replication of artificial intelligence studies. M Hutson, Science. 2018</p>
<p>Why most published research findings are false. J P Ioannidis, PLoS medicine. 28e1242005</p>
<p>S A Iqbal, J D Wallach, M J Khoury, S D Schully, J P Ioannidis, Reproducible research practices and transparency across the biomedical literature. 201614e1002333</p>
<p>The Importance of Prediction Model Validation and Assessment in Obesity and Nutrition Research. A E Ivanescu, P Li, B George, A W Brown, S W Keith, D Raju, D B Allison, International journal of obesity. 62005. 2016</p>
<p>Mimic-iv, a freely accessible electronic health record dataset. A E Johnson, L Bulgarelli, L Shen, A Gayles, A Shammout, S Horng, T J Pollard, S Hao, B Moody, B Gow, Scientific data. 10112023</p>
<p>Machine learning models for diabetes management in acute care using electronic medical records: A systematic review. A Kamel Rahimi, O J Canfell, W Chan, B Sly, J D Pole, C Sullivan, S Shrapnel, International Journal of Medical Informatics. 1621047582022</p>
<p>Leakage and the reproducibility crisis in machinelearning-based science. S Kapoor, A Narayanan, Patterns. 491008042023</p>
<p>T Khan, M Budzys, K Nguyen, A Michalas, arXiv:2403.03592Wildest dreams: Reproducible research in privacy-preserving neural network training. 2024arXiv preprint</p>
<p>RE-EVALUATE: Reproducibility in Evaluating Reinforcement Learning Algorithms. K Khetarpal, Z Ahmed, A Cianflone, R Islam, J Pineau, 2nd Reproducibility in Machine Learning Workshop at ICML. 2018</p>
<p>Black box or open science? assessing reproducibility-related documentation in ai research. F Koenigstorfer, A Haberl, D Kowald, T Ross-Hellauer, S Thalmann, Proceedings of the 57th Hawaii International Conference on System Sciences. the 57th Hawaii International Conference on System Sciences2024</p>
<p>Popularity bias in collaborative filtering-based multimedia recommender systems. D Kowald, E Lacic, BIAS WS at ECIR. pp. 2022Springer</p>
<p>The unfairness of popularity bias in music recommendation: A reproducibility study. D Kowald, M Schedl, E Lex, 42nd European Conference on IR Research. Springer20202020</p>
<p>Establishing and evaluating trustworthy ai: overview and research challenges. D Kowald, S Scher, V Pammer-Schindler, P Müllner, K Waxnegger, L Demelius, A Fessl, M Toller, I G Mendoza Estrada, I Šimić, Frontiers in Big Data. 714672222024</p>
<p>Kormedmcqa: Multi-choice question answering benchmark for korean healthcare professional licensing examinations. S Kweon, B Choi, M Kim, R W Park, E Choi, arXiv:2403.014692024arXiv preprint</p>
<p>The journal impact factor: A brief history, critique, and discussion of adverse effects. V Lariviere, C R Sugimoto, Springer handbook of science and technology indicators pp. 2019</p>
<p>Bias in peer review. C J Lee, C R Sugimoto, G Zhang, B Cronin, Journal of the American Society for information Science and Technology. 6412013</p>
<p>A M Leventi-Peetz, T Östreich, arXiv:2202.11452Deep learning reproducibility and explainable ai (xai). 2022arXiv preprint</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Opening up chatgpt: Tracking openness, transparency, and accountability in instruction-tuned text generators. A Liesenfeld, A Lopez, M Dingemanse, Proceedings of the 5th international conference on conversational user interfaces. the 5th international conference on conversational user interfaces2023</p>
<p>R Liu, N B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023arXiv preprint</p>
<p>Reproducibility in machine learning for health research: Still a ways to go. M B A Mcdermott, S Wang, N Marinsek, R Ranganath, L Foschini, M Ghassemi, Science Translational Medicine. 1358616552021</p>
<p>A survey on bias and fairness in machine learning. N Mehrabi, F Morstatter, N Saxena, K Lerman, A Galstyan, ACM computing surveys (CSUR). 5462021</p>
<p>Model Cards for Model Reporting. M Mitchell, S Wu, A Zaldivar, P Barnes, L Vasserman, B Hutchinson, E Spitzer, I D Raji, T Gebru, Proceedings of the Conference on Fairness, Accountability, and Transparency. the Conference on Fairness, Accountability, and TransparencyNew York, NY, USAACM201919</p>
<p>Checklist for Artificial Intelligence in Medical Imaging (CLAIM): A Guide for Authors and Reviewers. J Mongan, L Moy, C E Kahn, Radiology: Artificial Intelligence. 22e2000292020Radiological Society of North America</p>
<p>Containers for computational reproducibility. D Moreau, K Wiebels, C Boettiger, Nature Reviews Methods Primers. 31502023</p>
<p>Robustness of meta matrix factorization against strict privacy constraints. P Muellner, D Kowald, E Lex, 43rd European Conference on IR Research, ECIR 2021. Springer2021</p>
<p>A manifesto for reproducible science. M R Munafò, B A Nosek, D V M Bishop, K S Button, C D Chambers, N Percie Du Sert, U Simonsohn, E J Wagenmakers, J J Ware, J P A Ioannidis, Nature Human Behaviour. 112017Nature Publishing Group</p>
<p>P Nagarajan, G Warnell, P Stone, arXiv:1809.05676Deterministic implementations for reproducibility in deep reinforcement learning. 2018arXiv preprint</p>
<p>Reproducible Containers. O S Navarro Leija, K Shiptoski, R G Scott, B Wang, N Renner, R R Newton, J Devietti, Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems. the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating SystemsLausanne SwitzerlandACM2020</p>
<p>Preregistration Is Hard, And Worthwhile. B A Nosek, E D Beck, L Campbell, J K Flake, T E Hardwicke, D T Mellor, A E Van 't Veer, S Vazire, Trends in Cognitive Sciences. 23102019</p>
<p>B A Nosek, T E Hardwicke, H Moshontz, A Allard, K S Corker, A Dreber, F Fidler, J Hilgard, M Kline Struhl, M B Nuijten, Replicability, robustness, and reproducibility in psychological science. 202273</p>
<p>The potential of generative artificial intelligence across disciplines: Perspectives and future directions. K B Ooi, G W H Tan, M Al-Emran, M A Al-Sharafi, A Capatina, A Chakraborty, Y K Dwivedi, T L Huang, A K Kar, V H Lee, Journal of Computer Information Systems. 2023</p>
<p>A comparison of statistical and machine-learning techniques in evaluating the association between dietary patterns and 10-year cardiometabolic risk (2002-2012): the ATTICA study. D Panaretos, E Koloverou, A C Dimopoulos, G M Kouli, M Vamvakari, G Tzavelas, C Pitsavos, D B Panagiotakos, British Journal of Nutrition. 12032018Cambridge University Presspublisher</p>
<p>Data and its (dis)contents: A survey of dataset development and use in machine learning research. A Paullada, I D Raji, E M Bender, E Denton, A Hanna, Patterns. 2111003362021</p>
<p>The reproducibility crisis in science: A statistical counterattack. R Peng, Significance. 1232015</p>
<p>Improving reproducibility in machine learning research (a report from the NeurIPS 2019 reproducibility program). J Pineau, P Vincent-Lamarre, K Sinha, V Larivière, A Beygelzimer, F Buc, E Fox, H Larochelle, The Journal of Machine Learning Research. 22174782021</p>
<p>Reproducible automl: An assessment of research reproducibility of no-code automl tools. S Pletzl, A Haberl, T Ross-Hellauer, S Thalmann, Wirtschaftsinformatik 2024 Proceedings. 2024</p>
<p>Indicators of research quality, quantity, openness, and responsibility in institutional review, promotion, and tenure policies across seven countries. N Pontika, T Klebel, A Correia, H Metzler, P Knoth, T Ross-Hellauer, Quantitative Science Studies. 342022</p>
<p>L Pouchard, Y Lin, H Van Dam, Replicating Machine Learning Experiments in Materials Science. IOS Press2020</p>
<p>Radiologist vs Machine Learning: A Comparison of Performance in Cancer Imaging. D Provenzano, Y J Rao, S Goyal, S Haji-Momenian, J Lichtenberger, M Loew, 2021 IEEE Applied Imagery Pattern Recognition Workshop (AIPR). 2021</p>
<p>Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI. M Pushkarna, A Zaldivar, O Kjartansson, Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. the 2022 ACM Conference on Fairness, Accountability, and TransparencyNew York, NY, USAACM202222</p>
<p>L Quaranta, F Calefato, F Lanubile, A Taxonomy of Tools for Reproducible Machine Learning Experiments. CEUR Workshop Proceedings. 2022</p>
<p>A Step Toward Quantifying Independently Reproducible Machine Learning Research. E Raff, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>Quantifying Inherent Randomness in Machine Learning Algorithms. S Raste, R Singh, J Vaughan, V N Nair, SSRN Electronic Journal. 2022</p>
<p>Evaluation practices and effects of indicator use-a literature review. S D Rijcke, P F Wouters, A D Rushforth, T P Franssen, B Hammarfelt, Research evaluation. 2522016</p>
<p>. N P Rougier, K Hinsen, F Alexandre, T Arildsen, L A Barba, F C Y Benureau, C T Brown, P D Buyl, O Caglayan, A P Davison, M A Delsuc, G Detorakis, A K Diem, D Drix, P Enel, B Girard, O Guest, M G Hall, R N Henriques, X Hinaut, K S Jaron, M Khamassi, A Klein, T Manninen, P Marchesi, D Mcglinn, C Metzner, O Petchey, H E Plesser, T Poisot, K Ram, Y Ram, E Roesch, C Rossant, V Rostami, A Shifman, J Stachelek, M Stimberg, F Stollmeier, F Vaggi, G Viejo, J Vitay, A E Vostinar, R Yurchak, T Zito, PeerJ Computer Science. 32017PeerJ IncSustainable computational science: the ReScience initiative</p>
<p>P Rouzrokh, B Khosravi, S Faghani, M Moassefi, D V Vera Garcia, Y Singh, K Zhang, G M Conte, B J Erickson, Mitigating Bias in Radiology Machine Learning: 1. Data Handling. 20224e210290</p>
<p>Unraveling overoptimism and publication bias in ml-driven science. P Saidi, G Dasarathy, V Berisha, 2024</p>
<p>The nci imaging data commons as a platform for reproducible research in computational pathology. D P Schacherer, M D Herrmann, D A Clunie, H Höfener, W Clifford, W J Longabaugh, S Pieper, R Kikinis, A Fedorov, A Homeyer, Computer methods and programs in biomedicine. 2421078392023</p>
<p>Management of Machine Learning Lifecycle Artifacts: A Survey. M Schlegel, K U Sattler, ACM SIGMOD Record. 5142023</p>
<p>H Semmelrock, S Kopeinik, D Theiler, T Ross-Hellauer, D Kowald, arXiv:2307.10320Reproducibility in machine learning-driven research. 2023arXiv preprint</p>
<p>How do deep-learning framework versions affect the reproducibility of neural network models?. M Shahriari, R Ramler, L Fischer, Machine Learning and Knowledge Extraction. 442022</p>
<p>The natural selection of bad science. P E Smaldino, R Mcelreath, Royal Society open science. 391603842016</p>
<p>Packaging research artefacts with RO-Crate. S Soiland-Reyes, P Sefton, M Crosas, L J Castro, F Coppens, J M Fernández, D Garijo, B Grüning, M La Rosa, S Leo, E Carragáin, M Portier, A Trisovic, R C Community, P Groth, C Goble, Data Science. 522022IOS Press</p>
<p>Preregistration and reproducibility. E Strømland, Journal of Economic Psychology. 751021432019</p>
<p>Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. 2018arXiv preprint</p>
<p>A Practical Taxonomy of Reproducibility for Machine Learning Research. R Tatman, J Vanderplas, S Dane, CEUR Workshop Proceedings. 2018</p>
<p>Questionable practices in methodological deep learning research. D J Trosten, Proceedings of the Northern Lights Deep Learning Workshop. the Northern Lights Deep Learning Workshop20234</p>
<p>Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record. J Walonoski, M Kramer, J Nichols, A Quina, C Moesel, D Hall, C Duffett, K Dube, T Gallagher, S Mclachlan, Journal of the American Medical Informatics Association: JAMIA. 2532018</p>
<p>The replication crisis in psychology: An overview for theoretical and philosophical psychology. B J Wiggins, C D Christopherson, Journal of Theoretical and Philosophical Psychology. 392019</p>
<p>PROBAST: A Tool to Assess the Risk of Bias and Applicability of Prediction Model Studies. R F Wolff, K G Moons, R D Riley, P F Whiting, M Westwood, G S Collins, J B Reitsma, J Kleijnen, S Mallett, Annals of Internal Medicine. 17012019American College of Physicians</p>
<p>R Xu, N Baracaldo, J Joshi, arXiv:2108.04417Privacy-preserving machine learning: Methods, challenges and directions. 2021arXiv preprint</p>
<p>Re-producedPapers.org: Openly Teaching and Structuring Machine Learning Reproducibility. B Yildiz, H Hung, J Krijthe, C Liem, M Loog, G Migut, F Oliehoek, A Panichella, P Pawełczak, S Picek, M De Weerdt, J Van Gemert, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics. 2021</p>
<p>Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. K Young, T Tian, arXiv:1903.031762019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3426 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3426</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3426</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264128006</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09107v2.pdf" target="_blank">GLoRE: Evaluating Logical Reasoning of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3426.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3426.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-base (robustly optimized BERT pretraining approach, base size)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 125M-parameter Transformer encoder used as a supervised fine-tuning baseline in the GLoRE benchmark; fine-tuned per-dataset for five epochs to provide baseline accuracy on logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder (BERT-family) pre-trained with robust optimization; used here as a 125M-parameter supervised baseline fine-tuned on each dataset for five epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A suite of logical reasoning tasks including multi-choice reading comprehension (MRC), natural language inference (NLI), and true/false (yes/no) entailment problems requiring multi-step, symbolic and monotonicity reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised fine-tuning on each dataset (five epochs) to establish a classical supervised baseline for natural-language logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported per-dataset examples: LogiQA 2.0: 48.76% accuracy; LogiQA22: 33.22% accuracy; NaN-NLI: 90.02% accuracy; ProofWriter: 55.92% accuracy. (Other dataset-level numbers reported in paper's Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>This model is the baseline; other LLMs and reasoning-enhanced models are compared against it (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Underperforms humans on many MRC tasks (e.g., LogiQA datasets); shows evidence of learning superficial patterns (e.g., NaN-NLI high accuracy possibly from rule-based patterns), and limited generalization across distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Serves as supervised-fine-tuned baseline; comparisons in paper analyze zero-shot/few-shot LLMs vs. this supervised baseline (no internal ablations on RoBERTa reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3426.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-30B-supercot (instruction-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large language model (30B variant) evaluated zero-shot and few-shot on GLoRE, showing relatively low zero-shot MRC performance without in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B-supercot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 30B-parameter LLaMA-family model (instruction-tuned / 'supercot' variant as used in experiments); evaluated in zero-shot and few-shot ICL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (multiple constituent datasets including LogiQA, ReClor, NLI and TF tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>MRC, NLI and TF reasoning tasks probing multi-step and symbolic logical inference in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation and few-shot in-context learning (1-, 2-, 5-shot) using sampled demonstrations appended to the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot average reported ~32.34% (Table 2); few-shot improves to 39.62% at 5-shot (Table 3). Performance on many MRC tasks ~20% (worse than random in 4-way classification) without demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-base (supervised): e.g., LogiQA 2.0: 48.76% (higher than LLaMA on MRC).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LLaMA lags the RoBERTa supervised baseline on many tasks in zero-shot; few-shot ICL yields improvements (up to ~7.3 percentage points from 0-shot to 5-shot) but still often below supervised RoBERTa on several MRC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Poor zero-shot MRC performance (sometimes below random guess), sensitivity to in-context examples, weak generalization across dataset distributions, reliance on surface patterns rather than robust logical strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper analyzes zero-shot vs few-shot performance showing incremental gains from ICL but argues these reflect statistical adaptation rather than genuine logical inference; no deeper ablation for architecture presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3426.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B-parameter open-source instruction-tuned decoder-only LLM evaluated in zero- and few-shot settings on the GLoRE benchmark; shows similar zero-shot performance to LLaMA but underperforms supervised RoBERTa in many MRC cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source decoder-only Transformer model with ~40B parameters, instruction-tuned for following natural language prompts; evaluated zero-/few-shot on logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (MRC, NLI, TF datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks probe logical deduction, entailment, monotonicity, negation, and multi-premise reasoning over natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot and few-shot in-context learning; no additional fine-tuning reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot average reported ~32.28%; few-shot improves modestly (35.72% at 5-shot in Table 3). On some TF tasks Falcon performs better than other base LLMs but overall underperforms supervised baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-base (supervised): e.g., LogiQA 2.0: 48.76%; on many tasks Falcon is worse.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Small improvement from zero-shot to few-shot (~3.44 percentage points to 5-shot) but remains below supervised RoBERTa on several MRC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Similar to LLaMA: sensitive to data distribution, poor generalization on MRC without examples, reliance on surface features rather than robust logical chains of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comparison of zero-shot vs few-shot demonstrates ICL benefit but limited; no internal ablation of Falcon architecture or training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3426.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7b (Mixture-of-Experts variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts (MoE) architecture evaluated zero-shot on GLoRE; outperforms LLaMA and Falcon in the experiments, suggesting MoE benefits for logical reasoning generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-experts LLM composed of multiple 7B experts (name indicates 8 experts × 7B each); evaluated zero-shot on logical reasoning tasks in GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8×7B (implied by name)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (MRC, NLI, TF)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Suite of logical reasoning datasets requiring multi-step, symbolic, and monotonicity reasoning in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation; MoE architecture is highlighted as potentially advantageous.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to outperform LLaMA and Falcon in zero-shot (exact averaged number not provided separately beyond statements in text), demonstrating higher accuracy across tasks than these base LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against LLaMA (32.34%) and Falcon (32.28%) zero-shot; supervised RoBERTa remains competitive on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improves over comparable-size decoder-only models (LLaMA/Falcon) in zero-shot, indicating MoE can boost logical reasoning capability without in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still underperforms reasoning-enhanced models (e.g., QwQ-32B, DeepSeek R1) and shows sensitivity to dataset distribution; no per-dataset failure breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper notes Mixtral's superiority among base LLMs, suggesting MoE architecture is an important factor, but provides no detailed ablation isolating MoE contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3426.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI GPT-3.5-based conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-following conversational LLM (GPT-3.5 family) evaluated zero-shot and few-shot on GLoRE; shows moderate reasoning performance and substantial gains with few-shot ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI conversational instruction-tuned LLM (GPT-3.5 family); evaluated on GLoRE in zero-shot and few-shot regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (MRC, NLI, TF)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Natural-language logical reasoning covering multi-choice reading comprehension, NLI, and true/false entailment requiring chain-like and symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot and few-shot in-context learning (1-,2-,5-shot); standard instruction prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot average: 52.10% (Table 3 zero-shot row). Few-shot: 5-shot average 60.32% (improvement of ~8.22 percentage points). On ConTRoL ChatGPT achieves 58.45% per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-base (supervised): e.g., LogiQA 2.0: 48.76%; ChatGPT outperforms RoBERTa on several tasks but trails leading reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>ChatGPT typically outperforms the RoBERTa supervised baseline on many NLI and TF tasks in zero-shot; few-shot yields further gains (~8 percentage points at 5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inconsistent across datasets; demonstrated sensitivity to distribution and difficulty with long or multi-step reasoning chains. Improvements with ICL likely reflect statistical adaptation rather than human-like logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper analyzes zero-shot vs few-shot gains; cites broader literature arguing chain-of-thought correlates with outputs but may not causally produce reasoning. No internal ablation specific to ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3426.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's state-of-the-art multimodal/LLM used as a commercial strong baseline in GLoRE; shows strong zero-shot and few-shot performance on many MRC and reasoning tasks but exhibits dataset sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's large instruction-tuned model (details not specified in paper); evaluated zero-shot and few-shot on GLoRE datasets and compared to open-source reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (LogiQA 2.0, LogiQA22, ReClor, NLI and TF datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Comprehensive logical reasoning tasks spanning exam-style reading comprehension, entailment, monotonicity, negation, and synthetic rule reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot and few-shot in-context learning (1-,2-,5-shot); standard prompting and demonstration sampling as in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot average: 66.34% (Table 3). Few-shot (5-shot) average: 75.83%. Per-dataset highlights: ReClor 87.20%; NaN-NLI 75.74%; LogiQA 2.0: 72.25% (but drops to 58.49% on LogiQA22), FraCaS 75.35%, ProofWriter 59.66%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-base (supervised) e.g., LogiQA 2.0: 48.76%; GPT-4 substantially exceeds RoBERTa on many MRC and NLI tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Large absolute improvements over RoBERTa on multiple datasets and stronger than ChatGPT and other base LLMs; few-shot yields further ~9.49 percentage point gain from 0-shot to 5-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Marked sensitivity to dataset distribution (e.g., large drop from LogiQA 2.0 to LogiQA22), inconsistent NLI performance across datasets (HELP low at 46.01%), and variable TF performance; suggests reliance on dataset-specific patterns rather than a single robust logical reasoning mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports zero-shot vs few-shot comparisons showing notable gains; discusses distributional sensitivity and that ICL improvements likely reflect statistical adaptation not causal chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3426.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial reasoning-enhanced model by OpenAI (o1 family) evaluated on GLoRE; shows strong performance on several NLI datasets and competitive average performance among reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI 'o1' system card model variant (mini); described as a reasoning-enhanced / instruction-following commercial model evaluated on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (NLI, MRC, TF datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks testing entailment relations, monotonicity, negation, and multi-premise logical reasoning, including NLI datasets like HELP where monotonicity reasoning is required.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Instruction-tuned commercial model; evaluated in zero-shot settings (and compared against few-shot trends).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported notable performance on NLI dataset HELP: 63.69% (higher than QwQ-32B on that dataset). Average values not explicitly isolated for o1 mini but reported among top reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-base supervised and other LLMs (e.g., GPT-4 66.34% average); o1 mini generally outperforms many base LLMs and sometimes rival QwQ/DeepSeek on specific NLI tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Outperforms many vanilla LLMs and sometimes RoBERTa on NLI tasks like HELP; however, global average comparisons show QwQ-32B and DeepSeek R1 slightly higher overall in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Noted sensitivity to dataset distribution; performance advantages may be task-specific (e.g., stronger on monotonicity/HELP but not dominant across all datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper cites per-dataset comparisons showing o1 mini's strengths on some NLI datasets; no internal ablation of o1 mini training or architecture provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3426.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced open model trained with reinforcement learning incentives (per references) evaluated on GLoRE; achieves strong average performance (reported 75.14% average) and competitive TF/MRC results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source reasoning-enhanced model reported to use reinforcement learning to incentivize reasoning capabilities (paper cites DeepSeek-AI/Deepseek-r1); evaluated on GLoRE in zero-shot/few-shot conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (MRC/NLI/TF)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring exam-style logical MRC, NLI entailment and contradiction detection, and synthetic rule-based TF reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Reinforcement learning incentives aimed at improving reasoning ability; instruction-tuning and reasoning-focused training methodology reported in references.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported average accuracy ~75.14% across GLoRE. Example per-dataset numbers in Table 2: e.g., high MRC and TF results (detailed row: 'Deepseek, R1 76.22 81.49 77.88 90.01 71.63 78.37 62.05 75.74 72.58 59.96 75.29 80.51 75.14' mapped to datasets in table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-base supervised and GPT-4 (GPT-4 average 66.34%); DeepSeek R1 substantially exceeds GPT-4 and RoBERTa averages on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>DeepSeek R1 improves average accuracy over GPT-4 by ~8.8 percentage points (75.14% vs 66.34% average) and greatly over RoBERTa on many MRC/TF tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite high average performance, shows dataset sensitivity and unevenness (e.g., weaker on particular NLI subsets); the paper notes reasoning gaps similar to other models and need for broader evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper attributes part of the gains to reinforcement-learning-based training but does not present a detailed ablation isolating RL components; dataset-level analysis shows uneven generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3426.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3426.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B-parameter reasoning-enhanced open model trained with reinforcement learning techniques (as reported) that attains state-of-the-art average performance on the GLoRE benchmark in this study (78.95% average).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 32B-parameter model (QwQ family) described as leveraging reinforcement learning and specialized training for reasoning; evaluated across GLoRE datasets and reported as top-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE benchmark (LogiQA, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Comprehensive logical reasoning suite covering exam-style MRC, entailment/contradiction in NLI, monotonicity and negation challenges, and synthetic rule-proof tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Reported use of reinforcement learning / reasoning-specialized training and instruction-tuning; evaluated zero-shot and few-shot in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy reported at 78.95% across GLoRE (highest in the paper). Per-dataset highlights: ReClor 93.76%, AR-LSAT 92.35%, LogiQA22 86.30%, ProofWriter 82.40%. QwQ-32B outperforms GPT-4 (66.34% avg) and DeepSeek R1 (75.14% avg) on overall benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared to RoBERTa-base supervised (e.g., LogiQA 48.76%) and GPT-4 (66.34% average); QwQ-32B shows large absolute gains (e.g., +12.61 p.p. over DeepSeek R1 and +12.61 p.p. over GPT-4 in average; note exact per-dataset differences vary).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Substantial improvement over supervised RoBERTa and prior top LLMs; sets new state-of-the-art on several MRC and TF tasks (notably ReClor and AR-LSAT) and shows strong robustness on LogiQA22 (outperforming GPT-4 by 27.81 percentage points on that split).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Uneven performance in NLI: e.g., HELP 61.53% (lower than o1 mini's 63.69%), indicating limitations in monotonicity/negation reasoning; overall sensitivity to dataset distribution remains, suggesting gains are not universal across all logical subtypes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper attributes gains to RL-style or specialized training but provides limited internal ablation; analysis highlights QwQ's dominance on MRC/TF but relative weakness on fine-grained NLI tasks, indicating that training innovations improve some reasoning kinds more than others.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>RuleTaker: Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation <em>(Rating: 2)</em></li>
                <li>HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning <em>(Rating: 2)</em></li>
                <li>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Qwq-32b: Embracing the power of reinforcement learning <em>(Rating: 2)</em></li>
                <li>Chain-of-thought hub: A continuous effort to measure large models' reasoning performance <em>(Rating: 1)</em></li>
                <li>Big-Bench Hard (BBH) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3426",
    "paper_id": "paper-264128006",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "RoBERTa-base",
            "name_full": "RoBERTa-base (robustly optimized BERT pretraining approach, base size)",
            "brief_description": "A 125M-parameter Transformer encoder used as a supervised fine-tuning baseline in the GLoRE benchmark; fine-tuned per-dataset for five epochs to provide baseline accuracy on logical reasoning datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "Transformer encoder (BERT-family) pre-trained with robust optimization; used here as a 125M-parameter supervised baseline fine-tuned on each dataset for five epochs.",
            "model_size": "125M",
            "reasoning_task_name": "GLoRE benchmark (LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)",
            "reasoning_task_description": "A suite of logical reasoning tasks including multi-choice reading comprehension (MRC), natural language inference (NLI), and true/false (yes/no) entailment problems requiring multi-step, symbolic and monotonicity reasoning.",
            "method_or_intervention": "Supervised fine-tuning on each dataset (five epochs) to establish a classical supervised baseline for natural-language logical reasoning.",
            "performance": "Reported per-dataset examples: LogiQA 2.0: 48.76% accuracy; LogiQA22: 33.22% accuracy; NaN-NLI: 90.02% accuracy; ProofWriter: 55.92% accuracy. (Other dataset-level numbers reported in paper's Table 2.)",
            "baseline_performance": null,
            "improvement_over_baseline": "This model is the baseline; other LLMs and reasoning-enhanced models are compared against it (see other entries).",
            "limitations_or_failures": "Underperforms humans on many MRC tasks (e.g., LogiQA datasets); shows evidence of learning superficial patterns (e.g., NaN-NLI high accuracy possibly from rule-based patterns), and limited generalization across distributions.",
            "ablation_or_analysis": "Serves as supervised-fine-tuned baseline; comparisons in paper analyze zero-shot/few-shot LLMs vs. this supervised baseline (no internal ablations on RoBERTa reported).",
            "uuid": "e3426.0",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA-30B-supercot (instruction-tuned variant)",
            "brief_description": "An open-source large language model (30B variant) evaluated zero-shot and few-shot on GLoRE, showing relatively low zero-shot MRC performance without in-context examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B-supercot",
            "model_description": "A 30B-parameter LLaMA-family model (instruction-tuned / 'supercot' variant as used in experiments); evaluated in zero-shot and few-shot ICL settings.",
            "model_size": "30B",
            "reasoning_task_name": "GLoRE benchmark (multiple constituent datasets including LogiQA, ReClor, NLI and TF tasks)",
            "reasoning_task_description": "MRC, NLI and TF reasoning tasks probing multi-step and symbolic logical inference in natural language.",
            "method_or_intervention": "Zero-shot evaluation and few-shot in-context learning (1-, 2-, 5-shot) using sampled demonstrations appended to the prompt.",
            "performance": "Zero-shot average reported ~32.34% (Table 2); few-shot improves to 39.62% at 5-shot (Table 3). Performance on many MRC tasks ~20% (worse than random in 4-way classification) without demonstrations.",
            "baseline_performance": "RoBERTa-base (supervised): e.g., LogiQA 2.0: 48.76% (higher than LLaMA on MRC).",
            "improvement_over_baseline": "LLaMA lags the RoBERTa supervised baseline on many tasks in zero-shot; few-shot ICL yields improvements (up to ~7.3 percentage points from 0-shot to 5-shot) but still often below supervised RoBERTa on several MRC datasets.",
            "limitations_or_failures": "Poor zero-shot MRC performance (sometimes below random guess), sensitivity to in-context examples, weak generalization across dataset distributions, reliance on surface patterns rather than robust logical strategies.",
            "ablation_or_analysis": "Paper analyzes zero-shot vs few-shot performance showing incremental gains from ICL but argues these reflect statistical adaptation rather than genuine logical inference; no deeper ablation for architecture presented.",
            "uuid": "e3426.1",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-40B-instruct",
            "name_full": "Falcon-40B-instruct",
            "brief_description": "A 40B-parameter open-source instruction-tuned decoder-only LLM evaluated in zero- and few-shot settings on the GLoRE benchmark; shows similar zero-shot performance to LLaMA but underperforms supervised RoBERTa in many MRC cases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Falcon-40B-instruct",
            "model_description": "Open-source decoder-only Transformer model with ~40B parameters, instruction-tuned for following natural language prompts; evaluated zero-/few-shot on logical reasoning datasets.",
            "model_size": "40B",
            "reasoning_task_name": "GLoRE benchmark (MRC, NLI, TF datasets)",
            "reasoning_task_description": "Tasks probe logical deduction, entailment, monotonicity, negation, and multi-premise reasoning over natural language.",
            "method_or_intervention": "Zero-shot and few-shot in-context learning; no additional fine-tuning reported in this paper.",
            "performance": "Zero-shot average reported ~32.28%; few-shot improves modestly (35.72% at 5-shot in Table 3). On some TF tasks Falcon performs better than other base LLMs but overall underperforms supervised baselines.",
            "baseline_performance": "RoBERTa-base (supervised): e.g., LogiQA 2.0: 48.76%; on many tasks Falcon is worse.",
            "improvement_over_baseline": "Small improvement from zero-shot to few-shot (~3.44 percentage points to 5-shot) but remains below supervised RoBERTa on several MRC tasks.",
            "limitations_or_failures": "Similar to LLaMA: sensitive to data distribution, poor generalization on MRC without examples, reliance on surface features rather than robust logical chains of reasoning.",
            "ablation_or_analysis": "Comparison of zero-shot vs few-shot demonstrates ICL benefit but limited; no internal ablation of Falcon architecture or training reported.",
            "uuid": "e3426.2",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mixtral-8x7b",
            "name_full": "Mixtral-8x7b (Mixture-of-Experts variant)",
            "brief_description": "A mixture-of-experts (MoE) architecture evaluated zero-shot on GLoRE; outperforms LLaMA and Falcon in the experiments, suggesting MoE benefits for logical reasoning generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7b",
            "model_description": "Mixture-of-experts LLM composed of multiple 7B experts (name indicates 8 experts × 7B each); evaluated zero-shot on logical reasoning tasks in GLoRE.",
            "model_size": "8×7B (implied by name)",
            "reasoning_task_name": "GLoRE benchmark (MRC, NLI, TF)",
            "reasoning_task_description": "Suite of logical reasoning datasets requiring multi-step, symbolic, and monotonicity reasoning in natural language.",
            "method_or_intervention": "Zero-shot evaluation; MoE architecture is highlighted as potentially advantageous.",
            "performance": "Reported to outperform LLaMA and Falcon in zero-shot (exact averaged number not provided separately beyond statements in text), demonstrating higher accuracy across tasks than these base LLMs.",
            "baseline_performance": "Compared against LLaMA (32.34%) and Falcon (32.28%) zero-shot; supervised RoBERTa remains competitive on some tasks.",
            "improvement_over_baseline": "Improves over comparable-size decoder-only models (LLaMA/Falcon) in zero-shot, indicating MoE can boost logical reasoning capability without in-context examples.",
            "limitations_or_failures": "Still underperforms reasoning-enhanced models (e.g., QwQ-32B, DeepSeek R1) and shows sensitivity to dataset distribution; no per-dataset failure breakdown provided.",
            "ablation_or_analysis": "Paper notes Mixtral's superiority among base LLMs, suggesting MoE architecture is an important factor, but provides no detailed ablation isolating MoE contributions.",
            "uuid": "e3426.3",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI GPT-3.5-based conversational model)",
            "brief_description": "An instruction-following conversational LLM (GPT-3.5 family) evaluated zero-shot and few-shot on GLoRE; shows moderate reasoning performance and substantial gains with few-shot ICL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "OpenAI conversational instruction-tuned LLM (GPT-3.5 family); evaluated on GLoRE in zero-shot and few-shot regimes.",
            "model_size": null,
            "reasoning_task_name": "GLoRE benchmark (MRC, NLI, TF)",
            "reasoning_task_description": "Natural-language logical reasoning covering multi-choice reading comprehension, NLI, and true/false entailment requiring chain-like and symbolic reasoning.",
            "method_or_intervention": "Zero-shot and few-shot in-context learning (1-,2-,5-shot); standard instruction prompting.",
            "performance": "Zero-shot average: 52.10% (Table 3 zero-shot row). Few-shot: 5-shot average 60.32% (improvement of ~8.22 percentage points). On ConTRoL ChatGPT achieves 58.45% per paper.",
            "baseline_performance": "RoBERTa-base (supervised): e.g., LogiQA 2.0: 48.76%; ChatGPT outperforms RoBERTa on several tasks but trails leading reasoning-enhanced models.",
            "improvement_over_baseline": "ChatGPT typically outperforms the RoBERTa supervised baseline on many NLI and TF tasks in zero-shot; few-shot yields further gains (~8 percentage points at 5-shot).",
            "limitations_or_failures": "Inconsistent across datasets; demonstrated sensitivity to distribution and difficulty with long or multi-step reasoning chains. Improvements with ICL likely reflect statistical adaptation rather than human-like logical inference.",
            "ablation_or_analysis": "Paper analyzes zero-shot vs few-shot gains; cites broader literature arguing chain-of-thought correlates with outputs but may not causally produce reasoning. No internal ablation specific to ChatGPT.",
            "uuid": "e3426.4",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "OpenAI's state-of-the-art multimodal/LLM used as a commercial strong baseline in GLoRE; shows strong zero-shot and few-shot performance on many MRC and reasoning tasks but exhibits dataset sensitivity.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's large instruction-tuned model (details not specified in paper); evaluated zero-shot and few-shot on GLoRE datasets and compared to open-source reasoning-enhanced models.",
            "model_size": null,
            "reasoning_task_name": "GLoRE benchmark (LogiQA 2.0, LogiQA22, ReClor, NLI and TF datasets)",
            "reasoning_task_description": "Comprehensive logical reasoning tasks spanning exam-style reading comprehension, entailment, monotonicity, negation, and synthetic rule reasoning.",
            "method_or_intervention": "Zero-shot and few-shot in-context learning (1-,2-,5-shot); standard prompting and demonstration sampling as in experiments.",
            "performance": "Zero-shot average: 66.34% (Table 3). Few-shot (5-shot) average: 75.83%. Per-dataset highlights: ReClor 87.20%; NaN-NLI 75.74%; LogiQA 2.0: 72.25% (but drops to 58.49% on LogiQA22), FraCaS 75.35%, ProofWriter 59.66%.",
            "baseline_performance": "RoBERTa-base (supervised) e.g., LogiQA 2.0: 48.76%; GPT-4 substantially exceeds RoBERTa on many MRC and NLI tasks.",
            "improvement_over_baseline": "Large absolute improvements over RoBERTa on multiple datasets and stronger than ChatGPT and other base LLMs; few-shot yields further ~9.49 percentage point gain from 0-shot to 5-shot.",
            "limitations_or_failures": "Marked sensitivity to dataset distribution (e.g., large drop from LogiQA 2.0 to LogiQA22), inconsistent NLI performance across datasets (HELP low at 46.01%), and variable TF performance; suggests reliance on dataset-specific patterns rather than a single robust logical reasoning mechanism.",
            "ablation_or_analysis": "Paper reports zero-shot vs few-shot comparisons showing notable gains; discusses distributional sensitivity and that ICL improvements likely reflect statistical adaptation not causal chain-of-thought reasoning.",
            "uuid": "e3426.5",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "o1 mini",
            "name_full": "OpenAI o1 mini",
            "brief_description": "A commercial reasoning-enhanced model by OpenAI (o1 family) evaluated on GLoRE; shows strong performance on several NLI datasets and competitive average performance among reasoning-enhanced models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o1 mini",
            "model_description": "OpenAI 'o1' system card model variant (mini); described as a reasoning-enhanced / instruction-following commercial model evaluated on GLoRE.",
            "model_size": null,
            "reasoning_task_name": "GLoRE benchmark (NLI, MRC, TF datasets)",
            "reasoning_task_description": "Tasks testing entailment relations, monotonicity, negation, and multi-premise logical reasoning, including NLI datasets like HELP where monotonicity reasoning is required.",
            "method_or_intervention": "Instruction-tuned commercial model; evaluated in zero-shot settings (and compared against few-shot trends).",
            "performance": "Reported notable performance on NLI dataset HELP: 63.69% (higher than QwQ-32B on that dataset). Average values not explicitly isolated for o1 mini but reported among top reasoning-enhanced models.",
            "baseline_performance": "RoBERTa-base supervised and other LLMs (e.g., GPT-4 66.34% average); o1 mini generally outperforms many base LLMs and sometimes rival QwQ/DeepSeek on specific NLI tasks.",
            "improvement_over_baseline": "Outperforms many vanilla LLMs and sometimes RoBERTa on NLI tasks like HELP; however, global average comparisons show QwQ-32B and DeepSeek R1 slightly higher overall in the paper.",
            "limitations_or_failures": "Noted sensitivity to dataset distribution; performance advantages may be task-specific (e.g., stronger on monotonicity/HELP but not dominant across all datasets).",
            "ablation_or_analysis": "Paper cites per-dataset comparisons showing o1 mini's strengths on some NLI datasets; no internal ablation of o1 mini training or architecture provided.",
            "uuid": "e3426.6",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1",
            "brief_description": "A reasoning-enhanced open model trained with reinforcement learning incentives (per references) evaluated on GLoRE; achieves strong average performance (reported 75.14% average) and competitive TF/MRC results.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "Open-source reasoning-enhanced model reported to use reinforcement learning to incentivize reasoning capabilities (paper cites DeepSeek-AI/Deepseek-r1); evaluated on GLoRE in zero-shot/few-shot conditions.",
            "model_size": null,
            "reasoning_task_name": "GLoRE benchmark (MRC/NLI/TF)",
            "reasoning_task_description": "Benchmarks requiring exam-style logical MRC, NLI entailment and contradiction detection, and synthetic rule-based TF reasoning.",
            "method_or_intervention": "Reinforcement learning incentives aimed at improving reasoning ability; instruction-tuning and reasoning-focused training methodology reported in references.",
            "performance": "Reported average accuracy ~75.14% across GLoRE. Example per-dataset numbers in Table 2: e.g., high MRC and TF results (detailed row: 'Deepseek, R1 76.22 81.49 77.88 90.01 71.63 78.37 62.05 75.74 72.58 59.96 75.29 80.51 75.14' mapped to datasets in table).",
            "baseline_performance": "RoBERTa-base supervised and GPT-4 (GPT-4 average 66.34%); DeepSeek R1 substantially exceeds GPT-4 and RoBERTa averages on the benchmark.",
            "improvement_over_baseline": "DeepSeek R1 improves average accuracy over GPT-4 by ~8.8 percentage points (75.14% vs 66.34% average) and greatly over RoBERTa on many MRC/TF tasks.",
            "limitations_or_failures": "Despite high average performance, shows dataset sensitivity and unevenness (e.g., weaker on particular NLI subsets); the paper notes reasoning gaps similar to other models and need for broader evaluation.",
            "ablation_or_analysis": "Paper attributes part of the gains to reinforcement-learning-based training but does not present a detailed ablation isolating RL components; dataset-level analysis shows uneven generalization.",
            "uuid": "e3426.7",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "A 32B-parameter reasoning-enhanced open model trained with reinforcement learning techniques (as reported) that attains state-of-the-art average performance on the GLoRE benchmark in this study (78.95% average).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "Open-source 32B-parameter model (QwQ family) described as leveraging reinforcement learning and specialized training for reasoning; evaluated across GLoRE datasets and reported as top-performing.",
            "model_size": "32B",
            "reasoning_task_name": "GLoRE benchmark (LogiQA, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter)",
            "reasoning_task_description": "Comprehensive logical reasoning suite covering exam-style MRC, entailment/contradiction in NLI, monotonicity and negation challenges, and synthetic rule-proof tasks.",
            "method_or_intervention": "Reported use of reinforcement learning / reasoning-specialized training and instruction-tuning; evaluated zero-shot and few-shot in-context learning.",
            "performance": "Average accuracy reported at 78.95% across GLoRE (highest in the paper). Per-dataset highlights: ReClor 93.76%, AR-LSAT 92.35%, LogiQA22 86.30%, ProofWriter 82.40%. QwQ-32B outperforms GPT-4 (66.34% avg) and DeepSeek R1 (75.14% avg) on overall benchmark.",
            "baseline_performance": "Compared to RoBERTa-base supervised (e.g., LogiQA 48.76%) and GPT-4 (66.34% average); QwQ-32B shows large absolute gains (e.g., +12.61 p.p. over DeepSeek R1 and +12.61 p.p. over GPT-4 in average; note exact per-dataset differences vary).",
            "improvement_over_baseline": "Substantial improvement over supervised RoBERTa and prior top LLMs; sets new state-of-the-art on several MRC and TF tasks (notably ReClor and AR-LSAT) and shows strong robustness on LogiQA22 (outperforming GPT-4 by 27.81 percentage points on that split).",
            "limitations_or_failures": "Uneven performance in NLI: e.g., HELP 61.53% (lower than o1 mini's 63.69%), indicating limitations in monotonicity/negation reasoning; overall sensitivity to dataset distribution remains, suggesting gains are not universal across all logical subtypes.",
            "ablation_or_analysis": "Paper attributes gains to RL-style or specialized training but provides limited internal ablation; analysis highlights QwQ's dominance on MRC/TF but relative weakness on fine-grained NLI tasks, indicating that training innovations improve some reasoning kinds more than others.",
            "uuid": "e3426.8",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogiQA 2.0-an improved dataset for logical reasoning in natural language understanding",
            "rating": 2,
            "sanitized_title": "logiqa_20an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "RuleTaker: Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "ruletaker_transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation",
            "rating": 2,
            "sanitized_title": "not_another_negation_benchmark_the_nannli_test_suite_for_subclausal_negation"
        },
        {
            "paper_title": "HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning",
            "rating": 2,
            "sanitized_title": "help_a_dataset_for_identifying_shortcomings_of_neural_models_in_monotonicity_reasoning"
        },
        {
            "paper_title": "Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekai_deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Qwq-32b: Embracing the power of reinforcement learning",
            "rating": 2,
            "sanitized_title": "qwq32b_embracing_the_power_of_reinforcement_learning"
        },
        {
            "paper_title": "Chain-of-thought hub: A continuous effort to measure large models' reasoning performance",
            "rating": 1,
            "sanitized_title": "chainofthought_hub_a_continuous_effort_to_measure_large_models_reasoning_performance"
        },
        {
            "paper_title": "Big-Bench Hard (BBH)",
            "rating": 1,
            "sanitized_title": "bigbench_hard_bbh"
        }
    ],
    "cost": 0.01711525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 2025</p>
<p>Hanmeng Liu 
Hainan University
HaikouHainan</p>
<p>Zhiyang Teng 
ByteDance SG</p>
<p>Ruoxi Ning 
Westlake University
HangzhouZhejiangChina</p>
<p>Yiran Ding 
Westlake University
HangzhouZhejiangChina</p>
<p>Xiulai Li 
Hainan University
HaikouHainan</p>
<p>Xiaozhang Liu 
Hainan University
HaikouHainan</p>
<p>Yue Zhang 
Westlake University
HangzhouZhejiangChina</p>
<p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 20252E53C0DD83F28EA3B1EBC449B1AC2C90arXiv:2310.09107v2[cs.CL]Large Language ModelLarge Reasoning ModelLogical reasoningNatural Language Inference
Large language models (LLMs) have shown significant general language understanding abilities.However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding.To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios.Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date.GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.It garnered over 300 citations upon its release.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) [50,67], especially reasoning language models [18,51] demonstrate advanced capabilities in complex reasoning tasks and show significant adaptability and versatility across various applications, from simple everyday tasks to specialized domains such as coding, mathematics, law, medicine, and finance [11,22,34,37,76].Quantitative evaluation of LLM reasoning has thus become a very important task.To this end, existing work has considered mathematical reasoning [15,26], algorithmic problem solving [9,58], and knowledge-driven reasoning [25,73].</p>
<p>Logical reasoning is a cornerstone of human intelligence and has been a central focus in artificial intelligence research since its inception [16,29,33].However, evaluating verbal reasoning turned out to be too difficult in the 1950s due to insufficient natural language understanding (NLU) technologies, and thus AI researchers focused on formal logical reasoning instead [29,48,49].Since the 2010s, NLU has witnessed huge advances, where reading comprehension [8,21] and natural language inference [7,74] tasks were solved with high accuracies, which made verbal reasoning evaluation feasible [43,80].Figure 1 illustrates an example of logical reasoning in reading comprehension.To address such questions, LLMs must engage in multi-step, algorithmic, symbolic reasoning.This makes logical reasoning an ideal testbed for evaluating LLMs' ability to process complex natural language information accurately, robustly, and logically.</p>
<p>To this end, we introduce the General Logical Reasoning Evaluation (GLoRE) benchmark, designed to assess instruction-tuned LLMs on various logical reasoning tasks.GLoRE evaluates the strengths and limitations of LLMs in this domain, similar to how GLUE [71] and Super-GLUE [70] benchmark natural language understanding.GLoRE includes three types of logical reasoning tasks: Multi-choice Reading Comprehension [35], Natural Language Inference (NLI) [17], and True-or-False (Yes-or-No) Questions [13].These tasks encompass a wide range of logical reasoning phenomena, with high-quality datasets that remain challenging for pre-trained language models [13,27,32].In total, GLoRE covers 12 datasets with 72,848 instances.Since its release in 2023, GLoRE has been used for evaluating language models, receiving over 300 citations on ArXiv.</p>
<p>Using GLoRE, we report the logical reasoning capabilities of commercial models such as GPT-4 and OpenAI o1 [51], as well as popular open-source models such as LLaMA [67], Falcon [1], Mistral [30], DeepSeek R1 [18], and QwQ-32B [66].We test their instruction-following and problem-solving abilities in logical reasoning tasks.Results show that while commercial LLMs like GPT-4 still excel in zero-shot settings and approach human performance on specific datasets like ReClor, open-source models like QwQ-32B now rival or even surpass commercial counterparts in key tasks, achieving state-of-the-art results on multiple benchmarks.This underscores rapid advancements in open-source LLMs, narrowing the performance gap with commercial models.However, performance varies significantly across datasets, indicating sensitivity to data distribution.This sensitivity is further confirmed by observations that in-context learning and supervised fine-tuning primarily enhance LLM performance on specific test distributions, demonstrating their strong learning ability.While LLMs show promise in logical reasoning, their robustness to data distribution variations remains a challenge, highlighting the need for further improvement.</p>
<p>Related Work</p>
<p>Logical reasoning with natural language.Tapping into logical reasoning capabilities represents a holistic endeavour in natural language understanding (NLU).A variety of methods have been explored to realize this objective, including symbolic systems [45,47,55], fine-tuning of language models [28,41,71,78], and hybrid approaches combining neural and symbolic elements [36,59,60].</p>
<p>The recent introduction of evaluation datasets, notably LogiQA [43] and Reclor [80], has reinvigorated the focus on logical reasoning in NLP research.Logical reasoning is now leveraged in numerous probing tasks over large Pretrained Language Models (PLMs) and applied to downstream tasks such as question-answering and dialogue systems [6,63].Despite these advancements, the aspiration to emulate human-like logical reasoning capabilities within NLU systems remains a significant challenge for traditional models [27,43].In this study, our goal is not only to quantitatively evaluate the capability of Large Language Models (LLMs) in addressing the previously mentioned challenge but also to underscore the significance of our work in providing a validated platform for enhancing various reasoning methods with our data.</p>
<p>LLM reasoning evaluation.Despite progress in evaluating LLMs for specific reasoning tasks like arithmetic [57] and commonsense [4], a yawning gap exists in comprehensively assessing their logical reasoning.While LLMs excel at specific tasks like arithmetic reasoning [57], they face challenges in complex areas like multi-step reasoning [23] and abstract scenarios [24].ChatGPT exhibits strengths in chat-specific reasoning and some commonsense domains [4,53], but struggles with tasks requiring longer chains of inference [4].Other LLMs like FLAN-T5 [12], LLaMA [67], and PaLM [2] show potential in general deductive reasoning [61], while InstructGPT and Codex excel in specialized domains like medical reasoning [38].Despite these advances, limitations in data bias [52], and complex reasoning tasks necessitate further research and optimization to fully unlock the reasoning potential of LLMs [77].</p>
<p>Big-Bench Hard (BBH) [64] isolates 23 most challenging tasks from BIG-Bench [3].These tasks comprise general language understanding, arithmetic and algorithmic reasoning, and logical deduction.However, in comparison to our benchmark, the data size of the logical reasoning section in BBH is very small.HumanEval [10] serves as a hand-written evaluation set for coding.The programming problems included are designed to assess language comprehension, reasoning, algorithms, and simple mathematics.While similar to logical reasoning in that code generation necessitates complex reasoning skills, GLoRE differs in presenting logical reasoning problems via natural language prompts.ARB [62] is a benchmark for advanced reasoning over multiple fields like mathematics, physics, biology, chemistry, and law.Similar to GLoRE, it introduces a challenging subset of math and physics problems that require advanced symbolic reasoning.However, the benchmark constraints its problem on the above subjects with domain knowledge, not general logical reasoning questions, which is the focus of GLoRE.</p>
<p>The GLoRE Dataset</p>
<p>As mentioned in the introduction, GLoRE contains three NLU tasks: Multichoice Reading Comprehension, NLI, and Yes-or-No.First, Multi-choice reading comprehension [35] is essential in verbal reasoning tests, which cover abundant high-quality logical reasoning problems in the wild.Second, Unlike multi-choice reading comprehension, NLI [17] is more general and centric on entailment relations in a simpler task format, which is a fundamental task for evaluating reasoning abilities [19,54].Third, the Yes-or-No reasoning task [13] is a combination of question-answering and textual entailment, which can serve as a playground for testing models' reasoning abilities [14,65].The data statistics are shown in Table 1.</p>
<p>Multi-choice Reading Comprehension (MRC)</p>
<p>Within the standard multi-choice reading comprehension (MRC) task setting, a system is presented with a passage and a question, and the objective is to choose the most suitable answer from a set of candidate responses.Particularly, GLoRE contains five such datasets: LogiQA [43] is a logical MRC dataset derived from the Chinese Civil Service Examination, translated into English, and made available in both Chinese and English versions.We adopt LogiQA 2.0 [40] and use both the English (LogiQA 2.0) and Chinese (LogiQA 2.0 zh) test sets for our evaluation.</p>
<p>ReClor [80] comprises question-answering examples from the LSAT exams designed to assess human logical reasoning abilities.We use the development set for our testing as the test set does not provide gold labels.AR-LSAT [72] is a dataset of analytical reasoning questions from the Law School Admission Test.Each question contains five options rather than four.</p>
<p>LogiQA22 is collected and processed according to the LogiQA 2.0 format after ChatGPT was released.It incorporates the newly released Chinese Civil Servant Exams from 2022, which are not included in the original LogiQA dataset.</p>
<p>Natural Language Inference (NLI)</p>
<p>NLI is the task of determining the logical relationship between a hypothesis and a premise.The typical scheme involves text classification, where the model selects one of three labels: entailment, contradiction, and neutral.ConTRoL [39] is an NLI dataset that offers an in-depth examination of contextual reasoning within the NLI framework.Approximately 36.2% of premisehypothesis pairs fall under the category of logical reasoning in this dataset.We choose the logical reasoning portion for our evaluation.HELP [79] is an NLI dataset emphasizing monotonicity reasoning, a crucial concept in Natural Logic [46].We use the training set for our evaluation.TaxiNLI [31] is an NLI dataset that has been re-annotated based on MNLI [75], with categories include logical categories such as connectives, mathematical reasoning, and deduction.NaN-NLI [68] is a test suite designed to probe the capabilities of NLP models in capturing sub-clausal negation.The successful handling of sub-clausal negation can be seen as a strong indicator of a model's logical reasoning capacity.</p>
<p>True-or-False (Yes-or-No) Questions (TF)</p>
<p>FraCaS test suite [56] presents complex entailment problems involving multipremised contexts as a three-way classification task.The ability to determine entailment relationships in this context is closely tied to logical reasoning.RuleTaker [14] dataset is a synthetic creation designed to examine the reasoning ability of transformer models [69] over natural language rules.This task explicitly targets logical reasoning by asking models to reason over a set of rules and facts to generate true-or-false responses as output.ProofWriter [65] dataset generates sets of facts and rules, each followed by questions, which can be proven true or false using proofs of various depths.</p>
<p>Experiments</p>
<p>We employ GLoRE to assess the logical reasoning capabilities across different categories of language models, including traditional pre-trained models and reasoning-enhanced LLMs, both open-source and proprietary.We conduct a comprehensive comparative analysis of their performance against human benchmarks.</p>
<p>Experimental Settings</p>
<p>We adopted RoBERTa-base [44] as a baseline, fine-tuning it on the training set over five epochs for each dataset.The community models selected for comparison include Falcon-40b-instruct [1], LLaMA-30b-supercot [67] Mixtral-8x7b, DeepSeek R1 [18] and QwQ-32B [66].For OpenAI models, we choose ChatGPT, GPT-4 and o1 mini [51].</p>
<p>Our evaluation metrics consisted of classification accuracy scores.Additionally, we utilized reported accuracies for datasets where human performance data was available and recorded both the average and peak performance of human participants to establish a human baseline.For the LogiQA22 dataset, we engaged five co-authors as test subjects and computed their accuracy based on 150 test examples.</p>
<p>Main Results</p>
<p>Zero-shot results.Table 2 summarizes the zero-shot evaluation results.The first block shows human performance.The second block presents RoBERTabase's supervised fine-tuning results.With 125M parameters, RoBERTa-base achieves 48.76% and 33.22% accuracy on LogiQA 2.0 and LogiQA22, respectively, lagging behind human performance.It performs better on NLI and TF tasks than MRC tasks, likely due to output ambiguities.On NaN-NLI, RoBERTa achieves 90.02% accuracy, matching human performance, possibly due to learning superficial patterns from rule-based negation data.On ProofWriter, RoBERTabase scores 55.92%, indicating potential for specific logical reasoning tasks.</p>
<p>The third block shows zero-shot results for LLaMA, Falcon, and Mixtral.LLaMA and Falcon perform similarly (32.34% vs. 32.28%),suggesting comparable reasoning capabilities despite LLaMA-30B's smaller size.Both underperform RoBERTa-base on most tasks, except Falcon on RT.On MRC tasks, their accuracy is around 20%, worse than random guessing in 4-way classification, indicating challenges in logical reasoning without in-context demonstrations.</p>
<p>Performance gaps between LogiQA and LogiQA22 are smaller for these models, suggesting stable performance across data distributions without in-domain tuning.Mixtral-8x7b outperforms LLaMA and Falcon, demonstrating the effectiveness of mixture-of-expert models.</p>
<p>The fourth block provides zero-shot results ChatGPT and GPT-4.Both models, especially GPT-4, surpass RoBERTa-base on several MRC benchmarks.However, GPT-4's accuracy drops significantly on LogiQA22 (58.49% vs. 72.25% on LogiQA 2.0), indicating sensitivity to data distribution.In NLI and TF tasks, ChatGPT and GPT-4 outperform RoBERTa, with ChatGPT achieving 58.45% accuracy on ConTRoL, surpassing GPT-4.GPT-4's NLI performance varies across datasets, further highlighting its sensitivity to data distribution.TF task results show similar inconsistencies, suggesting model rationales differ from human reasoning.</p>
<p>The final block shows results for o1 mini, DeepSeek R1, and QwQ-32B, which achieve notable improvements over prior models.QwQ-32B attains the highest average accuracy (78.95%), surpassing GPT-4 (66.34%) and DeepSeek R1 (75.14%).It achieves state-of-the-art results on MRC tasks like ReClor (93.76%) and AR-LSAT (92.35%), indicating the need for more challenging benchmarks for logical reasoning.Its robustness is evident in LogiQA22 (86.30%), outperforming GPT-4 by 27.81 percentage points.However, QwQ-32B shows uneven performance on NLI datasets, such as HELP (61.53%, lagging behind o1 mini's 63.69%), suggesting its reasoning capabilities are less generalizable in tasks requiring fine-grained entailment analysis.</p>
<p>While GPT-4 retains an advantage on FraCas (75.35%),QwQ-32B surpasses GPT-4 on ReClor (93.76% vs. 87.20%),redefining state-of-the-art performance for MRC tasks.QwQ-32B and DeepSeek R1 showcase balanced performance across most tasks, with QwQ-32B achieving unprecedented TF results (e.g., 82.40% on ProofWriter, outperforming both GPT-4's 59.66% and DeepSeek R1's 80.51%).Though still below the human average overall, these models mark substantial progress -QwQ-32B's 78.95% average accuracy (vs.DeepSeek R1's 75.14% and GPT-4's 66.34%) highlights significant architectural or training innovations for logical inference.</p>
<p>Few-shot results.LLMs excel at in-context learning [20], where performance improves with context examples and demonstration methods [42].For this study, we randomly sampled instances (1 for 1-shot, 2 for 2-shot, and 5 for 5-shot) from each dataset and appended them to the prompt.We used the same model configuration as in the zero-shot scenario.Table 3 highlights the impact of in-context learning (ICL), as seen in GPT-4's 9% accuracy gain with 5-shot learning.However, this improvement stems from statistical adaptation rather than true reasoning, as models rely on superficial patterns rather than humanlike logical inference.This aligns with findings that chain-of-thought prompts correlate with outputs but do not causally drive reasoning [5].While reasoningenhanced models narrow the gap with human performance, their sensitivity to data distribution highlights the need for further research into more robust reasoning mechanisms.GLoRE's evolving framework will continue to track these advancements.</p>
<p>Analysis</p>
<p>Large language models vs. reasoning-enhanced models.The reasoningenhanced models like QwQ-32B, DeepSeek R1, and OpenAI's o1 mini demonstrate significant improvements over traditional LLMs.QwQ-32B, in particular, achieves the highest average performance (78.95%), indicating that its reinforcement learning framework or specialized training methodology enables better generalization across data distributions.While QwQ-32B dominates MRC and TF tasks, its relatively lower performance on NLI datasets like HELP (61.53%) suggests that even state-of-the-art models struggle with tasks requiring monotonicity or negation reasoning, highlighting the need for broader evaluation beyond task-specific robustness.Data leakage concerns.While GLoRE includes diverse datasets, potential data leakage risks arise from overlapping sources.GPT-4's lower accuracy on LogiQA22 (58.49%) compared to LogiQA 2.0 (72.25%) suggests limited exposure to newer data, reducing leakage concerns but highlighting distributional sensitivity.The benchmark's dynamic updates and inclusion of newly annotated datasets help mitigate leakage by testing models on unseen distributions.Sensitivity to data distribution.The above experiments show that the performance of LLMs is sensitive to the data distribution.Even though the underlying reasoning principles are the same, LLM performance varies significantly across datasets.This suggests that LLMs might not reason using the correct rationale but rely on superficial features.As shown in Table 2, although GPT-4 achieves near-human performance on datasets like ReClor (87.20%) and NaN-NLI (75.74%), it lags significantly on others (e.g., HELP at 46.01%).This inconsistency mirrors the behavior of reasoning-enhanced models like DeepSeek R1, revealing a critical divergence from human reasoning: once humans master a reasoning pattern, their performance generalizes robustly, whereas LLMs remain sensitive to data-specific features.</p>
<p>Conclusion</p>
<p>We constructed GLoRE, a dynamic and comprehensive benchmark tailored for assessing the logical reasoning capabilities of advanced language models, including GPT-4 and various strong open-source LLMs across multiple reasoning tasks.</p>
<p>Our findings indicate that QwQ-32B, a reasoning-enhanced model, sets a new state-of-the-art on the GLoRE benchmark, significantly narrowing the gap to human performance.This underscores the potential of targeted architectural and training innovations for logical reasoning.GLoRE will be continually maintained to track advancements in this rapidly evolving domain.</p>
<p>Fig. 1 .
1
Fig. 1.Instruction and question format for logical reading comprehension tasks.</p>
<p>Table 1 .
1
Data statistics.("E": entailment; "C": contradiction; "N": neutral.)
DatasetSizeTargetDatasetSizeTargetLogiQA 2.0 test1,572 4-way multi-choice ConTRoL805E, C, NLogiQA 2.0 zh test 1,594 4-way multi-choice HELP35,891E, C, NReClor dev500 4-way multi-choice TaxiNLI test10,071E, C, NAR-LSAT test230 5-way multi-choice NaN-NLI259E, C, NLogiQA221,354 4-way multi-choice FraCas346 Yes, No, NeutralRuleTaker dev 10,068Yes, NoProofWriter dev 10,158Yes, No</p>
<p>Table 3 .
3
Average accuracies on GLoRE few-shot evaluation.
Model0-shot 1-shot 2-shot 5-shotLLaMA32.34 32.89 35.03 39.62Falcon32.28 33.14 33.76 35.72ChatGPT 52.10 55.85 57.43 60.32GPT-466.34 70.31 71.44 75.83</p>
<p>. Mrc Nli Task, Avg Tf, Lq Lq Zh Rc Al Lq22 Ct Hl Tn Nn Fc Rt Dataset, Pw Human, Avg, 86.00 88.00 63.00 56.00 83.00 87.00 81.00 97.00 94.00 92.00 84.00 82.00 82.75</p>
<p>Human Ceiling. 95.00 96.00 100.00 91.00 99.00 94.00 95.00 100.00 100.00 97.00 95.00 93.00 96.25</p>
<p>. Deepseek, R1 76.22 81.49 77.88 90.01 71.63 78.37 62.05 75.74 72.58 59.96 75.29 80.51 75.14</p>
<p>All results are in %, the best ones are in bold, and the second best ones are in underline. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, LQ: LogiQA 2.0, RC : Re-Clor, AL: AR-LSAT, CT : ConTRoL, HL: HELP, TN : TaxiNLI, NN : NaN-NLI, FC : FraCas, RT : RuleTaker, PW : ProofWriter. Penedo, G.2023Table 2. LLMs' performance on the GLoRE benchmark. Falcon-40B: an open large language model with state-of-the-art performance</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, Palm 2 technical report. 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. B Bench Authors, 2023TMLR</p>
<p>Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>How likely do llms with cot mimic human reasoning?. G Bao, H Zhang, C Wang, L Yang, Y Zhang, arXiv:2402.160482024arXiv preprint</p>
<p>Logical reasoning for task oriented dialogue systems. S Beygi, M Fazel-Zarandi, A Cervone, P Krishnan, S R Jonnalagadda, 2022</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, Proc. of EMNLP. of EMNLP2015</p>
<p>A thorough examination of the CNN/daily mail reading comprehension task. D Chen, J Bolton, C D Manning, ACL. 2016</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>Chatgpt goes to law school. J H Choi, K E Hickman, A Monahan, D Schwarcz, Available at SSRN. 2023</p>
<p>H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. C Clark, K Lee, M W Chang, T Kwiatkowski, M Collins, K Toutanova, 2019</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, Proc. of IJCAI. of IJCAI2020</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>M J Cresswell, Logics and languages. Routledge19731st ed.</p>
<p>The pascal recognising textual entailment challenge. I Dagan, O Glickman, B Magnini, 2005MLCW</p>
<p>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025</p>
<p>Transforming question answering datasets into natural language inference datasets. D Demszky, K Guu, P Liang, 2018</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, L Li, Z Sui, A survey on in-context learning. 2023</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proc. of AACL. of AACL2019</p>
<p>. S Frieder, L Pinchetti, R R Griffiths, T Salvatori, T Lukasiewicz, P C Petersen, A Chevalier, J Berner, 2023Mathematical capabilities of chatgpt</p>
<p>Chain-of-thought hub: A continuous effort to measure large models' reasoning performance. Y Fu, L Ou, M Chen, Y Wan, H Peng, T Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Large language models are not abstract reasoners. G Gendron, Q Bao, M Witbrock, G Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR. the International Conference on Learning Representations (ICLR2021</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>J Huang, K C C Chang, Towards reasoning in large language models: A survey. 2023</p>
<p>Y Huang, M Fang, Y Cao, L Wang, X Liang, arXiv:2103.14349Dagn: Discourse-aware graph network for logical reasoning. 2021arXiv preprint</p>
<p>L Iwańska, Logical reasoning in natural language: It is all about knowledge. Minds and Machines. 1993</p>
<p>. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, Others, 2024Mixtral of experts</p>
<p>Taxinli: Taking a ride up the NLU hill. P Joshi, S Aditya, A Sathe, M Choudhury, 2020CoRR</p>
<p>ContractNLI: A dataset for document-level natural language inference for contracts. Y Koreeda, C Manning, Proc. of EMNLP Findings. of EMNLP Findings2021</p>
<p>Logic for problem solving. R Kowalski, 1979Ediciones Díaz de Santos</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepaño, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, 2023e0000198</p>
<p>RACE: Large-scale Reading Comprehension dataset from Examinations. G Lai, Q Xie, H Liu, Y Yang, E Hovy, EMNLP. 2017</p>
<p>Augmenting neural networks with first-order logic. T Li, V Srikumar, Proc. of ACL. of ACL2019</p>
<p>Competitionlevel code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, Others, 2022</p>
<p>Can large language models reason about medical questions?. V Liévin, C E Hother, O Winther, arXiv:2207.081432022arXiv preprint</p>
<p>Natural language inference in context -investigating contextual reasoning over long texts. H Liu, L Cui, J Liu, Y Zhang, 2020CoRR</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. H Liu, J Liu, L Cui, Z Teng, N Duan, M Zhou, Y Zhang, Speech, and Language Processing. 2023</p>
<p>Logicot: Logical chainof-thought instruction tuning. H Liu, Z Teng, L Cui, C Zhang, Q Zhou, Y Zhang, Proc. of EMNLP Findings. of EMNLP Findings2023</p>
<p>. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, What makes good in-context examples for gpt-3? (2021</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. J Liu, L Cui, H Liu, D Huang, Wang, Y Zhang, 2020CoRR</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXivRoberta: A robustly optimized bert pretraining approach. 2019</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Programs with common sense. J Mccarthy, 2002</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. J Mccarthy, P J Hayes, Machine Intelligence. 41969</p>
<p>The logic theory machine-a complex information processing system. A Newell, H Simon, IRE Transactions on Information Theory. 1956</p>
<p>OpenAI: Gpt-4 technical report. 2023</p>
<p>Openai, Openai o1 system card. 2024</p>
<p>Human-like problem-solving abilities in large language models using chatgpt. G Orrù, A Piarulli, C Conversano, A Gemignani, Frontiers in Artificial Intelligence. 11993502023</p>
<p>S Ott, K Hebenstreit, V Liévin, C E Hother, M Moradi, M Mayrhauser, R Praas, O Winther, M Samwald, arXiv:2301.11596Thoughtsource: A central hub for large language model reasoning data. 2023arXiv preprint</p>
<p>Collecting diverse natural language inference problems for sentence representation evaluation. A Poliak, A Haldar, R Rudinger, J E Hu, E Pavlick, A S White, B Van Durme, Proc. of EMNLP. of EMNLP2018</p>
<p>Theorist: A Logical Reasoning System for Defaults and Diagnosis. D Poole, R Goebel, R Aleliunas, 1987</p>
<p>Using the framework. S G Pulman, 1996</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. C Qin, A Zhang, Z Zhang, J Chen, M Yasunaga, D Yang, 2023</p>
<p>S Quan, J Yang, B Yu, B Zheng, D Liu, A Yang, X Ren, B Gao, Y Miao, Y Feng, arXiv:2501.01257Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. 2025arXiv preprint</p>
<p>Prover: Proof generation for interpretable reasoning over rules. S Saha, S Ghosh, S Srivastava, M Bansal, 2020</p>
<p>S Sanyal, H Singh, X Ren, Fairr: Faithful and robust deductive reasoning over natural language. 2022</p>
<p>A Saparov, R Y Pang, V Padmakumar, N Joshi, S M Kazemi, N Kim, H He, arXiv:2305.15269Testing the general deductive reasoning capacity of large language models using ood examples. 2023arXiv preprint</p>
<p>T Sawada, D Paleka, A Havrilla, P Tadepalli, P Vidas, A Kranias, J J Nay, K Gupta, A Komatsuzaki, Arb: Advanced reasoning benchmark for large language models. 2023</p>
<p>Neural natural logic inference for interpretable question answering. J Shi, X Ding, L Du, T Liu, B Qin, Proc. of EMNLP. of EMNLP2021</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, 2022</p>
<p>O Tafjord, B D Mishra, P Clark, Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2021</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Q Team, 2025</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation. T H Truong, Y Otmakhova, T Baldwin, T Cohn, J H Lau, K Verspoor, Proc. of AACL. of AACL2022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 2017CoRR</p>
<p>A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S R Bowman, SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. 2019</p>
<p>GLUE: A multitask benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018</p>
<p>From lsat: The progress and challenges of complex reasoning. S Wang, Z Liu, W Zhong, M Zhou, Z Wei, Z Chen, N Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2022</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo, W Ren, A Arulraj, X He, Z Jiang, Proc. of NeurIPS. of NeurIPS2024</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of AACL. of AACL2018</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of NAACL. of NAACL2018</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, arXiv:2307.024772023arXiv preprint</p>
<p>Logiformer. F Xu, J Liu, Q Lin, Y Pan, L Zhang, Proc. of SIGIR. of SIGIR2022</p>
<p>Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning. H Yanaka, K Mineshima, D Bekki, K Inui, S Sekine, J Bos, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (<em>SEM2019). the Eighth Joint Conference on Lexical and Computational Semantics (</em>SEM2019)2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. W Yu, Z Jiang, Y Dong, J Feng, Proc. of ICLR. of ICLR2020</p>            </div>
        </div>

    </div>
</body>
</html>
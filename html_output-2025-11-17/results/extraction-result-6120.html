<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6120 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6120</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6120</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-c29dbfbc17fa190b787a2662d49f08a38c8bd166</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c29dbfbc17fa190b787a2662d49f08a38c8bd166" target="_blank">ARB: Advanced Reasoning Benchmark for Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces ARB, a novel benchmark composed of advanced reasoning problems in multiple fields, featuring problems in mathematics, physics, biology, chemistry, and law, and introduces a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6120.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6120.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Advanced Reasoning Benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-domain benchmark of advanced reasoning problems (mathematics, physics, chemistry, biology, and law) drawn from graduate-level exams and professional resources, designed to be substantially more challenging than prior LLM benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Task-specific automatic scoring for multiple-choice and numerical questions, SymPy-based parsing for symbolic answers, and manual human grading for proof-like/open-response questions; supplemented by model-based rubric evaluation (GPT-4-generated rubrics) for symbolic and proof-like answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact-choice match for multiple-choice; parsed numerical equality within a relative error threshold of 1% (10^-2) for numerical answers; symbolic equivalence checking (SymPy normalization and permutation-based equivalence where possible); rubric-point allocation for intermediate steps; human expert correctness for proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Evaluated models include GPT-4 (gpt-4-0314), GPT-3.5 (gpt3.5-turbo-0301), text-davinci-003, and Claude (claude-v1.3-100k).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Mathematics, Physics, Chemistry, Biology, Law (focus on quantitative mathematical/physical domains).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a single theory but a suite of problems and reference solutions used to evaluate models' ability to generate correct scientific reasoning, symbolic derivations, and proof-like arguments; effectively evaluates LLM-generated 'theories' or solutions in advanced STEM domains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Current state-of-the-art models score well below 50% on demanding ARB subsets; symbolic answer manual scores for GPT-4: Math 18%, Physics 28% (Table 2); multiple-choice performance high on Law/MCAT but numerical and symbolic parts show low accuracy; parsing/formatting failures and reasoning errors are common.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ARB dataset (this work) comprising multiple-choice, short-answer numerical, symbolic, and open-response proof-like problems; sources include Putnam/contest books, university problem collections, PhD qualifying exams, MCAT practice tests, and bar exam practice sets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human graders are the gold standard for proof-like and many symbolic problems; model-based rubric scoring shows promising agreement with humans (correlations reported), but human grading remains necessary for many open-response items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential data contamination (some source material may be in model training sets), difficulty of automatically parsing symbolic / proof-like answers, expensive human grading for proofs, and imperfect coverage of alternative correct solutions by automated rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ARB: Advanced Reasoning Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6120.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6120.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rubric-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-generated rubric evaluation (GPT-4 rubric self-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation pipeline where GPT-4 (few-shot prompted) generates a rubric from a reference solution (typically a 10-point rubric) and then scores model-produced intermediate reasoning steps against that rubric to assign partial credit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Few-shot prompting of GPT-4 to produce stepwise rubrics from reference solutions; then GPT-4 is prompted to step through candidate solutions, assign points per rubric item, and produce a total score (out of 10).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Rubric coverage (does rubric capture key subproblems), rubric point spread (relative point allocation), and per-step partial credit following rubric; numeric total score compared to human-annotated rubric score.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (rubric generation and rubric-based scoring); GPT-3.5-turbo examined but found weaker for rubric generation.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Symbolic mathematics, physics symbolic problems, and proof-like mathematical problems within ARB.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Used to evaluate the correctness and completeness of LLM-generated intermediate reasoning steps (i.e., whether the generated solution constitutes a valid scientific explanation/derivation by decomposing the reference solution into graded steps).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human vs. GPT-4 rubric scores: average human eval scores (out of 10) — Physics Symbolic 5.00, Math Symbolic 3.13, Proof-like 2.65; model eval scores — Physics 5.05, Math 3.37, Proof-like 3.8; Pearson correlations: Physics 0.91, Math 0.78, Proof-like 0.82 (Table 5). Rubric coverage Likert means: Physics 4.42, Math 4.26, Proof-like 3.94 (Table 4). Main failure modes: extra credit assignment (up to 40% in proof-like), occasional hallucinated rubric items.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied on ARB symbolic and proof-like subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>GPT-4 rubric scores correlate moderately-highly with human scores (Pearson r = 0.91, 0.78, 0.82 across domains), and self-eval rarely deviates more than two points from human-grade; main mismatch is when model awards partial credit to solutions outside rubric where humans give zero.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Rubrics can fail to cover alternative correct solutions (though rare on ARB), GPT-4 sometimes grants extra or reduced credit erroneously (extra credit rates reported 18–40%), occasional hallucinated rubric items (15% for Math Symbolic in their sample), and rubrics are not yet reliable enough to replace human grading for all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ARB: Advanced Reasoning Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6120.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6120.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated answer-type evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answer-type specific automatic evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of automatic evaluation procedures tailored to answer type: multiple-choice exact match, numerical answer parsing with SymPy and relative-error thresholding, symbolic equivalence checking via SymPy normalization and variable-permutation equivalence, and manual grading for proof-like answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Format enforcement (instruct model to put final answer after 'ANSWER:' delimiter); for multiple-choice exact match to ground truth; for numerical: unit removal via regex, parse with SymPy, compare relative error < 0.01; for symbolic: SymPy normalization and equivalence up to variable permutation where possible; proof-like: human expert grading.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Delimiter presence and parsability; for numerical answers, correctness if |model - ground_truth| / ground_truth < 0.01; for symbolic answers, accepted if SymPy determines algebraic equivalence (within parser limitations); failure to parse or missing delimiter yields an incorrect mark.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to outputs of GPT-4, gpt-3.5-turbo, text-davinci-003, claude-v1.3-100k in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Numerical and symbolic problems primarily in mathematics and physics; multiple-choice across law and MCAT.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluates LLM-generated scientific solutions by checking final-answer format and automated symbolic/numerical equivalence to reference solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Multiple-choice parsing >97% with proper prompting; GPT-4 parsed correctly >99% of multiple-choice outputs. Numerical scoring uses 1% relative error; symbolic automatic parsing works for subset of expressions (function-form), but is error-prone; many symbolic/proof-like answers still require human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ARB (numerical, symbolic subsets); SymPy used for parsing and equivalence checking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Automated rules provide clear, fast scores for structured answer types; for symbolic and proof-like answers, automated methods under-cover correct but differently formatted solutions and require human adjudication for full reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SymPy parsing fails on many symbolic formats and set-notation; normalization across alternative equivalent forms is hard; formatting mismatches or omitted delimiter cause false negatives; unit handling requires heuristics (regex) and can be brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ARB: Advanced Reasoning Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6120.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6120.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT symbolic equivalence grading</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based symbolic equivalence grading using GPT-3.5 (student evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using GPT-3.5-turbo to compare two symbolic expressions and assign coarse equivalence scores (0, 0.5, 1), as a cheaper way to grade symbolic final answers extracted from stronger LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt GPT-3.5-turbo to compare an extracted student symbolic final answer to a reference expression and output an equivalence score (0, 0.5, 1) indicating non-equivalence, near-equivalence (e.g., up to a constant), or exact match.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Categorical equivalence judgement (0/0.5/1); used as a proxy to human symbolic equivalence checks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5-turbo-0301 used as the grader; student models evaluated included GPT-4 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Symbolic mathematics and symbolic physics answers in ARB.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluates whether an LLM-generated symbolic expression matches the ground truth symbolic expression (equivalence or close equivalence).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On a sample, GPT-3.5-turbo achieved zero false positives but a substantial number of false negatives; Table 8 reports accuracy ~0.67 (Physics Symbolic) and ~0.76 (Math Symbolic) with true positives and false negatives indicating under-recognition of equivalence, especially for expressions with >3 variables.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to ARB symbolic subsets; results reported in Table 8.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>GPT-3.5-turbo is conservative (low false positive) but under-credits correct but differently formatted expressions vs. human graders, leading to underestimation of true performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fails to recognize equivalence for more complex multi-variable expressions, sensitive to presentation/formatting of student answer, and may underestimate model performance by producing false negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ARB: Advanced Reasoning Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6120.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6120.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human expert grading</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual human grading of proof-like and complex symbolic solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human experts with mathematical training manually inspect and grade proof-like and many symbolic responses, especially when automatic parsing and model-based rubrics are insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Expert human graders read solution traces and compare intermediate reasoning to reference solutions, assigning partial credit including half-credit for partially completed subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Thorough inspection of intermediate steps, correctness of reasoning and derivations, and whether subproblems are solved; grading may assign 0.5 when at least half of subparts are correct.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to outputs from GPT-4 and other evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Proof-like mathematics, advanced symbolic problems in math and physics.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Human grading evaluates the extent to which the LLM-generated solution constitutes a correct scientific proof or valid derivation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human grading finds very low success rates on proof-like tasks (raw GPT-4 proof-like correctness ~5%; rises to 16% if graders give 0.5 for partially solved multipart problems).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ARB proof-like subset (Harvard quals, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human grading is the reference standard and is used to validate model-based rubric scores; human grades correlate with GPT-4 rubric-scores but remain necessary for many open-response tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Manual grading is time-consuming and expensive; attributing error types can be fuzzy; human inter-annotator agreement and consistency require effort.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ARB: Advanced Reasoning Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6120.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6120.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error taxonomy (extended)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended error taxonomy for LLM reasoning failures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A categorization of common failure modes of LLM reasoning on math and physics problems: misread/misunderstood problem, wrong approach, logical error / hallucination, and arithmetic/calculation errors (extension of a prior three-way taxonomy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual annotation of model chain-of-thought outputs to label error types; non-exclusive categories so a single solution can be labeled with multiple error types.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Proportions of solutions exhibiting each error class per problem type, reported as percentages (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (analysis of its errors reported), with implications for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Mathematics (numerical, symbolic, proof-like) and Physics (numerical, symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Labels used to analyze why LLM-generated scientific solutions (theories / derivations) are incorrect, distinguishing whether failure stems from misunderstanding, incorrect method, logical fallacy/hallucination, or calculation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 3 reports detailed percentages for GPT-4: e.g., Math Numerical: Wrong approach 25%, Logical/hallucination 88%, Arithmetic 48%, Correct answer 3%, Correct reasoning 3%; Math Symbolic: Misread 16%, Wrong approach 50%, Logical/hallucination 29%, Arithmetic 4%, Correct answer 16%, Correct reasoning 16%; Physics Numerical: Wrong approach 80%, Logical 53%, Arithmetic 6%, Correct answer 6% etc.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Derived from a subsample (20–40 problems per large subset) of ARB math and physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Error annotations performed by human graders using guidelines; these labels are used to interpret model failures and to guide rubric construction and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Error attribution is inherently fuzzy and non-exclusive; some mistakes (logical vs hallucination) are hard to distinguish; subsampling limits precise prevalence estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ARB: Advanced Reasoning Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6120.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6120.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formatting & extraction protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delimited-answer extraction and formatting checks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical evaluation protocol requiring models to print the final answer after an explicit delimiter 'ANSWER:' to enable automatic extraction and parsing of final responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt engineering rule: instruct models to append final answer after 'ANSWER:' delimiter; parse text after delimiter and mark as incorrect if missing. Additional parsing steps include regex unit stripping for numerical answers before SymPy parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presence of delimiter and parsability of extracted answer; absence of delimiter or unparseable answer triggers incorrect marking.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to GPT-4, gpt-3.5-turbo, text-davinci-003, and Claude in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>All ARB answer types (multiple-choice, numerical, symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A pragmatic convention to standardize final-answer extraction for automated evaluation of LLM-generated solutions to scientific problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>With appropriate prompting, all models output a parsable answer >97% of the time for multiple-choice; gpt-3.5-turbo failed to output a parsable answer ~25% of the time on Law multiple-choice subset, while other models failed <5% and GPT-4 was parsed >99% of the time.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ARB (used in all automated scoring pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>This protocol simplifies automated grading but can mischaracterize answers if models do not follow the format; human evaluation used to validate parsing-based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Rigid format requirement can penalize correct answers presented in different formats; some models may refuse to conform or produce multiple candidate answers, causing parsing failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ARB: Advanced Reasoning Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>GPTScore: Evaluate as you desire <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6120",
    "paper_id": "paper-c29dbfbc17fa190b787a2662d49f08a38c8bd166",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "ARB",
            "name_full": "Advanced Reasoning Benchmark",
            "brief_description": "A multi-domain benchmark of advanced reasoning problems (mathematics, physics, chemistry, biology, and law) drawn from graduate-level exams and professional resources, designed to be substantially more challenging than prior LLM benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Task-specific automatic scoring for multiple-choice and numerical questions, SymPy-based parsing for symbolic answers, and manual human grading for proof-like/open-response questions; supplemented by model-based rubric evaluation (GPT-4-generated rubrics) for symbolic and proof-like answers.",
            "evaluation_criteria": "Exact-choice match for multiple-choice; parsed numerical equality within a relative error threshold of 1% (10^-2) for numerical answers; symbolic equivalence checking (SymPy normalization and permutation-based equivalence where possible); rubric-point allocation for intermediate steps; human expert correctness for proofs.",
            "llm_model_name": "Evaluated models include GPT-4 (gpt-4-0314), GPT-3.5 (gpt3.5-turbo-0301), text-davinci-003, and Claude (claude-v1.3-100k).",
            "theory_domain": "Mathematics, Physics, Chemistry, Biology, Law (focus on quantitative mathematical/physical domains).",
            "theory_description": "Not a single theory but a suite of problems and reference solutions used to evaluate models' ability to generate correct scientific reasoning, symbolic derivations, and proof-like arguments; effectively evaluates LLM-generated 'theories' or solutions in advanced STEM domains.",
            "evaluation_results": "Current state-of-the-art models score well below 50% on demanding ARB subsets; symbolic answer manual scores for GPT-4: Math 18%, Physics 28% (Table 2); multiple-choice performance high on Law/MCAT but numerical and symbolic parts show low accuracy; parsing/formatting failures and reasoning errors are common.",
            "benchmarks_or_datasets": "ARB dataset (this work) comprising multiple-choice, short-answer numerical, symbolic, and open-response proof-like problems; sources include Putnam/contest books, university problem collections, PhD qualifying exams, MCAT practice tests, and bar exam practice sets.",
            "comparison_to_human": "Human graders are the gold standard for proof-like and many symbolic problems; model-based rubric scoring shows promising agreement with humans (correlations reported), but human grading remains necessary for many open-response items.",
            "limitations_or_challenges": "Potential data contamination (some source material may be in model training sets), difficulty of automatically parsing symbolic / proof-like answers, expensive human grading for proofs, and imperfect coverage of alternative correct solutions by automated rubrics.",
            "uuid": "e6120.0",
            "source_info": {
                "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Rubric-based evaluation",
            "name_full": "Model-generated rubric evaluation (GPT-4 rubric self-evaluation)",
            "brief_description": "An automated evaluation pipeline where GPT-4 (few-shot prompted) generates a rubric from a reference solution (typically a 10-point rubric) and then scores model-produced intermediate reasoning steps against that rubric to assign partial credit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Few-shot prompting of GPT-4 to produce stepwise rubrics from reference solutions; then GPT-4 is prompted to step through candidate solutions, assign points per rubric item, and produce a total score (out of 10).",
            "evaluation_criteria": "Rubric coverage (does rubric capture key subproblems), rubric point spread (relative point allocation), and per-step partial credit following rubric; numeric total score compared to human-annotated rubric score.",
            "llm_model_name": "GPT-4 (rubric generation and rubric-based scoring); GPT-3.5-turbo examined but found weaker for rubric generation.",
            "theory_domain": "Symbolic mathematics, physics symbolic problems, and proof-like mathematical problems within ARB.",
            "theory_description": "Used to evaluate the correctness and completeness of LLM-generated intermediate reasoning steps (i.e., whether the generated solution constitutes a valid scientific explanation/derivation by decomposing the reference solution into graded steps).",
            "evaluation_results": "Human vs. GPT-4 rubric scores: average human eval scores (out of 10) — Physics Symbolic 5.00, Math Symbolic 3.13, Proof-like 2.65; model eval scores — Physics 5.05, Math 3.37, Proof-like 3.8; Pearson correlations: Physics 0.91, Math 0.78, Proof-like 0.82 (Table 5). Rubric coverage Likert means: Physics 4.42, Math 4.26, Proof-like 3.94 (Table 4). Main failure modes: extra credit assignment (up to 40% in proof-like), occasional hallucinated rubric items.",
            "benchmarks_or_datasets": "Applied on ARB symbolic and proof-like subsets.",
            "comparison_to_human": "GPT-4 rubric scores correlate moderately-highly with human scores (Pearson r = 0.91, 0.78, 0.82 across domains), and self-eval rarely deviates more than two points from human-grade; main mismatch is when model awards partial credit to solutions outside rubric where humans give zero.",
            "limitations_or_challenges": "Rubrics can fail to cover alternative correct solutions (though rare on ARB), GPT-4 sometimes grants extra or reduced credit erroneously (extra credit rates reported 18–40%), occasional hallucinated rubric items (15% for Math Symbolic in their sample), and rubrics are not yet reliable enough to replace human grading for all cases.",
            "uuid": "e6120.1",
            "source_info": {
                "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Automated answer-type evaluation",
            "name_full": "Answer-type specific automatic evaluation pipeline",
            "brief_description": "A set of automatic evaluation procedures tailored to answer type: multiple-choice exact match, numerical answer parsing with SymPy and relative-error thresholding, symbolic equivalence checking via SymPy normalization and variable-permutation equivalence, and manual grading for proof-like answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Format enforcement (instruct model to put final answer after 'ANSWER:' delimiter); for multiple-choice exact match to ground truth; for numerical: unit removal via regex, parse with SymPy, compare relative error &lt; 0.01; for symbolic: SymPy normalization and equivalence up to variable permutation where possible; proof-like: human expert grading.",
            "evaluation_criteria": "Delimiter presence and parsability; for numerical answers, correctness if |model - ground_truth| / ground_truth &lt; 0.01; for symbolic answers, accepted if SymPy determines algebraic equivalence (within parser limitations); failure to parse or missing delimiter yields an incorrect mark.",
            "llm_model_name": "Applied to outputs of GPT-4, gpt-3.5-turbo, text-davinci-003, claude-v1.3-100k in the experiments.",
            "theory_domain": "Numerical and symbolic problems primarily in mathematics and physics; multiple-choice across law and MCAT.",
            "theory_description": "Evaluates LLM-generated scientific solutions by checking final-answer format and automated symbolic/numerical equivalence to reference solutions.",
            "evaluation_results": "Multiple-choice parsing &gt;97% with proper prompting; GPT-4 parsed correctly &gt;99% of multiple-choice outputs. Numerical scoring uses 1% relative error; symbolic automatic parsing works for subset of expressions (function-form), but is error-prone; many symbolic/proof-like answers still require human evaluation.",
            "benchmarks_or_datasets": "ARB (numerical, symbolic subsets); SymPy used for parsing and equivalence checking.",
            "comparison_to_human": "Automated rules provide clear, fast scores for structured answer types; for symbolic and proof-like answers, automated methods under-cover correct but differently formatted solutions and require human adjudication for full reliability.",
            "limitations_or_challenges": "SymPy parsing fails on many symbolic formats and set-notation; normalization across alternative equivalent forms is hard; formatting mismatches or omitted delimiter cause false negatives; unit handling requires heuristics (regex) and can be brittle.",
            "uuid": "e6120.2",
            "source_info": {
                "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ChatGPT symbolic equivalence grading",
            "name_full": "Model-based symbolic equivalence grading using GPT-3.5 (student evaluator)",
            "brief_description": "Using GPT-3.5-turbo to compare two symbolic expressions and assign coarse equivalence scores (0, 0.5, 1), as a cheaper way to grade symbolic final answers extracted from stronger LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt GPT-3.5-turbo to compare an extracted student symbolic final answer to a reference expression and output an equivalence score (0, 0.5, 1) indicating non-equivalence, near-equivalence (e.g., up to a constant), or exact match.",
            "evaluation_criteria": "Categorical equivalence judgement (0/0.5/1); used as a proxy to human symbolic equivalence checks.",
            "llm_model_name": "GPT-3.5-turbo-0301 used as the grader; student models evaluated included GPT-4 outputs.",
            "theory_domain": "Symbolic mathematics and symbolic physics answers in ARB.",
            "theory_description": "Evaluates whether an LLM-generated symbolic expression matches the ground truth symbolic expression (equivalence or close equivalence).",
            "evaluation_results": "On a sample, GPT-3.5-turbo achieved zero false positives but a substantial number of false negatives; Table 8 reports accuracy ~0.67 (Physics Symbolic) and ~0.76 (Math Symbolic) with true positives and false negatives indicating under-recognition of equivalence, especially for expressions with &gt;3 variables.",
            "benchmarks_or_datasets": "Applied to ARB symbolic subsets; results reported in Table 8.",
            "comparison_to_human": "GPT-3.5-turbo is conservative (low false positive) but under-credits correct but differently formatted expressions vs. human graders, leading to underestimation of true performance.",
            "limitations_or_challenges": "Fails to recognize equivalence for more complex multi-variable expressions, sensitive to presentation/formatting of student answer, and may underestimate model performance by producing false negatives.",
            "uuid": "e6120.3",
            "source_info": {
                "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Human expert grading",
            "name_full": "Manual human grading of proof-like and complex symbolic solutions",
            "brief_description": "Human experts with mathematical training manually inspect and grade proof-like and many symbolic responses, especially when automatic parsing and model-based rubrics are insufficient.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Expert human graders read solution traces and compare intermediate reasoning to reference solutions, assigning partial credit including half-credit for partially completed subproblems.",
            "evaluation_criteria": "Thorough inspection of intermediate steps, correctness of reasoning and derivations, and whether subproblems are solved; grading may assign 0.5 when at least half of subparts are correct.",
            "llm_model_name": "Applied to outputs from GPT-4 and other evaluated models.",
            "theory_domain": "Proof-like mathematics, advanced symbolic problems in math and physics.",
            "theory_description": "Human grading evaluates the extent to which the LLM-generated solution constitutes a correct scientific proof or valid derivation.",
            "evaluation_results": "Human grading finds very low success rates on proof-like tasks (raw GPT-4 proof-like correctness ~5%; rises to 16% if graders give 0.5 for partially solved multipart problems).",
            "benchmarks_or_datasets": "ARB proof-like subset (Harvard quals, etc.).",
            "comparison_to_human": "Human grading is the reference standard and is used to validate model-based rubric scores; human grades correlate with GPT-4 rubric-scores but remain necessary for many open-response tasks.",
            "limitations_or_challenges": "Manual grading is time-consuming and expensive; attributing error types can be fuzzy; human inter-annotator agreement and consistency require effort.",
            "uuid": "e6120.4",
            "source_info": {
                "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Error taxonomy (extended)",
            "name_full": "Extended error taxonomy for LLM reasoning failures",
            "brief_description": "A categorization of common failure modes of LLM reasoning on math and physics problems: misread/misunderstood problem, wrong approach, logical error / hallucination, and arithmetic/calculation errors (extension of a prior three-way taxonomy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Manual annotation of model chain-of-thought outputs to label error types; non-exclusive categories so a single solution can be labeled with multiple error types.",
            "evaluation_criteria": "Proportions of solutions exhibiting each error class per problem type, reported as percentages (see Table 3).",
            "llm_model_name": "GPT-4 (analysis of its errors reported), with implications for other models.",
            "theory_domain": "Mathematics (numerical, symbolic, proof-like) and Physics (numerical, symbolic).",
            "theory_description": "Labels used to analyze why LLM-generated scientific solutions (theories / derivations) are incorrect, distinguishing whether failure stems from misunderstanding, incorrect method, logical fallacy/hallucination, or calculation mistakes.",
            "evaluation_results": "Table 3 reports detailed percentages for GPT-4: e.g., Math Numerical: Wrong approach 25%, Logical/hallucination 88%, Arithmetic 48%, Correct answer 3%, Correct reasoning 3%; Math Symbolic: Misread 16%, Wrong approach 50%, Logical/hallucination 29%, Arithmetic 4%, Correct answer 16%, Correct reasoning 16%; Physics Numerical: Wrong approach 80%, Logical 53%, Arithmetic 6%, Correct answer 6% etc.",
            "benchmarks_or_datasets": "Derived from a subsample (20–40 problems per large subset) of ARB math and physics problems.",
            "comparison_to_human": "Error annotations performed by human graders using guidelines; these labels are used to interpret model failures and to guide rubric construction and evaluation.",
            "limitations_or_challenges": "Error attribution is inherently fuzzy and non-exclusive; some mistakes (logical vs hallucination) are hard to distinguish; subsampling limits precise prevalence estimates.",
            "uuid": "e6120.5",
            "source_info": {
                "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Formatting & extraction protocol",
            "name_full": "Delimited-answer extraction and formatting checks",
            "brief_description": "A practical evaluation protocol requiring models to print the final answer after an explicit delimiter 'ANSWER:' to enable automatic extraction and parsing of final responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt engineering rule: instruct models to append final answer after 'ANSWER:' delimiter; parse text after delimiter and mark as incorrect if missing. Additional parsing steps include regex unit stripping for numerical answers before SymPy parsing.",
            "evaluation_criteria": "Presence of delimiter and parsability of extracted answer; absence of delimiter or unparseable answer triggers incorrect marking.",
            "llm_model_name": "Applied to GPT-4, gpt-3.5-turbo, text-davinci-003, and Claude in experiments.",
            "theory_domain": "All ARB answer types (multiple-choice, numerical, symbolic).",
            "theory_description": "A pragmatic convention to standardize final-answer extraction for automated evaluation of LLM-generated solutions to scientific problems.",
            "evaluation_results": "With appropriate prompting, all models output a parsable answer &gt;97% of the time for multiple-choice; gpt-3.5-turbo failed to output a parsable answer ~25% of the time on Law multiple-choice subset, while other models failed &lt;5% and GPT-4 was parsed &gt;99% of the time.",
            "benchmarks_or_datasets": "ARB (used in all automated scoring pipelines).",
            "comparison_to_human": "This protocol simplifies automated grading but can mischaracterize answers if models do not follow the format; human evaluation used to validate parsing-based scoring.",
            "limitations_or_challenges": "Rigid format requirement can penalize correct answers presented in different formats; some models may refuse to conform or produce multiple candidate answers, causing parsing failures.",
            "uuid": "e6120.6",
            "source_info": {
                "paper_title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        },
        {
            "paper_title": "GPTScore: Evaluate as you desire",
            "rating": 1
        }
    ],
    "cost": 0.01575875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ARB: Advanced Reasoning Benchmark for Large Language Models</h1>
<p>Tomohiro Sawada ${ }^{1,2, \text {, }}$, Daniel Paleka ${ }^{1,3}$, Alexander Havrilla ${ }^{1,2}$, Pranav Tadepalli ${ }^{1,2}$, Paula Vidas ${ }^{1,}$<br>Alexander Kranias ${ }^{1,2}$, John J. Nay ${ }^{4,5}$, Kshitij Gupta ${ }^{1,6}$, Aran Komatsuzaki ${ }^{1,2, \ddagger}$<br>${ }^{1}$ DuckAI ${ }^{2}$ Georgia Tech ${ }^{3}$ ETH Zürich ${ }^{4}$ Nomos AI<br>${ }^{5}$ Stanford University Center for Legal Informatics ${ }^{6}$ MILA</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below $50 \%$ on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.</p>
<h2>1 Introduction</h2>
<p>In recent years, models such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2022], and Chinchilla [Hoffmann et al., 2022] have shown increasing performance across a wide variety of natural language tasks ranging from translation to reasoning [Bubeck et al., 2023, Laskar et al., 2023]. This rapid progress has been closely tracked and assessed by evaluating LLMs on benchmarks, which test model capabilities on a set of standardized problems. The GLUE benchmark [Wang et al., 2019b] for language understanding was first released in April 2018; but models such as BERT [Devlin et al., 2019] and GPT-2 [Radford et al., 2019] in the following year were already powerful enough to necessitate the "SuperGLUE" benchmark [Wang et al., 2019a]. Since then, the race between language models and benchmarks has increasingly favored the former.
Scaling up, model sizes and datasets alike, has led to rapid improvements on various natural language tasks on benchmarks like BIG-bench [Srivastava et al., 2022] and HELM [Liang et al., 2022]. Neural scaling laws [Kaplan et al., 2020, Caballero et al., 2023, Alabdulmohsin et al., 2022] have been used to predict the behavior of large scale models on various metrics. Nevertheless, LLM performance often increases unpredictably [Wei et al., 2022a], especially on tasks that require reasoning abilities. Predictions of performance on ML benchmarks often underestimate the rate of progress [Steinhardt, 2022]. Since progress has been faster than anticipated, new benchmarks need to be more difficult.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Models such as ChatGPT have shown the ability to pass entry-level examinations in fields such as law [Bommarito II and Katz, 2022], medicine [Kung et al., 2023], economics [Caplan, 2023], and mathematics [Shakarian et al., 2023]. Nevertheless, LLM understanding of many fields is reportedly shallow and unreliable [Shapira et al., 2023]. Expert reasoning in domains with specialized knowledge is essential for automated systems to augment skilled professionals [Noy and Zhang, 2023].
In this paper, we introduce a new benchmark dataset, ARB (Advanced Reasoning Benchmark), designed to evaluate expert reasoning abilities in mathematics, physics, chemistry, biology, and law. To make the benchmark more challenging than previous benchmarks, we extract graduate-level tasks from resources intended for domain professionals. The performance of current models such as GPT-4 on the quantitative parts of ARB is very low using standard prompting methods.
Our dataset offers improvements over existing benchmarks:</p>
<ul>
<li>Hundreds of problems requiring expert reasoning in quantitative subjects, where LLMs are known to underperform;</li>
<li>A large percentage of the problems are short-answer and open response questions, in contrast to the multiple-choice questions that dominated earlier benchmarks.</li>
</ul>
<p>In addition, we propose an automated rubric-based method allowing self-evaluation of intermediate reasoning steps. While not currently a substitute for human evaluation, rubrics generated by GPT-4 have good coverage, and self-evaluation scores track human grading surprisingly well.
We provide the instructions to access the dataset in the supplementary material.</p>
<h1>2 Related Work</h1>
<p>Improving the reasoning capabilities of LLMs has been a subject of recent interest, with a particular focus on advanced prompting techniques [Wei et al., 2022b, Kojima et al., 2023, Wang et al., 2023, Yao et al., 2023, Nye et al., 2021]. Such techniques have seen increasingly successful applications in solving reasoning problems involving commonsense reasoning and mathematics, by promoting active reasoning processes within the LLMs before yielding final answers.
Model architectures such as Minerva [Lewkowycz et al., 2022] have exemplified the enhancement of reasoning capabilities through fine-tuning on extensive datasets covering math and reasoning tasks. This has yielded improved performance across several benchmarks, including MATH [Hendrycks et al., 2021], GSM8K [Cobbe et al., 2021], and MMLU [Hendrycks et al., 2020]. Concurrently, other lines of research [Li et al., 2023, Lightman et al., 2023, Cobbe et al., 2021] have investigated the application of verification techniques to augment and enhance LLM performance.
Most of the aforementioned work has typically evaluated techniques against math benchmarks (e.g., GSM8K [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], SVAMP [Patel et al., 2021], ASDiv [Miao et al., 2020], AQuA [Ling et al., 2017], MAWPS [Koncel-Kedziorski et al., 2016], MultiArith [Roy and Roth, 2016]) and commonsense reasoning tasks (e.g., CSQA [Talmor et al., 2018], StrategyQA [Geva et al., 2021], HotpotQA [Yang et al., 2018]). Recently, several new benchmarks have been introduced for reasoning and planning tasks, such as the GPT-Planning Benchmark [Valmeekam et al., 2023], ALERT Reasoning Benchmark [Yu et al., 2022], JEEBench [Arora et al., 2023]), and [Gendron et al., 2023]. Additionally, comprehensive evaluation suites like the Chain-of-Thought Hub [Fu et al., 2023] have been proposed.
Despite their utility, existing benchmarks are limited in difficulty, represent a restricted range of reasoning challenges, and do not necessarily mirror real-world tasks demanding complex reasoning. Moreover, recent advancements such as Minerva [Lewkowycz et al., 2022] have revealed that these benchmarks may not offer sufficient challenge.
The rapid progress in LLM capabilities has led many to explore using LLMs in the LLM evaluation pipeline. Apart from using LLMs to generate evaluation tasks [Zhang et al., 2022, Perez et al., 2022], LLMs have increasingly been used as a proxy for human evaluation [Chiang and Lee, 2023, Liu et al., 2023, Fu et al., 2023, Kocmi and Federmann, 2023]. Useful LLM-based evaluation for alignment has been done using rubrics [Bai et al., 2022]. We explore the efficacy of rubrics for evaluation when applied to highly complex math and physics problems.</p>
<h1>3 Benchmark</h1>
<p>The key considerations when building a machine learning benchmark are:</p>
<ul>
<li>Difficulty. Most tasks have to be out of reach of current models; a benchmark where many models score over $95 \%$ is not useful for tracking differential AI development.</li>
<li>Usefulness. The tested skills should correlate with generally useful human skills.</li>
<li>Ease of evaluation. It should be straightforward for the model creators to compare the performances of different models. The scores should be interpretable.</li>
<li>Minimizing data contamination. A consistent issue with popular benchmarks is that the recent LLMs contain some tasks in their training data [OpenAI, 2023]. This leads to overestimation of true model capabilities.</li>
<li>Connection to general capabilities. If a model is trained on data similar to the benchmark, it is possible it achieves high performance without generalization or "intelligence", failing to solve novel tasks of similar difficulty [Chollet, 2019]. Conversely, problems should not be pathological or overly adversarial, to avoid the dangers of underclaiming [Bowman, 2021].</li>
</ul>
<h3>3.1 Formatting</h3>
<p>The benchmark consists of three types of questions: multiple choice, short answer, and open response, in descending order of proportion in the dataset.</p>
<ul>
<li>Multiple choice questions consist of a question and four to five possible answers, and the correct answer is the one that best answers the question. They were sourced from standardized tests, such as the MCAT and bar exam prep, and make up a large proportion of the dataset due to their ease of grading.</li>
<li>Short answer questions, on the other hand, ask for final answers in the format of a short phrase or mathematical expression. They were sourced from problem books such as Souza and Silva [2008], Gelca and Andreescu [2017], and physics book series Lim and Qiang [2001], Lim [2007], Lim [1998], Lim et al. [2019], and Lim [1996]. We generally avoided algebraic expressions, because of technical difficulties in the grading process.
A given algebraic expression may have several equivalent forms (e.g. nontrivial functional relations for the functions appearing in the final answer), and a grading scheme which accounts for all possible variations across our entire dataset is not feasible. Moreover, physics problems often require answers introducing new notation that is not explicitly mentioned in the problem statement.</li>
<li>Open response questions are more challenging: they consist of a question and a blank space for the answer. They were sourced from problem books and exams, such as the Harvard PhD comprehensive exams in mathematics [Harvard University, 2021]. Such tasks require manual grading. These questions are aspirational in nature, as current systems (e.g. ChatGPT) cannot produce satisfactory responses, even for the "elementary" problems.</li>
</ul>
<h3>3.2 Mathematics</h3>
<p>This part of the dataset is the most diverse. It includes contest mathematics problems as well as "university mathematics" (i.e. mathematics traditionally taught in universities at the undergraduate and beginning graduate level). The contest problems are sourced from Gelca and Andreescu [2017] and Brayman and Kukush [2018], and the university mathematics problems are sourced from Souza and Silva [2008] and Harvard University [2021]. The dataset does not include high school contest problems because those are already present in other well-known benchmarks [Hendrycks et al., 2021]. The Putnam and Brayman books both contain official solutions, which we also include in the dataset. This can be useful for fully automating the grading process, which we leave to future work.
For university mathematics, we pick Souza and Silva [2008] for its large selection of "standard" undergraduate mathematics problems, as well as many problems suitable for the short answer portions. We also select Harvard University [2021] because it covers topics that other collections of exams rarely not cover, such as representation theory of finite groups and algebraic topology.</p>
<p>Table 1: Types of problems in the benchmark by subject area.</p>
<table>
<thead>
<tr>
<th>Subject</th>
<th>Answer Type</th>
<th>Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mathematics</td>
<td>Numerical</td>
<td>52</td>
</tr>
<tr>
<td></td>
<td>Symbolic</td>
<td>34</td>
</tr>
<tr>
<td></td>
<td>Proof-like</td>
<td>19</td>
</tr>
<tr>
<td>Physics</td>
<td>Numerical</td>
<td>80</td>
</tr>
<tr>
<td></td>
<td>Numerical (w/ image)</td>
<td>18</td>
</tr>
<tr>
<td></td>
<td>Symbolic</td>
<td>18</td>
</tr>
<tr>
<td></td>
<td>Symbolic (w/ image)</td>
<td>13</td>
</tr>
<tr>
<td>Law</td>
<td>Multiple Choice</td>
<td>627</td>
</tr>
<tr>
<td>MCAT (Reading)</td>
<td>Multiple Choice</td>
<td>165</td>
</tr>
<tr>
<td>MCAT (Science)</td>
<td>Multiple Choice</td>
<td>144</td>
</tr>
<tr>
<td></td>
<td>Multiple Choice (w/ image)</td>
<td>37</td>
</tr>
</tbody>
</table>
<h1>3.3 Physics</h1>
<p>The physics problems are structured similarly as the math problems. The main difference is that some physics problems contain figures, and there are more problems with numerical answers. The problems were sourced from the Major American Universities PhD Qualifying Questions and Solutions series [Zhongguo-Kexue-Jishu-Daxue, 1990].</p>
<h3>3.4 MCAT</h3>
<p>The MCAT test contains multiple choice problems testing biology, psychology, chemistry, physics, and reading comprehension. The MCAT problems are sampled from the third edition of McGrawHill Education 3 MCAT Practice Tests [Campbell et al., 2017] and cover both science and reading questions. This book was chosen as very few of these problems appear in standard web-searchable sources, limiting contamination. As in the previous categories, we pick problems which are selfcontained. Because some MCAT science questions are accompanied by images, we accompany such questions with corresponding image files.</p>
<h3>3.5 Law</h3>
<p>Applying law involves the application logical reasoning, in addition to grasping legal knowledge. This makes assessments of legal skills an especially attractive type of language model benchmark, where we are attempting to assess the reasoning and intelligence of these models. Furthermore, if the models better understand law, they can be more reliable and ultimately more useful in real-world applications, potentially even increasing the efficiency and transparency of governments more broadly.
Most lawyers in the U.S. go to law school, graduate, then study for the Bar Examination, and then must pass the bar before going on to practice law professionally. To evaluate legal understanding of the models, we use an older Bar Examination practice set that, to the best of our knowledge, is not available online in a way that could have led to its inclusion in training data for the language models that we are assessing. The practice bar exam we administer to the various language models covers most major areas of law and therefore it tests legal reasoning and broad U.S. legal knowledge.</p>
<h2>4 Evaluation</h2>
<p>We evaluate current LLMs on all text-only problems in our dataset. Other LLM benchmark papers do not evaluate on multimodal tasks due to the lack of good multimodal models; we follow suit. Given public communications about GPT-4 [OpenAI, 2023] and Gemini [Ghahramani, 2023], it is likely the physics and MCAT image problems will be useful for testing multimodal LLMs soon.</p>
<p>Models We evaluate ChatGPT (gpt3.5-turbo-0301), GPT 3.5 (text-davinci-003), GPT-4 with 8 k context length (gpt-4-0314), and Claude (claude-v1.3-100k). We evaluate all question types using task-specific instructions and chain of thought. In chat models, we put the instructions as the system prompt; otherwise we put them at the beginning of the prompt.</p>
<p>In all problem types, in order to extract the model's final answer, we instruct the model to write its final answer at the end of the response after the delimiter ANSWER: . We then parse the model generated final answer as the remaining text after the delimiter. The response is marked as incorrect if the delimiter is not found. Due to the differences in evaluation for multiple choice versus open-ended responses, we adopt multiple evaluation procedures.</p>
<p>Multiple choice To evaluate multiple choice questions, we can simply compare the extracted final answer to the ground truth. A response is considered correct if the extracted choice matches the ground truth choice. With appropriate prompting, all models output a parsable answer $&gt;97 \%$ of the time. We conduct a separate manual evaluation on a sampled subset of the questions to check that our parsing procedure is not mischaracterizing the true performance of the model.</p>
<p>Numerical To evaluate problems with a numerical final answer, we first extract the delimited model answer as above. In the physics problems, many answers are in units; we prompt the model with information about the unit, and instruct it to fully simplify its answer and omit any units. However, sometimes the model forgets to do either or both, and so we apply a series of regexes to remove units. We then attempt to parse the result into a mathematical expression using Python's SymPy library [Meurer et al., 2017]. If this parsing fails, the answer is marked as incorrect. Once parsed, we score a the model answer as correct if $\frac{\text { 'model_answer-ground_truth' }}{\text { ground_truth }}&lt;0.01$.</p>
<p>Symbolic Problems with symbolic answers are less structured and harder to parse. To do so, we again leverage SymPy, first normalizing expressions to contain a default set of variable names and then checking for equivalence up to a permutation of the variables. However this approach is error-prone and only works for the subset of symbolic responses in a function form. More advanced responses, such as those containing set notation, require human evaluation.</p>
<p>Proof-like Natural language proofs cannot be evaluated automatically; the authors with training in mathematics grade the proofs. Further manual human evaluation requires a thorough inspection of the intermediate reasoning steps. This makes evaluation expensive in practice.</p>
<p>Model-based evaluation To address the difficulties in developing automated metrics for evaluating more advanced problems, we experiment with two model based approaches. First, we prompt ChatGPT to grade the equivalence of two symbolic expressions with score options 0 when the totally incorrect, 0.5 when the symbolic expressions are nearly the same e.g. equivalent up to a constant, and 1 when they are an exact match. Our prompting strategy can be found in the supplementary material.</p>
<p>More generally, we evaluate the capabilities of GPT-4 to grade intermediate reasoning chains via a rubric-based evaluation approach. For symbolic and proof-like problems, we few-shot prompt GPT-4 to create a 10-point rubric. This is done by handwriting a small set of initial rubrics for proof-like problems and prompting the model with these examples and the ground truth reference solution. The model assigns point values to intermediate steps using the reference solution as a guide. This process is illustrated in the supplementary material.</p>
<p>With model generated rubrics in hand, we then evaluate each question against its rubric. This is done by again prompting GPT-4 to go step by step through the model answer and assign partial credit based on the rubric. This provides a denser automatic evaluation metric on increasingly unstructured answers. As a nice side benefit, it makes human evaluation of complex symbolic questions much easier, significantly reducing the amount of time required per question.</p>
<h1>4.1 Results</h1>
<p>We now evaluate gpt-4, gpt-3.5-turbo, text-davinci-003, and claude-v1.3 on ARB. The results for the mechanically scored subjects are in Figure 1.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Accuracy of models over automatically scored components of the ARB benchmark. Numerical questions are evaluated with a relative error threshold of $10^{-2}$.</p>
<p>We see models generally do quite well on the multiple choice Law and MCAT subsets, but struggle significantly on questions with numerical final answers. GPT-4 is the only model capable of reliably simplifying complex expressions, but even GPT-4 struggles to reliably perform arithmetic and symbolic manipulations over long contexts.</p>
<p>On the multiple-choice questions, the only model that cannot reliably follow the answer formatting instructions is gpt-3.5-turbo. This happens for a variety of reasons, including the model refusing to answer or to commit to a single answer choice. On the Law benchmark, gpt-3.5-turbo does not output a parsable answer around 25% of the time. The other models exhibit this failure in less than 5% of multiple-choice questions, with GPT-4 being correctly parsed over 99% of the time.</p>
<p>We see a similarly low performance profile across models on symbolic problems, reported in Table 2.</p>
<p>Table 2: Manually parsed scores for symbolic answer questions.</p>
<table>
<thead>
<tr>
<th></th>
<th>Math Symbolic</th>
<th>Physics Symbolic</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-4-0314</td>
<td>18%</td>
<td>28%</td>
</tr>
<tr>
<td>gpt-3.5-turbo-0301</td>
<td>12%</td>
<td>6%</td>
</tr>
<tr>
<td>text-davinci-003</td>
<td>3%</td>
<td>6%</td>
</tr>
<tr>
<td>claude-v1.3-100k</td>
<td>3%</td>
<td>11%</td>
</tr>
</tbody>
</table>
<h3>4.2 What Kind of Errors Do LLMs Make?</h3>
<p>The GPT-4 evaluation paper [Bubeck et al., 2023] classified errors GPT-4 makes in single-pass evaluation on GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] into three types: <em>arithmetic mistakes</em>, <em>misunderstood statement</em>, and <em>wrong approach</em>. We make a more fine-grained analysis and extend it to math and physics problems in our dataset. The results are in Table 3.</p>
<p>The errors current LLMs make on the Mathematics part of ARB fall into five general types:</p>
<ul>
<li>Misunderstanding / answering only a part of the question / misread problem;</li>
<li>Wrong approach: the model's early chain of thought does not guess the right approach;</li>
</ul>
<p>Table 3: Mistakes on mathematics and physics problems in ARB, GPT-4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Misread</th>
<th style="text-align: center;">Wrong</th>
<th style="text-align: center;">Logical error</th>
<th style="text-align: center;">Arithmetic</th>
<th style="text-align: center;">Correct</th>
<th style="text-align: center;">Correct</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">problem</td>
<td style="text-align: center;">approach</td>
<td style="text-align: center;">or hallucination</td>
<td style="text-align: center;">mistake</td>
<td style="text-align: center;">answer</td>
<td style="text-align: center;">reasoning</td>
</tr>
<tr>
<td style="text-align: left;">Math Numerical</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$88 \%$</td>
<td style="text-align: center;">$48 \%$</td>
<td style="text-align: center;">$3 \%$</td>
<td style="text-align: center;">$3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Math Symbolic</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$29 \%$</td>
<td style="text-align: center;">$4 \%$</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">$16 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Math Proof-like</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$72 \%$</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Physics Numerical</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$53 \%$</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Physics Symbolic</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$37 \%$</td>
<td style="text-align: center;">$68 \%$</td>
<td style="text-align: center;">$31 \%$</td>
<td style="text-align: center;">$28 \%$</td>
<td style="text-align: center;">$12 \%$</td>
</tr>
</tbody>
</table>
<ul>
<li>Logical errors: the model uses a false implication between two statements;</li>
<li>Hallucinating facts or theorems: the model confabulates a statement that is false in general, or not applicable in context;</li>
<li>Arithmetic/calculation error: the model multiplies incorrectly, omits a term in an expression, gives a wrong numerical value for a fraction, and other similar mistakes.</li>
</ul>
<p>We grade GPT-4 using the above as a guideline. Our grading of the model's CoT answers is not mutually exclusive; if the model both uses an approach that doesn't go anywhere and makes a calculation error in it, we count it towards both categories. Note that the errors might not be independent: arithmetic mistakes could be more or less frequent in wrong approach solutions as opposed to the solutions with correct idea. We notice that the model is likely to make incorrect simplifications to get to some final answer in approaches that cannot work; this is expected, as prompting the model to produce a solution with a final answer often leads it to produce some final answer by any means.
When the model outputs a chain of implications, it is not always clear whether some false statement is due to a logical error, or it is a straight-out confabulation. We merge those two error types in Table 3.
Some problems ask for multiple things to be proven or calculated. Our graders gave the model a score of 0.5 if it correctly derived at least half of the "subproblems" (for example, homology groups of a given manifold). With this more benevolent form of grading, the performance of GPT-4 on the Proof-like problems jumps to $16 \%$. Where applicable, slight discrepancy with automatic evaluation is also possible due to the error tolerance.
We note that many of the problems in Physics Symbolic have correct symbolic answers even when there are flaws in the chain of thought reasoning of GPT-4. This is likely due to some kind of memorization, although not necessarily from the same sources: see Table 12 for an example.
It is possible that our graders underestimate the rate of arithmetic mistakes in some cases, especially when the approach is clearly wrong, or it is not clear whether a given error is due to faulty reasoning or due to a missed term in the calculations.
For the larger subsets (see Table 1), we subsample the problems to between 20 and 40 per subject area; this is enough for a ballpark estimate of the frequency of different errors, and is not worth increasing because attributing error types is inherently fuzzy.</p>
<h1>5 Model-based Rubric Evaluation</h1>
<p>As reasoning tasks increase in complexity, it gets harder to evaluate model performance. Symbolic final answers are in some cases difficult to grade automatically. Further, we are often more interested in the correctness of the reasoning used to produce the final answer; but evaluating intermediate reasoning steps requires expert human supervision. An ideal solution would be to use LLMs as evaluators based on a reference solution; unfortunately, there are major reliability issues.
To improve reliability, we propose generating rubrics as an important component of the evaluation process. The model generates the rubric from the reference solution, then evaluates any solution based on the generated rubric. To aid rubric generation, we give few-shot examples of human-written rubrics to the rubric-generating model run. We study this approach by conducting a human evaluation of GPT-4 generated rubrics and the GPT-4 grading of its own solutions using the generated rubrics.</p>
<p>We rate the quality of GPT-4 generated rubrics by hand in the first two rows of Table 4 Likert scores from 1-5 are assigned to both the coverage of the rubric, i.e. how well it captures key subproblems, and the point breakdown. Rubric quality scores are reported in Table 5 for symbolic and proof-like problems. We find GPT-4 designs rubrics which cover the crucial solution steps well, but struggles to properly allocate points to each step based on relative importance. However, it is much better than GPT-3.5-turbo, which tends to over-allocate points to only one or two solution steps.</p>
<p>Table 4: Evaluations of rubric quality and GPT-4 rubric evaluation failure cases. Rubric coverage and rubric point spread are on a 1-5 Likert scale. Alternative solutions is the percentage of correct solutions found not covered by the rubric. Extra/reduced credit track how often GPT-4 erroneously assigns or deducts points. Hallucinated rubric tracks how often GPT-4 assigns points by referring to a rubric item not actually present in the rubric.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Physics Symbolic</th>
<th style="text-align: center;">Math Symbolic</th>
<th style="text-align: center;">Proof-like</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rubric coverage</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">4.26</td>
<td style="text-align: center;">3.94</td>
</tr>
<tr>
<td style="text-align: left;">Rubric point spread</td>
<td style="text-align: center;">4.16</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">4.06</td>
</tr>
<tr>
<td style="text-align: left;">Alternative solutions</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Extra credit</td>
<td style="text-align: center;">$27 \%$</td>
<td style="text-align: center;">$18 \%$</td>
<td style="text-align: center;">$40 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Reduced credit</td>
<td style="text-align: center;">$11 \%$</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Hallucinated rubric</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>The obvious limitation of rubric scoring is the case of correct solutions not covered by the rubric. We find that on our benchmark, GPT-4 rarely generates a fully or even mostly partially correct solution that does not follow the rubric. Once done rating the model generated rubrics, we then manually grade GPT-4's solutions according to each rubric and compare the result to GPT-4's evaluation. We also annotate, for each problem, both whether GPT-4 assigns credit inappropriately or fails to assign credit when it should.</p>
<p>Table 5: Average scores (out of 10 points) when assigned by human annotators versus GPT-4. Correlation is the Pearson correlation coefficient between the two scores, over all problems.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Physics Symbolic</th>
<th style="text-align: center;">Math Symbolic</th>
<th style="text-align: center;">Proof-like</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human eval score</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">3.13</td>
<td style="text-align: center;">2.65</td>
</tr>
<tr>
<td style="text-align: left;">Model eval score</td>
<td style="text-align: center;">5.05</td>
<td style="text-align: center;">3.37</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: left;">Correlation</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.82</td>
</tr>
</tbody>
</table>
<p>We find a moderately high correlation between GPT-4's evaluation score and the manual score. In some cases, the model, assigns an extra point or two when compared to the annotated rubric score. However, the self-eval score almost never deviates more than two points from the ground truth. The main failure mode we detect is the assignment of partial credit to attempted solutions completely outside the problem rubric, where the human evaluation score is always zero. Taken together, we believe these results suggest that rubric-based evaluation is a promising automated evaluation method.
Having established rubric-based evaluation as a (imperfect) proxy for correctness, we now comment on the GPT-4 performance graded by the rubric. Table 5 shows GPT-4 is best at generating correct intermediate reasoning steps for physics questions. Inspecting the model outputs suggests that GPT-4 is good at recalling relevant and useful concepts in physics for solving the relevant problem; however, it can struggle with the mathematical manipulations required to solve the problem. The model is worse at recognizing the correct concepts and formulating an appropriate plan for the math questions, particularly for proof-like problems.</p>
<h1>6 Limitations and Conclusion</h1>
<p>In this paper, we presented ARB, a novel benchmark for evaluating advanced reasoning capabilities in large language models. Our dataset is composed of various problems from the sciences and law,</p>
<p>sourced from graduate-level exams and professional resources. Despite advancements in current LLMs, their performance remains very low on the quantitative subjects, in ARB's tasks. We also proposed a rubric-based self-evaluation method, enabling LLMs to grade their own reasoning. This method is not yet reliable enough to replace human grading. We hope this method can be extended to more reliable and cheap testing of complex model outputs.
As with all other benchmarks that are not created anew and kept secret, it is possible there is data contamination. For example, the MCAT books are not available for free in most jurisdictions, but it certainly possible that some model creators have trained on it anyway.
Finally, the benchmark does not remotely cover all aspects of human ability; a model solving this benchmark perfectly could still be much worse than most educated people in many aspects. Nevertheless, we hope that increasing the difficulty standards helps the research community ground the performance of increasingly powerful models more accurately.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>We thank Jeffrey Deng for developing and documenting the API, and building the project website. We would also like to thank Raunak Chowdhuri for helpful comments, and Zhangir Azerbayev for useful discussions early on in the project. TS is supported by NSF grant 1745583.</p>
<h2>References</h2>
<p>Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:22300-22312, 2022.</p>
<p>Daman Arora, Himanshu Gaurav Singh, and Mausam. Have LLMs advanced enough? A challenging problem solving benchmark for Large Language Models, 2023.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI, feedback, 2022.</p>
<p>Barbri. Barbri Practice Questions: Multistate Testing Practice Questions. Thomson/Bar/Bri, 2007. ISBN 9780314174017 .</p>
<p>Michael Bommarito II and Daniel Martin Katz. GPT takes the bar exam. arXiv preprint arXiv:2212.14402, 2022.</p>
<p>Samuel R. Bowman. The dangers of underclaiming: Reasons for caution when reporting how NLP systems fail, 2021.</p>
<p>Volodymyr Brayman and A. G. Kukush. Undergraduate Mathematics Competitions (1995-2016): Taras Shevchenko National University of Kyiv. Springer, 2018.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws, 2023.</p>
<p>Candice McCloskey Campbell, Shaun Murphree, Jennifer M. Warner, Amy B. Wachholz, Kathy A. Zahler, and George J. Hademenos. McGraw-Hill Education 3 MCAT Practice Tests, Third Edition. McGraw-Hill Education, Jan 2017. ISBN 1259859622.</p>
<p>Bryan Caplan. GPT retakes my midterm and gets an A, 2023. URL https://betonit.substack.com/ p/gpt-retakes-my-midterm-and-gets-an.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. Can Large Language Models be an alternative to human evaluations? arXiv e-prints, art. arXiv:2305.01937, may 2023. doi: 10.48550/arXiv.2305.01937.</p>
<p>François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with Pathways, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, jun 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire. arXiv e-prints, art. arXiv:2302.04166, feb 2023. doi: 10.48550/arXiv.2302.04166.</p>
<p>Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance, 2023.</p>
<p>Răzvan Gelca and Titu Andreescu. Putnam and beyond. Springer, 2017.
Gaël Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not abstract reasoners, 2023.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies, 2021.</p>
<p>Zoubin Ghahramani. Introducing PaLM 2, 2023. URL https://blog.google/technology/ai/ google-palm-2-ai-large-language-model.</p>
<p>Department of Mathematics Harvard University. Qualifying examination for fall 2021, Aug 2021. URL https://www.math.harvard.edu/media/quals-F21_with_solutions.pdf.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2020. URL https://arxiv.org/abs/2009. 03300 .</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874, 2021. URL https://arxiv.org/abs/2103.03874.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.</p>
<p>Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. GPT-4 passes the bar exam. Available at SSRN 4389233, 2023.</p>
<p>Tom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. arXiv e-prints, art. arXiv:2302.14520, feb 2023. doi: 10.48550/arXiv.2302.14520.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California, jun 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.</p>
<p>Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLOS Digital Health, 2 (2):e0000198, 2023.</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets, 2023.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making large language models better reasoners with step-aware verifier, 2023.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step, 2023.</p>
<p>Swee Cheng Lim, Choy Heng Lai, and Leong Chuan Kwek. Problems and solutions on optics. World Scientific, 2019 .</p>
<p>Yung-kuo Lim. Problems and solutions on thermodynamics and Statistical Mechanics. World Scientific, 1996.
Yung-kuo Lim. Problems and solutions in quantum mechanics: Major, American universities ph. D. qualifying questions and, solutions. World Scientific, 1998.</p>
<p>Yung-kuo Lim. Problems and solutions on electromagnetism. World Scientific Pub. Co, 2007.
Yung-kuo Lim and Yuan-qi Qiang. Problems and solutions on Mechanics. World Scientif., 2001.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning, to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada, jul 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https : //aclanthology.org/P17-1015.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using GPT-4 with better human alignment. arXiv e-prints, art. arXiv:2303.16634, mar 2023. doi: 10.48550/ arXiv. 2303.16634 .</p>
<p>Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Štěpán Roučka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Scopatz. Sympy: Symbolic computing in python. PeerJ Computer Science, 3: e103, jan 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs.103. URL https://doi.org/10.7717/ peerj-cs. 103 .</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online, jul 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.92. URL https://aclanthology.org/2020.acl-main. 92.</p>
<p>Shakked Noy and Whitney Zhang. Experimental evidence on the productivity effects of generative artificial intelligence. Available at SSRN 4375283, 2023.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021.</p>
<p>OpenAI. GPT-4 technical report, 2023.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems?, 2021.</p>
<p>Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022.</p>
<p>Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI. arXiv e-prints, art. arXiv:2204.01075, April 2022. doi: 10.48550/arXiv. 2204.01075.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems, 2016.
Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. An independent evaluation of ChatGPT on mathematical word problems (MWP). arXiv preprint arXiv:2302.13814, 2023.</p>
<p>Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever Hans or Neural Theory of Mind? Stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763, 2023.</p>
<p>Paulo N de Souza and Jorge N. Silva. Berkeley problems in Mathematics. Springer New York, 2008.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. doi: 10.48550/ARXIV.2206.04615. URL https://arxiv.org/abs/2206.04615.</p>
<p>Jacob Steinhardt. AI forecasting: One year in, 2022. URL https://bounded-regret.ghost.io/ ai-forecasting-one-year-in/.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. CoRR, abs/1811.00937, 2018. URL http://arxiv.org/ abs/1811.00937.</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change), 2023.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems, 2019a.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019b. In the Proceedings of ICLR.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models, 2022b. URL https : //arxiv.org/abs/2201.11903.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering, 2018.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.</p>
<p>Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, and Asli Celikyilmaz. ALERT: Adapting language models to reasoning tasks, 2022.</p>
<p>Sarah Zhang, Reece Shuttleworth, Derek Austin, Yann Hicke, Leonard Tang, Sathwik Karnik, Darnell Granberry, and Iddo Drori. A dataset and benchmark for automatically answering and generating machine learning final exams. arXiv preprint arXiv:2206.05442, 2022.</p>
<p>Hefei Zhongguo-Kexue-Jishu-Daxue. Major American universities Ph. D. qualifying questions and solutions. 5. Problems and solutions on thermodynamics and statistical mechanics. World Scientific, 1990.</p>
<h1>A Datasheet</h1>
<p>We present the data card, following the format proposed by Pushkarna et al. [2022].
Dataset Owners. contact@duckai.org.</p>
<p>Table 6: Data overview.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: left;">Task Type</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mathematics</td>
<td style="text-align: left;">Contest problems <br> University math proof</td>
<td style="text-align: center;">Gelca and Andreescu [2017], Brayman and Kukush [2018] <br> Souza and Silva [2008], Harvard University [2021]</td>
</tr>
<tr>
<td style="text-align: left;">Physics</td>
<td style="text-align: left;">PhD qualifying exam</td>
<td style="text-align: center;">Zhongguo-Kexue-Jishu-Daxue [1990]</td>
</tr>
<tr>
<td style="text-align: left;">Law</td>
<td style="text-align: left;">US Law Standardized Exam</td>
<td style="text-align: center;">Barbri [2007]</td>
</tr>
<tr>
<td style="text-align: left;">MCAT</td>
<td style="text-align: left;">Reading comprehension</td>
<td style="text-align: center;">[Campbell et al., 2017]</td>
</tr>
<tr>
<td style="text-align: left;">MCAT</td>
<td style="text-align: left;">College science</td>
<td style="text-align: center;">[Campbell et al., 2017]</td>
</tr>
</tbody>
</table>
<p>Dataset Overview. See Table 6.
Risk and Mitigation. There is little risk associated with this dataset, as it is intended for benchmarking reasoning capabilities of models, and it is too small to be used for advancing capabilities.
Maintenance Status. Limited Maintenance. The data will not get major updates, but any technical issues will be addressed.</p>
<p>Maintenance Plan. Any technical issues will be addressed.</p>
<ul>
<li>Versioning. No new versions are planned.</li>
<li>Updates. Updates are limited to bug and error fixes.</li>
<li>Errors. Error handling will be considered case by case.</li>
<li>Feedback. contact@duckai.org.</li>
</ul>
<p>Example: Typical Data Point. Each data point of the dataset consist of a pair of problem statement and ground truth solution. Table 7, Table 9 and Table 10 include problem statement and ground truth solution of typical data points.</p>
<p>Sensitive Human Attributes. We have not found any sensitive human attributes in our dataset.
Data Distributions. Table Section 3.1 shows the number of problems for each subject area and answer type. Text entries (problem statement, ground truth solution, ground truth answer) for all categories are in LaTeX (although obviously, the non-quantitative subjects have very few mathematical expressions).</p>
<h1>B Dataset format</h1>
<p>The benchmark dataset is available in .jsonl format, containing problem statements, ground truth solutions, and final ground truth answers for each entry. We additionally include metadata such as subject names and problem topics, where available.</p>
<p>We chose the four subject areas discussed earlier for several reasons. Primarily, the dataset focuses on math and physics, as these subjects present more challenging problems than existing benchmarks. However, to ensure a comprehensive evaluation of models, we also included subjects like Law and MCAT. This inclusion allows for assessing model performance across a wider range of technical domains, beyond the quantitative sciences.</p>
<p>Although previous works have evaluated recent models on law [Katz et al., 2023], we draw upon the established importance of broad benchmarks like SuperGLUE [Wang et al., 2019a]. Making a benchmark more comprehensive expands the evaluation scope while enhancing the dataset's significance in the wider AI research context.</p>
<h2>C Accessing the dataset</h2>
<p>We facilitate access to the dataset through API calls*. The problems can be accessed by the different splits and subject areas.</p>
<p>We use an API to host the data (rather than making it publicly available on popular platforms such as HuggingFace or Github) out of concern for data contamination. Many models released in the last few years have been trained on benchmark datasets OpenAI [2023], an approach that could inadvertently inflate the model's performance on the associated benchmark. There are several proposed strategies to mitigate this problem, including dataset poisoning and canary text methods.However, restricting dataset accessibility to web crawlers seemed to be the only way to ensure integrity of our dataset for future uses.</p>
<h2>D Model-based Rubric Eval</h2>
<p>Table 7 presents a GPT-4 generated rubric and self-evaluation for symbolic math questions. GPT-4 does a good job breaking the problem into important sub-parts of the reference solution. Further, it ensures a well-balanced point distribution among sub-problems, assigning similar point values to each sub-part. In contrast, less powerful models like GPT-3.5-turbo struggle to generate good rubrics, as they tend to under-specify sub-problems and allocate too many points to irrelevant steps.</p>
<p>The main downside on evaluating solutions only on the final answer is that the metric is not smooth; a small mistake in the middle of the solution always leads to an incorrect final answer. Grading using a rubric shows that GPT-4 generates some useful ideas, for example the correct formula for parameterizing an ellipse. The model also correctly identifies that the question's area is optimized by an isosceles triangle. Despite this, it is unable to correctly compute the final answer due to an earlier mistake in the response. This indicates that GPT-4 has some problem-solving abilities, but struggles to detect or recover from earlier errors in generation.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>D. 1 Using ChatGPT for Symbolic Evaluation</h1>
<p>Unlike GPT-4, GPT-3.5-turbo is not to write rubrics with good coverage of the reference solution and a fair point breakdown. Often the model will over-simplify the rubric and allocate far too many points to non-essential parts of the problem. However, GPT-3.5-turbo does possess some ability to reason about complex symbolic expressions. Motivated by this, we asked the model to grade the final answers to symbolic math and physics problems. While much easier to grade than intermediate reasoning steps, more involved symbolic expressions still require human evaluation to compare accurately. Using cheap models like GPT-3.5-turbo to automate this symbolic equivalence boosts our abilities to evaluate models on this more complicated class of reasoning problems. We prompt GPT-3.5-turbo to compare extracted model generated symbolic final answers from GPT-4 to the reference answer and record results in Table 8.</p>
<p>GPT-3.5-turbo is surprisingly good at not equating non-equivalent symbolic statements, achieving a false positive rate of 0 . However, this comes at of the model often not assigning credit when it should, leading to an underestimate of the true performance of the graded answers on symbolic tasks. In particular, GPT-3.5-turbo often fails when comparing expressions with more than 3 variables. Furthermore, we observe in several cases the student model GPT-4 is able to correctly generate the final answer, but does not present it in the expected format, making it impossible for GPT-3.5-turbo to correctly compare against the ground truth reference. These errors could potentially be avoided by giving the model access to the entire student generated reasoning trace, but we expect this to be a minimal help and perhaps even harm performance.</p>
<h2>E Interesting Examples</h2>
<p>Math Example. One common way GPT-4 gets numerical questions wrong is when it do some formal manipulations follows by a blackbox numerical calculation after which the model spits out an (incorrect) answer. (Table 10) The formal manipulation does not simplify the problem, and so all the work of solving the problem is done in the final step of the calculation.</p>
<p>MCAT Example. GPT-4 get's confused when meanings of words are implicit in prerequisite knowledge or contexts. In one example about DNA replication (Table 9), the model correctly identifies that the radioactive thymine is present in the two strands of nucleotides from the original DNA, it fails to deduce that both of the resulting double helices are radioactive. This seems to be because the model confuses the word "DNA" with "strands" of the DNA. When looking at choice C, the model (incorrectly) assumes that each of the four strands in the new double helices are radioactive, when it is clear from context that the choice is referring to the radioactive molecule being present somewhere in each double helix (not necessarily in each strand). Because of this misconception, the model chooses D.</p>
<p>Law Example. An unexpected mistake from GPT-4 in answering law questions is where the model reads too much into an answer choice. For example, GPT-4 incorrectly produced this answer and reasoning:
"B: This answer choice suggests that when two crossing offers are identical, one will be treated as an offer and the other as an acceptance. This accurately reflects the situation between Mom and Huck, as they both agreed on the same price."</p>
<p>And made this final answer choice:
"Yes, because when two crossing offers are identical in import, one will be treated as an offer and the other as an acceptance."</p>
<p>The error GPT-4 made is treating the statement in the answer choice ("when two crossing offers are identical, one will be treated as an offer and the other as an acceptance") as a fact, and then making a choice based on that fact being true. Better prompting could likely precent errors of this type.</p>
<h1>F Memorization</h1>
<p>While it is difficult to quantify potential problem leakage and memorization of similar problems, some outputs suggest this might be a relevant factor. In Table 12, GPT-3.5-turbo appears to be memorizing the answer to this symbolic physics problem, given that it is getting the formula for capacitance $C$ correct despite having the wrong reasoning. In contrast, GPT-4 stays faithful to its reasoning chain, resulting in a slightly inaccurate final answer. Note that the overall performance of all models we test is still somewhat low, hence not majorly affected by memorization of similar problems.</p>
<h2>G Prompts for Sections 4 and 5</h2>
<p>For OpenAI chat models (gpt-4 and gpt-3.5-turbo), following best practices, we prepend the system prompt. We do not use the system prompt for the other models due to there not being a well-established way to do so, and out of concerns of hurting performance on the other models. We note that omitting the system prompt should have little effect on the performance of the other models, as the directions of the system prompt are essentially restated in our user prompts.</p>
<h2>H Checklist Information.</h2>
<p>Dataset Intended Uses. The ARB benchmark dataset, documented within the paper, is primarily intended for research purposes. We do not advocate for this dataset to train models that help students cheat on exams. We hope that the research community will use this benchmark to better assess reasoning capabilities of language models.</p>
<p>Author Statement and License. We bear all responsibility in case of violation of rights. The ARB dataset is licensed under CC BY 4.0, and all helper code we release is under the MIT license. For all problems originating in books listed in Section 3, following [Hendrycks et al., 2021], we abide by Fair Use $\S 107$ : "the fair use of a copyrighted work, including such use by ... scholarship, or research, is not an infringement of copyright", where fair use is determined by "the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes" and "the effect of the use upon the potential market for or value of the copyrighted work".</p>
<p>Table 7: Example model generated rubric and evaluation. Evaluation with a rubric reveals the model begins with the model makes good partial progress but is unable to arrive at the correct final answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem <br> Statement</th>
<th style="text-align: center;">Find the maximum area of all triangles that can be inscribed in an ellipse with semiaxes $a$ and $b$, and describe the triangles that have maximum area.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Using the parameterization Using the parameterization</p>
<p>$$
x=a \cos t, y=b \sin t
$$</p>
<p>a triple of points on the ellipse is given by</p>
<p>$$
\left(a \cos t_{i}, b \sin t_{i}\right), \quad i=1,2,3
$$</p>
<p>So the area of an inscribed triangle is given by</p>
<p>$$
\frac{1}{2}\left|\begin{array}{lll|r}
1 &amp; a \cos t_{1} &amp; b \sin t_{1} \
1 &amp; a \cos t_{2} &amp; b \sin t_{2} \
1 &amp; a \cos t_{3} &amp; b \sin t_{3}
\end{array}\right|=\frac{a b}{2}\left|\begin{array}{lll}
1 &amp; \cos t_{1} &amp; \sin t_{1} \
1 &amp; \cos t_{2} &amp; \sin t_{2} \
1 &amp; \cos t_{3} &amp; \sin t_{3}
\end{array}\right|
$$</p>
<p>which is $a b$ times the area of a triangle inscribed in the unit circle. In the case of the circle, among all inscribed triangles with a given base $2 w(0&lt;w \leqslant 1)$, the one of maximum area is an isosceles triangle whose area equals</p>
<p>$$
g(w)=w\left(1+\sqrt{1-w^{2}}\right)
$$</p>
<p>Using elementary calculus one finds that the maximum of $g$ on the interval $0 \leqslant w \leqslant$ 1 occurs at $w=\sqrt{3} / 2$, corresponding to an equilateral triangle, and equals $3 \sqrt{3} / 4$. Alternatively, fixing one side of the triangle as the basis, we easily see that among all the inscribed triangles the one with the greatest area is isosceles because of the maximum height, showing that the angle at the basis is the same. Fixing another side we see that the triangle is indeed equilateral. Hence, the area is maximal when</p>
<p>$$
t_{2}=t_{1}+\frac{2 \pi}{3} \text { and } t_{3}=t_{2}+\frac{2 \pi}{3}
$$</p>
<p>that is, when the corresponding triangle inscribed in the unit circle is regular.
For the ellipse with semiaxes $a, b$, this corresponds to an inscribed triangle with maximum area equals $3 a b \sqrt{3} / 4$.</p>
<ul>
<li>Parameterize the ellipse using $x=a \cos t, y=b \sin t$ (2 points).</li>
<li>Write the expression for the area of an inscribed triangle using the determinant formula (2 points):</li>
</ul>
<p>$$
\frac{a b}{2}\left|\begin{array}{lll}
1 &amp; \cos t_{1} &amp; \sin t_{1} \
1 &amp; \cos t_{2} &amp; \sin t_{2} \
1 &amp; \cos t_{3} &amp; \sin t_{3}
\end{array}\right|
$$</p>
<p>Model
Generated
Rubric</p>
<ul>
<li>Connect the problem to the maximum area of an inscribed triangle in the unit circle ( 2 points).</li>
<li>Correctly identify that the maximum area occurs when the inscribed triangle is equilateral ( 2 points).</li>
<li>Find the maximum area of an inscribed triangle in the ellipse, and provide the correct answer ( 2 points):</li>
</ul>
<p>$$
\frac{3 a b \sqrt{3}}{4}
$$</p>
<p>To maximize the area of the inscribed triangle, we can use the following approach:</p>
<ol>
<li>Parameterize the ellipse: The equation of an ellipse with semiaxes $a$ and $b$ is given by $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=1$. We can parameterize the ellipse using the parameter $t$ as follows: $x=a \cos (t)$ and $y=b \sin (t)$, where $0 \leq t \leq 2 \pi$.</li>
<li>Determine the vertices of the triangle: Let $A\left(a \cos \left(t_{1}\right), b \sin \left(t_{1}\right)\right)$, $B\left(a \cos \left(t_{2}\right), b \sin \left(t_{2}\right)\right)$, and $C\left(a \cos \left(t_{3}\right), b \sin \left(t_{3}\right)\right)$ be the vertices of the inscribed triangle, where $0 \leq t_{1}, t_{2}, t_{3} \leq 2 \pi$.</li>
<li>Calculate the area of the triangle: Using the Shoelace Theorem, the area of the triangle is given by:</li>
</ol>
<p>$$
\text { Area }=\frac{1}{2}\left|\sum_{i=1}^{3} x_{i} y_{i+1}-\sum_{i=1}^{3} x_{i+1} y_{i}\right|
$$</p>
<p>where $\left(x_{4}, y_{4}\right)=\left(x_{1}, y_{1}\right)$.
Substituting the parameterized coordinates, we get:</p>
<p>$$
\begin{array}{r}
\text { Area }=\frac{1}{2}\left|a^{2} \cos \left(t_{1}\right) \sin \left(t_{2}\right)+a^{2} \cos \left(t_{2}\right) \sin \left(t_{3}\right)\right. \
\left.+a^{2} \cos \left(t_{3}\right) \sin \left(t_{1}\right)-a^{2} \cos \left(t_{2}\right) \sin \left(t_{1}\right)-a^{2} \cos \left(t_{3}\right) \sin \left(t_{2}\right)-a^{2} \cos \left(t_{1}\right) \sin \left(t_{3}\right)\right|
\end{array}
$$</p>
<p>Simplifying the expression, we get:</p>
<p>Model
Response</p>
<p>$$
\text { Area }=\frac{a^{2}}{2}\left|\cos \left(t_{1}\right) \sin \left(t_{2}\right)-\cos \left(t_{2}\right) \sin \left(t_{1}\right)+\right.
$$</p>
<p>Using the sine difference identity, $\sin (A-B)=\sin (A) \cos (B)-\cos (A) \sin (B)$, we can rewrite the area as:</p>
<p>$$
\text { Area }=\frac{a^{2}}{2}\left|\sin \left(t_{1}-t_{2}\right)+\sin \left(t_{2}-t_{3}\right)+\sin \left(t_{3}-t_{1}\right)\right|
$$</p>
<ol>
<li>Maximize the area: Notice that the maximum value of each sine term is 1 , and this occurs when the angles $t_{1}, t_{2}, t_{3}$ are equally spaced around the ellipse, i.e., when the triangle is equilateral. In this case, the area becomes:</li>
</ol>
<p>$$
\text { Area }=\frac{a^{2}}{2}|1+1+1|=\frac{3 a^{2}}{2}
$$</p>
<ol>
<li>Relate the maximum area to the semiaxes: Since the triangle is equilateral, the ratio of the area of the triangle to the area of the ellipse is constant. The area of the ellipse is given by $\pi a b$. Thus, we can write:</li>
</ol>
<p>$$
\frac{\operatorname{Area}_{\max }}{\pi a b}=\frac{\frac{3 a^{2}}{2}}{\pi a b}
$$</p>
<p>Solving for the maximum area, we get:</p>
<p>$$
\text { Area }_{\max }=\frac{3 a^{2}}{2} \cdot \pi a b=\frac{3 \pi a^{3} b}{2}
$$</p>
<p>Thus, the maximum area of an inscribed triangle in an ellipse with semiaxes $a$ and $b$ is $\frac{3 \pi a^{3} b}{2}$, and the triangles with maximum area are equilateral.
ANSWER: $\frac{3 \pi a^{3} b}{2}$</p>
<ol>
<li>The student parameterized the ellipse correctly using $x=a \cos (t)$ and $y=b \sin (t)$. (2 points)</li>
<li>The student used the Shoelace Theorem to find the area of the triangle, but did not use the determinant formula as specified in the rubric. ( 0 points)</li>
<li>The student did not explicitly connect the problem to the maximum area of an inscribed triangle in the unit circle. ( 0 points)</li>
<li>The student correctly identified that the maximum area occurs when the inscribed triangle is equilateral. ( 2 points)</li>
<li>The student found the maximum area of the inscribed triangle in the ellipse, but their answer is incorrect. The correct answer is $\frac{3 a b \sqrt{3}}{4}$, not $\frac{3 \pi a^{3} b}{2}$. ( 0 points)
Rubric Score: 4 points</li>
</ol>
<p>Table 8: Performance of GPT-3.5-turbo on symbolic equivalence versus human ground truth. The model achieves a false positive rate of 0 , but allows many false negatives. This suggests there are many cases in which the model does not recognize equivalent expressions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Physics Symbolic</th>
<th style="text-align: right;">Math Symbolic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">True positives</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">False positives</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">True negatives</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">22</td>
</tr>
<tr>
<td style="text-align: left;">False negatives</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy</td>
<td style="text-align: right;">0.67</td>
<td style="text-align: right;">0.76</td>
</tr>
</tbody>
</table>
<p>Table 9: Problem, solution and GPT-4 response for a question from the MCAT part of the benchmark. The model mistakes two copies of a DNA with the two amino acid strands in a DNA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem <br> Statement</th>
<th style="text-align: center;">If you replaced the thymine in a double helix with radioactive thymine and you allowed DNA replication to occur once, which of the following results would you expect? Choices: A: The DNA would not be radioactive in either of the double helices. B: The mRNA made from the double helices would be radioactive. C: The DNA in each of the double helices would be radioactive. D: The DNA in one of the double helices would be radioactive but not in the other one.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Ground Truth Solution</td>
<td style="text-align: center;">The correct answer is C. During semiconservative replication, the DNA helix unwinds so that both strands of DNA can serve as a template. Both strands are copied, producing two double helices. Each one consists of a template strand and a newly synthesized strand.</td>
</tr>
<tr>
<td style="text-align: center;">Model <br> Response</td>
<td style="text-align: center;">First, let's consider the process of DNA replication. DNA replication is a semiconservative process, which means that each new double helix formed contains one original strand and one newly synthesized strand. In this case, the original strand contains radioactive thymine. <br> Now let's analyze the answer choices: <br> A: The DNA would not be radioactive in either of the double helices. <br> This choice is incorrect because we know that the original strand contains radioactive thymine, and since replication is semi-conservative, each new double helix will have one original strand with radioactive thymine. <br> B: The mRNA made from the double helices would be radioactive. <br> This choice is unrelated to the question since it concerns mRNA instead of the DNA replication process. Therefore, we can rule out this choice. <br> C: The DNA in each of the double helices would be radioactive. <br> This choice suggests that both strands in each new double helix would be radioactive. However, we know that only the original strand contains radioactive thymine, and the newly synthesized strand does not. Thus, this choice is not correct. <br> D: The DNA in one of the double helices would be radioactive but not in the other one. This choice aligns with the concept of semi-conservative replication. Each new double helix contains one original strand (with radioactive thymine) and one newly synthesized strand (without radioactive thymine). <br> Therefore, the correct answer is: <br> ANSWER: D</td>
</tr>
</tbody>
</table>
<p>Table 10: Problem, solution and GPT-4 sesponse for a question from the Math Numerical part of the benchmark. The model often returns a numerical answer after doing some internal black-box numerical calculation, which is rarely correct.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Problem <br> Statement</th>
<th style="text-align: left;">Compute $\int_{0}^{\pi} \frac{x \sin x}{1+\sin ^{2} x} d x$.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>To compute the integral</p>
<p>$$
\int_{0}^{\pi} \frac{x \sin x}{1+\sin ^{2} x} d x
$$</p>
<p>Model Response</p>
<p>We use the example from the introduction for the particular function $f(x)=\frac{x}{1+x^{2}}$ to transform the integral into</p>
<p>$$
\pi \int_{0}^{\frac{\pi}{2}} \frac{\sin x}{1+\sin ^{2} x} d x
$$</p>
<p>This is the same as</p>
<p>$$
\pi \int_{0}^{\frac{\pi}{2}}-\frac{d(\cos x)}{2-\cos ^{2} x}
$$</p>
<p>which with the substitution $t=\cos x$ becomes</p>
<p>$$
\pi \int_{0}^{1} \frac{1}{2-t^{2}} d t=\left.\frac{\pi}{2 \sqrt{2}} \ln \frac{\sqrt{2}+t}{\sqrt{2}-t}\right|_{0} ^{1}=\frac{\pi}{2 \sqrt{2}} \ln \frac{\sqrt{2}+1}{\sqrt{2}-1}
$$</p>
<p>Now, we can substitute back $t=\sin x$, and we get:</p>
<p>$$
v=\frac{1}{2} \ln \left(1+\sin ^{2} x\right)+C
$$</p>
<p>Now, we can apply integration by parts:
$\int_{0}^{\pi} \frac{x \sin x}{1+\sin ^{2} x} d x=u v \int_{0}^{\pi}-\int_{0}^{\pi} v d u=\left[x \cdot \frac{1}{2} \ln \left(1+\sin ^{2} x\right)\right]<em 0="0">{0}^{\pi}-\int</em> x\right) d x$
The first term is:}^{\pi} \frac{1}{2} \ln \left(1+\sin ^{2</p>
<p>$$
\left[x \cdot \frac{1}{2} \ln \left(1+\sin ^{2} x\right)\right]_{0}^{\pi}=\pi \cdot \frac{1}{2} \ln (1)-0 \cdot \frac{1}{2} \ln \left(1+\sin ^{2} 0\right)=0
$$</p>
<p>Now, we need to compute the second term:</p>
<p>$$
-\int_{0}^{\pi} \frac{1}{2} \ln \left(1+\sin ^{2} x\right) d x
$$</p>
<p>Unfortunately, this integral does not have a closed-form solution in terms of elementary functions. However, it is possible to compute its numerical value using numerical integration methods, such as the trapezoidal rule, Simpson's rule, or Gaussian quadrature.
Using numerical integration, we can approximate the value of the integral to be approximately -1.9348 . Therefore, the final answer is: ANSWER: -1.9348 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*The API can be accessed through the following: https://arb.duckai.org/api/lib. The documentation can be found here: https://app.swaggerhub.com/apis-docs/arb-dataset/arb-api/1.0.5&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
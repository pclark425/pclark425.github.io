<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-464 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-464</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-464</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-265502630</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.07078v1.pdf" target="_blank">Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning</a></p>
                <p><strong>Paper Abstract:</strong> We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e464.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e464.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMPACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IMPACT (hybrid autonomous robot system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid cognitive-robotics system that integrates sub-symbolic reinforcement learning with a symbolic planning/knowledge module so that learned knowledge is automatically encoded into symbolic form for planning and behavior control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IMPACT hybrid cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>IMPACT is described as an autonomous robot system that integrates a sub-symbolic reinforcement learning unit (for perception and skill acquisition) with a symbolic planning and behavior coordination module. The integration is designed so that the knowledge acquired by the learning module is encoded into the symbolic knowledge base, which then drives high-level actions and planning, enabling continual and open-ended learning and the extension of symbolic knowledge as the robot interacts with the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic planning and symbolic knowledge base (procedural rules/instructions encoded as symbolic structures used for planning and behavior sequencing).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Connectionist sub-symbolic unit implemented as reinforcement learning (RL) for processing sensory data and acquiring behaviors (procedural neural learner).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular hybrid integration where the learning (imperative) module feeds learned knowledge into the symbolic knowledge base automatically (conversion/encoding of learnt patterns/skills into symbolic rules); symbolic module then uses those rules for planning and behavior control (task decomposition and execution).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Continual and open-ended learning where procedural skills learned via RL become available for symbolic reasoning and planning; improved adaptability and capability to form high-level plans from low-level learned skills; supports incremental extension of symbolic knowledge over time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Autonomous robot tasks (localization, navigation, exploration, behavior coordination) in open-world robotic settings (as described qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Enables generalization by allowing learned procedural skills to be abstracted into symbolic forms that can be recombined in new contexts; implied better adaptability to unanticipated tasks due to symbolic recomposition of learned skills.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic knowledge base provides interpretable rules for high-level decisions and action selection; however low-level learned behaviors remain opaque unless explicitly encoded into symbolic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Interpretability is partial: actions executed by sub-symbolic modules remain opaque unless their knowledge is converted to symbols; converting learned procedural knowledge into correct symbolic form requires robust mapping mechanisms and may be error-prone.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division-of-labor (dual-process) complementary framework: sub-symbolic fast learner (system 1) for procedural acquisition and symbolic slow reasoner (system 2) for planning, with automatic encoding bridging the two.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e464.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOFAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SOFAI (dual-system cognitive architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid cognitive architecture employing a fast learning system and a slow reasoning system with a metacognitive arbitrator supervising which system handles a given task, aimed at continual improvement and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SOFAI dual-system architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SOFAI comprises a fast learner (connectionist) that continually updates via experience and a slow reasoner (symbolic) that performs deliberative problem solving; a metacognitive module monitors agent state, environmental percepts, and the adequacy of system 1's solution and delegates to system 2 when necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic reasoning module for deliberative problem solving and meta-level monitoring (rule-based reasoning / symbolic planner).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Fast learning connectionist subsystem (neural-network style learning) for rapid, experience-driven predictions and behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Meta-cognitive arbitration: a supervisory module assesses confidence/adequacy of the fast learner and triggers symbolic reasoning when needed; knowledge can be shared between systems through the meta-module.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Robust detection of difficult or anomalous situations (e.g., adversarial inputs) by invoking symbolic reasoning; balanced trade-off between fast responses and careful deliberation; ability to improve system 1 over time through experience overseen by system 2.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General cognitive tasks and autonomous agent problem solving; adversarial robustness and anomaly detection are discussed qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improved adaptability to novel situations by routing ambiguous/problematic inputs to symbolic reasoning; supports continual learning and incremental improvement of the fast learner.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>When the symbolic system is invoked, decisions are interpretable (rule-based); however many routine decisions made by the fast learner remain opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not all decisions are explainable because system 1 remains opaque; arbitration policy (when to invoke system 2) is critical and may be suboptimal if not well-designed.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Dual-process theory (system 1/system 2) with a metacognitive monitor enabling division of labor and selective invocation of symbolic deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e464.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARACaS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARACaS (hybrid cognitive architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid architecture that combines connectionist reinforcement-learning perception units with separate symbolic modules for planning and behavior coordination in robotic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CARACaS hybrid architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CARACaS employs a connectionist reinforcement learning unit to process multi-source sensory data and learn environment representations and localization, while separate symbolic modules handle dynamic planning and behavior coordination; the symbolic layer uses learned information to guide higher-level decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic processing modules for planning and behavior coordination (rule-based control).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Connectionist reinforcement learning unit(s) for multisensory perception, representation learning and low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular separation with the neural RL unit providing perceptual/state estimates to symbolic planners; symbolic module issues high-level commands that the RL/perceptual unit grounds and executes.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines robust perceptual learning with interpretable high-level planning; resilience to sensory corruption via redundancy of learned percepts and symbolic sanity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Robotic localization, navigation and behavior coordination tasks (qualitatively described).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improved adaptability across sensor modalities by using neural perception with symbolic planning for composition in novel situations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic planning affords explainable high-level action rationale; low-level learned percepts are less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic policies are limited by predefined rules and may struggle in unanticipated situations if mapping from learned percepts to symbols is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Modularity and division-of-labor: connectionist perception feeding symbolic reasoning for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e464.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iCub</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>iCub (hybrid cognitive humanoid platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open humanoid robotics platform and cognitive architecture using hybrid techniques that combine neural learning modules with symbolic cognitive components for perception, learning and interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>icub: the design and realization of an open humanoid platform for cognitive and neuroscience research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>iCub hybrid cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>iCub is an embodied humanoid robot platform whose cognitive architecture uses both connectionist/sub-symbolic components for perception and motor control and symbolic modules for higher-level skills; some designs allow integration so new sub-symbolic skills can be incorporated into symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic modules for task-level representation and decision logic (symbolic knowledge/rule representations).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Connectionist neural modules for sensory processing, motor control and learned behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Integrated approach where learned sub-symbolic skills can be transferred into symbolic modules to expand cognitive capabilities; modular but supporting encoding of learning into symbolic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to extend symbolic skill repertoire from learned neural behaviors; improved human-robot interaction capacities and continuously extensible symbolic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Robotic cognition, social interaction and embodied learning experiments (platform-level capabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Supports open-ended learning and transfer of learned behaviors to symbolic reasoning, enabling better adaptation in physically embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic modules provide interpretable decision logic for high-level behaviors; many low-level behaviors remain neural and opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Partial interpretability and the difficulty of reliably encoding complex learned behaviors into symbolic forms; scalability of symbolic rule sets remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Modular hybrid cognitive-robotics framework emphasizing integration of learned skills into symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e464.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wu2022-hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid driving decision-making system (MLN + DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid driving decision-making system that integrates a Markov Logic Network (symbolic/probabilistic logic) with a Deep Q-Network (reinforcement-learning) to combine interpretable reasoning with learned driving policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLN + DQN hybrid driving decision system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The design uses a DQN (imperative RL) to learn driving policies and a Markov Logic Network (MLN) symbolic-probabilistic framework to represent and reason about driving rules; the MLN evaluates driving actions and provides interpretable decision-level assessments that abstract away opaque low-level policy choices.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Markov Logic Network (MLN) — probabilistic first-order logic representing rules and constraints about driving actions and contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Deep Q-Network (DQN) — deep reinforcement learning module learning driving policies from sensor input.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular hybridization: the DQN learns low-level policies while the MLN evaluates and abstracts actions for interpretable decision-making; MLN may provide supervisory evaluation and rule-based filtering of actions.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Interpretable evaluation of learned policies, partial correction/verification of unsafe learned actions via MLN constraints, potential improvement in safety and explainability without abandoning learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Autonomous driving decision-making tasks (safety-critical driving actions); discussed qualitatively in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Hybrid approach can reject or correct implausible learned actions via logical constraints, potentially improving out-of-distribution safety; exact OOD generalization not quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>MLN provides interpretable, probabilistic logical explanations for why certain actions are accepted or rejected.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Interpretability exists only at the MLN layer; the end-to-end policy (DQN) remains opaque; balancing MLN constraints and learned policies can be nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Complementary strengths: symbolic probabilistic constraints for interpretability and safety combined with neural learning for high-dimensional perception and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e464.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOGICDEF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOGICDEF (interpretable defense via inductive scene graph reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid adversarial-defense framework that extracts first-order logic rules from scene graphs and combines them with ConceptNet commonsense knowledge to enforce contextual consistency and detect implausible (adversarial) predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logicdef: An interpretable defense framework against adversarial examples via inductive scene graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LOGICDEF</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LOGICDEF builds a structured scene representation (scene graph) from perception outputs, mines logical context rules about objects and relationships, augments the rules with ConceptNet commonsense facts, and enforces contextual consistency to detect and correct incoherent model predictions caused by adversarial perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order logic rules mined from scene graphs and external commonsense knowledge (ConceptNet) representing constraints and contextual relations between objects.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Visual perception pipeline (CNN-based object detectors/scene graph generators) that produces object and relation candidates from images.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline/modular integration: perceptual outputs (objects, relations) are converted to symbolic scene graphs; logic rules (from mined rules + ConceptNet) are applied as constraints/post-hoc checks to detect and rectify implausible predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to detect visually implausible/adversarial predictions based on semantic inconsistency (e.g., improbable object co-occurrences), improved interpretability because failures can be explained in logic terms, and robustness improvements without compromising clean accuracy (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Adversarial defense for image classification / scene understanding (detection of adversarial patches and implausible labels).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improves detection of semantically implausible outputs produced by vision models, including adversarially altered inputs; uses external KG to broaden coverage of commonsense constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High — explanations are provided as logical consistency checks and rules (e.g., scene-level contradictions) that indicate why an output was deemed implausible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Depends on quality and coverage of mined rules and external KGs; may miss adversarial examples that respect contextual constraints; requires effective scene graph extraction and predicate mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Symbolic constraint enforcement over perceptual outputs; combines structured scene representation with commonsense knowledge to perform sanity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e464.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LENs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Explained Networks (LENs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that learns first-order logic rules from human-interpretable predicates to provide logical explanations for neural predictions, usable as a post-hoc explainer or as an end-to-end interpretable model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic explained networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Explained Networks (LENs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LENs take interpretable predicate inputs (high-level concepts) and learn mappings to outputs while forming explicit first-order logic rules that explain the decisions; LENs can be attached post-hoc to black-box models or trained end-to-end as an inherently interpretable model.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order logic rules in output space learned and expressed explicitly as logical formulas (rules over human-interpretable predicates).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>A neural learning component that maps inputs to predicates or directly to outputs while jointly inducing logical rules (neural modules used to estimate concept predicates or to classify).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Joint learning or post-hoc extraction: either train LEN end-to-end to produce outputs and logic simultaneously, or attach LEN as a post-hoc module that learns logical explanations over existing model predicates/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Provides human-readable logical explanations tied to model predictions; can enforce logical compliance during inference and discard spurious outputs inconsistent with learned rules; bridges neural generalization with symbolic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Explainability for classification tasks, detection of incoherent/adversarial predictions, and potentially improving structured prediction via logical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>By constraining outputs with learned logic, LENs can reduce spurious correlations and improve robustness in some settings; their generalization depends on quality of predicates and learned rules.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High — outputs are explained as combinations of learned first-order logic rules; can be used to produce explicit, human-readable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires inputs in the form of symbolic predicates or reliable concept predictors; performance depends on availability and correctness of human-interpretable predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Neuro-symbolic rule induction: jointly learn mapping and logical formulas to explain and constrain outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e464.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Network (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture in which neurons are associated 1-to-1 with logical propositions and activation functions are constrained to implement specified logical operations, enabling neural computation with explicit semantic meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logical neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logical Neural Network (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LNNs bind logical statements (propositional or first-order) to neural units so that network activations compute logical operations; connections have semantic meaning, and the network's architecture enforces logical constraints on activations.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic logic representation (propositional or first-order logic) explicitly encoded as neuron-level semantics and logical operators.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural network computation where activations and weights implement and learn parameters consistent with logical semantics (differentiable neural components constrained by logic).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight integration: the neural architecture is constructed so that neurons correspond to logical atoms/operations and learning adjusts parameters under logical constraints (neuro-symbolic embedding of logic into NN topology).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Semantically meaningful neural units enabling interpretability, ability to perform logical inference in a differentiable setting, and the potential to learn logic-consistent models that combine perception and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Logic-constrained learning and reasoning tasks; interpretability and logical inference over learned representations (discussed conceptually in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>By enforcing logical structure, LNNs can avoid some spurious correlations and generalize better on tasks where logical constraints are relevant; generalization depends on adequacy of logical encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High — direct mapping between neurons and logical statements yields transparent semantics and traceable inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires explicit logic specification or predicates; scalability and expressivity trade-offs when encoding complex logic into neurons; may be limited on raw perceptual inputs without predicate extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Neuro-symbolic unification: embedding symbolic logic into differentiable neural architectures so that neural computation corresponds to logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e464.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROCK / CORL / RPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROCK / CORL / RPC (compositional part-based hybrid models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of compositional part-based recognition models that combine neural part detectors/embeddings with symbolic commonsense or human prior rules about part relationships to improve robustness, interpretability and zero/few-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Compositional part-based hybrid recognition (ROCK/CORL/RPC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These systems decompose objects into constituent parts using learned neural detectors or part dictionaries, then use symbolic part-linkage rules or human prior knowledge to judge plausible whole-object compositions; a judgement block evaluates independent part-based scores and applies commonsense rules to finalize predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Human prior knowledge and commonsense rules about part relationships and compositional constraints (symbolic rules / part dictionaries specifying allowed topologies).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks for part detection and part-feature embedding (CNN-based part detectors, attention modules, or trainable part encoders).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular composition: neural part detectors provide part presence and scores; a symbolic judgement/evaluator block applies rule-based part-linkage constraints and commonsense knowledge to aggregate part evidence into final class prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Robustness to adversarial and partial corruptions (because reasoning over parts detects improbable part compositions), improved interpretability via human-understandable part relationships, and enhanced zero/few-shot generalization by recombining known parts into novel object concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Image classification, few-shot/zero-shot object recognition, and adversarial robustness benchmarks (qualitative claims of improved performance across attacks and generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Compositionality enables recognition of novel categories from few examples by reusing known parts; empirically claimed to generalize better to unknown adversaries and unseen classes than monolithic perceptual models (as described qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High — decisions can be explained in terms of detected parts and their symbolic relationships/topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires careful choice of parts that remain recognizable under deformation; relies on quality of part detectors and correctness/completeness of symbolic part-linkage rules; may struggle when parts are ambiguous or heavily occluded.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Compositionality and part-based reasoning: human prior knowledge about parts and their spatial/topological relations is combined with learned perceptual detectors to support robust, interpretable inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e464.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e464.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExCAR / KEMLP / KGIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExCAR, KEMLP, KGIN (knowledge-and-rule-augmented hybrid pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative hybrid pipelines that augment neural representation learning with symbolic structure learning or logic/probabilistic rules (e.g., learning logical rules over representations, injecting logic into probabilistic models, or modeling user intents with KG relations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid knowledge-and-rule-augmented representation learning (ExCAR/KEMLP/KGIN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>These systems combine neural representation learners (imperative embeddings and encoders) with symbolic/probabilistic rule learners or knowledge graph modules; examples include ExCAR (representation learning + structure learning of logical rules), KEMLP (integrating domain logical relationships into probabilistic pipelines), and KGIN (modeling user intents via attentive KG relation combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Learnt or provided logical rules, probabilistic logical frameworks, and knowledge graphs representing relations and symbolic domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural representation learning modules (embeddings, encoders, classifiers) and differentiable learning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight coupling via regularization or augmented loss (learning symbolic structure from representations or enforcing logical/probabilistic constraints on neural predictions); attention over KG relations to integrate symbolic context into neural scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved explainability (symbolic rules can be used to explain outputs), enhanced robustness to adversarial examples due to logical consistency checks, and better performance in low-data / zero-shot scenarios by leveraging external symbolic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Explainable causal reasoning, adversarial robustness benchmarks, recommendation systems and zero/few-shot learning tasks (as summarized in the survey's table).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>By combining symbolic priors with learned representations, these systems can generalize better under domain shifts and low-data regimes compared to purely neural baselines, though exact gains depend on quality of symbolic knowledge and integration method.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic outputs (rules, KG-based reasoning traces) provide post-hoc or inherent explanations; interpretability is improved relative to purely neural pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic knowledge coverage and correctness limit benefits; integrating noisy or misaligned KG info can introduce errors; learning and reasoning coupling may be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Complementary strengths: symbolic knowledge provides constraints and explanations while neural representations provide rich perceptual features; integration via structure learning or constraint enforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logicdef: An interpretable defense framework against adversarial examples via inductive scene graph reasoning <em>(Rating: 2)</em></li>
                <li>Logic explained networks <em>(Rating: 2)</em></li>
                <li>Logical neural networks <em>(Rating: 2)</em></li>
                <li>Recognizing Object by Components (CORL) <em>(Rating: 2)</em></li>
                <li>Recognizing Object by Components with human prior Knowledge (ROCK) <em>(Rating: 2)</em></li>
                <li>IMPACT (autonomous robot system descriptions) <em>(Rating: 1)</em></li>
                <li>SOFAI (dual-system cognitive architecture) <em>(Rating: 1)</em></li>
                <li>CARACaS hybrid architecture papers <em>(Rating: 1)</em></li>
                <li>Hybrid driving decision-making system integrating Markov Logic Networks and connectionist AI <em>(Rating: 1)</em></li>
                <li>ExCAR: Event graph knowledge enhanced explainable causal reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-464",
    "paper_id": "paper-265502630",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "IMPACT",
            "name_full": "IMPACT (hybrid autonomous robot system)",
            "brief_description": "A hybrid cognitive-robotics system that integrates sub-symbolic reinforcement learning with a symbolic planning/knowledge module so that learned knowledge is automatically encoded into symbolic form for planning and behavior control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "IMPACT hybrid cognitive architecture",
            "system_description": "IMPACT is described as an autonomous robot system that integrates a sub-symbolic reinforcement learning unit (for perception and skill acquisition) with a symbolic planning and behavior coordination module. The integration is designed so that the knowledge acquired by the learning module is encoded into the symbolic knowledge base, which then drives high-level actions and planning, enabling continual and open-ended learning and the extension of symbolic knowledge as the robot interacts with the environment.",
            "declarative_component": "Symbolic planning and symbolic knowledge base (procedural rules/instructions encoded as symbolic structures used for planning and behavior sequencing).",
            "imperative_component": "Connectionist sub-symbolic unit implemented as reinforcement learning (RL) for processing sensory data and acquiring behaviors (procedural neural learner).",
            "integration_method": "Modular hybrid integration where the learning (imperative) module feeds learned knowledge into the symbolic knowledge base automatically (conversion/encoding of learnt patterns/skills into symbolic rules); symbolic module then uses those rules for planning and behavior control (task decomposition and execution).",
            "emergent_properties": "Continual and open-ended learning where procedural skills learned via RL become available for symbolic reasoning and planning; improved adaptability and capability to form high-level plans from low-level learned skills; supports incremental extension of symbolic knowledge over time.",
            "task_or_benchmark": "Autonomous robot tasks (localization, navigation, exploration, behavior coordination) in open-world robotic settings (as described qualitatively).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Enables generalization by allowing learned procedural skills to be abstracted into symbolic forms that can be recombined in new contexts; implied better adaptability to unanticipated tasks due to symbolic recomposition of learned skills.",
            "interpretability_properties": "Symbolic knowledge base provides interpretable rules for high-level decisions and action selection; however low-level learned behaviors remain opaque unless explicitly encoded into symbolic representations.",
            "limitations_or_failures": "Interpretability is partial: actions executed by sub-symbolic modules remain opaque unless their knowledge is converted to symbols; converting learned procedural knowledge into correct symbolic form requires robust mapping mechanisms and may be error-prone.",
            "theoretical_framework": "Division-of-labor (dual-process) complementary framework: sub-symbolic fast learner (system 1) for procedural acquisition and symbolic slow reasoner (system 2) for planning, with automatic encoding bridging the two.",
            "uuid": "e464.0",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SOFAI",
            "name_full": "SOFAI (dual-system cognitive architecture)",
            "brief_description": "A hybrid cognitive architecture employing a fast learning system and a slow reasoning system with a metacognitive arbitrator supervising which system handles a given task, aimed at continual improvement and robustness.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SOFAI dual-system architecture",
            "system_description": "SOFAI comprises a fast learner (connectionist) that continually updates via experience and a slow reasoner (symbolic) that performs deliberative problem solving; a metacognitive module monitors agent state, environmental percepts, and the adequacy of system 1's solution and delegates to system 2 when necessary.",
            "declarative_component": "Symbolic reasoning module for deliberative problem solving and meta-level monitoring (rule-based reasoning / symbolic planner).",
            "imperative_component": "Fast learning connectionist subsystem (neural-network style learning) for rapid, experience-driven predictions and behaviors.",
            "integration_method": "Meta-cognitive arbitration: a supervisory module assesses confidence/adequacy of the fast learner and triggers symbolic reasoning when needed; knowledge can be shared between systems through the meta-module.",
            "emergent_properties": "Robust detection of difficult or anomalous situations (e.g., adversarial inputs) by invoking symbolic reasoning; balanced trade-off between fast responses and careful deliberation; ability to improve system 1 over time through experience overseen by system 2.",
            "task_or_benchmark": "General cognitive tasks and autonomous agent problem solving; adversarial robustness and anomaly detection are discussed qualitatively.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Improved adaptability to novel situations by routing ambiguous/problematic inputs to symbolic reasoning; supports continual learning and incremental improvement of the fast learner.",
            "interpretability_properties": "When the symbolic system is invoked, decisions are interpretable (rule-based); however many routine decisions made by the fast learner remain opaque.",
            "limitations_or_failures": "Not all decisions are explainable because system 1 remains opaque; arbitration policy (when to invoke system 2) is critical and may be suboptimal if not well-designed.",
            "theoretical_framework": "Dual-process theory (system 1/system 2) with a metacognitive monitor enabling division of labor and selective invocation of symbolic deliberation.",
            "uuid": "e464.1",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CARACaS",
            "name_full": "CARACaS (hybrid cognitive architecture)",
            "brief_description": "A hybrid architecture that combines connectionist reinforcement-learning perception units with separate symbolic modules for planning and behavior coordination in robotic systems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CARACaS hybrid architecture",
            "system_description": "CARACaS employs a connectionist reinforcement learning unit to process multi-source sensory data and learn environment representations and localization, while separate symbolic modules handle dynamic planning and behavior coordination; the symbolic layer uses learned information to guide higher-level decisions.",
            "declarative_component": "Symbolic processing modules for planning and behavior coordination (rule-based control).",
            "imperative_component": "Connectionist reinforcement learning unit(s) for multisensory perception, representation learning and low-level control.",
            "integration_method": "Modular separation with the neural RL unit providing perceptual/state estimates to symbolic planners; symbolic module issues high-level commands that the RL/perceptual unit grounds and executes.",
            "emergent_properties": "Combines robust perceptual learning with interpretable high-level planning; resilience to sensory corruption via redundancy of learned percepts and symbolic sanity checks.",
            "task_or_benchmark": "Robotic localization, navigation and behavior coordination tasks (qualitatively described).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Improved adaptability across sensor modalities by using neural perception with symbolic planning for composition in novel situations.",
            "interpretability_properties": "Symbolic planning affords explainable high-level action rationale; low-level learned percepts are less interpretable.",
            "limitations_or_failures": "Symbolic policies are limited by predefined rules and may struggle in unanticipated situations if mapping from learned percepts to symbols is imperfect.",
            "theoretical_framework": "Modularity and division-of-labor: connectionist perception feeding symbolic reasoning for planning.",
            "uuid": "e464.2",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "iCub",
            "name_full": "iCub (hybrid cognitive humanoid platform)",
            "brief_description": "An open humanoid robotics platform and cognitive architecture using hybrid techniques that combine neural learning modules with symbolic cognitive components for perception, learning and interaction.",
            "citation_title": "icub: the design and realization of an open humanoid platform for cognitive and neuroscience research",
            "mention_or_use": "mention",
            "system_name": "iCub hybrid cognitive architecture",
            "system_description": "iCub is an embodied humanoid robot platform whose cognitive architecture uses both connectionist/sub-symbolic components for perception and motor control and symbolic modules for higher-level skills; some designs allow integration so new sub-symbolic skills can be incorporated into symbolic modules.",
            "declarative_component": "Symbolic modules for task-level representation and decision logic (symbolic knowledge/rule representations).",
            "imperative_component": "Connectionist neural modules for sensory processing, motor control and learned behaviors.",
            "integration_method": "Integrated approach where learned sub-symbolic skills can be transferred into symbolic modules to expand cognitive capabilities; modular but supporting encoding of learning into symbolic knowledge.",
            "emergent_properties": "Ability to extend symbolic skill repertoire from learned neural behaviors; improved human-robot interaction capacities and continuously extensible symbolic knowledge.",
            "task_or_benchmark": "Robotic cognition, social interaction and embodied learning experiments (platform-level capabilities).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Supports open-ended learning and transfer of learned behaviors to symbolic reasoning, enabling better adaptation in physically embodied tasks.",
            "interpretability_properties": "Symbolic modules provide interpretable decision logic for high-level behaviors; many low-level behaviors remain neural and opaque.",
            "limitations_or_failures": "Partial interpretability and the difficulty of reliably encoding complex learned behaviors into symbolic forms; scalability of symbolic rule sets remains a challenge.",
            "theoretical_framework": "Modular hybrid cognitive-robotics framework emphasizing integration of learned skills into symbolic reasoning.",
            "uuid": "e464.3",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Wu2022-hybrid",
            "name_full": "Hybrid driving decision-making system (MLN + DQN)",
            "brief_description": "A hybrid driving decision-making system that integrates a Markov Logic Network (symbolic/probabilistic logic) with a Deep Q-Network (reinforcement-learning) to combine interpretable reasoning with learned driving policies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MLN + DQN hybrid driving decision system",
            "system_description": "The design uses a DQN (imperative RL) to learn driving policies and a Markov Logic Network (MLN) symbolic-probabilistic framework to represent and reason about driving rules; the MLN evaluates driving actions and provides interpretable decision-level assessments that abstract away opaque low-level policy choices.",
            "declarative_component": "Markov Logic Network (MLN) — probabilistic first-order logic representing rules and constraints about driving actions and contexts.",
            "imperative_component": "Deep Q-Network (DQN) — deep reinforcement learning module learning driving policies from sensor input.",
            "integration_method": "Modular hybridization: the DQN learns low-level policies while the MLN evaluates and abstracts actions for interpretable decision-making; MLN may provide supervisory evaluation and rule-based filtering of actions.",
            "emergent_properties": "Interpretable evaluation of learned policies, partial correction/verification of unsafe learned actions via MLN constraints, potential improvement in safety and explainability without abandoning learned policies.",
            "task_or_benchmark": "Autonomous driving decision-making tasks (safety-critical driving actions); discussed qualitatively in the survey.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Hybrid approach can reject or correct implausible learned actions via logical constraints, potentially improving out-of-distribution safety; exact OOD generalization not quantified in survey.",
            "interpretability_properties": "MLN provides interpretable, probabilistic logical explanations for why certain actions are accepted or rejected.",
            "limitations_or_failures": "Interpretability exists only at the MLN layer; the end-to-end policy (DQN) remains opaque; balancing MLN constraints and learned policies can be nontrivial.",
            "theoretical_framework": "Complementary strengths: symbolic probabilistic constraints for interpretability and safety combined with neural learning for high-dimensional perception and policy learning.",
            "uuid": "e464.4",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LOGICDEF",
            "name_full": "LOGICDEF (interpretable defense via inductive scene graph reasoning)",
            "brief_description": "A hybrid adversarial-defense framework that extracts first-order logic rules from scene graphs and combines them with ConceptNet commonsense knowledge to enforce contextual consistency and detect implausible (adversarial) predictions.",
            "citation_title": "Logicdef: An interpretable defense framework against adversarial examples via inductive scene graph reasoning",
            "mention_or_use": "mention",
            "system_name": "LOGICDEF",
            "system_description": "LOGICDEF builds a structured scene representation (scene graph) from perception outputs, mines logical context rules about objects and relationships, augments the rules with ConceptNet commonsense facts, and enforces contextual consistency to detect and correct incoherent model predictions caused by adversarial perturbations.",
            "declarative_component": "First-order logic rules mined from scene graphs and external commonsense knowledge (ConceptNet) representing constraints and contextual relations between objects.",
            "imperative_component": "Visual perception pipeline (CNN-based object detectors/scene graph generators) that produces object and relation candidates from images.",
            "integration_method": "Pipeline/modular integration: perceptual outputs (objects, relations) are converted to symbolic scene graphs; logic rules (from mined rules + ConceptNet) are applied as constraints/post-hoc checks to detect and rectify implausible predictions.",
            "emergent_properties": "Ability to detect visually implausible/adversarial predictions based on semantic inconsistency (e.g., improbable object co-occurrences), improved interpretability because failures can be explained in logic terms, and robustness improvements without compromising clean accuracy (qualitative claim).",
            "task_or_benchmark": "Adversarial defense for image classification / scene understanding (detection of adversarial patches and implausible labels).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Improves detection of semantically implausible outputs produced by vision models, including adversarially altered inputs; uses external KG to broaden coverage of commonsense constraints.",
            "interpretability_properties": "High — explanations are provided as logical consistency checks and rules (e.g., scene-level contradictions) that indicate why an output was deemed implausible.",
            "limitations_or_failures": "Depends on quality and coverage of mined rules and external KGs; may miss adversarial examples that respect contextual constraints; requires effective scene graph extraction and predicate mapping.",
            "theoretical_framework": "Symbolic constraint enforcement over perceptual outputs; combines structured scene representation with commonsense knowledge to perform sanity checks.",
            "uuid": "e464.5",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LENs",
            "name_full": "Logic Explained Networks (LENs)",
            "brief_description": "A framework that learns first-order logic rules from human-interpretable predicates to provide logical explanations for neural predictions, usable as a post-hoc explainer or as an end-to-end interpretable model.",
            "citation_title": "Logic explained networks",
            "mention_or_use": "mention",
            "system_name": "Logic Explained Networks (LENs)",
            "system_description": "LENs take interpretable predicate inputs (high-level concepts) and learn mappings to outputs while forming explicit first-order logic rules that explain the decisions; LENs can be attached post-hoc to black-box models or trained end-to-end as an inherently interpretable model.",
            "declarative_component": "First-order logic rules in output space learned and expressed explicitly as logical formulas (rules over human-interpretable predicates).",
            "imperative_component": "A neural learning component that maps inputs to predicates or directly to outputs while jointly inducing logical rules (neural modules used to estimate concept predicates or to classify).",
            "integration_method": "Joint learning or post-hoc extraction: either train LEN end-to-end to produce outputs and logic simultaneously, or attach LEN as a post-hoc module that learns logical explanations over existing model predicates/outputs.",
            "emergent_properties": "Provides human-readable logical explanations tied to model predictions; can enforce logical compliance during inference and discard spurious outputs inconsistent with learned rules; bridges neural generalization with symbolic interpretability.",
            "task_or_benchmark": "Explainability for classification tasks, detection of incoherent/adversarial predictions, and potentially improving structured prediction via logical constraints.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "By constraining outputs with learned logic, LENs can reduce spurious correlations and improve robustness in some settings; their generalization depends on quality of predicates and learned rules.",
            "interpretability_properties": "High — outputs are explained as combinations of learned first-order logic rules; can be used to produce explicit, human-readable explanations.",
            "limitations_or_failures": "Requires inputs in the form of symbolic predicates or reliable concept predictors; performance depends on availability and correctness of human-interpretable predicates.",
            "theoretical_framework": "Neuro-symbolic rule induction: jointly learn mapping and logical formulas to explain and constrain outputs.",
            "uuid": "e464.6",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LNN",
            "name_full": "Logical Neural Network (LNN)",
            "brief_description": "A neural architecture in which neurons are associated 1-to-1 with logical propositions and activation functions are constrained to implement specified logical operations, enabling neural computation with explicit semantic meaning.",
            "citation_title": "Logical neural networks",
            "mention_or_use": "mention",
            "system_name": "Logical Neural Network (LNN)",
            "system_description": "LNNs bind logical statements (propositional or first-order) to neural units so that network activations compute logical operations; connections have semantic meaning, and the network's architecture enforces logical constraints on activations.",
            "declarative_component": "Symbolic logic representation (propositional or first-order logic) explicitly encoded as neuron-level semantics and logical operators.",
            "imperative_component": "Neural network computation where activations and weights implement and learn parameters consistent with logical semantics (differentiable neural components constrained by logic).",
            "integration_method": "Tight integration: the neural architecture is constructed so that neurons correspond to logical atoms/operations and learning adjusts parameters under logical constraints (neuro-symbolic embedding of logic into NN topology).",
            "emergent_properties": "Semantically meaningful neural units enabling interpretability, ability to perform logical inference in a differentiable setting, and the potential to learn logic-consistent models that combine perception and reasoning.",
            "task_or_benchmark": "Logic-constrained learning and reasoning tasks; interpretability and logical inference over learned representations (discussed conceptually in the survey).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "By enforcing logical structure, LNNs can avoid some spurious correlations and generalize better on tasks where logical constraints are relevant; generalization depends on adequacy of logical encoding.",
            "interpretability_properties": "High — direct mapping between neurons and logical statements yields transparent semantics and traceable inferences.",
            "limitations_or_failures": "Requires explicit logic specification or predicates; scalability and expressivity trade-offs when encoding complex logic into neurons; may be limited on raw perceptual inputs without predicate extraction.",
            "theoretical_framework": "Neuro-symbolic unification: embedding symbolic logic into differentiable neural architectures so that neural computation corresponds to logical inference.",
            "uuid": "e464.7",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ROCK / CORL / RPC",
            "name_full": "ROCK / CORL / RPC (compositional part-based hybrid models)",
            "brief_description": "A family of compositional part-based recognition models that combine neural part detectors/embeddings with symbolic commonsense or human prior rules about part relationships to improve robustness, interpretability and zero/few-shot generalization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Compositional part-based hybrid recognition (ROCK/CORL/RPC)",
            "system_description": "These systems decompose objects into constituent parts using learned neural detectors or part dictionaries, then use symbolic part-linkage rules or human prior knowledge to judge plausible whole-object compositions; a judgement block evaluates independent part-based scores and applies commonsense rules to finalize predictions.",
            "declarative_component": "Human prior knowledge and commonsense rules about part relationships and compositional constraints (symbolic rules / part dictionaries specifying allowed topologies).",
            "imperative_component": "Neural networks for part detection and part-feature embedding (CNN-based part detectors, attention modules, or trainable part encoders).",
            "integration_method": "Modular composition: neural part detectors provide part presence and scores; a symbolic judgement/evaluator block applies rule-based part-linkage constraints and commonsense knowledge to aggregate part evidence into final class prediction.",
            "emergent_properties": "Robustness to adversarial and partial corruptions (because reasoning over parts detects improbable part compositions), improved interpretability via human-understandable part relationships, and enhanced zero/few-shot generalization by recombining known parts into novel object concepts.",
            "task_or_benchmark": "Image classification, few-shot/zero-shot object recognition, and adversarial robustness benchmarks (qualitative claims of improved performance across attacks and generalization).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Compositionality enables recognition of novel categories from few examples by reusing known parts; empirically claimed to generalize better to unknown adversaries and unseen classes than monolithic perceptual models (as described qualitatively).",
            "interpretability_properties": "High — decisions can be explained in terms of detected parts and their symbolic relationships/topologies.",
            "limitations_or_failures": "Requires careful choice of parts that remain recognizable under deformation; relies on quality of part detectors and correctness/completeness of symbolic part-linkage rules; may struggle when parts are ambiguous or heavily occluded.",
            "theoretical_framework": "Compositionality and part-based reasoning: human prior knowledge about parts and their spatial/topological relations is combined with learned perceptual detectors to support robust, interpretable inference.",
            "uuid": "e464.8",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ExCAR / KEMLP / KGIN",
            "name_full": "ExCAR, KEMLP, KGIN (knowledge-and-rule-augmented hybrid pipelines)",
            "brief_description": "Representative hybrid pipelines that augment neural representation learning with symbolic structure learning or logic/probabilistic rules (e.g., learning logical rules over representations, injecting logic into probabilistic models, or modeling user intents with KG relations).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Hybrid knowledge-and-rule-augmented representation learning (ExCAR/KEMLP/KGIN)",
            "system_description": "These systems combine neural representation learners (imperative embeddings and encoders) with symbolic/probabilistic rule learners or knowledge graph modules; examples include ExCAR (representation learning + structure learning of logical rules), KEMLP (integrating domain logical relationships into probabilistic pipelines), and KGIN (modeling user intents via attentive KG relation combinations).",
            "declarative_component": "Learnt or provided logical rules, probabilistic logical frameworks, and knowledge graphs representing relations and symbolic domain knowledge.",
            "imperative_component": "Neural representation learning modules (embeddings, encoders, classifiers) and differentiable learning pipelines.",
            "integration_method": "Tight coupling via regularization or augmented loss (learning symbolic structure from representations or enforcing logical/probabilistic constraints on neural predictions); attention over KG relations to integrate symbolic context into neural scoring.",
            "emergent_properties": "Improved explainability (symbolic rules can be used to explain outputs), enhanced robustness to adversarial examples due to logical consistency checks, and better performance in low-data / zero-shot scenarios by leveraging external symbolic knowledge.",
            "task_or_benchmark": "Explainable causal reasoning, adversarial robustness benchmarks, recommendation systems and zero/few-shot learning tasks (as summarized in the survey's table).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "By combining symbolic priors with learned representations, these systems can generalize better under domain shifts and low-data regimes compared to purely neural baselines, though exact gains depend on quality of symbolic knowledge and integration method.",
            "interpretability_properties": "Symbolic outputs (rules, KG-based reasoning traces) provide post-hoc or inherent explanations; interpretability is improved relative to purely neural pipelines.",
            "limitations_or_failures": "Symbolic knowledge coverage and correctness limit benefits; integrating noisy or misaligned KG info can introduce errors; learning and reasoning coupling may be computationally expensive.",
            "theoretical_framework": "Complementary strengths: symbolic knowledge provides constraints and explanations while neural representations provide rich perceptual features; integration via structure learning or constraint enforcement.",
            "uuid": "e464.9",
            "source_info": {
                "paper_title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logicdef: An interpretable defense framework against adversarial examples via inductive scene graph reasoning",
            "rating": 2,
            "sanitized_title": "logicdef_an_interpretable_defense_framework_against_adversarial_examples_via_inductive_scene_graph_reasoning"
        },
        {
            "paper_title": "Logic explained networks",
            "rating": 2,
            "sanitized_title": "logic_explained_networks"
        },
        {
            "paper_title": "Logical neural networks",
            "rating": 2,
            "sanitized_title": "logical_neural_networks"
        },
        {
            "paper_title": "Recognizing Object by Components (CORL)",
            "rating": 2,
            "sanitized_title": "recognizing_object_by_components_corl"
        },
        {
            "paper_title": "Recognizing Object by Components with human prior Knowledge (ROCK)",
            "rating": 2,
            "sanitized_title": "recognizing_object_by_components_with_human_prior_knowledge_rock"
        },
        {
            "paper_title": "IMPACT (autonomous robot system descriptions)",
            "rating": 1,
            "sanitized_title": "impact_autonomous_robot_system_descriptions"
        },
        {
            "paper_title": "SOFAI (dual-system cognitive architecture)",
            "rating": 1,
            "sanitized_title": "sofai_dualsystem_cognitive_architecture"
        },
        {
            "paper_title": "CARACaS hybrid architecture papers",
            "rating": 1,
            "sanitized_title": "caracas_hybrid_architecture_papers"
        },
        {
            "paper_title": "Hybrid driving decision-making system integrating Markov Logic Networks and connectionist AI",
            "rating": 1,
            "sanitized_title": "hybrid_driving_decisionmaking_system_integrating_markov_logic_networks_and_connectionist_ai"
        },
        {
            "paper_title": "ExCAR: Event graph knowledge enhanced explainable causal reasoning",
            "rating": 1,
            "sanitized_title": "excar_event_graph_knowledge_enhanced_explainable_causal_reasoning"
        }
    ],
    "cost": 0.02609375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning</p>
<p>Fuseini Mumuni fmumuni@umat.edu.gh 
Fuseini Mumuni
University of Mines and Technology
UMaT
TarkwaGhana</p>
<p>Alhassan Mumuni alhassan.mumuni@cctu.edu.gh 
Alhassan Mumuni: Cape Coast Technical University
P. O. Box DL 50Cape CoastGhana</p>
<p>Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning
671EB931C19E1CC2F44EAEB0BA5CA095Domain knowledgecognitive architecturebrain-inspired neural networkexplainable AIadversarial attackzero-shot generalization
We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-shot learning.Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications.Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications.In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions.Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects.Furthermore, models are usually trained on standard datasets with a closed-world assumption.Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero-or few-shot generalization problem.Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems.Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance.Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots.Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neurosciencethat is, to deepen human understanding on how the brain works in general, and how it handles these problems.</p>
<p>by several researchers [8], [9], [10]) have exceeded human performance in important tasks like clinical diagnosis.Deep learning has also been shown to be capable of outperforming human experts in product design and manufacturing.For instance, Mirhoseini et al. [11] show how DL improves accuracy and efficiency over human chip designers.In arts, deep learning-based AI models have demonstrated performance on par with humans [12]) in creative writing tasks like poetry composition.Also, AI manipulated video and audio content, known as Deepfakes, can look so authentic as to trick humans (see [13]) into thinking that they are real.</p>
<p>Need for knowledge priors and cognitive insights in deep learning</p>
<p>Presently, deep learning methods achieve superior performances than other machine learning approaches.To achieve this, they heavily rely on very large volumes of training data.However, in many practical situations, it is difficult to obtain sufficient training data.Therefore, data insufficiency poses severe limitations to deep learning systems, resulting in significant performance bottlenecks.Inherent noise, irrelevant features and outliers, which inevitably characterizes most large-scale training datasets, may also degrade performance.Consequently, knowledge-driven and braininspired approaches have been proposed to alleviate the over-reliance on training data.</p>
<p>• •</p>
<p>In contrast to raw training data that captures no underlying context, prior knowledge and cognitive insights can provide information about real-world relationships or utilize how the mind works to help in connecting the dots between input data and model decisions during inference or training of deep learning models.Besides alleviating the data insufficiency problem-this also helps in mitigating common problems that deep learning systems face in practical applications-adversarial attacks, explainability, generalization to unknown classes and data distributions (i.e., zero-shot learning), and learning with few examples (few-shot learning).</p>
<p>Knowledge representation</p>
<p>In many application domains, prior knowledge is readily available and can be harnessed to augment training data or the training process in deep learning.To achieve this goal, domain knowledge needs to be formalized and structured in a manner that facilitates its integration into deep learning models.Knowledge representation is a principled way of organizing prior knowledge for use by deep learning systems.</p>
<p>Mathematical equations: The common ways of representing knowledge include the use of explicit mathematical relations-mostly algebraic or differential equations-to describe real-world systems.These relations are usually employed to constrain the underlying model to conform to some governing mathematical laws.Such an approach can also be used to create surrogate models of complex systems for which data may not be readily available (e.g., [14], [15]).Mathematical knowledge can also be used to perform data argumentation or in simulation environments to create entirely new datasets (e.g., [16] for training machine learning models.</p>
<p>Knowledge graphs: Knowledge graphs (KGs) [17], [18] constitute another important class of techniques for knowledge representation in deep learning models.A knowledge graph organizes domain knowledge into structured relationships using nodes to represent physical and abstract entities, interconnecting edges to encode semantic relationships, and labels to define properties of the underlying entities.Besides this graphical form, knowledge graphs can also be represented by textual triples of the form (subject, predicate, object).This representation provides a succinct way of encoding simple relationships, e.g., (Argentina, WinnerOf, WorldCup) Ontologies are specialized knowledge graphs which are popular in representing biomedical data.Knowledge graphs have recently achieved promising results in many fields of artificial intelligence.In natural language processing (NLP), they have been successfully applied to improve the performance of recommender systems [19], question answering (QA) [20], [21]), language modelling (e.g., [22], and many more.</p>
<p>Symbolic logic: Symbolic logic is a popular approach that aims to encode knowledge by using logical representation of simple propositions and their relationships.It exploits symbolic representation of plain sentences to express relationships.Logic representation provides a framework for logical inference that allows logical outcomes to be inferred from given statements using appropriate semantics.</p>
<p>Two forms of logic representations are commonly employed in artificial intelligence and deep learning: propositional and first-order logic.</p>
<p>The incorporation of logical reasoning in deep learning is exemplified by Riegel et al. [23] in the logical neural network (LNN).Each neuron in the LNN is associated with a logical (first-order or propositional) statement, where the network activation functions are usually constrained to perform the specified logical operations on their inputs.Also, since there is a 1-to-1 mapping of neurons to logical units, each connection has a semantic meaning, thus, increasing the interpretability of the network.</p>
<p>Probabilistic Relationships: Probabilistic relationships are used to capture uncertainty in knowledge.In fact, knowledge in many real-world settings is characterized by incomplete information, often obtained from partial observation of complex systems.Other systems are affected by factors that cannot be determined a priori, or are liable to changes over time.Machine learning models therefore leverage knowledge represented in probabilistic forms to improve performance in these situations.For instance, in their Probabilistic faster R-CNN, Yi et al. [24] incorporate a probabilistic region proposal network to stochastically predict the objectness of candidate windows in a Faster R-CNN [25] framework used for object detection from remote imagery.The probabilistic model used is Bayesian inference, which assigns confidence scores describing the uncertainty associated with each region proposal, allowing relevant more useful regions to be selected based on their quality, rather than the fixed threshold selection approach that characterizes conventional Faster R-CNN method.</p>
<p>Implicit knowledge from pre-trained foundation models: Foundation models are very large-scale neural networks which are pre-trained on vast volumes of multimedia data (e.g., text, image or text-image pairs) readily available in various sources, mostly on the web.They are usually fine-tuned for the intended applications.Owing to their immense size and the large volumes of data they are trained on, foundation models implicitly capture a substantial amount of world knowledge which can be exploited in various downstream tasks like question answering and open-domain object detection.Examples of foundation models include BERT [26], PaLM [27], GPT-3 [28] and InstructGPT [29], the latter two being the key frameworks behind the influential conversational agent ChatGPT.These models excel in zeroshot and few-shot generalization tasks, had have also recently achieved impressive performance on brain stimulus decoding in a zero-and few-shot generalization settings.</p>
<p>Brain-inspired Techniques</p>
<p>Brain-inspired techniques of improving artificial intelligence usually focus on designing systems that leverage the power of the mind to solve complex problems.The two main approaches to realizing this goal are through cognitive architectures and brain-inspired deep neural networks.Braininspired cognitive architectures (BICAs) are popular in autonomous robot applications, where the goal is to attain general intelligence.Brain-inspired neural network may, in fact, be part of a BICA framework that would additionally include other subsystems to facilitate tasks like reasoning and planning.</p>
<p>External environment</p>
<p>Brain-inspired cognitive architectures (BICAs): BI-CAs describe both the nature of the mind, and equivalent computational models that facilitate the design of artificial intelligence systems that mimic biological cognition.The goal is to achieve in AI systems human-like characteristics such as emotions, memory, reasoning, life-long learning, adaptivity, and general problem-solving ability.Over the years, several BICAs have been proposed.Some of the successful ones in widespread use include Soar [30], Sigma [31], ACT-R [32], [33].The most important components of a typical BICA include different kinds of working and longterm memory, environment perception and motor or control modules.Figure 1 shows a simplified block diagram of a typical BICA framework.This generic model represents the mind not as a single complex unit, but as a group of distinct parts with specific roles that all work in synergy to achieve desired goals.The most important components of a typical BICA include different kinds of working and long-term memories, environment perception and motor or control modules.Brain-inspired deep neural networks: Despite their initial biological motivation, current state-of-the-art DNNs at best, represent a rather oversimplified abstraction of real neural networks.For instance, the more biologically realistic 'integrate-and-fire' neuron activity is only approximated by spiking neural networks [34].Still, deep convolutional neural networks (DCNNs) are a class of artificial neural networks that have a fair level of similarity in organization to the human visual cortex.From this perspective, convolution operations may play the role of simple cells while pooling layers perform the action of spatially invariant complex cells [35].Like their biological counterparts, DCNNs are adept at vision tasks.It is well known that the immense power of the human visual cortex is partly owed to its intricate organization that includes lateral and feedback or recurrent connections.However, traditional DCNNs main-tain exclusively feedforward connections.Unsurprisingly, architectural changes to accommodate more cortex-style interconnections in DCNNs have yielded promising results [36], [37], [38], [39], [40].</p>
<p>Other brain-inspired approaches of improving DNNs' adversarial robustness and zero-shot or few-shot generalization are being actively investigated.Some of the key approaches are discussed in Section 4.1.2.They include 1) modification of the backpropagation training scheme into a brain-informed predictive coding scheme; 2) improving the biological plausibility of the neuron model; 3) ensuring biological realism of the overall network architecture.</p>
<p>Despite the numerous studies confirming the immense potential of incorporating brain-inspired enhancements in artificial neural networks, works on this front still lag far behind conventional deep learning methods both in terms of volume and overall accuracy.This situation can be explained by the fact that available knowledge on the human brain is still far from complete.Fortunately, advancements in brain-inspired DNN techniques have now widely been explored in a bid to explain the structure and function of the brain.Using brain-inspired DNNs as surrogates allow non-invasive and easily adaptable simulations to be performed in order for neuroscientists to gain deeper insights into the organization and innerworkings of the brain.Such frameworks may be designed to simulate single biological neurons (e.g., [41], neuronal circuits [42], and whole brain regions (e.g., [43].The insights learnt from these simulations could in-turn, help to achieve better biological realism in artificial neural networks.In CORnet-S [44] leverage neuroscience and deep learning principles to design a braininspired convolutional neural network anatomically aligned with real brain regions and recurrent connectivity.This model achieves high performance on image classification datasets and at the same time provides further insights into the cognitive processes of the brain.</p>
<p>Motivation and Outline of work</p>
<p>When faced with unexpected situations such as adversarial attacks, data-driven deep learning models produce woefully poor results that can potentially lead to dire consequences.Additionally, deep learning systems struggle to cope in situations where certain categories are few or absent in the training data.To handle this problem, machine learning frameworks must be endowed with zero-or few-shot learning capability.Moreover, to ensure understanding and promote trust, especially in human-robot interactions, deep learning systems must be able to explain their decisions to human stakeholders.Based on recent promising research in the areas of knowledge-informed machine learning and brain-inspired cognitive systems, we believe that today's deep learning models can benefit immensely from widely available world knowledge and by incorporating useful concepts from biological systems to better solve the aforementioned problems.Finally, just as knowledge of the brain's structure and mechanisms underlying its function are critical in improving deep learning, it is also vital to be able to use insights from deep learning to improve understanding of cognitive neuroscience and psychology.Figure 3. llustration of adversarial attack: After corrupting the images on the left with adversarial noise (middle) at the pixel level, the resulting images (right) look unchanged from the perspective of a human observer, but the deep learning classifier wrongly classifies both images (i.e., on the right-hand side) as ostrich [45].</p>
<p>are already beginning to illuminate cognitive science, providing vital insights needed to better comprehend and apply new techniques to develop useful interventions in areas like psychology, medicine and biology.Furthermore, pretrained cross-modal vision-language networks have yielded promising results in brain stimuli decoding tests.Motivated by these prospects, we present the state-of-the-art research work in these directions, highlighting the significance of the results as well as the key innovations required to attain those achievements.Different from all previous surveys, this paper is particularly focused on knowledge-and cognitivedriven approaches that aim to mitigate problems of adversarial attacks, explainability and zero-or few-shot learning in deep learning frameworks.We also discuss approaches that leverage these developments to provide insights for better understanding of cognitive science.</p>
<p>The rest of this paper is organized as follows: Section 2 provides the background to the aforementioned issues.Section 3 presents methods of improving adversarial robustness, explainability and zero-shot learning using prior knowledge.In Section 4 we present methods for improving deep learning that rely on insights from cognitive science, and particularly outline the importance of cognitive architectures and brain-inspired DNNs.In addition, we describe some of the important challenges limiting the application of these techniques, and identify ways of overcoming Decoder ??Input Encoder ??</p>
<p>IT</p>
<p>V2 V4 V1</p>
<p>these challenges.We present brain decoding techniques that leverage prior knowledge from pre-trained vision-language models in Section 5. We discuss salient issues and future outlook in Section 6 and conclude in Section 7. The general outline of the paper is shown in Figure 4.</p>
<p>BACKGROUND</p>
<p>Adversarial Attacks</p>
<p>Deep learning is undoubtedly very powerful in many usecases.However, achieving human-level versatility and reliability in a broad range of tasks still remains a distant prospect.Deep learning systems can sometimes make bizarre predictions (e.g., [46] when they encounter unknown situations, resulting in catastrophic outcomes.More worryingly, state-of-the-art deep learning models can often be tricked by adversarial attacks, causing them to make utterly wrong predictions by modifying their inputs in subtle and seemingly harmless ways.This weakness can be exploited by human actors with malicious intents to mislead DL systems by corrupting their sensory inputs or by directly interfering with the prediction process itself.Adversarial attacks were first reported by Szegedy et al. [45] in AlexNet on the ImageNet dataset.By adding minor artificial noise to test samples from ImageNet, they observed that the AlexNet classifier would make wildly incorrect predictions, such as misclassifying the truck shown in Figure 3 as an ostrich (and even doing so with a high probability score).Since then the phenomenon has been extensively studied in computer vision (e.g., [47], [48], [49], [50]) and natural language processing (e.g., [51], [52], [53]) domains.</p>
<p>Conventional defenses and limitations: Popular defenses against adversarial attacks include 1) adversarial training (e.g., [54], [55]), where adversarial examples are used during training; 2) data augmentation (e.g., [56]), which attempts to create pseudo-adversarial samples for training; and 3) certified defenses (e.g., [57], [58]), which provide guaranteed robustness to bounded adversarial noise.However, despite their popularity, conventional data-driven strategies designed to defend against adversarial attacks lack the reasoning ability to detect illogical predictions, and hence, have limited applicability in most practical use cases.For example, approaches based on adversarial training [59], [60]) are simple to implement but they require large amounts of adversarial examples.Moreover, the training process can induce unintended bias which may harm performance (as per recent studies, e.g., [61]) on clean samples.Furthermore, such defenses are only effective against adversaries that they have been specifically trained to overcome, and even with that they have been shown to be vulnerable to counter-attacks [62], [63]).Conventional data augmentation may experience the robust overfitting [61]) problem, which leads to a trade-off between training and test-time accuracies.In achieving guaranteed robustness, certified defenses sacrifice computational time and standard accuracy on clean inputs, or are restricted to model-specific designs.To alleviate these problems, adversarial defenses can leverage prior knowledge to achieve higher robustness.Techniques for realizing this goal are covered in Section 3.1.</p>
<p>Explainable artificial intelligence</p>
<p>In practical applications, deep learning systems are typically employed to autonomously make decisions and take appropriate actions, or to aid practitioners to arrive at desired decisions.To achieve state-of-the-art performance, most of these approaches use highly sophisticated black-box models whose decisions cannot be understood by human experts.Despite their enormous capabilities and their widespread practical applications in many important domains, the inability to provide explanations on the rationale of their decisions is a major concern for their use in some highstake applications.As a consequence, practitioners and other stakeholders have questioned the suitability of these systems for safety-critical applications like autonomous driving, medical diagnosis and treatment recommendation, where the stakes may be too high to trust opaque decisions.To meet this need, explainable Artificial Intelligence (XAI) techniques have been proposed to provide humaninterpretable explanations of the decisions of deep learning systems.Explainability and interpretability are terms that are often used synonymously.More technically, interpretabilty is associated with models which highlight specific features as contributing to the decision, or whose decisions can readily be understood.On the other hand, explainability applies to models which provide explicit descriptions of the source of their decisions.</p>
<p>Understanding the decision process of DL models creates the trust needed to fully rely on these models to perform critical functions.Even in situations where artificial agents work in tandem with human experts and the models' predictions are only meant to assist humans arrive at final conclusions, it is vital to understand why certain decisions are made, as well as understand the limitations and biases of the machines' decision-making process.This understanding enables humans to be able to put the machines' decisions in context, and also determine which predictions of the machine to accept or reject, and which outputs to probe further.Thus, model explainability or interpretability is essential to collaborative decision-making (see [65], [66]) by experts and intelligent agents.Furthermore, when deep learning models make wrong decisions that lead to adverse outcomes, explainability can help in determining who to apportion blame (see [67], [68]) or settle legal [69], [70]).Hence, the demand for explainability is growing strongly in recent times [71].As shown in Figure 5, deep learning methods achieve remarkable accuracies but they require extremely large and complex models that rely on large-scale training data for their impressive performances.Owing to their immense complexity, as illustrated by Yang et al. [64] (see Figure 5), these high performing deep learning systems are characteristically opaque.In contrast, simpler and less accurate frameworks like decision tress, linear models, and simple knowledge-based methods like Markov Logic Networks, Bayesian networks and logic rules are, by their nature, more transparent and inherently interpretable.So, ideally, it would be better to enhance high-performing deep learning models with explainability, rather than adopting low-accuracy white-box models.</p>
<p>Conventional approaches to explainable AI: Feature attribution methods are among the most popular inter- pretability techniques in use.They rely on mapping specific features from the input data to model decisions using a feature-importance score usually computed at the pixel or patch level.Popular among feature attribution techniques include saliency visualization methods based on gradient computations -e.g., Grad-CAM [72]; Integrated Gradients (IG) [73] -and perturbation-based approaches such as locally interpretable model-agnostic explanations (LIME) [74]) and Shapley values (SHAP [75]) which rely on perturbing the input and examining the corresponding effect on model decisions.Attribution methods are utilized in diverse machine learning applications.For instance, in image classification (e.g., [76]), it is used to the highlighted (in the form of heatmaps) regions of the input image that are believed to be responsible for a positive prediction.In natural language processing (NLP) it is widely utilized in tasks like question answering (e.g., [77]) and visual question answering (VQA) (e.g., [72], [78]), the resultant heatmaps usually represent couples of words deemed relevant to a model's answer or image regions that most strongly influence a model's answer.Unfortunately, the quality of the resulting explanations is limited.Specifically, these methods cannot provide the required logic nor the expressivity needed to produce easily-comprehensible explanations from a human perspective.Furthermore, relevant features can be degraded by noise [79], leading to impaired explainability performance.Another major limitation of this category of techniques is that it only highlights regions the model considers important to the decision but the actual reason of the decision is not provided.Thus, attribution methods are unable to provide any meaningful connection between real-world contexts and the corresponding predictions.Multiple studies (see [80], [81], [82]) have also outlined other limitations of these methods.For instance, Figure 6  This flaw, known as the Clever Hans problem [84] is very common in image classification.However, as explained in Section 3.2, knowledge-informed interpretability methods like logical rules or human-in-the-loop techniques overcome these kinds of problems.Some interpretability methods [85], [86], [87]), called surrogate models, rely on training an inherently interpretable model as a surrogate to provide similar (but explainable) predictions as the original.However, this approach has clear limitations.First, performances of large and complex models designed for high performance cannot be replicated by simple interpretable models (models must be simple enough to facilitate interpretability) since the latter cannot achieve satisfactory performance on complex tasks.Prototypical Part Networks (ProtoPNets) [88], [89] are another popular group of frameworks that compose explainable decisions from composite parts.They are usually convolutional networks trained end-to-end on images to make predictions and simultaneously learn part-based prototypes associated with the object categories.Similarities with learned prototypes are then used to interpret model decisions at inference time.Given the superior performance of vision transformers in image classification and object detection, recent design [90], [91]) propose a transformerbased prototypical network, or ProtoPFormer, for interpretable image classification.The main problem faced with transformer frameworks is that, due to their tendency to encode long-term dependencies, transformers ultimately learn irrelevant background prototypes.ProtoPFormer introduces a parallel global prototype branch in the network to guide a local branch to focus on the foreground features.Generally, prototype-based explainability methods are very promising as they can identify high-level features that inform model decisions.Furthermore, unlike post hoc methods such as surrogate models, prototypical methods are inherently interpretable and do not rely on external interpreters for explanations.</p>
<p>Background Introducti
Conclusio Discussio
Inherent limitations of these conventional approaches call for alternative methods and the incorporation of prior  knowledge is one of the most promising options available.</p>
<p>For instance, surrogate models may not adequately reach the performances of state-of-the-art deep learning models if they maintain acceptable complexities that allow high degree of explainability.Saliency maps may highlight image regions that strongly influences a model's classification decision but such a map could wrongly focus on an irrelevant feature that is usually present around the relevant feature.Methods based on prototypical parts may still require additional logic to associate the relevant information for human understanding.Even the decision logic of inherently explainable methods is only obvious to people with fair knowledge of the models and hence their explanations of such models may not be relevant to end-users whose expertise is in other areas like medicine or law and justice.Ultimately, leveraging domain or expert knowledge to enhance explainability of deep learning models is often necessary to achieve desired results.Consequently, a wide variety of knowledge-driven approaches have been proposed to improve explanations of deep learning models by incorporating prior knowledge to meet human-centric explainability objectives.</p>
<p>Zero-shot and few-shot learning</p>
<p>Zero-shot Learning (ZSL) is an approach that aims at equipping machine learning models with the ability to generalize from observed classes during training to unseen targets at inference time.Zero-shot Learning can be considered in two ways: (1) The classic concept of ZSL, which assumes different distributions of source and target classes (that is, in deployment, the classifier is required to deal with objects from entirely different categories from those encountered during training); and (2) the more practical setting-called generalized ZSL-which considers samples from both source and target classes at test time (that is, the classifier will handle objects from both seen and unseen classes Overview of conventional approaches: Here, conventional approaches are understood as methods that do not leverage world knowledge.Zero-shot learning methods usually rely on common attributes between observed and unseen classes to facilitate generalization from the former to the latter.In image classification, some traditional approaches involve learning the desired compatibility function that maps between visual appearance and semantic attributes.To accomplish this task, these methods either map from visual to semantic (e.g., [92], [93], [94]) or from semantic to visual (e.g., [95], [96]) embedding space.Mapping from visual to semantic space incurs the so-called hubness problem [95]) -which causes the learner to be biased towards a small subset of classes -while the opposite transformation (i.e., semantic to visual) is difficult since one class attribute or description can possibly match the visual appearance of different images.Overall, this line of approaches has a severe performance limitation, especially in the generalized zero-shot learning setting, where there is a significant bias towards observed classes.</p>
<p>Subsequent works [97], [98] have been proposed to address this shortcoming by leveraging generative adversarial networks (GANs) to synthesize samples of unseen categories conditioned on the distribution of class attributes.These methods create synthetic images from scratch through data augmentation techniques.However, the generated images may lack sufficient details to adequately represent the target features, resulting in degraded performance [99] and may suffer from mode collapse [100].Although other families of approaches exist, prominently those that leverage the complementary benefits of GANs and or variational autoencoders (VAEs) methods (e.g., [101], [102]), the performance difference between generic machine learning and PORTABLE UPRIGHT zero-or few-shot learning is still wide.Moreover, most of these studies have been carried out on standard datasets.Thus, the models would inevitably experience further performance drop when deployed in practical autonomous robotic applications.</p>
<p>USING KNOWLEDGE TO IMPROVE ADVERSARIAL ROBUSTNESS, EXPLAINABILITY AND ZSL</p>
<p>Using prior knowledge to overcome Adversarial Attacks</p>
<p>Knowledge-based approaches to adversarial defenses can perform consistency checks (e.g., [103]) on models' decisions to ensure conformity with existing domain knowledge.Results that violate logical reasoning or other forms of knowledge are thus corrected or discarded.Some of the most effective approaches are based on object co-occurrence relationships, logic rules, and compositional part-based reasoning.</p>
<p>Co-occurrence relationships: Contextual knowledge in the form of category co-occurrence relationships (e.g., [104])-Figure 7has-been shown to be a highly effective way of detecting and preventing adversarial attacks.This method is based on the premise that certain classes of objects (e.g., an ant and an elephant or a bus and a toothbrush) are highly unlikely to be seen together in a given scene.This conflict may simply be due to mismatch scale or semantic context.With the approach, the decision is probed whenever a co-occurrence constraint is violated.Although counterattacks against this kind of defense have been proven (e.g., [105]) to be successful, the attackers require the data distribution of the target model to be known.Also, the attacker must know the concept that defines co-occurrence context from the victim's perspective.These requirements are however, difficult to meet in practice.Hence, co-occurrence relationships can offer potent defense in many real-world situations.To further improve robustness, knowledge-informed data augmentation (e.g., [106]) that employ co-occurence logic have been utilize for creating artificial categorical collisions which a network then learns to separate in order to resolve ambiguities.</p>
<p>Logic rules: Recently, the use of domain knowledge represented by first order logic rules has shown a significant promise in identifying incoherent, adversariallyinfluenced predictions in image classification tasks.For instance, Melacci et al. [107] exploit this kind of domain knowledge to enforce logical constraints on training data, encouraging the data to assume the appropriate marginal distribution.During inference, spurious predictions that do not align with training distribution are discarded.Although the method is flexible and can be applied to a wide range of problems, the logic rules needed to capture domain-specific properties must be formulated with expert knowledge.This requirement limits the application of the technique to situations where such expertise is available.To overcome this challenge, Ciravegna et al. [108] propose logic explained networks (LENs) which can directly learn first order logic rules from input data -provided in the form of humaninterpretable predicates -to capture object relationships in the target domain.LENs also facilitates interpretability by exploiting first-order logic rules in the output space.</p>
<p>In LOGICDEF (Figure 8), Yang et al. [109] are inspired by the fact that the human visual system easily detects incoherent scenes by mining contextual cues from a powerful scene representation that organizes its elements in a structured hierarchy of objects and their relationships.From a scene graph, LOGICDEF extracts logic rules about objects and their relationships and combines this information with commonsense knowledge derived from Con-ceptNet [110]-a large commonsense knowledge graph of frequently-used words and phrases collected from diverse sources-to enforce contextual consistency constraints over the scene elements.Besides being able to detect visually implausible predictions, LOGICDEF is also interpretable.</p>
<p>Logical reasoning techniques (e.g., [111], [112]) have also shown promise in facilitating certifiable adversarial defenses.For instance, Zhang et al. [112] utilize a graph convolutional neural network (GCN) for learning semantic features while probabilistic graphical logic framework in the form of Markov logic network (MLN) is used to enhance reasoning over plausible outputs.Similarly, Yang et al. [111] ncorporate MLN to perform reasoning in a deep learning setting.Although still in the early stages, these studies represent an interesting direction of research on certified robustness.</p>
<p>Compositional part-based reasoning: Finally, an emerging line of research in adversarial robustness is motivated by the part-based reasoning [113], [114], [115]) paradigm employed by biological vision.Human vision is extremely powerful.It has been developed through years of evolutionary adaptation.It is argued that the visual system prefers to recognize objects based on a representation that partitions them into distinguishable parts to allow easy identification using information on the relationships of these components.For this reason, biological vision is extremely robust and is difficult to be fooled by most practical adversarial attacks that modify only part of the target [116].While there is substantial evidence (e.g., [113], [114], [115]) supporting the aforementioned view that much of the remarkable robustness of the human visual system owes its strength to the compositional or part-based inference paradigm, deep learning methods aiming to circumvent adversarial attacks rarely utilize these robust representations.However, recent studies have demonstrated that part-based reasoning can achieve high generalizability generalization to varied and unknown adversaries (including adaptive attacks), unlike conventional methods which are typically designed to handle specific attacks.In their ROCK framework, Li et al. [117] propose an image classification model based on "Recognizing Object by Components with human prior Knowledge".The method first divides objects into distinct parts, then use this information to recognize whole objects based on part relationships that are predefined through human prior knowledge.ROCK uses a judgement block to produce the final prediction by first computing independent part-based prediction scores and then evaluating the part-linkage according to commonsense knowledge rules.Empirical results presented by the authors show that the approach is effective against different kinds of adversarial attacks and offers significantly better robustness than conventional defenses.Furthermore, Rock does not compromise clean accuracy on benign images. .Using co-occurrence logic (e.g., [104]), it is easy to detect the inconsistency of a bus and a toothbrush co-existing in a scene.A more logical combination would be a bus and a traffic sign.The implausible results are discounted as adversarial.</p>
<p>Figure 8. Left: The adversarial patch printed on the train causes a deep learning classifier to misclassify it as a cat.But, using logical context (Right), LOGICDEF [109] correctly discovers that the object must be a train</p>
<p>Knowledge-informed explainability methods</p>
<p>High-level concepts and human-in-the-Loop: Knowledge in the form of human-defined concepts have shown promising results in enhancing explainability in computer vision applications.Concepts (see Concept Bottlenecks [118] and Concept Activation Vectors [119] for typical examples) in this context refer to high-level semantic information such as the stripes of a zebra, the distinctive yellow beak that identifies a parrot, or the narrowing of the space in the knee joint that characterizes arthritis disease.This class of explainability frameworks [118], [120], [121], [122], [123] are popularly referred to as Concept Bottleneck Models (CBMs).CBMs operate by first predicting labels for human-understandable high-level concepts from the input data, and then using the information on predicted concepts to make the final prediction about the category of the image.Usually, CBMs require expert annotation of relevant concepts for training.This makes the approach labor-intensive and expensive to implement.</p>
<p>Human-in-the-hoop explainability methods leverage knowledge obtained directly from a human participant in the training or inference phase of deep learning systems.The premise here is that, the accuracy of a predictions is contingent on the corresponding explanation (i.e., reason for that decision) being correct.Therefore, by observing accompanying explanations, a human participant can intervene Adversarial Benign to correct explanations that do not match the particular example, causing the prediction itself to change.For instance, by allowing the user to correct mistakes over concept predictions, [118] (see Figure 9) achieves significant accuracy improvements at inference-time.Following a similar line of thought, other works (e.g., [124], [125]) achieve improved accuracy by dynamically correcting wrong explanations.</p>
<p>While human-in-the-loop approaches that utilize highlevel concepts represent an interesting research direction, researchers have pointed out their inherent limitations.For instance, [126] note that some image features other than the predicted concept may be responsible for a given prediction.Also, where there are high correlations between concepts, a particular concept may be implicated for a given prediction whereas in fact, the decision is caused by a correlated concept.Another important concern of this approach is that it requires a great deal of manual work to provide concept labels, and with large volumes of data it can be excessively laborious.Owing to this limitation, it is impractical to apply concept bottleneck modules on large-scale datasets containing millions of samples.However, effective techniques of mitigating this problem have yielded promising results.In ACE, Ghorbani et al. [127] propose to solve the problem by automating the annotation process.They achieve this goal by aggregating local patches in the input data into coherent and meaningful concepts with relevance to the network's prediction.Oikarinen et al. [122]) propose Labelfree CBM that obviates the need for concept annotations for training.The authors accomplish this feat in four steps.First, GPT-3 [28] is employed to automatically generate the initial concepts which are then filtered to remove undesirable ones.Second, the concept activation matrix on the training dataset the training data is computed.The third step learns a projection needed to create a concept bottleneck layer.The last step involves learning the weights of the final network layer to make predictions.Some Concept Bottleneck Models incorporate two tiers of human knowledge in their pipelines -human-defined concepts used in training; and direct human involvement in rectifying faulty concept predictions during inference.Another popular class of human-in-the-loop interpretability methods [128], [129], [130], [131]), known as eXplainatory Interractive Learning (XIL), employs human supervision to manually edit the heatmaps generated by conventional attribution methods like LIME, CAM Grad-CAM.Although Active Learning (AL) [132] also leverages human in the machine learning loop to improve performance, the fundamental difference is that XIL particularly focuses on achieving this goal by manipulating explanations.The intuition is that, by indicating areas in the saliency maps that are irrelevant to the given prediction, or important regions that have been missed by the attribution method, the network can learn to ignore spurious correlations in the data and produce more representative visualization maps.Usually, the expert-rectified heatmaps are presented to the model as additional annotated data in the spirit of data augmentation.Alternatively, the information from expert feedback may be incorporated into the XIL model through regularization with additional loss terms that seek to penalize deviations of the model's computed heatmaps from the human annotated ones.This line of works is also popular in other domains such as NLP [133] and multimodal multimodal visionlanguage frameworks [134].</p>
<p>Knowledge graphs: Knowledge graphs carry information in the form of entities, relations, and governing rules.Knowledge represented by this powerful data structure has proven effective when incorporated in machine learning frameworks to help generate explanations for model decisions.With knowledge graphs, explanations are easy to extract [135], [136] because of the rich semantic relations inherent in the representation.This kind of explainaibility is especially popular in applications such as product recommender systems (e.g., [19], [137], [138]) drug recommendations [139], [140]) and disease diagnosis [141], [142]).</p>
<p>In medical applications, especially in situations where data is limited, domain knowledge in the form of medical ontologies represented by knowledge graphs enhance both accuracy and explanainability of decisions.For instance, GRAM [143] incorporates an attention mechanism and ontological data to learn medical concepts in a framework that predicts and explains future onset of diseases from patients' medical history.In GRAM, ontological knowledge is used to learn the representations of medical codes, but this knowledge is not propagated directly to generate clinical visit embeddings and to contribute to final decision making.Yet, GRAM achieves significant improvement over the baseline in the insufficient data case, but does not take advantage of more data when available.Subsequent improvements (e.g., [144] leverage ontological knowledge in each step of the pipeline, and are, thus, able to exploit as much knowledge as provided by the data so as to boost performance.</p>
<p>Logic rules: As mentioned, some interpretability methods, notably conventional approaches that do not leverage domain knowledge, produce explanations that may be based on features irrelevant to the prediction, or may not be informative enough to capture the underlying rationale in a readily understandable form.Knowledge-driven solutions based on logical reasoning over symbolic rules help overcome both of these limitations.By enforcing compliance of explanations with prior knowledge about the tasks, rulebased methods [108], [145] ensure that only sound associations between predictions and explanations are allowed.Also, logic rules are unambiguous and are easy to understand by humans.Their strengths have therefore motivated state-of-the-art interpretability methods for deep learning.</p>
<p>From high-level concepts in the form of predicates, LENs [108] learns to provide easy-to-understand explanations using first-order logic statements.LENs has been proposed in two variants; as a post hoc module providing explanations for black-box frameworks, and as inherent explainability model trained end-to-end.One major limitation of approaches based on logical rules is the requirement of the inputs to be symbolic predicates, a requirement which is difficult to meet in application domains that cannot utilize text-based data.</p>
<p>Knowledge-informed zero-shot learning approaches</p>
<p>High-level concepts and logic rules: When presented with given object categories and explanations on their visual appearances, humans can seamlessly extend this knowledge Figure 9.By manually editing concept predictions, overall accuracy can be improved [118].</p>
<p>to unseen categories if additional descriptions of semantic relationships to the known classes are provided.For example, by describing a tiger as a big cat with black and yellow strips, it is easy for a reasoning agent familiar with the domestic cat to recognize a tiger when it is encountered for the first time.Many knowledge-informed zeroshot learning methods (e.g., [146] have exploited this idea to define visual concepts from text descriptions, and have achieved impressive results by reasoning over plausible concepts.The method is powerful as it can exploit the compositionality of objects with respect to known concepts, and logical rules can then be used to combine these concepts to match object descriptions of seen and unseen classes.For instance, [147] employ Logic Tensor Networks (LTNs) to achieve impressive performance in generalized zero-shot learning by adapting the relationship representations of concepts to distance measures, where closer relations are represented by shorter distances.Similarly, CLORE [146] extracts logical rules from textual explanations on images to capture high-level image attributes or concepts.This representation facilitates zero-shot image classification through logical reasoning over plausible concepts.</p>
<p>Establishing the relation between entities without explicit examples in the training set is a challenging task.Yet, Li [148] show that logic rules can be used to establish the connection between unseen relations and some observed relations from a knowledge graph.The rules are extracted directly from the knowledge graph embedding, followed by rule-guided learning to extract connections observed and new relations.Rule learning over knowledge graph framework avoids the need for relation descriptions to understand the connection between seen and unseen cases.Successful implementation of part-based reasoning requires a careful choice of parts.For example, the parts used must not undergo sever or unrecognizable deformations.This could seriously affect performance.If necessary, it is possible to include some of the common transformations in the training phase, so that the model learns to make appropriate decisions and explanations during inference.</p>
<p>Knowledge graphs: Zero-shot knowledge graph completion methods are designed to recover unseen relations that emerge at inference time without any association to existing triples encountered during training.Some works [151], [152] try to predict new entities inductively by aggregating their neighbors.Although LAN [152] improves this general philosophy of enhancing neighborhood aggregation with attention mechanism that utilizes logic rules to estimate attention weights, the approach still does not generalize to new target knowledge graph embeddings.Therefore, to handle different entity types that might emerge with new knowledge graphs, some more recent studies (e.g., [153]) propose to transfer entity-independent knowledge from seen to target domains through meta-learning.Another approach relies on utilizing generative adversarial networks (GANs) to infer valid relation embeddings from text descriptions (e.g., [154], [155]) or ontologies [156], [157].Generally, zero-and fewshot learning using knowledge 1.12 ⋯1.69 ⋯ 0.31 graphs has made significant progress.It can be utilized effectively by models to make predictions over data that is not directly encoded in the graph.However, since most knowledge graphs usually capture knowledge in a specific domain, extending these frameworks to open-world generalization tasks is challenging.For this reason, large-scale pre-trained models which encode a much wider scope of world knowledge are employed to solve domain-agnostic generalization problems.</p>
<p>Pre-trained foundation models and knowledge graphs: Another important area where knowledge-augmented deep learning has achieved tremendous performance is zero-shot and few-shot generalization leveraging implicit knowledge encoded in pre-trained foundation models.Since generalization of machine learning systems is usually limited by the quantity and representative quality of training data, researchers propose to leverage the enormous multimedia data (usually, text and images) to improve generalization.The data is mostly obtained from online knowledgebases like Wikipedia, Google search results and news platforms.Pre-trained foundation models are particularly useful in natural language processing, computer vision and joint language-vision tasks.They can broadly be categorized into three main groups according to their application domain:</p>
<p>1) Large Language Models (LLMs) are used in natural language processing domains for language modelling or text generation.Prominent examples of LLMs include the Bidirectional Encoder Representations from Transformers (BERT) [26], PaLM [27], Generative Pre-trained Transformer (GPT) series like GPT [158], GPT-3 [28] and InstructGPT [29].Typical applications LLMs include question answering (QA), a task which aims to train models on textual data to answer questions in natural language.Chatbots and conversational agents ChatGPT are examples of QA systems.2) Pre-trained vision models: these are commonly used in open-world image classification, object detection and image segmentation.Popular models in this category include Florence [159] DALL-E [160] and Segment Anything Model (SAM) [161].3) Multimodal Vision-Language frameworks -e.g., VL-BERT [162] and Contrastive Language-Image Pre-training (CLIP) [163] are focused on multimodal tasks such as visual question answering (VQA), where the goal is to train neural networks on images and text so that they can answer questions about visual scenes in natural language.</p>
<p>The general structure of a typical pre-trained foundation model, CLIP [163], is presented in Figure 10.It consists of three main processing stages.First, feature encoders are designed to extract textual and image features; second, a linear classifier creates a database of correct image-text pairs; finally, during inference, the model retrieves textual descriptions or caption that match given images.</p>
<p>Owing to the rich information contained in the massive training data, state-of-the-art foundation models implicitly acquire a vast amount of world knowledge and have shown incredible abilities to generalize to new tasks and data distributions.Studies have shown that, even out-of-the-box, pre-trained foundation models excel in few-shot and zeroshot generalization.For instance, Brown et al. [28] demonstrate that GPT-3 can perform diverse reasoning tasks when presented with a few in-context examples in the form of text prompts.Also, in the original study, Radford et al. [163] show through extensive comparison tests involving over 50 state-of-the-art models and 30 datasets that CLIP's zero-shot image retrieval performance is competitive even with supervised baselines.Similarly, with just a single prompt, [164] show impressive zero-shot reasoning capability of PaLM [27] and InstructGPT [29].Multiple studies have therefore leveraged this implicit knowledge as a stand-alone entity (e.g., [165] or to augment explicit knowledge represented in various forms, like knowledge graphs [20] to achieve zeroor few-shot generalization in downstream tasks.</p>
<p>The tasks that have mostly benefited from the zero-and few-shot generalization ability of pre-trained foundation models are question answering (QA) and visual question answering (VQA).The QA task aims to equip machines with the ability to automatically answer natural language quesions by extracting or generating appropriate text in response to questions.This capability is useful in applications requiring automatic response to human queries, such as customer support and therapy chatbots.VQA aims to equip machines to be able to provide answers in response to natural language questions on an image.VQA systems can aid visually impaired persons to understand the content of scenes from camera dada.</p>
<p>Commonsense QA and VQA tasks require external knowledge about the question or image content.However, despite empirical evidence showing impressive performance of foundation models in these domains, out-of-thebox, these models typically lack domain-specific knowledge or commonsense reasoning ability since they cannot capture facts in context.Moreover, the predominantly web-based multi-media corpora used to train foundation models do not contain adequate examples of some kinds of data while unduly overemphasizing others.Specifically, this information is subject to reporting bias, a situation where some rare events and facts are disproportionately represented at the expense of more common and relevant everyday facts.For example, compared to mundane activities like cleaning the kitchen sink, it is more likely for a school shooting incident to assume prominence online.Furthermore, some kinds of data are mostly accessible only in specialized domains.Yet, a deep learning model trained in an unsupervised manner to glean general knowledge requires sufficient number of examples on each of these cases to adequately capture relevant information required to achieve acceptable performance.Unsurprisingly, the developers of CLIP in their report [163], have bemoaned the model's uncharacteristically poor performance on "complex, or abstract tasks" like satellite image classification and tumor detection.However, it must be noted that the domain-specificity-rather than any inherent "complexity" or "abstractness" of these tasks-is to blame for this subpar performance.To address this limitation in downstream tasks, various techniques exploit commonsense or domain-specific knowledge encoded in knowledge graphs to augment pre-trained foundation models.The general workflow of this line of approaches is explained by Figure 11.2) and ( 4) then complement each other in the end task (5).</p>
<p>An important body of work seeks to augment pretrained foundation models with explicit knowledge from content-rich knowledge graphs like ConceptNet [110], WordNet [166] and ATOMIC [167].To capture contextual commonsense knowledge from these sources, these methods [168], [169] propose to fine-tune pretrained BERT [26] to commonsense knowledge graphs using the text phrases that represent graph nodes.Bosselut et al. [168] first develop a generative transformer model named COMmon-sEnse Transformer (COMET), and then use it for knowledge graph completion by re-training the pre-trained LLM on the existing set of knowledge tuples.The LLM learns the knowledge graph structure using the example tuples and then generates compatible tuples to complete the graph Malaviya et al. [169] transfer implicit knowledge from the pre-trained large language model using a masked language modeling loss to facilitate contextual knowledge graph completion.</p>
<p>The approach allows the vast amounts of implicit knowledge contained in powerful foundation models to be transferred to explicit representations in knowledge graphs.The method achieves improved results over BERT-only representations.However, in practical QA tasks, a pre-populated knowledge graph may not adequately capture the relevant context about a given question.To solve this commonsense zero-shot question answering problem, Bosselut et al. [168] employ the COMmonsEnse Transformer (COMET)  [168]-which is based on the generative pre-trained language model GPT [158]-to dynamically generate contextappropriate knowledge graph triples in response to each question posed.Presented with a question and a corresponding context as a root node, COMET generates intermediate inferences as child nodes that connect the given context to the answer choices which act as leaf nodes of the graph.Finally, probabilistic inference over the graph retrieves the most suitable answer.A common weakness of knowledge graph augmented learning is the limited coverage or scope of world knowledge encoded by these knowledge bases.This limitation might also lead to irrelevant knowledge being retrieved from a knowledge graph when the precise information is not captured by the graph.To improve coverage of commonsense facts across a variety of domains, KRISP [170], combines information from multiple knowledge graphs to augment pre-trained BERT for opendomain VQA.This design expands the range of application of the model beyond a single domain.
I1•TN … I1•T3 I1•T2 I1•T1 2 T1 T2 T3 … TN I1 I2 I3 ⋮ IN I1•T1 I1•T2 I1•T3 … I1•TN I2•T1 I2•T2 I2•T3 … I2•TN I3•T1 I3•T2 I3•T3 … I3•TN ⋮ ⋮ ⋮ ⋱ ⋮ IN•T1 IN•T2 IN•T3 … IN•TN
Other works try to overcome the problem of restricted scope or coverage of encoded knowledge -and hence, incidences of irrelevant knowledge retrieval -of publicly available knowledge graphs by leveraging pre-trained (LLMs such as) GPT-3 instead.Rather than utilizing explicit knowledge from structured knowledge graphs, some recent fewshot VQA methods such as PICa [165], KAT [171], Prophet [172] propose to leverage implicit and unstructured world knowledge from GPT-3 [28] for enhancing visual question answering.At test time, when required to answer a querie on an image-question pairs, PICa converts the images into corresponding captions.These captions and accompanying questions, along with a small sample of n-context VQA examples are then fed into the pre-trained language model as textual prompts to help retrieve relevant knowledge for making predictions.However, as GPT-3-only predictions may lack domain-specificity, KAT proposes to combine explicit knowledge from Wikidata [173] obtained through contrastive-learning, and implicit knowledge from GPT-3 to facilitate versatile/multipurpose but domain-appropriate predictions.</p>
<p>On the other hand, by noting that captions automatically generated from images may be limited in capturing all relevant scene information and semantic relationships required for reasoning-based VQA, Prophet [172] first learns relevant answer heuristics which are then included in the prompts to enhance the prediction task.With richer and more taskspecific information for answer prediction, Prophet surpasses existing state-of-the-art models in accuracy.</p>
<p>Foundation models are extremely large deep neural networks that take large amount of memory space and processing times.They typically require high-end systems to operate.Therefore, in practical applications, the demand on computational resources is a major limitation of models that directly utilize large-scale pre-trained models.Therefore, to address this problem for resource constraint zero-shot and few-shot visual question answering applications, more compact and efficient frameworks-e.g., VLC-BERT [174]-which uses the COMmmonsensE Transformer (COMET) to generate and integrate contextualized Knowledge into a pre-trained vision-Language model based on VL-BERT [162]-has achieved impressive results.With ap-proximately 118 M parameters, VLC-BERT achieves competitive accuracy with large-scale networks augmented with knowledge from pre-trained language models like GPT-3, whose parameter count is over 175 billion.</p>
<p>Zero-shot and few-shot generalization in QA and VQA based on foundation models have reached a matured level in terms of performance.The main challenges that remain to be addressed is the interpretability of these models.Owing to their large sizes and the wide scope of data they are trained on, it is difficult to interpret them at the decision level.Other important issues concern their use or misuse.Owing to their extraordinary power and immense versatility, they can be used for malicious purposes not intended by design, thus, raising privacy and ethical concerns.As these models are exposed to unrestricted store of information, the expressions they produce in response to questions need to respect social boundaries.Surprisingly, as at this point, much of the attention is still on improving test-time performance.It is however anticipated that when performances begin to plateau as most of the outstanding accuracyrestraining problems are addressed, attention would quickly switch to these other important issues.</p>
<p>Summary of the main features of prior knowledgeinformed approaches</p>
<p>Table 1 summarizes the major performance characteristics of the common knowledge-augmented deep learning frameworks.As shown, some of the knowledge-informed methods simultaneously address multiple objectives such as accuracy, explainability, adversarial robustness and zero-or few-shot generalization.In general, methods that leverage compositional part relationships excel at almost all four objectives.On the other hand, some approaches may enhance their main objective while compromising other figures of merit.The interpretability-accuracy trade-off -which still a subject of intense debate anyway (see [130], [131], [175], [176], [177])-associated with some knowledge-informed techniques is a classic example of this situation.</p>
<p>Explicit mathematical equations and physics-informed methods can enhance neural networks by leveraging the underlying knowledge to constrain the input or output space, or even the architecture of the model.These constraints can often lead to overall accuracy reduction, although the fidelity and conformance of the decisions with physical environments would improve.Other approaches aim to exploit mathematical representations to generate entirely new training datasets when none is available, or augment existing ones using synthetic data augmentation approaches.This class of methods invariably lead to improved accuracy and robustness.It should be noted that the accuracy improvement results presented are not meant to serve as a basis for comparing the various methods.Given that the tests are carried out under different settings and on different datasets, and since the tasks themselves exhibit varying degrees of difficulty, the performance figures are not relevant for comparison purposes.The information only explains which models achieve accuracy improvements over specified baselines.Unless otherwise stated, these baselines are the equivalent model configurations that do not leverage prior or domain knowledge.ACnet (Architecture-Constrained network) introduces math-guided constraints that, once satisfied, causes the model to obey underlying physical laws.This however, leads to a marginal drop in accuracy.</p>
<p>Employs logic rules to provide global explanations as logical combinations of learned graphical concepts.</p>
<p>By allowing expert intervention on concept predictions, CBMs consistently report simultaneous gains in explainability and final prediction accuracies by clear margins.This is despite intense debate ( [131], [175]) about the explainability-accuracy trade-off.</p>
<p>** The reported value is the accuracy (mean F1 scores over all predictions) improvement relative to the original CBM [118].</p>
<p>Accuracy improvement is relative to a baseline that does not employ CBM / HITL.Accuracy improvement is relative to a baseline that does not employ CBM / HITL.</p>
<p>To improve HTL-based interventions, CB2M employs memory which keeps track of the outcome of past interventions, allowing mistakes to be detected refinements to be made.* Improvement relative to CBM [118] CAIPI uses human feedback on incorrect explanations to create additional data (counterexamples) for training and shows that prudent data augmentation can achieve dual-interpretability and accuracy goals.</p>
<p>Consistent with the explainability-accuracy trade-off, most XIL frameworks, including eXBL, sacrifice accuracy for interpretability.</p>
<p>Employs techniques that use filters to refine output based on ground-truth highlighted regions and Grad-CAM predictions.Tested another technique that uses regions of interest (ROIs) maps to direct the networks attention to expert-provided annotations.All methods show improved interpretability.</p>
<p>CAIPI in Practice [extension of CAIPI [128] that focuses on data efficiency, allows human experts to edit explanations and also correct wrong predictions-resulting in competitive accuracy to be reached with fewer training examples.</p>
<p>KGIN models user intents as attentive combinations of KG relations.</p>
<p>Logic rules can simultaneously boost accuracy along with a combination of other objectives (e.g., explainability and adversarial robustness), especially when combined with other knowledge types like KG and probabilistic representations.</p>
<p>LOGICDEF uses rules to provide explanations as to why a system is attacked and this inspires measures × to overcome subsequent adversarial cases.Retrieves implicit knowledge from GPT-3 while using CLIP to extract explicit knowledge from knowledge bases.NA* -No accuracy baselines are available.</p>
<p>×</p>
<p>RPC [150] NA** Motivated by the incredible power of the human mind, and how humans excel remarkably in these tasks, brain-inspired techniques that leverage principles of biological cognition are proposed to exploit how the mind works to solve these problems.
√ √ √ √ √ √ √ √ √ ACnet [179] Math equations ↓ × × × GLGExplainer [
The design of cognitive architectures has follows three dominant approaches (see [193]) The first group of methods comprises symbolic cognitive architectures (e.g., EPIC [?], [194]; MusiCog [195] which rely on symbolic representations to encode real-world knowledge.These architectures then utilize predefined instructions or explicit rules to process the symbols in order to exploit encoded knowledge.This design makes them suitable for planning and logical reasoning tasks.Moreover, their use of explicit rules provides a clear view of their operating mechanisms and rationale for their decisions and actions.However, since the rules are explicitly defined and fixed, their scope is often very limited.Moreover, it is not easy to adapt them to unanticipated situations that arise in deployment.Therefore, symbolic cognitive architectures are the least suitable techniques for tasks that require adaptation through online learning.However, owing to their conciseness and grounding in first principles, symbolic representations are inherently transparent, making the reasoning process and consequent decisions of the cognitive architectures that adopt them highly interpretable.</p>
<p>The second group of approaches, known as connectionist or sub-symbolic architectures -e.g., ART [196]); MDB [197] -work similar to artificial neural networks.They are designed to process information like the brain by mimicking its conceptual organization with massively interconnected network of neurons (see, for example [198], [199]).Connectionist cognitive architectures may implement biological or artificial neuronal models, or an equivalent structural logic that facilitates learning.They acquire their knowledge by updating internal parameters or weights through a gradual process of interactions with their environments, thus mimicking a learning paradigm akin to biological cognitive process.A major problem with this category of cognitive systems is that the acquired knowledge directly maps perceptual inputs to final decisions in an opaque way, with no intermediate representation that can be readily interpreted by the model developer or the end user.</p>
<p>The final category, hybrid cognitive architectures, includes popular models like CARACaS [200], [201]; iCub [202], MIRIAM [203] and IMPACT [204].They incorporate both symbolic and connectionist capabilities in their design to facilitate learning and adaptation, as well as reasoning over acquired knowledge.By internal organization, the two most common categories of hybrid architectures that can be identified are modular and integrated systems.While integrated methods attempt to encode different aspects of domain knowledge and solve other learning-related problems using a single complex network, approaches based on modular cognitive architectures utilize specialized submodules to accomplish this goal in a task-dependent manner.For example, CARACaS [200] employs a hybrid technique that incorporates a connectionist reinforcement learning unit to process s ensory data from multiple sources to acquire knowledge about the environment in a robust way to aid localization and navigation.All other functions like dynamic planning and behavior coordination are handled by a separate symbolic processing module.Whereas some hybrid systems like CARACaS [200] use separate submodules for their symbolic and sub-symbolic components, others like iCub [202] and IMPACT [204] propose an integrated approach that allows new skills to be incorporated into their symbolic modules.IMPACT [204], for instance, is an autonomous robot system that facilitates continual and open-ended learning by integrating the sub-symbolic reinforcement learning unit and the symbolic planning unit in such a way that the knowledge acquired through learning is automatically encoded in symbolic form.This feature allows the symbolic knowledge base, which drives the robot's actions and behaviors, to be extended continuously as it interacts with its environment.</p>
<p>Overcoming Adversarial Attacks with cognitive insights</p>
<p>Cognitive architectures for adversarial robustness</p>
<p>Although cognitive architectures are not usually designed with the specific aim of addressing adversarial attacks, their quest to ensure general intelligence through learning, perception and reasoning invariably leads to the implicit goal of improving adversarial robustness.It should be noted that adversarial attacks are usually crafted to exploit vulnerabilities of conventional deep learning methods, and since most of these attacks generally pose no problems to humans, it can be argued that enhancing deep learning with human-like reasoning and perception capabilities implicitly constitutes adversarial defense.</p>
<p>Only learning-capable cognitive architectures with the potential to acquire new knowledge online and adapt their behaviors have the ability to handle new threats that arise in the course of operation.Additionally, such an agent requires knowledge and reasoning ability to make the right decisions.Hybrid robotic systems like IMPACT [204] and iCub [202] are designed to meet these requirements.</p>
<p>The approach of the hybrid systems is consistent with the dual process theory [205], [206]) of biological cognition which stipulates that the brain deploys two separate systems for fast and slow processing of information.The fast and slow systems are also called system 1 and system 2, respectively.According to this theory, system 1 is responsible for fast or spontaneous decision making under the control of a subconscious thing while system 2 adopts a rational and systematic reasoning approach to solving problems.The roles played by the two systems complement each other to achieve the desired balance in accuracy of decisions and speed of response.In the biologically-inspired cognitive architecture implementations, the fast system relies on making predictions from knowledge acquired through learning by a connectionist system (e.g., a deep neural network).The slow component employs reasoning and planning ability of a symbolic processing module to solve difficult problems whose solutions are not explicitly encoded by the learning system.The operational logic of such a brain-inspired cognitive architecture is akin to real biological cognition which is known to deploy conscious and subconscious "minds" to solve problems requiring either analytical and systematic reasoning or spontaneous action.</p>
<p>As a matter of fact, humans' remarkable ability to recognize aberrant situations such as adversarial attacks is developed over a long period of accumulated experience gathered through life-long learning, and problem solving.This capacity is consolidated through repeated interactions with their environments.Using such a strategy, the SOFAI [207], [208] cognitive architecture employs a fast learner that continually improves by updating itself through the problem-solving experience of a slow reasoner.SOFAI uses a subsidiary meta-cognitive module to supervise the cognitive activities of the two main cognitive systems.The metacognitive module keeps track of the agent's state, percepts from the environment, and the ability of the fast system to produce desired solution under the given conditions.By default, system 1 handles any problem if it can provide a satisfactory solution, otherwise the meta-cognitive arbitrator assigns the task to system 2.</p>
<p>Another critical factor that accounts for the robustness of human perception is the ability to combine different sensory modalities (e.g., visual, auditory, tactile, olfactory) in a statistically optimal way to create a unified world model, see [209], for example, for further discussion on this concept.To arrive at a sound judgement about the environment, the brain processes different kinds of sensory information and combines them to form a coherent representation of the environment.</p>
<p>Two main principles of multimodal processing are commonly employed: multisensory cue integration and cue segregation.Integration of multisensory cues involves their joint processing to characterize an environmental property as a single unit.Bayesian cue integration or deep learning can be employed to combine the multisensory signals.Cue segregation processes each cue separately to produce independent representations of the target environmental property.Figure 12 shows these concepts with examples.Figure 12a) shows the conceptual framework of multisensory integration.A navigation robot with LiDAR sensor and a stereo camera both measure the distance to the two cars blocking the road.The estimated distances from the two sensors are integrated to produce a single distance value.In this way, the effect of corrupted input from one modality is minimized.Figure 12b) depicts the principle of sensory segregation.The navigation robot featuring camera and audio sensors wishes to understand the scene in front, i.e., whether the cars have crashed or stopped safely in close proximity.Assuming the cars suddenly appear in front of the robot but the visual signal is unclear to inform this judgement.In this scenario, the independently processed visual signal and auditory signal must both indicate a crash for such a conclusion to be made.In other words, if there is a crash, the two cars must come close enough (visual), and the incident must be accompanied by a loud sound (auditory).</p>
<p>Cue integration of multisensory perception is popular in cognitive architectures, with most utilizing neural networks for integration.For example, in the DAC-h3 cognitive robot, Moulin-Frier et al. [210] employs an adaptive learner to learn the representations of multisensory inputs that include tactile, visual and linguistic modalities.This process is followed by data association and alignment to transform the resulting information into a unified percept.Also, the MDB cognitive architecture [197] employs a combination of sensors whose signal association is learned by a neural network while a genetic algorithm performs data alignment.Sometimes information from multiple sensors can be processed and handled separately, with neither integration nor segregation performed.It is possible, in this case, to use one modality for primary perception while another serves as back-up in case of failure of the main percept.</p>
<p>In general, using multimodal information allows the reasoning system to improve robustness and overcome challenges that may arise when one sensory modality produces degraded or flawed outputs as a result of component failure, adversarial attack, or noise in the sensory data.However, some sensors may negatively impact each other when used in combination, thus, negating the intended benefit of multisensory perception.For instance, electromagnetic interference from inertial sensors and analog indicators may hamper performance if proper care is not taken in their joint use.</p>
<p>Brain-inspired DNNs for adversarial robustness</p>
<p>Despite the impressive learning ability of deep neural networks, they are uncharacteristically sensitive to subtle, context-irrelevant changes in their input data.Consequently, state-of-the-art neural networks achieve human-like prediction accuracy in tasks like image classification under suitable conditions but there is still a large performance gap between these deep learning models and humans in terms of adversarial and general robustness.Furthermore, notwithstanding their often-touted biological motivation and organizational similarity, the analogy between current convolutional neural networks (CNNs) and the human brain is only valid at a crude conceptual level.</p>
<p>Unsurprisingly, some of the attempts seeking to close the robustness gap have focused on improving the biological realism of deep convolutional neural network models.By incorporating a CNN layer with neuroanatomical constraints that simulate the primate primary visual cortex (or V1), [36] achieve a remarkable improvement in adversarial robustness on image classification.Their network, named VOneNet, incorporates a model of the primate V1 in the initial block of layers, followed by a block of conventional CNN layers.By implementing convolution operations on the input data with Gabor filter bank of high and low spatial frequency filters (SFFs), the first layer within the V1 block have their weights constrained to produce similar output to the response of the equivalent component of the primary visual cortex.The following layer approximates cell nonlinearities using spectral and rectified linear transformations while the final layer simulates neuronal stochasticity by affine transformation and the injection of Gaussian noise.Interestingly, the model demonstrates high level of immunity against a wide range of adversarial and common perturbations, outperforming several conventional adversarial defenses.It also achieves a competitive level of Furthermore, by manipulating various components of the V1 block, the authors make very interesting observations with potential ramifi-cations for the understanding of adversarial robustness in neuroscience domains.First, when any of the components is removed or altered, the resulting model reduces in adversarial robustness, but still outperforms the baseline without the biological V1 component.This implies that all components in the primate V1 may work synergistically to achieve robustness.They also discover that some of the components of the V1 may respond differently to different types of adversarial and common image corruptions.For instance, removing high frequency Gabor filters in the first layer lowers robustness to white box attacks while increasing robustness to artifacts like image blur.Finally, incorporating neuronal stochasticity only at training time has more pronounced impact on adversarial robustness than employing this feature only during inference.</p>
<p>Baidya et al. [211] extend the VOneNet framework to incorporate an ensemble of multiple primary visual cortex models, each of which is tailored to mitigate a specific adversarial or corruption type.This modification results in improved robustness against a wider range of adversarial and common corruptions, suggesting that different V1 models may possess attributes relevant to solving different adversarial problems and can complement one another when used simultaneously.Dapello et al. [36] further extend their original work to demonstrate that functional simulation of late stage visual representations has a similarly beneficial effect on robustness.Specifically, the show that aligning the representations of a CNN and the Macaque Inferior Temporal (IT) cortex by enforcing similarity constraint between the biological and corresponding CNN components instills human-like recognition attributes into the model, including high adversarial robustness and human behavioral error patterns for unseen object classes.</p>
<p>In addition to incorporating high-level neuroanatomical concepts into deep learning model architectures, biological alignment has also been pursued from the perspective of task-aligned training.For example, using a conventional CNN, [212] propose to transfer the robustness of the visual system of the macaque monkey to the artificial neural network by jointly training their so-called MTL-Monkey DNN model on ImageNet dataset [213] and neural response data from the recordings of the primary visual cortex of macaque.During training, the network was required to concurrently classify an ImageNet instance and categorize the neural activity of a macaque monkey viewing the same image.The goal of this multi-task learning was to optimize the network on the individual task objectives, leading to good performance on both tasks, and yielding common representations that can be exploited by each task.To better understand the rationale behind the robustness of this class of braininspired networks, the authors leverage saliency visualization maps that explain the robustness characteristics of the model.Specifically, they observe that the jointly trained brain-inspired networks tend to focus more on salient image regions than noise or irrelevant perturbations, just like the primate visual cortex.These results show that training-level alignment can help transfer neuro-acuity from the brain to deep learning models, and equip them to improve on robustness and other performance characteristics that humans currently handle better than artificial intelligence systems.</p>
<p>Multisensory data integration</p>
<p>Multisensory data segregation More direct evidence of the utility of task-aligned neural networks in explaining human perception may be provided by recent studies [214], [215]) which demonstrate important similarities in the behavioral traits of DNNs and human or animal subjects in visual perception tasks.In particular, [214] show that DNNs trained on face recognition tasks suffer the face inversion effect [216]) -the dramatic drop in recognition accuracy for inverted faces (i.e., faces turned upside down) compared to upright faces -that also characterizes human face recognition.However, the same CNNs did not show this effect when trained on different but related tasks like generic image classification, even when the image categories include human faces.That is, the only setting that produces sensitivity to face orientation is the one in which the network is trained to recognize the identity of the faces involved (but not when the task is merely trying to tell whether the object in question is a human face or something else).Therefore, the precise task requirement is to blame rather than any representational peculiarity of the faces themselves.To further show that the face inversion phenomenon is not specific to faces, they trained their CNN on car model classification dataset and it achieves inferior accuracy on inverted cars compared to upright ones.Finally, training the network on inverted cars resulted in better performance on same.Therefore, the factors responsible for this behavioral anomaly can be attributed to task-specific optimizations developed through evolution and the fact that faces encountered in natural scenes are usually upright.These findings explain the real reasons for the observed phenomenon of face inversion effect, thus, challenging the longstanding assumptions that face recognition exhibits "special" intrinsic properties [217], [218]) not shared by psychological processes concerned with the recognition of other objects.</p>
<p>Methods that achieve representational alignment between neural networks and humans through similarity judgement experiments [219], [220] have also shown that human-alignment improves adversarial robustness and few-short generalization.Alignment in this manner allows neural networks to acquire effective representational capabilities, including the use of rich semantic information and global constraints.However, Sucholutsky and Griffiths [221] notice a U-shaped dependence between the performance and the degree of a neural network's alignment with human representations.Specifically, moderately-aligned models are less adversarially robust, and are also poorer few-shot learners compared to their weakly-and highly-aligned counterparts.Moreover, the alignment dynamics are highly influenced by the task or dataset used in the study.This suggests that neural networks must be optimized to a high degree of human-alignment to guarantee the benefits of human-like performance characteristics.When this cannot be ensured, empirical test should be conducted to establish that the level of alignment achieved is not within the "moderate" range and, hence, is not producing diminishing returns on adversarial and few-shot generalization performance.</p>
<p>Improving interpretability with cognitive insights</p>
<p>It is interesting to note that the lack of analytical support for the decisions of deep learning systems is not currently among the major problems brain-inspired artificial cognitive systems seek to address.In fact, it can be argued that the need for explainability is driven by the awareness that artificial intelligence currently lacks the level of intelligence needed to warrant unconditional trust by humans.If accuracy and robustness ultimately improve across many tasks and domains, trust will as well increase.After all, humans do not necessarily need to explain their decisions and actions.Therefore, it is clear why more effort on cognitive architectures and brain-inspired neural networks is rather vested in bestowing human-like intelligence capabilities to AI systems.This notwithstanding, in applications like human-robot interaction (HRI), mutual understanding and effective collaboration can be enhanced by ensuring the agent can make decisions and communicate in ways consistent with human subjects.</p>
<p>Improving interpretability with cognitive architectures</p>
<p>In cognitive architectures, learning modules represent and use their knowledge in a non-intuitive way.Therefore, decisions made by these modules cannot be understood by humans.On the other hand, symbolic architectures incorporate well-defined rules or instructions that guide behavior, making the resulting intelligent agents' actions readily interpretable.Cognitive architectures that utilize the dual learning and symbolic reasoning frameworks can easily provide explanations for the decisions of their reasoning modules if required.For instance, Augello et al. [222] propose a conceptual explainability framework for a hybrid cognitive architecture used in a robotic system for gesture recognition.The approach relies on the decision logic provided by system 2 (i.e., the reasoning module) to derive plausible explanations that characterize the categorization of gestures by the social robot.Unfortunately, since this hybrid architecture employs systems 1 and 2 alternatively, it follows that not all decisions can be explained by such a design.Specifically, the inherent opacity of the learning module precludes explanation, and whenever this module is engaged the decisions taken cannot be interpreted.</p>
<p>An alternative approach to explainability is utilized by Wu et al. [223], who proposed a hybrid cognitive architecture for autonomous vehicles with the aim of achieving interpretability and improved learning capacity at the same time.The model incorporates Markov logic network (MLN) for symbolic representation of domain knowledge and a reinforcement learning module, deep Q-network (DQN), as the connectionist component.The DQN is responsible for learning driving policies while the MLN learns logic rules for evaluating driving actions.Thus, the MLN provides interpretable reasoning-based decision-making that abstracts the opaque low-level decision process of the connectionist module.Here too, interpretability is only partial, since the lower decision-making layer remains opaque.Another useful approach to tackling the interpretability problem, used in CARACaS [200], solves the problem by utilizing a decision tree to generate logic rules from the model's decisions and actions.These rules are specifically meant for explaining the underlying rationale behind the given decisions.In their the design, the decision tree has access to all the relevant sensory information that triggered the given decision.Therefore, the generated rules are capable of capturing a sufficient amount of detail since that explains the agent's behavior.</p>
<p>A special form of interpretability, called cognitive salience in reference to the gradient-based saliency visualization methods used in image classification (described in Section 2.2 of this paper), is devised by Cranford et al. [224] based on cognitive modelling to explain how humans make decisions in a cybersecurity setting.Specifically, using the ACT-R cognitive architecture, the authors model how individuals weigh different features based on past experience on reward and penalty to make decisions on cybersecurity attacks.In this implementation, interpretation of feature importance from the ACT-R model is provided by cognitive salience, which is computed by taking the derivative of an interpolation equation that measures the retrieval of outcomes from memory.One of the most important behavioral traits discovered by the model is that instead of using all available information, humans usually make decisions by relying on past experiences, even when these decisions are irrational.The results show that interpretability derived from cognitive architectures may be useful in explaining psychological processes in the brain.</p>
<p>Improving interpretability with biologically-informed neural networks</p>
<p>Biologically-informed neural networks are inherently explainable neural network models that are designed to approximate the underlying structure and dynamics of real biological systems that they imitate.Like biological cognitive entities, these models are generally more capable of capturing the actual relationships of biological entities (input signals) and their interactions compared to conventional DNN models.Here, network components may have precise biological associations and connotations.For instance, nodes and their interconnections may represent gene ontology terms or biological pathways.One of the pioneering works on biologically-informed neural network is DCell [225], which is designed to facilitate effective cellular growth prediction by exploiting the hierarchical structure of gene ontologies.The structure of the network allows the progression of information flow from input genotype to phenotype response to be presented in a way that mimics the actual biological mechanism, making it an intuitively interpretable proxy for the real biological system.</p>
<p>In cases were the underlying structures or the dependencies of biological entities are deemed too complex to be fully represented in a truly transparent form, additional workarounds are employed.For instance, in a study to predict transcription factors (TFs) responsible for inducing specific transcriptional changes (targets) in gene expression during a disease, Magnusson et al. [226] design an interpretable network that does not seek to completely mimic biological gene regulatory networks from architectural point of view, but instead use a simplified representation that allows TF-target dependencies to be used for interpretation.A network with 250 hidden nodes encodes TF-target associations.To derive interpretations, they independently alter the numerical value of network nodes corresponding to each transcription factor and observe the response at the output node that represent target genes.</p>
<p>Biologically-informed neural networks have important practical applications.They are especially becoming extremely useful in oncology [227], [228]), where they are used in studying plausible drug-tumor interactions and drug-drug interactions leading to new drug discovery and effective treatments recommendations.In addition, interpretation of various parts of the networks representing biological subsystems provides insights on the complex mechanisms by which these systems function.However, these approaches are currently limited by the problem of incomplete domain knowledge that makes it difficult to fully reverse-engineer complex biological mechanisms.Hence, the performance of these networks may not reach the levels of black-box models.Also, approaches that rely on altering individual factors to determine their effects (e.g., [226]) may not be able to capture possible synergetic interactions that are only observable when all influencing factors act together.Therefore, there is still a lot of room for improvement, not only from the perspective of network architectures, but also from the point of view of the very knowledge that informs their design.</p>
<p>Another important approach used to simultaneously improve interpretability and biological realism in deep neural networks relies on reducing the number of learned features to a manageable number made up of only those that carry biologically useful information.This concept is motivated by the fact that one of the main factors that accounts for the poor interpretability of deep learning models is their very large feature space-state-of-the-art CNNs typically have millions of features.Furthermore, as described in Section 5, CNNs are known to encode highly relevant psychological information that aligns with the brain's own representation of natural stimuli.However, it has been suspected that some of the CNN features may be redundant and only a subset may actually encode psychologically relevant information.Consequently, through similarity judgment and object categorization alignment, Jha et al. [215] propose to determine the lowest CNN feature dimensions needed to fully characterize human psychological representations and facilitate interpretability.In the case of similarity judgments, Jha et al. propose the SimDR architecture (Figure 13) which incorporates a small-width bottleneck layer to project the 4096 features of the last fully connected layer of a VGG-19 network [229] to a lower dimensional space that retains the ability to emulate human similarity judgments.</p>
<p>They observe that as few as six feature dimensions are sufficient to capture biological representations required to accurately predict human similarity judgments.They then rank the reduced dimensions in order of importance according to the extent to which each contributes to explaining the observed variance.Finally, the embeddings of each dimension can be inspected for interpretability and cognitive insights.For example, the study shows that for each dataset, the highest ranked dimension captures broad, semantically meaningful categorical information about the objects in the dataset-e.g., mammals vs non-mammals, animate vs inanimate objects, etc.-implying that human judgement relies heavily on these semantics.</p>
<p>Tarigopula et al. [230] notice that feature reprojection from high dimensional to low dimensional space, as proposed by Jha et al. [215], may cause information loss, leading  [215].The network takes a pair of images as input and a bottleneck layer projects the learned features to a low-dimensional space.The inner product of the resulting feature map is used to compute similarity scores.The resulting model is easier to interpret since it has a reduced feature set as it retains only biologically-relevant features.</p>
<p>to degraded performance and poorer interpretability.The authors hypothesize that the learned features in their raw form may already encode relevant information required for capturing human similarity judgments, and hence, do not need further transformation.They therefore propose to prune out redundant features, leaving only a small subset of relevant features.Their approach resulted in improved performance, implying that indeed, the last fully connected layers of CNNs already exclusively capture psychologically relevant information.Besides boosting explainability and biological alignment through their semantically meaningful psychological representations, the implications of these studies are also significant for the future of efficient neural network design.Rather than downscaling high dimensional models, new designs may employ lightweight architectures that directly learn only relevant features through appropriate optimization and training regimes.This could lead to significant savings in computational resources and training times.Additionally, the resulting models would run faster in deployment and offer real-time performance, which is currently lacking in state-of-the-art models.</p>
<p>Improving zero-and few-shot generalization with cognitive insights</p>
<p>Cognitive architectures for zero-and few-shot generalization</p>
<p>Although, zero-and few-shot learning are not specifically implemented in cognitive architectures, this concept is implicit in the overall goal of autonomy and artificial general intelligence capabilities that some cognitive architectures attempt to achieve.Thus, an autonomous system must be able to recognize, understand and deal with situations even if it has little or no prior experience with the given situations.This capability is common in cognitive architectures designed for autonomous robots (e.g., iCub [202], IMPACT [204] and general-purpose systems designed to emulate human cognition (e.g., ACT-R [33], CLARION [231]).To meet high autonomy requirements, cognitive architectures typically employ a host of modules and subsystems that enable human-like learning, reasoning and adaptation to new environments.Some of the most important components needed to achieve this capability are: 1) a well-structured memory system used for storing knowledge and facilitating computations during reasoning and learning; 2) a multipurpose learning system that integrates various learning paradigms to attain various competences; 3) a motivationdriven goal generation module to direct the activities of the cognitive agent; 4) an attention system that allows the agent to focus on the most important information for the given task.</p>
<p>The exact implementations of these components vary widely across different architectures.Some architectures like ACT-R and LIDA strive for high biological realism.In this way, the artificial agents do not only serve as proficient zeroshot learners and reasoners deployed to perform tasks autonomously in unknown environments, but they also serve as frameworks to study biological cognition.For instance, Juvina, and Taatgen [232] adapt ACT-R by adding a decaying characteristic to its memory retrieval mechanism and utilize the resulting system to model cognitive suppression that facilitates studies on negative priming, inhibition-ofreturn and the Stroop effect-a phenomenon that describes cognitive interference caused by a mismatch in the representation of a stimulus and its description or name (for CNN rep.example, when the name of a color is printed in a different color).Also, by fitting ACT-R model to model to functional Magnetic Resonance Imaging (fMRI) signals of the brain, Borst and Anderson [233] perform analysis to locate several neural processes, such as fact retrieval from memory and motor responses, retrieval of mathematical facts from memory, working memory updates, motor responses, and visual encoding.Therefore, besides overcoming zero-shot and fewshot generalization problems, development of biologicallyinformed cognitive architectures can help to create platforms necessary for conducting experimental studies leading to improvement in the understanding of the human brain.A better understanding of biological cognition would, in turn, enhance the advancement of artificial agents since these principles are needed to improve biological realism of AI systems.</p>
<p>Brain-inspired DNN for few-shot learning</p>
<p>Whereas zero-shot generalization significantly relies on reasoning in terms of the relations between observed and new categories, few-shot learning capacity is influenced greatly by the learning characteristics of a model, specifically, the ability to learn in a data-efficient manner.In biological systems, synaptic plasticity [235] allows the brain to form new connectivity patterns to enable it to encode new information while retaining existing relevant knowledge in memory.This facilitates data-efficient learning, allowing the brain to generalize well from only a few examples.However, the learning mechanism of gradient-based backpropagation algorithms commonly employed in contemporary machine learning is not compatible with the synaptic plasticity dynamics (see for example, Clopath et al. [236], which require error computations to be local to the neuron.Backpropagation relies on computing a global error vector by transporting gradients backwards through network layers.However, recent studies (e.g., [237]) have shown that this backward transport mode is not required to achieve competitive performance.Specifically, Baydin et al. propose to compute gradients exclusively in the forward mode using directional derivatives, and show that the forward gradient method can be faster in reaching a reference level of validation loss.It is also less computationally expensive and more biologically plausible compared with the strict requirement of backward transportation of errors that the backpropagation algorithm upholds.</p>
<p>On the other hand.predictive coding is a brain-inspired method that approximates the learning mechanism of the backpropagation algorithm [238], [239], [240]) using local error computations.Unlike gradient-based learning which performs weight updates of learnable parameters at all nodes using a single error computed at the final layer, predictive coding relies on minimizing the prediction errors associated with each neuron.A comparison of backpropagation-and predictive coding-based error computation is illustrated in Fig. 14.</p>
<p>Previous work [241] has demonstrated the effectiveness of associative memory in achieving efficient prediction by storing relatable memory vectors of learned features which can later be retrieved when an incomplete pattern similar to a stored instance is presented.Meanwhile, more recent stud-ies [242], [243] show that predictive coding actually implements an effective associative memory system.Expectedly, predictive coding-based networks have therefore demonstrated promising performance in data-efficient learning or few-shot generalization in deep neural networks.</p>
<p>More concretely, empirical results by Lee et al. [234] show the superiority of predictive coding over backpropagation across various few-shot generalization settings in DNNs.In the study, the predictive coding network consistently outperforms the model trained by backpropagation under a range of few-shot learning protocols.The study also shows promising incremental learning ability of the network trained with predictive coding, which is able to learn new information (in this case new object categories) without severely compromising the existing knowledge.On the other hand, retraining a neural network that implements backpropagation on a new task setting may lead to the catastrophic forgetting problem [244], which causes the network to "forget" originally learned information when its weights are updated.Without any memory element to store previous information, catastrophic forgetting and poor few-shot generalization of standard backpropagation-based learning frameworks should not come as a surprise.Furthermore, local error computations in predictive coding networks can be carried out in parallel since they are not dependent on one another, unlike regular backpropagation-based errors which must progress in sequence from deeper layers to shallower ones.This independent weight update scheme can speed up the training process and allow optimization at the neuron level, potentially resulting in improved performance.Another way in which backpropagation may fair worse than predictive coding is the potential occurrence of accumulated rounding and other numerical computational errors caused by the sequential information processing.</p>
<p>Despite its strengths, predictive coding in deep learning is computationally demanding, and this raises serious difficulties when training large state-of-the-art neural network models.Yet, in the scheme of things, it can be argued that predictive coding is still attractive as a viable training method as it has also been shown to bring other benefits such as adversarial robustness [43] to deep learning systems.It should be noted that conventional adversarial training alone is computationally expensive, and combining adversarial training with few-shot learning goals would otherwise be very demanding to implement using conventional backpropagation-based training methods.Thus, in practical applications where artificial intelligent systems are required to exhibit good generalization and robustness properties, predictive coding can be leveraged to meet both objectives at the same time.This philosophy mimics biological systems better as they are not usually tailored for a single operational objective but instead can perform creditably under a wide range of situations.</p>
<p>Beyond training-level biologically-inspired enhancements, architectural-level modifications have also shown promising data efficiency, i.e., zero-shot and few-shot generalization capacity.For instance, Liu et al. [42] design the so-called Approximate Biological Neural Network (ABNN) to generate biologically plausible data used to fit two DNN variants -a Multi-Layer Perceptron (MLP) and a Long Short-Term Memory (LSTM) network.ABNN incorporates the Boundary Vector Cell model of the cortical neural circuit (structures credited with self-localization power in the brain) [245] and implements a host of other brain-inspired plausible features: different activation functions are employed to simulate non-uniform neuronal firing behaviors; dropout regularization models sparsity of neuron connections; injection of a small amount of Gaussian noise to the connection weights simulates plasticity.Other biologically plausible features include variable neural units, connections with feedback and lateral inhibitions.Empirical results confirm the effectiveness of the ABNN-trained models which achieve significant data efficiency.Although the degree to which each biological equivalent component benefits ABNN's data-efficient leaning ability has not been quantified, the fact that many of these modules may indeed play a major role in this behavior is well supported by current knowledge.For instance, cortical inhibition may preserve memory content [246] by preventing unwanted signals from corrupting it.This facilitates learning with few examples as some features learned can be captured in memory and would not be immediately lost when new knowledge is learned.The memory itself is facilitated by recurrent connectivity and enhanced by LSTM which captures long-term dependencies to enable the network to "remember" useful relationships needed for efficient learning and prediction.The study confirms, in line with existing knowledge on biological neural systems, that the ABNN-trained variant that combines recurrent connections with feedback and lateral inhibition and LSTM layer achieves better performance than the variants that employ MLP without these features.</p>
<p>Although the model employed in this study utilized oversimplified representations of various biological neural components, it nonetheless attains high data efficiency.Furthermore, the network variant that incorporates all the intricate features achieves the best performance, suggesting that zero-shot and few-shot generalizability in neural networks may be significantly enhanced by striving for more biological realism.This prospect is therefore exciting as there is still a wide scope for improvement in biological plausibility of this framework.For example, while the simulated neural inhibition mechanism proposed is exclusively localized between adjacent neurons, biological networks implement both local and global excitatory and inhibitory actions that help maintain stability at both levels.</p>
<p>Components of cognitive architectures commonly used to enhance adversarial robustness, interpretability, and zero-shot or few-shot generalization</p>
<p>To attain the competences needed for solving problems related to adversarial robustness, interpretability, and zeroshot or few-shot generalization, biologically-inspired cognitive architectures incorporate a wide range of features with suitable functionalities.In the literature, the most prominent among these features are: memory system, learning system, motivational system and attention mechanism.The features are usually implemented differently across different architectures.Their exact connections and relationships also vary widely from one architecture to another.Figure 15 shows, collectively, the main com-ponents required to achieve adversarial robustness, explainability and zero-or few-shot learning.Table 2 then presents the specific capabilities each of these components may enhance.We briefly describe the important features of these components next.</p>
<p>Memory system: To ensure adversarial robustness and zero-or few-shot generalization ability, a well-developed memory system embodying various parts that facilitate a variety of functions is required to store and maintain knowledge for effective decision-making: To enable easy and speedy access, working memory (WM)-a form of short term memory (STM)-temporarily holds the information that is currently being processed.The arrangement enables fast decision-making.After processing, the information is transferred to long-term memory (LTM) for a lasting storage as acquired knowledge.Long-term memory may store information as symbolic instructions or rules, with relevant metadata that describes context, relations or any other important background information that may be useful in decision making.In addition to symbolic elements, neural network weights may also be stored in long-term memory.</p>
<p>The functional components of LTM are 1) procedural memory, used for storing implicit knowledge learned from spontaneous behaviors like swallowing; 2) declarative memory, which is made up of semantic memory used for storing factual information about entities and their relationships; and 3) episodic memory for storing experiential knowledge acquired over time.ACT-R represents working memory as a collection of separate memory buffers to support parallel processing by individual modules.Temporary results of each model is stored in the module-specific WM buffer and a symbolic module coordinates the activities of these units.Some cognitive architectures-as in the case of ACT-R- usually store episodic memory information in the form of "stimulus, decision, outcome", and use it to guide behavior.</p>
<p>Learning system: Zero-shot and few shot learning capabilities, as well as high robustness are facilitated by a corresponding memory system that allows storage and retrieval of different forms of knowledge acquired through a powerful learning process.The learning should progress incrementally, without new information overriding existing knowledge, but instead used in enhancing it.Online learning may result in the update of components like neural network weights, symbolic instructions or rules, or associated metadata.An entirely new experience may involve the formation of new symbol structures.In the process, corresponding changes are made to procedural and declarative long-term memory contents.</p>
<p>The ability to learn online is imperative, as this feature enables the agent to adapt its behavior by learning from mistakes and good decisions alike.For effective zero-shot and few shot learning, multiple learning methods may be combined to enable the acquisition of different kinds of skills.</p>
<p>For example, in ACT-R and IMPACT, procedural learning is accomplished by composing new rules from example observations.This allows new skills and knowledge to be formed through experience.At the same time, these architectures implement associative learning through the action-reward paradigm, also known as reinforcement learning.Reinforcement learning facilitates adaptive behavior by enabling the agent to learn useful associations between actions and their outcomes.</p>
<p>Motivational system: Effective learning also requires a well-developed motivational system that enables intelligent agents to autonomously generate new goals and determine or select appropriate actions to achieve these goals in the presence of environmental constraints.A goal is usually a new state the agent would like to be in, and the motivation provides the reason for striving to be in that state.For example, IMPACT is a robotic platform designed for exploration.It can autonomously generate goals driven by motivations such curiosity and novelty that inspire it to discover new states by exploring its environment.In cognitive architec-  Online associative learning can be leveraged to provide weights over choices so as to improve the learning outcome.Decisions made by the learning module preclude explanation, unless the knowledge acquired through learning is used to consoli date symbolic rules which in turn provide explanations.</p>
<p>Online associative learning (usually formulated as reinforcement learning problem) involves the gradual learning of skills th rough learning -* repeated interactions with the environment.Out-of-the-box, associative learning does not facilitate exlainability.Working memory (WM) √ √ √ Short-term or working memory store temporary information that is essential for the requisite computations during decision-making.Long term memory
(LTM) √ √ √ Intrinsic motivation √ √ √
LTM stores the symbolic structures that encode acquired knowledge (e.g., facts) as well as the associated metadata that provi de guidelines on the use of the knowledge.Both short-term and long-term memory systems essential for meeting the capabilities required for explainabilty, adversarial robustness and zero-or few-shot learning.</p>
<p>Effective motivational systems facilitate adaptability and robustness as they can drivecognitive systems to generate new set of goals in response to changing environments, and select appropriate actions to achieve these intrinsic goals.Hence, they offer adve rsarial robustness and ZSL/FSL capabilities.</p>
<p>Attention maps can provide explanations by highlighting the most salient regions of the percept that influence a given decisi on.This is common in visual perception (e.g., [247] and [248].</p>
<p>tures, goals are usually generated to maximize the agent's overall pay-off (which is inherent in the motivation) once attained.These capabilities enable autonomous robots to learn new skills and acquire new competences needed for operating and achieving desired success in their environments.</p>
<p>Attention mechanism: To manage computational and memory overload, some cognitive architecture frameworks like CLARION and iCub implement attention mechanisms that enable them to focus on the most important information.For instance, in these cognitive architectures, data is received from multiple sensor modules but only the most relevant information to the task at hand is immediately processed.This prevents distractions and allows the agent to attend to the most important events quickly.Summary: All the components and features of cognitive architectures described in this section and in the preceding sections work in combination to meet the operational needs of intelligent agents that utilize them.To function satisfactorily in the open-world, these agents must satisfy key performance objectives.First, they should be able to make decisions in a zero-or few shot manner, i.e., in situations they have little prior experience in.This goal is usually achieved by leveraging the capabilities of learning and reasoning modules in tandem.Additionally, these agents must exhibit robustness to unusual and adversarial sensory inputs.Robustness is aided by access to multiple sensory cues.Finally, for applications in collaborative robotics and simulations of biological cognition, agents must be able to explain their decision logic.Usually, symbolic components provide this interpretability, although learning components based on neural networks that leverage explainable artificial intelligence techniques can independently provide explanations on their predictions over sensory input.</p>
<p>Presently, most cognitive architectures do not explicitly specify adversarial robustness, interpretability and zeroor few shot generalization objectives.Instead, their motivational systems generate goals that maximize utility of overall task performance, which implicitly could include any combination of these objectives.The lack of explicit requirement on these objectives makes it difficult to compare models on the effectiveness of their realization.In fact, it is sometimes hard to access whether a given model achieves some of these objectives.Consequently, some of the descriptions of cognitive architectures in the context of adversarial robustness, interpretability and zero-or few-shot generalization are based on direct relationships between their actual capabilities and the capabilities needed to achieve these specific objectives.The components of cognitive architectures discussed in this section are critical in attaining these capabilities.</p>
<p>BRAIN DECODING USING PRE-TRAINED DEEP LEARNING MODELS</p>
<p>Brain decoding allows the content of the brain-in this case the brain's response to visual stimuli-to be decoded by transforming it into semantics-rich images, facilitating a deeper understanding of how the brain encodes visual information.Eventually, it can also provide clues on what is on the mind, and help artificial agents to understand their human counterparts in human-computer interactions (HCI).And in conjunction with explainable AI techniques that enable humans to understand the decisions of machine learning systems, mutual understanding between humans and AI systems in collaborative tasks can be attained.</p>
<p>Implicit knowledge captured by pre-trained foundation models has proven useful in brain decoding.Specifically, CLIP [163] has shown the most exciting prospect in reconstructing natural scenes from functional Magnetic Resonance Imaging (fMRI) signals of the brain.For instance, BrainCLIP [249] achieves impressive results by fine-tuning CLIP on fMRI signals with additional image and text data provided as prompts.The zero-and few-shot generalization property of CLIP which has been confirmed by several studies (e.g., [250], [251]) is critical in the open-world neural decoding task.Previously, research aimed at decoding visual stimuli from brain activity recordings mainly concentrated on unimodal approaches that rely on either natural language descriptions of viewed images (e.g., [252], [253], [254]) or visual semantic information (e.g., [255]).These methods learn representations that map raw fMRI signals to textual representations or natural images.Although these approaches achieve satisfactory results in simpler tasks like visual stimuli classification -that is, categorizing the image that produces the given fMRI response -reconstruction of whole scenes with high semantic fidelity remains a major challenge.More recently, a number of studies (e.g., [43], [249]) have established that multimodal vision-language frameworks perform far better in this task than unimodal approaches due to a better representational ability of the former.</p>
<p>This assessment is consistent with the conclusions of the dual coding theory ( [256], [257]), which explains that the human brain efficiently encodes visual information on natural scenes using a combination of verbal and visual representations.According to this view, as shown in Figure 16, when an observer sees a tiger for example, several associated verbal concepts (such as yellow, stripes, strong, fierce) that have been established through experience and commonsense knowledge about the object are automatically registered in the brain.The corresponding dual image-linguistic representation encodes a wealth of background information with semantic relations to the content of the scene.This representation is far richer than what either modality could possibly capture alone.Unsurprisingly, therefore, the methods that adopt multimodal frameworks with the ability to simultaneously represent both visual and verbal concepts have yielded better results.</p>
<p>Another factor that accounts for the performance gap between regular and pre-trained models is the lack of sufficient quantities of training data.Producing sufficient quantities of fMRI responses to external stimuli is an extremely challenging activity.Hence, extensive datasets of paired stimuli-responses for training brain decoding networks is a difficult task.Eventually, models trained on the limited data would need to handle unseen samples (or those with only a few training examples) when deployed in-the-wild.Therefore, the neural decoding task is essentially a zeroor few-shot generalization problem.Fortunately, aided by their zero-of few-shot generalization ability, pre-trained foundation models, particularly CLIP-based brain decoding frameworks, have demonstrated strong neural decoding performance in the zero-shot setting.</p>
<p>The emergence of several state-of-the-art models like BrainCLIP [249], MindEye [258], Mind-Reader [259] and Brain-diffuser [260]  Figure 17 shows sample results of visual scenes gen-</p>
<p>Linguistic represenration</p>
<p>Visual represenration erated from input fMRI signals using Mind-Reader [259] The results show that while the appearances of objects reproduced by the model do not exactly match the original images, the overall high-level semantic content of the scene is preserved.This impressive performance on decoding brain signals to reveal visual information in the form of photorealistic images has the potential of helping to accelerate studies on how the brain represents visual information.</p>
<p>DISCUSSIONS</p>
<p>Conventional deep learning models are designed for specific applications under a set of (sometimes unrealistic) assumptions.Humans on the other hand, can learn from a variety of contexts and connect the acquired knowledge in powerful ways to solve even unfamiliar problems.These human-in-the-loop approaches are particularly useful for improving explainability.Here, human knowledge is usually employed to correct any wrong explanations given by a model, and since in some approaches the accuracy of the model's decision is contingent on the corresponding explanation being correct, this technique simultaneously improves explainability prediction ac-curacy.In human-in-the-loop frameworks, humans may interact with machine learning models through symbolic logic, natural language, or image annotations.Cormodel explanations through simple interfaces like logic rules is precise and unambiguous.However, it is currently difficult for the human participant to communicate exact intentions through saliency annotation because the annotation style only captures an imprecise representation of the user's in-tended input.It would be helpful for machine learning methods to be able to recognize the exact region intended by the user.One possible way of achieving this could be through annotation-specific regularizations during training.Also, in cases involving long training cycles, it is extremely laborious for humans to provide the needed debugging.Fortunately, a new line of works that seeks to engineer special artificial agents -called cognitive clones [261], [262]-capable of mimicking a individual's behavior and actions.These agents would be able to replace humans in the heatmap annotation process, thereby alleviating the burden in training this class of human-in-the-loop interpretability models.d) Zero-shot and few-shot generalizations: Many knowledge-informed approaches have achieved impressive results in generalizing to unseen instances.Implicit knowledge encoded by pre-trained foundation models has particularly demonstrated the ability to reach impressive zero-shot and few-shot generalization performance across different tasks, especially in question answering and visual question answering.This high generalization ability raises a major concern that borders on ethical and privacy matters.Specifically, as a result of pre-training on unrestricted amounts of data, these models acquire substantial world knowledge that can often be leveraged by end-users to generate privacy-infringing or ethically questionable content in ways that are unanticipated by their developers.Therefore, appropriate measures to ensure their proper use will be necessary.</p>
<p>2) Brain-inspired deep learning and cognitive architectures.</p>
<p>a) Cognitive architectures: In most cases, the effectiveness of cognitive architectures in handling the aforementioned problems lies in the fact that they strive for a more general human-like intelligence characteristic.Hybrid cognitive architectures are among the most ambitious models that attempt to achieve artificial general intelligence.They incorporate many features of the brain and adopt a similar problem-solving paradigm.Specifically, in line with the dual process theory of human cognition, they employ two separate systems for this purpose -a learning-based system, named system 1, for fast decision making; and a reasoning-based system, named system 2, for slow and analytically grounded problem solving.However, whereas in biological cognition each of these systems is adapted to play its intended evolutionary role without any conscious effort, the mechanisms that regulate this function in brain-inspired cognitive architectures far from optimal.Some recent implementations deploy a third system to play a supervisory role, in some cases (e.g., [263]) this involves selecting the solver between systems 1 and 2 without considering the suitability for the problem.This approach may lead to sub-optimal solutions being produced.Instead, an AI system should understand the scope of its own knowledge in advance, and activate the appropriate system based on the problem at hand, or immediately engage system 1 if promptness is a critical requirement.If the mechanisms that control the functions of systems 1 and 2 are methodically evaluated and improved along the lines of biological plausibility, performance could be enhanced and better simulation models developed to gain further insight into how humans carry out these functions.This may even lead to outcomes that allow humans to overcome current deficiencies such as the law of reversed effort [264] and tendency to choke under pressure [265], [266]) that currently characterize human decision making when stakes are high.These deficiencies are associated with the fact that the reasoning system takes over control of decision making when in fact the task could be better handled by the spontaneous learning system.b) Brain-inspired neural networks: Brain-inspired neural net-works aim to leverage the structure and information processing mechanisms of the brain to improve deep learning models.Some approaches make architectural enhancements to neural networks by incorporating modules that mimic various components of the brain.Other methods achieve biological plausibility by implementing training-level alignment through multi-task learning.Still, other techniques rely on similarity judgement optimizations to align the representations of neural networks and humans.In-formation processing techniques like predictive coding have been realized.The resulting models obtained from all these biologically-informed modifications show high performance in difficult settings compared with corresponding frameworks that do not employ these techniques.In particular, brain-inspired neural networks excel in adversarial robustness and zero-or few-shot generalization performance.Meanwhile, experiments based on similarity judgements have shown that although representational alignment -which characterizes the level of agreement between the representations of humans and DNN models -enhances adversarial robustness and few-shot generalization machine learning, the improvements are not absolute.In particular, models with low degree of alignment perform better in adversarial robustness and few-shot generalization tests than those with moderate level of alignment.How-ever, highly aligned models outperform both lowly and moderately aligned models.Although the exact reasons for this behavior are not yet established, we posit that this may be caused by the fact that moderate alignment may be enough to disrupt the original representational structure of the neural network but insufficient to instill human representational attributes needed to ensure human-like performance.Studies also reveal that alignment outcomes are dataset dependent.The implication of these findings is that developers of deep learning models seeking representational alignment must empirically determine the appropriate levels of alignment needed to achieve the required performance improvements.One of the main challenges of brain-inspired neural network design is the complexity of the brain.This makes it difficult to mimic most components accurately.Furthermore, human knowledge of the brain is still incomplete.Therefore, neural network designers can only achieve a limited degree of biological realism.Hence, neural networks still vastly underperform their human counterparts in adversarial robustness and zero-or few-shot generalization problems.However, the ability of biologically plausible deep neural networks to simulate behavioral phenomena associated with human task performance has opened up the opportunity to better understand the cognitive process.In contrast with ideal observer analysis which relies on rigid mathematical modeling to test various aspects of cognition, brain-inspired neural networks are flexible and, though training, are able to adapt their properties and representations to mirror the behavioral characteristics of human subjects performing a given task.Although, the exact extent to which deep learning models can improve their performance by leveraging knowledge of the brain is not yet known, results from current research on brain-inspired neural networks suggest that there is a wide scope for improvement in this direction.</p>
<p>3) Zero-and few-shot brain decoding using pre-trained models Importantly, for the neuroscience community, implicit knowledge in pre-trained models has shown a high potential for enabling the decoding of visual stimuli encoded by the brain.In particular, the zero-and few-shot generalization ability of CLIP is leveraged to efficiently utilize low volumes of functional magnetic resonance imaging data-since this data is not available in large quantities owing to the difficulty in recording samples-to perform open-world brain decoding and scene reconstruction.Although current approaches achieve impressive results in terms of their ability to recover global semantic information in the reconstructed scene, object-level details for complex scenes.One reason that could account for this deficiency is the fact that these brain-decoding neural networks currently seek to recover the entire stimuli signature from only the visual cortex.However, while the visual cortex plays a fundamental role in visual stimulus encoding, other brain regions may also handle useful aspects of visual information needed recover the original scene in its entirety.Therefore, future work could incorporate relevant signals from additional parts of the nervous system to improve decoding accuracy.</p>
<p>CONCLUSION</p>
<p>Artificial intelligence systems have already reached impressive performance levels in general application settings.However, in special situations, these models significantly fail to achieve satisfactory results.We consider adversarial robustness, explainability and zero-shot or fewshot generalization, which are among the most important problems faced by deep learning systems today.In this paper, we review the approaches of tackling these problems.Specifically, we focus exclusive on the methods that leverage prior knowledge and those that are motivated by the brain's mechanisms of operation.Both lines of approaches have achieved significant success in addressing these problems.Our work also covers studies that aim to leverage the capabilities of artificial intelligence in advancing cognitive science knowledge.Again, there are important accomplishments in this direction.Despite this progress, there is still a large gap between the performances of artificial intelligence models and humans under challenging situations.Also, a lot more is yet to be discovered in cognitive science.Overall, given the current directions of research and the pace of progress, the future prospects of both areas are bright.</p>
<p>Figure 1 .
1
Figure 1.Simplified block diagram showing the main components of a cognitive architecture.</p>
<p>Fortunately, simulations of brain-inspired deep neural networks</p>
<p>Figure 2 .
2
Figure 2. Architectural alignment of CORnet-S [44] with cortical areas V1, V2, V4, and IT in the ventral stream of the brain facilitates brain-like performance and provides further insights on its inner workings.</p>
<p>Figure 4 .
4
Figure 4. Structure and outline of the paper.</p>
<p>ExplainabilityFigure 5 .
5
Figure 5. Performance and explainability relationships of machine learning frameworks [64].</p>
<p>Figure 6 .
6
Figure 6.A chest X-ray image [83] from Covid-19 patient in (a) and a heatmap produced by Grad-CAM [72] incorrectly highlighting the text in the upper left corner as explanation for the positive diagnosis (b); The image on the right shows the correct heatmap capturing the features relevant for the diagnosis.</p>
<p>Figure7.Using co-occurrence logic (e.g.,[104]), it is easy to detect the inconsistency of a bus and a toothbrush co-existing in a scene.A more logical combination would be a bus and a traffic sign.The implausible results are discounted as adversarial.</p>
<p>Compositional part-based reasoning: Besides their effectiveness in adversarial defenses, compositional partbased reasoning methods have also been applied successfully in few-shot object recognition.In CORL (short for Recognizing Object by Components), He et al. [149] represent object categories with the knowledge of shared components and their spatial topologies.The knowledge base for training consists of two separate dictionaries, one for object parts and the other for their common spatial patterns.Trainable attention is also employed to reinforce object parts that are most relevant for recognizing each class.At testtime, the knowledge learned helps to recognize categories from just a few examples seen during training.Recognition as Part Composition (RPC), proposed by Mishra et al. [150] also achieves zero-shot and few-shot generalization by representing objects as constituent parts which are further decomposed into smaller units based on human humanlevel understanding.Besides zero-shot generalization, both CORL and RPC facilitate interpretability by virtue of the human understandable part relationships used for prediction.Additionally, RPC has shown adversarial robustness in object recognition tasks.It accomplishes this by being sensitive to improbable part compositions in the input.</p>
<p>KL</p>
<p>0.32 ... 0.05  Intervention wrong Ovenbird 0.0 ⋯ 0.32 ... 1.0 correct Worm Eating Warbler e.g.bone spurs wrong 1.48⋯0.15⋯ 0.09 KL Grade: 0 Intervention correct 1.48 ⋯1.0 ⋯ 0.09 KL Grade: 2 Intervention correct1.12⋯1.0 ⋯ 0.31</p>
<p>Figure 10 .
10
Figure 10.General procedure of training CLIP [163] for inference.</p>
<p>Figure 11 .
11
Figure 11.General workflow of integrating knowledge graph and pre-trained model for zero-and few-shot learning: Information from various online sources (1) is used to pre-train a neural network (2) which acquires broad world knowledge.On the other hand, domain-specific expert knowledge (3) is encoded in a knowledge (4).The two knowledge systems (2) and (4) then complement each other in the end task (5).</p>
<p>ExCAR integrates representation learning with structure learning of logical rules.KEMLP achieves high adversarial robustness via integrating domain knowledge represented by logical × relationships into probabilistic models.S-AL leverages ontological information to improve the handling of relations among classes.× Knowledge graph-based methods like OntoPrompt are effective in zero-and few-shot learning settings when used alone or in combination with other constructs.NA<em> -No accuracy baselines are available.Exploits geometric relationships among known and unknown objects in an image for zero-shot inference.NA</em> -No accuracy baselines are available.Proposes a Dense Graph Propagation (DGP) module to connect distant nodes of a knowledge graph in a graph convolutional neural network framework.NA<em> -No accuracy baselines are available.GRAN employs graph modeling to leverage visual characteristics and semantic relations of multiple objects in a scene.NA</em> -No accuracy baselines are available.Combining implicit knowledge from pre-trained foundation models and explicit knowledge from KGs dramatically improves ZSL/FSL.NA* -No accuracy baselines are available.</p>
<p>Figure 12 .
12
Figure 12.Two widely utilized principles of multimodal perception: multisensory data integration and multisensory data segregation.</p>
<p>Figure 13 .
13
Figure 13.CNN architecture of SimDR[215].The network takes a pair of images as input and a bottleneck layer projects the learned features to a low-dimensional space.The inner product of the resulting feature map is used to compute similarity scores.The resulting model is easier to interpret since it has a reduced feature set as it retains only biologically-relevant features.</p>
<p>Figure 14 .
14
Figure 14.Difference between (a) backpropagation and (b) predictive coding[234].Backpropagation computes a single error for the entire forward pass while predictive coding computes local errors for each activation unit.</p>
<p>Figure 15 .
15
Figure 15.Main components of a biologically-inspired cognitive architecture.</p>
<p>Figure 16 .
16
Figure 16.Dual coding account of scene representation: In addition to the visual impression, intuitive linguistic attributes associated with the objects present in the scene are also registered in the brain.</p>
<p>is a strong evidence to this fact.These models are first trained to map raw fMRI data to the aligned image and text embedding space of the CLIP model through contrastive learning.The resulting representation is then used to reconstruct the original scene that generated the brain response captured in the fMRI signals.For scene reconstruction, image generation are required to transform the aligned CLIP embeddings to natural images.The commonest image generation methods employed are Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models (DMs).</p>
<p>Figure 17 .
17
Figure 17.Sample results of scene reconstruction from fMRI signals.The op row shows the original image that generated the given fMRI response while the bottom row shows the corresponding reconstructed scene using Mind-Readern.</p>
<p>Table 1
1
Performance characteristics of the common knowledge-informed methods.The arrow symbols and indicate improvement ( ) and a drop ( ) in standard or clean accuracy (Acc.), respectively, with respect to the baseline while indicates no appreciable change.X and V indicate whether the given model possesses interpretability or explainability (XAI), adversarial robustness (Adv) and zero-or few-shot learning (ZSL/FSL) capabilities.The presented accuracy values are reported for the settings that yielded the best results relative to the baseline.
ModelKnowledge representationObjectiveHighlightsAcc.XAI Adv.ZSL/ROCK [117]Compositional parts↔×√×<em> ROCK's clean accuracies are competitive with the most effective methods that recognize object as-a-whole.Part-based reasoning methods are capable of achieving multiple objectives (adversarial robustness,CORL [149]Compositional partsNA</em><em>√×√interpretability and ZSL or FSL) simultaneously. NA </em><em>Empirical results on clean accuracies are not available.Compositional parts/RPC decompose images into two hierarchies -salient parts constituent concepts.conceptsNA </em>*Empirical results on clean accuracies are not available.MIDPhyNet [14]Physics-informed↑ 15%×××Here, a neural network is trained using intrinsic mode functions which are formed by decomposing a physics-based model of the system under investigation.Uses physical laws to render photorealistic scenes that provides additional training images as a form ofCGIntrinsics [178]Physics-based↑12.1%×××</p>
<p>Knowledge-informed methods attempt to enhance deep learning by equipping models with domain knowledge available to humans.This enables deep learning systems to handle robustness and data-insufficiency problems better.The knowledge mainly entails how the world works.
4IMPROVING DEEP LEARNING WITH COGNITIVEINSIGHTS180]KG/rulesNA√××CBM [118]CBM/ HITL↑ 14.5%<strong>√××CBM-AUC [181]CBM / HITL4.3%</strong>√××DL-AMC [15] CEMs [182]CBM / HITL CBM /HITL↑3.51% ≈5%√ √× ×√ ×CB2M [183]CBM / HITL6.3%<em><em>√××CAIPI [128]XIL / HITL↑ 37.6 %√××eXBL [130]XIL / HITL↓√××Ref. [131]XIL / HITL↓√××CAIPI [184]XIL / HITL↔√××KGIN [185]KG↑14.5%√××Ref. [107]Logic rules↑2.3%/1.1%</em> ×√LOGICDEF [109]Logic rules/KG↑55%√√ExCAR [186]Logic rules/ probabilistic↑ 6%√√KEMLP [187]Probabilistic/ Logic rules↔×√S-AL [188]OntologiesNA√√OntoPrompt [189]KG/ OntologiesNA</em>××CA-ZSL [190]KGNA<em>××DGP [191]KGNA</em>××GRAN [192]KGNA<em>××Prophet [172]GPT-3 /KGNA</em>××KAT [171]GPT-3 /KGNA*××</p>
<p>Table 2
2
Main features of cognitive architecture and the objectives they can help to meet.Meaning of symbols: means the feature helps in attaining the particular objective;means negative impact; and × means the objective is unaffected by the feature.Multisensory perception helps to conduct sanity checks in order to detect when any modality is inconsistent with the others as a result of adversarial attack.Symbolic processing provides interpretable rules and are the main source of interpretability in most cognitive architectures.
FeatureObjectiveHighlightsXAI Adv. ZSL/FSLMultisensory perception×√ <em>×Symbolic processing Online procedural learning√ * -</em>√ √√ √*Online procedural learning involves learning from examples or rules encountered through experience. Additionally, reinforceme nt
*</p>
<p>ing," nature, vol.518, no.7540, pp.529-533, 2015. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929,2020.Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 012-10 022.J. G. Nam, S. Park, E. J. Hwang, J. H. Lee, K.-N.Jin, K. Y. Lim, T. H. Vu, J. H. Sohn, S. Hwang, J. M. Goo et al., "Development and validation of deep learning-based automatic detection algorithm for malignant pulmonary nodules on chest radiographs," Radiology, vol.290, no. 1, pp. 218-228, 2019.W. Zhou, Y. Yang, C. Yu, J. Liu, X. Duan, Z. Weng, D. Chen, Q. Liang, Q. Fang, J. Zhou et al., "Ensembled deep learning model outperforms human experts in diagnosing biliary atresia from sonographic gallbladder images," Nature communications, vol.12, no. 1, p. 1259, 2021.
[6][7][8][9]
ACKNOWLEDGMENTSThe authors would like to thank...
Imagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. 201225</p>
<p>Deep inside convolutional networks: Visualising image classification models and saliency maps. K Simonyan, A Vedaldi, A Zisserman, arXiv:1312.60342013arXiv preprint</p>
<p>A review of recurrent neural networks: Lstm cells and network architectures. Y Yu, X Si, C Hu, J Zhang, Neural computation. 3172019</p>
<p>Generative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. 201427</p>
<p>. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Human-level control through deep reinforcement learn. </p>
<p>Deep neural network improves fracture detection by clinicians. R Lindsey, A Daluiski, S Chopra, A Lachapelle, M Mozer, S Sicular, D Hanel, M Gardner, A Gupta, R Hotchkiss, Proceedings of the National Academy of Sciences. 115452018</p>
<p>A graph placement methodology for fast chip design. A Mirhoseini, A Goldie, M Yazgan, J W Jiang, E Songhori, S Wang, Y.-J Lee, E Johnson, O Pathak, A Nazi, Nature. 59478622021</p>
<p>Artificial intelligence versus maya angelou: Experimental evidence that people cannot differentiate ai-generated from human-written poetry. N Köbis, L D Mossink, Computers in human behavior. 1141065532021</p>
<p>Fooled twice: People cannot detect deepfakes but think they can. N C Köbis, B Doležalová, I Soraperra, Iscience. 24112021</p>
<p>Midphynet: Memorized infusion of decomposed physics in neural networks to model dynamic systems. Z Zhang, R Rai, S Chowdhury, D Doermann, Neurocomputing. 4282021</p>
<p>Improved surrogate modeling of fluid dynamics with physics-informed neural networks. J C Wong, C Ooi, P.-H Chiu, M H Dao, arXiv:2105.018382021arXiv preprint</p>
<p>Playing for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part II 14</p>
<p>Knowledge graphs. A Hogan, E Blomqvist, M Cochez, C Damato, G D Melo, C Gutierrez, S Kirrane, J E L Gayo, R Navigli, S Neumaier, ACM Computing Surveys (Csur). 5442021</p>
<p>A survey on knowledge graphs: Representation, acquisition, and applications. S Ji, S Pan, E Cambria, P Marttinen, S Y Philip, IEEE transactions on neural networks and learning systems. 202133</p>
<p>Explainable reasoning over knowledge graphs for recommendation. X Wang, D Wang, C Xu, X He, Y Cao, T.-S Chua, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering. A Bosselut, R Le Bras, Y Choi, Proceedings of the AAAI conference on Artificial Intelligence. the AAAI conference on Artificial Intelligence202135</p>
<p>Qagnn: Reasoning with language models and knowledge graphs for question answering. M Yasunaga, H Ren, A Bosselut, P Liang, J Leskovec, arXiv:2104.063782021arXiv preprint</p>
<p>Jaket: Joint pre-training of knowledge graph and language understanding. D Yu, C Zhu, Y Yang, M Zeng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236638</p>
<p>R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, arXiv:2006.13155Logical neural networks. 2020arXiv preprint</p>
<p>Probabilistic faster r-cnn with stochastic region proposing: Towards object detection and recognition in remote sensing imagery. D Yi, J Su, W.-H Chen, Neurocomputing. 4592021</p>
<p>Fast r-cnn. R Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.023112022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>The Soar cognitive architecture. J E Laird, 2019MIT press</p>
<p>The sigma cognitive architecture and system: Towards functionally elegant grand unification. P S Rosenbloom, A Demski, V Ustun, Journal of Artificial General Intelligence. 7112016</p>
<p>Act-r: A cognitive architecture for modeling cognition. F E Ritter, F Tehranchi, J D Oury, Wiley Interdisciplinary Reviews: Cognitive Science. 103e14882019</p>
<p>An integrated theory of the mind. J R Anderson, D Bothell, M D Byrne, S Douglass, C Lebiere, Y Qin, Psychological review. 111410362004</p>
<p>Synaptic plasticity dynamics for deep continuous local learning (decolle). J Kaiser, H Mostafa, E Neftci, Frontiers in Neuroscience. 144242020</p>
<p>Convolutional neural networks as a model of the visual system: Past, present, and future. G W Lindsay, Journal of cognitive neuroscience. 33102021</p>
<p>Simulating a primary visual cortex at the front of cnns improves robustness to image perturbations. J Dapello, T Marques, M Schrimpf, F Geiger, D Cox, J J Dicarlo, Advances in Neural Information Processing Systems. 202033</p>
<p>Recurrent connections in the primate ventral visual stream mediate a tradeoff between task performance and network size during core object recognition. A Nayebi, J Sagastuy-Brena, D M Bear, K Kar, J Kubilius, S Ganguli, D Sussillo, J J Dicarlo, D L Yamins, Neural Computation. 3482022</p>
<p>A brain-inspired objectbased attention network for multi-object recognition and visual reasoning. H Adeli, S Ahn, G J Zelinsky, bioRxiv. 2022</p>
<p>Lcanets: Lateral competition improves robustness against corruption and attack. M Teti, G Kenyon, B Migliori, J Moore, International Conference on Machine Learning. PMLR202221252</p>
<p>Braininspired replay for continual learning with artificial neural networks. G M Van De Ven, H T Siegelmann, A S Tolias, Nature communications. 11140692020</p>
<p>Single cortical neurons as deep artificial neural networks. D Beniaguev, I Segev, M London, Neuron. 109172021</p>
<p>Functional connectome: Approximating brain networks with artificial neural networks. S Liu, A N Mavor-Parker, C Barry, arXiv:2211.129352022arXiv preprint</p>
<p>Multimodal neural networks better explain multivoxel patterns in the hippocampus. B Choksi, M Mozafari, R Vanrullen, L Reddy, Neural Networks. 1542022</p>
<p>Brainlike object recognition with high-performing shallow recurrent anns. J Kubilius, M Schrimpf, K Kar, R Rajalingham, H Hong, N Majaj, E Issa, P Bashivan, J Prescott-Roy, K Schmidt, Advances in neural information processing systems. 201932</p>
<p>Intriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.61992013arXiv preprint</p>
<p>Autopilot was active when a tesla crashed into a truck, killing driver. T B Lee, Ars Technica. 2019</p>
<p>Robust physical-world attacks on deep learning visual classification. K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, C Xiao, A Prakash, T Kohno, D Song, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Threat of adversarial attacks on deep learning in computer vision: A survey. N Akhtar, A Mian, Ieee Access. 64302018</p>
<p>Evading defenses to transferable adversarial examples by translation-invariant attacks. Y Dong, T Pang, H Su, J Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Understanding adversarial attacks on deep learning based medical image analysis systems. X Ma, Y Niu, L Gu, Y Wang, Y Zhao, J Bailey, F Lu, Pattern Recognition. 1101073322021</p>
<p>Adversarial attack and defense technologies in natural language processing: A survey. S Qiu, Q Liu, S Zhou, W Huang, Neurocomputing. 4922022</p>
<p>Adversarial training for aspectbased sentiment analysis with bert. A Karimi, L Rossi, A Prati, 2020 25th International conference on pattern recognition (ICPR). IEEE2021</p>
<p>Freelb: Enhanced adversarial training for natural language understanding. C Zhu, Y Cheng, Z Gan, S Sun, T Goldstein, J Liu, arXiv:1909.117642019arXiv preprint</p>
<p>The limitations of adversarial training and the blind-spot attack. H Zhang, H Chen, Z Song, D Boning, I S Dhillon, C.-J Hsieh, arXiv:1901.046842019arXiv preprint</p>
<p>Defense against adversarial attacks using feature scattering-based adversarial training. H Zhang, J Wang, Advances in Neural Information Processing Systems. 201932</p>
<p>Aroid: Improving adversarial robustness through online instance-wise data augmentation. L Li, J Qiu, M Spratling, arXiv:2306.071972023arXiv preprint</p>
<p>Provable defenses against adversarial examples via the convex outer adversarial polytope. E Wong, Z Kolter, International conference on machine learning. PMLR2018</p>
<p>Certified defenses against adversarial examples. A Raghunathan, J Steinhardt, P Liang, arXiv:1801.093442018arXiv preprint</p>
<p>Explaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, arXiv:1412.65722014arXiv preprint</p>
<p>Ensemble adversarial training: Attacks and defenses. F Tramèr, A Kurakin, N Papernot, I Goodfellow, D Boneh, P Mcdaniel, arXiv:1705.072042017arXiv preprint</p>
<p>Overfitting in adversarially robust deep learning. L Rice, E Wong, Z Kolter, International Conference on Machine Learning. PMLR2020</p>
<p>On the robustness of the cvpr 2018 white-box adversarial example defenses. A Athalye, N Carlini, arXiv:1804.032862018arXiv preprint</p>
<p>Adversarial examples are not easily detected: Bypassing ten detection methods. N Carlini, D Wagner, Proceedings of the 10th ACM workshop on artificial intelligence and security. the 10th ACM workshop on artificial intelligence and security2017</p>
<p>Unbox the black-box for the medical explainable ai via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond. G Yang, Q Ye, J Xia, Information Fusion. 772022</p>
<p>Designing transparency for effective human-ai collaboration. M Vössing, N Kühl, M Lind, G Satzger, Information Systems Frontiers. 2432022</p>
<p>help me help the ai": Understanding how explainability can support human-ai interaction. S S Kim, E A Watkins, O Russakovsky, R Fong, A Monroy-Hernández, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Who is to blame for crashes involving autonomous vehicles? exploring blame attribution across the road transport system. E Pöllänen, G J Read, B R Lane, J Thompson, P M Salmon, Ergonomics. 6352020</p>
<p>Compensation at the crossroads: Autonomous vehicles and alternative victim compensation schemes. T H Pearl, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. the 2019 AAAI/ACM Conference on AI, Ethics, and Society2019</p>
<p>Sue my car not me: Products liability and accidents involving autonomous vehicles. J K Gurney, U. Ill. JL Tech. &amp; Pol'y. 2472013</p>
<p>Autonomous vehicles: No driver no regulation. J Claybrook, S Kildare, Science. 36163972018</p>
<p>Tell me what i need to know: Consumers desire for information transparency in self-driving vehicles. E W HuffJr, S Day Grady, J Brinnkley, Proceedings of the Human Factors and Ergonomics Society Annual Meeting. the Human Factors and Ergonomics Society Annual MeetingSage CA; Los Angeles, CASAGE Publications202165</p>
<p>Grad-cam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, Proceedings of the IEEE. the IEEE2017</p>
<p>Axiomatic attribution for deep networks. M Sundararajan, A Taly, Q Yan, International conference on machine learning. 2017</p>
<p>why should i trust you?" explaining the predictions of any classifier. M T Ribeiro, S Singh, C Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data mining2016</p>
<p>A unified approach to interpreting model predictions. S M Lundberg, S.-I Lee, Advances in neural information processing systems. 201730</p>
<p>Evaluating the visualization of what a deep neural network has learned. W Samek, A Binder, G Montavon, S Lapuschkin, K.-R Müller, IEEE transactions on neural networks and learning systems. 201628</p>
<p>Task-driven visual saliency and attention-based visual question answering. Y Lin, Z Pang, D Wang, Y Zhuang, arXiv:1702.067002017arXiv preprint</p>
<p>Grad-cam aware supervised attention for visual question answering for post-disaster damage assessment. A Sarkar, M Rahnemoonfar, 2022 IEEE International Conference on Image Processing (ICIP). IEEE2022</p>
<p>Smoothgrad: removing noise by adding noise. D Smilkov, N Thorat, B Kim, F Viégas, M Wattenberg, arXiv:1706.038252017arXiv preprint</p>
<p>When explanations lie: Why many modified bp attributions fail. L Sixt, M Granz, T Landgraf, International Conference on Machine Learning. PMLR2020</p>
<p>Evaluating saliency map explanations for convolutional neural networks: a user study. A Alqaraawi, M Schuessler, P Weiß, E Costanza, N Berthouze, Proceedings of the 25th international conference on intelligent user interfaces. the 25th international conference on intelligent user interfaces2020</p>
<p>Fooling lime and shap: Adversarial attacks on post hoc explanation methods. D Slack, S Hilgard, E Jia, S Singh, H Lakkaraju, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. the AAAI/ACM Conference on AI, Ethics, and Society2020</p>
<p>Noblis researchers apply explainable artificial intelligence (xai) to a covid-19 x-ray detection study. Openoblisnai, </p>
<p>Unmasking clever hans predictors and assessing what machines really learn. S Lapuschkin, S Wäldchen, A Binder, G Montavon, W Samek, K.-R Müller, Nature communications. 10110962019</p>
<p>Distilling a neural network into a soft decision tree. N Frosst, G Hinton, arXiv:1711.097842017arXiv preprint</p>
<p>Lightweight surrogate random forest support for model simplification and feature relevance. S Kim, M Jeong, B C Ko, Applied Intelligence. 5212022</p>
<p>Protoseg: Interpretable semantic segmentation with prototypical parts. M Sacha, D Rymarczyk, Ł Struski, J Tabor, B Zielin ´ski, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Deep learning for casebased reasoning through prototypes: A neural network that explains its predictions. O Li, H Liu, C Chen, C Rudin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>This looks like that: deep learning for interpretable image recognition. C Chen, O Li, D Tao, A Barnett, C Rudin, J K Su, Advances in neural information processing systems. 201932</p>
<p>Protopformer: Concentrating on prototypical parts in vision transformers for interpretable image recognition. M Xue, Q Huang, H Zhang, L Cheng, J Song, M Wu, M Song, arXiv:2208.104312022arXiv preprint</p>
<p>Transformer-based multi-prototype approach for diabetic macular edema analysis in oct images. P L Vidal, J De Moura, J Novo, M Ortega, J S Cardoso, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2023</p>
<p>Devise: A deep visual-semantic embedding model. A Frome, G S Corrado, J Shlens, S Bengio, J Dean, M Ranzato, T Mikolov, Advances in neural information processing systems. 201326</p>
<p>Evaluation of output embeddings for fine-grained image classification. Z Akata, S Reed, D Walter, H Lee, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>An embarrassingly simple approach to zero-shot learning. B Romera-Paredes, P Torr, International conference on machine learning. PMLR2015</p>
<p>Ridge regression, hubness, and zero-shot learning. Y Shigeto, I Suzuki, K Hara, M Shimbo, Y Matsumoto, Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015. Porto, PortugalSpringerSeptember 7-11, 2015. 2015Proceedings, Part I 15</p>
<p>Learning a deep embedding model for zero-shot learning. L Zhang, T Xiang, S Gong, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Learning deep representations of fine-grained visual descriptions. S Reed, Z Akata, H Lee, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. H Zhang, T Xu, H Li, S Zhang, X Wang, X Huang, D N Metaxas, Proceedings. null2017</p>
<p>Training generative adversarial networks by solving ordinary differential equations. C Qin, Y Wu, J T Springenberg, A Brock, J Donahue, T Lillicrap, P Kohli, Advances in Neural Information Processing Systems. 202033</p>
<p>Towards principled methods for training generative adversarial networks. M Arjovsky, L Bottou, arXiv:1701.048622017arXiv preprint</p>
<p>f-vaegan-d2: A feature generating framework for any-shot learning. Y Xian, S Sharma, B Schiele, Z Akata, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition201910284</p>
<p>Latent embedding feedback and discriminative features for zeroshot classification. S Narayan, A Gupta, F S Khan, C G Snoek, L Shao, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XXII 16</p>
<p>Bert-defense: A probabilistic model based on bert to combat cognitively inspired orthographic adversarial attacks. Y Keller, J Mackensen, S Eger, arXiv:2106.014522021arXiv preprint</p>
<p>Exploiting multi-object relationships for detecting adversarial attacks in complex scenes. M Yin, S Li, Z Cai, C Song, M S Asif, A K Roy-Chowdhury, S V Krishnamurthy, 2021in proceedings</p>
<p>Zero-query transfer attacks on context-aware object detectors. Z Cai, S Rane, A E Brito, C Song, S V Krishnamurthy, A K Roy-Chowdhury, M S Asif, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202234</p>
<p>Segmix: Co-occurrence driven mixup for semantic segmentation and adversarial robustness. M A Islam, M Kowal, K G Derpanis, N D Bruce, International Journal of Computer Vision. 13132023</p>
<p>Domain knowledge alleviates adversarial attacks in multi-label classifiers. S Melacci, G Ciravegna, A Sotgiu, A Demontis, B Biggio, M Gori, F Roli, IEEE Transactions on Pattern Analysis and Machine Intelligence. 44122021</p>
<p>Logic explained networks. G Ciravegna, P Barbiero, F Giannini, M Gori, P Lió, M Maggini, S Melacci, Artificial Intelligence. 3141038222023</p>
<p>Logicdef: An interpretable defense framework against adversarial examples via inductive scene graph reasoning. Y Yang, J C Kerce, F Fekri, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Conceptneta practical commonsense reasoning tool-kit. H Liu, P Singh, BT technology journal. 2242004</p>
<p>Improving certified robustness via statistical learning with logical reasoning. Z Yang, Z Zhao, B Wang, J Zhang, L Li, H Pei, B Karlaš, J Liu, H Guo, C Zhang, Advances in Neural Information Processing Systems. 202235</p>
<p>Care: Certifiably robust learning with reasoning via variational inference. J Zhang, L Li, C Zhang, B Li, 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). </p>
<p>Hierarchical structure in perceptual representation. S E Palmer, Cognitive psychology. 941977</p>
<p>Recognition-by-components: a theory of human image understanding. I Biederman, Psychological review. 9421151987</p>
<p>Objects, parts, and categories. B Tversky, K Hemenway, Journal of experimental psychology: General. 21691984</p>
<p>When will ai misclassify? intuiting failures on natural images. M Nartker, Z Zhou, C Firestone, Journal of Vision. 2342023</p>
<p>Recognizing object by components with human prior knowledge enhances adversarial robustness of deep neural networks. X Li, Z Wang, B Zhang, F Sun, X Hu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>Concept bottleneck models. P W Koh, T Nguyen, Y S Tang, S Mussmann, E Pierson, B Kim, P Liang, International conference on machine learning. PMLR2020</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). B Kim, M Wattenberg, J Gilmer, C Cai, J Wexler, F Viegas, International conference on machine learning. PMLR2018</p>
<p>Probabilistic concept bottleneck models. E Kim, D Jung, S Park, S Kim, S Yoon, arXiv:2306.015742023arXiv preprint</p>
<p>Interactive concept bottleneck models. K Chauhan, R Tiwari, J Freyberg, P Shenoy, K Dvijotham, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Label-free concept bottleneck models. T Oikarinen, S Das, L M Nguyen, T.-W Weng, arXiv:2304.061292023arXiv preprint</p>
<p>Interpretable model-agnostic plausibility verification for 2d object detectors using domain-invariant concept bottleneck models. M Keser, G Schwalbe, A Nowzad, A Knoll, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Interpretations are useful: penalizing explanations to align neural networks with prior knowledge. L Rieger, C Singh, W Murdoch, B Yu, International conference on machine learning. PMLR2020</p>
<p>Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations. W Stammer, P Schramowski, K Kersting, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Concept correlation and its effects on concept-based models. L Heidemann, M Monnet, K Roscher, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Towards automatic concept-based explanations. A Ghorbani, J Wexler, J Y Zou, B Kim, Advances in neural information processing systems. 201932</p>
<p>Explanatory interactive machine learning. S Teso, K Kersting, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. the 2019 AAAI/ACM Conference on AI, Ethics, and Society2019</p>
<p>Making deep neural networks right for the right scientific reasons by interacting with their explanations. P Schramowski, W Stammer, S Teso, A Brugger, F Herbert, X Shao, H.-G Luigs, A.-K Mahlein, K Kersting, Nature Machine Intelligence. 282020</p>
<p>Learning from exemplary explanations. M T Hagos, K M Curran, B Mac Namee, arXiv:2307.060262023arXiv preprint</p>
<p>Interactive deep learning for explainable retinal disease classification. M Vasquez, S Shakya, I Wang, J Furst, R Tchoua, D Raicu, Medical Imaging 2022: Image Processing. SPIE202212032</p>
<p>A survey on active learning and human-in-the-loop deep learning for medical image analysis. S Budd, E C Robinson, B Kainz, Medical Image Analysis. 711020622021</p>
<p>Learning what makes a difference from counterfactual examples and gradient supervision. D Teney, E Abbasnedjad, A Van Den, Hengel, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part X 16</p>
<p>Visfis: Visual feature importance supervision with right-for-the-right-reason objectives. Z Ying, P Hase, M Bansal, Advances in Neural Information Processing Systems. 20223572</p>
<p>Explainable fact checking with probabilistic answer set programming. N Ahmadi, J Lee, P Papotti, M Saeed, arXiv:1906.091982019arXiv preprint</p>
<p>Generating knowledge aware explanation for natural language inference. Z Yang, Y Xu, J Hu, S Dong, Information Processing &amp; Management. 6021032452023</p>
<p>Kr-gcn: Knowledge-aware reasoning with graph convolution network for explainable recommendation. T Ma, L Huang, Q Lu, S Hu, ACM Transactions on Information Systems. 4112023</p>
<p>A survey on knowledge graph-based recommender systems. Q Guo, F Zhuang, C Qin, H Zhu, X Xie, H Xiong, Q He, IEEE Transactions on Knowledge and Data Engineering. 3482020</p>
<p>Meta-path guided graph attention network for explainable herb recommendation. Y Jin, W Ji, Y Shi, X Wang, X Yang, Health Information Science and Systems. 11152023</p>
<p>Cancer omic data based explainable ai drug recommendation inference: A traceability perspective for explainability. J Xi, D Wang, X Yang, W Zhang, Q Huang, Biomedical Signal Processing and Control. 791041442023</p>
<p>Embedding electronic health records onto a knowledge network recognizes prodromal features of multiple sclerosis and predicts diagnosis. C A Nelson, R Bove, A J Butte, S E Baranzini, Journal of the American Medical Informatics Association. 2932022</p>
<p>Medical knowledge graph: Data sources, construction, reasoning, and applications. X Wu, J Duan, Y Pan, M Li, Big Data Mining and Analytics. 622023</p>
<p>Gram: graph-based attention model for healthcare representation learning. E Choi, M T Bahadori, L Song, W F Stewart, J Sun, Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2017</p>
<p>Kame: Knowledge-based attention model for diagnosis prediction in healthcare. F Ma, Q You, H Xiao, R Chitta, J Zhou, J Gao, Proceedings of the 27th ACM International Conference on Information and Knowledge Management. the 27th ACM International Conference on Information and Knowledge Management2018</p>
<p>Generating explanations for conceptual validation of graph neural networks: An investigation of symbolic predicates learned on relevance-ranked sub-graphs. B Finzel, A Saranti, A Angerschmid, D Tafler, B Pfeifer, A Holzinger, KI-Künstliche Intelligenz. 363-42022</p>
<p>Zero-shot classification by logical reasoning on natural language explanations. C Han, H Pei, X Du, H Ji, arXiv:2211.032522022arXiv preprint</p>
<p>Prototypical logic tensor networks (proto-ltn) for zero shot learning. S Martone, F Manigrasso, F Lamberti, L Morra, 2022 26th International Conference on Pattern Recognition (ICPR). </p>
<p>Logicguided semantic representation learning for zero-shot relation classification. J Li, R Wang, N Zhang, W Zhang, F Yang, H Chen, arXiv:2010.160682020arXiv preprint</p>
<p>Corl: Compositional representation learning for few-shot classification. J He, A Kortylewski, A Yuille, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Interpretable compositional representations for robust few-shot generalization. S Mishra, P Zhu, V Saligrama, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022</p>
<p>Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach. T Hamaguchi, H Oiwa, M Shimbo, Y Matsumoto, arXiv:1706.056742017arXiv preprint</p>
<p>Logic attention based neighborhood aggregation for inductive knowledge graph embedding. P Wang, J Han, C Li, R Pan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Meta-knowledge transfer for inductive knowledge graph embedding. M Chen, W Zhang, Y Zhu, H Zhou, Z Yuan, C Xu, H Chen, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>Stochastic and dual adversarial gan-boosted zero-shot knowledge graph. X Liu, Y Guo, M Huang, X Xiang, CAAI International Conference on Artificial Intelligence. Springer2022</p>
<p>Generative adversarial zero-shot relational learning for knowledge graphs. P Qin, X Wang, W Chen, C Zhang, W Xu, W Y Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Ontozsl: Ontology-enhanced zero-shot learning. Y Geng, J Chen, Z Chen, J Z Pan, Z Ye, Z Yuan, Y Jia, H Chen, Proceedings of the Web Conference 2021. the Web Conference 20212021</p>
<p>Disentangled ontology embedding for zero-shot learning. Y Geng, J Chen, W Zhang, Y Xu, Z Chen, J Z Pan, Y Huang, F Xiong, H Chen, Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining. the 28th ACM SIGKDD conference on knowledge discovery and data mining2022</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>L Yuan, D Chen, Y.-L Chen, N Codella, X Dai, J Gao, H Hu, X Huang, B Li, C Li, arXiv:2111.11432Florence: A new foundation model for computer vision. 2021arXiv preprint</p>
<p>Zero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, International Conference on Machine Learning. PMLR2021</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, arXiv:2304.02643Segment anything. 2023arXiv preprint</p>
<p>Vl-bert: Pre-training of generic visual-linguistic representations. W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, J Dai, arXiv:1908.085302019arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>An empirical study of gpt-3 for few-shot knowledge-based vqa. Z Yang, Z Gan, J Wang, X Hu, Y Lu, Z Liu, L Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>WordNet: An electronic lexical database. G A Miller, 1998Nouns in wordnet</p>
<p>Atomic: An atlas of machine commonsense for if-then reasoning. M Sap, R Le Bras, E Allaway, C Bhagavatula, N Lourie, H Rashkin, B Roof, N A Smith, Y Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Comet: Commonsense transformers for automatic knowledge graph construction. A Bosselut, H Rashkin, M Sap, C Malaviya, A Celikyilmaz, Y Choi, arXiv:1906.053172019arXiv preprint</p>
<p>Commonsense knowledge base completion with structural and semantic context. C Malaviya, C Bhagavatula, A Bosselut, Y Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Krisp: Integrating implicit and symbolic knowledge for opendomain knowledge-based vqa. K Marino, X Chen, D Parikh, A Gupta, M Rohrbach, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202114121</p>
<p>Kat: A knowledge augmented transformer for visionand-language. L Gui, B Wang, Q Huang, A Hauptmann, Y Bisk, J Gao, arXiv:2112.086142021arXiv preprint</p>
<p>Prompting large language models with answer heuristics for knowledge-based visual question answering. Z Shao, Z Yu, M Wang, J Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202314983</p>
<p>Wikidata: a free collaborative knowledgebase. D Vrandec ˇic ´, M Krötzsch, Communications of the ACM. 57102014</p>
<p>Vlc-bert: visual question answering with contextualized commonsense knowledge. S Ravi, A Chinchure, L Sigal, R Liao, V Shwartz, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. C Rudin, Nature machine intelligence. 152019</p>
<p>Interpretable machine learning: Fundamental principles and 10 grand challenges. C Rudin, C Chen, Z Chen, H Huang, L Semenova, C Zhong, Statistic Surveys. 162022</p>
<p>Its just not that simple: an empirical study of the accuracy-explainability trade-off in machine learning for public policy. A Bell, I Solano-Kamaiko, O Nov, J Stoyanovich, Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. the 2022 ACM Conference on Fairness, Accountability, and Transparency2022</p>
<p>Cgintrinsics: Better intrinsic image decomposition through physically-based rendering. Z Li, N Snavely, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Enforcing analytic constraints in neural networks emulating physical systems. T Beucler, M Pritchard, S Rasp, J Ott, P Baldi, P Gentine, Physical Review Letters. 1269983022021</p>
<p>Global explainability of gnns via logic combination of learned concepts. S Azzolin, A Longa, P Barbiero, P Liò, A Passerini, arXiv:2210.071472022arXiv preprint</p>
<p>Concept bottleneck model with additional unsupervised concepts. Y Sawada, K Nakamura, IEEE Access. 107652022</p>
<p>M E Zarlenga, P Barbiero, G Ciravegna, G Marra, F Giannini, M Diligenti, Z Shams, F Precioso, S Melacci, A Weller, arXiv:2209.09056Concept embedding models. 2022arXiv preprint</p>
<p>Learning to intervene on concept bottlenecks. D Steinmann, W Stammer, F Friedrich, K Kersting, arXiv:2308.134532023arXiv preprint</p>
<p>Caipi in practice: towards explainable interactive medical image classification. E Slany, Y Ott, S Scheele, J Paulus, U Schmid, IFIP International Conference on Artificial Intelligence Applications and Innovations. Springer2022</p>
<p>Learning intents behind interactions with knowledge graph for recommendation. X Wang, T Huang, D Wang, Y Yuan, Z Liu, X He, T.-S Chua, Proceedings of the web conference 2021. the web conference 20212021</p>
<p>Excar: Event graph knowledge enhanced explainable causal reasoning. L Du, X Ding, K Xiong, T Liu, B Qin, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Knowledge enhanced machine learning pipeline against diverse adversarial attacks. N M Gürel, X Qi, L Rimanic, C Zhang, B Li, International Conference on Machine Learning. PMLR2021</p>
<p>Beyond onehot-encoding: Injecting semantics to drive image classifiers. A Perotti, S Bertolotto, E Pastor, A Panisson, World Conference on Explainable Artificial Intelligence. Springer2023</p>
<p>Ontology-enhanced prompt-tuning for few-shot learning. H Ye, N Zhang, S Deng, X Chen, H Chen, F Xiong, X Chen, H Chen, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022</p>
<p>Context-aware zero-shot recognition. R Luo, N Zhang, B Han, L Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Rethinking knowledge graph propagation for zero-shot learning. M Kampffmeyer, Y Chen, X Liang, H Wang, Y Zhang, E P Xing, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition201911496</p>
<p>From node to graph: Joint reasoning on visual-semantic relational graph for zero-shot detection. H Nie, R Wang, X Chen, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2022</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. I Kotseruba, J K Tsotsos, Artificial Intelligence Review. 5312020</p>
<p>An overview of the epic architecture for cognition and performance with application to humancomputer interaction. D E Kieras, D E Meyer, Human-Computer Interaction. 1241997</p>
<p>Generative music, cognitive modelling, and computer-assisted composition in musicog and manuscore. J B Maxwell, 2014</p>
<p>Adaptive resonance theory. G A Carpenter, S Grossberg, 2010</p>
<p>Multilevel darwinist brain (mdb): Artificial evolution in a cognitive architecture for real robots. F Bellas, R J Duro, A Faiña, D Souto, IEEE Transactions on autonomous mental development. 242010</p>
<p>Towards a theory of the laminar architecture of cerebral cortex: Computational clues from the visual system. R D Raizada, S Grossberg, Cerebral cortex. 1312003</p>
<p>Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain. R C O'reilly, Y Munakata, 2000MIT press</p>
<p>Envisioning cognitive robots for future space exploration. T Huntsberger, A Stoica, Multisensor, Multisource Information Fusion: Architectures, Algorithms, and Applications 2010. SPIE2010771077100D</p>
<p>The autonomous maritime navigation (amn) project: Field tests, autonomous and cooperative behaviors, data fusion, sensors, and vehicles. L Elkins, D Sellers, W R Monach, Journal of Field Robotics. 2762010</p>
<p>icub: the design and realization of an open humanoid platform for cognitive and neuroscience research. N G Tsagarakis, G Metta, G Sandini, D Vernon, R Beira, F Becchi, L Righetti, J Santos-Victor, A J Ijspeert, M C Carrozza, Advanced Robotics. 21102007</p>
<p>A mind-inspired architecture for adaptive hri. A Umbrico, R De Benedictis, F Fracasso, A Cesta, A Orlandini, G Cortellessa, International Journal of Social Robotics. 1532023</p>
<p>Integrating open-ended learning in the sense-plan-act robot control paradigm. A Oddi, R Rasconi, V G Santucci, G Sartor, E Cartoni, F Mannella, G Baldassarre, ECAI 2020. IOS Press2020</p>
<p>In two minds: dual-process accounts of reasoning. J S B Evans, Trends in cognitive sciences. 7102003</p>
<p>Thinking, fast and slow. D Kahneman, 2011macmillan</p>
<p>Combining fast and slow thinking for human-like and efficient navigation in constrained environments. M B Ganapini, M Campbell, F Fabiano, L Horesh, J Lenchner, A Loreggia, N Mattei, T Rahgooy, F Rossi, B Srivastava, arXiv:2201.070502022arXiv preprint</p>
<p>Fast and slow planning. F Fabiano, V Pallagani, M B Ganapini, L Horesh, A Loreggia, K Murugesan, F Rossi, B Srivastava, arXiv:2303.042832023arXiv preprint</p>
<p>The bayesian brain: the role of uncertainty in neural coding and computation. D C Knill, A Pouget, TRENDS in Neurosciences. 27122004</p>
<p>Dac-h3: a proactive robot cognitive architecture to acquire and express knowledge about the world and the self. C Moulin-Frier, T Fischer, M Petit, G Pointeau, J.-Y Puigbo, U Pattacini, S C Low, D Camilleri, P Nguyen, M Hoffmann, IEEE Transactions on Cognitive and Developmental Systems. 1042017</p>
<p>Combining different v1 brain model variants to improve robustness to image corruptions in cnns. A Baidya, J Dapello, J J Dicarlo, T Marques, arXiv:2110.106452021arXiv preprint</p>
<p>Towards robust vision by multi-task learning on monkey visual cortex. S Safarani, A Nix, K Willeke, S Cadena, K Restivo, G Denfield, A Tolias, F Sinz, Advances in Neural Information Processing Systems. 202134</p>
<p>Imagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Using deep convolutional neural networks to test why human face recognition works the way it does. K Dobs, J Yuan, J Martinez, N Kanwisher, 2022</p>
<p>Extracting lowdimensional psychological representations from convolutional neural networks. A Jha, J C Peterson, T L Griffiths, Cognitive science. 471e132262023</p>
<p>Looking at upside-down faces. R K Yin, Journal of experimental psychology. 8111411969</p>
<p>What is" special" about face perception?. M J Farah, K D Wilson, M Drain, J N Tanaka, Psychological review. 10534821998</p>
<p>The distributed human neural system for face perception. J V Haxby, E A Hoffman, M I Gobbini, Trends in cognitive sciences. 462000</p>
<p>Evaluating (and improving) the correspondence between deep neural networks and human representations. J C Peterson, J T Abbott, T L Griffiths, Cognitive science. 4282018</p>
<p>L Muttenthaler, L Linhardt, J Dippel, R A Vandermeulen, K Hermann, A K Lampinen, S Kornblith, arXiv:2306.04507Improving neural network representations using human similarity judgments. 2023arXiv preprint</p>
<p>Alignment with human representations supports robust few-shot learning. I Sucholutsky, T L Griffiths, arXiv:2301.119902023arXiv preprint</p>
<p>Towards a dual process approach to computational explanation in human-robot social interaction. A Augello, I Infantino, A Lieto, U Maniscalco, G Pilato, F Vella, Proceedings of the 1st CAID workshop at IJCAI. the 1st CAID workshop at IJCAI2017</p>
<p>A hybrid driving decisionmaking system integrating markov logic networks and connectionist ai. M Wu, F R Yu, P X Liu, Y He, IEEE Transactions on Intelligent Transportation Systems. 2432022</p>
<p>Cognitive salience of features in cyber-attacker decision making. E A Cranford, S Somers, K Mitsopoulos, C Lebiere, Proceedings of the 18th annual meeting of the international conference on cognitive modeling. the 18th annual meeting of the international conference on cognitive modelingUniversity Park, PA2020Applied Cognitive Science Lab, Penn State</p>
<p>Using deep learning to model the hierarchical structure and function of a cell. J Ma, M K Yu, S Fong, K Ono, E Sage, B Demchak, R Sharan, T Ideker, Nature methods. 1542018</p>
<p>Deep neural network prediction of genome-wide transcriptome signaturesbeyond the black-box. R Magnusson, J N Tegnér, M Gustafsson, npj Systems Biology and Applications. 8192022</p>
<p>A systematic review of biologically-informed deep learning models for cancer: fundamental trends for encoding and interpreting oncology data. M Wysocka, O Wysocki, M Zufferey, D Landers, A Freitas, BMC bioinformatics. 2412023</p>
<p>Biologically informed variational autoencoders allow predictive modeling of genetic and drug-induced perturbations. D Doncevic, C Herrmann, Bioinformatics. 3963872023</p>
<p>Very deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.15562014arXiv preprint</p>
<p>Improved prediction of behavioral and neural similarity spaces using pruned dnns. P Tarigopula, S L Fairhall, A Bavaresco, N Truong, U , Neural Networks. 1682023Hasson</p>
<p>The clarion cognitive architecture: Extending cognitive modeling to social simulation. R Sun, 2006Cognition and multi-agent interaction</p>
<p>A repetition-suppression account of between-trial effects in a modified stroop paradigm. I Juvina, N A Taatgen, Acta psychologica. 13112009</p>
<p>A step-by-step tutorial on using the cognitive architecture act-r in combination with fmri data. J P Borst, J R Anderson, Journal of Mathematical Psychology. 762017</p>
<p>Brain-inspired predictive coding improves the performance of machine challenging tasks. J Lee, J Jo, B Lee, J.-H Lee, S Yoon, Frontiers in Computational Neuroscience. 1610626782022</p>
<p>Synaptic plasticity: taming the beast. L F Abbott, S B Nelson, Nature neuroscience. 3112000</p>
<p>Connectivity reflects coding: a model of voltage-based stdp with homeostasis. C Clopath, L Büsing, E Vasilaki, W Gerstner, Nature neuroscience. 1332010</p>
<p>Gradients without backpropagation. A G Baydin, B A Pearlmutter, D Syme, F Wood, P Torr, arXiv:2202.085872022arXiv preprint</p>
<p>An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. J C Whittington, R Bogacz, Neural computation. 2952017</p>
<p>Theories of error back-propagation in the brain. Trends in cognitive sciences. 2332019</p>
<p>Predictive coding: towards a future of deep learning beyond backpropagation. B Millidge, T Salvatori, Y Song, R Bogacz, T Lukasiewicz, arXiv:2202.094672022arXiv preprint</p>
<p>Dense associative memory for pattern recognition. D Krotov, J J Hopfield, Advances in neural information processing systems. 201629</p>
<p>Prediction and memory: A predictive coding account. H C Barron, R Auksztulewicz, K Friston, Progress in neurobiology. 1921018212020</p>
<p>Associative memories via predictive coding. T Salvatori, Y Song, Y Hong, L Sha, S Frieder, Z Xu, R Bogacz, T Lukasiewicz, Advances in Neural Information Processing Systems. 202134</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. M Mccloskey, N J Cohen, Psychology of learning and motivation. Elsevier198924</p>
<p>Boundary vector cells in the subiculum of the hippocampal formation. C Lever, S Burton, A Jeewajee, J O'keefe, N Burgess, Journal of Neuroscience. 29312009</p>
<p>Neural inhibition for continual learning and memory. H C Barron, Current opinion in neurobiology. 672021</p>
<p>Learn to pay attention. S Jetley, N A Lord, N Lee, P H Torr, arXiv:1804.023912018arXiv preprint</p>
<p>Attention u-net: Learning where to look for the pancreas. O Oktay, J Schlemper, L L Folgoc, M Lee, M Heinrich, K Misawa, K Mori, S Mcdonagh, N Y Hammerla, B Kainz, arXiv:1804.039992018arXiv preprint</p>
<p>Brainclip: Bridging brain and visual-linguistic representation via clip for generic natural visual stimulus decoding from fmri. Y Liu, Y Ma, W Zhou, G Zhu, N Zheng, arXiv:2302.129712023arXiv preprint</p>
<p>Scaling up visual and visionlanguage representation learning with noisy text supervision. C Jia, Y Yang, Y Xia, Y.-T Chen, Z Parekh, H Pham, Q Le, Y.-H Sung, Z Li, T Duerig, International conference on machine learning. PMLR2021</p>
<p>Learning to prompt for vision-language models. K Zhou, J Yang, C C Loy, Z Liu, International Journal of Computer Vision. 13092022</p>
<p>Mapping between fmri responses to movies and their natural language annotations. K Vodrahalli, P.-H Chen, Y Liang, C Baldassano, J Chen, E Yong, C Honey, U Hasson, P Ramadge, K A Norman, NeuroImage. 1802018</p>
<p>Decoding naturalistic experiences from human brain activity via distributed representations of words. S Nishida, S Nishimoto, Neuroimage. 1802018</p>
<p>Toward a universal decoder of linguistic meaning from brain activation. F Pereira, B Lou, B Pritchett, S Ritter, S J Gershman, N Kanwisher, M Botvinick, E Fedorenko, Nature communications. 919632018</p>
<p>Generic decoding of seen and imagined objects using hierarchical visual features. T Horikawa, Y Kamitani, Nature communications. 81150372017</p>
<p>Mind and its evolution: A dual coding theoretical approach. A Paivio, 2014Psychology Press</p>
<p>For whom is a picture worth a thousand words? extensions of a dual-coding theory of multimedia learning. R E Mayer, V K Sims, Journal of educational psychology. 8633891994</p>
<p>Reconstructing the mind's eye: fmri-to-image with contrastive learning and diffusion priors. P S Scotti, A Banerjee, J Goode, S Shabalin, A Nguyen, E Cohen, A J Dempster, N Verlinde, E Yundler, D Weisberg, arXiv:2305.182742023arXiv preprint</p>
<p>Mind reader: Reconstructing complex images from brain activities. S Lin, T Sprague, A K Singh, Advances in Neural Information Processing Systems. 202235</p>
<p>Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion. F Ozcelik, R Vanrullen, arXiv:2303.053342023arXiv preprint</p>
<p>Towards digital cognitive clones for the decision-makers: adversarial training experiments. M Golovianko, S Gryshko, V Terziyan, T Tuunanen, Procedia Computer Science. 1802021</p>
<p>Digital clones and digital immunity: adversarial training handles both. V Branytskyi, M Golovianko, S Gryshko, D Malyk, V Terziyan, T Tuunanen, International Journal of Simulation and Process Modelling. 1822022</p>
<p>Interleaving fast and slow decision making. A Gulati, S Soni, S Rao, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>attention"!: Aldous huxley's epistemological route to salvation. F W Conner, The Sewanee Review. 8121973</p>
<p>On the fragility of skilled performance: What governs choking under pressure?. S L Beilock, T H Carr, Journal of experimental psychology: General. 13047012001</p>
<p>Exploring the brain activity related to missing penalty kicks: An fnirs study. M W Slutter, N Thammasan, M Poel, Frontiers in Computer Science. 3322021</p>            </div>
        </div>

    </div>
</body>
</html>
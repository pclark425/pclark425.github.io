<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3874 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3874</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3874</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-cccb129527fbe900c6f94dd8a6ad4b408c403152</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cccb129527fbe900c6f94dd8a6ad4b408c403152" target="_blank">Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study</a></p>
                <p><strong>Paper Venue:</strong> Journal of Medical Internet Research</p>
                <p><strong>Paper TL;DR:</strong> Large language models such as GPT-4 have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews.</p>
                <p><strong>Paper Abstract:</strong> Background The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence and bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3874.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3874.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-screen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT API-based Automated Title/Abstract Screening Workflow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python app/workflow that calls the OpenAI ChatGPT/GPT-4 API with plain‑language inclusion/exclusion criteria and individual title+abstract items to perform zero‑shot include/exclude classification and (on demand) provide free‑text reasoning and reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>GPT API-based screening workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A Python script that iterates over title+abstract records, constructs a natural-language prompt containing the dataset-specific inclusion and exclusion criteria, and calls the OpenAI ChatGPT/GPT-4 API to return a single token decision ('included' or 'excluded'). Optional appended prompts request the model's reasoning for decisions and reflection when a decision is incorrect. No additional model fine-tuning or embedding/indexing was performed; the approach is zero-shot prompting / plain prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Six real-world systematic/scoping review screening datasets previously labeled by ≥2 human reviewers containing 24,307 title+abstract records total (538 included). Datasets and sizes: IVM 279 (35 included), SSRI 3,989 (29 included), LPVR 1,456 (91 included), RAYNAUDS 942 (6 included), NOA 14,771 (354 included), LLM 2,870 (23 included). Domain: clinical medicine (COVID-19 therapeutics, Raynaud's, perioperative analgesia, and a scoping review of LLMs); language: English; records preprocessed to remove non‑English characters.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>For each dataset the human-specified inclusion and exclusion criteria (plain text strings) were supplied in the prompt together with the title and abstract. Instruction prompt (zero-shot) told the model: 'If any exclusion criteria are met or not all inclusion criteria are met, exclude the article. If all inclusion criteria are met, include the article. Only type "included" or "excluded".'</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot large language model classification via prompt engineering on individual title+abstract inputs; optional appended prompts used to elicit chain‑of‑thought style reasoning ('Explain your reasoning...') and reflection on incorrect classifications ('Explain why the decision given was incorrect...'). No retrieval augmentation, no fine-tuning, no embedding-based retrieval was used.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Primary output: single-word binary label 'included' or 'excluded' per title+abstract. Secondary/optional outputs: free-text explanation/reasoning for the decision and a reflective explanation when asked to analyze incorrect classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Comparison against human ground truth (decisions obtained from two independent human screeners with conflicts resolved by consensus). Metrics computed: accuracy, macro F1-score, sensitivity for included/excluded classes, Cohen's kappa (human interrater and GPT vs consensus), and prevalence‑adjusted and bias‑adjusted kappa (PABAK). Subsets of examples were also evaluated qualitatively by inspecting GPT reasoning and its ability to revise/refine decisions when asked to reflect.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Overall weighted accuracy 0.907 (total weighted), macro F1-score 0.600, sensitivity for included papers 0.764, sensitivity for excluded papers 0.910. Per-dataset accuracies ranged (examples): NOA accuracy 0.895 (354/14,771 included), RAYNAUDS 0.965, LPVR 0.949. Human interrater κ=0.46; PABAK between GPT decisions and consensus = 0.96. On the NOA corpus runtime ~643 minutes and approximate cost US$25.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lower sensitivity for included items (missed relevant studies) relative to excluded items; class imbalance caused low Cohen's κ despite high accuracy; three of six datasets focused on COVID-19 which may limit generalizability; token length limits constrain using full texts; cost and latency for large corpora; approach is zero-shot and may benefit from few‑shot or fine‑tuning; authors caution the method should be an aid not a replacement for human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared directly to human screening consensus (ground truth): high agreement by PABAK (0.96) though lower Cohen's κ (0.22 between GPT and final decisions in some tables due to imbalance). Paper compares aggregate performance qualitatively/numerically to prior semi-automated tools (Abstrackr, DistillerSR, RobotAnalyst) reporting similar ranges of sensitivity/missed-record rates drawn from prior studies; authors note GPT achieved comparable overall accuracy (0.91) on the 24,307-record corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3874.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system to assess risk of bias in clinical trials using natural language processing methods to extract and evaluate trial characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automating biomedical evidence synthesis: RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>RobotReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An NLP system developed to extract trial metadata and to automatically assess risk-of-bias items from clinical trial reports, using trained classifiers and information extraction approaches (published 2016). The paper references RobotReviewer as a prior automation tool in evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to biomedical trial reports (MEDLINE and trial publications) in prior work; the current paper only cites RobotReviewer and does not run it.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not described in detail in this paper; RobotReviewer in its own publications uses pre-specified extraction targets (bias domains) and trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prior work uses conventional NLP and supervised learning classifiers rather than the zero-shot LLM prompting used in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Extracted bias judgments and supporting text spans; automated risk-of-bias assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Cited prior evaluation showing ~7% difference in accuracy versus human reviewers (from RobotReviewer publications). In this paper RobotReviewer is only mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an example of prior automation that achieved similar capabilities to human reviewers for risk-of-bias assessment (≈7% difference reported in original RobotReviewer paper). No new results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in detail here; cited as part of the broader context that previous tools used more conventional ML/NLP pipelines possibly requiring pretraining or labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Original RobotReviewer work compared to human assessments; in this paper RobotReviewer is referenced as a comparator in the space of automated evidence synthesis tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3874.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrialStreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TrialStreamer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that continuously extracts and indexes clinical trial reports and key trial elements from the literature to map and browse medical evidence in real time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trialstreamer: a living, automatically updated database of clinical trial reports</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>TrialStreamer</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An automated pipeline that extracts key elements (e.g., interventions, outcomes) from full text reports and indexes them into a searchable, continuously updated database for evidence mapping; cited as an example of automated evidence extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applies to full-text clinical trial reports and MEDLINE-indexed records (cited prior work); in this paper it is discussed in related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not implemented here; TrialStreamer targets trial mapping and browsing using extraction rules and ML classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Conventional NLP extraction and classification pipelines (not LLM-based in the citation); used to synthesize trial-level evidence across many documents.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured extractions of trial metadata and an indexed database enabling searches and comparisons across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Prior TrialStreamer publications evaluated extraction accuracy and system coverage; only cited here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an example of a system that extracts key elements of information from full texts and infers comparative outcomes across indexed trials.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in detail in this paper; included to position the GPT screening workflow relative to other automation efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3874.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstrackr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstrackr</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-automated web tool that uses machine learning to learn from reviewer decisions to prioritize and assist title/abstract screening for systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Abstrackr</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A tool that learns from human screening labels (supervised learning) to prioritize and semi-automate abstract screening; uses non‑LLM NLP features (historically n‑grams, traditional ML) and interactive reviewer feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to systematic-review title/abstract corpora of varying sizes (examples cited: datasets ranging from ~5,243 to 47,385 records in comparative evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Reviewer-provided inclusion/exclusion decisions are used as labels; the tool learns patterns to prioritize remaining records.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised ML trained on labeled screening decisions to predict inclusion probability and prioritize records; not a zero-shot LLM approach.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Prioritized lists and predicted inclusion/exclusion probabilities; reviewer-in-the-loop labels remain primary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Cited evaluations report sensitivities (e.g., 0.96, 0.79, 0.92, 0.82 across datasets) and reductions in reviewer workload; referenced in the paper's related work and comparative discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported high specificity and workload reductions in prior studies; paper cites Abstrackr's sensitivities and low missed-record rates as benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May still miss studies if fully automated; requires labeled data and training; uses older NLP techniques compared to modern LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3874.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rayyan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rayyan</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A web and mobile application that assists systematic review screening via semi‑automated text‑mining features and reviewer interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rayyan-a web and mobile app for systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Rayyan</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Interactive screening platform that supports blinded duplicate screening, tagging, and semi-automated prioritization via text-mining algorithms (learns reviewer decisions to predict inclusion/exclusion). Historically employs classical NLP/text-mining rather than modern LLM zero‑shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Used on systematic review title/abstract corpora of many sizes; prior user studies reported on workload reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Uses reviewer actions and keywords; not an LLM-driven natural language prompt as used in the GPT workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised/semi-supervised text-mining that learns from reviewer labels to prioritize records.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Prioritized abstracts and suggestions for inclusion/exclusion; visual interface for reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Prior user studies cited showing high specificity and that 98% of relevant articles were included after screening ~75% of records in one study; cited in related-work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an effective semi-automated tool that reduced workload while risking minimal missed records when used with human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential risk of missing studies if used fully autonomously; uses older methods relative to LLM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3874.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistillerSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistillerSR (machine-learning prioritization tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial systematic-review platform with ML-based prioritization for title/abstract screening referenced as a prior automation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DistillerSR prioritization</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A machine-learning prioritization module embedded in the DistillerSR review platform that ranks records for screening based on learned patterns from labeled examples; cited as a comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied in prior evaluations on proprietary and public title/abstract datasets; not run in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Learns from labeled reviewer decisions rather than receiving explicit natural-language inclusion criteria in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised prioritization models (platform-specific); not LLM zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Prioritized lists; reviewer-facing ranking to speed screening.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Prior studies report variable missed-record proportions (including high missed proportions up to 100% in some datasets cited); used here as contextual comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned alongside other tools with mixed reported performance in prior comparative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance appears dataset-dependent; potential to miss relevant records if misapplied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3874.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobotAnalyst</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RobotAnalyst</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool for prioritizing references for systematic reviews using machine-learning and user interaction, evaluated in user studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prioritising references for systematic reviews with RobotAnalyst: a user study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>RobotAnalyst</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Interactive reference-prioritization system that applies ML to rank records for screening with user studies reporting usability and performance; cited in related work and comparative discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied in prior user studies on medical title/abstract datasets; not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Learns from reviewer labels and interactions, not natural-language inclusion prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised/interactive ML ranking (non-LLM in prior publications).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Prioritized record lists and reviewer interface.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>User studies and comparative analyses in prior work; cited here for context.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Performance fell between Abstrackr and DistillerSR in cited comparative evaluations (missed proportions reported in referenced meta-analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dataset-dependent performance; potential to miss records if used without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3874.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot ML screening (Moreno-Garcia 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A novel application of machine learning and zero-shot classification methods for automated abstract screening</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study applying zero-shot classification approaches to automated abstract screening; referenced as an alternative to heavy labeling/pretraining approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A novel application of machine learning and zero-shot classification methods for automated abstract screening in Decis Anal J. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Zero-shot classification for abstract screening</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Paper referenced (Moreno-Garcia et al., 2023) that applies zero-shot classification methods to automated abstract screening to reduce labeling/pretraining needs; listed as prior related work that shares conceptual similarity to zero-shot LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Cited as applied to systematic review title/abstract corpora in prior work; specifics are in that cited paper, not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Zero-shot classifiers accept label descriptions or topic prompts to classify unseen categories without task-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot classification approaches (may use transformer-based models or other zero-shot architectures) — cited to motivate the GPT zero-shot prompting approach used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Predicted include/exclude labels or prioritization scores without supervised retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Referenced as a prior method; evaluation details are in the original paper (not produced here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as demonstrating feasibility of zero-shot approaches for abstract screening and as part of the literature motivating the authors' zero-shot GPT approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not detailed in this paper beyond noting prior approaches sometimes require labor-intensive labeling or pretraining; zero-shot methods are presented as an alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3874.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3874.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-scoping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assessing the research landscape and utility of LLMs in the clinical setting: protocol for a scoping review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scoping review protocol (used as one of the six screening datasets) that surveys research on LLMs in clinical medicine; included here as a dataset in which inclusion/exclusion labels were used to validate the GPT screening app.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing the research landscape and utility of LLMs in the clinical setting: protocol for a scoping review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM clinical-scoping review dataset</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>This entry is a scoping review protocol about LLMs in clinical settings; in the present paper the dataset derived from that review (titles/abstracts plus inclusion/exclusion criteria and human labels) was used as one of six corpora to evaluate the GPT screening workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Dataset labeled by human reviewers: LLM dataset contains 2,870 records with 23 included; domain: machine learning/LLMs in clinical medicine.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Standard scoping-review inclusion/exclusion criteria supplied in plain text to the GPT prompts for screening.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not an LLM system itself; its labeled screening corpus was used as test input for the GPT zero-shot screening method.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Used as ground-truth labeled examples (included/excluded) for evaluation; not a synthesizer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Served as one of the datasets against which GPT decisions were compared using accuracy, F1, sensitivity, kappa, and PABAK.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>On this particular dataset GPT achieved accuracy 0.943, macro F1-score 0.594, sensitivity included 1.000, sensitivity excluded 0.942 (as reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This dataset represents one domain (LLMs in clinical medicine) and is small (23 included), so generalizability is limited; nonetheless it demonstrates the method's applicability across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automating biomedical evidence synthesis: RobotReviewer <em>(Rating: 2)</em></li>
                <li>Trialstreamer: a living, automatically updated database of clinical trial reports <em>(Rating: 2)</em></li>
                <li>Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool <em>(Rating: 2)</em></li>
                <li>Rayyan-a web and mobile app for systematic reviews <em>(Rating: 2)</em></li>
                <li>An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes <em>(Rating: 2)</em></li>
                <li>Prioritising references for systematic reviews with RobotAnalyst: a user study <em>(Rating: 2)</em></li>
                <li>A novel application of machine learning and zero-shot classification methods for automated abstract screening <em>(Rating: 2)</em></li>
                <li>Fine-tuning a classifier to improve truthfulness <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3874",
    "paper_id": "paper-cccb129527fbe900c6f94dd8a6ad4b408c403152",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "GPT-screen",
            "name_full": "GPT API-based Automated Title/Abstract Screening Workflow",
            "brief_description": "A Python app/workflow that calls the OpenAI ChatGPT/GPT-4 API with plain‑language inclusion/exclusion criteria and individual title+abstract items to perform zero‑shot include/exclude classification and (on demand) provide free‑text reasoning and reflection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "GPT API-based screening workflow",
            "system_or_method_description": "A Python script that iterates over title+abstract records, constructs a natural-language prompt containing the dataset-specific inclusion and exclusion criteria, and calls the OpenAI ChatGPT/GPT-4 API to return a single token decision ('included' or 'excluded'). Optional appended prompts request the model's reasoning for decisions and reflection when a decision is incorrect. No additional model fine-tuning or embedding/indexing was performed; the approach is zero-shot prompting / plain prompt engineering.",
            "input_corpus_description": "Six real-world systematic/scoping review screening datasets previously labeled by ≥2 human reviewers containing 24,307 title+abstract records total (538 included). Datasets and sizes: IVM 279 (35 included), SSRI 3,989 (29 included), LPVR 1,456 (91 included), RAYNAUDS 942 (6 included), NOA 14,771 (354 included), LLM 2,870 (23 included). Domain: clinical medicine (COVID-19 therapeutics, Raynaud's, perioperative analgesia, and a scoping review of LLMs); language: English; records preprocessed to remove non‑English characters.",
            "topic_or_query_specification": "For each dataset the human-specified inclusion and exclusion criteria (plain text strings) were supplied in the prompt together with the title and abstract. Instruction prompt (zero-shot) told the model: 'If any exclusion criteria are met or not all inclusion criteria are met, exclude the article. If all inclusion criteria are met, include the article. Only type \"included\" or \"excluded\".'",
            "distillation_method": "Zero-shot large language model classification via prompt engineering on individual title+abstract inputs; optional appended prompts used to elicit chain‑of‑thought style reasoning ('Explain your reasoning...') and reflection on incorrect classifications ('Explain why the decision given was incorrect...'). No retrieval augmentation, no fine-tuning, no embedding-based retrieval was used.",
            "output_type_and_format": "Primary output: single-word binary label 'included' or 'excluded' per title+abstract. Secondary/optional outputs: free-text explanation/reasoning for the decision and a reflective explanation when asked to analyze incorrect classifications.",
            "evaluation_or_validation_method": "Comparison against human ground truth (decisions obtained from two independent human screeners with conflicts resolved by consensus). Metrics computed: accuracy, macro F1-score, sensitivity for included/excluded classes, Cohen's kappa (human interrater and GPT vs consensus), and prevalence‑adjusted and bias‑adjusted kappa (PABAK). Subsets of examples were also evaluated qualitatively by inspecting GPT reasoning and its ability to revise/refine decisions when asked to reflect.",
            "results_summary": "Overall weighted accuracy 0.907 (total weighted), macro F1-score 0.600, sensitivity for included papers 0.764, sensitivity for excluded papers 0.910. Per-dataset accuracies ranged (examples): NOA accuracy 0.895 (354/14,771 included), RAYNAUDS 0.965, LPVR 0.949. Human interrater κ=0.46; PABAK between GPT decisions and consensus = 0.96. On the NOA corpus runtime ~643 minutes and approximate cost US$25.",
            "limitations_or_challenges": "Lower sensitivity for included items (missed relevant studies) relative to excluded items; class imbalance caused low Cohen's κ despite high accuracy; three of six datasets focused on COVID-19 which may limit generalizability; token length limits constrain using full texts; cost and latency for large corpora; approach is zero-shot and may benefit from few‑shot or fine‑tuning; authors caution the method should be an aid not a replacement for human reviewers.",
            "comparison_to_baselines_or_humans": "Compared directly to human screening consensus (ground truth): high agreement by PABAK (0.96) though lower Cohen's κ (0.22 between GPT and final decisions in some tables due to imbalance). Paper compares aggregate performance qualitatively/numerically to prior semi-automated tools (Abstrackr, DistillerSR, RobotAnalyst) reporting similar ranges of sensitivity/missed-record rates drawn from prior studies; authors note GPT achieved comparable overall accuracy (0.91) on the 24,307-record corpus.",
            "uuid": "e3874.0",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RobotReviewer",
            "name_full": "RobotReviewer",
            "brief_description": "An automated system to assess risk of bias in clinical trials using natural language processing methods to extract and evaluate trial characteristics.",
            "citation_title": "Automating biomedical evidence synthesis: RobotReviewer",
            "mention_or_use": "mention",
            "system_or_method_name": "RobotReviewer",
            "system_or_method_description": "An NLP system developed to extract trial metadata and to automatically assess risk-of-bias items from clinical trial reports, using trained classifiers and information extraction approaches (published 2016). The paper references RobotReviewer as a prior automation tool in evidence synthesis.",
            "input_corpus_description": "Applied to biomedical trial reports (MEDLINE and trial publications) in prior work; the current paper only cites RobotReviewer and does not run it.",
            "topic_or_query_specification": "Not described in detail in this paper; RobotReviewer in its own publications uses pre-specified extraction targets (bias domains) and trained models.",
            "distillation_method": "Prior work uses conventional NLP and supervised learning classifiers rather than the zero-shot LLM prompting used in this study.",
            "output_type_and_format": "Extracted bias judgments and supporting text spans; automated risk-of-bias assessments.",
            "evaluation_or_validation_method": "Cited prior evaluation showing ~7% difference in accuracy versus human reviewers (from RobotReviewer publications). In this paper RobotReviewer is only mentioned in related work.",
            "results_summary": "Mentioned as an example of prior automation that achieved similar capabilities to human reviewers for risk-of-bias assessment (≈7% difference reported in original RobotReviewer paper). No new results in this paper.",
            "limitations_or_challenges": "Not discussed in detail here; cited as part of the broader context that previous tools used more conventional ML/NLP pipelines possibly requiring pretraining or labeled data.",
            "comparison_to_baselines_or_humans": "Original RobotReviewer work compared to human assessments; in this paper RobotReviewer is referenced as a comparator in the space of automated evidence synthesis tools.",
            "uuid": "e3874.1",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "TrialStreamer",
            "name_full": "TrialStreamer",
            "brief_description": "A system that continuously extracts and indexes clinical trial reports and key trial elements from the literature to map and browse medical evidence in real time.",
            "citation_title": "Trialstreamer: a living, automatically updated database of clinical trial reports",
            "mention_or_use": "mention",
            "system_or_method_name": "TrialStreamer",
            "system_or_method_description": "An automated pipeline that extracts key elements (e.g., interventions, outcomes) from full text reports and indexes them into a searchable, continuously updated database for evidence mapping; cited as an example of automated evidence extraction.",
            "input_corpus_description": "Applies to full-text clinical trial reports and MEDLINE-indexed records (cited prior work); in this paper it is discussed in related work only.",
            "topic_or_query_specification": "Not implemented here; TrialStreamer targets trial mapping and browsing using extraction rules and ML classifiers.",
            "distillation_method": "Conventional NLP extraction and classification pipelines (not LLM-based in the citation); used to synthesize trial-level evidence across many documents.",
            "output_type_and_format": "Structured extractions of trial metadata and an indexed database enabling searches and comparisons across trials.",
            "evaluation_or_validation_method": "Prior TrialStreamer publications evaluated extraction accuracy and system coverage; only cited here.",
            "results_summary": "Mentioned as an example of a system that extracts key elements of information from full texts and infers comparative outcomes across indexed trials.",
            "limitations_or_challenges": "Not discussed in detail in this paper; included to position the GPT screening workflow relative to other automation efforts.",
            "uuid": "e3874.2",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Abstrackr",
            "name_full": "Abstrackr",
            "brief_description": "A semi-automated web tool that uses machine learning to learn from reviewer decisions to prioritize and assist title/abstract screening for systematic reviews.",
            "citation_title": "Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool",
            "mention_or_use": "mention",
            "system_or_method_name": "Abstrackr",
            "system_or_method_description": "A tool that learns from human screening labels (supervised learning) to prioritize and semi-automate abstract screening; uses non‑LLM NLP features (historically n‑grams, traditional ML) and interactive reviewer feedback.",
            "input_corpus_description": "Applied to systematic-review title/abstract corpora of varying sizes (examples cited: datasets ranging from ~5,243 to 47,385 records in comparative evaluations).",
            "topic_or_query_specification": "Reviewer-provided inclusion/exclusion decisions are used as labels; the tool learns patterns to prioritize remaining records.",
            "distillation_method": "Supervised ML trained on labeled screening decisions to predict inclusion probability and prioritize records; not a zero-shot LLM approach.",
            "output_type_and_format": "Prioritized lists and predicted inclusion/exclusion probabilities; reviewer-in-the-loop labels remain primary.",
            "evaluation_or_validation_method": "Cited evaluations report sensitivities (e.g., 0.96, 0.79, 0.92, 0.82 across datasets) and reductions in reviewer workload; referenced in the paper's related work and comparative discussion.",
            "results_summary": "Reported high specificity and workload reductions in prior studies; paper cites Abstrackr's sensitivities and low missed-record rates as benchmarks.",
            "limitations_or_challenges": "May still miss studies if fully automated; requires labeled data and training; uses older NLP techniques compared to modern LLMs.",
            "uuid": "e3874.3",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Rayyan",
            "name_full": "Rayyan",
            "brief_description": "A web and mobile application that assists systematic review screening via semi‑automated text‑mining features and reviewer interaction.",
            "citation_title": "Rayyan-a web and mobile app for systematic reviews",
            "mention_or_use": "mention",
            "system_or_method_name": "Rayyan",
            "system_or_method_description": "Interactive screening platform that supports blinded duplicate screening, tagging, and semi-automated prioritization via text-mining algorithms (learns reviewer decisions to predict inclusion/exclusion). Historically employs classical NLP/text-mining rather than modern LLM zero‑shot prompting.",
            "input_corpus_description": "Used on systematic review title/abstract corpora of many sizes; prior user studies reported on workload reduction.",
            "topic_or_query_specification": "Uses reviewer actions and keywords; not an LLM-driven natural language prompt as used in the GPT workflow.",
            "distillation_method": "Supervised/semi-supervised text-mining that learns from reviewer labels to prioritize records.",
            "output_type_and_format": "Prioritized abstracts and suggestions for inclusion/exclusion; visual interface for reviewers.",
            "evaluation_or_validation_method": "Prior user studies cited showing high specificity and that 98% of relevant articles were included after screening ~75% of records in one study; cited in related-work discussion.",
            "results_summary": "Cited as an effective semi-automated tool that reduced workload while risking minimal missed records when used with human oversight.",
            "limitations_or_challenges": "Potential risk of missing studies if used fully autonomously; uses older methods relative to LLM approaches.",
            "uuid": "e3874.4",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DistillerSR",
            "name_full": "DistillerSR (machine-learning prioritization tool)",
            "brief_description": "A commercial systematic-review platform with ML-based prioritization for title/abstract screening referenced as a prior automation approach.",
            "citation_title": "An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes",
            "mention_or_use": "mention",
            "system_or_method_name": "DistillerSR prioritization",
            "system_or_method_description": "A machine-learning prioritization module embedded in the DistillerSR review platform that ranks records for screening based on learned patterns from labeled examples; cited as a comparator.",
            "input_corpus_description": "Applied in prior evaluations on proprietary and public title/abstract datasets; not run in this study.",
            "topic_or_query_specification": "Learns from labeled reviewer decisions rather than receiving explicit natural-language inclusion criteria in prompts.",
            "distillation_method": "Supervised prioritization models (platform-specific); not LLM zero-shot prompting.",
            "output_type_and_format": "Prioritized lists; reviewer-facing ranking to speed screening.",
            "evaluation_or_validation_method": "Prior studies report variable missed-record proportions (including high missed proportions up to 100% in some datasets cited); used here as contextual comparison.",
            "results_summary": "Mentioned alongside other tools with mixed reported performance in prior comparative analyses.",
            "limitations_or_challenges": "Performance appears dataset-dependent; potential to miss relevant records if misapplied.",
            "uuid": "e3874.5",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RobotAnalyst",
            "name_full": "RobotAnalyst",
            "brief_description": "A tool for prioritizing references for systematic reviews using machine-learning and user interaction, evaluated in user studies.",
            "citation_title": "Prioritising references for systematic reviews with RobotAnalyst: a user study",
            "mention_or_use": "mention",
            "system_or_method_name": "RobotAnalyst",
            "system_or_method_description": "Interactive reference-prioritization system that applies ML to rank records for screening with user studies reporting usability and performance; cited in related work and comparative discussion.",
            "input_corpus_description": "Applied in prior user studies on medical title/abstract datasets; not used in this paper's experiments.",
            "topic_or_query_specification": "Learns from reviewer labels and interactions, not natural-language inclusion prompts.",
            "distillation_method": "Supervised/interactive ML ranking (non-LLM in prior publications).",
            "output_type_and_format": "Prioritized record lists and reviewer interface.",
            "evaluation_or_validation_method": "User studies and comparative analyses in prior work; cited here for context.",
            "results_summary": "Performance fell between Abstrackr and DistillerSR in cited comparative evaluations (missed proportions reported in referenced meta-analysis).",
            "limitations_or_challenges": "Dataset-dependent performance; potential to miss records if used without human oversight.",
            "uuid": "e3874.6",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Zero-shot ML screening (Moreno-Garcia 2023)",
            "name_full": "A novel application of machine learning and zero-shot classification methods for automated abstract screening",
            "brief_description": "A cited study applying zero-shot classification approaches to automated abstract screening; referenced as an alternative to heavy labeling/pretraining approaches.",
            "citation_title": "A novel application of machine learning and zero-shot classification methods for automated abstract screening in Decis Anal J. 2023",
            "mention_or_use": "mention",
            "system_or_method_name": "Zero-shot classification for abstract screening",
            "system_or_method_description": "Paper referenced (Moreno-Garcia et al., 2023) that applies zero-shot classification methods to automated abstract screening to reduce labeling/pretraining needs; listed as prior related work that shares conceptual similarity to zero-shot LLM prompting.",
            "input_corpus_description": "Cited as applied to systematic review title/abstract corpora in prior work; specifics are in that cited paper, not reproduced here.",
            "topic_or_query_specification": "Zero-shot classifiers accept label descriptions or topic prompts to classify unseen categories without task-specific training.",
            "distillation_method": "Zero-shot classification approaches (may use transformer-based models or other zero-shot architectures) — cited to motivate the GPT zero-shot prompting approach used in this paper.",
            "output_type_and_format": "Predicted include/exclude labels or prioritization scores without supervised retraining.",
            "evaluation_or_validation_method": "Referenced as a prior method; evaluation details are in the original paper (not produced here).",
            "results_summary": "Cited as demonstrating feasibility of zero-shot approaches for abstract screening and as part of the literature motivating the authors' zero-shot GPT approach.",
            "limitations_or_challenges": "Not detailed in this paper beyond noting prior approaches sometimes require labor-intensive labeling or pretraining; zero-shot methods are presented as an alternative.",
            "uuid": "e3874.7",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-scoping",
            "name_full": "Assessing the research landscape and utility of LLMs in the clinical setting: protocol for a scoping review",
            "brief_description": "A scoping review protocol (used as one of the six screening datasets) that surveys research on LLMs in clinical medicine; included here as a dataset in which inclusion/exclusion labels were used to validate the GPT screening app.",
            "citation_title": "Assessing the research landscape and utility of LLMs in the clinical setting: protocol for a scoping review",
            "mention_or_use": "use",
            "system_or_method_name": "LLM clinical-scoping review dataset",
            "system_or_method_description": "This entry is a scoping review protocol about LLMs in clinical settings; in the present paper the dataset derived from that review (titles/abstracts plus inclusion/exclusion criteria and human labels) was used as one of six corpora to evaluate the GPT screening workflow.",
            "input_corpus_description": "Dataset labeled by human reviewers: LLM dataset contains 2,870 records with 23 included; domain: machine learning/LLMs in clinical medicine.",
            "topic_or_query_specification": "Standard scoping-review inclusion/exclusion criteria supplied in plain text to the GPT prompts for screening.",
            "distillation_method": "Not an LLM system itself; its labeled screening corpus was used as test input for the GPT zero-shot screening method.",
            "output_type_and_format": "Used as ground-truth labeled examples (included/excluded) for evaluation; not a synthesizer.",
            "evaluation_or_validation_method": "Served as one of the datasets against which GPT decisions were compared using accuracy, F1, sensitivity, kappa, and PABAK.",
            "results_summary": "On this particular dataset GPT achieved accuracy 0.943, macro F1-score 0.594, sensitivity included 1.000, sensitivity excluded 0.942 (as reported in Table 3).",
            "limitations_or_challenges": "This dataset represents one domain (LLMs in clinical medicine) and is small (23 included), so generalizability is limited; nonetheless it demonstrates the method's applicability across domains.",
            "uuid": "e3874.8",
            "source_info": {
                "paper_title": "Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automating biomedical evidence synthesis: RobotReviewer",
            "rating": 2
        },
        {
            "paper_title": "Trialstreamer: a living, automatically updated database of clinical trial reports",
            "rating": 2
        },
        {
            "paper_title": "Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool",
            "rating": 2
        },
        {
            "paper_title": "Rayyan-a web and mobile app for systematic reviews",
            "rating": 2
        },
        {
            "paper_title": "An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes",
            "rating": 2
        },
        {
            "paper_title": "Prioritising references for systematic reviews with RobotAnalyst: a user study",
            "rating": 2
        },
        {
            "paper_title": "A novel application of machine learning and zero-shot classification methods for automated abstract screening",
            "rating": 2
        },
        {
            "paper_title": "Fine-tuning a classifier to improve truthfulness",
            "rating": 1
        }
    ],
    "cost": 0.01683775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study</h1>
<p>Eddie Guo ${ }^{1}$; Mehul Gupta ${ }^{1}$, MD; Jiawen Deng ${ }^{2}$, BHSc; Ye-Jean Park ${ }^{2}$; Michael Paget ${ }^{1}$, BFA; Christopher Naugler ${ }^{1}$, MD<br>${ }^{1}$ Cumming School of Medicine, University of Calgary, Calgary, AB, Canada<br>${ }^{2}$ Temerty Faculty of Medicine, University of Toronto, Toronto, AB, Canada</p>
<h2>Corresponding Author:</h2>
<p>Eddie Guo
Cumming School of Medicine
University of Calgary
3330 University Dr NW
Calgary, AB, T2N 1N4
Canada
Phone: 15879880292
Email: eddie.guo@ucalgary.ca</p>
<h4>Abstract</h4>
<p>Background: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources.</p>
<p>Objective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers.
Methods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts.
Results: Our results show an accuracy of 0.91 , a macro $F_{1}$-score of 0.60 , a sensitivity of excluded papers of 0.91 , and a sensitivity of included papers of 0.76 . The interrater variability between 2 independent human screeners was $\kappa=0.46$, and the prevalence and bias-adjusted $\kappa$ between our proposed methods and the consensus-based human decisions was $\kappa=0.96$. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications.
Conclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research.
(J Med Internet Res 2024;26:e48996) doi: $\underline{10.2196 / 48996}$</p>
<h2>KEYWORDS</h2>
<p>abstract screening; Chat GPT; classification; extract; extraction; free text; GPT; GPT-4; language model; large language models; LLM; natural language processing; NLP; nonopiod analgesia; review methodology; review methods; screening; systematic review; systematic; unstructured data</p>
<h2>Introduction</h2>
<p>Knowledge synthesis, the process of integrating and summarizing relevant studies in the literature to gain an improved understanding of a topic, is a key component in identifying knowledge gaps and informing future research endeavors on a topic of interest [1,2]. Systematic and scoping reviews are among the most commonly used and rigorous forms of knowledge synthesis across multiple disciplines [1,2]. Given that the results from systematic and scoping reviews can inform guidelines, protocols, and decision-making processes, particularly for stakeholders in the realms of health care, the quality of the evidence presented by such reviews can significantly impact generated recommendations [3].</p>
<p>The quality of systematic and scoping reviews is highly dependent on the comprehensiveness of the database searches and the subsequent article screening processes. Overlooking relevant articles during these critical steps can lead to bias [4], while including discrepant studies can yield misleading conclusions and increase discordant heterogeneity [5]. Thus, guidelines surrounding the conduct of clinical reviews, such as the Cochrane Handbook [6], recommend that article screening be completed in duplicate by at least 2 independent reviewers.</p>
<p>However, duplicate screening effectively doubles the financial and human resources needed to complete systematic reviews compared to single screening. This is especially problematic for small research groups, review projects with broad inclusion criteria (such as network meta-analyses), or time-constrained review projects (such as reviews relating to COVID-19 during the early stages of the pandemic) [7,8]. Additionally, there is often substantial interrater variability in screening decisions, leading to additional time spent on discussions to resolve disagreements [9]. Due to the time constraints and wasted resources that are often features of duplicate screening, research studies may also include a more tailored, sensitive search strategy that can lead to missing several articles during the retrieval process [10]. Furthermore, although the nuances of each study differ, many systematic reviews may contain thousands of retrieved articles, only to exclude the majority (ie, up to $90 \%$ ) from the title and abstract screening [10,11].</p>
<p>Recent developments in artificial intelligence and machine learning have made it possible to semiautomate or fully automate repetitive steps within the systematic review workflow [12-14]. Prominent examples of such applications include RobotReviewer [15], TrialStreamer [16], Research Screener [7], DistillerSR [17], and Abstrackr [18], which are artificial intelligence models developed to extract information from scientific articles or abstracts to judge study quality and infer treatment effects. More specifically, RobotReviewer (2016) was shown to have similar capabilities to assess the risk of bias assessment as a human reviewer, only differing by around $7 \%$ in accuracy [19]. Similarly, TrialStreamer was a system developed to extract key elements of information from full texts, such as inferring which interventions in a clinical paper worked best, along with comparisons in study outcomes between all relevant extracted full texts of a topic indexed on MEDLINE [20].</p>
<p>While there have been previous attempts at automating the title and abstract screening process, they often involved labor- or computationally-intensive labeling, pretraining, or vectorizations [21]. For instance, Rayyan and Abstrackr are 2 free web tools that provide a semiautomated approach to article filtering by using natural language processing algorithms to learn when and where a reviewer includes or excludes an article and subsequently mimics a similar approach [22,23]. Rayyan also demonstrated high specificity, wherein $98 \%$ of all relevant articles were included after the tool had screened $75 \%$ of all articles to be analyzed in a study [24]. While automation using these tools was found to save time, there was still minimal to substantive risk that there would be missing studies if the tool were fully independent or automated [22,23]. Furthermore, current programs may use previously standard methods, including n-grams, in comparison to more updated techniques, such as the generative pretrained transformer (GPT) model, which is trained with data from a general domain and does not require additional training to learn embeddings that can represent the semantics and contexts of words in relation to other words $[25,26]$.</p>
<p>In this paper, we introduce a novel workflow to screen titles and abstracts for clinical reviews by providing plain language prompts to the publicly available OpenAI GPT application programming interface (API). We aimed to assess GPT models' ability to accurately and efficiently identify relevant titles and abstracts from real-world clinical review data sets, as well as their ability to explain their decisions and reflect on incorrect classifications. We compare the performance of our model against ground truth labeling by 2 independent human reviewers across 6 review papers in the screening of over 24,000 titles and abstracts.</p>
<h2>Methods</h2>
<h2>Overview</h2>
<p>In our study, we obtained a corpus of title and abstract data sets that have already been filtered by a minimum of 2 human reviewers to train our model (Figure 1). Subsequently, we created a Python script that provides the screening criteria for each paper to the OpenAI Chat GPT or GPT-4 API, depending on the input token length. We then passed each paper to the API using a consistent instruction prompt to determine whether a paper should be included or excluded based on the contents of its title and abstract. The overall accuracy (computed by dividing papers selected by both GPT and human reviewers by the total number of papers), sensitivity of both included and excluded papers, and interrater reliability through Cohen $\kappa$ and prevalence-adjusted and bias-adjusted $\kappa$ (PABAK) were computed against the human-reviewed papers:</p>
<p>$$
\text { PABAK }=\frac{k p_{o b s}-1}{k-1}
$$</p>
<p>Where $k$ is the number of categories and $p_{o b s}$ is the proportion of included papers. All data and code are available in Mendeley data sets [27].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h3><strong>Data Collection</strong></h3>
<p>To validate our proposed inclusion and exclusion methodology, we obtained 6 title and abstract screening data sets from different systematic and scoping reviews previously published by the authors of this study, each screened by 2 independent reviewers with conflicts resolved through consensus. These projects cover various medical science topics and vary in size, methodology, and complexity of screening criteria (Table 1 and Table S1 in Multimedia Appendix 1 [28-33]). We obtained the inclusion and exclusion decision from expert reviewers for each title and abstract entry, as well as the criteria provided to the expert reviewers during the screening process. A summary of the review characteristics is presented in Table 2.</p>
<p><strong>Table 1.</strong> Included studies and their characteristics. The first 5 data sets are systematic reviews with meta-analyses. The last study is a scoping review.</p>
<table>
<thead>
<tr>
<th>Study title</th>
<th>Data set name</th>
<th>Included studies (538/24,307), n/N</th>
<th>Study type</th>
<th>Study topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Efficacy and Safety of Ivermectin for the Treatment of COVID-19: A Systematic Review and Meta-Analysis [29]</td>
<td>IVM^{a}</td>
<td>35/279</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>COVID-19 treatment and antimalarials</td>
</tr>
<tr>
<td>Efficacy and Safety of Selective Serotonin Reuptake Inhibitors in COVID-19 Management: A Systematic Review and Meta-Analysis [30]</td>
<td>SSRI^{b}</td>
<td>29/3989</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>COVID-19 treatment and antidepressants</td>
</tr>
<tr>
<td>Efficacy of Lopinavir-Ritonavir Combination Therapy for the Treatment of Hospitalized COVID-19 Patients: A Meta-Analysis [31]</td>
<td>LPVR^{c}</td>
<td>91/1456</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>COVID-19 treatment and antiretrovirals</td>
</tr>
<tr>
<td>The Use of Acupuncture in Patients With Raynaud's Syndrome: A Systematic Review and Meta-Analysis of Randomized Controlled Trials [32]</td>
<td>RAYNAUDS^{d}</td>
<td>6/942</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>Raynaud syndrome and acupuncture</td>
</tr>
<tr>
<td>Comparative Efficacy of Adjuvant Non-Opioid Analgesia in Adult Cardiac Surgical Patients: A Network Meta-Analysis [33]</td>
<td>NOA^{e}</td>
<td>354/14,771</td>
<td>Systematic review and meta-analysis of randomized and nonrandomized trials</td>
<td>Postoperative pain and analgesics</td>
</tr>
<tr>
<td>Assessing the Research Landscape and Utility of LLMs^{f} in the Clinical Setting: Protocol for a Scoping Review^{g}</td>
<td>LLM</td>
<td>23/2870</td>
<td>Scoping review</td>
<td>Machine learning in clinical medicine</td>
</tr>
</tbody>
</table>
<p>^{a}IVM: ivermectin.</p>
<p>^{b}SSRI: selective serotonin reuptake inhibitor.</p>
<p>^{c}LPVR: lopinavir-ritonavir.</p>
<p>^{d}RAYNAUDS: Raynaud syndrome.</p>
<p>^{e}NOA: nonopioid analgesia.</p>
<p>^{f}LLM: large language model.</p>
<p>^{g}Registered with Open Science Framework [28].</p>
<p>Table 2. Data formatting for the Python script automating screening with the generative pretrained transformer application programming interface. All non-English characters were removed before analysis.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data</th>
<th style="text-align: left;">Columns</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">df_info</td>
<td style="text-align: left;">- Dataset Name (str): name of data set</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">- Inclusion Criteria (str): screening inclusion criteria</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">- Exclusion Criteria (str): screening exclusion criteria</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Dataset $^{\mathrm{a}}$</td>
<td style="text-align: left;">- Title (str): paper title</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">- Abstract (str): paper abstract</td>
</tr>
</tbody>
</table>
<p>${ }^{a}$ The name of the data set must match Dataset Name in df_info.</p>
<h2>App Creation</h2>
<p>Given a data set, df_info, containing information about inclusion and exclusion criteria of the data sets containing titles and
abstracts to be reviewed, the app calls the OpenAI GPT API to classify each paper to be screened as either included or excluded. The app was coded in Python. The prompt given to the GPT API is provided in Textbox 1.</p>
<p>Textbox 1. Prompt given to the generative pretrained transformer application programming interface.
Instructions: You are a researcher rigorously screening titles and abstracts of scientific papers for inclusion or exclusion in a review paper. Use the criteria below to inform your decision. If any exclusion criteria are met or not all inclusion criteria are met, exclude the article. If all inclusion criteria are met, include the article. Only type "included" or "excluded" to indicate your decision. Do not type anything else.</p>
<p>Abstract: {abstract}
Inclusion criteria: {inclusion_criteria}
Exclusion criteria: {exclusion_criteria}</p>
<h2>Decision:</h2>
<p>Where "Decision:" is whether GPT API includes or excludes the article. Thus, the algorithm is as follows:
data_df &lt;- load(df_info)
for each dataset in data_df: for each row in dataset:
prompt &lt;- instructions + title + abstract + inclusion criteria }
+ exclusion criteria decision &lt;- GPT(prompt) row['decision'] &lt;- decision
save(dataset)</p>
<h2>Assessment and Data Analysis</h2>
<p>After the app was run on all data sets included in our analysis, the following metrics were computed: accuracy, macro $F_{1}$-score, sensitivity for decision tags, $\kappa$, and PABAK. A subset of the results was selected for the GPT models to explain their reasoning. The following prompt was appended to the beginning of the original prompt given to the API: "Explain your reasoning for the decision given with the information below." The human and GPT decisions were appended to the end of the prompt. A subset of incorrect results was selected for GPT to reflect on its incorrect answers. The following prompt was appended to the beginning of the original prompt given to the API: "Explain your reasoning for why the decision given was incorrect with
the information below." The human and GPT decisions were appended to the end of the prompt.</p>
<h2>Results</h2>
<p>The overall accuracy of the GPT models was 0.91 , the sensitivity of included papers was 0.76 , and the sensitivity of excluded papers was 0.91 (Table 3 and Figure 2). On the nonopioid analgesia (NOA) data set (354/14,771 included abstracts), the model ran for 643 minutes and 50.8 seconds, with an approximate cost of US $\$ 25$. The data set characteristics are detailed in Table 1, the model performance is in Table 3 and visualized in Figure 2, and the reasoning from GPT is tabulated in Table 4.</p>
<p>Table 3. Performance of generative pretrained transformer (GPT) in screening titles and abstracts against a human reviewer's ground truth. $\kappa$ (human) is the agreement between 2 independent human reviewers. $\kappa$ (screen) is the agreement between GPT and the final papers included and excluded in each data set.</p>
<table>
<thead>
<tr>
<th>Data set</th>
<th>Accuracy</th>
<th>Macro $F_{1}$-score</th>
<th>Sensitivity (included)</th>
<th>Sensitivity (excluded)</th>
<th>$\kappa$ (human)</th>
<th>$\kappa$ (screen)</th>
<th>PABAK $^{\mathrm{a}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>IVM $^{\text {b }}$</td>
<td>0.748</td>
<td>0.610</td>
<td>0.686</td>
<td>0.756</td>
<td>0.72</td>
<td>0.26</td>
<td>0.78</td>
</tr>
<tr>
<td>SSRI $^{\text {c }}$</td>
<td>0.846</td>
<td>0.595</td>
<td>0.966</td>
<td>0.949</td>
<td>0.58</td>
<td>0.21</td>
<td>0.99</td>
</tr>
<tr>
<td>LPVR $^{\text {d }}$</td>
<td>0.949</td>
<td>0.613</td>
<td>0.593</td>
<td>0.862</td>
<td>0.51</td>
<td>0.25</td>
<td>0.88</td>
</tr>
<tr>
<td>RAYNAUDS $^{\text {e }}$</td>
<td>0.965</td>
<td>0.607</td>
<td>0.833</td>
<td>0.966</td>
<td>0.91</td>
<td>0.22</td>
<td>0.99</td>
</tr>
<tr>
<td>NOA $^{\text {f }}$</td>
<td>0.895</td>
<td>0.601</td>
<td>0.782</td>
<td>0.898</td>
<td>0.35</td>
<td>0.23</td>
<td>0.95</td>
</tr>
<tr>
<td>LLM $^{\text {g }}$</td>
<td>0.943</td>
<td>0.594</td>
<td>1.000</td>
<td>0.942</td>
<td>0.69</td>
<td>0.21</td>
<td>0.98</td>
</tr>
<tr>
<td>Total (weighted)</td>
<td>0.907</td>
<td>0.600</td>
<td>0.764</td>
<td>0.910</td>
<td>0.46</td>
<td>0.22</td>
<td>0.96</td>
</tr>
<tr>
<td>Total (macro)</td>
<td>0.891</td>
<td>0.664</td>
<td>0.810</td>
<td>0.900</td>
<td>0.63</td>
<td>0.23</td>
<td>0.93</td>
</tr>
</tbody>
</table>
<p>${ }^{\text {a }}$ PABAK: prevalence-adjusted and bias-adjusted $\kappa$. ${ }^{\mathrm{b}}$ IVM: ivermectin. ${ }^{\text {c }}$ SSRI: selective serotonin reuptake inhibitor. ${ }^{\mathrm{d}}$ LPVR: lopinavir-ritonavir. ${ }^{\text {e }}$ RAYNAUDS: Raynaud syndrome. ${ }^{\mathrm{f}}$ NOA: nonopioid analgesia. ${ }^{\mathrm{g}}$ LLM: large language model.</p>
<p>Figure 2. (A-F) Confusion matrices for the included and excluded papers for all data sets and (G) the overall performance. IVM: ivermectin; LLM: large language model; LPVR: lopinavir-ritonavir; NOA: nonopioid analgesia; RAYNAUDS: Raynaud's syndrome; SSRI: selective serotonin reuptake inhibitor.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Table 4. Decision and reasoning of the generative pretrained transformer (GPT) application programming interface for a subset of 4 example titles and abstracts. Full abstracts are in ivermectin (IVM) data set [27].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Decision and reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">- Title: Use of Ivermectin in the Treatment of COVID-19: A Pilot Trial <br> - Abstract: Objectives: In this randomized open-label trial pilot study we assessed the antiviral effects and safety of various doses of ivermectin in patients with mild clinical symptoms of COVID-19. Method(s): Patients were randomly assigned to receive standard of care (SOC) treatment at hospital admission...</td>
<td style="text-align: center;">- Human: Included <br> - GPT: Included <br> - Reasoning: The study meets all of the inclusion criteria. It is a randomized controlled trial that compares ivermectin against standard of care treatment in adult COVID-19 patients. The study reports outcomes related to time to viral clearance and adverse events. Additionally, the study is published in a peer-reviewed journal. Therefore, the article should be included in the review paper.</td>
</tr>
<tr>
<td style="text-align: center;">- Title: What Does the COVID-19 Pandemic Mean for the Next Decade of Onchocerciasis Control and Elimination? <br> - Abstract: Background: Mass drug administration (MDA) of ivermectin for onchocerciasis has been disrupted by the coronavirus disease 2019 (COVID-19) pandemic. Mathematical modelling can help predict how missed/delayed MDA will affect short-term epidemiological trends and elimination prospects by 2030...</td>
<td style="text-align: center;">- Human: Excluded <br> - GPT: Excluded <br> - Reasoning: The article does not meet any of the inclusion criteria or exclusion criteria listed. It is not a randomized controlled trial or comparative cohort study, it does not compare ivermectin against standard of care or control group/placebo, it does not include adult COVID-19 inpatients and/or outpatients, and it does not report any of the specified outcomes. Additionally, there is no indication that the article is non-peer-reviewed. Therefore, the article is excluded.</td>
</tr>
<tr>
<td style="text-align: center;">- Title: Use of Ivermectin Is Associated With Lower Mortality in Hospitalized Patients With Coronavirus Disease 2019: The Ivermectin in COVID Nineteen Study <br> - Abstract: Background: Ivermectin was shown to inhibit severe acute respiratory syndrome coronavirus 2 replication in vitro, which has led to off-label use, but clinical efficacy has not been described previously. Research Question: Does ivermectin benefit hospitalized coronavirus disease 2019 (COVID-19) patients?...</td>
<td style="text-align: center;">- Human: Included <br> - GPT: Excluded <br> - Reasoning: The article meets all of the inclusion criteria and none of the exclusion criteria, therefore it should be included in the review paper. <br> - Reflection: The given decision of excluding the article is incorrect. The article meets all the inclusion criteria as it is a non-randomized comparative cohort study that compares ivermectin against standard of care or control group/placebo, includes adult COVID-19 inpatients, and reports mortality incidence. The article does not use ivermectin for prophylaxis of COVID-19 and is a peer-reviewed article. Therefore, the correct decision should be "Included."</td>
</tr>
<tr>
<td style="text-align: center;">- Title: What to Expect From Different Drugs Used in the Treatment of COVID-19: A Study on Applications and in Vivo and in Vitro Results <br> - Abstract: The end of 2019 was marked by the emergence of a new type of coronavirus (SARS-CoV-2), which has killed more than 240,000 people...</td>
<td style="text-align: center;">- Human: Excluded <br> - GPT: Included <br> - Reasoning: The study is not a randomized controlled trial or a non-randomized comparative cohort study, which is an inclusion criteria. Therefore, the article does not meet all inclusion criteria and must be excluded. <br> - Reflection: As per the given inclusion and exclusion criteria, the decision given is incorrect. The abstract does not mention the use of ivermectin in any of the studies. Therefore, the article cannot be included based on the inclusion criteria.</td>
</tr>
</tbody>
</table>
<h2>Discussion</h2>
<h2>Overview</h2>
<p>In this study, we assessed the performance of the OpenAI GPT API in the context of clinical review paper inclusion and exclusion criteria selection. We report an overall accuracy of 0.91 and a PABAK of 0.96 , indicating a high level of agreement between the app's decisions and the reference standard. However, the $\kappa$ was low, ranging from 0.21 to 0.26 , largely due to the imbalanced nature of the data sets in this study. The sensitivity of the included papers was 0.76 , suggesting that the app needs improvement to correctly identify relevant papers (Table 3 and Figure 2). The sensitivity of excluded papers was 0.91 , showing promise in excluding irrelevant papers. These results highlight the potential of large language models (LLMs) to support the clinical review process.</p>
<h2>Implications of GPT API's Performance in the Review Process</h2>
<p>GPT's performance has several implications for the efficiency and consistency of clinical review paper inclusion and exclusion criteria selection. By prioritizing the workflow and acting as an
aid rather than a replacement for researchers and reviewers, the GPT and other large language models have the potential to streamline the review process. This enhanced efficiency could save valuable time and effort for researchers and clinicians, allowing them to focus on more complex tasks and in-depth analysis. Further, the API does not require pretraining or seed articles and can provide reasoning for its decision to either include or exclude papers, an aspect traditional natural language processing algorithms lack in automated or semiautomated paper screening (Table 4). Interestingly, upon being asked to explain its reasoning for a subset of incorrect classifications, GPT corrected its initial decision. Ultimately, this increased efficiency, paired with reasoning capabilities, could contribute to the overall quality of clinical reviews, leading to more accurate and reliable conclusions in medical research.</p>
<p>The use of LLMs in the review process could also promote consistency in the selection of relevant papers. By automating certain aspects of the process and acting as an aid to researchers and clinicians, the model can streamline the review process and help reduce the potential for human error and bias, leading to more objective and reliable results [34]. This increased consistency could, in turn, improve the overall quality of the</p>
<p>evidence synthesized in clinical reviews, providing a more robust foundation for medical decision-making and the development of clinical guidelines.</p>
<p>The potential of LLMs as a decision tool becomes particularly valuable when resources are limited. In such situations, LLMs can be used as a first-pass decision aid, streamlining the review process, and allowing human screeners to focus on a smaller, more relevant subset of papers. By automating the initial screening process, LLMs can help reduce the workload for researchers and clinicians, enabling them to allocate their time and effort more efficiently.</p>
<p>In particular, using the GPT API as a first-pass decision aid can also help mitigate the risk of human error and bias in the initial screening phase, promoting a more objective and consistent selection of papers. While the API's sensitivity for including relevant papers may not be perfect, its high specificity for excluding irrelevant papers can still provide valuable support in narrowing down the pool of potentially relevant studies [10]. This can be particularly beneficial in situations where a large number of papers need to be screened and human resources are scarce [35].</p>
<h2>Comparison to Other Tools</h2>
<p>The comparison of our proposed machine learning method to other tools, such as Abstrackr [18], DistillerSR [17], and RobotAnalyst [36], provides evidence of its efficacy and reliability in the context of systematic review processes. On a data set of 24,307 abstracts and titles, our model achieved an accuracy of 0.91 and comparable sensitivity of 0.91 and 0.76 for excluded and included papers, respectively. The significant interrater agreement ( $\kappa=0.96$ ) between our proposed method and consensus-based human decisions, juxtaposed to the lower interrater variability between 2 independent human screeners $(\kappa=0.46)$, emphasizes the model's robustness. In comparison, Abstrackr reported overall sensitivities of $0.96,0.79,0.92$, and 0.82 on data sets ranging from 5243 to 47,385 records. When comparing the proportion of missed records across Abstrackr, DistillerSR, and RobotAnalyst on nonpublic medical title and abstract screening data sets, Abstrackr exhibited the lowest proportions of missed records, namely $28 \%, 5 \%$, and $0 \%$, respectively [37]. Conversely, DistillerSR showed a high proportion of missed records, reaching up to $100 \%$ in the last data set. RobotAnalyst's performance fell between the 2, with missed proportions of $70 \%, 23 \%$, and $100 \%$, respectively. Future work will explore comparative analyses in greater depth and on a broader array of data sets to compare state-of-the-art screening tools.</p>
<h2>Limitations and Challenges in Implementing GPT API in the Review Process</h2>
<p>While the GPT API shows promise in streamlining the review process, it is important to acknowledge its limitations and
challenges. One notable limitation is the disparity between the high specificity of 0.91 for excluding papers and the lower sensitivity of 0.76 for including papers. This discrepancy suggests that while the API effectively excludes irrelevant papers, it may not be as proficient in identifying relevant papers for inclusion. This could lead to the omission of important studies in the review process, potentially affecting the comprehensiveness and quality of the final review. Therefore, the GPT API should not be considered a replacement for human expertise. Instead, it should be viewed as a complementary tool that can enhance the efficiency and consistency of the review process. Human screeners should still be involved in the final decision-making process, particularly in cases where the API's sensitivity for including relevant papers may be insufficient [7]. Another limitation arises in the selection of data sets for screening; 3 of the 6 data sets focused on the efficacy of various drugs for COVID-19, potentially limiting the generalizability of the results from other types of studies. Further work will assess GPT on a greater diversity of studies. By combining the strengths of the GPT API with human expertise, researchers can optimize the review process and ensure the accuracy and comprehensiveness of the final review.</p>
<h2>Future Research and Development</h2>
<p>Several avenues for future research and development include refining the GPT API's performance in the clinical review paper context, incorporating metadata such as study type and year, and exploring few-shot learning approaches. Additionally, training a generator-discriminator model through fine-tuning could improve the API's performance [38]. Expanding the application of the GPT API to other areas of medical research or literature review could also be explored. This would involve large language models for tasks such as identifying and extracting study design information, patient characteristics, and adverse events. As the maximum token length increases with future iterations of the GPT model, screening entire papers may become feasible [39]. Furthermore, exploring the use of LLMs to generate clinical review papers could be a promising research direction.</p>
<h2>Conclusions</h2>
<p>The GPT API shows potential as a valuable tool for improving the efficiency and consistency of clinical review paper inclusion and exclusion criteria selection. While there are limitations and challenges to its implementation, its performance in this study suggests that it could have a broader impact on clinical review paper writing and medical research. Future research and development should focus on refining the API's performance, expanding its applications, and exploring its potential in other aspects of clinical research.</p>
<h1>Acknowledgments</h1>
<p>We would like to acknowledge the following expert reviewers for providing the screening decisions in the review data sets used in this study and for agreeing to make the data sets publicly available: Abhinav Pillai, Mike Paget, Christopher Naugler, Kiyan Heybati, Fangwen Zhou, Myron Moskalyk, Saif Ali, Chi Yi Wong, Wenteng Hou, Umaima Abbas, Qi Kang Zuo, Emma Huang,</p>
<p>Daniel Rayner, Cristian Garcia, Harikrishnaa Ba Ramaraju, Oswin Chang, Zachary Silver, Thanansayan Dhivagaran, Elena Zheng, and Shayan Heybati.</p>
<h1>Authors' Contributions</h1>
<p>EG contributed to conceptualization, methodology, software, formal analysis, investigation, writing the original draft, reviewing, editing, visualization, supervision, and project administration. MG was responsible for conceptualization, methodology, investigation, writing the original draft, reviewing, editing, supervision, and project administration. JD and YJP were involved in methodology, software, formal analysis, investigation, data curation, writing the original draft, and visualization. MP and CN contributed to writing, reviewing, and editing.</p>
<h2>Conflicts of Interest</h2>
<p>None declared.</p>
<h2>Multimedia Appendix 1</h2>
<p>Included studies and their inclusion and exclusion criteria.
[DOCX File , 20 KB-Multimedia Appendix 1]</p>
<h2>References</h2>
<ol>
<li>Sargeant JM, O'Connor AM. Scoping reviews, systematic reviews, and meta-analysis: applications in veterinary medicine. Front Vet Sci. 2020;7:11. [FREE Full text] [doi: 10.3389/fvets.2020.00011] [Medline: 32047759]</li>
<li>Garritty C, Stevens A, Hamel C, Golfam M, Hutton B, Wolfe D. Knowledge synthesis in evidence-based medicine. Semin Nucl Med. 2019;49(2):136-144. [doi: 10.1053/j.semnuc1med.2018.11.006] [Medline: 30819393]</li>
<li>Luchini C, Veronese N, Nottegar A, Shin JI, Gentile G, Granziol U, et al. Assessing the quality of studies in meta-research: review/guidelines on the most important quality assessment tools. Pharm Stat. 2021;20(1):185-195. [doi: 10.1002/pst. 2068] [Medline: 32935459]</li>
<li>Gartlehner G, Affengruber L, Titscher V, Noel-Storr A, Dooley G, Ballarini N, et al. Single-reviewer abstract screening missed 13 percent of relevant studies: a crowd-based, randomized controlled trial. J Clin Epidemiol. 2020;121:20-28. [FREE Full text] [doi: 10.1016/j.jclinepi.2020.01.005] [Medline: 31972274]</li>
<li>Fletcher J. What is heterogeneity and is it important? BMJ. 2007;334(7584):94-96. [FREE Full text] [doi: 10.1136/bmj.39057.406644.68] [Medline: 17218716]</li>
<li>The Cochrane Collaboration; Higgins JPT, Thomas J. Cochrane Handbook for Systematic Reviews of Interventions. London. The Cochrane Collaboration; 2021.</li>
<li>Chai KEK, Lines RLJ, Gucciardi DF, Ng L. Research screener: a machine learning tool to semi-automate abstract screening for systematic reviews. Syst Rev. 2021;10(1):93. [FREE Full text] [doi: 10.1186/s13643-021-01635-3] [Medline: 33795003]</li>
<li>Clark J, McFarlane C, Cleo G, Ramos CI, Marshall S. The impact of systematic review automation tools on methodological quality and time taken to complete systematic review tasks: case study. JMIR Med Educ. 2021;7(2):e24418. [FREE Full text] [doi: $10.2196 / 24418$ ] [Medline: 34057072]</li>
<li>Tuijn S, Janssens F, Robben P, van den Bergh H. Reducing interrater variability and improving health care: a meta-analytical review. J Eval Clin Pract. 2012;18(4):887-895. [doi: 10.1111/j.1365-2753.2011.01705.x] [Medline: 21726359]</li>
<li>Rathbone J, Carter M, Hoffmann T, Glasziou P. Better duplicate detection for systematic reviewers: evaluation of systematic review assistant-deduplication module. Syst Rev. 2015;4(1):6. [FREE Full text] [doi: 10.1186/2046-4053-4-6] [Medline: 25588387]</li>
<li>Polanin JR, Pigott TD, Espelage DL, Grotpeter JK. Best practice guidelines for abstract screening large - evidence systematic reviews and meta - analyses. Res Synth Methods. 2019;10(3):330-342. [FREE Full text] [doi: 10.1002/jrsm. 1354]</li>
<li>Marshall IJ, Wallace BC. Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. Syst Rev. 2019;8(1):163. [FREE Full text] [doi: 10.1186/s13643-019-1074-9] [Medline: 31296265]</li>
<li>Marshall IJ, Trikalinos TA, Soboczenski F, Yun HS, Kell G, Marshall R, et al. In a pilot study, automated real-time systematic review updates were feasible, accurate, and work-saving. J Clin Epidemiol. 2023;153:26-33. [FREE Full text] [doi: 10.1016/j.jclinepi.2022.08.013] [Medline: 36150548]</li>
<li>Blaizot A, Veettil SK, Saidoung P, Moreno-Garcia CF, Wiratunga N, Aceves-Martins M, et al. Using artificial intelligence methods for systematic review in health sciences: a systematic review. Res Synth Methods. 2022;13(3):353-362. [doi: 10.1002/jrsm. 1553] [Medline: 35174972]</li>
<li>
<p>Marshall I, Kuiper J, Banner E, Wallace BC. Automating biomedical evidence synthesis: RobotReviewer. Association for Computational Linguistics; Presented at: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations; July 30-August 4, 2017, 2017;7-12; Vancouver, Canada. URL: https://aclanthology. org/P17-4002.pdf [doi: 10.18653/v1/p17-4002]</p>
</li>
<li>
<p>Marshall IJ, Nye B, Kuiper J, Noel-Storr A, Marshall R, Maclean R, et al. Trialstreamer: a living, automatically updated database of clinical trial reports. J Am Med Inform Assoc. 2020;27(12):1903-1912. [FREE Full text] [doi: 10.1093/jamia/ocaa163] [Medline: 32940710]</p>
</li>
<li>Hamel C, Kelly SE, Thavorn K, Rice DB, Wells GA, Hutton B. An evaluation of DistillerSR's machine learning-based prioritization tool for title/abstract screening-impact on reviewer-relevant outcomes. BMC Med Res Methodol. 2020;20(1):256. [FREE Full text] [doi: 10.1186/s12874-020-01129-1] [Medline: 33059590]</li>
<li>Gates A, Johnson C, Hartling L. Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool. Syst Rev. 2018;7(1):45. [FREE Full text] [doi: 10.1186/s13643-018-0707-8] [Medline: 29530097]</li>
<li>Marshall IJ, Kuiper J, Wallace BC. RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials. J Am Med Inform Assoc. 2016;23(1):193-201. [FREE Full text] [doi: 10.1093/jamia/ocv044] [Medline: 26104742]</li>
<li>Nye BE, Nenkova A, Marshall IJ, Wallace BC. Trialstreamer: mapping and browsing medical evidence in real-time. Proc Conf. 2020;2020:63-69. [FREE Full text] [doi: 10.18653/v1/2020.acl-demos.9] [Medline: 34136886]</li>
<li>Moreno-Garcia CF, Jayne C, Elyan E, Aceves-Martins M. A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews. Decis Anal J. 2023;6:100162. [FREE Full text] [doi: 10.1016/j.dajour.2023.100162]</li>
<li>Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-automated screening of biomedical citations for systematic reviews. BMC Bioinformatics. 2010;11:55. [FREE Full text] [doi: 10.1186/1471-2105-11-55] [Medline: 20102628]</li>
<li>Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan-a web and mobile app for systematic reviews. Syst Rev. 2016;5(1):210. [FREE Full text] [doi: 10.1186/s13643-016-0384-4] [Medline: 27919275]</li>
<li>Olofsson H, Brolund A, Hellberg C, Silverstein R, Stenström K, Österberg M, et al. Can abstract screening workload be reduced using text mining? User experiences of the tool Rayyan. Res Synth Methods. 2017;8(3):275-280. [doi: 10.1002/irsm. 1237] [Medline: 28374510]</li>
<li>Shree P. The journey of Open AI GPT models. Medium. 2020. URL: https://medium.com/walmartglobaltech/ the-journey-of-open-ai-gpt-models-32d95b7fb7fb2 [accessed 2023-04-19]</li>
<li>O'Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text mining for study identification in systematic reviews: a systematic review of current approaches. Syst Rev. 2015;4(1):5. [FREE Full text] [doi: 10.1186/2046-4053-4-5] [Medline: 25588314]</li>
<li>Guo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C. Automated paper screening for clinical reviews using large language models. Mendeley Data. 2023. URL: https://data.mendeley.com/datasets/np79tmhkh5/1 [accessed 2023-12-15]</li>
<li>Assessing the research landscape and utility of LLMs in the clinical setting: protocol for a scoping review. OSF Registries. URL: https://osf.io/498k6 [accessed 2023-12-15]</li>
<li>Deng J, Zhou F, Ali S, Heybati K, Hou W, Huang E, et al. Efficacy and safety of ivermectin for the treatment of COVID-19: a systematic review and meta-analysis. QJM. 2021;114(10):721-732. [FREE Full text] [doi: 10.1093/qjmed/hcab247] [Medline: 34570241]</li>
<li>Deng J, Rayner D, Ramaraju HB, Abbas U, Garcia C, Heybati K, et al. Efficacy and safety of selective serotonin reuptake inhibitors in COVID-19 management: a systematic review and meta-analysis. Clin Microbiol Infect. 2023;29(5):578-586. [FREE Full text] [doi: 10.1016/j.cmi.2023.01.010] [Medline: 36657488]</li>
<li>Deng J, Zhou F, Hou W, Heybati K, Ali S, Chang O, et al. Efficacy of lopinavir-ritonavir combination therapy for the treatment of hospitalized COVID-19 patients: a meta-analysis. Future Virol. 2021 [FREE Full text] [doi: 10.2217/fvl-2021-0066] [Medline: 35145560]</li>
<li>Zhou F, Huang E, Zheng E, Deng J. The use of acupuncture in patients with Raynaud's syndrome: a systematic review and meta-analysis of randomized controlled trials. Acupunct Med. 2023;41(2):63-72. [FREE Full text] [doi: 10.1177/09645284221076504] [Medline: 35608095]</li>
<li>Heybati K, Zhou F, Lynn MJ, Deng J, Ali S, Hou W, et al. Comparative efficacy of adjuvant nonopioid analgesia in adult cardiac surgical patients: a network meta-analysis. J Cardiothorac Vasc Anesth. 2023;37(7):1169-1178. [doi: 10.1053/j.jvca.2023.03.018] [Medline: 37088644]</li>
<li>Zhang Y, Liang S, Feng Y, Wang Q, Sun F, Chen S, et al. Automation of literature screening using machine learning in medical evidence synthesis: a diagnostic test accuracy systematic review protocol. Syst Rev. 2022;11(1):11. [FREE Full text] [doi: 10.1186/s13643-021-01881-5] [Medline: 35031074]</li>
<li>van de Schoot R, de Bruin J, Schram R, Zahedi P, de Boer J, Weijdema F, et al. An open source machine learning framework for efficient and transparent systematic reviews. Nat Mach Intell. 2021;3(2):125-133. [FREE Full text] [doi: $10.1038 / \mathrm{s} 42256-020-00287-7]$</li>
<li>Przybyła P, Brockmeier AJ, Kontonatsios G, Le Pogam M, McNaught J, von Elm E, et al. Prioritising references for systematic reviews with RobotAnalyst: a user study. Res Synth Methods. 2018;9(3):470-488. [FREE Full text] [doi: 10.1002/irsm. 1311] [Medline: 29956486]</li>
<li>
<p>Gates A, Guitard S, Pillay J, Elliott SA, Dyson MP, Newton AS, et al. Performance and usability of machine learning for screening in systematic reviews: a comparative evaluation of three tools. In: AHRQ Methods for Effective Health Care. Rockville, MD. Agency for Healthcare Research and Quality (US); 2019.</p>
</li>
<li>
<p>Schade M. Fine-tuning a classifier to improve truthfulness. OpenAI. URL: https://help.openai.com/en/articles/ 5528730-fine-tuning-a-classifier-to-improve-truthfulness [accessed 2023-04-20]</p>
</li>
<li>Joshua J. What is the difference between the GPT-4 models? OpenAI. URL: https://help.openai.com/en/articles/ 7127966-what-is-the-difference-between-the-gpt-4-models [accessed 2023-04-19]</li>
</ol>
<h1>Abbreviations</h1>
<p>API: application programming interface
GPT: generative pretrained transformer
LLM: large language model
NOA: nonopioid analgesia
PABAK: prevalence and bias-adjusted kappa</p>
<p>Edited by T de Azevedo Cardoso, G Eysenbach; submitted 14.05.23; peer-reviewed by T Kang, M Chatzimina, I Bojic; comments to author 30.08.23; revised version received 30.08.23; accepted 28.09.23; published 12.01.24</p>
<p>Please cite as:
Guo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C
Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study
J Med Internet Res 2024;26:e48996
URL: https://www.jmir.org/2024/1/e48996
doi: $10.2196 / 48996$
PMID: 38214966
©Eddie Guo, Mehul Gupta, Jiawen Deng, Ye-Jean Park, Michael Paget, Christopher Naugler. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 12.01.2024. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4995 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4995</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4995</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-252070866</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.00840v2.pdf" target="_blank">FOLIO: Natural Language Reasoning with First-Order Logic</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4995.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4995.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FOLIO: Natural Language Reasoning with First-Order Logic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-annotated dataset of 1,430 FOL-valid natural language reasoning examples (487 stories) paired with verified first-order logic (FOL) formulas; tasks: NL reasoning (True/False/Unknown) and NL->FOL translation with syntactic and execution checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (dataset/task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset and benchmark designed to evaluate strict first-order logical reasoning in language models; contains WikiLogic and HybLogic subsets, FOL annotations, and an FOL inference engine for ground-truth label verification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO (Natural language reasoning with first-order logic) and NL-FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given NL premises and NL conclusions, decide truth value {True, False, Unknown} using only premises (NL reasoning). Also a NL->FOL translation task requiring syntactically valid FOL and matching execution labels when fed to an FOL prover.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Human expert annotation with parallel FOL formulas; verification with an FOL inference engine; used to evaluate models via supervised fine-tuning and few-shot prompting (incl. CoT, SC, ToT) and neurosymbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Majority baseline 38.5% accuracy; random 33.3%. Human expert accuracy on test set 95.98%. NL-FOL translation: few-shot syntactic validity (SynV) ≈ 93-94% for GPT-3.5/GPT-4; inference-execution accuracy (ExcAcc) low (GPT-3.5 few-shot ExcAcc 56.0%, GPT-4 few-shot ExcAcc 63.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dataset size limited (1.43k examples) due to high-quality expert annotation; contains complex, long reasoning chains that current LLMs fail on; HybLogic subset is particularly challenging; translations can be ambiguous; FOL subset excludes temporal/modal logics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Designed to be more complex and natural than prior logical reasoning datasets (e.g., RuleTaker, LogicNLI, ProofWriter); shows large gap between SOTA LLMs and human experts (≈32% gap for GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analysis shows majority of conclusions require 1–8 premises (mode depth 4); 28.7% require ≥5 depths. Models perform worse on examples with higher reasoning depth. Premise order shuffle caused ≈±1% change, indicating ordering not a strong heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4995.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A highly capable closed-weight large language model by OpenAI used as the strongest baseline in FOLIO experiments and as the base model for several logic-specific methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based conversational LM (proprietary details not disclosed in paper); used via few-shot prompting (NL, NL+FOL, FOL), chain-of-thought and advanced prompting strategies; used as base for neurosymbolic methods (Logic-LM, LINC, DetermLR).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning and NL->FOL translation (evaluation and prompting experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict truth values of conclusions given NL premises and/or their FOL annotations; generate FOL formulas in NL->FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting (8-shot/shot variants), Chain-of-Thought (CoT), Self-Consistency (SC), Tree-of-Thought (ToT); NL+FOL and FOL-only prompting; used as base for Logic-LM, LINC, DetermLR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot NL prompting: 61.3% (8-shot); other reported few-shot runs: 64.2% (NL prompt), CoT: 68.9%, CoT+SC: 69.5%, ToT: 70.0% (best few-shot prompt reported). NL+FOL few-shot: 65.21%. NL-FOL translation few-shot SynV 93.9%, ExcAcc 63.8%. Gap to human experts ≈31.82% (expert 95.98%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on long/complex reasoning chains (faulty reasoning paths ~65% of GPT-4 errors), wrong intermediate derivations (~25%), occasional syntactic comprehension issues (~5%), spurious commonsense shortcuts (~5%); lower accuracy on HybLogic vs WikiLogic, indicating difficulty with template-based deep logical chains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms GPT-3.5 and text-davinci-002 in most prompting settings; still outperformed by specialized neurosymbolic/logical methods (Logic-LM 78.1%, DetermLR 77.5%, LINC 73.1% on FOLIO test set).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Adding FOL annotations (NL+FOL prompt) slightly improves GPT-4 over NL-only. CoT yields >4% improvement over vanilla few-shot; SC yields additional ~0.6% over CoT; ToT slightly outperforms SC+CoT. GPT-4 benefits from FOL signals and advanced prompting but still fails on deep chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4995.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5-Turbo family model used as a baseline for few-shot and zero-shot prompting on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI conversational LM (parameter details not specified in paper); evaluated in zero/few-shot prompting and NL-FOL translation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning and NL->FOL translation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above: truth-value prediction and FOL generation/translation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot prompting, NL-only/FOL-only/NL+FOL prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot NL prompting reported as 58.3% in one run; an 8-shot zero/few-shot run shows 53.1% (table variations). NL-FOL translation few-shot SynV 93.3%, ExcAcc 56.0%. NL vs Hyb subset: Wiki 68.88% vs Hyb 47.70% (subset results in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Inconsistent benefit from adding FOL: GPT-3.5 performed worse using NL+FOL or FOL-only prompts in some settings; poorer handling of code-like FOL or translation accuracy vs GPT-4; lower absolute accuracy on deep reasoning examples than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms GPT-4 across prompting strategies; shows stronger WikiLogic performance than HybLogic, suggesting reliance on surface familiarity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Adding FOL to prompts hurt or did not improve GPT-3.5 in some runs (contrasting with GPT-4 which benefits). Zero-shot sometimes beats few-shot for certain GPT-3.5 runs (noted in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4995.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-002 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier OpenAI GPT-series model used as a prompting baseline for FOLIO experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A GPT-family model variant (prior to GPT-3.5/GPT-4). Used as a few-shot prompting baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot prompting to classify truth values of conclusions given premises.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting (8-shot reported as maximum fitting some contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 49.53% on FOLIO test set (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Substantially weaker than GPT-3.5 and GPT-4 on FOLIO logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms GPT-3.5 and GPT-4; far below expert human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No specific ablations reported for this model in paper beyond baseline few-shot numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4995.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter open foundation language model evaluated via few-shot prompting on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter autoregressive transformer (from LLaMA family); evaluated with few-shot prompting and some advanced prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>8-shot few-shot prompting to predict truth values.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy ≈33.63% (slightly above random 33.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs near-chance; shows limited emergent reasoning at 13B scale on strict FOL tasks in FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Much worse than LLaMA-70B and GPT-series models; scaling to 70B yields noticeable improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Model size effect: 13B → 70B increased accuracy from ~33.6% to ~44.0% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4995.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter LLaMA-family model evaluated in few-shot and CoT/ToT prompting setups on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>70B-parameter autoregressive transformer; evaluated with few-shot, Chain-of-Thought and Tree-of-Thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot truth-value classification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting, Chain-of-Thought and Tree-of-Thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot baseline ≈43.97%; with CoT ≈47.8%; with ToT ≈48.4% (small absolute gains).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even at 70B, performance remains substantially below GPT-4 and logic-specialized methods; struggles with many deep HybLogic examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Improves over 13B variant by ~10 percentage points, but still underperforms GPT-3.5/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Shows clear scaling benefit (13B → 70B) but limited absolute performance gain vs top-ranked models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4995.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-base (Devlin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional transformer encoder fine-tuned as a classifier on FOLIO to predict truth labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>110M-parameter bidirectional transformer encoder; fine-tuned with an added two-layer classification head for 3-way truth-value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>110M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervised fine-tuning to predict True/False/Unknown given NL story.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning on training split.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 56.83% on FOLIO test set (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower than larger transformer-based models and fine-tuned T5 variant; limited by encoder-only architecture and pretraining tasks for strict FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>BERT-large improves moderately to 59.0%; RoBERTa-large and Flan-T5-Large perform better (62.1% and 65.9% respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>BERT-large shows ~2.2% improvement over BERT-base, indicating modest gains from scaling model capacity in this family.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4995.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger BERT variant fine-tuned on FOLIO with modest improvements over BERT-base.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>340M-parameter bidirectional transformer encoder; fine-tuned on FOLIO for 3-way classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervised truth-value classification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 59.0% on FOLIO test set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still behind RoBERTa-large and Flan-T5-Large; limited capability on deep reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>BERT-large slightly worse than RoBERTa-large (62.1%) and Flan-T5-Large (65.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Shows small gains with larger encoder model size vs base.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4995.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robustly optimized BERT family model fine-tuned for FOLIO showing competitive supervised performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>340M-parameter transformer encoder (RoBERTa variant) fine-tuned with a classification head for the 3-way FOLIO task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>340M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervised truth-value classification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 62.1% on FOLIO test set. Subset: WikiLogic 60.71%, HybLogic 63.48% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Underperforms Flan-T5-Large in absolute terms; remains limited on higher-depth reasoning chains though HybLogic fine-tuned performance was slightly higher (likely due to template regularities).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms base encoder models and approaches Flan-T5-Large; worse than GPT-4 few-shot with advanced prompting and logic-specific methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Fine-tuning improved HybLogic performance more than WikiLogic, suggesting template regularities in HybLogic are learnable by supervised tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4995.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuneable encoder-decoder sequence model that achieved the best supervised fine-tuning result on FOLIO among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>783M-parameter instruction-tuned sequence-to-sequence transformer (T5 family), fine-tuned for the classification task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>783M</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning (supervised fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervised truth-value classification from NL premises and conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fully supervised fine-tuning with classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy 65.9% on FOLIO test set (best among fine-tuned models reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still falls short of GPT-4 few-shot with advanced prompting and logic-specialized methods; struggles with deep reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Best among fine-tuned models in this paper; slightly below best prompting-configured GPT-4 (~70.0%) and well below logic-specific neurosymbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No direct ablation but superior to encoder-only fine-tuned models, indicating decoder/seq2seq & instruction tuning help for reasoning-classification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4995.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that elicits intermediate reasoning steps from LLMs to improve complex reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>prompting technique</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; a prompting strategy that asks the LLM to produce intermediate steps (rationales) before final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Applied to FOLIO few-shot prompting for NL reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to encourage stepwise logical reasoning and improve truth-value predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot examples with step-by-step rationale; combined with SC or ToT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On GPT-4, CoT increased accuracy from ~64.2% (vanilla few-shot) to 68.9% (+≈4.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Helps but not sufficient for deep FOL chains; still produces faulty reasoning paths in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Improves performance over vanilla few-shot prompts for GPT-4; smaller or no gains for some models (varies by model).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>CoT combined with SC and ToT provides incremental gains (SC +0.6% over CoT; ToT slightly better than SC+CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4995.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation strategy that samples multiple CoT reasoning paths and picks the majority-final-answer to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>prompting/decoding technique</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregation method applied to multiple sampled chain-of-thought outputs to improve final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Applied in conjunction with CoT on FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate multiple sampled rationales to reduce variance and correct spurious paths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Sampling multiple CoT outputs and majority-voting final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On GPT-4, CoT+SC improved over CoT alone by ≈0.6% (68.9% → 69.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Adds modest gains; does not fully solve deep chain failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Small improvement over CoT; ToT still slightly better in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Demonstrates that ensemble-like aggregation of reasoning chains helps but only marginally on FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4995.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought prompting (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deliberative search-based prompting technique that explores multiple reasoning branches (a search tree) to improve problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>prompting/decoding technique</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses iterative branching and backtracking of reasoning steps guided by the LLM to find better solutions than single-chain CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Applied to FOLIO NL reasoning with GPT-4 and LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Explores alternative reasoning paths for complex logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Constructs a search tree of thoughts and heuristically explores branches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On GPT-4, ToT achieved ~70.0% accuracy (slightly better than CoT+SC). On LLaMA-70B, ToT improved performance modestly (~48.4% vs baseline ~43.97%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Computationally heavier; still fails on many long reasoning chains and yields faulty paths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Slightly outperforms CoT+SC in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Shows additional improvements over simple CoT, especially on models that can explore multiple branches effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e4995.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic approach that augments LLM outputs with a symbolic solver to improve faithful logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that couples an LLM with symbolic FOL tools/provers to produce and verify logical derivations; implemented with GPT-4 as base model in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use LLM to generate candidate derivations and symbolic prover to check/repair them for final truth-value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neurosymbolic integration: LLM + FOL prover verification/repair loop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 78.1% on FOLIO test set (largest reported improvement over few-shot GPT-4 baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on correctness of translation between NL and formal logic and prover capabilities; engineering overhead for integration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms vanilla GPT-4 few-shot and advanced prompting (CoT/ToT); demonstrates efficacy of neurosymbolic methods for strict FOL reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Not detailed in paper beyond overall performance, but result indicates neurosymbolic verification yields large gains (~+8–14% vs GPT-4 prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e4995.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LINC: A neurosymbolic approach for logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic method that combines language models with first-order logic provers to improve logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LINC (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that integrates LLM outputs with a FOL prover and neural components to form provable derivations; used with GPT-4 as base in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict truth-values by generating formal representations and using symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Neurosymbolic pipeline: NL->formal logic generation + prover execution + model-guided repair/selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 73.1% on FOLIO test set (≈+9% over few-shot GPT-4 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires reliable NL->FOL translation and integration; may be sensitive to translation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Improves substantially over pure prompting; below Logic-LM and DetermLR in reported numbers but still large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper reports average improvements over multiple runs; suggests neurosymbolic integration is effective for FOL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4995.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e4995.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DetermLR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DetermLR (From indeterminacy to determinacy: Augmenting logical reasoning capabilities with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLMs to produce more deterministic/faithful logical reasoning by integrating symbolic determinization processes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DetermLR (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method combining LLM outputs with deterministic symbolic procedures to reduce derivation indeterminacy and improve logical inference accuracy; evaluated using GPT-4 as base.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>FOLIO NL reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce verified logical derivations and final truth-value predictions using a determinization pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Post-processing and integration with symbolic logic solvers to determinize LLM inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracy 77.5% on FOLIO test set (one of the top-performing methods reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on robust NL->FOL translation; complexity of integration and resource needs not fully characterized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Comparable to Logic-LM (78.1%) and outperforms vanilla GPT-4 prompting; highlights benefits of deterministic neurosymbolic augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No fine-grained ablations reported in this paper; reported as part of comparative evaluation showing large gains vs prompting alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FOLIO: Natural Language Reasoning with First-Order Logic', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models <em>(Rating: 2)</em></li>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>LLaMA: Open and efficient foundation language models <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language (RuleTaker / ProofWriter related work) <em>(Rating: 1)</em></li>
                <li>Diagnosing the first-order logical reasoning ability through LogicNLI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4995",
    "paper_id": "paper-252070866",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "FOLIO",
            "name_full": "FOLIO: Natural Language Reasoning with First-Order Logic",
            "brief_description": "A human-annotated dataset of 1,430 FOL-valid natural language reasoning examples (487 stories) paired with verified first-order logic (FOL) formulas; tasks: NL reasoning (True/False/Unknown) and NL-&gt;FOL translation with syntactic and execution checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (dataset/task)",
            "model_description": "Dataset and benchmark designed to evaluate strict first-order logical reasoning in language models; contains WikiLogic and HybLogic subsets, FOL annotations, and an FOL inference engine for ground-truth label verification.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO (Natural language reasoning with first-order logic) and NL-FOL translation",
            "task_description": "Given NL premises and NL conclusions, decide truth value {True, False, Unknown} using only premises (NL reasoning). Also a NL-&gt;FOL translation task requiring syntactically valid FOL and matching execution labels when fed to an FOL prover.",
            "method_or_approach": "Human expert annotation with parallel FOL formulas; verification with an FOL inference engine; used to evaluate models via supervised fine-tuning and few-shot prompting (incl. CoT, SC, ToT) and neurosymbolic methods.",
            "performance": "Majority baseline 38.5% accuracy; random 33.3%. Human expert accuracy on test set 95.98%. NL-FOL translation: few-shot syntactic validity (SynV) ≈ 93-94% for GPT-3.5/GPT-4; inference-execution accuracy (ExcAcc) low (GPT-3.5 few-shot ExcAcc 56.0%, GPT-4 few-shot ExcAcc 63.8%).",
            "limitations_or_failure_cases": "Dataset size limited (1.43k examples) due to high-quality expert annotation; contains complex, long reasoning chains that current LLMs fail on; HybLogic subset is particularly challenging; translations can be ambiguous; FOL subset excludes temporal/modal logics.",
            "comparison": "Designed to be more complex and natural than prior logical reasoning datasets (e.g., RuleTaker, LogicNLI, ProofWriter); shows large gap between SOTA LLMs and human experts (≈32% gap for GPT-4).",
            "ablation_or_analysis_results": "Analysis shows majority of conclusions require 1–8 premises (mode depth 4); 28.7% require ≥5 depths. Models perform worse on examples with higher reasoning depth. Premise order shuffle caused ≈±1% change, indicating ordering not a strong heuristic.",
            "uuid": "e4995.0",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A highly capable closed-weight large language model by OpenAI used as the strongest baseline in FOLIO experiments and as the base model for several logic-specific methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based conversational LM (proprietary details not disclosed in paper); used via few-shot prompting (NL, NL+FOL, FOL), chain-of-thought and advanced prompting strategies; used as base for neurosymbolic methods (Logic-LM, LINC, DetermLR).",
            "model_size": null,
            "logical_reasoning_task": "FOLIO NL reasoning and NL-&gt;FOL translation (evaluation and prompting experiments)",
            "task_description": "Predict truth values of conclusions given NL premises and/or their FOL annotations; generate FOL formulas in NL-&gt;FOL.",
            "method_or_approach": "Few-shot prompting (8-shot/shot variants), Chain-of-Thought (CoT), Self-Consistency (SC), Tree-of-Thought (ToT); NL+FOL and FOL-only prompting; used as base for Logic-LM, LINC, DetermLR.",
            "performance": "Few-shot NL prompting: 61.3% (8-shot); other reported few-shot runs: 64.2% (NL prompt), CoT: 68.9%, CoT+SC: 69.5%, ToT: 70.0% (best few-shot prompt reported). NL+FOL few-shot: 65.21%. NL-FOL translation few-shot SynV 93.9%, ExcAcc 63.8%. Gap to human experts ≈31.82% (expert 95.98%).",
            "limitations_or_failure_cases": "Struggles on long/complex reasoning chains (faulty reasoning paths ~65% of GPT-4 errors), wrong intermediate derivations (~25%), occasional syntactic comprehension issues (~5%), spurious commonsense shortcuts (~5%); lower accuracy on HybLogic vs WikiLogic, indicating difficulty with template-based deep logical chains.",
            "comparison": "Outperforms GPT-3.5 and text-davinci-002 in most prompting settings; still outperformed by specialized neurosymbolic/logical methods (Logic-LM 78.1%, DetermLR 77.5%, LINC 73.1% on FOLIO test set).",
            "ablation_or_analysis_results": "Adding FOL annotations (NL+FOL prompt) slightly improves GPT-4 over NL-only. CoT yields &gt;4% improvement over vanilla few-shot; SC yields additional ~0.6% over CoT; ToT slightly outperforms SC+CoT. GPT-4 benefits from FOL signals and advanced prompting but still fails on deep chains.",
            "uuid": "e4995.1",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-Turbo",
            "brief_description": "OpenAI's GPT-3.5-Turbo family model used as a baseline for few-shot and zero-shot prompting on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "OpenAI conversational LM (parameter details not specified in paper); evaluated in zero/few-shot prompting and NL-FOL translation tasks.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO NL reasoning and NL-&gt;FOL translation",
            "task_description": "Same as above: truth-value prediction and FOL generation/translation.",
            "method_or_approach": "Zero-shot and few-shot prompting, NL-only/FOL-only/NL+FOL prompts.",
            "performance": "Few-shot NL prompting reported as 58.3% in one run; an 8-shot zero/few-shot run shows 53.1% (table variations). NL-FOL translation few-shot SynV 93.3%, ExcAcc 56.0%. NL vs Hyb subset: Wiki 68.88% vs Hyb 47.70% (subset results in Table 6).",
            "limitations_or_failure_cases": "Inconsistent benefit from adding FOL: GPT-3.5 performed worse using NL+FOL or FOL-only prompts in some settings; poorer handling of code-like FOL or translation accuracy vs GPT-4; lower absolute accuracy on deep reasoning examples than GPT-4.",
            "comparison": "Underperforms GPT-4 across prompting strategies; shows stronger WikiLogic performance than HybLogic, suggesting reliance on surface familiarity.",
            "ablation_or_analysis_results": "Adding FOL to prompts hurt or did not improve GPT-3.5 in some runs (contrasting with GPT-4 which benefits). Zero-shot sometimes beats few-shot for certain GPT-3.5 runs (noted in experiments).",
            "uuid": "e4995.2",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "text-davinci-002",
            "name_full": "text-davinci-002 (OpenAI)",
            "brief_description": "An earlier OpenAI GPT-series model used as a prompting baseline for FOLIO experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "A GPT-family model variant (prior to GPT-3.5/GPT-4). Used as a few-shot prompting baseline.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO NL reasoning",
            "task_description": "Few-shot prompting to classify truth values of conclusions given premises.",
            "method_or_approach": "Few-shot prompting (8-shot reported as maximum fitting some contexts).",
            "performance": "Reported accuracy 49.53% on FOLIO test set (Table 4).",
            "limitations_or_failure_cases": "Substantially weaker than GPT-3.5 and GPT-4 on FOLIO logical reasoning.",
            "comparison": "Underperforms GPT-3.5 and GPT-4; far below expert human performance.",
            "ablation_or_analysis_results": "No specific ablations reported for this model in paper beyond baseline few-shot numbers.",
            "uuid": "e4995.3",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LLaMA-13B",
            "name_full": "LLaMA 13B",
            "brief_description": "A 13-billion-parameter open foundation language model evaluated via few-shot prompting on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B",
            "model_description": "13B-parameter autoregressive transformer (from LLaMA family); evaluated with few-shot prompting and some advanced prompting.",
            "model_size": "13B",
            "logical_reasoning_task": "FOLIO NL reasoning",
            "task_description": "8-shot few-shot prompting to predict truth values.",
            "method_or_approach": "Few-shot prompting baseline.",
            "performance": "Reported accuracy ≈33.63% (slightly above random 33.3%).",
            "limitations_or_failure_cases": "Performs near-chance; shows limited emergent reasoning at 13B scale on strict FOL tasks in FOLIO.",
            "comparison": "Much worse than LLaMA-70B and GPT-series models; scaling to 70B yields noticeable improvement.",
            "ablation_or_analysis_results": "Model size effect: 13B → 70B increased accuracy from ~33.6% to ~44.0% (Table 4).",
            "uuid": "e4995.4",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LLaMA-70B",
            "name_full": "LLaMA 70B",
            "brief_description": "A 70-billion-parameter LLaMA-family model evaluated in few-shot and CoT/ToT prompting setups on FOLIO.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-70B",
            "model_description": "70B-parameter autoregressive transformer; evaluated with few-shot, Chain-of-Thought and Tree-of-Thought prompting.",
            "model_size": "70B",
            "logical_reasoning_task": "FOLIO NL reasoning",
            "task_description": "Few-shot truth-value classification.",
            "method_or_approach": "Few-shot prompting, Chain-of-Thought and Tree-of-Thought prompting.",
            "performance": "Few-shot baseline ≈43.97%; with CoT ≈47.8%; with ToT ≈48.4% (small absolute gains).",
            "limitations_or_failure_cases": "Even at 70B, performance remains substantially below GPT-4 and logic-specialized methods; struggles with many deep HybLogic examples.",
            "comparison": "Improves over 13B variant by ~10 percentage points, but still underperforms GPT-3.5/GPT-4.",
            "ablation_or_analysis_results": "Shows clear scaling benefit (13B → 70B) but limited absolute performance gain vs top-ranked models.",
            "uuid": "e4995.5",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "BERT-base",
            "name_full": "BERT-base (Devlin et al.)",
            "brief_description": "A bidirectional transformer encoder fine-tuned as a classifier on FOLIO to predict truth labels.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_description": "110M-parameter bidirectional transformer encoder; fine-tuned with an added two-layer classification head for 3-way truth-value prediction.",
            "model_size": "110M",
            "logical_reasoning_task": "FOLIO NL reasoning (supervised fine-tuning)",
            "task_description": "Supervised fine-tuning to predict True/False/Unknown given NL story.",
            "method_or_approach": "Fully supervised fine-tuning on training split.",
            "performance": "Accuracy 56.83% on FOLIO test set (Table 4).",
            "limitations_or_failure_cases": "Lower than larger transformer-based models and fine-tuned T5 variant; limited by encoder-only architecture and pretraining tasks for strict FOL.",
            "comparison": "BERT-large improves moderately to 59.0%; RoBERTa-large and Flan-T5-Large perform better (62.1% and 65.9% respectively).",
            "ablation_or_analysis_results": "BERT-large shows ~2.2% improvement over BERT-base, indicating modest gains from scaling model capacity in this family.",
            "uuid": "e4995.6",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "BERT-large",
            "name_full": "BERT-large",
            "brief_description": "Larger BERT variant fine-tuned on FOLIO with modest improvements over BERT-base.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_description": "340M-parameter bidirectional transformer encoder; fine-tuned on FOLIO for 3-way classification.",
            "model_size": "340M",
            "logical_reasoning_task": "FOLIO NL reasoning (supervised fine-tuning)",
            "task_description": "Supervised truth-value classification.",
            "method_or_approach": "Fully supervised fine-tuning.",
            "performance": "Accuracy 59.0% on FOLIO test set.",
            "limitations_or_failure_cases": "Still behind RoBERTa-large and Flan-T5-Large; limited capability on deep reasoning.",
            "comparison": "BERT-large slightly worse than RoBERTa-large (62.1%) and Flan-T5-Large (65.9%).",
            "ablation_or_analysis_results": "Shows small gains with larger encoder model size vs base.",
            "uuid": "e4995.7",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "RoBERTa-large",
            "name_full": "RoBERTa-large",
            "brief_description": "A robustly optimized BERT family model fine-tuned for FOLIO showing competitive supervised performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "340M-parameter transformer encoder (RoBERTa variant) fine-tuned with a classification head for the 3-way FOLIO task.",
            "model_size": "340M",
            "logical_reasoning_task": "FOLIO NL reasoning (supervised fine-tuning)",
            "task_description": "Supervised truth-value classification.",
            "method_or_approach": "Fully supervised fine-tuning.",
            "performance": "Accuracy 62.1% on FOLIO test set. Subset: WikiLogic 60.71%, HybLogic 63.48% (Table 6).",
            "limitations_or_failure_cases": "Underperforms Flan-T5-Large in absolute terms; remains limited on higher-depth reasoning chains though HybLogic fine-tuned performance was slightly higher (likely due to template regularities).",
            "comparison": "Outperforms base encoder models and approaches Flan-T5-Large; worse than GPT-4 few-shot with advanced prompting and logic-specific methods.",
            "ablation_or_analysis_results": "Fine-tuning improved HybLogic performance more than WikiLogic, suggesting template regularities in HybLogic are learnable by supervised tuning.",
            "uuid": "e4995.8",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Flan-T5-Large",
            "name_full": "Flan-T5-Large",
            "brief_description": "A finetuneable encoder-decoder sequence model that achieved the best supervised fine-tuning result on FOLIO among tested models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large",
            "model_description": "783M-parameter instruction-tuned sequence-to-sequence transformer (T5 family), fine-tuned for the classification task.",
            "model_size": "783M",
            "logical_reasoning_task": "FOLIO NL reasoning (supervised fine-tuning)",
            "task_description": "Supervised truth-value classification from NL premises and conclusions.",
            "method_or_approach": "Fully supervised fine-tuning with classification head.",
            "performance": "Accuracy 65.9% on FOLIO test set (best among fine-tuned models reported).",
            "limitations_or_failure_cases": "Still falls short of GPT-4 few-shot with advanced prompting and logic-specialized methods; struggles with deep reasoning chains.",
            "comparison": "Best among fine-tuned models in this paper; slightly below best prompting-configured GPT-4 (~70.0%) and well below logic-specific neurosymbolic methods.",
            "ablation_or_analysis_results": "No direct ablation but superior to encoder-only fine-tuned models, indicating decoder/seq2seq & instruction tuning help for reasoning-classification.",
            "uuid": "e4995.9",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "Prompting technique that elicits intermediate reasoning steps from LLMs to improve complex reasoning performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "prompting technique",
            "model_description": "Not a model; a prompting strategy that asks the LLM to produce intermediate steps (rationales) before final answer.",
            "model_size": null,
            "logical_reasoning_task": "Applied to FOLIO few-shot prompting for NL reasoning",
            "task_description": "Used to encourage stepwise logical reasoning and improve truth-value predictions.",
            "method_or_approach": "Few-shot examples with step-by-step rationale; combined with SC or ToT in experiments.",
            "performance": "On GPT-4, CoT increased accuracy from ~64.2% (vanilla few-shot) to 68.9% (+≈4.7%).",
            "limitations_or_failure_cases": "Helps but not sufficient for deep FOL chains; still produces faulty reasoning paths in many cases.",
            "comparison": "Improves performance over vanilla few-shot prompts for GPT-4; smaller or no gains for some models (varies by model).",
            "ablation_or_analysis_results": "CoT combined with SC and ToT provides incremental gains (SC +0.6% over CoT; ToT slightly better than SC+CoT).",
            "uuid": "e4995.10",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (SC)",
            "brief_description": "A decoding/aggregation strategy that samples multiple CoT reasoning paths and picks the majority-final-answer to improve robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "prompting/decoding technique",
            "model_description": "Aggregation method applied to multiple sampled chain-of-thought outputs to improve final-answer accuracy.",
            "model_size": null,
            "logical_reasoning_task": "Applied in conjunction with CoT on FOLIO",
            "task_description": "Aggregate multiple sampled rationales to reduce variance and correct spurious paths.",
            "method_or_approach": "Sampling multiple CoT outputs and majority-voting final answer.",
            "performance": "On GPT-4, CoT+SC improved over CoT alone by ≈0.6% (68.9% → 69.5%).",
            "limitations_or_failure_cases": "Adds modest gains; does not fully solve deep chain failure modes.",
            "comparison": "Small improvement over CoT; ToT still slightly better in experiments.",
            "ablation_or_analysis_results": "Demonstrates that ensemble-like aggregation of reasoning chains helps but only marginally on FOLIO.",
            "uuid": "e4995.11",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Tree-of-Thought",
            "name_full": "Tree-of-Thought prompting (ToT)",
            "brief_description": "Deliberative search-based prompting technique that explores multiple reasoning branches (a search tree) to improve problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "prompting/decoding technique",
            "model_description": "Uses iterative branching and backtracking of reasoning steps guided by the LLM to find better solutions than single-chain CoT.",
            "model_size": null,
            "logical_reasoning_task": "Applied to FOLIO NL reasoning with GPT-4 and LLaMA-70B",
            "task_description": "Explores alternative reasoning paths for complex logical inference.",
            "method_or_approach": "Constructs a search tree of thoughts and heuristically explores branches.",
            "performance": "On GPT-4, ToT achieved ~70.0% accuracy (slightly better than CoT+SC). On LLaMA-70B, ToT improved performance modestly (~48.4% vs baseline ~43.97%).",
            "limitations_or_failure_cases": "Computationally heavier; still fails on many long reasoning chains and yields faulty paths.",
            "comparison": "Slightly outperforms CoT+SC in this benchmark.",
            "ablation_or_analysis_results": "Shows additional improvements over simple CoT, especially on models that can explore multiple branches effectively.",
            "uuid": "e4995.12",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "brief_description": "A neurosymbolic approach that augments LLM outputs with a symbolic solver to improve faithful logical reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Logic-LM (method)",
            "model_description": "Method that couples an LLM with symbolic FOL tools/provers to produce and verify logical derivations; implemented with GPT-4 as base model in experiments.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO NL reasoning",
            "task_description": "Use LLM to generate candidate derivations and symbolic prover to check/repair them for final truth-value prediction.",
            "method_or_approach": "Neurosymbolic integration: LLM + FOL prover verification/repair loop.",
            "performance": "Reported accuracy 78.1% on FOLIO test set (largest reported improvement over few-shot GPT-4 baselines).",
            "limitations_or_failure_cases": "Relies on correctness of translation between NL and formal logic and prover capabilities; engineering overhead for integration.",
            "comparison": "Outperforms vanilla GPT-4 few-shot and advanced prompting (CoT/ToT); demonstrates efficacy of neurosymbolic methods for strict FOL reasoning.",
            "ablation_or_analysis_results": "Not detailed in paper beyond overall performance, but result indicates neurosymbolic verification yields large gains (~+8–14% vs GPT-4 prompting).",
            "uuid": "e4995.13",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "LINC",
            "name_full": "LINC: A neurosymbolic approach for logical reasoning",
            "brief_description": "A neurosymbolic method that combines language models with first-order logic provers to improve logical reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LINC (method)",
            "model_description": "Method that integrates LLM outputs with a FOL prover and neural components to form provable derivations; used with GPT-4 as base in experiments.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO NL reasoning",
            "task_description": "Predict truth-values by generating formal representations and using symbolic inference.",
            "method_or_approach": "Neurosymbolic pipeline: NL-&gt;formal logic generation + prover execution + model-guided repair/selection.",
            "performance": "Reported accuracy 73.1% on FOLIO test set (≈+9% over few-shot GPT-4 baseline).",
            "limitations_or_failure_cases": "Requires reliable NL-&gt;FOL translation and integration; may be sensitive to translation errors.",
            "comparison": "Improves substantially over pure prompting; below Logic-LM and DetermLR in reported numbers but still large gains.",
            "ablation_or_analysis_results": "Paper reports average improvements over multiple runs; suggests neurosymbolic integration is effective for FOL tasks.",
            "uuid": "e4995.14",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "DetermLR",
            "name_full": "DetermLR (From indeterminacy to determinacy: Augmenting logical reasoning capabilities with LLMs)",
            "brief_description": "A method that augments LLMs to produce more deterministic/faithful logical reasoning by integrating symbolic determinization processes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DetermLR (method)",
            "model_description": "Method combining LLM outputs with deterministic symbolic procedures to reduce derivation indeterminacy and improve logical inference accuracy; evaluated using GPT-4 as base.",
            "model_size": null,
            "logical_reasoning_task": "FOLIO NL reasoning",
            "task_description": "Produce verified logical derivations and final truth-value predictions using a determinization pipeline.",
            "method_or_approach": "Post-processing and integration with symbolic logic solvers to determinize LLM inferences.",
            "performance": "Reported accuracy 77.5% on FOLIO test set (one of the top-performing methods reported).",
            "limitations_or_failure_cases": "Relies on robust NL-&gt;FOL translation; complexity of integration and resource needs not fully characterized in this paper.",
            "comparison": "Comparable to Logic-LM (78.1%) and outperforms vanilla GPT-4 prompting; highlights benefits of deterministic neurosymbolic augmentation.",
            "ablation_or_analysis_results": "No fine-grained ablations reported in this paper; reported as part of comparative evaluation showing large gains vs prompting alone.",
            "uuid": "e4995.15",
            "source_info": {
                "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models",
            "rating": 2,
            "sanitized_title": "from_indeterminacy_to_determinacy_augmenting_logical_reasoning_capabilities_with_large_language_models"
        },
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "LLaMA: Open and efficient foundation language models",
            "rating": 2,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "Transformers as soft reasoners over language (RuleTaker / ProofWriter related work)",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language_ruletaker_proofwriter_related_work"
        },
        {
            "paper_title": "Diagnosing the first-order logical reasoning ability through LogicNLI",
            "rating": 1,
            "sanitized_title": "diagnosing_the_firstorder_logical_reasoning_ability_through_logicnli"
        }
    ],
    "cost": 0.02044625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FOLIO: Natural Language Reasoning with First-Order Logic
11 Oct 2024</p>
<p>Simeng Han 
Yale University</p>
<p>Hailey Schoelkopf 
Yale University</p>
<p>Yilun Zhao 
Yale University</p>
<p>Zhenting Qi 
Harvard University</p>
<p>Martin Riddell 
Yale University</p>
<p>Wenfei Zhou 
NVIDIA</p>
<p>James Coady 
Yale University</p>
<p>David Peng 
Yale University</p>
<p>Yujie Qiao 
Yale University</p>
<p>Luke Benson 
Yale University</p>
<p>Lucy Sun 
Yale University</p>
<p>Alex Wardle-Solano 
Yale University</p>
<p>Hannah Szabo 
Yale University</p>
<p>Ekaterina Zubova 
Yale University</p>
<p>Matthew Burtell 
Yale University</p>
<p>Jonathan Fan 
Iowa City West High School</p>
<p>Yixin Liu 
Yale University</p>
<p>Brian Wong 
Yale University</p>
<p>Malcolm Sailor 
Yale University</p>
<p>Ansong Ni 
Yale University</p>
<p>Linyong Nan 
Yale University</p>
<p>Jungo Kasai 
University of Washington</p>
<p>Tao Yu 
University of Hong Kong</p>
<p>Rui Zhang 
Penn State University
8 Meta AI</p>
<p>Alexander R Fabbri 
Salesforce Research</p>
<p>Wojciech Kryści Ński 
Salesforce Research</p>
<p>Semih Yavuz 
Salesforce Research</p>
<p>Ye Liu 
Salesforce Research</p>
<p>Victoria Xi 
Lin 
Shafiq Joty 
Salesforce Research</p>
<p>Yingbo Zhou 
Salesforce Research</p>
<p>Caiming Xiong 
Salesforce Research</p>
<p>Rex Ying 
Yale University</p>
<p>Arman Cohan 
Yale University</p>
<p>Dragomir Radev 
Yale University</p>
<p>Salesforce Research</p>
<p>FOLIO: Natural Language Reasoning with First-Order Logic
11 Oct 202491EE60091D091474A0C7AA791AED16AEarXiv:2209.00840v3[cs.CL]
Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks.However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model.We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations.FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion.The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine.In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset.Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models.For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models.Our results show that a subset of FOLIO presents a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved remarkable performance on a variety of natural language tasks (OpenAI et al., 2023;Touvron et al., 2023;Srivastava et al., 2023;Wang et al., 2019a).Logical reasoning is a central component for intelligent systems and should be sufficiently and independently evaluated (Russell and Norvig, 2010).</p>
<p>However, existing natural language tasks are inadequate in measuring the complex logical reasoning capability of a model (Srivastava et al., 2023;Saparov and He, 2023;Tian et al., 2021).</p>
<p>Several datasets related to logical reasoning have recently been proposed.However, existing benchmarks often exhibit limited complexity in reasoning or lack language naturalness.Some of these common benchmarks do not specifically evaluate logical reasoning independently of other forms of reasoning (Yu et al., 2020;Liu et al., 2021).Those specifically designed for measuring logical reasoning are insufficient in terms of logical reasoning complexity and natural language variety.As shown in Table 1, examples in RuleTaker (Clark et al., 2020) and LogicNLI (Tian et al., 2021) need at most five depths of reasoning.The entire corpus of RuleTaker or LogicNLI has fewer than 50 distinct abstract syntax trees.RuleTaker has only 101 words in its vocabulary and LogicNLI has 1077 words in the vocabulary.Moreover, none of them are written by humans with information drawn from real-world knowledge, making them less applicable to real-world reasoning scenarios.The logical deduction portion in BigBench (Srivastava et al., 2023) requires commonsense reasoning besides logical deduction.ProntoQA (Saparov and He, 2023) only contains logical reasoning questions that are answerable with repeated applications of the Modus Ponens inference rule.</p>
<p>We present a natural language reasoning dataset, FOLIO, with first-order logic reasoning problems which require the models to decide the correctness of conclusions given a world defined by the premises.In FOLIO, we aim to ensure high lan- A FOLIO example based on the Wild Turkey Wikipedia page: https://en.wikipedia.org/wiki/Wild_turkeyNL premises NL Conclusions -&gt; Labels 1.There are six types of wild turkeys: Eastern wild turkey, Osceola wild turkey, Gould's wild turkey, A. Tom is an Ocellated wild turkey.-&gt; True Merriam's wild turkey, Rio Grande wild turkey, and the Ocellated wild turkey.</p>
<p>B. Tom is an Eastern wild turkey.-&gt; False 2. Tom is not an Eastern wild turkey.</p>
<p>C. Joey is a wild turkey.-&gt; Unknown 3. Tom is not an Osceola wild turkey.4. Tom is also not a Gould's wild turkey.5. Tom is neither a Merriam's wild turkey, nor a Rio Grande wild turkey.6. Tom is a wild turkey.</p>
<p>FOL Premises</p>
<p>FOL conclusions -&gt; Labels
1. ∀x(WildTurkey(x) → (EasternWildTurkey(x) ∨ OsceolaWildTurkey(x) ∨ GouldsWildTurkey(x) A. OcellatedWildTurkey(tom) -&gt; True ∨ MerriamsWildTurkey(x) ∨ RiograndeWildTurkey(x) ∨ OcellatedWildTurkey(x))) B. EasternWildTurkey(tom) -&gt; False 2. ¬EasternWildTurkey(tom)
C. WildTurkey(joey) -&gt; Unknown 3. ¬OsceolaWildTurkey(tom)) 4. ¬GouldsWildTurkey(tom) 5. ¬MerriamsWildTurkey(tom) ∧ ¬RiograndeWildTurkey(tom) 6. WildTurkey(tom) Table 2: An example story in FOLIO based on the knowledge from the Wikipedia page on wild turkeys.The story consists of five premises and three conclusions with their corresponding FOL formulas and labels for the conclusions.All five premises are needed to infer the conclusions.The model needs to reason under logic patterns with universal quantification (∀), negation (¬), conjunction (∧), and disjunction (∨).guage naturalness and complexity, an abundant vocabulary, and factuality while also maintaining high reasoning complexity.FOLIO is a high-quality and manually curated dataset, written by CS undergraduate and graduate students and researchers in academia and industry.To ensure the conclusions of our examples follow the premises logically, we annotated all reasoning examples with first-order logic (FOL) formulas.An example of FOLIO is shown in Table 2. Based on our annotations, we propose a new NL-FOL translation task where an NL reasoning example is translated into its FOL counterpart.Finally, we benchmark the performance of strong LMs in both fully supervised and few-shot settings to understand their capabilities in logical reasoning (i.e., deriving the truth value of a logical conclusion from NL premises).</p>
<p>Under the few-shot setting, the most capable publicly available LLM so far achieves only 53.1% on the stories written in a hybrid manner, which is slightly better than random.</p>
<p>To sum up, the contributions of this paper are threefold.1) We release a natural language reasoning dataset written by expert annotators, FOLIO, with first-order logical reasoning problems.2) We use formal logic, i.e., FOL to ensure the logical validity of the examples written in NL and propose a new NL-FOL translation task.3) We benchmark the performance of LMs by fine-tuning models and prompting LLMs with few-shot examples, on the FOLIO reasoning task.We hope that FOLIO, as a challenging logical reasoning dataset, will be used to facilitate measuring progress in the logical reasoning capabilities of language models.</p>
<p>Related Work</p>
<p>Datasets for reasoning from text</p>
<p>Developing models that can reason in texts has been a core goal in NLP since the field's early days (Cooper et al., 1996).Since then, there has been massive progress in reasoning over text.Various benchmarks that focus on different aspects of reasoning over textual inputs are proposed, including natural language inference (NLI) (Bowman et al., 2015;Wang et al., 2019b), reasoning for commonsense knowledge (Talmor et al., 2019;He et al., 2021) and multi-hop reasoning (Yang et al., 2018;Chen et al., 2020).Among these reasoning abilities, logical reasoning has recently attracted an increasing amount of study.ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2021) both collected multiplechoice questions from standardized graduate admission examinations, answering which requires various types of logical reasoning.However, these datasets cover mixed forms of reasoning and are not intended to test logical reasoning in isolation.</p>
<p>Meanwhile, testing logical reasoning in isolation without involving other forms of reasoning has also attracted researchers in recent years.CLUTRR (Sinha et al., 2019) covers inductive reasoning, which is beyond the scope of first-order logic.Synthetic corpuses of deductive reasoning are proposed to evaluate the deductive reasoning ability of pretrained LMs (Clark et al., 2021;Saeed et al., 2021;Tian et al., 2021).However, these datasets do not contain highly natural sentences and often cover limited forms of logic while FOL is much more expressive.Kazemi et al. (2023) created a dataset for reasoning with contradictory information.Kawabata and Sugawara (2023) crowdsourced rationales for over 3000 examples based on ReClor (Yu et al., 2020).ProntoQA (Saparov and He, 2023) is comprised solely of logical reasoning queries that can be resolved through applying the Modus Ponens inference rule while FOLIO questions require applications of multiple types of inference rules.As shown in Table 1, FOLIO is the first large-scale first-order logic (FOL) reasoning dataset with formal logic annotations in FOL.FO-LIO is logically diverse and complex with complex natural language sentences and a rich vocabulary.</p>
<p>Reasoning using large language models</p>
<p>Reasoning has been demonstrated as one of the emergent abilities of LLMs of sufficient scale recently (Talmor et al., 2020;Wei et al., 2022a;Chowdhery et al., 2022).One such emergent behavior, Chain-of-Thought prompting (Wei et al., 2022b), consists of a series of intermediate reasoning steps output by an LLM.This improves the performance on arithmetic, commonsense, and symbolic reasoning benchmarks significantly.There has been a line of research continuing on from Chain-of-Thought (Kojima et al., 2022;Li et al., 2022;Yao et al., 2023) to elicit reasoning behavior from LLMs.Building on Chain-of-Thought prompting, many techniques used on top of LLMs to improve downstream performance have been formalized into control flows and programs.These are called language model cascades (Dohan et al., 2022), subsuming techniques such as Chain-of-Thought prompting, STaR (Zelikman et al., 2022), and Selection-Inference (Creswell et al., 2022) for reasoning.Dasgupta et al. (2022) studied the reasoning ability of LLMs but only used a small set of 48 syllogisms with only two premises each.Saparov and He (2023) created a synthetic dataset that and showed that LLMs are capable of making correct individual deduction steps.</p>
<p>With FOLIO, we aim to set a high standard, ensuring that achieving high performance through superficial strategies and shallow heuristics is prevented, allowing a robust evaluation of the firstorder logic reasoning capabilities of LLMs.We show that many LLMs fall short on complex firstorder logic reasoning, and that significant room for improvement in this area remains.</p>
<p>FOLIO Corpus Construction</p>
<p>We collected FOLIO through a carefully designed manual annotation process to achieve high-quality examples that necessitate complex logical reasoning.Writing natural language reasoning stories with FOL requires sufficient knowledge in both semantic parsing and first-order logic, as well as strong analytical skills.Given the complexities of such annotations, we selected annotators based on a few important criteria to ensures that our dataset is annotated with the highest level of precision and expertise, reflecting the complexity and nuance required for first-order logical reasoning.1).Our annotators are either college or graduate students who are native English speakers or possess nearnative proficiency in English.4 2).They possess formal education in first-order logic, having either completed relevant coursework or undertaken self-directed studies in first-order logic or seman-tic parsing.At the NL quality check stage, only annotators who are experts in natural language processing or computational linguistics are involved.For the FOL quality check, only annotators who are experts in first-order logic are involved.We also give the annotators several training sessions on how to write a story, by providing them with detailed annotation guidelines.All stories and FOL annotations in FOLIO are written and reviewed by expert annotators, including CS undergraduate and graduate students, and senior researchers, who met the aforementioned criteria.</p>
<p>We develop our dataset in six stages: WikiLogic collection, HybLogic collection, NL quality control, FOL quality control, NL-FOL alignment and FOL verification, spending 980 man-hours in total.</p>
<p>Example collection</p>
<p>We collected our dataset using two different methods in order to obtain examples that are both logically diverse and complex and have abundant abstract syntax tree (AST) variations.The annotators are free to write stories based on any topic they want while writing the stories.</p>
<p>WikiLogic: annotation from scratch using Wikipedia articles as seeds.At this annotation stage, the annotators are asked to select random Wikipedia pages by repeatedly using the Wikipedia Special Random link. 1 The Wikipedia articles are used to develop ideas for topics to write new stories.We ask the annotators to create new stories from scratch without using templates based on realworld knowledge, which should be plausible in general.Each of the stories is composed of several premises and conclusions with truth values of True, False, or Unknown (see Table 2 for an example).We also ask the annotators to write parallel FOL sentences for both the premises and conclusions.This results in a wide range of topics, abundant AST variations, and a wide vocabulary for FOLIO.Table 1 shows a comparison of FOLIO with other reasoning datasets that purely evaluate first-order logic or deductive reasoning.</p>
<p>HybLogic: hybrid annotation The task of generating logically sound stories from scratch for a set of facts is very time-consuming for human writers, where the main challenge is to create complex and varied logical patterns to arrive at a conclusion.To address the problems of solely using manual 1 https://en.wikipedia.org/wiki/Special:Randomannotation, we also consider a hybrid approach to facilitate the process.Our hybrid method is based on a common form of logical stories: syllogisms.A syllogism consists of two premises and a single conclusion, and the conclusion states some facts about the entities and categories in the premises.</p>
<p>In this approach, we first generate logically valid stories, which are templates containing abstract categories and entities, by combining multiple syllogisms into a single story template: the conclusion of one syllogism is used as a premise for the next syllogism.There are 256 logically distinct types of syllogisms and 24 of them are valid (Lehman, 1973).We use various combinations of 24 valid syllogisms.We also add in conjunction, disjunction, and implication.We show an example of the resulting templates in Appendix B. We then ask human annotators to assign nouns, phrases, or clauses to the abstract entities or categories that reflect real-life scenarios to each template and write logically-valid stories in natural language.The usage of the template is to ensure that we have a set of varied and complex logical stories with multiple conclusions.There are many ways of expressing the same logic template in natural language, and so the generated templates augment, rather than limit, the creativity of humans.</p>
<p>Quality control for NL sentences</p>
<p>To ensure the highest quality of the dataset, we dedicated considerable attention to the following key aspects of the natural language sentences during the quality control process.</p>
<p>Factuality and bias Our dataset prioritizes realism and factual accuracy, steering clear of biases and stereotypes linked to identity markers like race, ethnicity, gender, sexuality, nationality, class, and religion.Toward these objectives, we manually screened all stories and found that 39.2% of the stories suffer from at least one of these issues.We implemented a detailed protocol to rewrite these stories.The protocol is in Appendix C.</p>
<p>Language quality Apart from grammar, we make sure the sentences in our dataset are highly natural.All the sentences are first checked with a grammar checking tool, Grammarly.Our annotators who have graduated from or are senior students studying English Literature conducted a thorough round of review for grammatical correctness and language naturalness.We also eliminate natural language ambiguity when it is possible.We include rules on eliminating ambiguity in Appendix D. Employing these rules effectively reduces the ambiguity of natural language in this reasoning dataset, but incurs the tradeoff of limiting variations in some usage of language.However, we note that there is still sufficient variation in terms of sentence structures and logical structures as shown in Table 1.</p>
<p>Quality control for FOL formulas</p>
<p>We adopt the FOL definitions and syntax most widely used in the AI community (Russell and Norvig, 2010).We include more details on the definition of FOL we consider and the FOL modelling convention in Appendix E In preliminary investigations, we found that the human-written FOL formulas suffer from FOL consistency issues, which necessitates an additional round of quality control for FOL formulas.</p>
<p>FOL consistency One NL sentence can be translated into FOL through multiple non-equivalent ways.For example, sometimes additional information inferred from a sentence can be represented in FOL, leading to multiple representations.We therefore design an annotation protocol for FOL translation in order to ensure that our FOL translations are as consistent as possible across all examples in our dataset.We highlight a few important strategies used in the annotation protocol in Appendix F.</p>
<p>NL-FOL alignment review</p>
<p>Apart from checking whether NL and FOL express equivalent meanings, we also add necessary commonsense knowledge in both the NL and FOL premises.Sometimes humans do not write certain commonsense knowledge in the premises that is required in the FOL reasoning process, which is based solely on the premises given.We add such knowledge as additional premises at this stage.In particular, intrinsic properties of some predicates are required in the FOL reasoning process.For example, "LocatedIn(x,y)" should be transitive and "BeFamily(x,y)" should be symmetric.</p>
<p>FOL verification</p>
<p>Recognizing that the FOL formula annotations can be error-prone, we verify the syntactic validity and label consistency of FOL formula annotations with an FOL inference engine.We include the details of the FOL inference engine in Appendix G.</p>
<p>Dataset statistics</p>
<p>We show basic statistics of FOLIO and demonstrate the abundant vocabulary and logical complexity of FOLIO: Tables 1, 3 and Figure 1.Natural language complexity We use the Dale-Chall Readability Formula (Dale andChall, 1948, 1995) to show the text complexity of FOLIO following (Singh et al., 2023;Arps et al., 2022;Wei et al., 2021).We show the distribution of readability in Appendix H.</p>
<p>Basic statistics</p>
<p>Logical complexity and diversity statistics</p>
<p>As shown in Figure 1, the mode of reasoning depths is four in FOLIO.28.7% of the examples need five or more depths of reasoning to infer the conclusions, while the previous datasets needed at most five reasoning depths as shown in Table 1.This illustrates the logical complexity of FOLIO.Table 1 shows that FOLIO also has a much larger number of distinct ASTs than the previous datasets, indicating that FOLIO is much more logically diverse.larger than the previous synthetically constructed datasets for logical reasoning.</p>
<p>Vocabulary and topics</p>
<p>Task Definition</p>
<p>We define two new tasks based on FOLIO, natural language reasoning with first-order logic and NL-FOL translation.</p>
<p>Natural language reasoning with first-order logic</p>
<p>Each natural language (NL) story S in FOLIO consists of n premises: P = {p 1 , p 2 , ..., p n } and m conclusions: H = {h 1 , h 2 , ..., h m }.All NL stories are annotated with parallel FOL stories SF , which are sets of FOL formulas consisting of n premises P F = {pf 1 , pf 2 , ..., pf n } and m conclusions HF = {hf 1 , hf 2 , ..., hf m }. pf i and hf i are logically and semantically similar to p i and h i , respectively.Given P and H, the goal is to determine the truth values of the conclusions: "True", "False" or "Unknown", based on FOL reasoning.</p>
<p>NL-FOL translation</p>
<p>We propose a new natural language to first-order logic translation (NL-FOL translation) task alongside our reasoning dataset.The goal of this task is to translate an NL story S to an FOL story F S.</p>
<p>In particular, each of the NL sentence p i or h i and the parallel FOL formula pf i or hf i should be logically and semantically equivalent.Moreover, the truth values for the conclusions should be the same based on the NL story S and the parallel FOL story F S. In our dataset, the premises and conclusions are set up in such a way to ensure that the inference engine always returns an answer given enough resources such as time and memory.Unlike previous work (Singh et al., 2020) which translates problems with a single premise and a single hypothesis, our task is on translating examples of various lengths with a focus on stories with multiple premises.Thus, it also requires the models to consider discourse-level consistencies as opposed to translation at the sentence level.</p>
<p>NL-FOL evaluation metrics Two metrics are adopted to evaluate NL-FOL translation to capture different aspects of the generation results: 1).Syntactic validity (SynV).The Syntactic Validity score measures whether the FOL formulas are syntactically valid.The score will be 1 if all FOL formulas of an example can pass the syntactic check and 0 otherwise 2).Inference Engine execution accuracy (ExcAcc).The group of translated FOL for premises and conclusions in one story is fed into our inference engine to output the truth value for each conclusion.We define the accuracy of the output labels as the execution accuracy.We leave for future work the design of a more reliable metric of NL-FOL translation.</p>
<p>Experiments</p>
<p>In this section, we describe our experiments and main results.</p>
<p>Experimental setup</p>
<p>Tasks We conduct experiments on the two tasks in §4: NL reasoning with first-order logic (logical reasoning) and NL-FOL translation (NL-FOL).</p>
<p>Dataset split We split FOLIO by 70%/15%/15% split for the train/validation/test sets with 1,001/203/226 examples respectively.We split by story so that models are evaluated on unseen stories.</p>
<p>Evaluation metrics</p>
<p>We use accuracy for evaluating logical reasoning performance.For NL-FOL translation, we use the metrics in Section 4.2.</p>
<p>Models</p>
<p>We test the logical reasoning capabilities of LMs using fully supervised fine-tuning and few-shot prompting.We also test NL-FOL translation with few-shot prompting.</p>
<p>Fully supervised fine-tuning As fine-tuning baselines, we experiment with BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020).We finetune the base and large versions of both BERT and RoBERTa, with an additional two-layer classification layer to predict the truth values.For the second task, i.e., NL-FOL translation, we only report fewshot prompting methods.Prompting strategies We experiment with incorporating recent prompting strategies into GPT-4 as they have shown improvements in the general reasoning performance of LLMs.The prompting strategies include chain-of-thought (CoT) prompting (Wei et al., 2022b), chain-of-thought prompting with self-consistency (Wang et al., 2023) and treeof-thought prompting (Yao et al., 2023).</p>
<p>Few-shot prompting</p>
<p>Logical reasoning methods</p>
<p>We also test recent methods specifically designed for logical reasoning: Logic-LM (2023), LINC (Olausson et al., 2023) and DetermLR (Sun et al., 2023), using GPT-4 as the base model.For the second task (NL-FOL translation), we use the same examples as in the Few-Shot NL experiments except that all the conclusions are included in each example.</p>
<p>We run experiments on five randomly sampled sets of examples from the training set and report the average accuracy.</p>
<p>Main results</p>
<p>Logical reasoning</p>
<p>The majority baseline of our dataset is 38.5% since in our test set, there are 87, 78 and 61 examples with labels of true, false and unknown respectively.As shown in Table 4, BERTbase and RoBERTa-base have similar performance on FOLIO with 56.83% accuracy.BERT-large has a 2.2% improvement over BERT-base.RoBERTalarge improves 3.1% over BERT-large.Flan-T5-Large achieves the highest performance in the finetuning setting and the accuracy is 65.7%.The model sizes of text-davinci-002, GPT-3.5-Turbo and GPT-4 are hidden from public3 .CoT stands for chain-of-thought prompting (Wei et al., 2022b).SC stands for self-consistency (Wang et al., 2023).ToT stands for tree-of-thought prompting (Yao et al., 2023).</p>
<p>We show that zero-shot prompting GPT-3.5 achieves better results than few-shot prompting text-davinci-002.Under few-shot NL prompting setting, LLama-13B achieves 33.63%, which is only slightly better than chance (33%).LLama-70B achieves 43.97%, around 10% better than LLaMA-13B and obtains improvements of around 4% with Chain-of-thought prompting and Tree of Thought prompting.Text-davinci-002 achieves 49.53% and GPT-3.5 achieves 58.34%.GPT-4 achieves the best results among GPT series models.</p>
<p>Incorporating recent prompting strategies increases the performance of vanilla few-shot prompting.Chain-of-thought prompting achieves more than a 4% increase over GPT-4.Self-consistency (SC) improves chain-of-thought prompting by 0.6% percent.Tree-of-thought prompting achieves slightly better result than self-consistency with chain-of-thought prompting.For the results of recent methods developed for logical reasoning, LINC (Olausson et al., 2023) achieves around a 9% increase over few-shot prompting GPT-4.Both Logic-LM (GPT-4)( 2023) and DetermLR ( 2023) achieves more than a 13% increase over few-shot prompting GPT-4, showing the superiority of the methods on logical reasoning.</p>
<p>NL-FOL translation</p>
<p>Table 5 shows the results of NL-FOL translation.The syntactic validity scores are around 93% with both GPT-3.5-Turbo and GPT-4.This indicates that language models with sufficient scales are good at picking up the patterns for FOL formulas and generating syntactically valid FOL formulas.However, GPT-3.5-Turbo and GPT-4 are not yet good at translating an NL story to a logically or semantically similar FOL counterpart, as indicated by the low inference engine execution accuracy score.</p>
<p>Error Analysis</p>
<p>Below we provide analysis of our results and key findings, providing additional insights into our dataset FOLIO and the current capabilities of LLMs in logical reasoning.example chains.We note that the presence and prevalence of these difficult examples are unique to FOLIO.FOLIO's unique complexity reveals that current LMs are limited in their ability to extrapolate to longer and more complex reasoning chains, and suggests an avenue for further study.Models have higher accuracy on WikiLogic than on HybLogic As shown in Table 6, in logical reasoning, GPT-3.5 and GPT-4 achieve substantially lower results on HybLogic than on WikiLogic and the result is slightly higher than chance.We hypothesize that this is because HybLogic has high logical complexity that the SoTA LLMs like GPT-4 cannot solve yet while WikiLogic examples require shorter reasoning chains which the model is already capable of solving.Moreover, since the examples in WikiLogic are created from scratch by humans, it is possible that LLMs have seen similar texts with similar logical patterns in the training data.However, fine-tuning RoBERTa-large achieves higher performance on HybLogic than on WikiLogic.This is likely because HybLogic is created from templates and some of the logical patterns can be learned during fine-tuning.</p>
<p>Models have higher accuracy on examples with fewer reasoning depths than on those with higher number of reasoing depths</p>
<p>In NL-FOL translation, performs 10 points better on HybLogic than WikiLogic.This could be because WikiLogic has more distinct and diverse sentence-level logical and language patterns and FOL annotations.WikiLogic has 53 ASTs while HybLogic has 33.Despite being more logically complex on a story level, FOL translations for Hy-bLogic stories have simpler logical structures on a statement level.We include case study for one WikiLogic example and one HybLogic example in Appendix I and further analysis on model performance in Appendix J.</p>
<p>Faulty path 65%</p>
<p>Wrong derivation 25%</p>
<p>Wrong syntactic comprehension 5%</p>
<p>Spurious shortcut 5%</p>
<p>GPT-4 Output</p>
<p>We know that all children are human (premise 1) and if someone is underage, they are a child (premise 2).People are either underage or of age (premise 3).If someone is of age, they can vote (premise 4) and get married (premise 5).If Jack is a child and a human, then Jack is neither able to vote nor get married (premise 6).We don't have any information about Jack's age, so we cannot determine if he is a child or of age.Therefore, we cannot determine if Jack is able to vote and get married.</p>
<p>Table 8: Case study for the scenario where a model is unable to form the correct reasoning chain.</p>
<p>Human evaluation on model outputs We conduct human evaluation on the GPT-4 model outputs with wrong truth value predictions.As shown in Table 7, approximately 65% of the time, the model struggles to construct accurate reasoning chains for complex problems with intricate steps, leading to faulty reasoning paths and indicating a limited ability to solve problems with long reasoning chains.In 25% of cases, erroneous derivations occur within certain reasoning steps, highlighting potential inaccuracies and flaws in logical deductions.5% of conclusions in FOLIO have a complex syntactic structure, posing comprehension challenges for GPT-4.5% of outputs show that GPT-4 leverage commonsense reasoning to employ spurious shortcuts that lead to the wrong truth value for the conclusion.We provide a case study for the "Faulty path" scenario in Table 8.In this instance, the model can perform simple derivations from the premises, like "If someone is of age, they can vote and get married."However, because of the problem's complexity, the model struggles to identify the essential intermediate steps and cannot ascertain the truth value of conclusions, such as "Jack is not a child."</p>
<p>Human performance</p>
<p>We collected truth value annotations of logical reasoning for FOLIO test set from expert and nonexpert annotators.Our expert annotators are computer science college students familiar with FOL.Non-expert annotators are community college or high school students who have not taken the SAT.Both expert and non-expert annotators are native English speakers.Expert annotations achieve an accuracy of 95.98% while non-expert annotations achieves 61.82%, with a gap of 34.16%.This shows that sufficient domain knowledge of FOL is necessary for good performance on FOLIO.The expert and GPT-4 gap is 31.82%,suggesting significant room for model improvement.</p>
<p>Conclusion</p>
<p>We introduced FOLIO, an expert-written dataset for logical reasoning equipped with FOL formulas.The examples in FOLIO are created based on real-world knowledge with natural language.It exhibits a large number of distinct logic patterns and a large vocabulary.Experiments show that FOLIO presents a challenge for one of the most capable Large Language Model publicly available.</p>
<p>Limitations</p>
<p>We focus on collecting a very high-quality dataset in evaluating logical reasoning rather than merely a large dataset.Optimizing for quality required us to adopt a rigorous annotation process with domain experts selected based on a few important criteria as mentioned in Appendix A: Annotator Selection.Significantly scaling up this process would have required resources beyond our current means and we are unable further expand our dataset for investigating how the size of training data affects the performance of fine-tuning experiments.We encourage the community to apply our annotation protocol to expand this realistic and complex FOL reasoning story set.</p>
<p>A Annotator Selection</p>
<p>Given the complexities of our annotations, we selected annotators based on a few important criteria 1).Our annotators are either college or graduate students who are native English speakers or possess near-native proficiency in English. 42).They possess formal education in first-order logic, having either completed relevant coursework or undertaken self-directed studies in first-order logic or semantic parsing.At the NL quality check stage, only annotators who are experts in natural language processing or computational linguistics are involved.For the FOL quality check, only annotators who are experts in first-order logic are involved.We also give the annotators several training sessions on how to write a story, by providing them with detailed annotation guidelines.All stories and FOL annotations in FOLIO are written and reviewed by expert annotators, including CS undergraduate and graduate students, and senior researchers, who met the aforementioned criteria.</p>
<p>B HybLogic Template Example</p>
<p>An example the resulting template is as follows:</p>
<p>Conclusions:</p>
<p>[Unknown] a is an S.</p>
<p>[True] If a is either a C or a D, then a is not either an A or a B.</p>
<p>C Factuality and Bias Elimination Protocol</p>
<p>We rewrote those that are not reflective of wellestablished scientific, historical, or legal facts.We took out stories that had strongly opinionated language and contained gender, racial, and classist biases.We accept certain classes of "psychologically fundamental generalizations" (Leslie, 2008), however, such as "Covid is transmitted through the air" or "Tigers eat other animals," that may not be factually invariant but add logical and semantic nuances to the stories.For stories that pertain to generalization, such as "All As are Bs," we have added specifiers like "all Dan knows" to give a degree of reasonable factuality.For example, "All science fiction that Dan knows comes from an imaginative process" has a more reasonable degree of factuality than "All science fiction comes from an imaginative process."</p>
<p>D Language Quality Control</p>
<p>• We always use "either-or" to express exclusive disjunction.We use either "A or B" or "A or B, or both" to express inclusive disjunction.In English "or" itself can be interpreted as either inclusive disjunction or exclusive disjunction.Adding "or both" cancels the exclusive disjunction distinctly.However, it is less common in the wild than just using "or".we could add "or both" if it is important to emphasize the inclusive part semantically or contextually or for factuality; and do not add "or both" if it is not.We rely on the language model to figure out if it should be inclusive or exclusive, therefore not sacrificing naturalness.</p>
<p>• It is more natural to say "Some A is B" rather than "there exists an A such that A is B." "All A are B" can be more natural than "If A then B".</p>
<p>• Writing NL sentences that express negation over exclusive-or ("either both or neither") can be cumbersome but we found one natural ways of expressing these situations: "Each morning, John either works out and stretches, or he does neither".</p>
<p>Other common issues in NL quality include singular/plural issues, especially in statements that deal with both categories and individual members of those categories; as well as ambiguities resulting from improper introduction of, or failure to introduce, proper nouns.</p>
<p>E First-Order Logic E.1 First-Order Logic VS Natural Language FOL enables deriving facts from other facts (Russell and Norvig, 2010).In the context of logical reasoning in modern NLP, FOL, as a logical form, is a more explicit logical representation than its NL counterpart and can be used as input to an FOL prover in order to obtain the exact truth values for the conclusions.FOL has no ambiguity while ambiguity can occur at various levels of NLP.FOL can thus be a good interface between how LMs are trained and how logical conclusions are reasoned.</p>
<p>E.2 FOL definition</p>
<p>We include the following operators: negation ¬, conjunction ∧, disjunction ∨, implication →, universal quantifier ∀, existential quantifier ∃, equal =.Following (Russell and Norvig, 2010), we consider temporal logic and modal logic as special-purpose logics.Consequently, they are beyond the scope of the definition of first-order logic used in our dataset.</p>
<p>E.3 FOL modeling conventions</p>
<p>We use n-place predicates when applicable for the expressivity of the FOL formulas.However, we do not use the Davidsonian (Davidson, 2001) or neo-Davidsonian semantics (Parsons, 1990) because translating the majority of the FOL formulas in our dataset only requires one-place and twoplace predicates.Therefore the Davidsonian or neo-Davidsonian semantics are not necessary for the expressivity of the FOL formulas.</p>
<p>For example, "Enjoy dressing up in oldfashioned clothing" is rendered as "Enjoy(x, dressingUp, oldFashionedClothing)".</p>
<p>F FOL Annotation Protocol</p>
<p>We therefore design an annotation protocol for first-order logic translation in order to ensure that our FOL translations are as consistent as possible across all examples in our dataset.We highlight a few important strategies used in the annotation protocol.a).First-order logic formulas need to preserve as much as possible the semantics of natural language sentences.b).First-order logic formulas should stay as faithful to the structure of the original NL sentence as possible.c).Semantic decomposition is not needed unless necessary for maintaining the NL expressivity.This means that "John is a bachelor" can be translated into FOL simply as "Bachelor(John)".d).In terms of abstraction, we neglect tense and remove all the plural forms of verbs.</p>
<p>G FOL Inference Engine</p>
<p>Although there are many provers widely used in the community (McCune, 2005(McCune, -2010;;Sutcliffe, 2017;Nipkow et al., 2002) , we adopt the inference engine provided in the Stanford CS221 course page5 , which is a compact module designed specifically for procesing first-order logic statements.The inference engine does not support input in the FOL syntax adopted by standard education material (Russell and Norvig, 2010), which is used in our dataset.We therefore developed a FOL parser in order to convert the FOL formulas written by humans to the input format of the inference engine.The converter is a semantic parser tool written in Python.Although LLMs such as GPT-4 can be utilized to conduct the conversion, it is hard to ensure the GPT-4 outputs are always correct.</p>
<p>Proving a story requires three steps.First, the FOL statements of the premises and conclusions of a story annotated by humans are converted to Python code.Then, the code snippets are used as input to the theorem prover.Finally, the theorem prover outputs whether the conclusions are True / False / Unknown, based on the premises.predictions are wrong for all conclusions.</p>
<p>Table 10 shows a story from HybLogic with a more complex FOL reasoning process.Inferred from premises 4 and 5, James does not perform better than others.With premises 3, 2 and 1, we know that James is not good at time management.Therefore, conclusion B is False.It cannot be determined if James exercises every week, thus the first conclusion is Unknown.The truth value of p → q is the same as ¬p ∨ q.It is not true that James does not perform better than others.It is also false that James exercises every week and is good at time management.Thus conclusion C is False.For this example, GPT-4 predicted the correct truth value only for conclusion A and RoBERTa made correct predictions for conclusions A and B.</p>
<p>J Model Performance Analysis</p>
<p>Models have more tendency to predict "True" compared with "False" or "Unknown" labels Confusion matrices in Figure 4 for the fine-tuning and 8-shot NL prompt results both show that LLMs are significantly better at making the correct predictions for conclusions with labels of True than the conclusions with labels of False or Unknown.The accuracy on examples with False or Unknown conclusions is 61.9% with fine-tuning and 54.0% with few-shot prompting.They also tend to make more predictions of True than the other labels.</p>
<p>Model performance is not affected by the premise ordering To test if the premise ordering in FOLIO has spurious correlations with the conclusion label which a model can exploit, we shuffle the input premises to evaluate models.We find that accuracy increases or decreases by roughly 1% in most settings compared to our unshuffled premises.This indicates that the ordering of premises in FO-LIO examples does not yield significant information about the label, and thus models will not be able to use the premise ordering as a strong heuris- tic or statistical feature for its predictions.</p>
<p>Using both NL sentences and FOL formulas in the prompt performs better FOL formulas have a clearer and more straightforward logical structure than NL sentences.Therefore, we test GPT-3.5 and GPT-4 with another two settings for truth value prediction using few-shot prompting: 1) using only FOL formulas in the prompt; 2) using both NL sentences and FOL formulas by concatenating each NL sentence and its annotated FOL statement.As shown in Table 11, the performance slightly increases in the NL+FOL setting for GPT-4 while GPT-3.5 performs worse in both the NL+FOL and the FOL-only settings.In other words, FOL always serves as additional useful information for GPT-4, but not for GPT-3.5 regardless of whether FOL is concatenated with NL.This observation resonates with the finding that GPT-4 performs much better than GPT-3.5 on code-related tasks (Ni et al., 2023).</p>
<p>Figure 1 :
1
Figure 1: Distribution of reasoning depths</p>
<p>Figure 1 demonstrates the distribution of the number of examples in the WikiLogic and HybLogic sets versus the number of premises needed to arrive at a conclusion, showing that most of the conclusions from WikiLogic require one to five premises while those from HybLogic require five to eight premises.</p>
<p>Figure</p>
<p>Figure Accuracies of different models categorized into examples with different reasoning depths.</p>
<p>P. All S are M. Either S or A. All A are B. All D are B. No C are B. a is either a C or a P.</p>
<p>Figure 4 :
4
Figure 4: Confusion matrices for the results of finetuning RoBERTa-Large and few-shot prompting GPT-4.</p>
<p>Table 1 :
1
Comparison of FOLIO with other datasets related to logical reasoning.#Distinct AST stands for the number of distinct abstract syntax trees, representing the number of distinct sentence-level logic structures in the corpus.FOLIO is the first expert-written dataset for FOL reasoning equipped with parallel FOL formulas.The examples are mostly aligned with real-world knowledge and use highly natural wordings.It also has a greater variety than the previous datasets in terms of reasoning depths with a larger number of distinct logic patterns and a large vocabulary.
DatasetSizeReasoningText SourceReal-World Resources# Reasoning DepthVocab# Distinct ASTCLUTTER (2019)6k InductiveSynthetic××-×RECLOR (2020)6k Mixed forms GMAT, LSAT exams✓×-×LogiQA (2021)8.6k Mixed forms NCSE exams✓×-×RuleTaker (2020)500k DeductiveSynthetic×0 ∼ 510148ProofWriter (2021) 500k DeductiveSynthetic×0 ∼ 510148LogicNLI (2021)20k FOLSynthetic×1 ∼ 5107730BigBench (2022)1300 Mixed forms Human-WrittenPartially×--ProntoQA (2023)200 DeductiveSynthetic✓1, 3, 5--FOLIO (ours)1,435 FOLExpert-written✓0 ∼ 7435176</p>
<p>Table 3 shows that examples based on Wikipedia make up the largest portion of FOLIO, with 304 stories, 1,353 NL and FOL premise pairs, and 753 NL and FOL conclusion pairs.Hybrid annotations consist of 183 stories with 1,054 NL and FOL premise pairs, and 682 NL and FOL conclusion pairs in total.</p>
<p>Table 3 :
3
Table 3 shows that our dataset has a vocabulary of 4,351 words, and the examples based on Wikipedia account for 74% of the total vocabulary even though the WikiLogic stories take up only 63% of the total number of stories.The vocabulary of FOLIO is also significantly Statistics based on different data collection methods of FOLIO.#Words is the average number of words per NL sentence.
Source#Stories #Premises #ConclusionsNLLogicVocab #Words Complexity #Depth ASTWikiLogic304135375332508.500 -14 grade1 -551HybLogic1831054682190211.520 -14 grade5 -825Total4872407143543519.860 -14 grade765-8</p>
<p>Table 4 :
4
Logical reasoning results of fully supervised fine-tuning and few-shot prompting on FOLIO test set.
ModelSizeAcc (%)majority baseline-38.5%random probability-33.3 %Fully supervised fine-tuneBERT-base110M56.8BERT-large340M59.0RoBERTa-base110M56.8RoBERTa-large340M62.1Flan-T5-Large783M65.90-shot NL PromptGPT-3.5-Turbo-53.1GPT-4-61.38-shot NL PromptLLama-13B13B33.6LLama-70B70B44.0LLama-70B -CoT70B47.8LLama-70B -ToT70B48.4text-davinci-002-49.5GPT-3.5-Turbo-58.3GPT-4-64.2GPT-4 -CoT (2022b)-68.9GPT-4 -CoT with SC (2023) -69.5GPT-4 ToT (2023)-70.0LR-specific MethodsLogic-LM (2023)-78.1LINC (2023)-73.1DetermLR (2023)-77.5</p>
<p>Table 5 :
5
NL-FOL translation results on FOLIO.SynV measures syntactic validity and ExcAcc measures the inference engine execution accuracy.
ModelZero-ShotFew-ShotSynv ExcAcc Sync ExcAccGPT-3.5-Turbo 68.450.4 93.3 56.0GPT-486.151.7 93.9 63.8</p>
<p>Table 6 :
6
Performance differences on the WikiLogic and HybLogic subset of FOLIO.WikiLogic has more diverse logical structures while HybLogic stories have higher reasoning depths.
MethodModelWikiHybFine-tuningRoBERTa-large 60.71 63.48NL PromptingGPT-3.5-Turbo68.88 47.70GPT-475.43 53.10NL-FOL ExcAcc GPT-3.5-Turbo45.17 61.82GPT-459.12 67.93</p>
<p>Table 7 :
7
Human evaluation on GPT-4 model outputs with incorrect truth value predictions Example Premises 1.All children are human.2. If someone is underage, then they are a child.3.People are either underage or of age. 4. If someone is of age, then they can vote.5.If someone is of age, they can legally get married.6.If Jack is a child and a human, then Jack is neither able to vote nor able to get married.Conclusion -&gt; Label: Jack is able to vote and get married.-&gt; True.</p>
<p>Table 11 :
11
Comparison of the results across different input formats with few-shot prompting.NL, NL-FOL, FOL, NL + FOL stands for NL prompting, execution accuracy of NL-FOL translation, using only FOL in the prompt and using concatenated NL and FOL in the prompt respectively.
ModelNLNL-FOL FOL NL+FOLGPT-3.5 58.3455.9657.9257.75GPT-464.1663.8264.0165.21
In experimenting with different prompts, we found 8 shot examples to perform slightly better. It is also the maximum number of examples that fits in the text-davinci-002 context.
Hereafter, "GPT-3.5" refers to GPT-3.5-Turbo.
By "near-native" we mean with English speaking and understanding ability that closely mirrors that of a native English speakers.
https://stanford-cs221.github.io/spring2022/ assignments/logic/index.html
H Distribution of ReadabilityWe show the distribution of readability in Figure3.I Case studyTable9shows a story from WikiLogic along with the GPT-4 and RoBERTa-Large predictions.Conclusion A is True given premises 5 and 3. From the premises, it cannot be determined if Cerura vinula has thin antennae or if it is a pest.Thus conclusions B and C are Unknown.GPT-4 predictions are correct for conclusions A and C while RoBERTa
HHUplexity at text complexity DE challenge 2022. David Arps, Jan Kels, Florian Krämer, Yunus Renz, Regina Stodden, Wiebke Petersen, Proceedings of the GermEval 2022 Workshop on Text Complexity Assessment of German Text. the GermEval 2022 Workshop on Text Complexity Assessment of German TextPotsdam, GermanyAssociation for Computational Linguistics2022</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, Michael Pieler, Shivanshu Usvsn Sai Prashanth, Laria Purohit, Jonathan Reynolds, Ben Tow, Samuel Wang, Weinbach, 10.48550/ARXIV.2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022arXiv preprint</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Hy-bridQA: A dataset of multi-hop question answering over tabular and textual data. Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Yang, Wang , 10.18653/v1/2020.findings-emnlp.91Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Peter Clark, Oyvind Tafjord, Kyle Richardson, CoRR, abs/2002.05867Transformers as soft reasoners over language. 2020</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Using the framework. Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, LRE 62-051 D-16The FraCaS Consortium. 1996Technical Report</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>A formula for predicting readability. Edgar Dale, Jeanne S Chall, Educational Research Bulletin. 2711948</p>
<p>Readability Revisited: The New Dale-Chall Readability Formula. Edgar Dale, Jeanne S Chall, 1995Brookline Books</p>
<p>Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.07051Language models show human-like content effects on reasoning. 2022arXiv preprint</p>
<p>105The Logical Form of Action Sentences. Donald Davidson, 10.1093/0199246270.003.0006Essays on Actions and Events. Oxford University Press2001</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, 10.48550/ARXIV.2207.10342Language model cascades. Rif A. Saurous; Kevin Murphy, and Charles Sutton2022arXiv preprint</p>
<p>WinoLogic: A zero-shot logic-based diagnostic dataset for Winograd Schema Challenge. Weinan He, Canming Huang, Yongmei Liu, Xiaodan Zhu, 10.18653/v1/2021.emnlp-main.307Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Evaluating the rationale understanding of critical reasoning in logical reading comprehension. Akira Kawabata, Saku Sugawara, arXiv:2311.183532023Preprint</p>
<p>Boardgameqa: A dataset for natural language reasoning with contradictory information. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, arXiv:2306.079342023PreprintXin Xu, Vaiva Imbrasaite, and Deepak Ramachandran</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.119162022arXiv preprint</p>
<p>Two sets of perfect syllogisms. Anne Lehman, 10.1305/ndjfl/1093891016Notre Dame Journal of Formal Logic. 1431973</p>
<p>Generics: Cognition and Acquisition. Sarah-Jane Leslie, 10.1215/00318108-2007-023The Philosophical Review. 11712008</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336On the advance of making language models better reasoners. 2022arXiv preprint</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Ro{bert}a: A robustly optimized {bert} pretraining approach. 2020arXiv preprint</p>
<p>. W Mccune, 2005-2010. Prover9 and mace4</p>
<p>Dragomir Radev, and Arman Cohan. 2023. L2ceval: Evaluating language-to-code generation capabilities of large language models. Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, Shafiq Joty, Yingbo Zhou, arXiv:2309.17446Preprint</p>
<p>Isabelle/Hol a Proof Assistant for Higher-Order Logic. Tobias Nipkow, Lawrence C Paulson, Markus Wenzel, 2002Springer</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, Roger Levy, 10.18653/v1/2023.emnlp-main.313Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Josh Openai, Others Achiam, arXiv:2303.08774Gpt-4 technical report. 2023Preprint</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, 10.18653/v1/2023.findings-emnlp.248Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Events in the Semantics of English. Terence Parsons, 1990MIT PressCambridge, MA, USA</p>
<p>Stuart Russell, Peter Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall20103 edition</p>
<p>RuleBERT: Teaching soft rules to pre-trained language models. Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti, 10.18653/v1/2021.emnlp-main.110Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Language models can (kind of) reason: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, International Conference on Learning Representations. 2023</p>
<p>Exploring neural models for parsing natural language into first-order logic. Hrituraj Singh, Milan Aggrawal, Balaji Krishnamurthy, arXiv:2002.065442020arXiv preprint</p>
<p>Misery loves complexity: Exploring linguistic complexity in the context of emotion detection. Pranaydeep Singh, Luna De Bruyne, Orphée De Clercq, Els Lefever, 10.18653/v1/2023.findings-emnlp.857Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>CLUTRR: A diagnostic benchmark for inductive reasoning from text. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, William L Hamilton, 10.18653/v1/D19-1458Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, +447 AuthorsAbhinav Rastogi, +447 AuthorsarXiv:2206.046152023Preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models. Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, Rui Yan, arXiv:2310.186592023Preprint</p>
<p>G Sutcliffe, The TPTP Problem Library and Associated Infrastructure. From CNF to TH0, TPTP. 201759v6.4.0.</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems. 202033</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Preprint</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Curran Associates, Inc2019a32</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 2019b32</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Linguistic complexity loss in text-based therapy. Jason Wei, Kelly Finn, Emma Templeton, Thalia Wheatley, Soroush Vosoughi, 10.18653/v1/2021.naacl-main.352Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022barXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 10.48550/ARXIV.2203.14465Star: Bootstrapping reasoning with reasoning. 2022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
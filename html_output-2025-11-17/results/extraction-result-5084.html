<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5084 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5084</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5084</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-272afc28d03890160b1f2808cc551c962ea9138c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/272afc28d03890160b1f2808cc551c962ea9138c" target="_blank">ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics, and introduces two novel statement auto Formalization methods: prompt retrieval and distilled backtranslation.</p>
                <p><strong>Paper Abstract:</strong> We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving. We report baseline results on statement autoformalization via in-context learning. Moreover, we introduce two novel statement autoformalization methods: prompt retrieval and distilled backtranslation.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5084.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5084.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code‑Davinci‑002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercially available decoder-only transformer model from OpenAI specialized for code generation and code-aware reasoning; used here as a strong few-shot in-context baseline for autoformalization and informalization of theorem statements into/from Lean 3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large pre-trained code-specialized decoder-only transformer (Codex family) used via the OpenAI API for few-shot in-context generation; architecture and exact parameter count are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofNet autoformalization / informalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Autoformalization: translate natural-language undergraduate-level math theorem statements into Lean 3 formal statements (strict dependent‑type logical reasoning). Informalization: generate natural-language statements from Lean statements.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot in-context learning (12-shot prompt); retrieval-augmented prompting (prompt retrieval using k=4 nearest mathlib statements retrieved by embedding-ada-002 against the model's tentative autoformalization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot Code-davinci-002: Formalization typecheck rate 23.7%, BLEU 25.1, formalization accuracy 13.4%. Informalization: compile rate 100%, BLEU 13.2, accuracy 62.3%. Retrieval-augmented Codex: typecheck rate 45.2%, BLEU 14.8, formalization accuracy 16.1% (accuracy +2.7 points over baseline; typecheck +21.5 points).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Still far from perfect: many autoformalizations fail to typecheck or are lexically incorrect; generates lexical errors (wrong mathlib identifiers), may miss implicit hypotheses (e.g., required typeclasses like inner_product_space), and sometimes produces incorrect logical content. BLEU is a poor predictor of correctness. Sensitivity to prompt composition.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms the open-source PROOFGPT models on these autoformalization/informalization tasks in this paper; retrieval augmentation notably improves Codex. Authors consider Minerva models superior overall (more data/scale) but do not evaluate them here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Retrieval against the model's provisional autoformalization (hat y) is more effective than retrieving against the original NL statement. Typecheck rate correlates strongly with formalization accuracy; BLEU correlates poorly. Retrieval improves typecheck and accuracy but may decrease BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5084.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5084.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROOFGPT-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROOFGPT (1.3B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source decoder-only causal language model initialized from Pythia weights and fine-tuned on an 8B-token 'proof-pile' corpus of mathematical text, released by the authors to provide openly available math-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>proofGPT-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only causal transformer initialized from Pythia-like weights and fine-tuned on the 'proof-pile' (math-heavy corpus). Reported training hyperparameters and 10.5B tokens seen during training; trained with GPT-NeoX.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofNet autoformalization / informalization (autoformalizing NL theorem statements to Lean 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate undergraduate-level natural-language theorem statements into Lean 3 formal statements (strict dependent-type theory) and informalization (Lean→NL).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot in-context learning baseline (6-shot prompt due to 2048-token context); fine-tuning via distilled backtranslation (student) using synthetic NL←→Lean pairs generated by a teacher model and a monolingual Lean corpus (mathlib) to create 90,530 NL-formal pairs for supervised finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot (in-context) proofGPT-1.3B: typecheck rate 5.9%, BLEU 8.1, formalization accuracy 0%. After distilled backtranslation fine-tuning: typecheck rate 19.4%, BLEU 10.7, formalization accuracy 3.2% (improves substantially over in-context baseline but remains far below Codex).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very low end-to-end formalization accuracy; often produces non-typechecking or looping/repetitive outputs; struggles with lexical choices specific to mathlib and with inferring implicit hypotheses and required typeclasses. Smaller parameter count and less math pretraining than top proprietary models likely limits performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Distilled backtranslation makes proofGPT-1.3B outperform its own few-shot baseline and also outperform the larger proofGPT-6.7B few-shot in-context performance; still substantially below Code-davinci-002. Authors regard PROOFGPT as inferior to Minerva due to less data and parameter range.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Fine-tuning with distilled backtranslation (using Davinci-codex-002 as teacher and mathlib as monolingual corpus) raised typecheck rate and accuracy; typecheck rate is recommended as a proxy metric correlating strongly with formalization accuracy. The authors note prompt-size/context length tradeoffs (6-shot for proofGPT due to shorter context).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5084.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5084.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROOFGPT-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROOFGPT (6.7B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger open-source PROOFGPT model (6.7B parameters) fine-tuned on the same proof-pile math corpus, intended to test scaling effects for math-specialized LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>proofGPT-6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only causal transformer initialized from Pythia-like development weights and fine-tuned on the proof-pile corpus; context length 2048 tokens; trained with GPT-NeoX.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6.7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofNet autoformalization / informalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same autoformalization tasks: NL→Lean and Lean→NL translations targeting strict formal logical correctness in Lean 3.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot in-context learning (6-shot prompt due to context length); evaluated as a baseline; no distilled backtranslation reported for this model in main results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot proofGPT-6.7B: typecheck rate 4.3%, BLEU 4.7, formalization accuracy 0%; informalization compile rate 0.70, BLEU 6.0, accuracy 6.5%. Did not produce any correct formalizations in the evaluation set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite being larger than the 1.3B model, it produced no correct formalizations in this evaluation; frequently loops/repeats short phrases, produces non-typechecking outputs, and makes lexical/mathlib identifier errors. Authors hypothesize smaller parameter count relative to Codex and differences in pretraining limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms code-davinci-002 by a wide margin on autoformalization; distilled backtranslation improved the 1.3B student to outperform the 6.7B few-shot baseline. Suggests that raw parameter size is not the only determinant — pretraining data composition and finetuning matter.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Observation: repetitive looping failure mode appeared in PROOFGPT generations but not in Code-davinci-002, suggesting scale/data/decoding differences influence degeneration. Authors note prompt/context length differences (fewer shots for proofGPT) and that distilled backtranslation can compensate for lack of parallel data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5084.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5084.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Davinci‑Codex‑002 (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Davinci-codex-002 (teacher model used for backtranslation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI code-capable large language model used as a teacher to generate synthetic NL (natural language) inputs from monolingual Lean theorems (backtranslation) to create training pairs for finetuning PROOFGPT (student).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>davinci-codex-002 (teacher LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A code-apt large language model (OpenAI) used as an oracle/teacher to sample synthetic NL←Lean backtranslations when building pseudo-parallel training data for distilled backtranslation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Synthetic backtranslation generation for autoformalization student training</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate NL paraphrases/statements given Lean theorem statements (i.e., backtranslation) to produce synthetic NL→Lean training pairs used to fine-tune a student model for strict formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Used in a distilled backtranslation pipeline: a few-shot prompt C is constructed, teacher samples X_i ~ P_LLM(X | C, Y_i) for Y_i drawn from monolingual Lean corpus (mathlib), producing synthetic NL for each Lean statement; student model is then fine-tuned on these pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not directly measured as a standalone solver in the paper; performance reported indirectly via student improvement: distilled backtranslation using davinci-codex-002 as teacher led proofGPT-1.3B to reach typecheck 19.4% and accuracy 3.2% (vs 5.9% typecheck and 0% accuracy few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality of synthetic NL depends on teacher; synthetic pairs can reflect teacher biases and errors. The approach relies on availability of a strong teacher LM and a large monolingual formal corpus (mathlib).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Used as a stronger black-box teacher than in-context-only approaches; authors note prior work using theorem prover feedback is not possible for Lean, motivating this approach. No direct comparison of different teacher models reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Distilled backtranslation improved student performance substantially over student few-shot; authors do not provide ablation isolating teacher choice, but report final student checkpoints at 15k steps (min validation loss).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autoformalization with large language models <em>(Rating: 2)</em></li>
                <li>Unsupervised neural machine translation with generative language models only <em>(Rating: 2)</em></li>
                <li>Generative language modeling for automated theorem proving <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
                <li>Formal mathematics statement curriculum learning <em>(Rating: 1)</em></li>
                <li>Hypertree proof search for neural theorem proving <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5084",
    "paper_id": "paper-272afc28d03890160b1f2808cc551c962ea9138c",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "Code‑Davinci‑002",
            "name_full": "OpenAI Codex (code-davinci-002)",
            "brief_description": "A commercially available decoder-only transformer model from OpenAI specialized for code generation and code-aware reasoning; used here as a strong few-shot in-context baseline for autoformalization and informalization of theorem statements into/from Lean 3.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex)",
            "model_description": "A large pre-trained code-specialized decoder-only transformer (Codex family) used via the OpenAI API for few-shot in-context generation; architecture and exact parameter count are not specified in this paper.",
            "model_size": null,
            "logical_reasoning_task": "ProofNet autoformalization / informalization",
            "task_description": "Autoformalization: translate natural-language undergraduate-level math theorem statements into Lean 3 formal statements (strict dependent‑type logical reasoning). Informalization: generate natural-language statements from Lean statements.",
            "method_or_approach": "Few-shot in-context learning (12-shot prompt); retrieval-augmented prompting (prompt retrieval using k=4 nearest mathlib statements retrieved by embedding-ada-002 against the model's tentative autoformalization).",
            "performance": "Few-shot Code-davinci-002: Formalization typecheck rate 23.7%, BLEU 25.1, formalization accuracy 13.4%. Informalization: compile rate 100%, BLEU 13.2, accuracy 62.3%. Retrieval-augmented Codex: typecheck rate 45.2%, BLEU 14.8, formalization accuracy 16.1% (accuracy +2.7 points over baseline; typecheck +21.5 points).",
            "limitations_or_failure_cases": "Still far from perfect: many autoformalizations fail to typecheck or are lexically incorrect; generates lexical errors (wrong mathlib identifiers), may miss implicit hypotheses (e.g., required typeclasses like inner_product_space), and sometimes produces incorrect logical content. BLEU is a poor predictor of correctness. Sensitivity to prompt composition.",
            "comparison": "Outperforms the open-source PROOFGPT models on these autoformalization/informalization tasks in this paper; retrieval augmentation notably improves Codex. Authors consider Minerva models superior overall (more data/scale) but do not evaluate them here.",
            "ablation_or_analysis_results": "Retrieval against the model's provisional autoformalization (hat y) is more effective than retrieving against the original NL statement. Typecheck rate correlates strongly with formalization accuracy; BLEU correlates poorly. Retrieval improves typecheck and accuracy but may decrease BLEU.",
            "uuid": "e5084.0",
            "source_info": {
                "paper_title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "PROOFGPT-1.3B",
            "name_full": "PROOFGPT (1.3B parameters)",
            "brief_description": "An open-source decoder-only causal language model initialized from Pythia weights and fine-tuned on an 8B-token 'proof-pile' corpus of mathematical text, released by the authors to provide openly available math-specialized models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "proofGPT-1.3B",
            "model_description": "Decoder-only causal transformer initialized from Pythia-like weights and fine-tuned on the 'proof-pile' (math-heavy corpus). Reported training hyperparameters and 10.5B tokens seen during training; trained with GPT-NeoX.",
            "model_size": "1.3B",
            "logical_reasoning_task": "ProofNet autoformalization / informalization (autoformalizing NL theorem statements to Lean 3)",
            "task_description": "Translate undergraduate-level natural-language theorem statements into Lean 3 formal statements (strict dependent-type theory) and informalization (Lean→NL).",
            "method_or_approach": "Few-shot in-context learning baseline (6-shot prompt due to 2048-token context); fine-tuning via distilled backtranslation (student) using synthetic NL←→Lean pairs generated by a teacher model and a monolingual Lean corpus (mathlib) to create 90,530 NL-formal pairs for supervised finetuning.",
            "performance": "Few-shot (in-context) proofGPT-1.3B: typecheck rate 5.9%, BLEU 8.1, formalization accuracy 0%. After distilled backtranslation fine-tuning: typecheck rate 19.4%, BLEU 10.7, formalization accuracy 3.2% (improves substantially over in-context baseline but remains far below Codex).",
            "limitations_or_failure_cases": "Very low end-to-end formalization accuracy; often produces non-typechecking or looping/repetitive outputs; struggles with lexical choices specific to mathlib and with inferring implicit hypotheses and required typeclasses. Smaller parameter count and less math pretraining than top proprietary models likely limits performance.",
            "comparison": "Distilled backtranslation makes proofGPT-1.3B outperform its own few-shot baseline and also outperform the larger proofGPT-6.7B few-shot in-context performance; still substantially below Code-davinci-002. Authors regard PROOFGPT as inferior to Minerva due to less data and parameter range.",
            "ablation_or_analysis_results": "Fine-tuning with distilled backtranslation (using Davinci-codex-002 as teacher and mathlib as monolingual corpus) raised typecheck rate and accuracy; typecheck rate is recommended as a proxy metric correlating strongly with formalization accuracy. The authors note prompt-size/context length tradeoffs (6-shot for proofGPT due to shorter context).",
            "uuid": "e5084.1",
            "source_info": {
                "paper_title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "PROOFGPT-6.7B",
            "name_full": "PROOFGPT (6.7B parameters)",
            "brief_description": "A larger open-source PROOFGPT model (6.7B parameters) fine-tuned on the same proof-pile math corpus, intended to test scaling effects for math-specialized LMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "proofGPT-6.7B",
            "model_description": "Decoder-only causal transformer initialized from Pythia-like development weights and fine-tuned on the proof-pile corpus; context length 2048 tokens; trained with GPT-NeoX.",
            "model_size": "6.7B",
            "logical_reasoning_task": "ProofNet autoformalization / informalization",
            "task_description": "Same autoformalization tasks: NL→Lean and Lean→NL translations targeting strict formal logical correctness in Lean 3.",
            "method_or_approach": "Few-shot in-context learning (6-shot prompt due to context length); evaluated as a baseline; no distilled backtranslation reported for this model in main results.",
            "performance": "Few-shot proofGPT-6.7B: typecheck rate 4.3%, BLEU 4.7, formalization accuracy 0%; informalization compile rate 0.70, BLEU 6.0, accuracy 6.5%. Did not produce any correct formalizations in the evaluation set.",
            "limitations_or_failure_cases": "Despite being larger than the 1.3B model, it produced no correct formalizations in this evaluation; frequently loops/repeats short phrases, produces non-typechecking outputs, and makes lexical/mathlib identifier errors. Authors hypothesize smaller parameter count relative to Codex and differences in pretraining limit performance.",
            "comparison": "Underperforms code-davinci-002 by a wide margin on autoformalization; distilled backtranslation improved the 1.3B student to outperform the 6.7B few-shot baseline. Suggests that raw parameter size is not the only determinant — pretraining data composition and finetuning matter.",
            "ablation_or_analysis_results": "Observation: repetitive looping failure mode appeared in PROOFGPT generations but not in Code-davinci-002, suggesting scale/data/decoding differences influence degeneration. Authors note prompt/context length differences (fewer shots for proofGPT) and that distilled backtranslation can compensate for lack of parallel data.",
            "uuid": "e5084.2",
            "source_info": {
                "paper_title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Davinci‑Codex‑002 (teacher)",
            "name_full": "Davinci-codex-002 (teacher model used for backtranslation)",
            "brief_description": "An OpenAI code-capable large language model used as a teacher to generate synthetic NL (natural language) inputs from monolingual Lean theorems (backtranslation) to create training pairs for finetuning PROOFGPT (student).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "davinci-codex-002 (teacher LM)",
            "model_description": "A code-apt large language model (OpenAI) used as an oracle/teacher to sample synthetic NL←Lean backtranslations when building pseudo-parallel training data for distilled backtranslation.",
            "model_size": null,
            "logical_reasoning_task": "Synthetic backtranslation generation for autoformalization student training",
            "task_description": "Generate NL paraphrases/statements given Lean theorem statements (i.e., backtranslation) to produce synthetic NL→Lean training pairs used to fine-tune a student model for strict formalization.",
            "method_or_approach": "Used in a distilled backtranslation pipeline: a few-shot prompt C is constructed, teacher samples X_i ~ P_LLM(X | C, Y_i) for Y_i drawn from monolingual Lean corpus (mathlib), producing synthetic NL for each Lean statement; student model is then fine-tuned on these pairs.",
            "performance": "Not directly measured as a standalone solver in the paper; performance reported indirectly via student improvement: distilled backtranslation using davinci-codex-002 as teacher led proofGPT-1.3B to reach typecheck 19.4% and accuracy 3.2% (vs 5.9% typecheck and 0% accuracy few-shot).",
            "limitations_or_failure_cases": "Quality of synthetic NL depends on teacher; synthetic pairs can reflect teacher biases and errors. The approach relies on availability of a strong teacher LM and a large monolingual formal corpus (mathlib).",
            "comparison": "Used as a stronger black-box teacher than in-context-only approaches; authors note prior work using theorem prover feedback is not possible for Lean, motivating this approach. No direct comparison of different teacher models reported.",
            "ablation_or_analysis_results": "Distilled backtranslation improved student performance substantially over student few-shot; authors do not provide ablation isolating teacher choice, but report final student checkpoints at 15k steps (min validation loss).",
            "uuid": "e5084.3",
            "source_info": {
                "paper_title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autoformalization with large language models",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised neural machine translation with generative language models only",
            "rating": 2
        },
        {
            "paper_title": "Generative language modeling for automated theorem proving",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1
        },
        {
            "paper_title": "Formal mathematics statement curriculum learning",
            "rating": 1
        },
        {
            "paper_title": "Hypertree proof search for neural theorem proving",
            "rating": 1
        }
    ],
    "cost": 0.01173675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics</h1>
<p>Zhangir Azerbayev<br>Yale College*<br>zhangir.azerbayev@yale.edu<br>Hailey Schoelkopf<br>EleutherAI, Yale College<br>hailey.schoelkopf@yale.edu<br>Dragomir Radev<br>Yale University<br>dragomir.radev@yale.edu</p>
<p>Bartosz Piotrowski<br>University of Warsaw*<br>bartoszpiotrowski@post.pl<br>Edward W. Ayers<br>Carnegie Mellon University<br>contact@edayers.com<br>Jeremy Avigad<br>Carnegie Mellon University<br>avigad@cmu.edu</p>
<h4>Abstract</h4>
<p>We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving. We report baseline results on statement autoformalization via in-context learning. Moreover, we introduce two novel statement autoformalization methods: prompt retrieval and distilled backtranslation.</p>
<h2>1 Introduction</h2>
<p>The creation of an automatic mathematician, that is, a system capable of autonomously posing conjectures and proving theorems, is a longstanding challenge in mathematics and artificial intelligence [Gelernter, 1959]. In recent years, neural generative language modelling has emerged as a promising approach to automating aspects of mathematics [Rabe and Szegedy, 2021].
One approach to applying language models to mathematics has been to treat mathematical reasoning in natural language as a sequence learning task [Welleck et al., 2021, 2022, Lewkowycz et al., 2022]. A key advantage of mathematical reasoning in natural language is the abundance of natural language mathematics data on the internet [Lewkowycz et al., 2022].
An alternative approach is to use language models to guide formal proof-search in an interactive theorem prover (ITP) [Yang and Deng, 2019, Polu and Sutskever, 2020, Polu et al., 2022, Jiang et al., 2022a]. A salient advantage of this method is that the ITP acts as a verifier for the language model's reasoning, enabling the natural implementation of bootstrapping techniques such as expert iteration [Silver et al., 2017, Polu et al., 2022].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ProofNet dataset (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Formal theorem statement: <br> theorem exercise_4_5_14 {G : Type*}</td>
</tr>
<tr>
<td style="text-align: center;">Lean mathlib</td>
<td style="text-align: center;">[group G] [fintype G] <br> (hG : card G = 312) :</td>
</tr>
<tr>
<td style="text-align: center;">Formal theorem statement: <br> theorem exists_subgroup_card_pow_prime</td>
<td style="text-align: center;">$\exists$ (p : N) (P : sylow p G), <br> P.normal</td>
</tr>
<tr>
<td style="text-align: center;">[fintype G] (p : N) {n : N}</td>
<td style="text-align: center;">Natural language theorem statement:</td>
</tr>
<tr>
<td style="text-align: center;">G) :</td>
<td style="text-align: center;">Prove that a group of order 312 has a normal</td>
</tr>
<tr>
<td style="text-align: center;">$\exists \mathrm{K}:$ subgroup G, fintype.card K = $\mathrm{p}^{\wedge} \mathrm{n}$</td>
<td style="text-align: center;">Sylow $p$-subgroup for some prime $p$ dividing its order.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Natural language proof:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Proof. Since $</td>
</tr>
</tbody>
</table>
<p>Figure 1: A sample theorem statement from mathlib, show on the left, and a sample theorem statement from ProofNet, shown on the right. Mathlib emphasizes including the most abstract and general formulations of mathematical results, whereas ProofNet predominantly tests the ability of models to apply those results to concrete problems.</p>
<p>Autoformalization, the task of automatically formalizing mathematics, seeks to build a bridge between informal and formal mathematical reasoning [Wang et al., 2018, Szegedy, 2020, Wu et al., 2022a], with the potential of extracting a training signal from vast corpora of natural language mathematics data while still grounding a system's reasoning in formal logic. However, lack of parallel data between informal and formal mathematics means that autoformalization suffers from a lack of standard benchmarks to guide progress in the field.
To remedy this gap, we propose ProofNet, ${ }^{2}$ a benchmark consisting of parallel natural language and formal mathematics that can be used to evaluate autoformalization and theorem proving. The ProofNet benchmark consists of 371 parallel formal theorem statements, natural language theorem statements, and natural language proofs sourced from the exercises of popular undergraduatelevel pure mathematics textbooks. Formal statements are expressed in the Lean 3 theorem prover [de Moura et al., 2015], and depend on Lean's mathlib [mathlib Community, 2020].
Language-model-based theorem provers and autoformalization systems have typically been evaluated on benchmarks consisting of competition and olympiad-style problems [Zheng et al., 2022, Wu et al., 2022a]. While such problems require complex reasoning, their solutions only depend on a relatively small set of elementary facts about integers, real numbers, counting, and geometry. In contrast, modern research mathematics requires the mastery of a massive body of theory made up of thousands of definitions, lemmas, and theorems. The Lean 3 formalization of perfectoid spaces, an important definition in research-level arithmetic geometry, depends on over 3000 distinct theorems and definitions [Buzzard et al., 2020]. How to effectively reason over such a large repository of knowledge is an important unsolved problem in applying language models to mathematics [Irving et al., 2016, Wu et al., 2022b, Tworkowski et al., 2022] .
ProofNet falls short of requiring mastery of all of modern mathematics, but poses the still ambitious goal of reasoning over the core of an undergraduate mathematics, including basic analysis, algebra, number theory, and topology. We hope that this benchmark will spur the development of language models that are able to reason effectively over large knowledge bases.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In order to obtain stronger baselines on ProofNet, we train and open-source the PROOFGPT language models at scales of 1.3 billion and 6.7 billion parameters. These models are trained on the proof-pile, an 8 billion token dataset of mathematical text. To our knowledge, these are the only open-source language models fine-tuned for general mathematics.
We establish baselines for ProofNet theorem autoformalization using in-context learning [Brown et al., 2020]. Moreover we introduce two novel theorem autoformalization methods that outperform our few-shot baselines. Prompt retrieval uses nearest-neighbor search against an embedding database to create a prompt consisting of the mathlib declarations most relevant to a particular natural language theorem. Distilled backtranslation is a method inspired by work in unsupervised machine translation [Lample et al., 2017, Han et al., 2021a] that finetunes a language model for autoformalization at a large scale without the need for parallel data.</p>
<h1>2 The ProofNet Benchmark</h1>
<p>Dataset collection Problems in the ProofNet benchmark are primarily drawn from exercises in popular undergraduate mathematics textbooks. For a complete list of sources, see Appendix B.
Not all textbook exercises lend themselves naturally to formalization. In particular, we only consider for inclusion in ProofNet problems meeting the following criteria:</p>
<ul>
<li>Self-containment. Problems should only depend on the results commonly taught in an undergraduate curriculum. In particular, this rules out problems that are split into multiple sequentially dependent parts, or those using nonstandard notations.</li>
<li>Naturality of formalization. Not all kinds of mathematical problems can be naturally formalized, such as word problems, and such problems are excluded. We do not include exercises that require computing an unknown quantity. We do not include problems that depend on parts of Lean's mathlib that are relatively less mature, such as Euclidean geometry or combinatorics.</li>
<li>Low risk of train-test overlap. Because language models are often pre-trained on large corpora mined from the internet that include mathlib, we refrain from including statements that are in mathlib or are likely to be added to mathlib in the future. In practice, this means we avoid the abstract "theory-building" style of theorems that constitute mathlib, and instead choose problems that involve applying general results to specific cases. For more insight into the stylistic differences between mathlib and ProofNet problems, see Figure 1.</li>
</ul>
<p>Beyond the above criteria, problems were selected for broad coverage of the undergraduate curriculum and to range in difficulty from straightforward applications of the definitions to those requiring tremendous creativity. Problems statements are transcribed into $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ and formalized by human annotators proficient in Lean. Natural language proofs are adapted from online solutions manuals, or in a few cases, written by the annotators.</p>
<p>Supported Tasks As ProofNet includes parallel natural language statements, natural language proofs, and formal statements, the dataset supports the evaluation of the following distinct tasks:</p>
<ul>
<li>Formal theorem proving. Given a formal statement of a theorem, produce a formal proof.</li>
<li>Informal theorem proving. Given an informal statement, produce an informal proof.</li>
<li>Autoformalization and informalization of statements. Given an informal (formal) statement, produce a corresponding formal (informal) statement.</li>
<li>Autoformalization of proofs. Given an informal theorem statement, its informal proof, and its formal statement, produce a formal proof.</li>
</ul>
<h2>3 The PROOFGPT models</h2>
<p>Many approaches to quantitative reasoning with language models depend on pre-training or finetuning a model on large corpora of mathematical text, which significantly boosts downstream performance [Hendrycks et al., 2021, Polu and Sutskever, 2020, Lample et al., 2022, Lewkowycz et al.,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: center;">Size (GB)</th>
<th style="text-align: center;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">arXiv.math</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">4.9 B</td>
</tr>
<tr>
<td style="text-align: left;">Stack Exchanges</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.3 B</td>
</tr>
<tr>
<td style="text-align: left;">Formal math libraries</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">59 M</td>
</tr>
<tr>
<td style="text-align: left;">ProofWiki + Wikipedia math articles</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">6.6 M</td>
</tr>
<tr>
<td style="text-align: left;">Open source books</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">6.5 M</td>
</tr>
<tr>
<td style="text-align: left;">MATH</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.9 M</td>
</tr>
</tbody>
</table>
<p>Table 1: Composition of the proof-pile.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">arXiv.math perplexity</th>
<th style="text-align: left;">proof-pile perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1B parameters:</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$\quad$ Pythia 1.4B</td>
<td style="text-align: left;">3.82</td>
<td style="text-align: left;">4.12</td>
</tr>
<tr>
<td style="text-align: left;">proof-GPT 1.3B</td>
<td style="text-align: left;">3.17</td>
<td style="text-align: left;">3.47</td>
</tr>
<tr>
<td style="text-align: left;">6B parameters:</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$\quad$ Pythia 6.9B</td>
<td style="text-align: left;">3.36</td>
<td style="text-align: left;">3.62</td>
</tr>
<tr>
<td style="text-align: left;">proof-GPT 6.7B</td>
<td style="text-align: left;">3.12</td>
<td style="text-align: left;">3.43</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of model perplexities on the test set of the arXiv subset of the proof-pile and the entire proof-pile. Documents were joined using two newline characters and perplexity was calculated with a stride equal to the model's context length, which is 2048 for all models shown.
2022]. Motivated by these results, we train and open-source the PROOFGPT models at sizes of 1.3 billion and 6.7 billion parameters ${ }^{3}$. The PROOFGPT models are decoder-only causual language models initialized with Pythia weights [Biderman et al., 2023] ${ }^{4}$, and then fine-tuned on the proof-pile, a corpus of mathematical text whose composition is detailed in Table 1. Fine-tuning was performed using the GPT-NeoX library [Andonian et al., 2021]. For training hyperparameters, see Appendix A. In Table 2, we show that the PROOFGPT models outperform Pythia base models and GPT-2 at standard mathematical reasoning tasks.</p>
<p>We regard the PROOFGPT model suite as inferior to the Minerva models [Lewkowycz et al., 2022] due to the fact that the PROOFGPT models are fine-tuned on an order of magnitude less mathematical text and span a smaller parameter range. However, we hope that the research community will benefit from PROOFGPT's open-source weights and dataset.</p>
<h1>4 Methodology and Experiments</h1>
<p>In this work, we evaluate the capabilities of pre-trained language models on autoformalizing and informalizing theorem statements. Due to the engineering challenges of implementing neural theorem proving systems in Lean, we leave an investigation of formal theorem proving and proof autoformalization to future work.</p>
<h3>4.1 Autoformalization methods</h3>
<p>We employ in-context learning with large language models as a strong baseline for the autoformalization of theorem statements [Wu et al., 2022a]. Moreover, we introduce two novel methods for boosting autoformalization performance above the few-shot baseline: prompt retrieval and distilled backtranslation.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.1.1 Few-shot autoformalization and informalization</h1>
<p>In-context learning is a simple and powerful method for adapting language models to sequence-tosequence tasks [Brown et al., 2020].
For our in-context baselines, we perform inference using the OpenAI API's Code-davinci-002 endpoint [Chen et al., 2021] and the proofGPT 1.3B and 6.7B models. Prompts are listed are given in Appendix C.</p>
<p>Because there may be multiple ways to formalize the same statement in Lean and no general way to automatically verify whether two statements that are not definitionally equal have the same mathematical content, autoformalizations are evaluated for correctness by a human expert. Informalizations are also judged by a human expert.</p>
<h3>4.1.2 Prompt retrieval</h3>
<p>A blessing and a curse of current language models is that few-shot learning performance is highly sensitive to the exact prompt that is used [Kojima et al., 2022]. In particular, it is plausible that greater few-shot learning performance can be achieved by retrieving the few-shot examples that are most relevant to a particular question.
We implement a prompt retrieval procedure for statement autoformalization as follows. Suppose we have a knowledge-base $\mathcal{K}$ of formal statements. First, we generate an autoformalization $\hat{y}$ of a statement $x$ using our standard in-context procedure. Then we produce dense vector representations of $\hat{y}$ and the formal statements in $\mathcal{K}$. We retrieve the $k$-nearest-neighbors of $\hat{y}$ in $\mathcal{K}$, and include them in the few-shot prompt. For the precise format of the prompt, see Appendix C.</p>
<p>We opt to retrieve against $\hat{y}$ instead of against $x$ because this method was significantly more performant in our preliminary experiments.
In our experiments, we create a knowledge-base $\mathcal{K}$ by taking our $y$ s to be 90,530 statements from Lean mathlib and use $k=4$. We use the OpenAI API's embedding-ada-002 endpoint to generate text embeddings.</p>
<h3>4.1.3 Distilled backtranslation</h3>
<p>A significant obstacle to fine-tuning language models on the autoformalization of theorem statements is the lack of large parallel corpora of formal and informal mathematics. In the face of this limitation, we draw on prior work leveraging generative models for unsupervised translation between natural languages. In particular, we use distilled backtranslation, a methodology inspired by Han et al. [2021a].
Distilled backtranslation proceeds as follows. Suppose we have a large language model $P_{L L M}(\cdot)$ pre-trained on monolingual data in both the source and target language, a monolingual corpus $\left{Y_{i}\right}$ in the target language. We wish to fine-tune a "student" model $P_{\theta}(Y \mid X)$ to translate a sequence $X$ in the source language to a corresponding sequence $Y$ in the target language. First, we manually construct a few-shot prompt $C$ consisting of $X \mid Y$ pairs. Then, we sample synthetic backtranslations $X_{i} \sim P_{L L M}\left(X \mid C, Y_{i}\right)$. Finally, we fine-tune $P_{\theta}(\cdot)$ on the synthetic pairs to predict $P(Y \mid X)$.
In our experiments, we fine-tune proofGPT-1.3B using distilled backtranslation with informal mathematics as the source language and Lean 3 theorems as the target language. We use the theorems in Lean's mathlib as the target language's monolingual corpus. We use Davinci-codex-002 as our teacher LM and proofGPT-1.3B as our student model. Fine-tuning hyperparameters are described in Appendix D</p>
<h2>5 Results and Discussion</h2>
<h3>5.1 In-context learning</h3>
<p>In Table 3, we present our experimental results for autoformalization and informalization of ProofNet theorem statements. Although conceptually simple and easy to implement, our Code-davinci-002 in-context learning baseline achieves highly nontrivial performance, correctly formalizing $13.4 \%$ of theorems. The ProofGPT models do not formalize any statements correctly, likely</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Formalization</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Informalization</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Typecheck rate</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Compile rate</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">proofGPT-1.3B</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">4.3</td>
</tr>
<tr>
<td style="text-align: left;">proofGPT-6.7B</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">6.5</td>
</tr>
<tr>
<td style="text-align: left;">Codex</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">62.3</td>
</tr>
<tr>
<td style="text-align: left;">Prompt retrieval:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Codex</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Dist. backtrans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">proofGPT-1.3B</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of few-shot learning with LLMs on formalization and informalization of ProofNet statements. In addition to reporting autoformalization accuracy, we also report typecheck rate, which is the proportion of a model's samples that are well-formed statements in Lean's dependent type theory. If a model simply copies a formal statement from its prompt, we do not consider that a positive sample when calculating typecheck rate. For the informalization task, we also report compile rate, i.e what proportion of the model's samples produce $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ that compiles. The most common reason why informal generations fail to compile is that they contain Unicode characters frequently used in Lean's mathlib but not accepted by the pdflatex compiler. To calculate BLEU scores, we split on whitespace and use BLEU-4 with smoothing. Note that formalization BLEU scores being higher than informalization BLEU scores is likely because natural language contains more lexically distinct but semantically equivalent statements.
owing to their smaller parameter count. However, they demonstrate some signal on the typecheck rate and BLEU metrics. Note that even generating statements that typecheck in Lean 3's strict type theory is a nontrivial feat.</p>
<p>Informalization accuracy is much higher than formalization accuracy for all models, supporting the intuitive claim that informalization is an easier task than formalization. This result also suggests that large pre-trained language models have a strong grasp of the semantics of formal mathematics, and primarily struggle with generating lexically correct and type correct Lean code.</p>
<p>We further observe that among Code-davinci-002's generations that typecheck, roughly half are correct formalizations. This is consistent with our hypothesis that Code-davinci-002 has a strong grasp of the semantics of mathematics, since the model displays high accuracy conditional on having generated valid Lean.</p>
<h1>5.2 Prompt Retrieval and Distilled Backtranslation</h1>
<p>In Table 3, we additionally include autoformalization scores for the prompt retrieval and distilled backtranslation models. Applying prompt retrieval to the Code-davinci-002 model significantly boosts performance, increasing accuracy by 2.7 points and, notably, increasing typecheck rate by 21.5 points.</p>
<p>Distilled backtranslation improves the autoformalization performance of the PROOFGPT 1.3B model not merely above the in-context performance of PROOFGPT 1.3b, but also above the incontext learning performance of PROOFGPT 6.7B.</p>
<p>Automatic metrics Typecheck rate correlates strongly with formalization accuracy, and we recommend that typecheck rate be used as a predictor of autoformalization performance when evaluating accuracy is too costly. The BLEU metric correlates well with performance on many NLP tasks [Papineni et al., 2002], but correlates poorly with performance code tasks [Chen et al., 2021]. Our findings illustrate that just as with code, BLEU is a poor guide to formalization performance, as prompt retrieval increases Code-davinci-002 formalization accuracy but decreases BLEU by over 10 points.</p>
<h1>5.3 Qualitative Analysis</h1>
<p>We ground our remaining analysis in four case studies: two that demonstrate successful formalizations produced by Code-davinci-002, and two that are representative of our methods' most common failure cases.</p>
<p>Case study 1. The following is a correct and idiomatic Lean mathlib formalization of exercise 10.7.4 in Michael Artin's Algebra produced by Code-davinci-002:</p>
<p>NL: Let $R$ be a ring, with $M$ an ideal of $R$. Suppose that every element of $R$ which is not in $M$ is a unit of $R$. Prove that $M$ is a maximal ideal and that moreover it is the only maximal ideal of $R$.
Code-davinci-002 output:
theorem exercise_10_7_10 {R : Type*} [ring R] (M : ideal R)
(hM : $\forall \mathrm{x}: \mathrm{R}, \mathrm{x} \notin \mathrm{M} \rightarrow$ is_unit x):
is_maximal $\mathrm{M} \wedge \forall(\mathrm{N}$ : ideal R$)$, is_maximal $\mathrm{N} \rightarrow \mathrm{N}=\mathrm{M}$
In this example, Code-davinci-002 demonstrates strong knowledge of mathlib and some reasoning capability. For example, the model correctly identifies that ideals are expressed using a typeclass ideal $R$, but maximal ideals are expressed using a predicate is_maximal. Moreover, the model correctly recognizes that an instance of the ideal type can be implicitly coerced to a set $R$ in order to apply the set-theoretic notation $\notin$. Finally, the model correctly infers that the best way to express "only maximal ideal" is the statement $\forall$ (N : ideal R), is_maximal $\mathrm{N} \rightarrow \mathrm{N}=\mathrm{R}$, which demonstrates some logical reasoning ability.</p>
<p>Case study 2. The following problem, exercise 6.4.2 from Michael Artin's Algebra, is an example of vanilla Davinci-code-002 making a mistake that is rectified by prompt retrieval.</p>
<p>NL: Prove that no group of order $p q$, where $p$ and $q$ are prime, is simple.
Code-davinci-002 output:
theorem exercise_6_4_2 {G : Type<em>} [group G] [fintype G] {p q : N}
(hp : nat.prime p) (hq : nat.prime q) (h : fintype.card G = p * q) :
$\neg$ simple_group G
Code-davinci-002 + retrieval output:
theorem exercise_6_4_2 {G : Type</em>} [group G] [fintype G] {p q : N}
[hp : fact(nat.prime p)] [hq : fact(nat.prime q)]
(h : fintype.card G = p * q):
$\neg$ is_simple_group G
Vanilla Davinci-code-002 makes a single lexical error: the model has correctly formalized the hypotheses of the problem, but has tried to use a nonexistent predicate simple_group instead of the correct is_simple_group. The retrieval-augmented model uses the correct declaration because one of the retrieved mathlib statements explicitly uses is_simple_group.</p>
<p>Case study 3. The following is an incorrect formalization of exercise 6.16 from Sheldon Axler's Linear Algebra produced by Code-davinci-002. The retrieval-augmented Code-davinci-002 autoformalization, not shown here, is also incorrect.</p>
<p>NL: Suppose $U$ is a subspace of $V$. Prove that $U^{\perp}={0}$ if and only if $U=V$
Code-davinci-002 output:
theorem exercise_6_16 {K : Type<em>} [field K]
${\mathrm{V}:$ Type</em>} [add_comm_group V] [vector_space K V] {U : subspace V} :
U.orthogonal $={\mathrm{O}} \leftrightarrow \mathrm{U}=\top$</p>
<p>ProofNet label:
theorem exercise_6_16 {K : Type*} [is_R_or_C K] [inner_product_space K V]
$(\mathrm{U}:$ submodule K V$)$ :
U.orthogonal $=\perp \leftrightarrow \mathrm{U}=\top$</p>
<p>Here, Code-davinci-002 makes three straightforward lexical mistakes: in mathlib, vector spaces over a field $K$ are denoted by module K V rather than vector_space K V , subspaces of a vector space by submodule K V rather than subspace V , and the trivial subspace is denoted by $\perp$ rather than ${0}$. However, the model also makes a much more significant logical error. In order for the orthogonal complement $U^{\perp}$ of a subspace to make sense, the space must be endowed with a inner product space structure rather than merely a vector space structure, which is expressed in mathlib as inner_product_space K V. Furthermore, inner product spaces are only defined over the real and complex fields, so one must also declare [is_R_or_C K]. Reliably inferring these kinds of implicit hypotheses is a major challenge for autoformalization systems.</p>
<p>Case Study 4: Looping. When generating formal and natural language theorem statements with PROOFGPTs, the model will often repeatedly generate the same short phrase instead of producing a plausible theorem. For example, consider the attempted formalization of exercise 10.1.13 from Michael Artin's Algebra generated by PROOFGPT 6.7B via in-context learning.</p>
<p>NL: An element $x$ of a ring $R$ is called nilpotent if some power of $x$ is zero.
Prove that if $x$ is nilpotent, then $1+x$ is a unit in $R$.
proofGPT-6.7b output:
theorem nilpotent_of_nilpotent_of_nilpotent_of_nilpotent_of_nilpotent nilpotent_of_nilpotent_of_nilpotent_of_nilpotent_of_nilpotent_of_</p>
<p>Prior work on decoding methods has shown that the likelihood of a repeated phrase increases with each repetition, and that greedy decoding generates text with higher likelihood than natural text [Holtzman et al., 2019]. These two findings constitute a plausible explanation for repetitive looping if the correct autoformalization is assigned low likelihood by the model. We observe that repetitive looping does not occur with Code-davinci-002, suggesting that the problem may disappear with scale (although there are many other differences between our small-scale models and Code-davinci002).</p>
<h1>6 Related Work</h1>
<p>Language modeling for theorem proving Language models have found success in theorem proving both in the natural language setting [Lewkowycz et al., 2022, Welleck et al., 2021], and within many major ITPs such as Metamath [Polu and Sutskever, 2020], Isabelle [Jiang et al., 2022a], and Lean [Han et al., 2021b, Polu et al., 2022]. Popular benchmarks for evaluating language modelbased provers are Hendrycks et al. [2021] and Welleck et al. [2021] for natural language, and Zheng et al. [2022] for formal.</p>
<p>Autoformalization Recent work in autoformalization with language models was sparked by Wu et al. [2022a], which demonstrated that models can autoformalize Isabelle theorem statements via in-context learning. In Jiang et al. [2022b], the authors demonstrate a method for autoformalizing proofs in Isabelle. However, their method depends on the availability of a performant black-box automated theorem prover, which is not available for Lean at the time of writing.</p>
<p>Interactive Theorem Proving Work in formal theorem proving and autoformalization depends on libraries of formalized mathematics. This work directly depends on Lean's mathlib, but indirectly benefits from lessons learned from other proofs systems such as Coq [Bertot and Castéran, 2004], Mizar [Grabowski et al., 2010], and Isabelle [Nipkow et al., 2002].</p>
<p>Unsupervised Machine Translation Because the amount of parallel formal and natural language text is negligible, autoformalization faces many of the same challenges as unsupervised machine translation [Lample et al., 2017, Han et al., 2021a, Garcia et al., 2023]. Our distilled backtranslation method is inspired by the distilled and iterated backtranslation algorithm of Han et al. [2021a]. However, the authors of this work regard backtranslation as a temporary hack and foresee that in-context learning will be enough to elicit maximal performance from a sufficiently good language model, as is now the case for unsupervised translation [Garcia et al., 2023].</p>
<h1>7 Conclusion</h1>
<p>We introduced ProofNet, a benchmarking consisting of parallel natural language theorem statements, natural language proofs, and formal theorem statements in Lean 3. We have shown that pre-trained large language models achieve non-trivial but far from consistent performance via incontext learning on the autoformalization of ProofNet statements. Moreover, we have proposed prompt retrieval and distilled backtranslation, two methods that improve autoformalization performance above baseline.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank the Hoskinson Center for Formal Mathematics at Carnegie Mellon University for its generous funding and for providing a stimulating work environment. We additionally thank EleutherAI for providing compute to train the PROOFGPT models. Piotrowski was supported also by the grant of National Science Center, Poland, no. 2018/29/N/ST6/02903, and by the Kosciuszko Fundation.</p>
<h2>References</h2>
<p>Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach. GPTNeoX: Large Scale Autoregressive Language Modeling in PyTorch, 8 2021. URL https://www.github.com/eleutherai/gpt-neox.</p>
<p>Yves Bertot and Pierre Castéran. Interactive Theorem Proving and Program Development - Coq'Art: The Calculus of Inductive Constructions. Springer, 2004.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: a scaling suite for language model interpretability research. Computing Research Repository, 2023. doi: 10.48550/arXiv.2201.07311. URL https://arxiv.org/abs/2201.07311v1. version 1.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.</p>
<p>Kevin Buzzard, Johan Commelin, and Patrick Massot. Formalising perfectoid spaces. In Jasmin Blanchette and Catalin Hritcu, editors, Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, CPP 2020, New Orleans, LA, USA, January 20-21, 2020, pages 299-312. ACM, 2020. doi: 10.1145/3372885.3373830. URL https://doi.org/10.1145/3372885.3373830.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.</p>
<p>Leonardo Mendonça de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von Raumer. The Lean theorem prover (system description). In Amy P. Felty and Aart Middeldorp, editors, Conference on Automated Deduction (CADE) 2015, pages 378-388. Springer, 2015.</p>
<p>Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. The unreasonable effectiveness of few-shot learning for machine translation, 2023. URL https://arxiv.org/abs/2302.01398.</p>
<p>Herbert L. Gelernter. Realization of a geometry theorem proving machine. In IFIP Congress, 1959.
Adam Grabowski, Artur Kornilowicz, and Adam Naumowicz. Mizar in a nutshell. J. Formalized Reasoning, 3(2):153-245, 2010.</p>
<p>Jesse Michael Han, Igor Babuschkin, Harrison Edwards, Arvind Neelakantan, Tao Xu, Stanislas Polu, Alex Ray, Pranav Shyam, Aditya Ramesh, Alec Radford, and Ilya Sutskever. Unsupervised neural machine translation with generative language models only, 2021a. URL https://arxiv.org/abs/2110.05448.</p>
<p>Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers, and Stanislas Polu. Proof artifact co-training for theorem proving with language models, 2021b. URL https://arxiv.org/abs/2102.06203.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2019. URL https://arxiv.org/abs/1904.09751.</p>
<p>Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas Een, Francois Chollet, and Josef Urban. Deepmath - deep sequence models for premise selection. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf.</p>
<p>Albert Q. Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers, 2022a. URL https://arxiv.org/abs/2205.10893.</p>
<p>Albert Q. Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs, 2022b. URL https://arxiv.org/abs/2210.12283.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2022. URL https://arxiv.org/abs/2205.11916.</p>
<p>Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only, 2017. URL https://arxiv.org/abs/1711.00043.</p>
<p>Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aurélien Rodriguez, and Timothée Lacroix. Hypertree proof search for neural theorem proving, 2022. URL https://arxiv.org/abs/2205.11491.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858.</p>
<p>The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs. ACM, jan 2020. doi: 10.1145/3372885.3373824. URL https://doi.org/10.1145\%2F3372885.3373824.</p>
<p>Tobias Nipkow, Lawrence C. Paulson, and Markus Wenzel. Isabelle/HOL - A Proof Assistant for Higher-Order Logic. Springer, 2002.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318, 2002.</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020. URL https://arxiv.org/abs/2009.03393.</p>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning, 2022. URL https://arxiv.org/abs/2202.01344.</p>
<p>Markus N. Rabe and Christian Szegedy. Towards the automatic mathematician. In André Platzer and Geoff Sutcliffe, editors, Automated Deduction - CADE 28, pages 25-37, Cham, 2021. Springer International Publishing. ISBN 978-3-030-79876-5.</p>
<p>David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, L. Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. ArXiv, abs/1712.01815, 2017.</p>
<p>Christian Szegedy. A promising path towards autoformalization and general artificial intelligence. In Christoph Benzmüller and Bruce Miller, editors, Intelligent Computer Mathematics, pages 3-20, Cham, 2020. Springer International Publishing. ISBN 978-3-030-53518-6.</p>
<p>Szymon Tworkowski, Maciej Mikuła, Tomasz Odrzygóźdź, Konrad Czechowski, Szymon Antoniak, Albert Jiang, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, and Yuhuai Wu. Formal premise selection with language models, 2022. URL http://aitp-conference.org/2022/abstract/AITP_2022_paper_32.pdf.</p>
<p>Qingxiang Wang, Cezary Kaliszyk, and Josef Urban. First experiments with neural translation of informal to formal mathematics. In Florian Rabe, William M. Farmer, Grant O. Passmore, and Abdou Youssef, editors, Intelligent Computer Mathematics, pages 255-270, Cham, 2018. Springer International Publishing. ISBN 978-3-319-96812-4.</p>
<p>Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language, 2021. URL https://arxiv.org/abs/2104.01112.</p>
<p>Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models, 2022. URL https://arxiv.org/abs/2205.12910.</p>
<p>Yuhuai Wu, Albert Q. Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models, 2022a. URL https://arxiv.org/abs/2205.12615.</p>
<p>Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=TrjbxzRcnf-.</p>
<p>Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants, 2019. URL https://arxiv.org/abs/1905.09381.</p>
<p>Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: a cross-system benchmark for formal olympiad-level mathematics. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=9ZPegFuFTFv.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Parameter</td>
<td style="text-align: center;">1.3 B</td>
<td style="text-align: center;">6.7 B</td>
</tr>
<tr>
<td style="text-align: left;">Tokens</td>
<td style="text-align: center;">10.5 billion</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Training Steps</td>
<td style="text-align: center;">40,000</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate Max</td>
<td style="text-align: center;">$2 \cdot 10^{-4}$</td>
<td style="text-align: center;">$1.2 \cdot 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate Min</td>
<td style="text-align: center;">$2 \cdot 10^{-5}$</td>
<td style="text-align: center;">$1.2 \cdot 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">Adam</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Adam Betas</td>
<td style="text-align: center;">$(0.9,0.95)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Adam Eps</td>
<td style="text-align: center;">$1 \cdot 10^{-6}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LR Scheduler</td>
<td style="text-align: center;">Cosine w/ warm-up</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LR Warm-up Steps</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Effective Batch Size</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Precision</td>
<td style="text-align: center;">FP16</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Gradient Clipping</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: PROOFGPT training hyperparameters.</p>
<h1>A PROOFGPT training</h1>
<p>Table 4 displays hyperparameters for PROOFGPT training on the PROOF-PILE.</p>
<h2>B Problem Sources</h2>
<p>The following is a complete list of sources ProofNet draws from:</p>
<ul>
<li>Analysis: Walter Rudin's Principles of Mathematical Analysis 3rd ed, Charles C. Pugh's Real Mathematical Analysis 1st ed, Elias M. Stein and Rami Shakarchi's Complex Analysis 1st ed.</li>
<li>Linear Algebra: Sheldon Axler's Linear Algebra Done Right 2nd ed.</li>
<li>Abstract Algebra: David S. Dummit and Richard M. Foote's Abstract Algebra 3rd ed, I.N. Herstein's Abstract Algebra 3rd ed, and Michael Artin's Algebra 1st ed.</li>
<li>Topology: James Munkres' Topology 2nd ed.</li>
<li>Examinations: Putnam Competition.</li>
</ul>
<h2>C Prompts</h2>
<p>Prompts are viewable in the open-source repository ${ }^{5}$. We use a 12-shot prompt for Code-davinci002 autoformalization and informalization, and a 6-shot prompt for proofGPT autoformalization and informalization. We give proofGPT models fewer examples because of its shorter context (2048 tokens compared to 8192), we only use the last six examples when prompting proofGPT.</p>
<p>For retrieval augmented models, we use a 3-shot prompt, where each example consists of 4 reference formal statements and one NL-formal pair.</p>
<h2>D Finetuning</h2>
<p>Our fine-tuning dataset of backtranslations consists of 90,530 NL-formal pairs. Both the Pythia-1.4b and PROOFGPT-1.3B model are finetuned according to the hyperparameters above. The models evaluated in Table 3 are the minimum validation loss checkpoint, which occurs at 15,000 training steps.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">Setting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training Steps</td>
<td style="text-align: center;">20,000</td>
</tr>
<tr>
<td style="text-align: center;">Learning Rate (LR)</td>
<td style="text-align: center;">$5 \cdot 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: center;">Adam Betas</td>
<td style="text-align: center;">$(0.9,0.999)$</td>
</tr>
<tr>
<td style="text-align: center;">Adam Eps</td>
<td style="text-align: center;">$1 \cdot 10^{-8}$</td>
</tr>
<tr>
<td style="text-align: center;">Weight Decay</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">LR Scheduler</td>
<td style="text-align: center;">Cosine w/ warm-up</td>
</tr>
<tr>
<td style="text-align: center;">LR Warm-up Steps</td>
<td style="text-align: center;">2000</td>
</tr>
<tr>
<td style="text-align: center;">Effective Batch Size</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">FP16</td>
</tr>
<tr>
<td style="text-align: center;">Gradient Clipping</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Student training hyperparameters.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/zhangir-azerbayev/ProofNet/tree/main/eval/prompts&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
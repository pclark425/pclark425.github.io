<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5763 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5763</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5763</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-87dde6e5f221bf697d79b74f2efafaca9da220fd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/87dde6e5f221bf697d79b74f2efafaca9da220fd" target="_blank">RAGLog: Log Anomaly Detection using Retrieval Augmented Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This research work explores the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs and the experimental results show much promise.</p>
                <p><strong>Paper Abstract:</strong> The ability to detect log anomalies from system logs is a vital activity needed to ensure cyber resiliency of systems. It is applied for fault identification or facilitate cyber investigation and digital forensics. However, as logs belonging to different systems and components differ significantly, the challenge to perform such analysis is humanly challenging from the volume, variety and velocity of logs. This is further complicated by the lack or unavailability of anomalous log entries to develop trained machine learning or artificial intelligence models for such purposes. In this research work, we explore the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs. We used a Question and Answer configuration pipeline. To the best of our knowledge, our experiment which we called RAGLog is a novel one and the experimental results show much promise.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5763.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5763.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAGLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAGLog: Log Anomaly Detection using Retrieval Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot log anomaly detection pipeline that stores samples of normal log entries in a vector database (selected via random or k-means clustering), retrieves nearest normal examples with dense OpenAI embeddings, and prompts an LLM (GPT-3.5 Davinci) in a QA format to label a queried log entry as 'normal' or 'abnormal'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative large language model used as a prompt-driven classifier in a Retrieval-Augmented Generation pipeline; the paper does not report architecture or parameter counts, but uses the model with temperature=0.1 and supplies retrieved normal-log context to the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Zero-shot, prompt-based QA classification within a Retrieval-Augmented Generation (RAG) pipeline: (1) encode logs with OpenAI embeddings and store normal samples in a vector DB; (2) retrieve nearest normal examples; (3) feed retrieved examples + queried log entry into GPT-3.5 via a Q&A prompt that returns 'normal' or 'abnormal'.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured textual system logs (time-sequenced log entries)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log entries / semantic anomalies / outlier events in log streams</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL; Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated with precision, recall, and F1. Reported aggregated performance for the RAGLog system: Precision = 0.91, Recall = 0.88, F1 = 0.89 (zero-shot, tested on held-out ~20% sample).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against LogPrompt (zero-shot) which reported Precision=0.25, Recall=0.83, F1=0.38; RAGLog substantially outperformed that zero-shot baseline, especially on precision and F1. The paper also reports that using k-means clustering to select normal samples improved results (notably on BGL) versus random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High resource consumption and execution latency (LLM per-log invocation); cost constraints limited testing to a sample of the datasets; general LLM limitations noted (token capacity, potential for hallucination), although the authors reported no hallucinated output in their runs; the approach analyzes one log entry at a time which impacts throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5763.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The LLM used in RAGLog to perform semantic comparison and zero-shot classification of log entries, invoked via prompt to return 'normal' or 'abnormal'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercial generative transformer LLM (architecture/parameter count not specified in the paper) used as the final classifier in the RAG pipeline with temperature set to 0.1; receives retrieved normal-log context in a QA prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based zero-shot classification (QA prompt), fed with retrieved context from a vector DB (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured textual log entries (sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Semantic anomalies / anomalous log messages</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL; Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As used within RAGLog: system-level Precision=0.91, Recall=0.88, F1=0.89.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Part of RAGLog which outperformed LogPrompt zero-shot baseline in reported metrics; no direct comparison to non-LLM classical methods reported quantitatively in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Costly to run at scale; latency per-item; general LLM constraints (token window limits, hallucination risk) discussed; paper notes limited scope of testing due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5763.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The two-stage paradigm used: dense-vector retrieval of relevant normal-log examples from a vector database followed by generation/analysis by an LLM to classify a queried entry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dense-vector retrieval (OpenAI embeddings) to find semantically-similar normal log entries via inner-product similarity, then generation/QA by an LLM (GPT-3.5); retriever implemented with LangChain.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Retrieve nearest normal examples to provide contextual evidence to an LLM; classify queried log entry in a zero-shot QA prompt (no anomalous examples provided during inference).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured log sequences (textual log lines), used as retrieval corpus of normal samples</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Semantic anomalies / outlier log messages</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL; Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When implemented as RAGLog, achieved Precision=0.91, Recall=0.88, F1=0.89 (system-level).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>RAG construct with cluster-selected normal samples outperformed random-selection population on BGL; RAGLog outperformed LogPrompt zero-shot baseline in reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires maintaining a vector DB of normal samples and the retrieval pipeline; performance depends on quality/coverage of stored normal samples; additional resource and latency costs from retrieval + LLM steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5763.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Embedding model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI pre-trained embedding model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained embedding encoder (and associated decoder step as described) used to convert log entries to dense vectors for similarity retrieval in the RAG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Embedding model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained embedding model (not specified which variant) used to encode log entries into dense vectors stored in a vector database; inner-product similarity used for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Supports dense-vector retrieval to provide nearest-normal examples to the LLM for zero-shot classification.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Textual log entries</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Supports detection of semantic anomalies by enabling retrieval of semantically-similar normal examples</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL; Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No standalone metrics reported; component contributes to overall RAGLog performance (Precision=0.91, Recall=0.88, F1=0.89).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared in isolation to alternative embedding methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper does not specify embedding variant or hyperparameters; retrieval quality depends on embedding fidelity and vector DB coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5763.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain retriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain retriever (library)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The retrieval implementation used to fetch nearest vectors (normal log samples) from the vector database via similarity search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LangChain retriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Tooling/framework component (LangChain) used to perform dense-vector retrieval over stored embeddings; inner-product similarity scoring to return top matches.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Provides retrieval step for the RAG zero-shot classification pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Vectorized representations of textual logs</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>N/A (infrastructure for detecting anomalies via retrieval + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL; Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No independent metrics reported; used within RAGLog that achieved Precision=0.91, Recall=0.88, F1=0.89.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared to other retrievers in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality dependent on embedding model and stored sample selection; not evaluated in isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5763.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogPrompt: Prompt Engineering Towards ZeroShot and Interpretable Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced zero-shot LLM log-analysis method (prior work) used as a numerical baseline for comparison; reported very low precision but high recall in zero-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogPrompt: Prompt Engineering Towards ZeroShot and Interpretable Log Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM zero-shot (unspecified in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineering approach for zero-shot log analysis evaluated in prior work; paper reports low precision in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Zero-shot prompt-based classification (various prompt formats tested per referenced work: self-prompt, chain-of-thought, in-context, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured log entries</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log messages</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in this paper as Precision=0.25, Recall=0.83, F1=0.38 (used as a comparison for zero-shot classification).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>RAGLog substantially improved precision and F1 compared to LogPrompt's reported zero-shot results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low precision in zero-shot settings (high false positive rate), per the referenced results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5763.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogGPT / ChatGPT (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior attempted application of ChatGPT to log anomaly detection referenced in the paper; authors report sensitivity to prompt formulation, window size limitations, and high false-positive rates in that work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-family conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conversational LLM used in referenced experiments with varied prompt constructs and windowing; architecture/size not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based experiments with varying prompt formats and window sizes for log input.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured log sequences</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous log entries</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Described qualitatively as having high false positive rates and sensitivity to prompt/window settings; no numeric metrics reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Referenced as showing limitations of direct ChatGPT application for anomaly detection compared with more structured approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Non-trivial selection of optimal prompt and window size; high false positive rates reported in the referenced study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5763.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BGL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BGL (BlueGene/L) system logs dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real-world HPC log dataset from a BlueGene/L supercomputer that contains many new log messages over time and labeled anomalies with class imbalance; used to evaluate RAGLog.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset of system log messages with time sequences and labeled anomaly indicators; noted for a wide distribution of log patterns and evolving messages over time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Used as evaluation data for zero-shot RAGLog; authors sampled normal entries to populate vector DB (either random or via k-means cluster sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-sequenced unstructured textual logs</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous / alert log messages (binary labels present)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BGL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RAGLog reported system-level metrics (Precision=0.91, Recall=0.88, F1=0.89) over held-out test samples; paper notes clustering helped performance on BGL versus random sampling but does not provide per-dataset metric split in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>RAGLog's clustering-based population outperformed random population on BGL; compared against LogPrompt zero-shot baseline overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dataset exhibits wide pattern distribution and changing message vocabulary over time, making detection more challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5763.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5763.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thunderbird system logs dataset (Sandia National Labs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A labeled log dataset containing alert and non-alert messages from Sandia National Labs; used alongside BGL to evaluate RAGLog. Reported to have more concentrated pattern distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset of system logs with binary anomaly labels; authors sampled normal entries to populate the vector DB (random vs clustered) and evaluated RAGLog performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Used to evaluate RAGLog zero-shot classification with vector DB populated by normal samples (random or k-means cluster sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Unstructured textual logs (sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Alert vs non-alert / anomalous log messages</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RAGLog reported overall system-level metrics across datasets: Precision=0.91, Recall=0.88, F1=0.89; authors note Thunderbird performed well with both random and clustered sample population strategies (no per-dataset numeric split provided in the table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performed well under both population strategies; compared favorably to LogPrompt zero-shot baseline overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>None specific beyond those stated for RAGLog; dataset has more concentrated pattern distribution which may make retrieval easier.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAGLog: Log Anomaly Detection using Retrieval Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogPrompt: Prompt Engineering Towards ZeroShot and Interpretable Log Analysis <em>(Rating: 2)</em></li>
                <li>LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection <em>(Rating: 2)</em></li>
                <li>An Assessment of ChatGPT on Log Data <em>(Rating: 2)</em></li>
                <li>Log-based Anomaly Detection without Log Parsing <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5763",
    "paper_id": "paper-87dde6e5f221bf697d79b74f2efafaca9da220fd",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "RAGLog",
            "name_full": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
            "brief_description": "A zero-shot log anomaly detection pipeline that stores samples of normal log entries in a vector database (selected via random or k-means clustering), retrieves nearest normal examples with dense OpenAI embeddings, and prompts an LLM (GPT-3.5 Davinci) in a QA format to label a queried log entry as 'normal' or 'abnormal'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (Davinci)",
            "model_description": "Generative large language model used as a prompt-driven classifier in a Retrieval-Augmented Generation pipeline; the paper does not report architecture or parameter counts, but uses the model with temperature=0.1 and supplies retrieved normal-log context to the prompt.",
            "model_size": null,
            "anomaly_detection_method": "Zero-shot, prompt-based QA classification within a Retrieval-Augmented Generation (RAG) pipeline: (1) encode logs with OpenAI embeddings and store normal samples in a vector DB; (2) retrieve nearest normal examples; (3) feed retrieved examples + queried log entry into GPT-3.5 via a Q&A prompt that returns 'normal' or 'abnormal'.",
            "data_type": "Unstructured textual system logs (time-sequenced log entries)",
            "anomaly_type": "Anomalous log entries / semantic anomalies / outlier events in log streams",
            "dataset_name": "BGL; Thunderbird",
            "performance_metrics": "Evaluated with precision, recall, and F1. Reported aggregated performance for the RAGLog system: Precision = 0.91, Recall = 0.88, F1 = 0.89 (zero-shot, tested on held-out ~20% sample).",
            "baseline_comparison": "Compared against LogPrompt (zero-shot) which reported Precision=0.25, Recall=0.83, F1=0.38; RAGLog substantially outperformed that zero-shot baseline, especially on precision and F1. The paper also reports that using k-means clustering to select normal samples improved results (notably on BGL) versus random selection.",
            "limitations_or_failure_cases": "High resource consumption and execution latency (LLM per-log invocation); cost constraints limited testing to a sample of the datasets; general LLM limitations noted (token capacity, potential for hallucination), although the authors reported no hallucinated output in their runs; the approach analyzes one log entry at a time which impacts throughput.",
            "uuid": "e5763.0",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3.5 (Davinci)",
            "name_full": "GPT-3.5 (Davinci)",
            "brief_description": "The LLM used in RAGLog to perform semantic comparison and zero-shot classification of log entries, invoked via prompt to return 'normal' or 'abnormal'.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (Davinci)",
            "model_description": "A commercial generative transformer LLM (architecture/parameter count not specified in the paper) used as the final classifier in the RAG pipeline with temperature set to 0.1; receives retrieved normal-log context in a QA prompt.",
            "model_size": null,
            "anomaly_detection_method": "Prompt-based zero-shot classification (QA prompt), fed with retrieved context from a vector DB (RAG).",
            "data_type": "Unstructured textual log entries (sequences)",
            "anomaly_type": "Semantic anomalies / anomalous log messages",
            "dataset_name": "BGL; Thunderbird",
            "performance_metrics": "As used within RAGLog: system-level Precision=0.91, Recall=0.88, F1=0.89.",
            "baseline_comparison": "Part of RAGLog which outperformed LogPrompt zero-shot baseline in reported metrics; no direct comparison to non-LLM classical methods reported quantitatively in the paper.",
            "limitations_or_failure_cases": "Costly to run at scale; latency per-item; general LLM constraints (token window limits, hallucination risk) discussed; paper notes limited scope of testing due to cost.",
            "uuid": "e5763.1",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval Augmented Generation",
            "brief_description": "The two-stage paradigm used: dense-vector retrieval of relevant normal-log examples from a vector database followed by generation/analysis by an LLM to classify a queried entry.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Dense-vector retrieval (OpenAI embeddings) to find semantically-similar normal log entries via inner-product similarity, then generation/QA by an LLM (GPT-3.5); retriever implemented with LangChain.",
            "model_size": null,
            "anomaly_detection_method": "Retrieve nearest normal examples to provide contextual evidence to an LLM; classify queried log entry in a zero-shot QA prompt (no anomalous examples provided during inference).",
            "data_type": "Unstructured log sequences (textual log lines), used as retrieval corpus of normal samples",
            "anomaly_type": "Semantic anomalies / outlier log messages",
            "dataset_name": "BGL; Thunderbird",
            "performance_metrics": "When implemented as RAGLog, achieved Precision=0.91, Recall=0.88, F1=0.89 (system-level).",
            "baseline_comparison": "RAG construct with cluster-selected normal samples outperformed random-selection population on BGL; RAGLog outperformed LogPrompt zero-shot baseline in reported metrics.",
            "limitations_or_failure_cases": "Requires maintaining a vector DB of normal samples and the retrieval pipeline; performance depends on quality/coverage of stored normal samples; additional resource and latency costs from retrieval + LLM steps.",
            "uuid": "e5763.2",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "OpenAI Embedding model",
            "name_full": "OpenAI pre-trained embedding model",
            "brief_description": "Pre-trained embedding encoder (and associated decoder step as described) used to convert log entries to dense vectors for similarity retrieval in the RAG pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI Embedding model",
            "model_description": "Pre-trained embedding model (not specified which variant) used to encode log entries into dense vectors stored in a vector database; inner-product similarity used for retrieval.",
            "model_size": null,
            "anomaly_detection_method": "Supports dense-vector retrieval to provide nearest-normal examples to the LLM for zero-shot classification.",
            "data_type": "Textual log entries",
            "anomaly_type": "Supports detection of semantic anomalies by enabling retrieval of semantically-similar normal examples",
            "dataset_name": "BGL; Thunderbird",
            "performance_metrics": "No standalone metrics reported; component contributes to overall RAGLog performance (Precision=0.91, Recall=0.88, F1=0.89).",
            "baseline_comparison": "Not compared in isolation to alternative embedding methods in the paper.",
            "limitations_or_failure_cases": "Paper does not specify embedding variant or hyperparameters; retrieval quality depends on embedding fidelity and vector DB coverage.",
            "uuid": "e5763.3",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LangChain retriever",
            "name_full": "LangChain retriever (library)",
            "brief_description": "The retrieval implementation used to fetch nearest vectors (normal log samples) from the vector database via similarity search.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LangChain retriever",
            "model_description": "Tooling/framework component (LangChain) used to perform dense-vector retrieval over stored embeddings; inner-product similarity scoring to return top matches.",
            "model_size": null,
            "anomaly_detection_method": "Provides retrieval step for the RAG zero-shot classification pipeline.",
            "data_type": "Vectorized representations of textual logs",
            "anomaly_type": "N/A (infrastructure for detecting anomalies via retrieval + LLM)",
            "dataset_name": "BGL; Thunderbird",
            "performance_metrics": "No independent metrics reported; used within RAGLog that achieved Precision=0.91, Recall=0.88, F1=0.89.",
            "baseline_comparison": "Not compared to other retrievers in the paper.",
            "limitations_or_failure_cases": "Quality dependent on embedding model and stored sample selection; not evaluated in isolation.",
            "uuid": "e5763.4",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LogPrompt",
            "name_full": "LogPrompt: Prompt Engineering Towards ZeroShot and Interpretable Log Analysis",
            "brief_description": "A referenced zero-shot LLM log-analysis method (prior work) used as a numerical baseline for comparison; reported very low precision but high recall in zero-shot experiments.",
            "citation_title": "LogPrompt: Prompt Engineering Towards ZeroShot and Interpretable Log Analysis",
            "mention_or_use": "mention",
            "model_name": "LLM zero-shot (unspecified in this paper's summary)",
            "model_description": "Prompt-engineering approach for zero-shot log analysis evaluated in prior work; paper reports low precision in zero-shot settings.",
            "model_size": null,
            "anomaly_detection_method": "Zero-shot prompt-based classification (various prompt formats tested per referenced work: self-prompt, chain-of-thought, in-context, etc.).",
            "data_type": "Unstructured log entries",
            "anomaly_type": "Anomalous log messages",
            "dataset_name": null,
            "performance_metrics": "Reported in this paper as Precision=0.25, Recall=0.83, F1=0.38 (used as a comparison for zero-shot classification).",
            "baseline_comparison": "RAGLog substantially improved precision and F1 compared to LogPrompt's reported zero-shot results.",
            "limitations_or_failure_cases": "Low precision in zero-shot settings (high false positive rate), per the referenced results.",
            "uuid": "e5763.5",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LogGPT / ChatGPT (prior work)",
            "name_full": "LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection",
            "brief_description": "Prior attempted application of ChatGPT to log anomaly detection referenced in the paper; authors report sensitivity to prompt formulation, window size limitations, and high false-positive rates in that work.",
            "citation_title": "LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (GPT-family conversational model)",
            "model_description": "Conversational LLM used in referenced experiments with varied prompt constructs and windowing; architecture/size not specified in this paper.",
            "model_size": null,
            "anomaly_detection_method": "Prompt-based experiments with varying prompt formats and window sizes for log input.",
            "data_type": "Unstructured log sequences",
            "anomaly_type": "Anomalous log entries",
            "dataset_name": null,
            "performance_metrics": "Described qualitatively as having high false positive rates and sensitivity to prompt/window settings; no numeric metrics reproduced here.",
            "baseline_comparison": "Referenced as showing limitations of direct ChatGPT application for anomaly detection compared with more structured approaches.",
            "limitations_or_failure_cases": "Non-trivial selection of optimal prompt and window size; high false positive rates reported in the referenced study.",
            "uuid": "e5763.6",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BGL",
            "name_full": "BGL (BlueGene/L) system logs dataset",
            "brief_description": "A real-world HPC log dataset from a BlueGene/L supercomputer that contains many new log messages over time and labeled anomalies with class imbalance; used to evaluate RAGLog.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Dataset of system log messages with time sequences and labeled anomaly indicators; noted for a wide distribution of log patterns and evolving messages over time.",
            "model_size": null,
            "anomaly_detection_method": "Used as evaluation data for zero-shot RAGLog; authors sampled normal entries to populate vector DB (either random or via k-means cluster sampling).",
            "data_type": "Time-sequenced unstructured textual logs",
            "anomaly_type": "Anomalous / alert log messages (binary labels present)",
            "dataset_name": "BGL",
            "performance_metrics": "RAGLog reported system-level metrics (Precision=0.91, Recall=0.88, F1=0.89) over held-out test samples; paper notes clustering helped performance on BGL versus random sampling but does not provide per-dataset metric split in the table.",
            "baseline_comparison": "RAGLog's clustering-based population outperformed random population on BGL; compared against LogPrompt zero-shot baseline overall.",
            "limitations_or_failure_cases": "Dataset exhibits wide pattern distribution and changing message vocabulary over time, making detection more challenging.",
            "uuid": "e5763.7",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Thunderbird",
            "name_full": "Thunderbird system logs dataset (Sandia National Labs)",
            "brief_description": "A labeled log dataset containing alert and non-alert messages from Sandia National Labs; used alongside BGL to evaluate RAGLog. Reported to have more concentrated pattern distribution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": "Dataset of system logs with binary anomaly labels; authors sampled normal entries to populate the vector DB (random vs clustered) and evaluated RAGLog performance.",
            "model_size": null,
            "anomaly_detection_method": "Used to evaluate RAGLog zero-shot classification with vector DB populated by normal samples (random or k-means cluster sampling).",
            "data_type": "Unstructured textual logs (sequences)",
            "anomaly_type": "Alert vs non-alert / anomalous log messages",
            "dataset_name": "Thunderbird",
            "performance_metrics": "RAGLog reported overall system-level metrics across datasets: Precision=0.91, Recall=0.88, F1=0.89; authors note Thunderbird performed well with both random and clustered sample population strategies (no per-dataset numeric split provided in the table).",
            "baseline_comparison": "Performed well under both population strategies; compared favorably to LogPrompt zero-shot baseline overall.",
            "limitations_or_failure_cases": "None specific beyond those stated for RAGLog; dataset has more concentrated pattern distribution which may make retrieval easier.",
            "uuid": "e5763.8",
            "source_info": {
                "paper_title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogPrompt: Prompt Engineering Towards ZeroShot and Interpretable Log Analysis",
            "rating": 2
        },
        {
            "paper_title": "LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection",
            "rating": 2
        },
        {
            "paper_title": "An Assessment of ChatGPT on Log Data",
            "rating": 2
        },
        {
            "paper_title": "Log-based Anomaly Detection without Log Parsing",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 1
        }
    ],
    "cost": 0.014854999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RAGLog: Log Anomaly Detection using Retrieval Augmented Generation</h1>
<p>Jonathan Pan, Swee Liang Wong, Yidi Yuan<br>Home Team Science and Technology Agency, Singapore<br>Jonathan_Pan@htx.gov.sg, Wong_Swee_Liang@htx.gov.sg, Yuan_Yidi@htx.gov.sg</p>
<h4>Abstract</h4>
<p>The ability to detect log anomalies from system logs is a vital activity needed to ensure cyber resiliency of systems. It is applied for fault identification or facilitate cyber investigation and digital forensics. However, as logs belonging to different systems and components differ significantly, the challenge to perform such analysis is humanly challenging from the volume, variety and velocity of logs. This is further complicated by the lack or unavailability of anomalous log entries to develop trained machine learning or artificial intelligence models for such purposes. In this research work, we explore the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs. We used a Question and Answer configuration pipeline. To the best of our knowledge, our experiment which we called RAGLog is a novel one and the experimental results show much promise.</p>
<p>Keywords- Log analysis; Retrieval Augmented Generation, Large Language Model</p>
<h2>I. INTRODUCTION</h2>
<p>The analysis of logs to detect anomalies is an important research topic with practical importance in the field of failure identification [1], [2] and security threat detection [3], [4]. Logs are generated by systems or applications that are codified and configured to report relevant information about the state of the applications or software while running. Here, the application may refer to any software running to perform specific task or tasks. It could be a mobile application, operating system running inside an Internet of Things (IoTs) device or a cloud compute node performing computational tasks. It could also be an environment of compute nodes working collectively on multiple tasks. Such logs and their log entries are generated based on its current configuration at the time of the log generation. These entries are also affected by the state of the application during its execution and its dependent factors that may originate from within the operating environment and executing platform of the application. It would be affected by external factors like users or external systems interacting with the application.</p>
<p>These internal and external factors affecting the log generation may change abruptly and progressively over time resulting in corresponding log entries being included into the log generation process. These factors may originate from planned changes like planned maintenance tasks. They may also originate from unplanned activities. Additionally, these changes may be induced by benign or malicious intent. For the latter,
with the intent to evade detection, even if the logs are not tampered, its entries will be elusive to classical detection techniques. These further complicates the composition of logs to be analyzed.</p>
<p>The objective of performing analysis on logs is done to facilitate the detection of anomalous activities so that immediate or corresponding remediation may be done to contain or remediate the issue recorded in the logs. This is part of the attempt to enhance system resiliency against system faults, degradation and intentionally induced cyber physical attacks. It is also used to facilitate the investigation or analysis of what may have induced the occurrence of such anomalous activities. The scope of this research work is on the detection of such anomalous activities from the logs. However, due to the characteristics of logs, namely being voluminous, varied, and contextual, regular log analysis is difficult, warranting the need for automation. While rule or signature-based automation solution helps, the contextual or semantic complexity of logs limits its efficacy [16].</p>
<p>There is many research work done to develop AI algorithms to detect anomalies from logs. However, log analysis using AI algorithms require extensive preparation and data requirements to train the models [6][7]. The capability of log analysis using AI algorithms to detect anomalies has several challenges and constraints to deal with before it contributes significantly to its intended objectives of keeping system resilient. For supervised models, there is the challenge of acquiring sufficient anomalous data points to train such models. For unsupervised models, it will be the ability to detect the variety and variations of anomalies in logs. Recent research work to apply Large Language Models (LLMs) to log analysis processes have shown promising results but are constrained by model limits such as token capacity, ability to remember and hallucinations. Also, there is limited evaluation on its efficacy.</p>
<p>In our research work, we seek to address these constraints using a Retrieval Augmented Generation approach with a vector database and evaluate its performance in detecting log anomalies. Our novel log anomaly detection solution termed RAGLog uses a Retrieval Augmented Generation construct with a vector database to store subset of normal log entries and a Large Language Model to perform zero shot semantic analysis of the queried log entry. Our pipeline process requires minimal</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>data preprocessing and does not require log parsing. It uses unsupervised clustering to enhance log anomaly detection.</p>
<p>In the next section, we will cover the challenges and complexity of performing log analysis. This is followed by a review of current log analysis algorithms including Large Language Models. A coverage of our RAG construct is described in the section that follows with the details of the experimental setup and its evaluation. This paper concludes with a summary of this work and potential future research direction.</p>
<h2>II. BACKGROUND INFORMATION</h2>
<p>In this section, we articulate the background information related to the need for the analysis of logs, its complexity, and challenges with current log analysis algorithms.</p>
<h2>A. Need for Analysis of Logs</h2>
<p>Logs are generated by software driven applications running on systems or devices to provide information to aid developers and system engineers with their analysis of system's state and condition. It is also used as a form of audit trail to log the occurrences of events in chronological manner. The analysis of logs is also done to facilitate investigation after the occurrence of an incident related to the system that generates the logs. This incident could be in the form of system defect and a malicious or unintended breach of the system. With investigation, the log could provide the means to reconstruct the occurrence of the incident. With such forms of analysis, an investigator or system engineer would attempt to identify the occurrence of anomalous events through the logs. However, to identify such anomalies, one would need to know how to spot such anomalies from voluminous entries posted into the log files.</p>
<h2>B. Challenges with Log Analysis</h2>
<p>The form for logs is typically unique to how the software has been developed or configured to post entries into these textual files. Also, each system or software component may adopt its own logging format and information lexicon representation that details the state of the run-time system when logs are posted. Such information within the logs is highly context specific to the environment which the system resides in [16]. For example, information like the IP addresses or hostnames or resource identities. Entries in the logs are dependent also on the configuration surrounding the involved system and their own respective environmental conditions. In addition to the contextual settings, the log entries are sequenced by its chronological occurrence of events or state. Hence such log entries have a time dimension.</p>
<p>Hence, the analysis of such log datasets requires contextual understanding of the system or component that generates such logs. Also, the analysis requires the means to classify or distinguish what is a normal log entry and what is not a normal log entry. For the latter, such information of recognizing an abnormal entry would be constrained to what may be conceivable based on the engineering design of the system involved or known instances of events that could cause an anomalous event like a cyber security breach attempt. However,
there will be instances where such information or knowledge is only acquired through the occurrence of the event that in turn induces the anomalous log entries. Hence the challenges with log analysis are the need for semantic comprehension [15][16] to perform good log analysis and the challenge of having limitedly available information about the form of anomalies that could occur.</p>
<h2>C. Challenges with Large Language Models</h2>
<p>Current Large Language Models (or Generative Artificial Intelligence) have inherent limitations that includes limits to the size of the tokens that they can handle which in turns limits how much contextual information LLMs can take in or recall as well as potential for hallucinations [17]. Solutions are being researched upon to address such limitations with information retrieval techniques that will be described in subsequent sections.</p>
<h2>III. Related Work</h2>
<p>In this section, we review the current log analysis algorithmic development and their strengths and limitations.</p>
<h2>A. Multi-staged Log Processing</h2>
<p>The current log analysis algorithmic designs typically involve multiple stages of log processing before analysis is applied. It typically starts with log parsing that converts raw logs into structured data features. These extracted features would undergo further transformation as they are typically represented as textual features and would be converted to numerical forms. Log partitioning typically follows that involves converting the contiguous $\log$ into associative partitions to improve anomaly classification. This may involve the use of time-based partitions, partitions organized by windows of similar or compatible operations or identifier-based divisions of log entries. Finally, the anomaly detection algorithm would then be applied after these pre-processing.</p>
<p>Thus far, there are very few developmental attempts to develop an integrated model that could ingest raw log data for immediate model training and inference. Based on our survey, one by Hashemi and Mntyl [5] and Le and Zhang [15] ingress logs without log parsers. Le and Zhang observed that log parsers could cause inaccurate log parsing due to misinterpretation of the semantic meaning of the log analysis and not handle Out-ofvocabulary (OOV) words well. Our approach removes the need for log parsing, allowing inferences on raw log data inputs.</p>
<h2>B. Algorithms to detect Anomalous Events from Logs</h2>
<p>Many of the log analysis algorithms focused on the key area of detecting anomalous events from logs. From the survey work done by He et al. [6] and Chen et al. [7], the algorithms are either supervised or unsupervised machine learning algorithms. These algorithms may be based on classical machine learning algorithms or deep learning algorithms. Supervised learning algorithms are constrained by the availability of anomalous data with labels within the training datasets. Additionally, even with the availability of anomalous data within the log datasets, the class imbalance could pose a significant challenge to the training</p>
<p>of the model. Also, these models may need to undergo retraining or be reconstructed to internalize the new knowledge. With unsupervised learning algorithms [12][13] for log analysis, their challenge is the efficiency of the algorithms to detect the variety and variations of anomalies captured in log entries as such anomalies may occur and vary significantly over a prolonged period of time. When new anomalies are discovered, these unsupervised models will require retraining. Our approach uses the vector database to store only small samples of normal log entries. The LLM will do anomaly detection without any samples of anomalous log entries. Hence it is a zero shot classifier.</p>
<h2>C. LLM for Log Analysis</h2>
<p>There were recent attempts to apply Large Language models to perform log analysis. Qi et al. [18] proposed a framework for log-based anomaly detection using ChatGPT using varied prompt constructs, window sizes and input sequences. Their work showed the non-triviality of an optimal prompt, window size limitations as well as high false positive rates. Mudgal et al. [19] designed specific prompts with ChatGPT for log parsing that had excellent performance. However, with other areas of log analysis like anomaly detection and log summarization, the LLM exhibited limitations that warrant further research. Liu et al. [20] tested their LogPrompt model in zero-shot scenarios with varying number of provided log samples and different prompt formats (self-prompt, CoT prompts and In-context Prompt). The zero-shot test results showed promise when compared with our log analysis algorithms and other Deep Learning architectures. However, it had very low precision scores which is not optimal if applied to log analysis for operations and maintenance activities to support resiliency.</p>
<p>These preliminary experimentations demonstrate the necessity for further research in applying LLM for log analysis, especially in detecting anomalies in logs, forming the basis of this research work.</p>
<h2>IV. MODEL</h2>
<p>Our construct uses the Retrieval Augmented Generative (RAG) model [22] to analyze log entries by querying its store of samples of normal log entries. In our work, the store is a vector database. The Large Language Model would need to perform semantic analysis between the retrieved log samples from the database and the queried log entry.</p>
<p>This creates an end-to-end model construct that is simple to use and adept for any log source, unlike many other similarly purposed algorithms mentioned in our previous section for log analysis which require the use of multi-stage log processing pipeline.</p>
<h2>A. Formulation for RAG</h2>
<p>The RAG operates in two stages. The first is the retrieval of contextually relevant information. The second involves using the retrieved information to generate the corresponding response. This can be formulated with $x$ as the provided input, which is the queried log entry, $z$ as the set of relevant log entries
from the vector database and $y$ as the generated output from the LLM $f$. This can be expressed in the form.</p>
<p>$$
y=f(x, z)
$$</p>
<p>Our model construct uses dense-vector retrieval approach [23] that encodes the log entries into vector embedding representations using a pre-trained Embedding model from OpenAI. The retrieval score is computed through inner products between the queried vector from the provided log entry against vectors stored in the vector database that contains provided samples of normal log entries. When the retrieval score matches the criteria like highest similarity score or minimal threshold score, the vector database will return the resultant vectors. The retriever that we used is from LangChain [24].</p>
<h2>B. LLM for Semantic Analysis</h2>
<p>With the retrieved vectors, the vector embedding will be decoded using the corresponding decoder by Embedding model. This turns the vector back to the original log entry representation. We frame the log anomaly detection as a Question and Answer [23], using a Question and Answer prompt template to include the best matched retrieved normal log entries to analyze whether a queried log entry is normal or abnormal. The prompt is fed to the Language Large Model.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. RAGLog Architecture</p>
<h2>V. Methodology and Analysis</h2>
<p>In our experiment setup, we designed our experiment to address our research question whether Retrieval Augmented Generation in LLM could perform log anomaly detection.</p>
<h2>A. Log Datasets</h2>
<p>For our log datasets, we used BGL [9] and Thunderbird [21]. These are two popular datasets typically used by researchers to evaluate their log models [7].</p>
<p>The BGL are open real-world datasets from HPC from a BlueGene/L supercomputer at Lawrence Livermore National Labs. This dataset has an important characteristic associated with their appearance of many new log messages in the timeline of the data, that is, the systems change over time. The Thunderbird open dataset of logs was collected by Sandia National Lab. It contains alert and non-alert messages. Both</p>
<p>datasets are labelled with sizeable imbalance for the anomaly class.</p>
<h2>B. Evaluation Metrics</h2>
<p>As the dataset used had binary classification labels, we used Precision to measure the accuracy of the model against type I error (true positive) and Recall to measure the accuracy of the models against type II error (true negative). Finally, we used F1 score to measure the harmonic mean of precision and recall.</p>
<p>$$
\begin{gathered}
\text { Precision }=\frac{T P}{T P+F P} \
\text { Recall }=\frac{T P}{T P+F N} \
F 1 \text { score }=2 \times \frac{\text { Precision } \times \text { Recall }}{\text { Precision }+ \text { Recall }}
\end{gathered}
$$</p>
<p>TP (True Positive) represents the number of correctly classified anomalies, TN (True Negative) represents normal log entries and FP (False Positive) is the number of incorrect anomaly classification. FN (False Negative) is the number of incorrect classifications of log entries as normal while the label or ground truth states overwise.</p>
<h2>C. Experimentation Preparation and Evaluation</h2>
<p>We populated the vector database using two approaches. The first approach was to populate the database with randomly selected samples from the log datasets that contain only normal log entries. The second approach was to populate the database with selected samples of the log database with normal log entries. For this selection, we first applied unsupervised k-means clustering to the dataset and populated the database from random sampling from the cluster classes. We used the elbow approach to select the number of cluster classes. For both approaches, to facilitate our evaluation, we kept the same number of records persisted in the vector databases.</p>
<p>While we applied the same evaluation techniques for both log datasets, we observed notable differences in the distribution of log patterns for both: namely, BGL has a wider distribution surface compared to Thunderbird. The following are the kmeans clustering visualizations of both datasets.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Chart 1. BGL Cluster Visualization Map
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Chart 2. Thunderbird Cluster Visualization Map
For our experiment, we configured the sample size of 10,000 log entries for each cluster class and cluster classes of 5 for BGL dataset. For random sampling, that would be $50,000 \log$ entries that are randomly selected. For Thunderbird, we used the same sample size of 10,000 from each of the 4 classes for the clustered approach and 40,000 for random selection.</p>
<p>After selecting log entries samples, they were then populated into the vector database. Using one predefined prompt template from a Question and Answer pipeline, we will assess the efficacy of our solution to detect log anomalies. In our prompt template, we explicitly directed GPT 3.5 (Davinci) with temperature of 0.1 to generate answers in the form of 'normal' or 'abnormal' to facilitate our evaluation. Due to cost constraints of using GPT 3.5, we randomly sampled from the $20 \%$ of both log datasets that has been set aside for testing.</p>
<h2>D. Results and Analysis</h2>
<p>From our experiment test results shown below in Chart 3, we observed that the clustering approach yielded better results for BGL log datasets as compared to random selection.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Chart 3. Evaluation performance comparison between clustered approach and random selection for both datasets.</p>
<p>The Thunderbird log dataset generally performed well with both randomized and clustering approach. This could be due to concentration of the log pattern distribution for this dataset.</p>
<p>We further evaluated our results with others who had applied zero shot classification using LLM [20]. Also the output from the LLM had either normal or abnormal returns with no other textual hallucination noted.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1 score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LogPrompt [20]</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.83</td>
<td style="text-align: left;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">RAGLog (Ours)</td>
<td style="text-align: left;">0.91</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">0.89</td>
</tr>
</tbody>
</table>
<p>Table 1. Evaluation Comparison for Zero-Shot Classification</p>
<h2>VI. CONCLUSION AND Future DireCTIONS</h2>
<p>Our research work explored the use of Retrieval Augmented Generation model as log anomaly detector (RAGLog). The model's vector database only contained samples of normal log entries that were selected using unsupervised k-means clustering. It achieved good F1 scores when analyzing log entries using zero shot approach for anomalies, with the LLM being given only normal log entries for semantic analysis.</p>
<p>The constraints posed by this approach is the high resource consumption and execution latency for running the LLM and performing log analysis one log entry at a time. Hence, our next step will be to further optimize our RAG model approach to analyze logs faster with larger volumes.</p>
<h2>REFERENCES</h2>
<p>[1] A. Pecchia, D. Cotroneo, Z. Kalbarczyk, and R.K. Iyer, "Improving logbased field failure data analysis of multi-node computing systems", DSN'11: Proc. of the 41st IEEE/IFIP International Conference on Dependable Systems and Networks, pages 97-108. IEEE, 2011.
[2] W. Xu, L. Huang, A. Fox, D. Patterson, and M.I. Jordon, "Detecting large-scale system problems by mining console logs", SOSP'09: Proc. of the ACM Symposium on Operating Systems Principles, 2009.
[3] A. Brandao and P. Georgieva, "Log Files Analysis For Network Intrusion Detection," 2020 IEEE 10th International Conference on Intelligent Systems (IS), 2020, pp. 328-333, doi: 10.1109/IS48319.2020.9199976.
[4] M. Moh, S. Pininti, S. Doddapaneni and T. Moh, "Detecting Web Attacks Using Multi-stage Log Analysis," 2016 IEEE 6th International Conference on Advanced Computing (IACC), 2016, pp. 733-738, doi: 10.1109/IACC.2016.141.
[5] S. Hashemi and M. Mntyl, "OneLog: Towards End-to-End Training in Software Log Anomaly Detection", arXiv, arXiv:2104.07324, https://doi.org/10.48550/arXiv.2104.07324.
[6] S. He, J. Zhu, P. He and M. R. Lyu, "Experience Report: System Log Analysis for Anomaly Detection," 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE), 2016, pp. 207218, doi: 10.1109/ISSRE.2016.21.
[7] Z. Chen, J. Liu, W. Gu, Y. Su, and M. R. Lyu, "Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection,", arXiv, arXiv:2107.05908, https://doi.org/10.48550/arXiv.2107.05908.
[8] J. Snell, K. Swersky, and R. S. Zemel, "Prototypical networks for fewshot learning", Neural Information Processing Systems, 2017.
[9] A. Oliner and J. Stearley, "What supercomputers say: A study of five system logs", 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07). IEEE, pp 575-584, 2007.
[10] V. H. Le and H. Zhang, "Log-based Anomaly Detection without Log Parsing", 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), Nov 2021.
[11] M. Du, F. Li, G. Zheng, and V. Srikumar, "Deeplog: Anomaly detection and diagnosis from system logs through deep learning", Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 1285-1298, 2017.
[12] A. Farzad and T. A. Gulliver, "Unsupervised log message anomaly detection", ICT Express 6, 3, 229-237, 2020.
[13] D. Biplob, M. Solaimani, M. A. G. Gulzar, N. Arora, C. Lumezanu, J. Xu, B. Zong, H. Zhang, G. Jiang and L. Khan, "LogLens: A real- time log analysis system." In 2018 IEEE 38th international conference on distributed computing systems (ICDCS), pp. 1052-1062. IEEE, 2018.
[14] J. P. Poh, J. Y. C. Lee, K. X. Tan, and E. Tan, "Physical access log analysis: An unsupervised clustering approach for anomaly detection." In Proceedings of the 3rd International Conference on Data Science and Information Technology, pp. 12-18. 2020.
[15] V. H. Le and H. Zhang, "Log-based Anomaly Detection without Log Parsing", 2021 36 $6^{\text {th }}$ IEEE/ACM International Conference on Automated Software Engineering (ASE), Nov 2021.
[16] A. Ekelhart, E. Kiesling and K. Kurniawan, "Taming the logs Vocabularies for semantic security analysis", SEMANTiCS 2028 - $14^{\text {th }}$ International Conference on Semantic Systems, Science Direct, Procedia Comput Science 137, pp. 109-119, 2018.
[17] R. Zhao, H. Chen, W. Wang, F. Jiao, X.L. Do, C Qin, B. Ding, X. Guo, M. Li, X. Li and S. Joty, "Retrieving Multimodal Information for Augmented Generaion: A Survey, arXiv:2303.10868v2, 2023.
[18] J. Qi, S. Huang, Z. Luan, C. Fung, H. Yang and D. Qian, "LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection", arXiv:2309.01189v1, 2023.
[19] P. Mudgal and R. Wouhaybi, "An Assessment of ChatGPT on Log Data", arXiv:2309.07938v1, 2023.
[20] Y. Liu, S. Tao, W. Meng, J. Wang, W. Ma, Y. Zhao, Y. Chen, H. Yang, Y. Jiang and X. Chen, "LogPrompt: Prompt Engineering Towards ZeroShot and Interpretable Log Analysis", arXiv:2308.07610v1, 2023.
[21] A. Oliner and J. Stearley, "What supercomputers say: A study of five system logs," in DSN, 2007.
[22] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kttler, M. Lewis,W. Yih, T. Rocktschel, et al., "Retrieval-augmented generation for knowledge-intensive nlp tasks", Advances in Neural Information Processing Systems, 33:9459-9474, 2020.
[23] K. Lee, M. W. Chang, and K. Toutanov, "Latent retrieval for weakly supervised open domain question answering", arXiv:1906.00300, 2019.
[24] H. Chase, LangChain, langchain.com.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>(C) 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
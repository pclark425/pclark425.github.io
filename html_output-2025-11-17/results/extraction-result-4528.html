<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4528 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4528</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4528</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-c7face35e84f2cb04fb1600d54298799aa0ed189</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c7face35e84f2cb04fb1600d54298799aa0ed189" target="_blank">ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps is introduced, and it is found that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</p>
                <p><strong>Paper Abstract:</strong> When conducting literature reviews, scientists often create literature review tables—tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing arxivDIGESTables, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against human-authored reference tables, we develop DecontextEval, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs’ abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4528.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4528.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schema-Generation (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Schema (Aspect) Generation for Literature Review Tables</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using large language models to infer a schema — a set of shared aspects/columns — from a set of scholarly papers (titles, abstracts, and optionally captions and in-text references). The LLM proposes human-interpretable comparison aspects (e.g., 'Task', 'Dataset size', 'Evaluation metric').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo; Mixtral 8x22 (Mistral); GPT-4-Turbo used for auxiliary generation in pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Decomposed Schema-Generation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Decompose table generation into a schema-generation step: LLMs are prompted (zero-shot or few-shot) with titles+abstracts of M input papers and optional additional context (generated caption, gold caption, gold caption + in-text references, or few-shot example tables). Prompts request exactly N aspects to match the reference table's column count. For captioned settings, the pipeline first (optionally) uses GPT-3.5/GPT-4-Turbo to generate or expand captions/column descriptions; for the decontextualized featurizer, a separate instruction to a smaller instruct model (Mistral-8x7B-Instruct-v0.1) is used to produce stand-alone descriptions of column names+values. The output aspects are returned as a JSON list and later fed to value-generation. Prompts include strict format constraints and fallback retries when outputs violate format or count constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varied per-table (M), dataset totals: 7,542 unique papers across 2,228 tables</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Primarily computer science (computer vision, datasets, methods), with some Physics/Quant Bio/Statistics/Math</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Qualitative comparison aspects and patterns (design principles, dataset/method attributes, evaluation metrics, implementation details) — i.e., descriptive/principled patterns used to compare papers rather than formal causal laws</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>"Task" vs "Intended Application" (same semantic aspect), "Evaluation metric", "Annotation method", "Maximum resolution", "Training batch size" (examples of aspects the LLM produced)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic schema alignment (DECONTExtEval) combining LLM-based decontextualization + sentence-transformer similarity scoring and human evaluation (Likert ratings for usefulness, specificity, insightfulness) and human judgments of matches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DECONTExtEval (sentence-transformer + decontext) precision ~70-85% (reported range); Llama 3 (70B) aligner precision 37-55% on predicted matches (low precision/high recall); schema recall increases with additional context (caption, in-text references, few-shot examples) — newer vs older tables difference small (1–3 percentage points). Human ratings: matched vs unmatched aspects had comparable usefulness (means ~3.7–4.1 on 5-pt scale); inter-annotator agreement Krippendorff's α = 0.56 for aspect ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to joint generation baseline (single-call LM), decomposed schema generation with additional context outperforms low-context baselines. For alignment scorers: Exact match had very low recall (conservative), Llama3 had very high recall but low precision, and the selected sentence-transformer + decontext featurizer was the best trade-off. Human evaluation compared matched (M) vs non-matched (NM) aspects and found NM aspects often equally useful.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) LLMs can propose meaningful, human-useful comparison aspects (schemas) from input papers; 2) providing additional context (table captions, in-text references, few-shot examples) improves schema recall (better reconstruction of human-authored schemas); 3) decomposing schema and value generation reduces hallucinations compared to end-to-end joint generation; 4) novel LLM-generated aspects that do not match gold schemas can still be judged as useful, specific, and sometimes more useful than baseline outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLMs produce aspects of differing granularity/topics than references (overly specific or overly general); hallucination and lexical mismatch complicate automatic alignment; some LLM aligners (e.g., Llama 3 scorer) hallucinate matches (low precision); format and context window limits require batching and fallback retries; dataset bias (ArXiv-heavy, English, mostly CS) limits generality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4528.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4528.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value-Generation (LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Value Extraction for Table Cells (Extractive QA-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLMs to extract or generate the cell values for each (paper, aspect) pair by prompting on full-texts (or title+abstract) using questions derived from column names/descriptions; includes retry policies and query rewriting to reduce empty outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo for value generation; GPT-4-Turbo used to generate column definitions/queries</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Decomposed Value-Generation with Query Rewriting & Retry Policy</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given a schema (set of aspects), the pipeline: (1) uses GPT-4-Turbo to generate concise definitions for each column when caption/context is available; (2) rewrites definitions into one-line extractive questions (or uses template queries when no context); (3) queries GPT-3.5-Turbo on the full text of each target paper to extract the answer for each (paper, aspect) cell; (4) applies style-consistency rewriting for table display; (5) employs a retry policy generating up to 4 rephrased queries if the model returns empty, reducing empty-value rate from ~30% -> ~7.5%. For no-context setting, a simple template query is used.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varied per-table; value extraction evaluated over many table cells from the 7,542 unique papers (dataset-wide)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Primarily computer science literature (dataset/method descriptions etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Extractive descriptive facts and qualitative summaries for each aspect (e.g., dataset sizes, tasks, annotation procedures, performance-related attributes) rather than formal theoretical laws</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Extracted cell values corresponding to aspects such as 'Number of images' (e.g., '10,000 images'), 'Dataset Type' ('image-text pairs'), 'Evaluation metric' (e.g., 'mean accuracy'), or boolean indicators (e.g., 'Controlled' = '✓') — examples reported in qualitative tables and error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated overlap scorers (exact match, Jaccard, sentence-transformer similarity) using the reference schema to avoid schema alignment; human evaluation over sampled gold vs generated value pairs labeled as complete/partial/none; inter-annotator agreement measured (Cohen's κ).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-assessed complete matches: Column Names only: 21.13% complete, 22.54% partial, 56.34% none; +Captions: 18.84% complete, 31.30% partial, 49.86% none; +In-text refs: 22.65% complete, 31.77% partial, 45.59% none. Inter-annotator agreement Cohen's κ = 0.55 for value judgments. Automated scorers show sentence-transformer permissive behavior; exact match overly strict.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared settings with different context: Column names only vs add caption vs add in-text references. Additional context improved partial matches but did not drastically increase complete matches according to human judgement; automated metrics sometimes failed to detect improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) LLMs can extract concise values from full texts but complete exact matches to human-authored table values are relatively low (~20%); 2) adding caption and in-text reference context increases partial matches, suggesting improved partial correctness or recall of relevant facets; 3) retry/query-rewriting substantially reduces empty outputs; 4) evaluation is sensitive to scorer permissiveness and schema alignment errors.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Many correct value matches lack lexical overlap making automated evaluation difficult; frequent partial/incomplete outputs; dependency on available full text; value extraction sometimes requires disambiguating column semantics (complex aspects) or cross-cell inference; domain generalization limited by dataset composition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4528.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4528.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECONTEXTEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DECONTEXTEVAL: LLM-augmented Automatic Evaluation for Literature Review Tables</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation framework that uses LLM-generated decontextualized descriptions of column headers plus a sentence-transformer similarity scorer to align generated and reference table aspects robustly to lexical variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-8x7B-Instruct-v0.1 (for decontextualization); Llama 3 (70B) evaluated as scorer but found to hallucinate matches; sentence-transformers (all-MiniLM-L6-v2) used as final scorer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (Mistral-8x7B-Instruct-v0.1) for decontext; 70B for Llama 3 where noted</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DECONTEXTEVAL (decontext featurizer + sentence-transformer scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct f(a_i^m, a_j^r) as g(φ(a_i^m), φ(a_j^r)) where φ is a featurizer that prompts an LLM to expand a column name plus its values into a stand-alone description (decontextualization), and g is a similarity scorer (sentence-transformer cosine similarity). Threshold t set (0.7) to decide matches. Evaluated multiple configurations: name-only, name+values, decontext; scorers included Exact Match, Jaccard, sentence-transformer, and Llama 3 (prompted to output aligned columns). DECONTEXTEVAL selected as the best trade-off with precision ~70–85% and acceptable yield.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>evaluated across a validation subset (25% of ARXIVDIGESTABLES) and additional manual evaluation on ~50 tables; overall dataset 2,228 tables linking to 7,542 papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Scientific literature tables (arXiv papers; mostly computer science)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Not laws per se — used to identify alignment of qualitative aspects/ patterns between generated and reference schemas (semantic equivalence of aspect labels and their decontextualized descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Not direct laws; concrete outputs are decontextualized descriptions like 'question: What evaluation metric did this dataset use?' derived from column headers and values to enable robust matching between 'Task' and 'Intended Application' or between 'Annotations' and 'Annotation method'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Calibrated against human judgments on ~50 tables to estimate precision of predicted matches; then used to report schema recall across experiments. Human evaluation also used for downstream validation of aspect usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DECONTEXTEVAL precision ~70–85%; Llama 3 aligner precision 37–55% (but high recall). Threshold chosen t=0.7 for sentence-transformer cosine similarity. Exact-match baseline had very low recall.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Exact Match (very low recall), Jaccard (lexical), Llama 3 (high recall but low precision) and various featurizers including name-only and name+values. The decontext + sentence-transformer combination had the best precision/recall tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decontextualizing column headers using an LLM greatly improves interpretability and alignment across different surface forms; combining that with a sentence-transformer scorer yields a substantially better automatic metric (higher precision) than naive lexical scorers or prompting a large LLM to align columns directly.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLMs used for decontextualization can introduce errors; Llama 3 as a direct aligner hallucinated matches leading to low precision; alignment depends on quality of underlying decontextualization and thresholds; automated metric still imperfect and human evaluation remains necessary for assessing utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model <em>(Rating: 2)</em></li>
                <li>Automatic generation of review matrices as multi-document summarization of scientific papers <em>(Rating: 2)</em></li>
                <li>On-the-fly table generation <em>(Rating: 1)</em></li>
                <li>A question answering framework for decontextualizing user-facing snippets from scientific documents <em>(Rating: 2)</em></li>
                <li>MultiXScience: A large-scale dataset for extreme multidocument summarization of scientific articles <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4528",
    "paper_id": "paper-c7face35e84f2cb04fb1600d54298799aa0ed189",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "Schema-Generation (LLM)",
            "name_full": "LLM-based Schema (Aspect) Generation for Literature Review Tables",
            "brief_description": "Using large language models to infer a schema — a set of shared aspects/columns — from a set of scholarly papers (titles, abstracts, and optionally captions and in-text references). The LLM proposes human-interpretable comparison aspects (e.g., 'Task', 'Dataset size', 'Evaluation metric').",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo; Mixtral 8x22 (Mistral); GPT-4-Turbo used for auxiliary generation in pipeline",
            "model_size": null,
            "method_name": "Decomposed Schema-Generation",
            "method_description": "Decompose table generation into a schema-generation step: LLMs are prompted (zero-shot or few-shot) with titles+abstracts of M input papers and optional additional context (generated caption, gold caption, gold caption + in-text references, or few-shot example tables). Prompts request exactly N aspects to match the reference table's column count. For captioned settings, the pipeline first (optionally) uses GPT-3.5/GPT-4-Turbo to generate or expand captions/column descriptions; for the decontextualized featurizer, a separate instruction to a smaller instruct model (Mistral-8x7B-Instruct-v0.1) is used to produce stand-alone descriptions of column names+values. The output aspects are returned as a JSON list and later fed to value-generation. Prompts include strict format constraints and fallback retries when outputs violate format or count constraints.",
            "number_of_papers": "varied per-table (M), dataset totals: 7,542 unique papers across 2,228 tables",
            "domain_or_field": "Primarily computer science (computer vision, datasets, methods), with some Physics/Quant Bio/Statistics/Math",
            "type_of_laws_extracted": "Qualitative comparison aspects and patterns (design principles, dataset/method attributes, evaluation metrics, implementation details) — i.e., descriptive/principled patterns used to compare papers rather than formal causal laws",
            "example_laws_extracted": "\"Task\" vs \"Intended Application\" (same semantic aspect), \"Evaluation metric\", \"Annotation method\", \"Maximum resolution\", \"Training batch size\" (examples of aspects the LLM produced)",
            "evaluation_method": "Automatic schema alignment (DECONTExtEval) combining LLM-based decontextualization + sentence-transformer similarity scoring and human evaluation (Likert ratings for usefulness, specificity, insightfulness) and human judgments of matches",
            "performance_metrics": "DECONTExtEval (sentence-transformer + decontext) precision ~70-85% (reported range); Llama 3 (70B) aligner precision 37-55% on predicted matches (low precision/high recall); schema recall increases with additional context (caption, in-text references, few-shot examples) — newer vs older tables difference small (1–3 percentage points). Human ratings: matched vs unmatched aspects had comparable usefulness (means ~3.7–4.1 on 5-pt scale); inter-annotator agreement Krippendorff's α = 0.56 for aspect ratings.",
            "comparison_baseline": "Compared to joint generation baseline (single-call LM), decomposed schema generation with additional context outperforms low-context baselines. For alignment scorers: Exact match had very low recall (conservative), Llama3 had very high recall but low precision, and the selected sentence-transformer + decontext featurizer was the best trade-off. Human evaluation compared matched (M) vs non-matched (NM) aspects and found NM aspects often equally useful.",
            "key_findings": "1) LLMs can propose meaningful, human-useful comparison aspects (schemas) from input papers; 2) providing additional context (table captions, in-text references, few-shot examples) improves schema recall (better reconstruction of human-authored schemas); 3) decomposing schema and value generation reduces hallucinations compared to end-to-end joint generation; 4) novel LLM-generated aspects that do not match gold schemas can still be judged as useful, specific, and sometimes more useful than baseline outputs.",
            "challenges_limitations": "LLMs produce aspects of differing granularity/topics than references (overly specific or overly general); hallucination and lexical mismatch complicate automatic alignment; some LLM aligners (e.g., Llama 3 scorer) hallucinate matches (low precision); format and context window limits require batching and fallback retries; dataset bias (ArXiv-heavy, English, mostly CS) limits generality.",
            "uuid": "e4528.0",
            "source_info": {
                "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Value-Generation (LLM)",
            "name_full": "LLM-based Value Extraction for Table Cells (Extractive QA-style)",
            "brief_description": "Using LLMs to extract or generate the cell values for each (paper, aspect) pair by prompting on full-texts (or title+abstract) using questions derived from column names/descriptions; includes retry policies and query rewriting to reduce empty outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo for value generation; GPT-4-Turbo used to generate column definitions/queries",
            "model_size": null,
            "method_name": "Decomposed Value-Generation with Query Rewriting & Retry Policy",
            "method_description": "Given a schema (set of aspects), the pipeline: (1) uses GPT-4-Turbo to generate concise definitions for each column when caption/context is available; (2) rewrites definitions into one-line extractive questions (or uses template queries when no context); (3) queries GPT-3.5-Turbo on the full text of each target paper to extract the answer for each (paper, aspect) cell; (4) applies style-consistency rewriting for table display; (5) employs a retry policy generating up to 4 rephrased queries if the model returns empty, reducing empty-value rate from ~30% -&gt; ~7.5%. For no-context setting, a simple template query is used.",
            "number_of_papers": "varied per-table; value extraction evaluated over many table cells from the 7,542 unique papers (dataset-wide)",
            "domain_or_field": "Primarily computer science literature (dataset/method descriptions etc.)",
            "type_of_laws_extracted": "Extractive descriptive facts and qualitative summaries for each aspect (e.g., dataset sizes, tasks, annotation procedures, performance-related attributes) rather than formal theoretical laws",
            "example_laws_extracted": "Extracted cell values corresponding to aspects such as 'Number of images' (e.g., '10,000 images'), 'Dataset Type' ('image-text pairs'), 'Evaluation metric' (e.g., 'mean accuracy'), or boolean indicators (e.g., 'Controlled' = '✓') — examples reported in qualitative tables and error analysis.",
            "evaluation_method": "Automated overlap scorers (exact match, Jaccard, sentence-transformer similarity) using the reference schema to avoid schema alignment; human evaluation over sampled gold vs generated value pairs labeled as complete/partial/none; inter-annotator agreement measured (Cohen's κ).",
            "performance_metrics": "Human-assessed complete matches: Column Names only: 21.13% complete, 22.54% partial, 56.34% none; +Captions: 18.84% complete, 31.30% partial, 49.86% none; +In-text refs: 22.65% complete, 31.77% partial, 45.59% none. Inter-annotator agreement Cohen's κ = 0.55 for value judgments. Automated scorers show sentence-transformer permissive behavior; exact match overly strict.",
            "comparison_baseline": "Compared settings with different context: Column names only vs add caption vs add in-text references. Additional context improved partial matches but did not drastically increase complete matches according to human judgement; automated metrics sometimes failed to detect improvements.",
            "key_findings": "1) LLMs can extract concise values from full texts but complete exact matches to human-authored table values are relatively low (~20%); 2) adding caption and in-text reference context increases partial matches, suggesting improved partial correctness or recall of relevant facets; 3) retry/query-rewriting substantially reduces empty outputs; 4) evaluation is sensitive to scorer permissiveness and schema alignment errors.",
            "challenges_limitations": "Many correct value matches lack lexical overlap making automated evaluation difficult; frequent partial/incomplete outputs; dependency on available full text; value extraction sometimes requires disambiguating column semantics (complex aspects) or cross-cell inference; domain generalization limited by dataset composition.",
            "uuid": "e4528.1",
            "source_info": {
                "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DECONTEXTEVAL",
            "name_full": "DECONTEXTEVAL: LLM-augmented Automatic Evaluation for Literature Review Tables",
            "brief_description": "An automatic evaluation framework that uses LLM-generated decontextualized descriptions of column headers plus a sentence-transformer similarity scorer to align generated and reference table aspects robustly to lexical variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-8x7B-Instruct-v0.1 (for decontextualization); Llama 3 (70B) evaluated as scorer but found to hallucinate matches; sentence-transformers (all-MiniLM-L6-v2) used as final scorer",
            "model_size": "7B (Mistral-8x7B-Instruct-v0.1) for decontext; 70B for Llama 3 where noted",
            "method_name": "DECONTEXTEVAL (decontext featurizer + sentence-transformer scorer)",
            "method_description": "Construct f(a_i^m, a_j^r) as g(φ(a_i^m), φ(a_j^r)) where φ is a featurizer that prompts an LLM to expand a column name plus its values into a stand-alone description (decontextualization), and g is a similarity scorer (sentence-transformer cosine similarity). Threshold t set (0.7) to decide matches. Evaluated multiple configurations: name-only, name+values, decontext; scorers included Exact Match, Jaccard, sentence-transformer, and Llama 3 (prompted to output aligned columns). DECONTEXTEVAL selected as the best trade-off with precision ~70–85% and acceptable yield.",
            "number_of_papers": "evaluated across a validation subset (25% of ARXIVDIGESTABLES) and additional manual evaluation on ~50 tables; overall dataset 2,228 tables linking to 7,542 papers",
            "domain_or_field": "Scientific literature tables (arXiv papers; mostly computer science)",
            "type_of_laws_extracted": "Not laws per se — used to identify alignment of qualitative aspects/ patterns between generated and reference schemas (semantic equivalence of aspect labels and their decontextualized descriptions)",
            "example_laws_extracted": "Not direct laws; concrete outputs are decontextualized descriptions like 'question: What evaluation metric did this dataset use?' derived from column headers and values to enable robust matching between 'Task' and 'Intended Application' or between 'Annotations' and 'Annotation method'.",
            "evaluation_method": "Calibrated against human judgments on ~50 tables to estimate precision of predicted matches; then used to report schema recall across experiments. Human evaluation also used for downstream validation of aspect usefulness.",
            "performance_metrics": "DECONTEXTEVAL precision ~70–85%; Llama 3 aligner precision 37–55% (but high recall). Threshold chosen t=0.7 for sentence-transformer cosine similarity. Exact-match baseline had very low recall.",
            "comparison_baseline": "Compared to Exact Match (very low recall), Jaccard (lexical), Llama 3 (high recall but low precision) and various featurizers including name-only and name+values. The decontext + sentence-transformer combination had the best precision/recall tradeoff.",
            "key_findings": "Decontextualizing column headers using an LLM greatly improves interpretability and alignment across different surface forms; combining that with a sentence-transformer scorer yields a substantially better automatic metric (higher precision) than naive lexical scorers or prompting a large LLM to align columns directly.",
            "challenges_limitations": "LLMs used for decontextualization can introduce errors; Llama 3 as a direct aligner hallucinated matches leading to low precision; alignment depends on quality of underlying decontextualization and thresholds; automated metric still imperfect and human evaluation remains necessary for assessing utility.",
            "uuid": "e4528.2",
            "source_info": {
                "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model",
            "rating": 2
        },
        {
            "paper_title": "Automatic generation of review matrices as multi-document summarization of scientific papers",
            "rating": 2
        },
        {
            "paper_title": "On-the-fly table generation",
            "rating": 1
        },
        {
            "paper_title": "A question answering framework for decontextualizing user-facing snippets from scientific documents",
            "rating": 2
        },
        {
            "paper_title": "MultiXScience: A large-scale dataset for extreme multidocument summarization of scientific articles",
            "rating": 1
        }
    ],
    "cost": 0.014554499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ARXIVDIGESTABLES: Synthesizing Scientific Literature into Tables using Language Models</h1>
<p>Benjamin Newman ${ }^{\text {a }}$ Yoonjoo Lee ${ }^{\circ *}$<br>Aakanksha Naik ${ }^{\circ}$ Pao Siangliulue ${ }^{\circ}$ Raymond Fok ${ }^{\text {a }}$<br>Juho Kim ${ }^{\circ}$ Daniel S. Weld ${ }^{\text {a }}$ Joseph Chee Chang ${ }^{\circ}$ Kyle Lo ${ }^{\circ}$<br>${ }^{\text {a }}$ University of Washington ${ }^{\circ}$ KAIST ${ }^{\circ}$ Allen Institute for AI<br>blnewman@cs.washington.edu, yoonjoo.lee@kaist.ac.kr</p>
<h4>Abstract</h4>
<p>When conducting literature reviews, scientists often create literature review tables tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing $\square$ ARXIVDIGESTABLES, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against humanauthored reference tables, we develop DECONTEXTÉVAL, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs' abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.</p>
<p>○ blnewman/arxivDIGESTables
C bnewm0609/arxivDIGESTables</p>
<h2>1 Introduction</h2>
<p>Conducting literature reviews by reading and synthesizing information across a large set of documents is vital for scientists to stay abreast of their fields yet is increasingly laborious as the number of scientific publications grows exponentially (Jinha, 2010; Bornmann et al., 2021). At the core of this</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic of our literature review table generation task: (1) synthesize multiple input papers into a table with both (2) a schema (columns) and (3) values. Each row corresponds to an input paper.
sensemaking process is identifying a schema, a set of important aspects that are useful for comparing and contrasting prior literature (Russell et al., 1993). The results of this process are often presented in the form of literature review tables, whose rows are a set of papers and whose columns are a set of aspects that the papers share (Figure 1).</p>
<p>In this work, we conceptualize the task of literature review table generation by decomposing it into two sub-tasks: (1) Schema-generation: Determining a set of relevant shared aspects given a set of input papers, and (2) Value-generation: Determining the value given an aspect and a paper. For example, a table for a set of computer vision papers on video datasets (rows) might have a schema with aspects like "task" or "size" (columns); cell values under the "task" column may say "VQA" or "classification" (values).</p>
<p>Prior work has largely investigated each of the two sub-tasks independently. In particular, the large body of literature on document-grounded question-answering (Kwiatkowski et al., 2019; Dasigi et al., 2021; Lee et al., 2023), information extraction (Luan et al., 2018), and query (Zhong et al., 2021; Xu and Lapata, 2020) or aspect-based summarization (Yang et al., 2023; Ahuja et al.,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Side-by-side comparison of a reference literature review table from an ArXiv paper and a model-generated table given the same input papers. The generated table has reconstructed two gold aspects: the pink and blue aspects are the same, despite surface form differences (e.g., "Task" vs "Intended Application"). The generated table has also proposed two novel aspects that are still relevant and useful, like "evaluation metric" (green) or "Annotation method" (yellow) not to be confused with reference table's "Annotations".</p>
<p>2022) advances methods that are also suitable for generating values conditioned on an aspect. In our example above, values for aspect "size" can be answers to questions like "How many videos are in this dataset?".</p>
<p>In contrast, schema generation from a set of documents remains relatively under-explored, even though it is a crucial and effortful part of the manual literature review process. Prior work like Zhang and Balog (2018) infers new schemas from pre-existing ones, while recent work like Wang et al. (2024) assumes users can clearly articulate a schema in a short natural language query to infer aspects directly. This paper studies the use of language models for literature review table generation with a focus on unifying these two sub-tasks. This presents us with two research challenges:</p>
<p>First, we note a lack of large-scale, high-quality datasets of literature review tables to serve as a benchmark for this task. Second, similar to challenges faced in summarization and other grounded generation tasks, semantically similar content can be expressed with different surface forms, which makes automatic evaluation difficult even with a high-quality dataset. An example of these surface form differences is in Figure 2. To address these challenges:</p>
<ul>
<li>
<p>In §2, we curate and release ARXIVDIGESTABLES,<sup>1</sup> a dataset of 2,228 high-quality literature review tables scraped and filtered from 16 years of ArXiv papers uploaded between April 2007 and November 2023. These tables compare and contrast a total of 7,542 unique papers using a total of 7,634 columns and 43,905 values. This is the result of extensive filtering on an initial set of around 2.5 million extracted tables to ensure high quality, based on a strict set of desiderata. Finally, we link every table to rich paper content: (1) every input paper (row) has corresponding full text document, and (2) every table has its caption and in-line textual references extracted from the table's source paper for contextual information.</p>
</li>
<li>
<p>In §5, we present DECONTEXTEVAL, an au-</p>
</li>
</ul>
<p><sup>1</sup>DIGESTables stands for Document Information Gathering and Extraction for Scientific Tables</p>
<p>tomatic evaluation framework for comparing model-generated and human-authored tables. Our approach overcomes the difficulty in matching semantically-similar but lexicallydifferent column names by using a language model to expand column names into descriptions grounded in documents. Combining with a small textual similarity model results in a matcher that is nearly twice more precise than prompting Llama 3 (70B), which often hallucinates matches.</p>
<p>We formalize the literature review table generation task (§3) and introduce our framework for literature review table generation and detail our implementations using open and closed models (§4).</p>
<p>Finally in $\S 6$, we evaluate LMs on this generation task, addressing two key questions: (1) what contextual information is needed to steer language models to reconstruct human-authored schemas? and (2) are generated aspects that don't match gold still useful? For (1), we find that language models have higher recall by conditioning on more context that specifies the purpose of the table (e.g., captions, in-line references, other example tables). For (2), we find that novel aspects not in the reference tables can still be of comparable usefulness, specificity, and insightfulness.</p>
<h2>2 Creating $\square$ ARXIVDIGESTABLES</h2>
<p>Desiderata To enable research in synthesizing literature review tables, we first collect and curate a set of reference tables to ground our task and enable evaluation. To ensure this data is realistic, high-quality, and focused on supporting literature review, we decide on the following desiderata for including tables in our $\square$ ARXIVDIGESTABLES dataset:</p>
<ol>
<li>Tables should be ecologically valid-reflecting real syntheses authored by researchers rather than artificial annotation;</li>
<li>Tables should be focused on summarizing multiple aspects of a set of papers as opposed to tables for reporting empirical results;</li>
<li>Tables should follow a common structure where each row represents a single document and each column represents a specific aspect.
Based on these goals, we used the procedure below to construct $\square$ ARXIVDIGESTABLES:</li>
</ol>
<p>Data Source To ensure our task and benchmark are grounded in realistic cases, we collected a
dataset real-world literature review tables from open access ArXiv papers from April 2007 until November 2023. We subsequently filter these tables down to a high-quality set of 2,228 tables that meet our desiderata, as seen in Figure 3.</p>
<p>Extracting Tables The first step in our data collection pipeline is to extract the tables from papers published on the ArXiv preprint server. To start, we consider approximately 800,000 papers that have LaTeX source available. We then use unarXive (Saier et al., 2023) to convert the ArXiv source into XML. From these XML documents, we extract $\sim 2.5$ million tables.</p>
<p>Filtering Tables As a first filtering pass, we remove tables that are likely to be misparsed or unusable, filtering those with fewer than 400 or more than 15,000 characters. We also remove tables that have no table cell tags within them. Toward Desiderata 3, we filter out tables that have fewer than two citations, two rows, or two columns. We also remove any tables that have citations in more than one column, as these are often tables where papers are values rather than rows. This leaves approximately 211,000 tables.</p>
<p>Matching Rows to Papers We use heuristics to convert XML-formatted tables into JSON objects that allow us to directly index the tables by paper and aspect (See §A. 1 for details). At this stage, the citation information is usually contained within a cell in a table. For instance, an example cell with the header "Model" might have the value "BERT (Devlin et al., 2019)". We extract the citations from these cells and place them in their own column called "References". Rows without citations are assumed to refer to the source paper containing the table. After this step in the process we remove any tables where the algorithm failed and any tables that now have fewer than two rows, leaving 47,876 tables.</p>
<p>Obtaining Table Citation Metadata unarXive (Saier et al., 2023) helpfully links each citation in the table to a bibliography item. We use endpoints from the Semantic Scholar API (Kinney et al., 2023) to obtain titles and abstracts. This occasionally fails for various reasons (e.g., the bibliography text is missing information, the paper is missing from or could not be found in the Semantic Scholar database). We filter out any tables that have fewer than two matched citations, leaving us with 44,617 tables.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Pipeline for curating $\square$ ARXIVDIGESTABLES involves extensive data cleaning and filtering. The full pipeline filters from 2.5 million starting tables published in 800,000 papers to 2,228 tables published in 1,723 papers. Data pipeline described in §2.</p>
<p>Grounding to Paper Texts To meet Desiderata 2, we want to ensure that the information in the table actually comes from the cited paper. For instance, a common type of table reports experimental results whose values require actual experimentation and cannot be derived from the input papers' text alone. To filter such columns, we remove any that have math symbols or floating point numbers. Additionally, to make sure the generation task is tractable, we remove any rows whose papers do not have publicly-available full texts.</p>
<p>Final Filter and Manual Verification The last step applies a set of stringent filters and manually identifies and corrects any parsing errors (Details in §A.2). Finally, we produce a set of 2,228 highquality tables. (See Appendix §A. 5 for a sample instance.)</p>
<p>Dataset Statistics We present summary statistics in Table 1 of our high-quality set of $\square$ ARXIVDIGESTABLES. ${ }^{2}$ We are also interested in the types of aspects represented in the tables, the topics of the columns, and the fields the tables come from. To categorize the table aspects, we use simple heuristics (Table 2). We find $\sim 40 \%$ of the columns are categorical or boolean, which are more suitable for supporting inter-paper comparisons, while the other $\sim 60 \%$ are more descriptive. To obtain column topics, we manually annotate columns in $\sim 50$ tables- $\sim 38 \%$ are about datasets, $\sim 20 \%$ are about methods, and the rest are on other</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>topics such as applications or tasks. Finally, we use the ArXiv API to obtain which archive a table's paper was submitted to. We find a majority $(1,985)$ of the tables come from computer science publications, with others coming from Physics, Quantitative Biology, Statistics, Math, and other fields (See Appendix A.4).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Min</th>
<th style="text-align: center;">Max</th>
<th style="text-align: center;">Median</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Papers</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">4.944</td>
<td style="text-align: center;">11016</td>
</tr>
<tr>
<td style="text-align: left;">Aspects</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">3.426</td>
<td style="text-align: center;">7634</td>
</tr>
</tbody>
</table>
<p>Table 1: Number of papers (rows) and aspects (columns) in $\square$ ARXIVDIGESTABLES. Of the 11,0016 total rows there are 7,542 unique papers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Aspect Type</th>
<th style="text-align: center;">\% of Cols</th>
<th style="text-align: center;">Example Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Category</td>
<td style="text-align: center;">$35.5 \%$</td>
<td style="text-align: center;">"Open" vs "Proprietary"</td>
</tr>
<tr>
<td style="text-align: left;">Entity</td>
<td style="text-align: center;">$27.3 \%$</td>
<td style="text-align: center;">"CNN/Daily Mail", "Reddit"</td>
</tr>
<tr>
<td style="text-align: left;">Numeric</td>
<td style="text-align: center;">$21.7 \%$</td>
<td style="text-align: center;">" 10,000 "</td>
</tr>
<tr>
<td style="text-align: left;">Text</td>
<td style="text-align: center;">$9.7 \%$</td>
<td style="text-align: center;">" $\ldots$ collected via various $\ldots$ "</td>
</tr>
<tr>
<td style="text-align: left;">Boolean</td>
<td style="text-align: center;">$5.8 \%$</td>
<td style="text-align: center;">" $\checkmark$ " vs " $\boldsymbol{\Omega}$ "</td>
</tr>
</tbody>
</table>
<p>Table 2: Types of aspects in $\square$ ARXIVDIGESTABLES's columns.</p>
<h2>3 Literature Review Table Generation</h2>
<p>Equipped with our dataset, we formalize the task of generating literature review tables.</p>
<p>Task Definition We define our table generation task as follows: Given an input set of $M$ documents $d_{1}, \ldots, d_{M}$, generate a table with $M$ rows and any number of columns $N \geq 2$. Each row $r_{1}, \ldots, r_{M}$ corresponds to a unique input document. Each column $c_{1}, \ldots, c_{N}$ represents a unique aspect. Taken together, the columns constitute a schema. The</p>
<p>table then has $N \times M$ values, with one value in each cell. ${ }^{3}$ The cell values should be derived from the input documents.</p>
<p>Generation We consider two main approaches to generate a table given a set of input documents. (1) The schema and values could be jointly generated, e.g. in a single call to a language model. This approach is fast, but initial experiments found it more prone to hallucinations and generic column names (e.g., "Title" or "Year"). (2) The generation process can be decomposed into separate schema and value generation steps. This approach is slower but allows us to overcome context window limits and leverage prior work in aspect-based question answering to perform value generation.</p>
<p>Evaluation We evaluate our approaches by determining whether the generated schemas are useful and values are correct. We consider a generated schema to be useful if its aspects either match those in the corresponding human-authored table in $\square$ ARXIVDIGESTAbles or if human evaluators rate them to be useful. ${ }^{4}$ These two conditions allow us to measure how well systems reconstruct reference table aspects ( $\S 5.1$ ) and evaluate their ability to generate novel aspects (§6.1). Second, we evaluate correctness of values as we would for any information extraction or QA task: for a pair of aligned columns (and rows), we judge whether the predicted cell value is semantically equivalent to the gold cell value (see §5.2).</p>
<h2>4 Experiments</h2>
<p>We prompt language models to perform either joint or decomposed generation.</p>
<h3>4.1 Base Models</h3>
<p>We use two language models, one open-weight, Mixtral 8x22 (Mistral AI, 2024), and one closed weight, GPT-3.5-Turbo (Open AI, 2022). To avoid gaming our recall metric, we instruct all models to generate schemas with the same number of aspects as the corresponding reference tables. (More prompting details in Appendix §B.5).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.2 Joint Table Generation</h3>
<p>We represent input papers using their titles and abstracts, which usually have enough information to form useful schemas and are easier to fit in the context window of models. We use a zero-shot table generation prompt (Appendix §B.1). We treat this condition as our baseline.</p>
<h3>4.3 Decomposed Table Generation</h3>
<p>Step 1: Schema generation Like in joint generation, we represent input papers using their titles and abstracts. We explore a range of prompts, each including a different piece of additional context (detailed in §4.3.1).</p>
<p>Step 2: Value generation Similar to extractive QA, for each aspect-paper pair, we prompt the model to generate a cell value based on the aspect name and the full text of the paper. After generating values for each paper given an aspect, we instruct a model to rewrite the values to be shorter and more consistent in style for display in table format. For this step, we use GPT 3.5-Turbo for speed and accuracy (Open AI, 2022) (prompt in Appendix §B.3).</p>
<h3>4.3.1 Additional Context</h3>
<p>To further investigate what contextual information is needed to steer language models to reconstruct human-authored tables, we test the following additional contexts, which could be added to either schema and/or value generation (see Appendix $\S B$ for prompts): (1) a generated caption where GPT-3.5-Turbo generates a short description that is consistent with all input papers; (2) the gold caption from the reference table; (3) the gold caption and in-text references, which include referencing sentences from the table's source paper; and (4) few-shot in-context examples, consisting of five reference table examples from $\square$ ARXIVDIGESTAbles retrieved based on cosine similarity between caption embeddings (Reimers and Gurevych, 2019).</p>
<h2>5 Developing an Automatic Metric</h2>
<p>Below we describe the design of our automatic evaluation procedure with two components: evaluating the schema and values for a generated table.</p>
<h3>5.1 Schema Evaluation</h3>
<p>Challenges The key challenge in assessing how well a generated table reconstructs a reference</p>
<p>table lies in determining schema alignmentsidentifying which columns convey the same information despite different phrasing. Two issues make schema alignment difficult. First, reference tables tend to present information concisely, making column headers and values hard to interpret without additional context (e.g., a column might be named "VQA" instead of "video quality assessment"). Second, information in generated and reference tables might have low lexical overlap despite semantic similarity, a problem also observed in summarization evaluation (Lin, 2004).</p>
<p>Problem Definition To formalize the schema alignment problem, recall that a table schema is a set of $N$ aspects. Given a model-generated table schema, $S^{\prime n}=\left{a_{1}^{m}, \ldots, a_{N}^{m}\right}$, a reference table schema $S^{r}=\left{a_{1}^{r}, \ldots, a_{N}^{r}\right}$, and a threshold $0 \leq t \leq 1$, our goal is to construct a scoring function $f$ to score each pair of aspects, $\left(a_{i}^{m}, a_{j}^{r}\right)$, such that $f\left(a_{i}^{m}, a_{j}^{r}\right)&gt;t$ if and only if human raters would agree that $a_{i}^{m}$ and $a_{j}^{r}$ convey the same information.</p>
<p>Alignment Framework We propose to define $f$ as the composition of two functions: a featurizer $(\phi)$, and a scorer $(g)$. The goal of the featurizer is to improve aspect interpretability by incorporating additional context, while the goal of the scorer is to account for meaning-preserving lexical diversity, leading to better schema alignments.</p>
<p>Configurations of $f$ We study three featurizers $\phi$ : (1) "name" only takes the column name as-is, (2) "values" concatenates all values under a column to the name, and (3) "decontext" prompts a language model ${ }^{5}$ to generate a stand-alone description (Choi et al., 2021; Newman et al., 2023), given the column name and its values.
We also study four scoring functions $g$ :</p>
<ul>
<li>Exact Match, which assigns a score of 1 if $\phi\left(a_{i}^{m}\right)=\phi\left(a_{j}^{r}\right)$ and 0 otherwise.</li>
<li>Jaccard, which computes Jaccard similarity of the featurized aspects, with stopwords removed.</li>
<li>Sentence Transformers, which encodes featurized aspects using all-MiniLM-L6-v2 and computes cosine similarity between them (Reimers and Gurevych, 2019).</li>
<li>Llama 3, which prompts Llama 3 (70B) Chat with generated and reference tables, with the column headers replaced by featurized versions, in-</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>structions to output aligned columns, and ten in-context examples. All pairs of columns returned by the LLM are assigned a score of 1 , and 0 otherwise. Refer to $\S$ B. 4 for prompting details.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Recall averaged over different contexts and systems. The band represents $95 \%$ confidence interval. Llama3 scorers have high recall, but low precision. Sentence Transformers (decontext) has the best trade-off.</p>
<p>Calibrating Schema Alignment We first run various combinations of $(\phi, g, t)$ and compute schema recall (i.e., proportion of reference table aspects matched to generated table aspects) on $25 \%$ of the tables in ARXIVDIGESTAbLes. In Figure 4, we observe a wide range of recall trade-offs: (1) Exact match has very low recall, as expected, serving as our conservative bound. (2) Llama 3 aligners tend to predict many more matches than other configurations despite that half of the in-context examples are tables with no matches. Llama 3 aligners serve as our upper bound. We perform human evaluation on $\sim 50$ tables and find that Llama 3 aligners have between $37-55 \%$ precision on their predicted matches. ${ }^{6}$ (3) Focusing our attention on the configurations that yield recall between these two bounds, we evaluate a range of configurations on the same tables and arrive at DecontextEval, our best configuration with $\phi$ using decontext features, $g$ using sentence transformers, and $t=0.7$; we find DECONTExtEval performs at $70-85 \%$ precision with acceptable yield.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>5.2 Value Evaluation</h3>
<p>Automated value evaluation suffers from the same issues that complicate schema evaluation, but one issue specific to value evaluation is reliance on accurate schema alignments. If aspects are incorrectly matched by a schema alignment metric, performance on value evaluation might rise/drop undeservedly. Therefore, we propose evaluating value generation in isolation, instead of an end-to-end table evaluation setting.</p>
<p>Specifically, we use the reference table's schemas as input to our value generation module. This ensures that every value in the reference table has a corresponding generated value (barring generation failures), bypassing the need for schema alignment. Following $\S 4.3 .1$, we consider three settings using different types of contexts: (1) "Column Names" only, (2) "Caption Context" which adds the table caption, and (3) "All Context" which further adds in-text references. Prompts used for each setting are in Appendix §B.3. We then use the same suite of scorers from $\S 5.1$ (except Llama 3, which we observed was low-precision) to compute overlap between pairs of generated and reference table's values.</p>
<h2>6 Results</h2>
<h3>6.1 Schema Evaluation Results</h3>
<p>Automated Evaluation Figure 5 shows the ability of GPT-3.5-Turbo and Mixtral 8x22 to reconstruct schemas (as measured via DeconteXtEVAL) using various types of additional contexts described in §4.3.1. Turning back to the question: How does the amount of context provided affect table reconstruction? (1) We see that low context prompts (e.g., a baseline with no additional context, caption-only) perform the worst while high context prompts (e.g., in-text references, in-context examples) perform best. This trend is fairly stable across systems. (2) Interestingly, though adding context improves reconstruction, it does not make the task trivial - even the best performing systems are far from perfect.</p>
<p>One potential concern for this analysis is that the models we use may have seen the older tables during training, which could inflate performance. To address this, we compute recall separately on subsets of newer and older tables (those from before or after January 2023 constituting 30\% and 70\% of our data respectively) for the high context prompts. We find that there is minimal difference between
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Schema recall for GPT-3.5-Turbo and Mixtral $8 \times 22$, using various types of additional contexts. All scores are computed using our best metric: sentence transformer-based scorer with decontext featurizer. More context improves recall, but does not lead to completely reproducing reference table schemas.
these two sets (the newer tables have recalls on average 1-3 percentage points lower).</p>
<p>Human Evaluation Our automated evaluation measures how well LMs can recover the reference tables' aspects, but leaves an additional question: Are LM-generated novel aspects which do not match with gold aspects also useful? To investigate this, we collect human assessments of generated aspects. Annotators are provided a generated table and the titles of all input papers. They are then prompted to provide a 5-point Likert scale rating for each of the following aspects: (1) general usefulness for understanding the input papers, (2) specificity to input papers (i.e., would this aspect be applicable to any other set of papers), and (3) insightfulness of the generated aspect (i.e., capturing novelty). ${ }^{7}$ We also instruct annotators to only judge based on the quality of the aspects only, ignoring the values which are evaluated separately. After collecting these ratings, we separated the rated aspects into two groups-ones that matched a gold aspect (M), and ones that did not (NM). The annotators were blind to the conditions when rating the aspects, and inter-annotator agreement was 0.56 (Krippendorff's $\alpha$ ).</p>
<p>Comparing ratings on matched and unmatched aspects, we did not find aspects that matched to be rated significantly higher than ones that did not (Table 3; Mann-Whitney U tests). This suggests that novel generated aspects are of comparable quality</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Caption+In-text Ref</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{M}$</td>
<td style="text-align: center;">NM</td>
<td style="text-align: center;">$\mathbf{M}$</td>
<td style="text-align: center;">NM</td>
</tr>
<tr>
<td style="text-align: left;">Useful</td>
<td style="text-align: center;">$3.70(1.74)$</td>
<td style="text-align: center;">$4.07(1.06)$</td>
<td style="text-align: center;">$3.92(0.69)$</td>
<td style="text-align: center;">$3.73(1.17)$</td>
</tr>
<tr>
<td style="text-align: left;">Specific</td>
<td style="text-align: center;">$2.88(1.26)$</td>
<td style="text-align: center;">$3.06(1.34)$</td>
<td style="text-align: center;">$2.85(1.31)$</td>
<td style="text-align: center;">$2.75(1.35)$</td>
</tr>
<tr>
<td style="text-align: left;">Insightful</td>
<td style="text-align: center;">$1.86(1.04)$</td>
<td style="text-align: center;">$1.93(1.21)$</td>
<td style="text-align: center;">$2.34(1.25)$</td>
<td style="text-align: center;">$2.27(1.19)$</td>
</tr>
<tr>
<td style="text-align: left;"># Samples</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">208</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">283</td>
</tr>
</tbody>
</table>
<p>Table 3: Mean (SD) ratings from human assessments of generated aspects that match the gold schema (M) with those that do not (NM).
(usefulness, specificity, insightfulness) to gold aspects or even have a higher quality (usefulness of aspects from Caption+In-text References). Moreover, aspects from Caption+In-text Reference are shown to be more useful and specific than the Baseline's, but were less insightful. This suggests an interesting tradeoff between our reconstruction objective, and possibly a different objective like creativity.</p>
<p>Error Analysis Finally, we report some qualitative observations of errors in the generated schemas we used for human evaluation. These point to future areas of improvement. Comparing outputs from the baseline to the Caption+In-text References condition, we find that the latter tends to output more specific aspects. For example, for one table, the Mixtral baseline produces aspects "Model Architecture" and "Application", while the Caption+In-text References Mixtral system generates the more specific aspects "Maximum resolution" and "Training batch size". We also note a few differences between schemas generated in the Caption+In-text references setting the reference tables' schemas, as well as categories of aspects that can pose difficulty for generation in Table 5 and additional examples in Appendix §D.</p>
<h3>6.2 Value Evaluation Results</h3>
<p>Automated Evaluation. Figure 6 shows the performance of GPT-3.5-Turbo on value generation, using various types of additional contexts (described in §5.2.) We see that scorers continue to follow the same trend observed during schema alignment, with the sentence transformer scorer being fairly permissive while an exact match is overly strict. Interestingly, unlike schema reconstruction, we observe that incorporating additional context does not seem to improve value generation accuracy; we dig deeper into this during human evaluation. Finally, like schema alignment, models are far from perfect in value generation.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Value generation accuracy for GPT-3.5-Turbo using various types of additional contexts, as computed by different scorers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: center;">Complete</th>
<th style="text-align: center;">Partial</th>
<th style="text-align: center;">None</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Col. Names</td>
<td style="text-align: center;">$21.13 \%(75)$</td>
<td style="text-align: center;">$22.54 \%(80)$</td>
<td style="text-align: center;">$56.34 \%(200)$</td>
</tr>
<tr>
<td style="text-align: left;">+ Captions</td>
<td style="text-align: center;">$18.84 \%(65)$</td>
<td style="text-align: center;">$31.30 \%(108)$</td>
<td style="text-align: center;">$49.86 \%(172)$</td>
</tr>
<tr>
<td style="text-align: left;">+ IT-Refs</td>
<td style="text-align: center;">$22.65 \%(77)$</td>
<td style="text-align: center;">$31.77 \%(108)$</td>
<td style="text-align: center;">$45.59 \%(155)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Proportion of matched gold-generated value pairs for various context settings, according to human assessment.</p>
<p>Human Evaluation. We conduct additional human evaluation to investigate whether adding context indeed has no impact on value accuracy, or our automated metrics are not sensitive enough to capture differences. We randomly sample 30 tables and compare gold vs generated values for these tables under all three settings. For each goldgenerated value pair, we have two annotators label whether it is a complete match, partial match or unmatched. Partial matches include cases where values are lists of items and the generated value misses or adds some (e.g., "DPO" vs "DPO, PPO"), or cases where the gold and generated values have a hypernymy relationship (e.g., "graph neural networks" vs "GATs"). Inter-annotator agreement is 0.55 (Cohen's $\kappa$ ). Table 4 presents results from this assessment, showing that adding additional context leads to a significant improvement in partial matches. However, many matches have no lexical overlap (e.g., "X" vs "No") or require some inference (e.g., "Yes" under a column called "sensors deployed" should match a value like "sensors used to monitor air quality"). This indicates that there is scope for further research in developing more sensitive featurizers and scorers for value evaluation.</p>
<h2>7 Related Work</h2>
<h3>7.1 Schema Generation for Literature Review</h3>
<p>Synthesizing schemas from research papers has been previously studied in contexts like identifying relations between papers (Shahaf et al., 2012; Lee</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Challenge Type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Different Granularity</td>
<td style="text-align: left;">The generated schema might be a high-level category (e.g. "data types"), while the reference <br> schema includes more specific aspects (e.g. "image", "text", "audio", etc.)</td>
</tr>
<tr>
<td style="text-align: left;">Different topics</td>
<td style="text-align: left;">The generated schema might have a different variety of topics than the reference schema <br> (e.g. {"model architecture", "dataset used", "performance metric"} versus just dataset <br> properties {"color", "context"})</td>
</tr>
<tr>
<td style="text-align: left;">Complex Aspects</td>
<td style="text-align: left;">Aspects combine information from multiple cells, which can mislead the value generator. <br> E.g. "dataset size" leads to some values pertaining to training data and others to test data.</td>
</tr>
<tr>
<td style="text-align: left;">Overly Specific</td>
<td style="text-align: left;">A predicted aspect might only apply to one paper</td>
</tr>
</tbody>
</table>
<p>Table 5: Qualitative observations of challenges with generated tables
et al., 2024), organizing research threads (Kang et al., 2023), discovering papers for ideation (Hope et al., 2022; Kang et al., 2022), or constructing intermediate scaffolds for better multi-document summarization (Shah et al., 2021). These works often assume fixed or sparse schemas, focus on a sub-component of schema generation, or do not evaluate intermediate tables. More closely related to our work, SciDaSynth is an interactive interface for creating "data tables" from a set of papers (Wang et al., 2024), which infers aspects from users' questions about the papers. However, identifying and articulating good comparison aspects can be nontrivial for users, motivating our aim of automatically inducing salient aspects. Hashimoto et al. (2017) explore automated aspect extraction for literature review tables and point out that more specific aspects are useful but hard to generate.</p>
<h3>7.2 Datasets for Scientific Table Generation</h3>
<p>Prior work has also released datasets of tables (Bai et al., 2023; Gupta et al., 2023). Bai et al. (2023) build a dataset of numeric result tables, while Gupta et al. (2023) release 4.4 k distantly supervised and 1.5 k manually annotated tables with material compositions from papers. Unlike ARXIVDIGESTAbles, these datasets do not necessarily link tables to input papers. Multidocument summarization datasets, like MultiXScience (Lu et al., 2020) and MS’2 (DeYoung et al., 2021), are related to table generation but yield sparse tables or use fixed schemas. Finally, there are datasets for other table-related tasks such as table extraction from PDFs (Gemelli et al., 2023), table retrieval (Gao and Callan, 2017), column annotation (Korini et al., 2022), table-to-text generation (Moosavi et al., 2021), table transformation (Chen et al., 2021), and table generation (Wu et al., 2022). However, these datasets either do not focus on scientific tasks or comparing papers.</p>
<h3>7.3 Automated evaluation using LMs</h3>
<p>As LMs have improved, they have also increasingly been used for automatic evaluation across NLP tasks, including summarization and QA that our work is similar to (Zhang et al., 2020; Wang et al., 2023; Lu et al., 2024; Zheng et al., 2024; Murahari et al., 2024). Some work on table generation has used a combination of automated and human evaluation. Hashimoto et al. (2017) use ROUGE (Lin, 2004) and human evaluation (Nenkova et al., 2007) to evaluate generated summaries of a table. Zhang and Balog (2018) evaluates schema selection via automatic entity ranking using ground truth entities. These works largely focus on measuring content overlap, whereas our automated metric incorporates table structure and context and our human evaluation focuses on downstream utility.</p>
<h2>8 Conclusion</h2>
<p>Language models have the potential to help scientists organize papers during literature review by synthesizing tables with schemas that aid comparison. In this work, we curate ARXIVDIGESTABLES, a dataset of such tables and additional contexts that can be used to evaluate systems' abilities to produce such tables. We present DecontextEval, an automatic evaluation framework for comparing model-generated and human-authored reference tables. We then use this evaluation framework to investigate two research questions: what context is needed to reconstruct human-authored tables, and whether generated aspects that don't align with references are also useful, specific and insightful. We release our artifacts to help spur development of literature review table generation systems, and seed potential for their role in evaluating systems' scientific synthesis abilities.</p>
<h2>Limitations</h2>
<p>We only study scientific papers from ArXiv. While in theory, scientists in many fields produce literature review tables, we restrict our reference tables to ones that we can scrape from ArXiv. This means many of the papers in our dataset come from fields that are most represented on ArXiv (e.g. computer science) and fewer come from medicine, humanities, or social science publications. Additionally, all of the tables in our high quality set are in English, even though literature review tables may also be used in other languages.</p>
<p>Reconstructing tables is difficult. While DeCONTEXTEval is effective at matching generated and reference table columns, and we test providing different additional context to steer the table generation models, many generated table columns do not match with the reference columns. Though we presented a human evaluation protocol that showed utility for generated columns that do not match the reference columns, such evaluation is costly. Future work should investigate automatic metrics that correlate with human utility evaluations as well.</p>
<h2>Ethical Considerations and Broader Impact</h2>
<p>Generated literature review tables might misrepresent authors' work. Generating literature review tables requires taking aspects of papers out of their original context to show them to users. Similar to summarization, this process has the potential to misrepresent the original work either due to the table cell values not having enough context, or less accurate models introducing hallucinations. Additional checks would have to be implemented if such tables were to be deployed in user-facing situations.</p>
<p>Literature review tables may discourage reading original sources. The resource we present is meant to encourage the development of methods to construct literature review tables. If the field iterates on this task and develops systems that perform very well, the tables may have all of the information that a given reader wants to see. This could discourage readers from finding the original source of the claims. That said, the rows in the tables in our benchmark do include citations, so readers could trace values back to their sources. However, readers are not guaranteed to follow these citations, so generated tables could encourage poor scholarly practices.</p>
<h2>References</h2>
<p>OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and Red Avila et al. 2023. Gpt-4 technical report.</p>
<p>Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin Horecka, and Greg Durrett. 2022. ASPECTNEWS: Aspect-oriented summarization of news documents. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6494-6506, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, and Alan Ritter. 2023. Schema-driven information extraction from heterogeneous tables. ArXiv, abs/2305.14336.</p>
<p>Lutz Bornmann, Robin Haunschild, and Rüdiger Mutz. 2021. Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Humanities and Social Sciences Communications, 8(1):1-15.</p>
<p>Mingda Chen, Sam Wiseman, and Kevin Gimpel. 2021. WikiTableT: A large-scale data-to-text dataset for generating Wikipedia article sections. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 193-209, Online. Association for Computational Linguistics.</p>
<p>Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sentences stand-alone. Transactions of the Association for Computational Linguistics, 9:447-461.</p>
<p>Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599-4610, Online. Association for Computational Linguistics.</p>
<p>Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. MS'2: Multidocument summarization of medical studies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 74947513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Kyle Yingkai Gao and Jamie Callan. 2017. Scientific table search using keyword queries. ArXiv, abs/1707.03423.</p>
<p>Andrea Gemelli, Emanuele Vivoli, and Simone Marinai. 2023. CTE: A dataset for contextualized table extraction. Preprint, arXiv:2302.01451.</p>
<p>Tanishq Gupta, Mohd Zaki, Devanshi Khatsuriya, Kausik Hira, N M Anoop Krishnan, and Mausam. 2023. DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13465-13483, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Hayato Hashimoto, Kazutoshi Shinoda, Hikaru Yokono, and Akiko Aizawa. 2017. Automatic generation of review matrices as multi-document summarization of scientific papers. In BIRNDL@ SIGIR.</p>
<p>Tom Hope, Ronen Tamari, Daniel Hershcovich, Hyeonsu B Kang, Joel Chan, Aniket Kittur, and Dafna Shahaf. 2022. Scaling creative inspiration with fine-grained functional aspects of ideas. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1-15.</p>
<p>Arif E Jinha. 2010. Article 50 million: an estimate of the number of scholarly articles in existence. Learned publishing, 23(3):258-263.</p>
<p>Hyeonsu B Kang, Xin Qian, Tom Hope, Dafna Shahaf, Joel Chan, and Aniket Kittur. 2022. Augmenting scientific creativity with an analogical search engine. ACM Transactions on Computer-Human Interaction, 29(6):1-36.</p>
<p>Hyeonsu B Kang, Tongshuang Wu, Joseph Chee Chang, and Aniket Kittur. 2023. Synergi: A mixed-initiative system for scholarly synthesis and sensemaking. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-19.</p>
<p>Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, et al. 2023. The semantic scholar open data platform. arXiv preprint arXiv:2301.10140.</p>
<p>Keti Korini, Ralph Peeters, and Christian Bizer. 2022. Sotab: the wdc schema. org table annotation benchmark. In CEUR Workshop Proceedings, volume 3320, pages 14-19. RWTH Aachen.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Yoonjoo Lee, Hyeonsu B Kang, Matt Latzke, Juho Kim, Jonathan Bragg, Joseph Chee Chang, and Pao Siangliulue. 2024. Paperweaver: Enriching topical paper alerts by contextualizing recommended papers with user-collected papers. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 1-19.</p>
<p>Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. 2023. QASA: advanced question answering on scientific articles. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yao Lu, Yue Dong, and Laurent Charlin. 2020. MultiXScience: A large-scale dataset for extreme multidocument summarization of scientific articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8068-8074, Online. Association for Computational Linguistics.</p>
<p>Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. 2024. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation. Advances in Neural Information Processing Systems, 36.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Mistral AI. 2024. Mixtral of experts. https: //mistral.ai/news/mixtral-of-experts/. Accessed on March 25, 2024.</p>
<p>Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych. 2021. SciGen: a dataset for reasoning-aware text generation from scientific tables. In NeurIPS Datasets and Benchmarks.</p>
<p>Vishvak Murahari, Ameet Deshpande, Peter Clark, Tanmay Rajpurohit, Ashish Sabharwal, Karthik Narasimhan, and Ashwin Kalyan. 2024. QualEval: Qualitative evaluation for model improvement. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 2093-2111, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing (TSLP), 4(2):4-es.</p>
<p>Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. 2023. A question answering framework for decontextualizing user-facing snippets from scientific documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3194-3212, Singapore. Association for Computational Linguistics.</p>
<p>Open AI. 2022. Introducing ChatGPT. https:// openai.com/index/chatgpt/. Accessed on March 25, 2024 .</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</p>
<p>Daniel M. Russell, Mark J. Stefik, Peter Pirolli, and Stuart K. Card. 1993. The cost structure of sensemaking. In Proceedings of the INTERACT '93 and CHI '93 Conference on Human Factors in Computing Systems, CHI '93, page 269-276, New York, NY, USA. Association for Computing Machinery.</p>
<p>Tarek Saier, Johan Krause, and Michael Färber. 2023. unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network. In 2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL), pages 66-70, Los Alamitos, CA, USA. IEEE Computer Society.</p>
<p>Darsh J. Shah, L. Yu, Tao Lei, and Regina Barzilay. 2021. Nutri-bullets hybrid: Consensual multidocument summarization. In North American Chapter of the Association for Computational Linguistics.</p>
<p>Dafna Shahaf, Carlos Guestrin, and Eric Horvitz. 2012. Metro maps of science. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1122-1130.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is ChatGPT a good NLG evaluator? a preliminary study. In Proceedings of the 4th New Frontiers in Summarization Workshop, pages 1-11, Singapore. Association for Computational Linguistics.</p>
<p>Xingbo Wang, Samantha L Huey, Rui Sheng, Saurabh Mehta, and Fei Wang. 2024. SciDaSynth: Interactive structured knowledge extraction and synthesis from scientific literature with large language model. arXiv preprint arXiv:2404.13765.</p>
<p>Xueqing Wu, Jiacheng Zhang, and Hang Li. 2022. Text-to-table: A new way of information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2518-2533, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yumo Xu and Mirella Lapata. 2020. Coarse-to-fine query focused multi-document summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3632-3645, Online. Association for Computational Linguistics.</p>
<p>Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Xiaoman Pan, Linda Petzold, and Dong Yu. 2023. OASum: Large-scale open domain aspectbased summarization. In Findings of the Association
for Computational Linguistics: ACL 2023, pages 4381-4401, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Shuo Zhang and Krisztian Balog. 2018. On-the-fly table generation. The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for querybased multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905-5921, Online. Association for Computational Linguistics.</p>
<h2>A Data Processing</h2>
<h2>A. 1 XML Parsing</h2>
<p>At this point in the pipeline, the tables we are considering are represented in XML format. Unfortunately, sometimes XML-formatted tables have column headers that span multiple rows, rows can have insufficient numbers of columns, cells may span multiple columns rows, etc. This makes it hard to enforce Desiderata 3. To address these difficulties, we design heuristics to parse the XML formatted tables into a JSON object that allows us to directly index the tables by paper and aspect. Our heuristics cannot be completed for all tablessometimes they fail completely, and other times they fail on particular rows. We also experimented using GPT-4 (Achiam et al., 2023) for these difficult cases, but still found errors due to insufficient layout information being maintained in the conversion from LaTeX source to XML.</p>
<h2>A. 2 High Quality Data Filters</h2>
<p>To achieve our set of tables, we apply a number of stringent filters. We remove any tables whose headers came from merging two rows or that have a</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Field</th>
<th style="text-align: center;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Computer Science</td>
<td style="text-align: center;">1985</td>
</tr>
<tr>
<td style="text-align: center;">Electrical Engineering and Systems Science</td>
<td style="text-align: center;">131</td>
</tr>
<tr>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Quantitative Biology</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: center;">Statistics</td>
<td style="text-align: center;">19</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">Quantitative Finance</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Economics</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 6: Fields of study represented in the high-quality dataset.
row without a citation to avoid misformatted tables. We also deduplicate the tables using an exact string match on all columns minus the references column, and deduplicate individual rows (which includes citations). To meet Desiderata 2, and avoid filtering out tables that have empirical results, we filter out any columns that have floating point numbers, formulas, or figures. After these steps, we remove any tables that have fewer than two citations, rows or columns, leaving us with our final set.</p>
<h2>A. 3 Medium Quality Data Filters</h2>
<p>In addition to our high-quality dataset that is likely to meet our desiderata, we also release a larger set of 22,283 tables with fewer filters. These tables are not manually checked, are filtered less stringently, and do not have linked full-texts. In particular:</p>
<ul>
<li>Papers in rows are required to have titles and abstracts, but not required to have full-texts. This potentially makes value generation difficult because all of the values have to come from the title and abstract.</li>
<li>Tables are not required to have in-text references. This potentially makes schema generation difficult, as any additional context has to come from the caption (if present).</li>
<li>Tables with at most one row with no citation are allowed, as opposed to all rows having citations.</li>
<li>Tables with multi-row or hierarchical headers are allowed. These can sometimes lead to misformatted tables.</li>
</ul>
<h2>A. 4 Field of Study</h2>
<p>A full break-down of the fields of study represented in the high-quality dataset is in Table 6.</p>
<h2>A. 5 Example Data Instance</h2>
<p>Below is an example instance from ARXIVDIGESTAbles. (Some of the keys
rephrased and values are elided for clarity)</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="nx">Table</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span><span class="w"> </span><span class="mi">53648</span><span class="nx">c28</span><span class="o">-</span><span class="nx">a2b2</span><span class="o">-</span><span class="mi">4</span><span class="nx">e41</span><span class="o">-...</span>
<span class="nx">Paper</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span><span class="w"> </span><span class="m m-Double">2305.14525</span><span class="nx">v1</span>
<span class="nx">Caption</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;A categorization of</span>
<span class="s">    scope regarding design</span>
<span class="s">    variations observed in</span>
<span class="s">    collected corpora. The three</span>
<span class="s">    columns are high-level design</span>
<span class="s">    variation types, low-level</span>
<span class="s">    details assumptions over</span>
<span class="s">    visual designs...&quot;</span>
<span class="nx">In</span><span class="o">-</span><span class="nx">Text</span><span class="w"> </span><span class="nx">References</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span><span class="nx">Section</span><span class="p">:</span><span class="w"> </span><span class="nx">Design</span><span class="w"> </span><span class="nx">Variations</span>
<span class="w">    </span><span class="nx">Text</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;In addition to chart</span>
<span class="s">        type, we have also</span>
<span class="s">        observed scope...in Table</span>
<span class="s">        {{table:&lt;table id&gt;}}...&quot;</span><span class="p">},</span>
<span class="w">    </span><span class="o">...</span>
<span class="p">],</span>
<span class="nx">Table</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">References</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;{{cite:9a81b16</span>
<span class="s">        }}&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;{{cite:d5b4bb4}}&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s">&quot;{{cite:342c0c4}}&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;{{</span>
<span class="s">        cite:6697498}}&quot;</span><span class="p">],</span>
<span class="w">    </span><span class="s">&quot;Design Variation Type&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;</span>
<span class="s">        composite arrangement&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;</span>
<span class="s">        mark and glyph&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;mark and</span>
<span class="s">        glyph&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;coordinate space</span>
<span class="s">        &quot;</span><span class="p">],</span>
<span class="w">    </span><span class="s">&quot;Assumption&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s">&quot;only multiple</span>
<span class="s">        -view charts&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;only</span>
<span class="s">        proportion-related charts</span>
<span class="s">        &quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;only timeline-related</span>
<span class="s">        infographics&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;in</span>
<span class="s">        Cartesian coordinate space</span>
<span class="s">        &quot;</span><span class="p">]</span>
<span class="p">},</span>
<span class="nx">Citation</span><span class="w"> </span><span class="nx">Info</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="nx">Cite</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span><span class="w"> </span><span class="mi">9</span><span class="nx">a81b16</span><span class="p">,</span>
<span class="nx">Title</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Composition and</span>
<span class="s">    Configuration Patterns in</span>
<span class="s">    Multiple-View Visualizations&quot;</span><span class="p">,</span>
<span class="nx">Abstract</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Multiple-view</span>
<span class="s">    visualization (MV) is a layout</span>
<span class="s">    design technique...&quot;</span><span class="p">,</span>
<span class="nx">Full</span><span class="w"> </span><span class="nx">Text</span><span class="p">:</span><span class="w"> </span><span class="err">&quot;</span><span class="mi">1</span><span class="w"> </span><span class="nx">Introduction</span><span class="w"> </span><span class="nx">We</span>
<span class="w">    </span><span class="nx">present</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="k">in</span><span class="o">-</span><span class="nx">depth</span><span class="w"> </span><span class="nx">study</span><span class="w"> </span><span class="nx">on</span>
<span class="w">    </span><span class="nx">how</span><span class="w"> </span><span class="nx">multiple</span><span class="w"> </span><span class="nx">views</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">used</span><span class="w"> </span><span class="k">in</span>
<span class="w">        </span><span class="nx">practice</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">integrate</span><span class="w"> </span><span class="nx">our</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">results</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">recommendation</span>
<span class="w">    </span><span class="nv">system</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">layout</span><span class="w"> </span><span class="nv">design</span>
<span class="w">    </span>...<span class="err">&quot;</span>
<span class="err">}, ...</span>
<span class="err">}</span>
</code></pre></div>

<h2>B Prompts</h2>
<h2>B. 1 Prompt for table generation (Baseline)</h2>
<p>System Prompt: You are an intelligent and precise assistant that can understand the contents of research papers. You are knowledgable on different fields and domains of science, in particular computer science. You are able to interpret research papers, create questions and answers, and compare multiple papers.</p>
<p>User Prompt: [System]</p>
<p>We would like you to build a table that has each paper as a row and, as each column, a dimension that compares between the papers. You will be given multiple papers labeled Paper 1, 2, and so on. You will be provided with the title and content of each paper. Please create a table that compares and contrasts the given papers. Make { col_num} dimensions which are phrases that can compare multiple papers, so that the table has {col_num} columns. The table should also have { paper_num} papers as rows. Return a JSON object of the following format:</p>
<div class="codehilite"><pre><span></span><code>    json
{json_format}
</code></pre></div>

<p><strong>Check that the table has { paper_num} papers as rows and {column_num} dimensions as
columns.</strong>.</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Paper Content]</span>
<span class="na">{paper1} {paper2} ... {paperN}</span>
</code></pre></div>

<h2>B. 2 Prompt for schema generation</h2>
<p>System prompt is the same as the one from table generation.</p>
<h2>B.2.1 Schema generation with generated captions</h2>
<div class="codehilite"><pre><span></span><code><span class="k">User</span><span class="w"> </span><span class="nl">Prompt</span><span class="p">:</span><span class="w"> </span><span class="o">[</span><span class="n">System</span><span class="o">]</span>
<span class="n">Imagine</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="nl">scenario</span><span class="p">:</span><span class="w"> </span><span class="n">A</span>
<span class="w">    </span><span class="k">user</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">making</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="nc">table</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span>
<span class="w">    </span><span class="n">scholarly</span><span class="w"> </span><span class="n">paper</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">contains</span>
<span class="w">    </span><span class="n">information</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">multiple</span>
<span class="w">    </span><span class="n">papers</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">compares</span><span class="w"> </span><span class="n">these</span>
<span class="w">    </span><span class="n">papers</span><span class="p">.</span><span class="w"> </span><span class="k">To</span><span class="w"> </span><span class="n">compare</span><span class="w"> </span><span class="ow">and</span>
<span class="w">    </span><span class="n">contrast</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">papers</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">user</span>
<span class="w">    </span><span class="n">provides</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">title</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">content</span>
<span class="w">        </span><span class="k">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">paper</span><span class="p">.</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="k">is</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="nl">following</span><span class="p">:</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span>
<span class="w">        </span><span class="n">papers</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">find</span>
<span class="w">        </span><span class="n">aspects</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">shared</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span>
<span class="w">        </span><span class="n">given</span><span class="w"> </span><span class="n">research</span><span class="w"> </span><span class="n">papers</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span><span class="p">,</span>
<span class="w">        </span><span class="k">within</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">aspect</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span>
<span class="w">        </span><span class="n">identify</span><span class="w"> </span><span class="err">{</span><span class="n">num_columns</span><span class="err">}</span>
<span class="w">        </span><span class="n">attributes</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="k">to</span>
<span class="w">        </span><span class="n">compare</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">papers</span><span class="p">.</span>
</code></pre></div>

<p>First, you should return the list of similar aspects as a Python list as follows: "["&lt; similar aspect that all given papers shared&gt;", ...]". Then, think of each aspect as the topic for the Related Work section of the user's paper. Finally, find attributes that can compare the given papers within the Related Work section. Return a JSON object in the following format:</p>
<div class="codehilite"><pre><span></span><code>    json
{{
    &quot;&lt;attribute 1&gt;&quot;: [&quot;&lt;comparable
        attribute within the aspect
        1&gt;&quot;, &quot;&lt;comparable attribute
        within the aspect 1&gt;&quot;, ...],
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    ...
]}
~..
</code></pre></div>

<p>[Paper Content]
{paper1} {paper2} ... {paperN}</p>
<div class="codehilite"><pre><span></span><code>Please ensure that your response
    strictly follows the given
    format. Adherence to the
    specified structure is
    mandatory.
</code></pre></div>

<h2>B.2.2 Schema generation with caption and in-text references</h2>
<p>Generation of schemas with captions does not include the in-text references part in the prompt below. This prompt is when the number of in-text references is $K$</p>
<h2>User Prompt: [System]</h2>
<p>Imagine the following scenario: A user is making a table for a scholarly paper that contains information about multiple papers and compares these papers. To compare and contrast the papers, the user provides the title and content of each paper. To help you build the table, the user provides a caption of this table, which is referred to in the paper as additional information.</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">Caption</span><span class="o">]</span>
</code></pre></div>

<p>{caption}
[In-text reference]
{section header 1: in-text
reference 1}{section header 2:
in-text reference 2}...{ section header K: in-text reference K}</p>
<p>Your task is the following: Given a list of papers and table caption, you should identify { num_columns } table columns to compare given research papers.</p>
<p>Return a list in the following format:</p>
<div class="codehilite"><pre><span></span><code>    \List
[&quot;&lt;comparable attribute within
    the table caption&gt;&quot;, &quot;&lt;
    comparable attribute within
    the table caption&gt;&quot;, ...]
</code></pre></div>

<p>[Paper Content]
{paper1} {paper2} ... {paperN}</p>
<div class="codehilite"><pre><span></span><code>Please ensure that your response
    strictly follows the given
    format. Adherence to the
    specified structure is
    mandatory.
</code></pre></div>

<h2>B.2.3 Schema generation with few-shot examples</h2>
<div class="codehilite"><pre><span></span><code><span class="k">User</span><span class="w"> </span><span class="nl">Prompt</span><span class="p">:</span><span class="w"> </span><span class="o">[</span><span class="n">System</span><span class="o">]</span>
</code></pre></div>

<p>Imagine the following scenario: A user is making a table for a scholarly paper that contains information about multiple papers and compares these papers. To compare and contrast the papers, the user provides the title and content of each paper. To help you build the table, the user provides similar tables that you can refer to as follows:
{Table 1: few-shot example table 1}{Table 2: few-shot example table 2}...{Table 5: few-shot example table 5}</p>
<p>Your task is the following: Given a list of papers and table examples, you should identify {num_columns} table columns to compare given research papers Return a list in the following format:
[List]</p>
<p>["<comparable attribute>", "&lt; comparable attribute&gt;", ...]
[List]
{paper1} {paper2} ... {paperN}
Please ensure that your response strictly follows the given format. Adherence to the specified structure is mandatory.</p>
<h2>B. 3 Prompt for value generation</h2>
<p>Answer a question using the provided scientific paper.</p>
<p>Your response should be a JSON object with the following fields:</p>
<ul>
<li>answer: The answer to the question. The answer should use concise language, but be comprehensive. Only provide answers that are objectively supported by the text in paper</li>
<li>excerpts: A list of one or more <em>EXACT</em> text spans extracted from the paper that support the answer. Return between at most ten spans, and no more that 800 words. Make sure to cover all aspects of the answer above.</li>
</ul>
<p>If there is no answer, return an empty dictionary, i.e., '{}'.</p>
<h2>Paper:</h2>
<p>{ full_text }
Given the information above, please answer the question: "{ question }".</p>
<p>Using this strategy to generate values for columns requires the creation of questions describing the corresponding columns, for which we follow a two-step generation process. First, we
prompt an LLM, specifically GPT-4-Turbo to generate descriptions for every column conditioned on additional context (either reference captions, or reference captions and in-text references). For the setting that does not use any additional context, this step is skipped.</p>
<div class="codehilite"><pre><span></span><code>CAPTION_PROMPT = &quot;&quot;&quot;
A user is making a table for a
    scholarly paper that contains
    information about multiple
    papers and compares these
    papers.
This table contains a column
    called {column}. Please write
    a brief definition for this
    column.
</code></pre></div>

<p>Here is the caption for the table : {caption}.</p>
<div class="codehilite"><pre><span></span><code>Definition:
&quot;&quot;&quot;
</code></pre></div>

<p>CAPTION_WITH_REF_PROMPT = """
A user is making a table for a scholarly paper that contains information about multiple papers and compares these papers.
This table contains a column called {column}. Please write a brief definition for this column.</p>
<p>Here is the caption for the table : {caption}.</p>
<p>Following is some additional information about this table: {in_text_ref}.</p>
<h2>Definition:</h2>
<p>" " "</p>
<p>Then, LLMs are prompted to rewrite generated definitions as concise queries. For the nocontext setting, we use a simple template to produce queries containing the column name.</p>
<div class="codehilite"><pre><span></span><code>CONTEXT_QUERY = &quot;Rewrite this
    description as a one-line
    question.&quot;
</code></pre></div>

<p>NO_CONTEXT_QUERY = "From the provided paper full-text, can you extract {column}?"</p>
<p>Our preliminary experiments show that the value generation module often returns empty values (in $30 \%$ cases on average), which motivates us to add a retry policy. Under this policy, we generate four additional queries with minor rephrasing and retry value generation with them. We observe that this reduces the proportion of empty values to $\sim 7.5 \%$. If all retries produce empty values, we return an empty value.</p>
<div class="codehilite"><pre><span></span><code>CONTEXT_RETRY_QUERIES
original_query + &quot;Return a
    summary of this information&quot;
original_query + &quot;Try to extract
    this information.&quot;
original_query + &quot;Summarize
    information about this.&quot;
original_query + &quot;What
    information can you find about
        this?&quot;
</code></pre></div>

<p>NO_CONTEXT_RETRY _QUERIES
Extract information about {column } aspect from this paper.
What information can you find about {column}?
We want to create a table comparing papers. Extract the information from this paper that goes in the column called {column}.
In a literature review table comparing multiple papers, what information from this paper would go under column { column}?</p>
<h2>B. 4 Prompt for Llama-3 Scorer for Automatic Evaluation</h2>
<div class="codehilite"><pre><span></span><code><span class="nv">Given</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">tables</span>,<span class="w"> </span><span class="nv">match</span><span class="w"> </span><span class="nv">column</span>
<span class="w">    </span><span class="nv">headers</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">their</span><span class="w"> </span><span class="nv">columns</span><span class="w"> </span><span class="nv">have</span>
<span class="w">    </span><span class="nv">very</span><span class="w"> </span><span class="nv">similar</span><span class="w"> </span><span class="nv">values</span>.<span class="w"> </span><span class="nv">Most</span>
<span class="w">    </span><span class="nv">columns</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">match</span>.
<span class="nv">Respond</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">json</span><span class="w"> </span><span class="nv">list</span>,<span class="w"> </span><span class="nv">whose</span>
<span class="w">    </span><span class="nv">elements</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">element</span><span class="w"> </span><span class="nv">lists</span>
</code></pre></div>

<p>. The first element is the key of Object 1 and the matching key of Object 2.
For example, if the key 'Dataset size' and 'Number of training examples' are matched, you should return '[['Dataset size ', 'Number of training examples']]. If no keys contain the same information, then just output an empty list '[]'</p>
<p>Table 1:
[In-context example humanauthored table]</p>
<p>Table 2:
[In-context example generated table]</p>
<p>Response: [In-context example human-aligned aspects]</p>
<h2>B. 5 LM Prompting Details</h2>
<p>Truncation and Error Handling. As our evaluation tests language models' capabilities of schema rediscovery, we implemented strategies for handling other types of errors from language model generation (e.g., a different number number of schemas between generated and reference tables, or the format of the generated output not matching with a format given in the prompt). We take both preventative as well as fall-back measures to deal with these errors:</p>
<ol>
<li>Preventative: To address the issue of generating tables when context window might be insufficient due to the large number of input papers, we adopted the following approach by: (1) dividing the paper sets into smaller batches to ensure the total length of input papers does not exceed the context window size, (2) dividing the columns that need to be created into smaller batches to ensure the total number of columns from whole batches does not exceed the number of columns in human-authored tables, and (3) subsequently joining these smaller tables together without the need for further generation. The batch size is chosen based on the model and input paper</li>
</ol>
<p>Conditions for Paper Representation
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Diagram of prompting methods under experiment conditions.
representation. In general, though, we used a threshold of 20 abstracts per batch, determined by using the average length of the top $20 \%$ longest abstracts to ensure that even long abstracts could fit within the context along with the in-context examples and prompts. We also set the number of max tokens as high as the model can handle.
2. Fall-back: When encountering an error, we retry querying the model with the same prompt, and due to stochasticity in the generation process, models occasionally recover. The errors we handled with fallback strategies are as follows: (1) when the output doesn't align with the format specified in the prompt, (2) when the number of schemas, papers, and values don't match the reference table, and (3) when the entire context exceeds the context window of the base model.
3. Removal: We allow up to five retries before abandoning the input.</p>
<h2>C Human Evaluation</h2>
<p>When performing human evaluation for the novel schema, we assessed each column based on the following criteria:</p>
<ul>
<li>Usefulness: the degree to which this column helps in understanding and comparing the set of input papers.</li>
<li>Specificity: the degree to which a column is specific to the particular set of input papers, rather than applying to any generic set of papers.</li>
<li>Insightfulness: the degree to which a column is about novel and deep aspects. An insightful column goes beyond surface-level information and captures novel or unexpected aspects (e.g., "Method" column may be useful, but it may not be considered highly insightful.)</li>
</ul>
<p>The annotation interface used was created using Streamlit ${ }^{8}$ and can be found in Figure 8.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The interface used for annotating generated table column quality.</p>
<h1>D Qualitative Error Analysis</h1>
<p>For each of the error types listed in Table 5, we include a generated table that illustrates the error and a reference table when appropriate.</p>
<h2>1. Different Topics see Table 7</h2>
<ol>
<li>Different Granularity see Table 8</li>
<li>Complex aspects see Table 9</li>
<li>Overly Specific see Table 10</li>
</ol>
<p>Reference:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;"># categories</th>
<th style="text-align: center;">evaluation metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">765 adbf</td>
<td style="text-align: center;">fine-grained</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">mean accuracy</td>
</tr>
<tr>
<td style="text-align: center;">4 fe 680 c</td>
<td style="text-align: center;">face</td>
<td style="text-align: center;">9,131</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source dataset</th>
<th style="text-align: center;">Target dataset</th>
<th style="text-align: center;">Number of images</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">765 adbf</td>
<td style="text-align: center;">FGVC-Aircraft</td>
<td style="text-align: center;">FGVC-Aircraft</td>
<td style="text-align: center;">10,000 images of airplanes</td>
</tr>
<tr>
<td style="text-align: center;">4 fe 680 c</td>
<td style="text-align: center;">VGGFace2</td>
<td style="text-align: center;">VGGFace2</td>
<td style="text-align: center;">3.31 million images</td>
</tr>
</tbody>
</table>
<p>Table 7: Different Topics: Reference table (top), Predicted table (bottom). We can see that our system generates a different (and redundant) set of aspects compared to the reference.</p>
<p>Reference:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Data size</th>
<th style="text-align: center;">Image</th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;">Tags</th>
<th style="text-align: center;">Video</th>
<th style="text-align: center;">Audio</th>
<th style="text-align: center;">3D Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3919117</td>
<td style="text-align: center;">Twitter100K</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">15514398</td>
<td style="text-align: center;">Xmedia</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">12,000</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset Name</th>
<th style="text-align: center;">Dataset Size</th>
<th style="text-align: center;">Data Types</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3919117</td>
<td style="text-align: center;">Twitter100k</td>
<td style="text-align: center;">100,000 image-text pairs</td>
<td style="text-align: center;">LDA, Bag-of-Word (BoW), ...</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">15514398</td>
<td style="text-align: center;">XMedia, Wikipedia, $\ldots$</td>
<td style="text-align: center;">12,000 media instances</td>
<td style="text-align: center;">text, image, video, audio, 3D model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: Different Granularities: Reference table (top), Predicted table (bottom). Some aspects are removed from each table to highlight the difference in granularity. The reference table separately splits out the various data types while the generated one has a single "Data Types" column.</p>
<p>Reference:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Classes</th>
<th style="text-align: center;">Signer</th>
<th style="text-align: center;">Videos</th>
<th style="text-align: center;">Videos per Class</th>
<th style="text-align: center;">Controlled</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">64745485</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3200</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">54446047</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$11-45$</td>
<td style="text-align: center;">25513</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$\times$</td>
</tr>
</tbody>
</table>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sign Language</th>
<th style="text-align: center;">Dataset Size</th>
<th style="text-align: center;">Number of Subjects</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">64745485</td>
<td style="text-align: center;">Dataset of Argentinian Sign</td>
<td style="text-align: center;">3200 videos, 64 LSA signs,</td>
<td style="text-align: center;">10 subjects</td>
</tr>
<tr>
<td style="text-align: center;">54446047</td>
<td style="text-align: center;">Language (LSA) presented Large-scale sign language dataset created</td>
<td style="text-align: center;">10 subjects over 25,000 annotated videos</td>
<td style="text-align: center;">222 subjects</td>
</tr>
</tbody>
</table>
<p>Table 9: Complex Aspects: Reference table (top), Predicted table (bottom).</p>
<p>Generated:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hate Speech Dataset</th>
<th style="text-align: center;">Misinformation Dataset</th>
<th style="text-align: center;">Number of Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">253018764</td>
<td style="text-align: center;">Mentions [...] Hate Speech Dataset</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">multiple hate speech datasets</td>
</tr>
<tr>
<td style="text-align: center;">10326133</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Introduces LIAR dataset...</td>
<td style="text-align: center;">12,836</td>
</tr>
</tbody>
</table>
<p>Table 10: Overly Specific: The table shown is the predicted table. Note that the aspects "Hate Speech Dataset" and "Misinformation Dataset" only apply to a single paper each.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://streamlit.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ Predicted matches are rated either as incorrect, partially, or completely correct. The lower bound only counts complete matches and the upper bound includes partial matches.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-440 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-440</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-440</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-267710805</p>
                <p><strong>Paper Title:</strong> Framework for evaluating code generation ability of large language models</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have revolutionized various applications in natural language processing and exhibited proficiency in generating programming code. We propose a framework for evaluating the code generation ability of LLMs and introduce a new metric, pass‐ratio@n , which captures the granularity of accuracy according to the pass rate of test cases. The framework is intended to be fully automatic to handle the repetitive work involved in generating prompts, conducting inferences, and executing the generated codes. A preliminary evaluation focusing on the prompt detail, problem publication date, and difficulty level demonstrates the successful integration of our framework with the LeetCode coding platform and highlights the applicability of the pass‐ratio@n metric.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e440.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e440.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-detail mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between prompt natural-language detail and generated code correctness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Insufficient or less-detailed natural-language prompts lead to generated code that fails specific test cases; increasing prompt detail (constraints + examples) improved correctness in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based code generation evaluation pipeline (LeetCode-integrated)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated pipeline that extracts LeetCode problems, generates prompts at varying detail levels, runs multiple LLM inferences, and executes resulting code against LeetCode test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>problem description / prompt (natural-language problem statement, constraints, execution examples)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated source code (Python solutions produced by LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/ambiguous specification in prompt (insufficient prompt detail)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When prompts omitted constraints or examples (lower-detail prompts), generated code sometimes implemented incorrect parameter choices or logic (e.g., wrong sort order) that caused failures on edge test inputs; more detailed prompts (constraints + examples) reduced such errors.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>prompt-to-code generation step (specification given to model)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical execution of generated code on LeetCode test suites and manual inspection of failing cases (automated test run flagged failing test; manual code review identified incorrect parameter usage)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Count of solved problems (pass/fail per problem) for each prompt type across a sample of 10 LeetCode problems: Type1 (description only) solved 6/10, Type2 (description+constraints) solved 7/10, Type3 (description+constraints+examples) solved 8/10; specific failing test case example returned expected 2 but actual 1 (one test case comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Incorrect or underspecified prompts caused concrete test failures (e.g., one problem produced output 1 instead of expected 2), lowering pass/fail based metrics; moving to more-detailed prompts increased number of fully-solved problems (from 6 to 8 in the small sample).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in the paper's small sample: performance improved monotonically with prompt detail (6, 7, 8 solved of 10 for types 1–3 respectively); prevalence across broader corpora not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or underspecified natural-language prompt (missing constraints or illustrative examples); LLMs rely on prompt context and can choose incorrect default behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide richer prompts including explicit constraints and representative examples (Type 3 prompts); include function signature; automate prompt templates to ensure consistent detail.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>In the reported experiment, Type 3 prompts solved 8/10 problems vs 6/10 for Type 1, demonstrating empirical improvement on the sampled set; single-case manual correction (changing trainers.sort(reverse=True) to trainers.sort()) fixed the specific bug.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / code generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Framework for evaluating code generation ability of large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e440.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e440.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Publication-date / training-data gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between problem publication date and model training data leading to degraded performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Problems published after the LLM training cutoff are less likely to be solved correctly by the model, revealing a discrepancy between the assumed specification availability and the model's learned code behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM evaluation on temporally partitioned LeetCode problems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation grouping LeetCode problems by publication year (pre- and post-model training date) and difficulty, running multiple inferences and scoring with execution-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>problem publication metadata / temporal context (natural-language problem statements with publication dates)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated source code (LLM outputs executed on platform test suites)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset temporal mismatch / unseen specification (training-data absence)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>LLMs performed substantially better on problems published before their training cutoff than on problems published after; the model lacks exposure to newer problem variants and contexts and thus its generated code can misalign with the current specification space.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model knowledge (training data) vs evaluation dataset (new problem specifications)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Stratified experiment by publication year: selected 10 problems per year/difficulty and ran 5 inferences per problem; compared solved counts across years.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counts of fully solved problems (all test cases passed) out of 10 per year/difficulty. Example reported: for problems up to 2021 GPT-3.5 solved 8 easy, 4 medium, 3 hard; performance declined for 2022 and 2023 problems (many categories had 0 solved). Also compared pass@1, pass@5, and pass-ratio@5 across years.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Significant drop in task success on post-training problems, sometimes reaching zero fully-solved problems (pass@1/pass@5 = 0 for some 2022/2023 groups), indicating biased evaluation if temporal leakage is not controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across the evaluated sample: higher success for pre-training (≤2021) problems and notable decline for 2022–2023 problems; exact prevalence across all LeetCode not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Model training cutoff and inclusion/exclusion of up-to-date examples in the training data; models cannot reliably generalize to totally new problem contexts absent training exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Select evaluation problems explicitly published after the model's training cutoff to test generalization; iteratively retrain or update models to include newer data; analyze publication-date stratified performance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Paper documents the effectiveness of the stratification at exposing the gap (i.e., it revealed the performance drop); model retraining or updates are proposed as necessary but not evaluated quantitatively within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / code generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Framework for evaluating code generation ability of large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e440.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e440.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metric granularity gap (pass@k vs pass-ratio@n)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Binary pass@k metric fails to capture partial correctness; pass-ratio@n captures per-test-case granularity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The widely-used pass@k metric treats a solution as either fully correct or incorrect and therefore cannot represent near-correct solutions; pass-ratio@n is introduced to measure the averaged (squared) per-solution pass fraction across multiple inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evaluation metrics for code generation (pass@k and pass-ratio@n)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparative metric analysis where multiple inferences per problem (n) are executed and results are reported using both pass@k and the proposed pass-ratio@n to reflect partial test-case success.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>evaluation metric specification / test-suite pass descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation scripts that compute pass@k and pass-ratio@n from test execution outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete evaluation specification / binary scoring mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>pass@k's binary formulation (solution must pass all tests) treats near-misses (e.g., passing most but not all test cases) the same as total failure, hiding useful information about partial solution quality; this misalignment between 'degree of correctness' (natural-language notion) and metric leads to loss of signal.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / scoring</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Direct comparison between pass@k results and the newly computed pass-ratio@n on example problems where solutions passed varying subsets of test cases; reported cases where pass@k=0 while pass-ratio@5>0.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>pass-ratio@n computed as mean over n solutions of (passed_test_cases/total_test_cases)^2; comparisons shown with pass@1 and pass@5 for n=5. Example: a problem with 3 of 5 solutions fully passing had pass@1=60%, pass@5=100% and pass-ratio@5=78%; another problem had pass@k=0 but pass-ratio@5=61%.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Binary metrics can report zero (no solutions) despite high partial pass rates, underrepresenting the model's problem-solving capability and misguiding conclusions; pass-ratio@n recovers nuance and shows intermediate skill levels.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across the studied example problems and in the broader per-year/difficulty experiment where pass@1/pass@5 were often zero for recent problems while pass-ratio@5 showed nonzero values.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Simplifying evaluation to a binary success criterion rather than measuring granularity of test-case-level success.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Introduce pass-ratio@n (average squared pass fraction across n inferences) to weight higher pass rates more and reflect partial correctness; choose appropriate n depending on LLM randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated qualitatively: pass-ratio@5 produced meaningful intermediate values (e.g., 78%, 61%) where pass@k either overstated (pass@5=100% when only some runs passed?) or understated (pass@k=0) capability; quantitative benefits shown in reported examples but full statistical validation deferred.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / evaluation metrics / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Framework for evaluating code generation ability of large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e440.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e440.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-case coverage gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Insufficient or non-diverse test suites in benchmark datasets creating misleading evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Many existing code-generation datasets have few and simple test cases, which can inflate pass@k scores; using platforms with extensive test suites (e.g., LeetCode, APPS) provides deeper evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Benchmark dataset selection for LLM code evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Protocol and dataset selection guidance emphasizing the number, diversity, and recency of test cases when evaluating LLM-generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset documentation / test-suite specifications</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>test harness / execution environment (LeetCode test runner via CLI)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>insufficient test coverage / dataset inadequacy</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Benchmarks like HumanEval and MBPP often include only a small number of simple tests (e.g., ~3 test cases), which can cause inflated pass@k scores and fail to detect edge-case or performance issues; hence evaluations using those datasets may not reflect real-world model capability.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation datasets / test-suite design</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Literature analysis and argumentation comparing typical dataset sizes to LeetCode/APPS which offer many more test cases; empirical use of LeetCode's larger test sets in the framework.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No single numeric prevalence provided, but examples: HumanEval contains 164 problems with multiple unit tests; MBPP contains 1000 problems with 3 test cases; LeetCode problems often provide >100 test cases in aggregate for deep evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using small test suites can overestimate model performance (high pass@k on trivial/sparse tests) and reduce ability to detect partial-correctness or brittle solutions; switching to richer test suites produces more reliable functional assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common across many prior datasets (HumanEval/MBPP); the paper argues this is a prevalent limitation in existing benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Historical benchmark design choices favoring small handcrafted tests and simplicity; difficulty/cost in assembling large, diverse test suites.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Prefer large real-world platforms (LeetCode, APPS) for evaluation because they provide numerous test cases; require more comprehensive test-case design when creating benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantified with large-scale statistics in this paper, but framework integration with LeetCode enabled deeper evaluation and revealed partial passes captured by pass-ratio@n that would've been lost on small test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / benchmarking / software testing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Framework for evaluating code generation ability of large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e440.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e440.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training-data leakage risk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation bias due to inclusion of benchmark problems in model training data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks or coding problems used for evaluation may have been present in the model's training corpus, producing overoptimistic performance estimates; the paper emphasizes selecting problems published after model training to avoid this contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Temporal-control dataset selection for unbiased LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Filtering and selection of LeetCode problems by publication date relative to model training date to avoid evaluating on problems the model likely saw during training.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset provenance / publication-date metadata</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>benchmark selection scripts and evaluation harness</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset leakage / contamination (benchmark in training data)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>If evaluation problems are included in a model's training data, evaluations reflect memorization rather than generalization; this misalignment between assumed unseen specification and model's prior exposure leads to misleadingly high performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset selection / training data overlap</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Temporal partitioning (select problems published after model release/training date) and observing performance differences between pre- and post-training problems.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparative solved counts and metric changes across temporal partitions; the paper reports declines in solved-problem counts for post-training problems as evidence of leakage impact on pre-training problem performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially large: pre-training problems show substantially better performance, implying inflated evaluation if leakage is not controlled; concrete numeric examples show higher solve rates pre-2021 vs near-zero fully-solved rates in many 2022/2023 groups.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Mentioned as a general concern but prevalence is not quantitatively measured in the paper; the paper recommends guarding against it via temporal filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Training corpora include public coding problems and solutions; benchmarks are not always annotated for inclusion in training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Select evaluation problems published after model training cutoff, or otherwise verify that evaluation items were not present in training data; use platforms that regularly add new problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The temporal partitioning approach revealed degradation on post-training problems in the experiments (demonstrating the approach's diagnostic power); model retraining to include new data is proposed but not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / benchmark evaluation / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Framework for evaluating code generation ability of large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Measuring coding challenge competence with apps <em>(Rating: 2)</em></li>
                <li>Competition-level code generation with alphacode <em>(Rating: 1)</em></li>
                <li>SPoC: Search-based Pseudocode to Code <em>(Rating: 1)</em></li>
                <li>CodeBLEU: a method for automatic evaluation of code synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-440",
    "paper_id": "paper-267710805",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Prompt-detail mismatch",
            "name_full": "Mismatch between prompt natural-language detail and generated code correctness",
            "brief_description": "Insufficient or less-detailed natural-language prompts lead to generated code that fails specific test cases; increasing prompt detail (constraints + examples) improved correctness in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-based code generation evaluation pipeline (LeetCode-integrated)",
            "system_description": "Automated pipeline that extracts LeetCode problems, generates prompts at varying detail levels, runs multiple LLM inferences, and executes resulting code against LeetCode test cases.",
            "nl_description_type": "problem description / prompt (natural-language problem statement, constraints, execution examples)",
            "code_implementation_type": "generated source code (Python solutions produced by LLMs)",
            "gap_type": "incomplete/ambiguous specification in prompt (insufficient prompt detail)",
            "gap_description": "When prompts omitted constraints or examples (lower-detail prompts), generated code sometimes implemented incorrect parameter choices or logic (e.g., wrong sort order) that caused failures on edge test inputs; more detailed prompts (constraints + examples) reduced such errors.",
            "gap_location": "prompt-to-code generation step (specification given to model)",
            "detection_method": "Empirical execution of generated code on LeetCode test suites and manual inspection of failing cases (automated test run flagged failing test; manual code review identified incorrect parameter usage)",
            "measurement_method": "Count of solved problems (pass/fail per problem) for each prompt type across a sample of 10 LeetCode problems: Type1 (description only) solved 6/10, Type2 (description+constraints) solved 7/10, Type3 (description+constraints+examples) solved 8/10; specific failing test case example returned expected 2 but actual 1 (one test case comparison).",
            "impact_on_results": "Incorrect or underspecified prompts caused concrete test failures (e.g., one problem produced output 1 instead of expected 2), lowering pass/fail based metrics; moving to more-detailed prompts increased number of fully-solved problems (from 6 to 8 in the small sample).",
            "frequency_or_prevalence": "Observed in the paper's small sample: performance improved monotonically with prompt detail (6, 7, 8 solved of 10 for types 1–3 respectively); prevalence across broader corpora not quantified.",
            "root_cause": "Ambiguous or underspecified natural-language prompt (missing constraints or illustrative examples); LLMs rely on prompt context and can choose incorrect default behaviors.",
            "mitigation_approach": "Provide richer prompts including explicit constraints and representative examples (Type 3 prompts); include function signature; automate prompt templates to ensure consistent detail.",
            "mitigation_effectiveness": "In the reported experiment, Type 3 prompts solved 8/10 problems vs 6/10 for Type 1, demonstrating empirical improvement on the sampled set; single-case manual correction (changing trainers.sort(reverse=True) to trainers.sort()) fixed the specific bug.",
            "domain_or_field": "machine learning / code generation evaluation",
            "reproducibility_impact": true,
            "uuid": "e440.0",
            "source_info": {
                "paper_title": "Framework for evaluating code generation ability of large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Publication-date / training-data gap",
            "name_full": "Mismatch between problem publication date and model training data leading to degraded performance",
            "brief_description": "Problems published after the LLM training cutoff are less likely to be solved correctly by the model, revealing a discrepancy between the assumed specification availability and the model's learned code behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM evaluation on temporally partitioned LeetCode problems",
            "system_description": "Evaluation grouping LeetCode problems by publication year (pre- and post-model training date) and difficulty, running multiple inferences and scoring with execution-based metrics.",
            "nl_description_type": "problem publication metadata / temporal context (natural-language problem statements with publication dates)",
            "code_implementation_type": "generated source code (LLM outputs executed on platform test suites)",
            "gap_type": "dataset temporal mismatch / unseen specification (training-data absence)",
            "gap_description": "LLMs performed substantially better on problems published before their training cutoff than on problems published after; the model lacks exposure to newer problem variants and contexts and thus its generated code can misalign with the current specification space.",
            "gap_location": "model knowledge (training data) vs evaluation dataset (new problem specifications)",
            "detection_method": "Stratified experiment by publication year: selected 10 problems per year/difficulty and ran 5 inferences per problem; compared solved counts across years.",
            "measurement_method": "Counts of fully solved problems (all test cases passed) out of 10 per year/difficulty. Example reported: for problems up to 2021 GPT-3.5 solved 8 easy, 4 medium, 3 hard; performance declined for 2022 and 2023 problems (many categories had 0 solved). Also compared pass@1, pass@5, and pass-ratio@5 across years.",
            "impact_on_results": "Significant drop in task success on post-training problems, sometimes reaching zero fully-solved problems (pass@1/pass@5 = 0 for some 2022/2023 groups), indicating biased evaluation if temporal leakage is not controlled.",
            "frequency_or_prevalence": "Observed across the evaluated sample: higher success for pre-training (≤2021) problems and notable decline for 2022–2023 problems; exact prevalence across all LeetCode not provided.",
            "root_cause": "Model training cutoff and inclusion/exclusion of up-to-date examples in the training data; models cannot reliably generalize to totally new problem contexts absent training exposure.",
            "mitigation_approach": "Select evaluation problems explicitly published after the model's training cutoff to test generalization; iteratively retrain or update models to include newer data; analyze publication-date stratified performance.",
            "mitigation_effectiveness": "Paper documents the effectiveness of the stratification at exposing the gap (i.e., it revealed the performance drop); model retraining or updates are proposed as necessary but not evaluated quantitatively within this study.",
            "domain_or_field": "machine learning / code generation evaluation",
            "reproducibility_impact": true,
            "uuid": "e440.1",
            "source_info": {
                "paper_title": "Framework for evaluating code generation ability of large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Metric granularity gap (pass@k vs pass-ratio@n)",
            "name_full": "Binary pass@k metric fails to capture partial correctness; pass-ratio@n captures per-test-case granularity",
            "brief_description": "The widely-used pass@k metric treats a solution as either fully correct or incorrect and therefore cannot represent near-correct solutions; pass-ratio@n is introduced to measure the averaged (squared) per-solution pass fraction across multiple inferences.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Evaluation metrics for code generation (pass@k and pass-ratio@n)",
            "system_description": "Comparative metric analysis where multiple inferences per problem (n) are executed and results are reported using both pass@k and the proposed pass-ratio@n to reflect partial test-case success.",
            "nl_description_type": "evaluation metric specification / test-suite pass descriptions",
            "code_implementation_type": "evaluation scripts that compute pass@k and pass-ratio@n from test execution outcomes",
            "gap_type": "incomplete evaluation specification / binary scoring mismatch",
            "gap_description": "pass@k's binary formulation (solution must pass all tests) treats near-misses (e.g., passing most but not all test cases) the same as total failure, hiding useful information about partial solution quality; this misalignment between 'degree of correctness' (natural-language notion) and metric leads to loss of signal.",
            "gap_location": "evaluation metrics / scoring",
            "detection_method": "Direct comparison between pass@k results and the newly computed pass-ratio@n on example problems where solutions passed varying subsets of test cases; reported cases where pass@k=0 while pass-ratio@5&gt;0.",
            "measurement_method": "pass-ratio@n computed as mean over n solutions of (passed_test_cases/total_test_cases)^2; comparisons shown with pass@1 and pass@5 for n=5. Example: a problem with 3 of 5 solutions fully passing had pass@1=60%, pass@5=100% and pass-ratio@5=78%; another problem had pass@k=0 but pass-ratio@5=61%.",
            "impact_on_results": "Binary metrics can report zero (no solutions) despite high partial pass rates, underrepresenting the model's problem-solving capability and misguiding conclusions; pass-ratio@n recovers nuance and shows intermediate skill levels.",
            "frequency_or_prevalence": "Observed across the studied example problems and in the broader per-year/difficulty experiment where pass@1/pass@5 were often zero for recent problems while pass-ratio@5 showed nonzero values.",
            "root_cause": "Simplifying evaluation to a binary success criterion rather than measuring granularity of test-case-level success.",
            "mitigation_approach": "Introduce pass-ratio@n (average squared pass fraction across n inferences) to weight higher pass rates more and reflect partial correctness; choose appropriate n depending on LLM randomness.",
            "mitigation_effectiveness": "Demonstrated qualitatively: pass-ratio@5 produced meaningful intermediate values (e.g., 78%, 61%) where pass@k either overstated (pass@5=100% when only some runs passed?) or understated (pass@k=0) capability; quantitative benefits shown in reported examples but full statistical validation deferred.",
            "domain_or_field": "machine learning / evaluation metrics / code generation",
            "reproducibility_impact": true,
            "uuid": "e440.2",
            "source_info": {
                "paper_title": "Framework for evaluating code generation ability of large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Test-case coverage gap",
            "name_full": "Insufficient or non-diverse test suites in benchmark datasets creating misleading evaluation",
            "brief_description": "Many existing code-generation datasets have few and simple test cases, which can inflate pass@k scores; using platforms with extensive test suites (e.g., LeetCode, APPS) provides deeper evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Benchmark dataset selection for LLM code evaluation",
            "system_description": "Protocol and dataset selection guidance emphasizing the number, diversity, and recency of test cases when evaluating LLM-generated code.",
            "nl_description_type": "dataset documentation / test-suite specifications",
            "code_implementation_type": "test harness / execution environment (LeetCode test runner via CLI)",
            "gap_type": "insufficient test coverage / dataset inadequacy",
            "gap_description": "Benchmarks like HumanEval and MBPP often include only a small number of simple tests (e.g., ~3 test cases), which can cause inflated pass@k scores and fail to detect edge-case or performance issues; hence evaluations using those datasets may not reflect real-world model capability.",
            "gap_location": "evaluation datasets / test-suite design",
            "detection_method": "Literature analysis and argumentation comparing typical dataset sizes to LeetCode/APPS which offer many more test cases; empirical use of LeetCode's larger test sets in the framework.",
            "measurement_method": "No single numeric prevalence provided, but examples: HumanEval contains 164 problems with multiple unit tests; MBPP contains 1000 problems with 3 test cases; LeetCode problems often provide &gt;100 test cases in aggregate for deep evaluation.",
            "impact_on_results": "Using small test suites can overestimate model performance (high pass@k on trivial/sparse tests) and reduce ability to detect partial-correctness or brittle solutions; switching to richer test suites produces more reliable functional assessments.",
            "frequency_or_prevalence": "Common across many prior datasets (HumanEval/MBPP); the paper argues this is a prevalent limitation in existing benchmarks.",
            "root_cause": "Historical benchmark design choices favoring small handcrafted tests and simplicity; difficulty/cost in assembling large, diverse test suites.",
            "mitigation_approach": "Prefer large real-world platforms (LeetCode, APPS) for evaluation because they provide numerous test cases; require more comprehensive test-case design when creating benchmarks.",
            "mitigation_effectiveness": "Not quantified with large-scale statistics in this paper, but framework integration with LeetCode enabled deeper evaluation and revealed partial passes captured by pass-ratio@n that would've been lost on small test sets.",
            "domain_or_field": "machine learning / benchmarking / software testing",
            "reproducibility_impact": true,
            "uuid": "e440.3",
            "source_info": {
                "paper_title": "Framework for evaluating code generation ability of large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Training-data leakage risk",
            "name_full": "Evaluation bias due to inclusion of benchmark problems in model training data",
            "brief_description": "Benchmarks or coding problems used for evaluation may have been present in the model's training corpus, producing overoptimistic performance estimates; the paper emphasizes selecting problems published after model training to avoid this contamination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Temporal-control dataset selection for unbiased LLM evaluation",
            "system_description": "Filtering and selection of LeetCode problems by publication date relative to model training date to avoid evaluating on problems the model likely saw during training.",
            "nl_description_type": "dataset provenance / publication-date metadata",
            "code_implementation_type": "benchmark selection scripts and evaluation harness",
            "gap_type": "dataset leakage / contamination (benchmark in training data)",
            "gap_description": "If evaluation problems are included in a model's training data, evaluations reflect memorization rather than generalization; this misalignment between assumed unseen specification and model's prior exposure leads to misleadingly high performance.",
            "gap_location": "dataset selection / training data overlap",
            "detection_method": "Temporal partitioning (select problems published after model release/training date) and observing performance differences between pre- and post-training problems.",
            "measurement_method": "Comparative solved counts and metric changes across temporal partitions; the paper reports declines in solved-problem counts for post-training problems as evidence of leakage impact on pre-training problem performance.",
            "impact_on_results": "Potentially large: pre-training problems show substantially better performance, implying inflated evaluation if leakage is not controlled; concrete numeric examples show higher solve rates pre-2021 vs near-zero fully-solved rates in many 2022/2023 groups.",
            "frequency_or_prevalence": "Mentioned as a general concern but prevalence is not quantitatively measured in the paper; the paper recommends guarding against it via temporal filtering.",
            "root_cause": "Training corpora include public coding problems and solutions; benchmarks are not always annotated for inclusion in training sets.",
            "mitigation_approach": "Select evaluation problems published after model training cutoff, or otherwise verify that evaluation items were not present in training data; use platforms that regularly add new problems.",
            "mitigation_effectiveness": "The temporal partitioning approach revealed degradation on post-training problems in the experiments (demonstrating the approach's diagnostic power); model retraining to include new data is proposed but not evaluated.",
            "domain_or_field": "machine learning / benchmark evaluation / dataset curation",
            "reproducibility_impact": true,
            "uuid": "e440.4",
            "source_info": {
                "paper_title": "Framework for evaluating code generation ability of large language models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Measuring coding challenge competence with apps",
            "rating": 2,
            "sanitized_title": "measuring_coding_challenge_competence_with_apps"
        },
        {
            "paper_title": "Competition-level code generation with alphacode",
            "rating": 1,
            "sanitized_title": "competitionlevel_code_generation_with_alphacode"
        },
        {
            "paper_title": "SPoC: Search-based Pseudocode to Code",
            "rating": 1,
            "sanitized_title": "spoc_searchbased_pseudocode_to_code"
        },
        {
            "paper_title": "CodeBLEU: a method for automatic evaluation of code synthesis",
            "rating": 1,
            "sanitized_title": "codebleu_a_method_for_automatic_evaluation_of_code_synthesis"
        }
    ],
    "cost": 0.01381875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Framework for evaluating code generation ability of large language models</p>
<p>Sangyeop Yeo 
Division of Artificial Intelligence
University of Science and Technology
DaejeonRepublic of Korea</p>
<p>Yu-Seung Ma ysma@etri.re.kr 
Division of Artificial Intelligence
University of Science and Technology
DaejeonRepublic of Korea</p>
<p>Artificial Intelligence Computing Research Laboratory
Electronics and Telecommunications Research Institute
DaejeonRepublic of Korea</p>
<p>Artificial Intelligence Computing Research Laboratory
Electronics and Telecommunications Research Institute
DaejeonRepublic of Korea</p>
<p>Sang Cheol Kim 
Artificial Intelligence Computing Research Laboratory
Electronics and Telecommunications Research Institute
DaejeonRepublic of Korea</p>
<p>| Hyungkook Jun 
Artificial Intelligence Computing Research Laboratory
Electronics and Telecommunications Research Institute
DaejeonRepublic of Korea</p>
<p>| Taeho Kim </p>
<p>Electronics and Telecommunications Research Institute
DaejeonRepublic of Korea</p>
<p>Framework for evaluating code generation ability of large language models
09DEE3D1A804BED46BDC1450CB9D197810.4218/etrij.2023-0357Received: 27 August 2023 Revised: 22 November 2023 Accepted: 20 December 2023code generationevaluation metriclarge language modelnatural language processingsoftware engineering
Large language models (LLMs) have revolutionized various applications in natural language processing and exhibited proficiency in generating programming code.We propose a framework for evaluating the code generation ability of LLMs and introduce a new metric, pass-ratio@n, which captures the granularity of accuracy according to the pass rate of test cases.The framework is intended to be fully automatic to handle the repetitive work involved in generating prompts, conducting inferences, and executing the generated codes.A preliminary evaluation focusing on the prompt detail, problem publication date, and difficulty level demonstrates the successful integration of our framework with the LeetCode coding platform and highlights the applicability of the pass-ratio@n metric.</p>
<p>| INTRODUCTION</p>
<p>Recently, large language models (LLMs) have emerged as powerful tools for natural language processing [1][2][3], revolutionizing various applications such as text generation, translation, and question answering.Although initially intended for natural language understanding and generation, LLMs have also displayed remarkable proficiency in comprehending and generating programming code.Employing LLMs to generate source code implies utilizing advanced machine-learning models to assist in implementing code for specific tasks or functions.Several LLMs, such as Codex, AlphaCode, and CodeGen [4][5][6], are available for generating source code.These models have been trained on extensive datasets comprising both source code and textual descriptions from various programming languages to provide coding solutions, fix syntax errors, and even help designing algorithms.</p>
<p>Software developers increasingly use LLMs to facilitate code generation.By simply providing a natural language description outlining the desired functionality, LLMs generate the corresponding code.LLMs that generate code can boost developer efficiency, assist with rapid prototyping, ensure that the code implements specific rules, reduce mistakes, and assist nonexperts.Although LLMs offer these advantages, their performance should be further improved, and a thorough quality evaluation is required.In fact, the generated code must be rigorously reviewed and tested to ensure that it meets the design requirements and is error-free.</p>
<p>No general accepted guidelines for evaluating the quality of source code generated by LLMs are available.LLMs are evaluated using diverse metrics and data, thus undermining consistency and hindering comparisons of different models and techniques.Moreover, inconsistent evaluations impede understanding the effectiveness and reliability of LLM-based code generation.</p>
<p>BLEU [7], CodeBLEU [8], and pass@k [9] are often used to assess code functionality.However, the BLEU and CodeBLEU metrics evaluate codes based on their syntactic similarity to a specific single answer.Consequently, even if an LLM generates functionally correct code, it may receive a low score if its syntax differs from that of the reference solution.Alternatively, the pass@k metric assesses functionality based on the actual execution results and may thus be more appropriate for evaluating codes.The pass@k metric evaluates the generated code by executing it and checking whether all the test cases are passed.However, this binary approach, which solely assesses whether the code is completely correct, hinders quality evaluation at a more granular level.This limitation underscores the necessity of introducing additional metrics to capture varying degrees of accuracy.</p>
<p>The selection of datasets for evaluating code generation is also important.Several studies [10][11][12] have been proposed to evaluate the quality of LLMs for code generation.However, the datasets used in the experiments varied in terms of program size, complexity, and format.For example, some datasets consist of specifications and codes that are only a few lines long and relatively straightforward, whereas others encompass complex problems found in coding challenges.Additionally, they coding problems may had been included during LLM training.Hence, datasets should be carefully selected to ensure realistic and unbiased assessment of the models.Therefore, data selection guidelines should be established.</p>
<p>To address the above-mentioned problems, we propose a systematic framework for evaluating LLMs with an emphasis on functionality.Nonfunctional evaluations, such as readability and complexity, have been extensively studied [10][11][12][13].Our framework improves previous research by addressing non-functionality.We first describe factors for dataset selection.We then propose a process for evaluating an LLM with a dataset that meets specific criteria.In addition, we address the limitations of the existing evaluation methods by introducing a new metric, pass-ratio@n, which captures the multifaceted nature of code quality.Because evaluating LLMs requires considerable repetitive work, from generating queries, making inferences, and executing the generated codes, we introduce a fully automatic process.</p>
<p>The contributions of this study are as follows:</p>
<p>• We propose an evaluation framework to assess the code generation ability of LLMs.The framework emphasizes integration with common coding platforms.• We derive a new metric, pass-ratio@n, which provides a more granular measure of accuracy by considering the pass rate of test cases across n inferences.• We conducted a preliminary evaluation of the proposed framework to demonstrate its fully automatic capabilities.</p>
<p>The remainder of this paper is organized as follows.Section 2 presents related work on metrics and datasets.Section 3 outlines the dataset conditions for LLM evaluation.Section 4 presents an evaluation framework that includes the new metric.Section 5 describes the proposed framework.Section 6 discusses the limitations, and Section 7 presents our conclusions.</p>
<p>| RELATED WORK</p>
<p>| Metric pass@k</p>
<p>The pass@k [4,9] metric allows to assess the code generation ability of an LLM based on the code execution results.Given a coding problem, the pass@k metric involves analyzing k different code solutions generated by the LLM.If at least one of the k generated solutions passes all the tests, the LLM is considered to have solved the problem.</p>
<p>The pass@k metric introduced in Kulal et al. [9] was further refined in Chen et al. [4] to consider additional n solutions, with n &gt; k.The Codex model was evaluated using the pass@k metric in Chen et al. [4].The pass@k metric is expressed as
pass@k :¼ E problems 1 À nÀc k À Á n k À Á " # ,ð1Þ
where c represents the number of correct code solutions.</p>
<p>To obtain functional correctness according to the pass@k metric, the code should pass all the tests, resembling validation by human programmers of real-world code.</p>
<p>However, perfectly passing all the tests for a given coding problem is challenging not only for LLMs but also for humans.While examining test success is important, it is also valuable to consider the proportion of test cases that have been successfully passed.</p>
<p>| Datasets</p>
<p>Selecting appropriate datasets is essential for properly assessing the performance of LLMs.Methods that evaluate the code generation ability of LLMs based on code execution, such as the pass@k metric, depend the quality of test data.Common datasets for these types of assessments include HumanEval [4] and MBPP [14].</p>
<p>HumanEval [4] contains 164 handwritten Python problems, each accompanied by a function signature, descriptions, and multiple unit tests.MBPP [14] comprises 1000 crowdsourced Python problems, each with task descriptions, code solutions, and three test cases.Their multilingual versions for code generative models, HumanEval-X [15] and MBXP [16]), have also been developed in recent projects.</p>
<p>The abovementioned datasets typically involve relatively simple programming problems.For example, they involve problems with a source code of fewer than 200 characters and approximately 10 lines.Evaluating LLMs using simple coding problems may result in inflated and unrealistic pass@k scores.In Roziére [17], GPT-4 solved 67% of the problems with only a single inference (that is, pass@1), and the CodeLlama-34B model achieved a 53.7% pass@1 score and 88.2% pass@100 score.A high pass@100 value implies a high possibility that the LLM can find a solution within 100 attempts.However, it is impractical to perform several inferences for a single coding problem because it is resource-intensive, and the most appropriate solution among all the possible answers should be selected.</p>
<p>More realistic problems are being used from coding platforms, such as Codeforces and LeetCode.For example, the APPS dataset [18] includes 10,000 real-world problems from various open-access coding websites, such as Codeforces and Kattis, with 131,836 test cases.LeetCode problems have often been used for evaluations in recent studies [19,20].The LeetCode platform provides useful information for each problem including problem description, topic category, difficulty level, input/output examples, publication date, and massive test data.However, the information available in LeetCode has not been fully exploited when evaluating code generation using LLMs [19,20].In previous studies, LLMs were mainly analyzed focusing on the language type or difficulty level.Hence, various challenges remain for fully using the rich information provided by coding platforms such as LeetCode and automating evaluation, which is essential for assessing the real-world programming ability of LLMs.</p>
<p>| FACTORS FOR DATASET SELECTION</p>
<p>To evaluate the code generation ability of LLMs, various datasets encompassing a wide range of real-world coding problems should be used.Previous studies [4,9,17,20] have often assessed LLMs from a limited perspective, mainly focusing on particular programming languages or difficulty levels.</p>
<p>However, additional factors must be considered when evaluating the code generation ability of LLMs.For instance, the temporal context involves determining whether coding problems are created before or after training the model.This helps ensure that the model does not directly learn the solutions or is exposed to similar problems during training.Real-world relevance is another factor to filter overly simple and outdated coding problems.Furthermore, the availability of the test data is essential to verify the functionality of generated code.Hence, we evaluate code functionality based on the execution results of actual test cases.The amount of test data is important because data scarcity impedes ensuring code correctness.Diverse test data ensure that the code solution is not skewed toward specific inputs or scenarios and allows to determine whether the LLM correctly solves a coding problem.However, most existing datasets lack a comprehensive set of test cases.</p>
<p>Platforms such as LeetCode can help handling the factors for properly evaluating the coding ability.Leet-Code is a popular online platform used to solve coding problems and provides a vast collection of problems.Moreover, each coding problem contains representative information such as the difficulty level and topic.Considering the abovementioned factors, the advantages of using LeetCode are as follows.First, LeetCode regularly hosts coding challenges and adds up-to-date problems.This allows to assess an LLM using coding problems created after model training.In fact, using coding problems published after the LLM release allows to evaluate the LLM ability to handle new and potentially unseen challenges.Second, the coding problems in LeetCode are real-world coding interview questions and challenges faced in actual software development scenarios.Finally, it provides a test dataset for each coding problem, often comprising more than 100 test cases, enabling a deep evaluation of the quality of generated code.</p>
<p>| PROPOSED FRAMEWORK AND METRIC</p>
<p>| Framework architecture</p>
<p>We propose a framework for assessing the code generation ability of LLMs in alignment with platforms such as LeetCode.The proposed framework is expected to support various metrics, and the entire evaluation process is automated.</p>
<p>Figure 1 shows the architecture of the proposed evaluation framework.The architecture is divided into three stages: i) coding problem preparation, ii) LLM execution, and iii) code analysis.These stages proceed sequentially, and storing the outcomes of the previous stage allows skipping it and proceeding to the next stage.</p>
<p>(1) Coding problem preparation Evaluating the code generation ability of LLMs generally involves determining the number of given coding problems that are solved using the generated codes.However, this approach often overlooks the multifaceted analysis of coding problems.For instance, it is meaningful to understand the influence of accuracy with adequate granularity of the prompt input on the LLM for solving a coding problem and evaluate the output variability based on the prompt details.Another factor to consider for a coding problem is the publication date.This temporal information is helpful in ensuring that the problems are not part of the LLM training data.</p>
<p>Table 1 presents the information that must be extracted during coding problem preparation.The problems are designed using the LeetCode platform.Items such as problem descriptions, execution examples, and constraints are used to generate prompts with varying levels of detail.Aspects such as difficulty level, topic, and publication date are used to analyze the evaluation results from various perspectives.Finally, the signature is required to set up an execution environment to run the test cases.</p>
<p>LeetCode offers a graphical user interface that allows users to handle coding problems individually.However, manually extracting coding problems by individually clicking on each problem and copying and pasting the description is labor-intensive.To evaluate LLMs using several coding problems, automating problem extraction is crucial.Our implementation automatically extracts the F I G U R E 1 Overview of LLM evaluation framework using LeetCode.</p>
<p>T A B L E 1 Information to be extracted from every coding problem.</p>
<p>Item Description</p>
<p>Problem description</p>
<p>Content of the task to be solved through programming.This typically encompasses the purpose of the problem.</p>
<p>Execution examples</p>
<p>Sample inputs and expected outputs.</p>
<p>Constraints</p>
<p>Limitations, rules, or conditions that must be followed.(2) LLM execution This stage consists of three steps: generating prompts, performing inferences with prompts, and saving the generated codes.</p>
<p>• Generate prompt: Referring to Table 1, this step creates prompts of different depths: (i) only problem description, (ii) adding constraints, and (iii) adding execution examples.By using prompts with varying levels of detail, we can examine the influence of the prompts and determine those that produce the best results.The function signature is also included to produce a code appropriate to the structure.• Infer from prompt: In this step, the LLM generates the intended code.We feed the prompts created in the previous step to the LLM and obtain the source code produced as an inference result.Inference can be performed using any kind of LLM, including in-house, downloaded, and commercially available models.• Save generated codes: The LLM execution phase generates code using several prompts, resulting in multiple problem-prompt pairs, and several inference iterations are performed per pair.We store the results after each inference to avoid future re-executions and reuse prior work.The stored source code instances are augmented with metadata including the corresponding coding problem, prompt details, and target programming language, facilitating future performance analysis.When storing, outliers are filtered.</p>
<p>(3) Generated code analysis This stage comprises two steps: (i) running the generated codes with test cases and (ii) analyzing the results according to the metrics.</p>
<p>• Run generated codes with test cases: To verify the functional correctness of the generated code, it should be executed for test cases.LeetCode provides a test execution environment.After submitting the source code on the platform and activating the test process, it shows the test results, which include the total number of test cases executed, count of successful cases, and detailed insights on runtime and memory consumption.Our implementation automates these repetitive processes by using the corresponding LeetCode CLI library.• Analyze results according to metrics: Codes generated by the LLM are evaluated using predetermined metrics that reflect aspects such as accuracy, efficiency, and readability.Leveraging the information extracted alongside coding problems allows to analyze the LLM capabilities from various perspectives.For example, analyses based on criteria such as problem complexity, publication date, topic, and level of prompt detail are possible.This method not only offers insights into the LLM abilities but also identifies areas for improvement, possibly guiding future refinements.</p>
<p>| Metric pass-ratio@n</p>
<p>The pass@k metric is widely used to evaluate the effectiveness of code generated by LLMs based on the execution results.However, this metric considers the code to be either completely correct or incorrect using a binary pass/fail criterion.Consequently, varying degrees of correctness cannot be considered.Consider a simple example involving a coding problem with 10 test cases.Let us focus on the pass@1 metric, which is the simplest form of pass@k.For pass@1, an LLM-generated code solution receives a score of 1 only if it passed all 10 tests.If it fails in the 10 test cases, the score is 0.Even if a code passes 9 out of the 10 test cases, the score is still 0, as if it had passed none.This binary scoring system fails to acknowledge for partial success in the test cases.This limitation remains even when k &gt; 1.Consequently, pass@k lacks the granularity to distinguish nearly correct from entirely incorrect solutions.This scoring limitation emphasizes the need for more refined metrics to evaluate LLM code generation.Thus, we propose a new metric, pass-ratio@n.The proposed pass-ratio is the proportion of passed tests.Considering the probabilistic nature of LLMs, which may not produce identical code solutions across inferences, we perform n inferences.The average pass ratio value across the n solutions is then used to calculate the score, thereby mitigating the bias from a single inference.</p>
<p>For a given solution i (0 &lt; i ≤ n), pass-ratio i is calculated by squaring the ratio obtained by dividing the number of passed test cases for code i by the total number of test cases.By squaring the value, solutions with higher pass ratios are assigned more weight, thus reflecting a higher degree of accuracy.</p>
<p>pass-ratio i ¼</p>
<p>the number of passed test cases at code i the number of test cases 2 :</p>
<p>The value of pass-ratio@n represents the average passratio across n generated codes:</p>
<p>pass-ratio@n ¼ P n i¼1 pass-ratio i n :</p>
<p>We analyzed the proposed framework to ensure that the LLM evaluation was fully automatic and effective.This analysis focused on the feasibility of the proposed framework and metrics rather than on an exhaustive analysis of the LLM performance.We chose GPT-3.5 and GPT-4 as the LLMs to be evaluated and integrated them with the commercial application programming interface (API) provided by OpenAI to access their LLMs.We conducted analyses to evaluate the LLM prompt details, problem publication date, and proposed metric.</p>
<p>| Analysis of prompt details</p>
<p>We first explored the impact of the prompt details on code generation.We assumed that more details increased the accuracy of code generation.To confirm this hypothesis, we devised three types of prompts with varying levels of detail: The problem type thus ranged from the least (i) to the most (iii) detailed.For the analysis, we randomly selected 10 coding problems from LeetCode.The three types of prompts were created per coding problem, thus evaluating 30 queries (10 problems Â 3 queries).We generated source codes for the 30 queries using GPT-3.5 and measured the code accuracy using LeetCode.Table 2 lists the results of whether the generated codes passed the tests.In this analysis, the source code was generated once by a single inference per prompt.</p>
<p>Table 2 shows that the codes generated using prompts of types 1 and 2 solved 6 and 7 out of the 10 problems, respectively, while those of type 3 solved 8, indicating the best performance among the three types of prompts.</p>
<p>For a deeper analysis, we present a description of coding problem 4 in Table 2.This problem was retrieved from LeetCode.</p>
<p>(Source https://leetcode.com/problems/maximum-matching-of-players-with-trainers/).</p>
<p>Description of coding problem 4</p>
<p>Title: Maximum matching of players with trainers You are given a 0-indexed integer array players, where players[i] represents the ability of the ith player.You are also given a 0-indexed integer array trainers, where trainers [j] represents the training capacity of the jth trainer.</p>
<p>The ith player can match with the jth trainer if the player's ability is less than or equal to the trainer's training capacity.Additionally, the ith player can be matched with at most one trainer, and the jth trainer can be matched with at most one player.</p>
<p>Return the maximum number of matchings between players and trainers that satisfy these conditions.</p>
<p>For problem 4, the source code generated by GPT-3.5 using the three types of prompts is shown in Figure 2.Among the three prompts, only the code generated with the type 3 prompt passed all the test cases.The code generated with the type 2 prompt was implemented similarly to that generated with type 3.However, the code from the type 2 prompt failed to pass the test case with input (players= [1,1000000000], trainers=[1000000000,1]).Although the expected output for the test case was 2, its output was 1.This originated from the fourth line of the code, in which an incorrect parameter value was used.Changing the code from 'trainers.sort(reverse=True)' to 'trainers.sort()' corrected the error.While this analysis was based on a small sample of 10 coding problems, our initial observations suggest that there is a correlation between the level of detail in the prompt and accuracy of the source code generated by the LLM.</p>
<p>| Analysis of publication date</p>
<p>We also designed an experiment to evaluate the source code generation ability of the LLM according to the publication year of the coding problem.Coding problems published before the LLM training date are probably easy for the model to solve, but those created after model training may be more difficult.</p>
<p>To investigate this assumption, we organized the coding problems according to the publication date.Considering that GPT-3.5 was initially released on March 15, 2022, we chose coding problems from 2023, 2022, and up to 2021 for evaluation.We also employed an additional classification based on the difficulty levels of easy, medium, and hard.For each year and difficulty level, we randomly selected 10 coding problems and conducted five inferences per problem using GPT-3.5, resulting in an examination of five generated code solutions per problem.</p>
<p>Table 3 lists the number of coding problems successfully solved (i.e., every test case in a problem passed) out of a total of 10 problems per group.</p>
<p>Table 3 shows that GPT-3.5 performs well on coding problems published until 2021, solving eight easy, four medium-difficulty, and three hard problems.However, its performance declines when confronted with problems from 2022 and 2023.For 2022 and 2023, some easy and medium-difficulty problems were solved, but no difficult problem was solved.</p>
<p>While GPT demonstrated proficiency in solving problems up to 2021, it encountered difficulties with problems that emerged after its training date.The decline in performance for problems from 2022 and 2023 may be attributed to the introduction of new contexts or topics not included in the training data.However, a true understanding of natural language specifications requires the ability to handle evolving problems.This shows the fundamental difference between human cognition and current LLMs.Humans can infer, extrapolate, and make educated guesses about unfamiliar topics based on prior knowledge and experience.Although models like GPT-3.5 can attempt similar tasks using patterns extracted from their vast datasets, they are inherently limited by existing knowledge.Consequently, they cannot genuinely understand or reason for completely new topics or contexts without sufficient reference in their training data.This underscores the importance of iterative training and model updates in artificial intelligence systems.</p>
<p>| Analysis of pass-ratio@n metric</p>
<p>We first evaluated the pass-ratio@n calculation using three example coding problems in comparison with pass@k.We generated five coding solutions per problem using LLM inference.Table 4 lists the number of passed test cases and total number of test cases per LLMgenerated code solution.Given that five solutions were generated, we set n to 5 for the pass-ratio@n metric.Similarly, we configured n to 5 in pass@k while varying k from 1 to 5 to facilitate a comprehensive comparison with pass-ratio@5.</p>
<p>Table 4 lists the pass@k (k = 1-5, n = 5) and passratio@n (n = 5) scores.For the first coding problem, the five generated solutions passed every test case, resulting in pass@k (k = 1-5, n = 5) and pass-ratio@5 with values of 100%.For the second coding problem, the solutions for problems 2, 3, and 5 passed all the tests, while those for problems 1 and 4 failed to pass some of the 135 test cases, ultimately receiving a fail classification.As three of the five solutions passed, the pass@1 score was 60%.For pass@5, because at least one of the five solutions passed, the result was 100%.The pass-ratio@5, calculated as the average of the squared pass rates of each solution, was 78%.In the third coding problem, the five solutions failed, resulting in pass@k (k = 1-5, n = 5) of 0%.However, the generated solutions passed a high portion of the 61 test cases, suggesting that they were effective.Only pass-ratio@5 reflected this phenomenon.In fact, passratio@5 was 61% for the third problem, reflecting the test pass rate.</p>
<p>We then integrated our framework with the LeetCode platform to examine the applicability of the pass-ratio@n metric.This analysis involved calculating and comparing pass@1, pass@5, and pass-ratio@5 for GPT-3.5 considering the publication date and difficulty of the coding problems.The n value for pass-ratio@n was set to 5, which implied five inference attempts per problem.</p>
<p>For each category, 10 coding problems were randomly chosen based on the publication year and difficulty level, resulting in 50 inferences per category.For the three difficulty levels (easy, medium, and hard), this number was multiplied by 3, resulting in 150 inferences.Finally, the publication years up to 2021, 2022, and 2023, multiplied the required inferences by 3, resulting in a total of 450 inferences.This task was labor-intensive, but the proposed framework automated the repetitive processes.Table 5 lists the pass@1, pass@5, and pass-ratio@5 scores for GPT-3.5 across 10 randomly selected problems in each category.For the easy problems, all the metrics showed high scores exceeding 60%.For the medium difficulty, the scores drastically dropped for problems published after 2022.This trend was more pronounced for the hard problems.Notably, the pass@1 and pass@5 metrics recorded scores of 0 in 2022 and 2023, indicating that no problems were solved.Nevertheless, as inferred from the values of the proposed pass-ratio@5, GPT-3.5 managed to pass some of the test cases, indicating certain problem-solving ability.In contrast, the pass@k metrics failed to capture the potential problem-solving ability of the model.</p>
<p>Finally, we extended our evaluation to include GPT-4 by applying the same method to calculate pass-ratio@5 as for GPT-3.5. Figure 3 shows the pass-ratio@5 scores for the GPT-3.5 and GPT-4 LLMs.Both models performed relatively well for problems dating up to 2021.For such problems, a pass-ratio@5 value of over 70% indicated a commendable problem-solving ability.However, pass-ratio@5 dropped to less than half for problems after 2021, and this decline was more pronounced for hard problems.GPT-4 generally exhibited a higher passratio@5 than GPT-3.5.When analyzing the trends of these values across the different categories for each model, similar trends were observed for GPT-3.5 and GPT-4.</p>
<p>T A B L E 4 Comparison of pass@k (k = 1-5, n = 5) and pass-ratio@5 for three coding problems.pass-ratio@5 100% 78% 61%</p>
<p>Coding</p>
<p>T A B L E 5 Results of pass@1, pass@5, and pass-ratio@5 using GPT-3.5 for 10 coding problems in various categories.</p>
<p>Difficulty Year</p>
<p>pass@1 pass@5 pass-ratio@5 level (n = 5) (n = 5) F I G U R E 3 Results of pass-ratio@5 for GPT-3.5 and GPT-4 according to publication year and difficulty level.DISCUSSION</p>
<p>Easy</p>
<p>For the framework evaluation in Section 5, we selected 10 coding problems from different publication years and difficulty levels.Although we performed 450 inferences across three years, three difficulty levels, and five repetitions, the number of inferences was insufficient to generalize the performance of the LLM.To address this limitation, future studies should incorporate a more extensive group of coding problems.Nevertheless, the proposed framework seems to provide efficiency with time and cost savings by automating the evaluation of LLMs.</p>
<p>The performance of the proposed pass-ratio@n method may be affected by the value of n.When n ¼ 1, the performance evaluation is on a single inference.The pass-ratio@1 value is adequate under low randomness (for example, low temperature in an LLM) because it often yields identical code solutions for the same input.However, under higher randomness, this approach may not be reliable because of the increased variability in the generated codes.In such cases, increasing n can improve the generalization of the evaluation, although this requires multiple inferences and executes the generated code solutions for all the test data, which can be resource-intensive.</p>
<p>Using the pass-ratio@n metric, the LLM ability for code generation is evaluated based on the pass rate of test cases.Therefore, the effectiveness of this metric is influenced by the quality of the test cases.This aspect becomes important because the number and complexity of test cases vary across coding problems.Thus, the adopted real-world coding platforms should be reputable to ensure credibility of the findings.</p>
<p>Our framework evaluates LLMs and can use coding platforms, such as LeetCode.However, this approach requires integration of an API that facilitates communication with these platforms through a CLI.In addition, we must adhere to the licensing agreements of the data providers and use the problem data appropriately.</p>
<p>| CONCLUSION</p>
<p>We introduce a framework for systematically evaluating the code generation ability of LLMs.We first analyze essential factors to consider in dataset selection and determine that coding platforms such as LeetCode can adequately cover such factors.The proposed framework is intended to be fully automated to manage the repetitive processes involved in generating queries, conducting inferences, and executing the generated codes.In addition, a new metric, pass-ratio@n, is introduced to measure accuracy with granularity by considering the test pass rate.</p>
<p>Our preliminary experimental results indicated that the prompt details affected the quality of the generated source code, and the targeted model was less effective at solving coding problems published after the date of LLM training.In addition, the pass-ratio@n metric could successfully measure the closeness of the generated code to being complete and functional, representing a nuanced and useful way to assess the performance of LLMs in code generation.This study was aimed solely to confirm the applicability of the proposed framework using a small number of problems and generated code.To generalize our findings, a systematic analysis with a larger dataset is necessary.Future studies will involve refining the framework and conducting a comprehensive evaluation of various LLMs using the proposed framework and metric.</p>
<p>• (Type 1 )
1
Basic problem description: This is a simple straightforward description of the coding problem.• (Type 2) Problem description with constraints: An extended version of the type 1 description including constraints imposed to the solution.• (Type 3) Problem description with constraints and examples: This is the most detailed type of query and encompasses both constraints and illustrative examples.</p>
<p>T A B L E 2
2
Pass/fail results of source codes generated by GPT-3.5 for 10 LeetCode coding problems.U R E 2 Comparison of generated codes according to prompt details.</p>
<p>T A B L E 3
3
Solved problems out of 10 randomly selected coding problems categorized by publication year and difficulty level using GPT-3.5 with five inferences per problem.</p>
<p>They cover input ranges, sizes, or other restrictions.listed in Table 1 by leveraging third-party open-source command line interface (CLI) libraries for LeetCode.
Difficulty levelCoding complexity of a problem. Manyplatforms, including LeetCode,categorize problems into three difficultylevels: easy, medium, and hard.TopicsSpecific category or area of algorithms, datastructures, and programming concepts.This categorization enables analysisbased on specific domains.PublicationPublication date of a coding problem. Usingdateproblems published after trainingconfirms that the evaluated LLM doesnot have pre-existing knowledge.
SignatureStructure of a function, encompassing the function name, parameter types, and return types.information</p>
<p>YEO ET AL.
YEO ET AL.
YEO ET AL.
ORCIDYu-Seung Ma https://orcid.org/0000-0002-4168-5515Sang Cheol Kim https://orcid.org/0000-0002-1925-2588Taeho Kim https://orcid.org/0000-0002-5061-206XThis work was supported by an Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant (2022-0-00995, automated reliable source code generation from natural language descriptions, 95%) and a National Research Council of Science &amp; Technology (NST) grant (Global-23-001, SeCode: Collaborative intelligent model for secure program code generator, 5%) funded by the Korea government (MSIT).CONFLICT OF INTEREST STATEMENTThe authors declare that there are no conflicts of interest.
Comparative study of text representation and learning for persian named entity recognition. M M Abdollah Pour, S Momtazi, info:doi/10.4218/etrij.2021-0269ETRI J. 4452022</p>
<p>Simple and effective neural coreference resolution for korean language. C Park, J Lim, J Ryu, H Kim, C Lee, info:doi/10.4218/etrij.2020-0282ETRI J. 4362021</p>
<p>Automatic extraction of similar poetry for study of literary texts: An experiment on hindi poetry. A Prakash, N K Singh, S K Saha, info:doi/10.4218/etrij.2019-0396ETRI J. 4432022</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, info:doi/10.48550/arXiv.2107.033742021arXiv preprintEvaluating large language models trained on code</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A D Lago, T Hubert, P Choy, C De Masson D'autume, I Babuschkin, X Chen, P.-S Huang, J Welbl, S Gowal, A Cherepanov, J Molloy, D J Mankowitz, E S Robson, P Kohli, N Freitas, K Kavukcuoglu, O Vinyals, Sci. 37866242022</p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. E Nijkamp, B Pang, H Hayashi, L Tu, H Wang, Y Zhou, S Savarese, C Xiong, International Conference on Learning Representations). 2022252668917</p>
<p>BLEU: A method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, 2002Association for Computational LinguisticsPhiladelphia, PA, USA</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. S Ren, D Guo, S Lu, L Zhou, S Liu, D Tang, N Sundaresan, M Zhou, A Blanco, S Ma, info:doi/10.48550/arXiv.2009.102972020arXiv preprint</p>
<p>S Kulal, P Pasupat, K Chandra, M Lee, O Padon, A Aiken, P S Liang, SPoC: Search-based Pseudocode to Code, Proceedings of the 33rd International Conference on Neural Information Processing. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Investigating code generation performance of chatgpt with crowdsourcing social data. Y Feng, S Vanam, M Cherukupally, W Zheng, M Qiu, H Chen, IEEE 47th Annual Computers, Software, and Applications conference (COMPSAC). Torino, Italy2023</p>
<p>An empirical evaluation of github copilot's code suggestions. N Nguyen, S Nadi, IEEE/ACM 19th International Conference on Mining Software Repositories (MSR). Pittsburgh, PA, USA2022</p>
<p>Assessing the quality of github copilot's code generation. B Yetistiren, I Ozsoy, E Tuzun, Proc. of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering. of the 18th International Conference on Predictive Models and Data Analytics in Software EngineeringSingapore2022</p>
<p>Smartmark: Software watermarking scheme for smart contracts. T Kim, Y Jang, C Lee, H Koo, H Kim, IEEE/ACM 45th International Conference on Software Engineering (ICSE). Melbourne, Australia2023</p>
<p>J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, C Sutton, info:doi/10.48550/arXiv.2108.07732Program synthesis with large language models. 2021arXiv preprint</p>
<p>Code-GeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-XX. Q Zheng, X Xia, X Zou, Y Dong, S Wang, Y Xue, Z Wang, L Shen, A Wang, Y Li, T Su, Z Yang, J Tang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data MiningLong Beach, CA, USA2023</p>
<p>Multilingual evaluation of code generation models. B Athiwaratkun, S K Gouda, Z Wang, X Li, Y Tian, M Tan, W U Ahmad, S Wang, Q Sun, M Shang, The Eleventh International Conference on Learning Representations). 2023</p>
<p>Code llama: Open foundation models for code. B Roziére, 2023</p>
<p>Measuring coding challenge competence with apps. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, J Steinhardt, Proc. of the Neural Information Processing Systems Track on Datasets and Benchmarks). of the Neural Information essing Systems Track on Datasets and Benchmarks)2021</p>
<p>X.-Y Li, J.-T Xue, Z Xie, M Li, info:doi/10.48550/arXiv.2305.10679Think outside the code: Brainstorming boosts large language models in code generation. 2023arXiv preprint</p>
<p>OpenAI, Gpt-4 technical report. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
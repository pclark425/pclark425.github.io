<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-811 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-811</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-811</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-274165791</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.13768v2.pdf" target="_blank">Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have enabled the emergence of LLM agents: autonomous systems capable of achieving under-specified goals and adapting post-deployment, often without explicit code or model changes. Evaluating these agents is critical to ensuring their performance and safety, especially given their dynamic, probabilistic, and evolving nature. However, traditional approaches such as predefined test cases and standard redevelopment pipelines struggle to address the unique challenges of LLM agent evaluation. These challenges include capturing open-ended behaviors, handling emergent outcomes, and enabling continuous adaptation over the agent's lifecycle. To address these issues, we propose an evaluation-driven development approach, inspired by test-driven and behavior-driven development but reimagined for the unique characteristics of LLM agents. Through a multivocal literature review (MLR), we synthesize the limitations of existing LLM evaluation methods and introduce a novel process model and reference architecture tailored for evaluation-driven development of LLM agents. Our approach integrates online (runtime) and offline (redevelopment) evaluations, enabling adaptive runtime adjustments and systematic iterative refinement of pipelines, artifacts, system architecture, and LLMs themselves. By continuously incorporating evaluation results, including fine-grained feedback from human and AI evaluators, into each stage of development and operation, this framework ensures that LLM agents remain aligned with evolving goals, user needs, and governance standards.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e811.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e811.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-System Evaluation Gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gap between Model-level (QA) and System-level (Interactive/Procedural) Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed tendency for literature and benchmarks to evaluate LLMs on static QA/benchmark tasks while under-evaluating system-level interactive behaviors (tool use, planning, multi-step workflows), producing a performance/diagnostic gap that hides failures in deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Observed missing coverage of out-of-LLM components such as planning modules, tool interfaces, retrieval pipelines, memory, and guardrails</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>process / evaluation practice</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Paper proposes lifecycle-spanning evaluation-driven development (EDD) and reference architecture embedding online/offline evaluation, intermediate-pipeline evaluation, and AgentOps observability to reduce the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Over-reliance on static benchmarks and aggregated metrics, predominant model-level evaluations, lack of adaptive/online testing, and insufficient evaluation of intermediate pipelines and tool integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e811.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EDD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation-Driven Development (EDD) for LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A process-model introduced in this paper that embeds continuous, adaptive evaluation across the entire lifecycle of LLM agents to drive runtime adaptations and offline redevelopment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Integrates AgentOps Infrastructure, observability, evaluation component, feedback loops, dynamic test/safety case generation</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>process / architectural</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Procedural embedding of continuous offline and online evaluation, stepwise test-case generation, dynamic escalation to human evaluators, and feedback pipelines that feed runtime logs into redevelopment cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of lifecycle integration in evaluation practices; EDD is proposed to close the diagnostic and performance gap by making evaluation formative rather than only summative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e811.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer²</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer² (Self-adaptive Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced architectural intervention that enables dynamic, task-specific weight adjustments at inference time to allow real-time adaptation without full retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer variant enabling dynamic per-task weight modulation during inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Dynamic weight adjustment at inference (task-conditioned adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Enables dynamic, task-specific weight adjustments during inference so agents can adapt behavior online without retraining the base model.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Described qualitatively in paper as enabling real-time adaptation; no quantitative metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e811.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SASA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-disciplined Autoregressive Sampling (SASA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced inference-time intervention that applies self-evaluation heuristics to steer token sampling away from undesirable outputs, improving response quality and safety.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling/self-evaluation mechanism applied during autoregressive decoding</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Self-evaluation steering applied during token sampling (inference-level control)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>inference / sampling strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Uses internal contextual representations to evaluate candidate continuations and bias sampling away from unsafe or low-quality tokens, serving as a runtime safety/quality mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Qualitative improvement in steering outputs away from undesirable behavior claimed; no numeric before/after provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e811.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system-level component combining retrieval from external knowledge sources with generation, used in agent pipelines to improve factuality and contextual relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline integrating a retriever (knowledge base/API) with an LLM generator to ground responses</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tax Assistant retrieval tasks (case study)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Retriever + context engine + generator; retrieval guardrails, citation checking</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural / pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used in the Tax Assistant case study to retrieve ATO documentation to ground agent responses; the paper suggests refining retrieval guardrails and pipelines to reduce hallucination and incorrect citations.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Case-study level qualitative improvements in retrieval accuracy and citation relevance after pipeline refinements; no numeric metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Incorrect or low-quality retrieval leading to poor downstream interactive performance despite high standalone model QA scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e811.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training method referenced as a way to fine-tune LLMs to align outputs with human preferences and safety goals during redevelopment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning fine-tuning guided by human preference labels</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Suggested as an offline redevelopment approach when system-level evaluations indicate deficiencies traceable to the LLM; used to align model behavior with desired outcomes and safety constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e811.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guardrails</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-layered Guardrails</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectural safety and correctness enforcement mechanisms layered across the agent to detect and block unsafe or incorrect actions and triggers for escalation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System-level guardrail components enforcing policy, safety filters, and runtime checks</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>safety / control for tool use and execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Exception handling, content filters, policy enforcement, escalation to human review</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change / safety mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used to enforce correctness and safety across reasoning, retrieval, and tool invocation; evaluation triggers flag deviations to guardrails for mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e811.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentBench: Evaluating LLMs as Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited benchmark focused on evaluating LLMs in agentic roles (multi-step tasks, tool use, trajectory-based assessments) referenced as an example of agent-focused evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AgentBench: Evaluating llms as agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>AgentBench (suite of agentic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step reasoning / tool use / trajectory-based</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Benchmark framework for measuring agent behaviors across multi-step tasks and tool use; cited as relevant to system-level evaluation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e811.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlanBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlanBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited benchmark for evaluating planning and reasoning about change, focused on tasks that exercise multi-step planning capabilities of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>PlanBench (planning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Extensible planning benchmark cited as addressing multi-step reasoning evaluation needs for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e811.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StreamBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StreamBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited preprint proposing benchmarks for continuous improvement of language agents, relevant to online/continuous evaluation of agents rather than static QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Streambench: Towards benchmarking continuous improvement of language agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>StreamBench (continuous agent evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>continuous evaluation / online adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>benchmark / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Benchmarking approach emphasizing in-situ, continuous assessment of agent behaviors over time to capture evolving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e811.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e811.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-Learning Survey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool learning with large language models: A survey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited survey that summarizes architectures, methods and challenges for enabling LLMs to learn and use external tools — directly relevant to interactive/tool-use performance gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tool learning with large language models: A survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>tool-use interfaces, API wrappers, tool-selection policies, execution orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>survey / taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Summarizes architectural patterns and training/engineering practices for tool use, highlighting gap areas where QA-oriented training does not suffice for robust tool-driven behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AgentBench: Evaluating llms as agents <em>(Rating: 2)</em></li>
                <li>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change <em>(Rating: 2)</em></li>
                <li>Streambench: Towards benchmarking continuous improvement of language agents <em>(Rating: 2)</em></li>
                <li>Tool learning with large language models: A survey <em>(Rating: 2)</em></li>
                <li>Agentquest: A modular benchmark framework to measure progress and improve llm agents <em>(Rating: 1)</em></li>
                <li>Agentboard: An analytical evaluation board of multi-turn llm agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-811",
    "paper_id": "paper-274165791",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "Model-System Evaluation Gap",
            "name_full": "Gap between Model-level (QA) and System-level (Interactive/Procedural) Evaluation",
            "brief_description": "Observed tendency for literature and benchmarks to evaluate LLMs on static QA/benchmark tasks while under-evaluating system-level interactive behaviors (tool use, planning, multi-step workflows), producing a performance/diagnostic gap that hides failures in deployment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Observed missing coverage of out-of-LLM components such as planning modules, tool interfaces, retrieval pipelines, memory, and guardrails",
            "training_method": null,
            "intervention_type": "process / evaluation practice",
            "intervention_description": "Paper proposes lifecycle-spanning evaluation-driven development (EDD) and reference architecture embedding online/offline evaluation, intermediate-pipeline evaluation, and AgentOps observability to reduce the gap.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Over-reliance on static benchmarks and aggregated metrics, predominant model-level evaluations, lack of adaptive/online testing, and insufficient evaluation of intermediate pipelines and tool integrations.",
            "uuid": "e811.0",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "EDD",
            "name_full": "Evaluation-Driven Development (EDD) for LLM Agents",
            "brief_description": "A process-model introduced in this paper that embeds continuous, adaptive evaluation across the entire lifecycle of LLM agents to drive runtime adaptations and offline redevelopment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": true,
            "architectural_features": "Integrates AgentOps Infrastructure, observability, evaluation component, feedback loops, dynamic test/safety case generation",
            "training_method": null,
            "intervention_type": "process / architectural",
            "intervention_description": "Procedural embedding of continuous offline and online evaluation, stepwise test-case generation, dynamic escalation to human evaluators, and feedback pipelines that feed runtime logs into redevelopment cycles.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Lack of lifecycle integration in evaluation practices; EDD is proposed to close the diagnostic and performance gap by making evaluation formative rather than only summative.",
            "uuid": "e811.1",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Transformer²",
            "name_full": "Transformer² (Self-adaptive Transformers)",
            "brief_description": "Referenced architectural intervention that enables dynamic, task-specific weight adjustments at inference time to allow real-time adaptation without full retraining.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": "Transformer variant enabling dynamic per-task weight modulation during inference",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "Dynamic weight adjustment at inference (task-conditioned adaptation)",
            "training_method": null,
            "intervention_type": "architectural change",
            "intervention_description": "Enables dynamic, task-specific weight adjustments during inference so agents can adapt behavior online without retraining the base model.",
            "intervention_effect": "Described qualitatively in paper as enabling real-time adaptation; no quantitative metrics provided in this paper.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.2",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "SASA",
            "name_full": "Self-disciplined Autoregressive Sampling (SASA)",
            "brief_description": "Referenced inference-time intervention that applies self-evaluation heuristics to steer token sampling away from undesirable outputs, improving response quality and safety.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": "Sampling/self-evaluation mechanism applied during autoregressive decoding",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "Self-evaluation steering applied during token sampling (inference-level control)",
            "training_method": null,
            "intervention_type": "inference / sampling strategy",
            "intervention_description": "Uses internal contextual representations to evaluate candidate continuations and bias sampling away from unsafe or low-quality tokens, serving as a runtime safety/quality mechanism.",
            "intervention_effect": "Qualitative improvement in steering outputs away from undesirable behavior claimed; no numeric before/after provided in this paper.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.3",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A system-level component combining retrieval from external knowledge sources with generation, used in agent pipelines to improve factuality and contextual relevance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": "Pipeline integrating a retriever (knowledge base/API) with an LLM generator to ground responses",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tax Assistant retrieval tasks (case study)",
            "interactive_task_type": "tool use / retrieval",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "Retriever + context engine + generator; retrieval guardrails, citation checking",
            "training_method": null,
            "intervention_type": "architectural / pipeline",
            "intervention_description": "Used in the Tax Assistant case study to retrieve ATO documentation to ground agent responses; the paper suggests refining retrieval guardrails and pipelines to reduce hallucination and incorrect citations.",
            "intervention_effect": "Case-study level qualitative improvements in retrieval accuracy and citation relevance after pipeline refinements; no numeric metrics reported.",
            "hypothesized_cause_of_gap": "Incorrect or low-quality retrieval leading to poor downstream interactive performance despite high standalone model QA scores.",
            "uuid": "e811.4",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "RLHF",
            "name_full": "Reinforcement Learning from Human Feedback",
            "brief_description": "Training method referenced as a way to fine-tune LLMs to align outputs with human preferences and safety goals during redevelopment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": "Reinforcement learning fine-tuning guided by human preference labels",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": "RLHF",
            "intervention_type": "training method",
            "intervention_description": "Suggested as an offline redevelopment approach when system-level evaluations indicate deficiencies traceable to the LLM; used to align model behavior with desired outcomes and safety constraints.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.5",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Guardrails",
            "name_full": "Multi-layered Guardrails",
            "brief_description": "Architectural safety and correctness enforcement mechanisms layered across the agent to detect and block unsafe or incorrect actions and triggers for escalation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": null,
            "model_description": "System-level guardrail components enforcing policy, safety filters, and runtime checks",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "safety / control for tool use and execution",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "Exception handling, content filters, policy enforcement, escalation to human review",
            "training_method": null,
            "intervention_type": "architectural change / safety mechanism",
            "intervention_description": "Used to enforce correctness and safety across reasoning, retrieval, and tool invocation; evaluation triggers flag deviations to guardrails for mitigation.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.6",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "AgentBench",
            "name_full": "AgentBench: Evaluating LLMs as Agents",
            "brief_description": "A cited benchmark focused on evaluating LLMs in agentic roles (multi-step tasks, tool use, trajectory-based assessments) referenced as an example of agent-focused evaluation.",
            "citation_title": "AgentBench: Evaluating llms as agents",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "AgentBench (suite of agentic tasks)",
            "interactive_task_type": "multi-step reasoning / tool use / trajectory-based",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "benchmark",
            "intervention_description": "Benchmark framework for measuring agent behaviors across multi-step tasks and tool use; cited as relevant to system-level evaluation needs.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.7",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "PlanBench",
            "name_full": "PlanBench",
            "brief_description": "Cited benchmark for evaluating planning and reasoning about change, focused on tasks that exercise multi-step planning capabilities of LLMs.",
            "citation_title": "Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "PlanBench (planning tasks)",
            "interactive_task_type": "planning / multi-step reasoning",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "benchmark",
            "intervention_description": "Extensible planning benchmark cited as addressing multi-step reasoning evaluation needs for agents.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.8",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "StreamBench",
            "name_full": "StreamBench",
            "brief_description": "Cited preprint proposing benchmarks for continuous improvement of language agents, relevant to online/continuous evaluation of agents rather than static QA.",
            "citation_title": "Streambench: Towards benchmarking continuous improvement of language agents",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "StreamBench (continuous agent evaluation)",
            "interactive_task_type": "continuous evaluation / online adaptation",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": "benchmark / evaluation methodology",
            "intervention_description": "Benchmarking approach emphasizing in-situ, continuous assessment of agent behaviors over time to capture evolving performance.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.9",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Tool-Learning Survey",
            "name_full": "Tool learning with large language models: A survey",
            "brief_description": "A cited survey that summarizes architectures, methods and challenges for enabling LLMs to learn and use external tools — directly relevant to interactive/tool-use performance gaps.",
            "citation_title": "Tool learning with large language models: A survey",
            "mention_or_use": "mention",
            "model_or_agent_name": null,
            "model_description": null,
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "tool use",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": null,
            "performance_gap_observed": null,
            "architectural_features": "tool-use interfaces, API wrappers, tool-selection policies, execution orchestration",
            "training_method": null,
            "intervention_type": "survey / taxonomy",
            "intervention_description": "Summarizes architectural patterns and training/engineering practices for tool use, highlighting gap areas where QA-oriented training does not suffice for robust tool-driven behaviour.",
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e811.10",
            "source_info": {
                "paper_title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AgentBench: Evaluating llms as agents",
            "rating": 2,
            "sanitized_title": "agentbench_evaluating_llms_as_agents"
        },
        {
            "paper_title": "Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
            "rating": 2,
            "sanitized_title": "planbench_an_extensible_benchmark_for_evaluating_large_language_models_on_planning_and_reasoning_about_change"
        },
        {
            "paper_title": "Streambench: Towards benchmarking continuous improvement of language agents",
            "rating": 2,
            "sanitized_title": "streambench_towards_benchmarking_continuous_improvement_of_language_agents"
        },
        {
            "paper_title": "Tool learning with large language models: A survey",
            "rating": 2,
            "sanitized_title": "tool_learning_with_large_language_models_a_survey"
        },
        {
            "paper_title": "Agentquest: A modular benchmark framework to measure progress and improve llm agents",
            "rating": 1,
            "sanitized_title": "agentquest_a_modular_benchmark_framework_to_measure_progress_and_improve_llm_agents"
        },
        {
            "paper_title": "Agentboard: An analytical evaluation board of multi-turn llm agents",
            "rating": 1,
            "sanitized_title": "agentboard_an_analytical_evaluation_board_of_multiturn_llm_agents"
        }
    ],
    "cost": 0.015223500000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EVALUATION-DRIVEN DEVELOPMENT OF LLM AGENTS: A PROCESS MODEL AND REFERENCE ARCHITECTURE
27 Mar 2025</p>
<p>Boming Xia 
CSIRO's Data61</p>
<p>University of New South Wales
‡ Australian National University</p>
<p>Qinghua Lu 
CSIRO's Data61</p>
<p>University of New South Wales
‡ Australian National University</p>
<p>Liming Zhu 
CSIRO's Data61</p>
<p>University of New South Wales
‡ Australian National University</p>
<p>Zhenchang Xing 
CSIRO's Data61</p>
<p>Dehai Zhao 
CSIRO's Data61</p>
<p>Hao Zhang 
CSIRO's Data61</p>
<p>EVALUATION-DRIVEN DEVELOPMENT OF LLM AGENTS: A PROCESS MODEL AND REFERENCE ARCHITECTURE
27 Mar 20258CA6552B1ADE35839BE0E2B20600D579arXiv:2411.13768v2[cs.SE]Large Language ModelLLMAgentEvaluationArchitectureAI SafetyResponsible AI
Large Language Models (LLMs) have enabled the emergence of LLM agents-autonomous systems capable of achieving under-specified goals and adapting post-deployment, often without explicit code or model changes.Evaluating these agents is critical to ensuring their performance and safety, especially given their dynamic, probabilistic, and evolving nature.However, traditional approaches, such as predefined test cases and standard redevelopment pipelines, struggle to address the unique challenges of LLM agent evaluation.These challenges include capturing open-ended behaviors, handling emergent outcomes, and enabling continuous adaptation over the agent's lifecycle.To address these issues, we propose an evaluation-driven development approach, inspired by test-driven and behavior-driven development but reimagined for the unique characteristics of LLM agents.Through a multivocal literature review (MLR), we synthesize the limitations of existing LLM agent evaluation methods and introduce a novel process model and reference architecture tailored for evaluation-driven development of LLM agents.Our approach integrates online (runtime) and offline ((re)development) evaluations, enabling adaptive runtime adjustments and systematic iterative refinement of pipelines, artifacts, system architecture, and LLMs themselves.By continuously incorporating evaluation results-including fine-grained feedback from human and AI evaluators-into each stage of development and operation, this framework ensures that LLM agents remain aligned with evolving goals, user needs, and governance standards.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are large-scale, pretrained language models with tens of billions of parameters, adaptable to a wide range of downstream tasks, including question answering, summarization, and reasoning [1].Recent advancements in LLMs have led to the emergence of LLM-based agents, commonly known as LLM agents, which autonomously perform complex tasks (e.g., the AI Scientist [2]).Unlike traditional AI/LLM systems that rely on predefined inputs and structured task execution, LLM agents dynamically interpret high-level, under-specified goals.They perceive context, reason, plan, and execute workflows, often integrating external tools, knowledge bases, and other agents to extend their capabilities [3].</p>
<p>Despite their transformative potential, LLM agents introduce significant concerns regarding performance and safety [4].Their open-ended reasoning, planning and execution processes can produce unpredictable, inconsistent, or even unsafe behaviors, particularly when navigating ambiguous instructions or novel scenarios.Moreover, their autonomy-such as selecting tools, adjusting workflows, and adapting responses-raises governance challenges, as these agents may take actions misaligned with human intent or regulatory constraints.Thus, rigorous, continuous evaluation is essential to ensure that LLM agents operate as expected and evolve safely over time.</p>
<p>While increasing research attention has been devoted to benchmarking and testing frameworks for LLMs and LLM agents (e.g., [5,6]), existing approaches largely fail to address the full scope of LLM agent evaluation.Classical software engineering methods-such as Test-Driven Development (TDD) and Behavior-Driven Development (BDD)-have proven effective for deterministic systems with clear, predefined specifications.However, these methods are not inherently suited to LLM agents, which operate in open-ended, adaptive contexts and continuously adjust their behavior based on real-world interactions.The effective evaluation of LLM agents requires the following considerations:</p>
<p>• Evaluating LLM Agents at the System Level.Current evaluation frameworks predominantly assess LLMs at the model level, focusing on such as prompt-response correctness or benchmark-based scores.However, LLM agents are compound AI systems that include not only an LLM but also out-of-LLM components [7], such as context engines, planning modules, memory, and guardrails [8].While tools and frameworks like Inspect.AI 1 and DeepEval2 provide partial support for system-level evaluation, they often focus narrowly on specific interactions, such as Retrieval Augmented Generation (RAG) or tool invocations, without addressing the full runtime scope.Effective system-level evaluation must encompass both agent pipelines (including prompts, intermediate results, and final results) and artifacts (including goals, memory, reasoning, plans, workflows, tools, knowledge bases, other agents, LLMs, and guardrails) [9,8].Without such a systemwide evaluation approach, significant failure modes-such as flawed reasoning, incorrect tool usage, or unintended side effects-may remain undetected, undermining the reliability and safety of LLM agents in deployment.• Embedding Evaluation as a Continuous, Adaptive Process.Unlike traditional software systems, where evaluation is typically confined to predefined testing phases, LLM agents require continuous, dynamic evaluation throughout their lifecycle.These agents continuously learn and evolve (e.g., integrate new knowledge) based on real-world interactions.However, existing evaluation methodologies-such as static benchmarks and predefined test cases-fail to account for this evolving nature of LLM agents [10,11].By incorporating real-time monitoring, retrospective analysis, and structured feedback loops, evaluation can shift from being a diagnostic tool to a proactive mechanism that guides adaptive refinements.This ensures that evaluation not only detects performance deviations but also informs how LLM agents should adjust their strategies to maintain reliability and alignment with evolving objectives.• Closing the Loop: Using Evaluation Results to Drive Adaptation.In conventional AI/LLM evaluations, performance issues typically trigger model retraining, fine-tuning, or prompt adjustments.However, for LLM agents, evaluation results must inform a broader range of adaptive changes, including architectural refinements, pipeline/artifact adjustments, and test/safety case updates [12].The absence of structured feedback integration mechanisms leads to fragmented and ad-hoc adaptations, limiting long-term agent evolution.Ensuring traceability-where test cases, failures, and corrective actions are systematically recorded and revisited-is essential for translating evaluation insights into meaningful system improvements.By embedding structured feedback pipelines, LLM agents can iteratively evolve to enhance decision-making, align with governance requirements, and proactively mitigate emerging risks.</p>
<p>To address these challenges, this paper introduces an evaluation-driven approach that builds on the iterative principles of TDD and BDD while adapting to the unique demands of LLM agents.TDD emphasizes immediate feedback at each development step, and BDD promotes shared understanding of requirements among stakeholders.Similarly, our approach embeds continuous feedback loops throughout the entire lifecycle of LLM agents.However, it extends beyond the pre-defined, pre-deployment focus of TDD and BDD to address the non-deterministic behaviors and post-deployment evolution inherent to LLM systems.This ensures that evaluation remains dynamic and central to both development and operational phases, enabling continuous refinement and alignment with evolving objectives and real-world contexts.</p>
<p>Our method embeds continuous evaluation across the LLM agent lifecycle, enabling proactive improvement and adaptability.We systematically integrate existing evaluation practices and architectural considerations, guided by the following research questions (RQs):</p>
<ol>
<li>
<p>RQ1: What are the key challenges in evaluating LLM agents across their lifecycle?To answer this, we conduct a systematic MLR of the current evaluation landscape for LLM agents.Our review examines critical evaluation dimensions-including the contexts, methodological approaches, and architectural considerations-to identify limitations in existing evaluation practices.We particularly emphasize the need for structured, continuous evaluation mechanisms that extend beyond pre-deployment benchmarking.2. RQ2: How can evaluation be procedurally integrated into the development and operation of LLM agents?Building on insights from RQ1, we propose a structured process model that systematically embeds evaluation throughout the LLM agent lifecycle.This model establishes procedural mechanisms for continuous evaluation, enabling real-time and iterative validation, and targeted refinements both during runtime adaptation and system redevelopment.</p>
</li>
<li>
<p>RQ3: How can evaluation be architecturally integrated into the development and operation of LLM agents?Informed by both RQ1 and RQ2, we develop a reference architecture that embeds evaluation structurally within the agent's design.This architecture provides the foundational infrastructure required to support both online and offline evaluations, ensuring dynamic adaptation, proactive risk mitigation, and evidence-driven improvements to the agent's reasoning, workflows, and interactions.</p>
</li>
</ol>
<p>The progression from systematically analyzing existing practices (RQ1), to proposing an integrative process model (RQ2), and finally to designing a supporting reference architecture (RQ3) ensures a cohesive, lifecycle-spanning framework for evaluation-driven LLM agent design and operation.</p>
<p>The remainder of this paper is organized as follows: Section II provides background and discusses related work.Section III outlines the methodological details.Section IV presents the MLR results and key findings that motivate the development of the process model and reference architecture.Section V introduces a software process model for guiding LLM agent evaluation, while Section VI proposes a reference architecture for evaluation-driven development of LLM agents.Section VII evaluates the paper's three main contributions.Finally, Section VIII concludes the paper and outlines directions for future work.</p>
<p>2 Background and Related Work</p>
<p>Importance of Evaluating LLM Agents</p>
<p>Evaluation plays a critical role in ensuring the performance and safety of LLM agents across different deployment contexts.LLM agents require multi-faceted evaluation strategies that extend beyond model accuracy to encompass various factors like reasoning and planning quality, system-level interactions, and dynamic adaptation [13].Evaluation serves several key functions: verifying accuracy and functional correctness [14], examining quality and risk attributes such as fairness [15,16], and conducting capability evaluations to identify both intended behaviors and potential unintended consequences [17].Additionally, continuous evaluation enables adaptive improvements and risk control, allowing agents to refine their behavior over time [18].</p>
<p>Despite its importance, existing evaluation practices struggle to meet the demands of LLM agents due to their complex system architecture and adaptive operational behavior.Unlike conventional AI models and systems, LLM agents integrate multiple components-such as context engines, planning modules, memory, and guardrails-each of which contributes to overall performance.These interdependencies introduce new challenges in designing comprehensive evaluation methodologies.</p>
<p>Test-Driven, Behavior-Driven, and Evaluation-Driven Development</p>
<p>Traditional software engineering methodologies, such as Test-Driven Development (TDD) [19] and Behavior-Driven Development (BDD) [20,21], have been widely adopted to ensure software correctness and alignment with user requirements.These methodologies rely on structured validation processes, which have proven effective for deterministic systems with well-defined, stable specifications.</p>
<p>• TDD enforces software correctness by requiring developers to write unit tests before implementation.This follows a strict red-green-refactor cycle, ensuring that each new functionality meets predefined specifications before integration.• BDD extends TDD by emphasizing user-defined scenarios (e.g., "Given-When-Then") to validate system behavior from a stakeholder's perspective.This approach fosters collaboration, aligning software functionality with business or user requirements.</p>
<p>While effective for traditional software, these methods face significant limitations when applied to LLM-based agents,</p>
<p>which operate under open-ended, evolving objectives and generate non-deterministic outputs.The primary challenges include:</p>
<p>• Static Requirements: TDD and BDD assume well-defined, fixed requirements that remain stable throughout development.However, LLM agents dynamically interpret tasks, adapt to new contexts, and modify workflows in response to real-time interactions, making pre-specified test cases insufficient.• Binary Testing Outcomes: Traditional unit tests rely on pass/fail assertions, which are poorly suited for LLM agents, where responses may vary in correctness or appropriateness depending on context.Unlike deterministic systems, an LLM agent's output is often probabilistic, meaning multiple responses may be valid within a given scenario.• Inability to Capture Qualitative and Contextual Factors: Many key aspects of LLM agent evaluation-such as reasoning/planning coherence, fairness, and adherence to ethical constraints-cannot be effectively measured using predefined test assertions or structured scenarios alone.</p>
<p>Evaluation-Driven Development (EDD) embeds continuous, adaptive.and actionable evaluation across the entire LLM agent lifecycle, transforming evaluation from a static, pre-deployment checkpoint into an ongoing driver of system evolution.Unlike TDD and BDD, which rely on predefined tests and fixed requirements, EDD integrates real-time feedback, adaptive evaluation, and post-deployment monitoring to ensure LLM agents remain safe and responsive to evolving operational demands.This is particularly critical for LLM agents, which operate in open-ended, contextually dynamic environments where static validation is insufficient.By continuously refining workflows, reasoning, and tool integration based on real-world insights, EDD provides a structured approach to maintaining reliability and alignment with intended objectives.Table 1 summarizes the key distinctions between these approaches.</p>
<p>Existing LLM Agent Evaluation Methods</p>
<p>Existing evaluation frameworks largely focus on the model level [13], assessing LLM capabilities in areas such as coding [22,23,24] and domain-specific applications (e.g., healthcare [25,26], legal [27,28], and finance [29,30]).While informative, these benchmarks often fail to account for system-level sociotechnical interactions in complex, multi-component agents [6,31].</p>
<p>Many evaluation methods for LLM agents rely heavily on fixed benchmarks, which pose significant limitations.These static tests risk data contamination [32] and fail to adapt to the dynamic, evolving contexts of real-world applications.While live benchmarks (e.g., [33,34]) have emerged as a promising alternative, offering improved adaptability through periodic updates, they still fall short of addressing the core challenges of LLM agent evaluation.Specifically, these System-level evaluation frameworks have garnered significant attention, with many approaches emphasizing end-to-end success metrics (e.g., success/pass rate) as the primary measure of performance.While these metrics provide a holistic view of system effectiveness, they often overlook critical intermediate decision-making steps, which are essential for diagnosing failures and understanding system behavior [31].To address this limitation, step-based and trajectory-based evaluation methods have been proposed [31,35].These methods offer finer granularity, enabling detailed insights into workflow efficiency, agent reasoning, and the progression of decision-making processes.However, while such granular approaches enhance diagnostic capabilities, they also introduce trade-offs.Overemphasis on step-level or trajectory-level analysis can lead to excessive complexity, making it challenging to aggregate results into a coherent system-level assessment [36].Therefore, a careful balance must be struck between granularity and holistic evaluation to ensure that frameworks are both diagnostically informative and practically actionable.</p>
<p>Another emerging approach is the use of safety cases, which provide structured, evidence-based justifications for LLM agent reliability [37,12].These frameworks help regulators and developers assess operational risks but require continuous updates to remain relevant in evolving deployment conditions.</p>
<p>Despite these advancements, current methods lack a structured approach that integrates evaluation results into continuous system improvement and real-time risk control-a gap that evaluation-driven development aims to address.</p>
<p>Methodology</p>
<p>This study employs a MLR 3 (Fig. 1) to synthesize insights from both academic and industry sources on LLM agent evaluation, following established guidelines [38,39].MLR was chosen because it allows for a comprehensive understanding of both academic theories and industry practices.The findings from the MLR directly informed the process model and reference architecture, ensuring they are empirically grounded and aligned with real-world evaluation challenges.</p>
<p>MLR Planning</p>
<p>The planning phase began with identifying the need for a multivocal approach to capture both research-driven and practitioner-driven perspectives.An initial research protocol was developed, including explicit RQs, search terms, and inclusion/exclusion criteria to ensure comprehensive coverage.A pilot study was conducted to test the effectiveness and relevance of search queries, leading to refinements in the final protocol.</p>
<p>Multivocal Literature Review (MLR)</p>
<p>Databases and Search Strategy: Academic sources were retrieved from Google Scholar, IEEE Xplore, ACM Digital Library, Science Direct, and Springer, while Google Search was used to identify relevant grey literature (e.g., practitioner blogs, evaluation frameworks, open-source tools).The primary search terms were: ("large language model" OR "LLM" OR "agent") AND ("evaluate" OR "benchmark" OR "test"), with variations (e.g., pluralization, noun-verb shifts, and verb forms) to maximize retrieval.Searches were performed on June 5, 2024, with a focus on literature published since 2022, reflecting post-ChatGPT advancements.</p>
<p>Screening and Selection: After removing duplicates, sources were screened in three stages: title, abstract, and full-text review.Two authors independently conducted each stage to ensure consistency.Inter-rater reliability was calculated using Cohen's Kappa (κ = 0.78), indicating strong agreement.Discrepancies were resolved through discussion.The selection criteria prioritized sources addressing tools, frameworks, or platforms for LLM agent evaluation with clear theoretical or empirical contributions.Excluded sources included those unrelated to evaluation, lacking credibility or substantial contributions, or inadequately documented tools.Eligible sources included academic papers, conference proceedings, technical reports, white papers, and preprints (e.g., arXiv) available in English.</p>
<p>Expanded Search: To broaden coverage and ensure no relevant study was missed, backward and forward snowballing was conducted on the final selection, as per [40].Additionally, critical new sources published after the initial search were included based on author discussions.Saturation was considered achieved when no significant new themes emerged from additional sources.</p>
<p>Quality Assessment: To ensure rigor, academic sources were evaluated using three criteria:</p>
<p>• Authority: Assessed based on venue reputation and author credentials.</p>
<p>• Methodological Rigor: Evaluated for explicit methodology, reproducibility, and depth of analysis.</p>
<p>• Objectivity: Ensured neutrality and avoidance of promotional bias.</p>
<p>For grey literature and tools, credibility was assessed based on:</p>
<p>• Source Reputation: Evaluated based on publisher standing (e.g., company reports, reputable blogs) or tool popularity (e.g., GitHub stars).</p>
<p>• Documentation: Presence of clear usage guides, benchmarks, or test cases.</p>
<p>• Maintenance Activity: Frequency of updates and community engagement.</p>
<p>Finally, our analysis included 134 academic sources and 27 tools, frameworks, and platforms for LLM agent evaluation.</p>
<p>Data Extraction and Mapping: Data was extracted using a structured coding framework, mapping key evaluation dimensions relevant to the research questions.This included:</p>
<p>• Evaluation Practices: Coverage across pre-deployment, post-deployment, and continuous monitoring.</p>
<p>• Evaluation Methods: Types of metrics used (e.g., end-to-end vs. intermediate pipeline/artifact assessments).</p>
<p>• Evaluation Outputs and Adaptation Mechanisms: How results inform system refinement, risk control, and iterative improvements.</p>
<p>• Architectural Considerations: Evaluation-related components in system design, including feedback loops, observability infrastructure, and safety mechanisms.</p>
<p>These insights directly shaped the process model (capturing procedural evaluation integration) and the reference architecture (capturing architectural evaluation integration).</p>
<p>Data Synthesis: A thematic analysis identified recurring evaluation patterns, challenges, and best practices.To complement qualitative findings, quantitative frequency counts were computed to assess the distribution of evaluation methods, system coverage, and adaptation strategies.Findings were synthesized to guide the process model and reference architecture, ensuring that both are: (1) Empirically grounded, (2) Aligned with practical challenges, and (3) Facilitative of real-world agent evaluation.</p>
<p>The resulting reference architecture was designed to be industry-cross-cutting, conceptually rigorous, and facilitative, aligning with established principles of empirically grounded reference architectures [41].To enhance robustness, the synthesized themes underwent triangulation through internal review and were further validated via feedback from co-authors specializing in LLM agent design and evaluation.This iterative validation process ensured that the findings were not only methodologically sound but also practically applicable to real-world AI system evaluation.</p>
<p>MLR Results</p>
<p>To address RQ1, the MLR, comprising 134 academic and 27 grey sources, reveals several critical challenges in current evaluation practices that impede LLM agent design.</p>
<p>Challenges in LLM Agent Evaluation</p>
<p>Fragmented Lifecycle Coverage</p>
<p>A key limitation in LLM agent evaluation is the overwhelming pre-deployment focus, with limited attention to postdeployment and continuous evaluation (see Fig. 2).Our analysis shows that in academic literature, 93.28% (125/134) of evaluations occur before deployment, while post-deployment (2.24%, 3/134) and continuous evaluation (4.48%, 6/134) remain severely underexplored.While offline evaluations help validate performance under controlled conditions, they fail to capture emergent behaviors, operational degradation, and real-world interactions that arise post-deployment.</p>
<p>By contrast, grey literature presents a more balanced distribution: 44.44% (12/27) of evaluations occur pre-deployment, 14.81% (4/27) post-deployment, and 40.74% (11/27) continuously.This suggests that industry-driven practices recognize the need for ongoing monitoring, though post-deployment evaluation remains underrepresented even in grey sources, indicating that real-world observability mechanisms are still insufficiently integrated.</p>
<p>Implications: The lack of lifecycle-spanning evaluation mirrors the technical debt phenomenon in traditional software engineering [42], where early-stage testing limitations incur operational costs.In LLM agents, the consequences include:</p>
<p>• Missed Real-World Failures: Pre-deployment tests cannot detect emergent issues (e.g., adversarial attacks, data drift, tool misuse) that manifest only in production.</p>
<p>• Reactive Mitigation: Without post-deployment monitoring, issues such as bias amplification or security vulnerabilities are only detected after affecting users.</p>
<p>• Siloed Insights: Disconnected evaluation phases prevent feedback loops where real-world usage data could inform iterative refinements.</p>
<p>Addressing this fragmentation requires evaluation frameworks that span the entire lifecycle, aligning with AgentOps principles [43] to support ongoing monitoring, adaptation, and refinement.</p>
<p>Over-Reliance on Aggregated Metrics</p>
<p>LLM agent evaluation is often constrained by the dominance of high-level and aggregated metrics, which obscure critical system-level insights.Our analysis (see Fig. 3) reveals that 66.42% (89/134) of academic studies rely predominantly on aggregate measures such as overall accuracy, task success rate, or binary pass/fail outcomes.While these metrics offer a broad performance snapshot, they fail to capture fine-grained failure patterns, such as planning  Despite their diagnostic limitations, aggregated metrics remain the dominant evaluation approach in academic studies.Only 21.64% (29/134) assess intermediate pipelines and artifacts, and an even smaller fraction (11.94%, 16/134) employ mixed evaluations that integrate both intermediate and end-to-end insights.This imbalance suggests that most academic evaluations focus on outcome-based assessments.By treating evaluation as a final validation step rather than a continuous diagnostic tool, these studies overlook the complexities of real-world agent behavior.</p>
<p>In contrast, grey literature places greater emphasis on evaluating system-level interactions.14.81% (4/27) of sources focus on model-level evaluation, 29.63% (8/27) assess system-level performance, and 55.56% (15/27) employ mixed evaluations.This distribution reflects a stronger industry-driven focus on assessing LLM agents in real-world operational contexts.</p>
<p>While intermediate evaluations provide deeper insights into system behavior, they introduce several practical challenges [36].First, narrowly scoped evaluations often struggle to establish meaningful performance thresholds, as they may not fully capture real-world complexities or cumulative risks.Second, consolidating multiple fine-grained assessments into a comprehensive system-wide evaluation remains difficult, as fragmented evaluations make it unclear how different capabilities contribute to overall performance.Third, these challenges are compounded by the increased cost and complexity of designing and executing detailed assessments, particularly when domain expertise or extensive manual review is required.Thus, while intermediate evaluations offer higher diagnostic value, they must be carefully integrated with end-to-end evaluations to ensure both interpretability and practical scalability.</p>
<p>Implications:</p>
<p>The over-reliance on aggregated metrics limits the ability to diagnose and improve LLM agent behavior.By focusing only on final outputs, such metrics obscure how agents reason, plan, and interact with external tools, making it difficult to pinpoint the root causes of failures.This lack of process-level visibility prevents targeted refinements, as developers are unable to systematically adjust prompts, retrieval mechanisms, or tool integrations based on evaluation insights.Without deeper assessments, failures in decision-making, reasoning steps, or tool usage may go unnoticed until deployment, leading to unreliable and suboptimal agent performance.</p>
<p>Gaps Between Model-Level and System-Level Evaluations</p>
<p>Current evaluation disproportionately focus on the underlying models, often at the expense of system-level evaluations.Our analysis (see Fig. 4) reveals that 66.42% (89/134) of academic studies focus primarily on evaluating model performance-such as response coherence, factual correctness, and task-specific accuracy-while only 21.64% (29/134) assess system-level interactions.Even fewer studies (11.94%, 16/134) employ integrated evaluations that jointly examine model-and system-level dynamics.</p>
<p>In contrast, grey literature adopts a more system-aware perspective.Implications: The disconnect between model-level evaluations and real-world system operations has several critical consequences:</p>
<p>• Evaluation outcomes do not reflect real-world agent behavior: Model-level evaluations alone overlook how agents operate within workflows, integrate with external tools, and adapt to dynamic environments.As a result, models that perform well in isolation may fail in deployment due to execution delays, tool failures, or cascading decision errors.</p>
<p>• Risk of misaligned refinement: Prioritizing model-level evaluations may lead to optimizations that do not translate into real-world effectiveness.For example, improving factual correctness does not necessarily enhance an agent's ability to handle structured tasks requiring API interactions.</p>
<p>Lack of Adaptive Evaluations</p>
<p>A fundamental limitation in current LLM agent evaluation is the dominance of predefined, static benchmarks and test cases, which assume fixed conditions and fail to capture the complexity and unpredictability of real-world interactions.Our analysis (see Fig. 5) reveals that only 2.24% (3/134) of academic studies explicitly incorporate evaluation methodologies that move beyond static test cases.In contrast, grey literature shows a somewhat higher emphasis on contextual variability, with 18.52% (5/27) integrating evaluation components that extend beyond fixed benchmarks.Despite this, the vast majority of evaluations in both academic (97.76%, 131/134) and grey literature (81.48%, 22/27) remain grounded in predefined test scenarios, limiting their ability to evaluate an agent in dynamic and evolving settings.</p>
<p>Implications: Static evaluations present a fundamental methodological limitation: predefined benchmarks inherently fail to capture the full range of conditions and challenges LLM agents encounter post-deployment.This reflects the curse of dimensionality-as the complexity of tasks, input variations, and external interactions increases, static evaluation sets become increasingly inadequate [10].Even large-scale benchmarks cover only a fraction of potential real-world scenarios, leading to evaluations that provide an incomplete and potentially misleading picture of an agent's robustness and safety.As a result, models optimized for benchmark success may fail to generalize, struggle with unforeseen inputs, or exhibit performance breakdowns when faced with real-world variability.</p>
<p>The continued reliance on predefined test cases risks reinforcing superficial improvements rather than advancing truly robust AI capabilities.To ensure evaluation results align with deployment realities, future methodologies must move beyond fixed benchmarks and integrate more contextually adaptive testing approaches that dynamically expose agents to a broader and more representative range of task conditions [44].Although extensive evaluation data is available, its impact on agent refinement remains limited.Our analysis (see Fig. 6) reveals that only 29.10% (39/134) of academic studies explicitly report leveraging evaluation results to drive iterative design improvements.In contrast, 81.48% (22/27) of grey literature sources incorporate evaluation feedback into the refinement of agent behavior, architectures, or tool integration.This discrepancy suggests that, in academic research, evaluations are often treated as isolated validation steps rather than continuous inputs into the design process.Specifically, in academic sources, 70.90% (95/134) of studies rely on static evaluation checkpoints, where agent performance and safety is measured at predefined milestones (e.g., final benchmark scores) without mechanisms for systematic adaptation.Grey literature, however, exhibits a more iterative approach, indicating that evaluation-driven refinements are considered a core part of agent development in industry applications.</p>
<p>Implications: The lack of evaluation-driven adaptation limits the robustness and responsiveness of LLM agents, leading to several key challenges:</p>
<p>• Failure to close the loop between evaluation and development: Treating evaluation as a static checkpoint rather than an iterative process prevents meaningful refinements.Agents optimized for predefined tests may not generalize well to evolving user needs or task complexities.To bridge this gap, evaluation must function as a continuous feedback loop, where real-world insights directly inform prompt tuning, retrieval optimization, and agent reasoning updates.</p>
<p>• Agents remain reactive rather than adaptive: Without mechanisms to systematically integrate evaluation feedback, LLM agents risk stagnation-responding to new inputs without evolving their decision-making processes.A structured approach to evaluation-driven adaptation is needed, where model-level refinements (e.g., retrieval and prompt optimization) are complemented by system-level adjustments (e.g., refining workflow coordination and external tool usage).</p>
<p>• Underutilization of failure patterns as a learning signal: Evaluations often expose recurring weaknesses-such as reasoning errors or ineffective tool use-but these insights remain disconnected from agent improvement cycles.Future evaluation strategies should shift from pass/fail assessments toward diagnostic evaluations that systematically track failure trends over time, enabling proactive mitigation rather than reactive fixes.</p>
<p>Limitations of AI-Only Evaluations and the Need for Human-in-the-Loop</p>
<p>Our analysis (Fig. 7 Implications:</p>
<p>• Automated evaluations may amplify biases: AI evaluators inherit biases from their training data, potentially reinforcing systemic inequities if not balanced by human judgment.Industry best practices recommend hybrid evaluation, as it provides deeper insights into nuanced aspects of agent performance, such as ethical reasoning, user acceptance, and coherence [47].</p>
<p>• Critical risks in high-stakes applications: Domains such as healthcare, finance, and legal decision-making require rigorous oversight due to the severe consequences of errors.AI-only evaluations may fail to detect subtle yet critical failure patterns, making human oversight mandatory to ensure thorough assessment and mitigate potential harm [47].</p>
<p>Bridging Evaluation Challenges to Design Solutions</p>
<p>The challenges outlined above highlight critical gaps in existing LLM agent evaluation practices, revealing limitations in lifecycle coverage, metric design, system integration, adaptability, evaluation-driven refinement, and the balance between human and AI assessors.Addressing these issues requires a structured approach that integrates evaluation not only as a validation mechanism but as a core driver of agent design and development.</p>
<p>To systematically address these challenges, we propose a process model that embeds evaluation throughout the LLM agent lifecycle, ensuring continuous feedback loops and adaptation.Additionally, our reference architecture provides an architectural framework that operationalizes evaluation-driven design, supporting both model-and system-level refinements.</p>
<p>Table 2 summarizes how each identified challenge is mapped to key strategies within the process model and reference architecture.This structured linkage ensures that evaluation is not merely a final assessment stage but an active and iterative component of LLM agent development.</p>
<p>This structured approach ensures that evaluation is not a passive, post-deployment checkpoint but an active driver of LLM agent improvement.The following sections detail the proposed process model (Sec.5) and reference architecture (Sec.6), which together provide a foundation for evaluation-driven LLM agent design.</p>
<p>Process Model for LLM Agent Evaluation</p>
<p>Building on the insights from RQ1, we now introduce a structured Process Model for LLM Agent Evaluation (Fig. 8).While traditional evaluation approaches often focus on pre-deployment testing or rely on high-level success metrics, LLM agents require a lifecycle-spanning, risk-aware, and adaptive evaluation process.AgentOps Infrastructure supports comprehensive logging of offline and online evaluations.</p>
<p>Gaps Between Model-Level and System-Level Evaluations</p>
<p>Evaluate at both model and system levels, linking model evaluations to inform system evaluations.</p>
<p>Operation Layer ensures evaluations account for out-of-model components and interactions.</p>
<p>Lack of Adaptive Evaluations</p>
<p>Adopt adaptive evaluations that dynamically update test/safety cases based on evolving operational conditions.</p>
<p>Incorporate dynamic test/safety case generation and runtime evaluation within the Operation Layer.</p>
<p>Failure to Leverage Evaluation for Continuous Agent Improvement</p>
<p>Establish closed feedback loops where evaluation results drive iterative agent refinements.</p>
<p>Deploy structured evaluation-to-adaptation pipelines in the Operation Layer to enable online adaptation and offline redevelopment.</p>
<p>Limitations of AI-Only</p>
<p>Evaluations and the Need for Human-in-the-Loop Consider both AI and with human (expert) evaluations, especially in high-stake applications.</p>
<p>Evaluation Component supports various evaluators, integrating human review for high-stakes evaluation and AI for scalable evaluation.</p>
<p>Our model integrates offline and online evaluation, structured feedback loops, and real-world adaptability, ensuring continuous monitoring and refinement of LLM agents across their lifecycle.By specifying evaluation activities, risk-based prioritization, and iterative improvement mechanisms, this process model bridges conceptual evaluation challenges with practical implementation needs, laying the foundation for the reference architecture in RQ3.</p>
<p>Step 1: Define Evaluation Plan</p>
<p>A well-defined evaluation plan is essential for structured, goal-driven, and risk-aware evaluations.Without it, evaluations risk being reactive, misaligned with real-world constraints, and unable to support continuous improvement.Unlike traditional evaluation plans, which are static and pre-defined, our approach is dynamic, risk-aware, and lifecycle-spanning.It adapts to evolving user needs, governance regulations, and system architecture, ensuring evaluations remain relevant, structured, and actionable.This dynamic adaptation is facilitated through feedback loops, continuous monitoring, and iterative refinement processes integrated into the evaluation framework.</p>
<p>Key Inputs:</p>
<p>The evaluation plan is informed by three primary inputs:</p>
<p>• User Goals: High-level objectives representing user needs (e.g., "An LLM-powered financial advisor should provide tax suggestions tailored to individual income brackets and locations").These goals capture user expectations and operational contexts, serving as a foundation for defining evaluation objectives.• Governance Requirements: Legal, ethical, and safety constraints (e.g., ensuring compliance with the EU AI Act).These requirements outline compliance criteria and ethical guidelines that the evaluation must address [48].• Initial Agent Architecture: The system's structural and behavioral blueprint, detailing key components, dependencies, and initial risk trade-offs.This input provides insight into technical complexities and potential areas of vulnerability that require focused evaluation.</p>
<p>Process Steps: The evaluation planning process involves five key activities to ensure systematic, risk-aware, and adaptable assessments:</p>
<ol>
<li>Understand User Goals: Translates high-level goals into structured, testable evaluation scenarios, defining expected interactions, environmental conditions, and task outcomes [19,49,50].For example, an LLM-driven legal assistant tasked with drafting contracts or summarizing case law must ensure factual accuracy and citation  [48].For instance, in the US, a healthcare LLM agent must comply with the Health Insurance Portability and Accountability Act, necessitating thorough evaluations of its data handling practices.3. Analyze Initial Agent Architecture: Identify potential risks, dependencies, and failure points within the system's architecture.This includes analyzing the agent's reasoning pipelines, guardrails, external integrations, and memory systems.For example, an agent accessing real-time data via APIs may require evaluations of RAG capabilities, API latency, and privacy safeguards.4. Risk-Based Prioritization: Classify and prioritize evaluation scenarios along three dimensions: (1) impact severity (e.g., safety or financial consequences), (2) domain sensitivity (e.g., healthcare versus general use), and (3) stakeholder risk tolerance [51,52].To address challenges in quantifying risks at the initial stages of system development, a dual analysis is recommended-employing forward analysis to explore potential scenarios and pathways that may lead to risk, alongside backward analysis to derive appropriate thresholds for early evaluation units from the expected final harm.For instance, in high-risk domains such as medical diagnosis, strict evaluation criteria would be enforced, while in lower-risk contexts like email drafting, more flexible criteria can be applied.5. Generate Evaluation Plan: Consolidate insights from the previous steps into a detailed evaluation plan, covering objectives, prioritized risks, governance requirements, and execution pipelines.This plan supports continuous evaluation and iterative improvement, ensuring dynamic alignment with evolving governance landscapes through integrated feedback mechanisms.</li>
</ol>
<p>Key Outputs:</p>
<p>The main output is a structured, adaptive evaluation plan that ensures systematic, risk-aware assessments throughout the agent's lifecycle, including:</p>
<p>• Evaluation Scope and Purpose: Defines objectives (e.g., accuracy, compliance), identifies evaluation targets (e.g., reasoning pipelines, retrieval-augmented generation [53]), and establishes criteria for assessing guardrail effectiveness [13].Specific metrics may include precision, recall, and fairness indices to assess these targets.• Evaluation Strategy and Methodology: Describes the overall approach to evaluation, incorporating both offline and online evaluation stages.Offline evaluations focus on controlled environments to establish performance baselines, while online evaluations assess system behavior under real-world conditions.This strategy ensures iterative validation, with continuous feedback loops to refine agent behavior, update evaluation criteria, and adjust testing priorities based on emerging insights.• Evaluation Criteria and Metrics: Combines qualitative and quantitative metrics (e.g., success rates, response times, robustness scores) with interpretability methods to understand agent decision-making processes [54].</p>
<p>Additionally, preliminary safety cases [37] may be developed-structured arguments providing early evidence of system safety and compliance.These cases evolve over time, supported by continuous validation against updated risk assessments and governance requirements.</p>
<p>Step 2: Develop Evaluation Test Cases</p>
<p>Building on the evaluation plan established in Step 1, this step focuses on translating evaluation objectives into concrete, testable cases.Unlike traditional software testing, which often relies on static test suites, our approach emphasizes the dynamic and adaptive nature of LLM agents, integrating continuous feedback to evolve test cases over time.This ensures comprehensive coverage of both predictable and emergent behaviors, making the evaluation process responsive to real-world conditions.</p>
<p>Key Inputs: Test case development is informed by three primary sources:</p>
<p>• Evaluation Plan: Defines evaluation objectives, scenarios, and criteria from Step 1, ensuring alignment with user goals, governance requirements, and risk-based priorities.• Historical Evaluation Results: Logs and performance data from prior evaluations (e.g., AgentOps logs) guide the refinement of test cases.This feedback loop allows for the continuous adaptation of test cases based on real-world issues and system evolution [55].• Domain Knowledge Base: Includes domain-specific guidelines, regulatory requirements, and expert insights.</p>
<p>This helps create realistic, high-impact test cases reflecting operational challenges and domain-specific risks.</p>
<p>Process Steps: Test case development follows four interconnected activities designed to ensure both breadth and depth in evaluation:</p>
<ol>
<li>Identify Evaluation Benchmarks and Frameworks: Select standardized benchmarks (e.g., AgentBench [6]) and evaluation frameworks (e.g., LangSmith and DeepEval) to establish performance baselines.These frameworks support such as automated execution, trajectory-based evaluations, and online monitoring.Selection is guided by Step 1 (e.g., risk levels), ensuring that critical capabilities are prioritized.In the case of a tax-copilot LLM agent, this step involves using benchmarks to evaluate core functionalities such as RAG and web-browsing using selected tools/frameworks.2. Collect Domain Knowledge Bases: Leverage domain-specific resources to create contextually relevant test cases [56,57,52].For a tax-copilot deployed in the US, this includes integrating IRS guidelines, tax codes, and relevant case law.Test cases might simulate filing taxes for gig workers, applying international tax treaties for expatriates, or managing multi-state tax obligations for remote employees, ensuring the agent handles real-world complexities.3. Curate Test Data with Domain Experts: Collaborate with subject matter experts to identify edge cases, nuanced scenarios, and potential failure modes not captured by standardized benchmarks [56,57].Using the same tax-copilot as an example, tax professionals can help design challenging scenarios, such as resolving conflicting interpretations of tax laws (e.g., home office deductions) or identifying errors in taxpayer-provided data that could trigger audits.4. Generate Synthetic Test Data with LLMs: Use LLMs to create synthetic data that expands coverage beyond real-world datasets when needed [58,59].Synthetic cases simulate rare or high-risk scenarios that may be underrepresented in existing data.This complements, rather than replaces, expert-curated and real-world data.For the same tax copilot, synthetic data can simulate rare tax scenarios, such as filing for high-net-worth individuals with complex portfolios, or businesses undergoing mergers, ensuring the agent is robust against edge cases.</li>
</ol>
<p>Throughout these activities, test case selection and refinement are shaped by an underlying sensitivity to risk.Evaluations prioritize scenarios where system failures could have the most significant operational or ethical consequences, ensuring that critical vulnerabilities are identified and addressed early.</p>
<p>Key Outputs:</p>
<p>The outcomes of this step support comprehensive, adaptive evaluation strategies:</p>
<p>• Comprehensive Test Suite: A curated set of test cases targeting both standard scenarios (routine tasks, common workflows) and edge cases (rare, complex, or high-risk conditions).Test cases evaluate both final outputs (e.g., task success rates) and intermediate artifacts (e.g., reasoning traces, decision-making workflows).• Selected Benchmarks and Frameworks: A tailored collection of evaluation tools and platforms that support robust, repeatable assessments, including capabilities for real-time monitoring and dynamic scenario adaptation.</p>
<p>By continuously integrating feedback from real-world operations, adapting to emerging risks, and incorporating domain expertise, this approach ensures that evaluation is not a static checkpoint but an ongoing process embedded within the agent's lifecycle.This dynamic, context-aware strategy aligns with the broader principles of evaluation-driven development, supporting both iterative improvement and proactive risk mitigation.</p>
<p>Step 3: Conduct Offline and Online Evaluations</p>
<p>This step implements a balanced approach that transitions from controlled offline evaluation to real-world online evaluation.Offline evaluations verify that the agent meets baseline standards before deployment under controlled conditions, while online evaluations provide continuous performance and safety monitoring in real-world settings under evolving demands.Together, they form a Continuous Evaluation and Improvement Loop, enabling the agent to adapt to evolving user behaviors, regulatory shifts, and domain constraints.</p>
<p>Key Inputs: Main inputs to evaluation processes include:</p>
<p>• Identified Benchmarks: Benchmarks, identified in Step 2, provide generalized evaluations as performance baselines and can inform more targeted test case generation.• Test Cases: Developed in Step 2, these test cases form the backbone of offline evaluations, systematically assessing the agent's behavior against controlled datasets and specific scenarios.They validate the agent's ability to deliver correct, complete, and contextually appropriate outputs across typical and high-impact use cases.</p>
<p>• Evaluation Frameworks: These frameworks enhance evaluation processes by supporting tasks like test case execution, artifact analysis, and pipeline evaluations.They enable scalable and comprehensive assessments across both offline and online contexts.</p>
<p>Process Steps: Evaluation activities center around:</p>
<ol>
<li>Evaluate Final Results: This step measures the agent's overall performance by assessing end-to-end outcomes, focusing on criteria such as success/pass rate, accuracy, and user satisfaction.The purpose here is to verify that the agent's ultimate outputs align with intended objectives and user needs.However, it often lacks the granularity needed to diagnose specific issues in the agent's decision-making processes or to trace the root cause of failures [31,60].</li>
</ol>
<p>To mitigate this, online real-time monitoring mechanisms track user interactions, decision logs, and performance fluctuations, automatically flagging anomalies indicative of drift, hallucinations, or systemic failures.These insights serve not only as reactive debugging tools but also as proactive signals to refine workflows and adjust evaluation focus during offline redevelopment.</p>
<ol>
<li>Evaluate Intermediate Pipelines and Artifacts: Beyond outcome-based validation, evaluating the agent's intermediate pipelines (e.g., prompts, intermediate results) and execution artifacts (e.g., plans [61], retrieved knowledge [53], and tool outputs [62]).The goal is to ensure logical consistency, coherence, and alignment with the agent's objectives, preventing error propagation into final outputs [35].Integrating these evaluations with final output assessments provides a comprehensive understanding of system behavior.When deployed online, continuous monitoring tools are embedded within the architecture to automatically flag deviations in intermediate outputs, facilitating proactive issue detection and correction.</li>
</ol>
<p>To ensure evaluation outcomes drive meaningful refinements, evaluations can integrates AI evaluators for scalable assessments while leveraging human oversight for further reliability [63].Intermediate and final result evaluations feed into an adaptive decision mechanism that dynamically determines:</p>
<p>• When evaluation results warrant direct agent adjustments (e.g., modifying plans or tool configurations based on systematic failure patterns).</p>
<p>• When escalation to human review is required due to ambiguous, high-risk, or low-confidence AI evaluation results.</p>
<p>For example, in a medical AI assistant agent, real-time monitoring and user feedback may capture that certain diagnoses repeatedly conflict with medical guidelines when users communicate in languages for which training data may be less available.If confidence thresholds are breached, it triggers human expert review to assess potential errors and seek proper resolution.</p>
<p>Outputs: This step generates a comprehensive set of evaluation results from offline and online evaluations, offering complementary insights:</p>
<p>• Offline Evaluations: Provide key metrics, error analyses, and pass/fail summaries from controlled test scenarios, establishing baseline performance.</p>
<p>• Online Evaluations: Capture real-world metrics such as user impact, adaptive responses, and behavioral patterns under diverse operational conditions.</p>
<p>• Actionable Feedback: Evaluation results are synthesized into actionable insights, guiding both real-time adjustments and long-term system improvements through structured feedback mechanisms that integrate seamlessly with the agent's continuous development lifecycle.</p>
<p>This integrated approach ensures that evaluations are both diagnostic and prescriptive, driving continuous improvements in agent performance, safety, and adaptability while maintaining coherence with the agent's evolving operational environment.</p>
<p>Step 4: Analyze and Improve</p>
<p>This step translates evaluation results into actionable improvements, addressing both runtime (online) and redevelopment (offline) adjustments [64].Runtime improvements involve real-time adaptations to refine pipelines and artifacts, ensuring the agent remains responsive to evolving contexts.Redevelopment focuses on iterative changes to architectural components-and, where relevant, the LLM itself-to secure long-term performance and safety.This dual-pronged Evaluation-Driven Development of LLM Agents approach fosters a continuous feedback loop, where insights from runtime evaluations inform systematic redevelopment efforts, creating an iterative cycle of improvement.</p>
<p>Key Inputs:</p>
<p>The improvements are mainly informed by the following resources:</p>
<p>• System-Level Evaluation Results (from Step 3): Encompass offline (controlled) and online (real-world) insights.Offline results highlight baseline performance gaps and systemic deficiencies, while online results reveal operational metrics, user feedback, and emerging patterns, guiding both immediate and iterative improvements.</p>
<p>• Model Evaluation Results: Although this paper focuses on system-level concerns rather than direct model evaluations, findings from model-level evaluations (e.g., vendor documentation or academic benchmarks) can illuminate known model limitations.These insights help identify system-level adaptations-such as enhanced guardrails, dynamic prompt modifications, or risk-aware routing strategies-to mitigate potential failures without direct model intervention.</p>
<p>Process Steps: This step includes both runtime and redevelopment time improvement activities:</p>
<ol>
<li>Improve During Runtime (Online): LLM agents continuously refine their performance during runtime by dynamically adjusting pipelines and artifacts based on real-time evaluation results [65].If logs or user feedback indicate frequent tool invocation failures in multi-step tasks, the agent can adapt by retrying with alternative tools or leveraging cached data.Similarly, real-time user corrections (e.g., clarifying ambiguous instructions like "I meant 'quarterly' instead of 'monthly'") update context memories, ensuring future responses align with refined inputs.</li>
</ol>
<p>To enhance adaptability, mechanisms such as Transformer² [66] enable dynamic, task-specific weight adjustments during inference, allowing real-time adaptation without retraining.Additionally, Self-disciplined Autoregressive Sampling (SASA) [67] provides a self-evaluation mechanism that steers token selection away from undesirable outputs by leveraging internal contextual representations, improving both response quality and safety.These real-time adaptations are supported by automated feedback mechanisms (e.g., anomaly detectors and threshold-based triggers).However, while frequent adjustments improve responsiveness, excessive modifications risk destabilizing long-term performance, underscoring the need to balance real-time adaptability with system stability.</p>
<p>Improve During Redevelopment (Offline):</p>
<p>This phase involves substantial post-deployment improvements informed by persistent issues identified through offline evaluations and systemic insights from online results.Key activities include:</p>
<p>• Refine Agent Architecture: Address design-level deficiencies by enhancing architectural components such as reasoning and planning modules and guardrails [68].For example, if evaluations reveal that the agent occasionally generates inappropriate responses when interacting with sensitive content, guardrails can be refined to include stricter content filtering mechanisms or domain-specific safety rules.Additionally, improvements may involve reconfiguring the data retrieval system to prioritize higher-quality information sources or optimizing inter-component workflows for enhanced efficiency and reliability.• Fine-Tune/Retrain/Select LLM: Where system-level evaluations indicate major deficiencies traceable to the LLM itself, it may be necessary to fine-tune or retrain the model (e.g., reinforcement learning from human/task feedback [69,70]).Alternatively, an off-the-shelf LLM more aligned with the agent's performance and safety goals could be adopted.Importantly, model-level insights, such as known tendencies for hallucination or bias, guide the design of complementary system-level mitigations, ensuring holistic reliability.</p>
<p>Outputs: This step produces several key outputs that collectively enhance the LLM agent's design and operational safety:</p>
<p>• Safety Cases: Updated safety cases integrate evidence from offline and online evaluations, ensuring the agent's behavior remains safe under new or evolving conditions.These updates are informed by structured evaluation metrics, linking operational performance directly to risk assessment models, thereby enhancing transparency and explainability for various stakeholders.</p>
<p>• Refined Agent Architecture: Documented modifications to system components reflect iterative improvements applied during redevelopment.These refinements align the architecture with identified performance gaps and operational goals.• Updated LLM: Outputs may include fine-tuned, retrained, or newly selected LLMs, aligned with performance and safety goals.• Refined Pipelines/Artifacts: Adjustments to runtime pipelines and artifacts ensure alignment with runtime feedback and evaluation-driven improvements, supporting both immediate operational effectiveness and long-term system resilience.</p>
<p>External Env. Agent
Knowledge
This integrated approach ensures that both runtime adaptations and offline redevelopment efforts are not only reactive to current issues but also proactive in anticipating future challenges.By embedding continuous feedback mechanisms within the system architecture, the process fosters sustainable growth in agent performance, safety, and adaptability.</p>
<p>Reference Architecture for Evaluation-Driven Development of LLM Agents</p>
<p>To address RQ3, we propose a reference architecture (Fig. 9) that structurally embeds evaluation as a core driver of LLM agent development.Unlike traditional architectures that treat evaluation as a post-hoc, isolated activity, our design integrates continuous feedback loops, dynamic evaluation triggers, and adaptive learning pathways.This transforms evaluation from a passive assessment mechanism into an active driver of real-time system adaptation and long-term improvement.</p>
<p>Our design is guided by three key principles:</p>
<p>• Lifecycle Integration: Evaluation is embedded across both pre-deployment (design and development) and post-deployment (runtime and redevelopment) stages, ensuring that insights are continuously available for iterative improvement.• Meaningful Feedback Loops: Proactive feedback loops enable rapid runtime adjustments and comprehensive offline refinements, so that evaluation results actively shape system behavior.• Evaluation as an Active Driver: Evaluation mechanisms are designed not merely for diagnostics but to directly inform decision-making and adaptive processes, fostering self-improvement in response to changing operational demands.</p>
<p>Building on insights from the MLR and process model, our architecture extends previous reference architectures for designing foundation model based systems [7] and responsible AI systems [4] by uniquely centering evaluation as a first-class architectural concern.The architecture comprises three interconnected layers that systematically integrate evaluation throughout the agent lifecycle.</p>
<p>Supply Chain Layer</p>
<p>The Supply Chain Layer establishes the foundational design, functionality, and evaluation criteria for the LLM agent.It comprises a series of preparatory steps-represented with dotted lines to indicate their supportive role-that collectively set the stage for effective evaluation and subsequent system adaptation.Key steps include:</p>
<p>• Plan and Design: Define user goals, governance requirements, and a high-level architecture to shape evaluation criteria and identify relevant scenarios.This step underpins benchmark selection, test case creation, and alignment with compliance and performance standards.</p>
<p>• Collect and Process Data: Gather and preprocess data for model fine-tuning and agent testing, with an emphasis on preparing evaluation data for off-the-shelf LLMs.</p>
<p>• Build/Select and Evaluate Model: Assess proprietary models using tools such as system cards (e.g., the OpenAI o1 System Card [71]) or through black-box API testing, while open-source models are fine-tuned and evaluated against defined performance goals.</p>
<p>• Build and Evaluate System: Integrate the LLM with core system components (e.g., guardrails) and conduct system-level evaluations to verify functional correctness, quality, and risk.Offline evaluations confirm agent readiness prior to deployment.</p>
<p>A dedicated repository consolidates key artifacts-test cases, safety cases, and offline evaluation results-that serve as performance baselines for subsequent evaluation-driven refinements.</p>
<p>Agent Layer</p>
<p>The Agent Layer forms the adaptive core of the LLM agent, supporting information processing, reasoning and planning, and dynamic behavior adjustment.This layer is divided into two segments: External Environments and the Agent Module.</p>
<p>External Environments consist of all entities that shape the agent's operational context by providing essential data and situational insights.These include:</p>
<p>• Users: Who offer explicit or implicit feedback that guides the agent's actions and informs iterative improvements.</p>
<p>• Other Agents: With which the agent collaborates via task distribution, data sharing, or multi-agent communication [72], enabling coordinated task execution.</p>
<p>• Tools: Task-specific APIs and processing modules that the agent uses to execute workflows and enhance decision-making.</p>
<p>• Knowledge Bases: Static or dynamic sources (e.g., databases, ontologies) that provide contextual and domain-specific information to support reasoning.</p>
<p>• Context Engine: Aggregates data from external environments and memory, ensuring responses remain contextually relevant.</p>
<p>• Reasoning &amp; Planning: Generates actionable plans and workflows based on contextual inputs, serving as a key target for evaluation.</p>
<p>• Execution: Orchestrates task sequences, exception handling, and external system interactions to ensure smooth operation.</p>
<p>• Multi-layered Guardrails: Enforce correctness and safety constraints, triggering alerts when deviations from expected behavior occur [8].</p>
<p>• Memory: Stores operational feedback, user preferences, and past decisions, enabling iterative adaptation over time.</p>
<p>A key aspect of the Agent Layer is its integration with evaluation insights.Artifacts from the Supply Chain Layer-such as test cases and safety cases-serve as evaluation references, while runtime feedback loops ensure continuous adaptation.For example, in a financial risk assessment scenario, if an agent consistently misclassifies regulatory risks, evaluation insights can trigger refinements in decision policies, prompting updates to its reasoning models and knowledge retrieval mechanisms.</p>
<p>External Validity: While the MLR is grounded in a comprehensive synthesis of academic and industry sources, its applicability across diverse domains and operational contexts has not been extensively validated.Future work will focus on real-world case studies and scalability assessments to evaluate its adaptability, ensuring that domain-specific nuances are addressed.These real-world case studies will focus on application domains such as healthcare and finance to ensure that the findings are both transferable and scalable across contexts.</p>
<p>Construct Validity: The unification of diverse evaluation practices into a single framework risks oversimplification or misrepresentation.To mitigate this, the data extraction and synthesis process was explicitly documented, ensuring transparency and reproducibility.This documentation allows for critical review and facilitates refinement in future iterations of the framework.Additionally, by continuously updating and refining the evaluation practices based on ongoing research, we ensure that the framework remains relevant and accurate.</p>
<p>Evaluation of the Process Model</p>
<p>The process model provides a structured framework for integrating evaluation across the lifecycle of LLM agents.To validate its applicability, adaptability, and effectiveness, we employed a two-pronged approach: (1) a real-world case study involving the design and development of a LLM-based Tax Assistant system in Australia, and (2) a synthesis of industry practices based on publicly available practitioner reports and posts.</p>
<p>Case Study: Validation with an LLM-based Tax Assistant</p>
<p>To validate the proposed process model, we applied it to a real-world project involving the design and development of an LLM-based Tax Assistant.This system is designed to assist tax professionals in Australia by providing automated analysis of complex tax scenarios.It employs RAG to retrieve and integrate knowledge from Australian Taxation Office (ATO) documentation, ensuring that its responses are both informed and contextually relevant.</p>
<p>Step 1: Define Evaluation Plan.The evaluation commenced with the development of a comprehensive plan tailored to the Tax Assistant's core function: providing professional, accurate, and compliant tax recommendations to tax professionals operating under Australian tax laws.</p>
<p>User goals focused on supporting tax professionals by analyzing tax scenarios and generating reliable advice based on ATO guidelines.Governance requirements emphasized strict compliance with ATO regulations, enforcing robust data privacy and security measures in alignment with the Australian Privacy Principles (APPs), and adhering to Australia's AI ethics principles to foster trust and transparency in AI-assisted tax advisory.</p>
<p>The agent's architecture was analyzed using the AgentArcEval framework [73] to identify critical components and risk factors.A risk-based prioritization strategy was implemented to assess potential failure points and legal risks.For example, a user might inadvertently submit personal financial data for tax calculation, or the system could generate responses containing sensitive information beyond the scope of retrieval.Hallucinated or misleading tax advice could lead to serious legal and financial repercussions.</p>
<p>To mitigate these risks, the evaluation plan outlined key objectives such as retrieval accuracy, generation quality, response efficiency, and data privacy protection, each categorized with priority levels based on their potential impact.</p>
<p>Step 2: Develop Evaluation Test Cases.For evaluation, DeepEval was selected as the primary framework, as it provides a comprehensive evaluation suite supporting various LLM assessment metrics, including faithfulness, answer relevancy, contextual relevancy, contextual precision, hallucination, toxicity, and bias.</p>
<p>Due to the open-ended nature of tax advisory questions, special emphasis was placed on contextual and answer relevancy.Unlike tax compliance questions, which often have definitive yes/no answers, tax advisory questions involve multiple valid interpretations depending on the user's risk tolerance and tax strategy.</p>
<p>After an initial benchmarking evaluation, additional tax-specific datasets were curated.This process involved collaboration with tax professionals and ATO representatives to ensure real-world applicability.The test cases covered a broad range of scenarios, including standard tax inquiries, complex tax deductions, and cross-border taxation issues.</p>
<p>Unlike general-purpose LLM applications, the availability of real-world tax data was sufficient for evaluation purposes.Consequently, while DeepEval supports synthetic data generation, it was not required in this case.</p>
<p>Step 3: Conduct Offline and Online Evaluations.Since our involvement in the project was primarily pre-deployment, the evaluation focused on offline assessments, targeting both intermediate retrieval processes and final generated tax advice.</p>
<p>Given that a proprietary LLM API was employed, our evaluation focused on the end-to-end performance of the Tax Assistant rather than assessing the underlying LLM model itself.Evaluations were structured to be modular, allowing intermediate and final evaluations to be conducted independently based on specific objectives.</p>
<p>Although full-scale online deployment and post-deployment evaluations were beyond our project scope, we conducted pilot testing within controlled simulated environments using DeepEval's LLM observability tools.These tests enabled detailed logging and tracing, allowing for quick identification of performance bottlenecks and retrieval errors.Additionally, the system incorporated human-in-the-loop feedback, ensuring that flagged responses could be reviewed and refined based on expert input.</p>
<p>Step 4: Analyze and Improve.Evaluation results were leveraged to drive both runtime adaptations and offline redevelopment improvements.</p>
<p>Real-time feedback mechanisms enabled dynamic adjustments to retrieval workflows and reasoning/planning processes.Specific issues, such as redundant retrievals or irrelevant citations, were identified and addressed through workflow optimizations.</p>
<p>The agent architecture was refined to improve RAG accuracy and enhance safety mechanisms, particularly by adjusting retrieval guardrails to prevent leakage of sensitive or outdated tax information.Since a proprietary LLM API was used, modifications to the model itself were not within our scope.Instead, improvements focused on optimizing system-level performance and enhancing retrieval efficiency.</p>
<p>Insights from Practitioner Reports and Posts</p>
<p>To validate the proposed process model, we analyzed practitioner and industry posts [54,59,74].These sources detail real-world evaluation practices for LLM systems/agents and illustrate how structured evaluation frameworks are applied in practice.</p>
<p>Practitioners consistently emphasize the importance of starting with interpretable evaluation frameworks.For example, the Cognition.AI post [74] describes using binary pass/fail metrics combined with detailed critiques to assess outcomes, ensuring that evaluators explicitly confirm whether outputs meet defined criteria and uncover hidden assumptions about system behavior.This approach directly supports Step 1 of our process model, where the evaluation plan is established by defining clear objectives, criteria, and risk prioritizations.In practice, pass/fail metrics help verify that an LLM agent avoids unsafe outputs in high-stakes scenarios, while accompanying critiques pinpoint areas for refinement-such as managing ambiguous inputs.</p>
<p>Furthermore, insights from the Microsoft post [59] and the Cognition AI evaluation methodology underscore the challenges of dynamic, open-ended tasks.They advocate for benchmarks that evolve over time, drawing on real-world data (including production logs and user feedback) to capture data saturation and shifting requirements.This dynamic benchmarking approach validates Step 2 of our process model, where test case development integrates both scenariospecific and synthetic data to adapt to emerging challenges.For instance, dynamic benchmarks might incorporate user feedback to refine test cases for conversational adaptability or task-specific accuracy, ensuring ongoing relevance across diverse contexts.</p>
<p>The Cognition AI post [74] also highlights the importance of conducting both offline and online evaluations-a central tenet of our Step 3. Offline evaluations use controlled environments and synthetic data to establish performance baselines, while online evaluations rely on simulated user interactions, operational metrics (such as latency, resource utilization, and coherence), and autonomous feedback mechanisms.These dual evaluation modes ensure that both intermediate pipelines and final outputs are rigorously tested, mirroring real-world conditions where an agent must continuously adapt.</p>
<p>Finally, iterative improvement cycles are critical.As noted in [54] and demonstrated in the Cognition AI methodology, evaluation outcomes must feed back into system design to prompt runtime adaptations and offline redevelopment.External model evaluations-such as updated system cards or independent benchmarks-help identify limitations (for example, in handling complex multi-step tasks) and drive necessary architectural refinements.This aligns with Step 4 of our process model, where continuous feedback and iterative improvements ensure the system evolves to meet new challenges.</p>
<p>By integrating these practitioner insights, our process model is shown to be strongly aligned with proven industry methodologies.Structured planning, dynamic test case development, comprehensive offline/online evaluations, and iterative refinement together create a robust framework that is both theoretically sound and practically effective for evaluating real-world LLM agents.HITL for governance, compliance audits, and oversight HITL for agent decision refinement but not structured for evaluation HITL-driven evaluation cycles, real-time feedback integration, and expert-guided interventions</p>
<p>Evaluation of the Reference Architecture</p>
<p>The reference architecture is evaluated using: (1) a comparative analysis with existing reference architectures, and (2) an analytical validation to assess its adherence to established software architecture principles.</p>
<p>Comparative Analysis</p>
<p>Since the proposed reference architecture builds upon two existing architectures-one for designing Responsible AI systems [4] and another for Foundation Model-based agent design [7]-it is essential to evaluate how it extends, adapts, and enhances prior designs.This comparative analysis examines the differences and improvements introduced in our architecture, demonstrating its novel contributions to evaluation-driven LLM agent development.</p>
<p>The comparative analysis is structured around several key dimensions below.The Evaluation-Driven Reference Architecture advances beyond existing designs by embedding evaluation as a core architectural principle rather than a post-hoc or auxiliary process (see Table 3).</p>
<p>• Architectural Scope: The primary focus of the architecture (e.g., general LLM agent design vs. evaluationdriven design).</p>
<p>• Modularity and Extensibility: The extent to which architectural components are modular and adaptable to evolving system requirements, particularly for evaluation.</p>
<p>• Support for Evaluation-Driven Development: The degree to which the architecture explicitly integrates evaluation as a first-class concern across the LLM lifecycle.</p>
<p>• Observability and Monitoring: The presence of structured observability features such as logging, performance tracking, failure analysis, and real-time monitoring for continuous evaluation.</p>
<p>• Human-in-the-Loop Integration: The extent to which the architecture enables human oversight, feedback loops, and iterative adjustments to improve model performance and reliability.</p>
<p>Analytical Validation</p>
<p>We conducted an analytical validation of the proposed reference architecture (Fig. 9) using the ISO/IEC 25010 quality attributes [75], ensuring a comprehensive assessment of its capability to support evaluation-driven LLM agent development:</p>
<p>Functional Suitability: The architecture ensures completeness by embedding evaluation across all layers-Supply Chain, Agent, and Operation-enabling holistic coverage of both system and operational contexts.Continuous refinement of test and safety cases enhances correctness by aligning evaluations directly with operational goals, ensuring appropriateness in diverse scenarios.</p>
<p>Performance Efficiency: The AgentOps Infrastructure supports real-time monitoring and adaptive feedback loops, minimizing latency in detecting and responding to performance issues.Dynamic generation of evaluation artifacts (test and safety cases) optimizes evaluation frequency and scope, balancing resource utilization with risk control.</p>
<p>Compatibility: By explicitly integrating external environments-including users, tools, knowledge bases, and other agents-the architecture facilitates seamless compatibility and interoperability, ensuring coherent interactions within complex operational ecosystems.</p>
<p>Usability (Operability and Learnability): Observability mechanisms (logging, tracing, analytics) embedded in the AgentOps Infrastructure enable intuitive diagnostics and enhanced operability.The structured documentation of evaluation results and artifacts, supported by hybrid human-AI evaluators, contributes to easier understanding and interpretation by developers and stakeholders.</p>
<p>Reliability: Fault tolerance is systematically addressed through multi-layered guardrails and real-time alerts triggered by the observability infrastructure.Structured tracking of operational data enables rapid identification and mitigation of failures, improving the system's robustness and recoverability.</p>
<p>Security: Auditability and accountability are enforced through comprehensive logging and tracing mechanisms.Continuous evaluation loops ensure timely detection of anomalous behaviors and security risks, proactively safeguarding against vulnerabilities.</p>
<p>Maintainability: The modular design clearly separates concerns-evaluation logic, agent operations, and infrastructure-which simplifies independent updates and maintenance.Structured artifacts such as dynamically generated test and safety cases facilitate analysability and systematic updates, enhancing maintainability without extensive disruptions.</p>
<p>Flexibility: Adaptability is inherent in the architecture through dynamic updates driven by real-time evaluation insights.Scalability is achieved through automated AI-based evaluations complemented by selective human reviews, effectively handling varying operational demands and risk profiles.</p>
<p>Safety: Continuous risk identification and mitigation are core to the architecture, with multi-layered guardrails and adaptive safety cases systematically capturing emerging operational threats and ensuring proactive management of potential safety concerns.</p>
<p>Overall, the analytical validation confirms that the proposed architecture comprehensively addresses ISO/IEC 25010 quality attributes, providing a robust and adaptive foundation for evaluation-driven development of LLM agents.</p>
<p>Conclusion</p>
<p>This paper introduced a structured approach for integrating evaluation systematically into the development and operation of LLM agents.By addressing three core research questions, we advanced understanding in the evaluation-driven development of complex, adaptive LLM agents:</p>
<p>First, through a comprehensive Multivocal Literature Review (RQ1), we identified critical gaps in current evaluation practices, including fragmented lifecycle coverage, over-reliance on aggregated metrics, insufficient adaptive evaluations, and inadequate integration of human evaluators.These findings underscored the necessity for lifecycle-spanning, context-sensitive evaluation methods capable of continuous and dynamic assessment.</p>
<p>Second, we proposed a structured process model (RQ2) to procedurally embed continuous evaluation into the development lifecycle of LLM agents.This model integrates risk-based prioritization, continuous feedback loops, and adaptive testing strategies, ensuring that evaluation outcomes actively guide both immediate runtime adjustments and long-term redevelopment activities.</p>
<p>Third, we developed a reference architecture (RQ3) that architecturally supports this evaluation-driven approach.This architecture embeds evaluation mechanisms at multiple levels-including supply chain preparations, adaptive agent operations, and continuous operational monitoring-facilitating real-time adaptations and proactive refinements informed by both AI and human evaluator inputs.</p>
<p>Collectively, these contributions establish a cohesive framework where evaluation transcends its traditional diagnostic role, becoming an integral, proactive driver of agent evolution.</p>
<p>Figure 3 :
3
Figure 3: Distribution of Evaluation Metrics</p>
<p>Figure 6 :
6
Figure 6: Comparison of Evaluation-Driven Adaptation vs. Evaluation as Checkpoints</p>
<p>Figure 9 :
9
Figure 9: Reference Architecture for Evaluation-Driven Development of LLM Agents</p>
<dl>
<dt>Table 1 :</dt>
<dt>1</dt>
<dt>Comparison of TDD, BDD, and Evaluation-Driven Development (EDD) for LLM Agents</dt>
<dt>AspectTDDBDDEvaluation-Driven DevelopmentScopePre-deploymentPre-deploymentEntire lifecycle (pre-&amp; post-deployment)RequirementsStatic, explicitStakeholder-definedEvolving, context-drivenEvaluation ApproachUnit tests with binary pass/failUser scenario-based acceptance testingAdaptive, continuous evaluation using multi-dimensional metricsReal-time systemFeedback SourceDevelopersStakeholdersmonitoring, humans, andAI evaluatorsAdaptabilityLow (manual test updates)Moderate (stakeholder-driven revisions)High (self-adjusting, real-time refinements)Binary pass/failPass/fail onMulti-dimensionalValidation Focuson codestakeholder-assessment (quantitative &amp;correctnessdefined behaviorqualitative analysis)Post-Yes (continuousDeploymentNoNomonitoring and iterativeAdaptationrefinement)Suitability for LLM AgentsLowModerateHigh</dt>
<dt>• Limited Post-Deployment Evaluation: Both TDD and BDD primarily focus on pre-deployment validation, assuming that once software passes tests, it remains reliable.However, LLM agents continuously evolve in response to user feedback, external knowledge updates, and environmental changes, necessitating ongoing evaluation beyond deployment.•Lack of Support for Emergent Behaviors: Unlike conventional software, LLM agents may exhibit emergent capabilities and unexpected failure modes as they interact with new and uncertain inputs.Static test cases cannot anticipate these behaviors, leading to gaps in evaluation coverage.</dt>
<dd>
<p>144 Grey: 33 Academic: 4404 Grey: 222 Academic: 70 Grey: 7 Academic: 134 Grey: 27
MLR PlanningIdentification of the Need for MLRResearch Protocol DevelopmentPilot StudyMLR ProtocolMLRKeywordScreeningExpandedQualityDataDataSearch&amp; SelectionSearchAssessmentExtractionSynthesisAcademicExtracted DataEmpirically Grounded ResultsFigure 1: Research Methodologybenchmarks are not truly "live" in the sense of enabling in-situ, real-time assessment within interactive environments. Asa result, both fixed and periodically updated benchmarks struggle to adequately capture the complexities and emergentbehaviors of LLM agents in practical, real-time settings.</p>
</dd>
</dl>
<p>Comparison of AI, Human, and Hybrid Evaluators in Academic vs. Grey Literature typically prioritize automation efficiency, while industry practices recognize that human evaluators contribute essential interpretability, context-awareness, and ethical oversight, especially in ambiguous or high-stakes scenarios.
10088.06% (118/134)Academic Sources Grey Sources80Percentage (%)40 6044.44% (12/27)44.44% (12/27)0 20AI EvaluatorsHuman Evaluators Evaluator Type 2.99% (4/134) 11.11% (3/27)Hybrid Evaluators 8.96% (12/134)Figure 7:) reveals a strong reliance on AI evaluators in academic studies, with 88.06% (118/134) employingfully automated methods, minimal human involvement in 2.99% (4/134), and only 8.96% (12/134) using hybridhuman-AI evaluations. In contrast, grey literature presents a more balanced distribution: 44.44% (12/27) use AI-only
[45,46]ions, another 44.44% (12/27) integrate hybrid human-AI approaches, and 11.11% (3/27) prioritize human-led evaluations.AI-only evaluators, which rely predominantly on automated scoring or computation-based assessments (e.g., LLMbased evaluators[45,46]), offer scalability and consistency but often lack sensitivity to qualitative dimensions such as reasoning coherence, ethical judgment, and user-centric factors.The disparity indicates that academic evaluations</p>
<p>Table 2 :
2
Mapping Identified Challenges to Process Model Strategies and Architectural Considerations
Identified ChallengeProcess ModelReference ArchitectureFragmented Lifecycle CoverageContinuously conduct evaluations across entire agent lifecycle.Integrate observability in the Operation Layer to continuously monitor/evaluate agent behavior.Support evaluating both finalOver-Reliance onoutcome and intermediateAggregated Metricspipeline/artifact while consideringthe trade-offs.</p>
<p>Errors in legal interpretations, outdated statutes, or incorrect case references could lead to severe consequences, such as contractual disputes or legal malpractice.2. Incorporate Governance Requirements: Integrate legal and ethical constraints into the evaluation plan to ensure compliance with applicable regulations like the EU AI Act
validity.Understand User Goals1.Define Evaluation PlanUser Goals, Governance Reqs, Initial Agent ArchitectureRisk-DrivenPrioritizationIncorporateGovernanceRequirementsGenerateAnalyze Initial AgentEvaluation PlanArchitectureEvaluation Plan2. Generate Evaluation Test CasesDomainIdentify EvaluationKnowledge BasesBenchmarks/FrameworksCollect DomainnoIdentifiedKnowledge BasesBenchmarks/yesGenerate SyntheticFrameworksCurate Test DataData with LLMswith Domain Expertsmore data needed?Test Cases3. Conduct Offline &amp; OnlineEvaluationsEval. ResultsEvaluate Final ResultsEvaluate Intermediate Pipelines and ArtifactsMeaningful FeedbackImprove During Runtime (Online)4. Analyze and ImproveModel Evaluation ResultsRefine Pipelines/ArtifactsSafety CasesImprove During Redevelopment (Offline)RefinedPipelines/ArtifactsRefine Agent ArchitectureFine-Tune/ Retrain/ Select LLMRefined Agent ArchitectureUpdated LLMFigure 8: Process Model for LLM Agent Evaluation</p>
<p>Table 3 :
3
Comparative Analysis of Reference Architectures
DimensionResponsible AI Systems Design [4]Foundation Model-Based Agents Design [7]Evaluation-Driven LLM Agent DevelopmentScopeGeneral AI system design with responsible AI as the core principleFoundation model-based autonomous agent designContinuous evaluation as the driver for continuous refinmentsModularityModular but primarilyModular for agent-centricModular with plug-and-playandfor responsible AIautonomy, supportingevaluators, test/safety caseExtensibilityenforcementmulti-agent coordinationgenerators, and observabilityEvaluation IntegrationBriefly mentions responsible AI validator as a design patternSome continuous assessments but primarily focuses on risk and governanceContinuous, structured evaluation with adaptable safety/test case generation, and real-time feedback loopsGovernance-orientedSupports task monitoring,Observabilitymonitoring (audit logs, risk dashboards,self-reflection, and risk assessment but lacksEmbed AgentOps to support full observabilityblack-box validation)structured observabilityHuman-in-the-Loop(HITL)
https://github.com/UKGovernmentBEIS/inspect_ai
https://docs.confident-ai.com/
The research protocol and data extraction form are uploaded as supplementary materials for review purpose.
Operation LayerThe Operation Layer serves as the central hub for continuous evaluation and adaptation, ensuring that evaluation insights directly inform agent refinements and system evolution.It consists of two key components: Evaluation and AgentOps Infrastructure.EvaluationUnlike traditional architectures that treat evaluation as a periodic validation step, our design embeds continuous and adaptive evaluation mechanisms, ensuring that real-time assessment drives operational improvements.Key features include using hybrid evaluator for continuous evaluations, whose results in turn serve as direct inputs for system modifications:• Immediate runtime adaptations: Evaluation results trigger dynamic updates to agent operations, such as refining prompt structures, adjusting reasoning pathways, or modifying retrieval and tool invocation strategies based on performance signals and detected inconsistencies.• Systematic long-term refinements: Evaluation findings guide structured improvements to safety cases, the evolution of evaluation datasets, and the calibration of decision-making heuristics.This ensures that the agent continuously adapts to shifting operational conditions while maintaining alignment with performance and governance requirements.• Escalation to human evaluators for high-uncertainty scenarios: AI-based evaluations efficiently handle routine assessments, but when confidence thresholds are exceeded-such as detecting ambiguous failures, ethical concerns, or novel operational risks-evaluations are escalated for human review to ensure nuanced, context-aware judgments.• Continuous test and safety case evolution: Rather than relying on static benchmarks, evaluation insights dynamically expand assessment coverage by generating new test cases and safety constraints.This adaptive approach ensures that emerging risks, edge cases, and real-world operational shifts are progressively integrated into the evaluation framework.AgentOps Infrastructure and ObservabilityThe AgentOps Infrastructure ensures seamless integration of evaluation-driven insights into operational workflows, providing essential monitoring and intervention mechanisms:• Proactive Issue Detection: Observability mechanisms track anomalies and deviations, triggering automated evaluations.• Feedback Integration Pipeline: Links evaluation outputs (e.g., production logs) directly to refinement processes, ensuring real-time and iterative improvement.In summary, this architecture establishes evaluation as an integral mechanism for continuous agent adaptation and learning, ensuring LLM agents remain safe, context-aware, and aligned with evolving operational requirements.By bridging gaps in lifecycle-spanning evaluation, real-time adaptation, and evaluation-driven agent development, it enables a structured approach to developing more safer, and more performant LLM agents.Validation and EvaluationThis section validates and evaluates the three proposed contributions, starting with a methodological evaluation of the MLR results that form the foundation for the process model and reference architecture.The evaluation includes both theoretical validation and practical application in the context of real-world scenarios, highlighting how the proposed solutions address key challenges in LLM agent evaluation.Methodological Evaluation of the MLR Results: Threats to ValidityInternal Validity: Potential biases in data selection and synthesis may impact the accuracy of the findings.To address this, we implemented a rigorous MLR process with well-defined inclusion and exclusion criteria.The screening process was enhanced by internal reviews to ensure consistency and minimize subjective biases.Practitioner discussions further validated the synthesized insights, providing practical alignment with real-world evaluation practices.Any discrepancies during synthesis were resolved through iterative discussions to maintain methodological rigor and alignment.
On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Engineering AI Systems: Architecture and DevOps Essentials. Len Bass, Qinghua Lu, Ingo Weber, Liming Zhu, 2025Addison-Wesley</p>
<p>Responsible AI: Best Practices for Creating Trustworthy AI Systems. Qinghua Lu, Liming Zhu, Jon Whittle, Xiwei Xu, 2023Addison-Wesley</p>
<p>A survey on benchmarks of multimodal large language models. Jian Li, Weiheng Lu, arXiv:2408.086322024arXiv preprint</p>
<p>Agentbench: Evaluating llms as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Towards responsible generative ai: A reference architecture for designing foundation model based agents. Qinghua Lu, Liming Zhu, Xiwei Xu, Zhenchang Xing, Stefan Harrer, Jon Whittle, 2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C). IEEE2024</p>
<p>A taxonomy of multi-layered runtime guardrails for designing foundation model-based agents: Swiss cheese model for ai safety by design. Md Shamsujjoha, Qinghua Lu, Dehai Zhao, Liming Zhu, 2024</p>
<p>A taxonomy of agentops for enabling observability of foundation model based agents. Liming Dong, Qinghua Lu, Liming Zhu, 2024</p>
<p>John Burden, arXiv:2407.09221Evaluating ai evaluation: Perils and prospects. 2024arXiv preprint</p>
<p>Rethink reporting of evaluation results in ai. Ryan Burnell, Wout Schellaert, John Burden, Fernando Tomer D Ullman, Joshua B Martinez-Plumed, Danaja Tenenbaum, Lucy G Rutar, Jascha Cheke, Melanie Sohl-Dickstein, Mitchell, Science. 38066412023</p>
<p>Safety cases at aisi. 2024AISI</p>
<p>An ai system evaluation framework for advancing ai safety: Terminology, taxonomy, lifecycle mapping. Boming Xia, Qinghua Lu, Liming Zhu, Zhenchang Xing, Proceedings of the 1st ACM International Conference on AI-Powered Software. the 1st ACM International Conference on AI-Powered Software2024</p>
<p>Optimizing llm accuracy. Openai, </p>
<p>Yanlin Wang, Tianyue Jiang, Mingwei Liu, Jiachi Chen, Zibin Zheng, arXiv:2407.00456Beyond functional correctness: Investigating coding style inconsistencies in large language models. 2024arXiv preprint</p>
<p>Prioritizing safeguarding over autonomy: Risks of llm agents for science. Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, arXiv:2402.042472024arXiv preprint</p>
<p>Evaluating frontier models for dangerous capabilities. Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, arXiv:2403.137932024arXiv preprint</p>
<p>Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Hung-Yi Lee, arXiv:2406.08747Streambench: Towards benchmarking continuous improvement of language agents. 2024arXiv preprint</p>
<p>Test driven development: By example. Kent Beck, 2022Addison-Wesley Professional</p>
<p>BDD in Action: Behavior-driven development for the whole software lifecycle. John , Ferguson Smart, Jan Molak, 2023Simon and Schuster</p>
<p>Wikipedia contributors. Behavior-driven development -Wikipedia, the free encyclopedia. 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur, Bani Yusuf, Haolan Zhan, arXiv:2406.15877Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. 2024arXiv preprint</p>
<p>Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, arXiv:2308.018612023arXiv preprint</p>
<p>Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset. Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Advances in Neural Information Processing Systems. 202436</p>
<p>Medbench: A largescale chinese benchmark for evaluating medical large language models. Yan Cai, Linlin Wang, Ye Wang, Gerard De Melo, Ya Zhang, Yanfeng Wang, Liang He, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge, arXiv:2309.16289Lawbench: Benchmarking legal knowledge of large language models. 2023arXiv preprint</p>
<p>Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models. Neel Guha, Julian Nyarko, Daniel Ho, Christopher Ré, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, Advances in Neural Information Processing Systems. 202436</p>
<p>Pixiu: A large language model, instruction data and evaluation benchmark for finance. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, Jimin Huang, arXiv:2306.054432023arXiv preprint</p>
<p>Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, Bertie Vidgen, arXiv:2311.11944Financebench: A new benchmark for financial question answering. 2023arXiv preprint</p>
<p>Agentquest: A modular benchmark framework to measure progress and improve llm agents. Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence, arXiv:2404.064112024arXiv preprint</p>
<p>Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, arXiv:2310.180182023arXiv preprintOier Lopez de Lacalle, and Eneko Agirre</p>
<p>Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, arXiv:2406.19314A challenging, contamination-free llm benchmark. 2024arXiv preprint</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Watch every step! llm agent learning via iterative step-level process refinement. Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li, arXiv:2406.111762024arXiv preprint</p>
<p>Sabotage evaluations for frontier models. Joe Benton, Misha Wagner, Eric Christiansen, Cem Anil, Ethan Perez, Jai Srivastav, Esin Durmus, Deep Ganguli, Shauna Kravec, Buck Shlegeris, </p>
<p>Marie Davidsen Buhl, Gaurav Sett, Leonie Koessler, Jonas Schuett, Markus Anderljung, arXiv:2410.21572Safety cases for frontier ai. 2024arXiv preprint</p>
<p>Guidelines for including grey literature and conducting multivocal literature reviews in software engineering. Vahid Garousi, Michael Felderer, Mika V Mäntylä, Information and Software Technology. 106Feb 2019</p>
<p>Systematic literature reviews in software engineering-a systematic literature review. Information and software technology. Barbara Kitchenham, David Pearl Brereton, Mark Budgen, John Turner, Stephen Bailey, Linkman, 200951</p>
<p>Guidelines for snowballing in systematic literature studies and a replication in software engineering. Claes Wohlin, Proceedings of the 18th international conference on evaluation and assessment in software engineering. the 18th international conference on evaluation and assessment in software engineering2014</p>
<p>Empirically-grounded reference architectures: a proposal. Matthias Galster, Paris Avgeriou, Proceedings of the joint ACM SIGSOFT conference-QoSA and ACM SIGSOFT symposium-ISARCS on Quality of software architectures-QoSA and architecting critical systems-ISARCS. the joint ACM SIGSOFT conference-QoSA and ACM SIGSOFT symposium-ISARCS on Quality of software architectures-QoSA and architecting critical systems-ISARCS2011</p>
<p>The wycash portfolio management system. Ward Cunningham, ACM Sigplan Oops Messenger. 421992</p>
<p>A taxonomy of agentops for enabling observability of foundation model based agents. Liming Dong, Qinghua Lu, Liming Zhu, arXiv:2411.052852024arXiv preprint</p>
<p>Safety Institute. Technical blog: Strengthening ai agent hijacking evaluations. U S Ai, January</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202336</p>
<p>Define your evaluation metrics. Google Cloud, </p>
<p>Emerging patterns in building genai products. Bharani Subramaniam, Martin Fowler, 2025</p>
<p>Compl-ai framework: A technical interpretation and llm benchmarking suite for the eu artificial intelligence act. Philipp Guldimann, Alexander Spiridonov, Robin Staab, Nikola Jovanović, Mark Vero, Velko Vechev, Anna Gueorguieva, Mislav Balunović, Nikola Konstantinov, Pavol Bielik, arXiv:2410.079592024arXiv preprint</p>
<p>Thomas Bartz-Beielstein, Carola Doerr, Daan Van Den, Jakob Berg, Sowmya Bossek, Tome Chandrasekaran, Andreas Eftimov, Pascal Fischbach, Kerschke, La William, Manuel Cava, Lopez-Ibanez, arXiv:2007.03488Benchmarking in optimization: Best practice and open issues. 2020arXiv preprint</p>
<p>Specification by example: how successful teams deliver the right software. Gojko Adzic, 2011Simon and Schuster</p>
<p>How to choose a threshold for an evaluation metric for large language models. Bhaskarjit Sarmah, Mingshu Li, Jingrao Lyu, Sebastian Frank, Nathalia Castellanos, Stefano Pasquali, Dhagash Mehta, arXiv:2412.121482024arXiv preprint</p>
<p>Task-specific llm evals that do &amp; don't work. Eugene Yan, 2024</p>
<p>Ragchecker: A fine-grained framework for diagnosing retrieval-augmented generation. Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang Wang, Shichao Sun, Huanyu Li, arXiv:2408.080672024arXiv preprint</p>
<p>Creating a llm-as-a-judge that drives business results. Hamel Husain, 2024</p>
<p>Evaluation -langsmith documentation. Langsmith, 2024</p>
<p>Introducing v0. 5 of the ai safety benchmark from mlcommons. Bertie Vidgen, Adarsh Agrawal, Ahmed M Ahmed, Victor Akinwande, Namir Al-Nuaimi, Najla Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti, Max Bartolo, arXiv:2404.122412024arXiv preprint</p>
<p>Anka Reuel, Amelia Hardy, Chandler Smith, Max Lamparth, Malcolm Hardy, Mykel, Kochenderfer, arXiv:2411.12990Betterbench: Assessing ai benchmarks, uncovering issues, and establishing best practices. 2024arXiv preprint</p>
<p>On llms-driven synthetic data generation, curation, and evaluation: A survey. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, Haobo Wang, arXiv:2406.151262024arXiv preprint</p>
<p>Microsoft Jane Huang. Evaluating large language model (llm) systems: Metrics, challenges, and best practices. 2024</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, arXiv:2401.131782024arXiv preprint</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202436</p>
<p>Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen, arXiv:2405.17935Tool learning with large language models: A survey. 2024arXiv preprint</p>
<p>International network of ai safety institutes joint testing exercise: Improving methodologies for ai model evaluations across global languages. Members of the International Network of AI Safety Institutes. February 2025</p>
<p>Get feedback about the quality of an agentic application. Databricks, </p>
<p>Building effective agents. Anthropic, 2024. February 19, 2025</p>
<p>Transformer 2 : Self-adaptive llms. Qi Sun, Edoardo Cetin, Yujin Tang, arXiv:2501.062522025arXiv preprint</p>
<p>Large language models can be strong self-detoxifiers. Ching-Yun Ko, Pin-Yu Chen, Payel Das, Youssef Mroueh, Soham Dan, Georgios Kollias, Subhajit Chaudhury, Tejaswini Pedapati, Luca Daniel, arXiv:2410.038182024arXiv preprint</p>
<p>Agente: From autonomous web navigation to foundational design principles in agentic systems. Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, Ravi Kokku, arXiv:2407.130322024arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730Advances in neural information processing systems</p>
<p>When llm meets domain experts. Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, Advances in Neural Information Processing Systems. 202336</p>
<p>Openai o1 system card. Openai, 2024</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848202325arXiv preprint</p>
<p>Evaluating the architecture of large language model-based agents. Qinghua Lu, Dehai Zhao, Yue Liu, Hao Zhang, Liming Zhu, Xiwei Xu, Angela Shi, Tristan Tan, SSRN 5017297. 2024</p>
<p>A review of openai's o1 and how we evaluate coding agents. Cognition, Ai, 2024</p>
<p>Systems and software engineering -systems and software quality requirements and evaluation (square) -system and software quality models. 2011</p>            </div>
        </div>

    </div>
</body>
</html>
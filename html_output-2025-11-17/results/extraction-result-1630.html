<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1630 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1630</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1630</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-250526228</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2207.06572v4.pdf" target="_blank">i-Sim2Real: Reinforcement Learning of Robotic Policies in Tight Human-Robot Interaction Loops</a></p>
                <p><strong>Paper Abstract:</strong> Sim-to-real transfer is a powerful paradigm for robotic reinforcement learning. The ability to train policies in simulation enables safe exploration and large-scale data collection quickly at low cost. However, prior works in sim-to-real transfer of robotic policies typically do not involve any human-robot interaction because accurately simulating human behavior is an open problem. In this work, our goal is to leverage the power of simulation to train robotic policies that are proficient at interacting with humans upon deployment. But there is a chicken and egg problem -- how to gather examples of a human interacting with a physical robot so as to model human behavior in simulation without already having a robot that is able to interact with a human? Our proposed method, Iterative-Sim-to-Real (i-S2R), attempts to address this. i-S2R bootstraps from a simple model of human behavior and alternates between training in simulation and deploying in the real world. In each iteration, both the human behavior model and the policy are refined. For all training we apply a new evolutionary search algorithm called Blackbox Gradient Sensing (BGS). We evaluate our method on a real world robotic table tennis setting, where the objective for the robot is to play cooperatively with a human player for as long as possible. Table tennis is a high-speed, dynamic task that requires the two players to react quickly to each other's moves, making for a challenging test bed for research on human-robot interaction. We present results on an industrial robotic arm that is able to cooperatively play table tennis with human players, achieving rallies of 22 successive hits on average and 150 at best. Further, for 80% of players, rally lengths are 70% to 175% longer compared to the sim-to-real plus fine-tuning (S2R+FT) baseline. For videos of our system in action, please see https://sites.google.com/view/is2r.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1630.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1630.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>i-S2R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Sim-to-Real</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative pipeline that bootstraps a coarse human behavior model from initial real-world data, trains robot policies in simulation against that model, fine-tunes the policy with humans in the loop on the real robot, updates the human model with newly collected real interactions, and repeats this loop to improve sim-to-real transfer for cooperative human-robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ABB IRB 120T + Festo linear actuator table tennis robot</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An 8-DOF system: ABB IRB 120T 6-DOF robotic arm mounted on a 2D Festo linear actuator with a table-tennis paddle end-effector; controlled at high frequency (policy outputs 8 joint velocities at 75Hz, ABB EGM at ~248Hz and Festo at up to 125Hz). Used to play cooperative table tennis rallies with human players.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>human-robot interaction / robotic table tennis (general robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Custom PyBullet simulation replicating the robot, contact dynamics and table environment; ball trajectories are simulated with Newtonian dynamics including gravity and drag (no spin); simulation incorporates modeled sensor/action latencies and injected observation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics simulation (robot and contact dynamics modeled in PyBullet; ball dynamics with drag modeled; sensor/actuator latency and observation noise simulated) — not high-fidelity (spin and some perception/physics details omitted).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>robot rigid-body dynamics and contact dynamics (via PyBullet), ball Newtonian flight with air drag (Cd, density, mass, cross-sectional area), restitution/friction empirically estimated, sensor observation noise (uniform noise), and component latencies modeled as Gaussians.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>ball spin is not modeled; perception (vision) imperfections other than added noise not fully modeled; some surface/material effects, wear-and-tear of balls, precise calibration mismatches, and correlation between subsequent human shots were simplified/omitted; human behavior modeled as independent uniform sampling from estimated start/velocity/landing ranges rather than a full probabilistic or sequential model.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical table tennis setup with the 8-DOF ABB+Festo robot, stereo Ximea cameras tracking ball at 125Hz feeding a 3D tracker (interpolated to 75Hz), human participants (non-professional) playing cooperative rallies; real-world fine-tuning done live with humans in the loop (~2 hours per cycle).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>closed-loop high-frequency joint control for cooperative table tennis rallying (returning balls to human reliably, maximizing rally length)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning using an Evolutionary Search method (Blackbox Gradient Sensing, BGS) in simulation with iterative sim/real loops; policies warm-started between rounds and fine-tuned on-robot with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>rally length (number of successive paddle touches in a rally); additional metrics: robot hit/return rate and normalized rally length (z-scored per-player).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>On the physical robot i-S2R policies achieved average rallies of ~22 successive hits (mean) with best observed rallies up to 150 hits; aggregated improvement over S2R+FT was ~9% in mean rally length, and for 80% of players rally lengths were 70% to 175% longer compared to the S2R+FT baseline. Performance varied by player skill (strong gains for beginner/intermediate, negative or mixed for an advanced player).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Not standard broad dynamics randomization, but the simulation injected: uniform position noise on ball observations (2x ball diameter), parameterized Gaussian sensor/action latencies sampled per episode (measured µ and σ per component), empirically estimated variability in restitution/friction parameters, and removal of detection outliers when building human ball distributions; human throw distribution was sampled uniformly across inferred min/max ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Missing ball spin modeling, perception and calibration mismatches (vision detection limits, tracker errors), variability in estimated delays, surface/material differences and wear (balls/table/paddle), simplified human behavior (independent uniform sampling, no sequential correlation), and residual differences in dynamics not captured by PyBullet.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>iteratively improving a human behavior model from real interaction data; explicit modeling of sensor and action latency in simulation (necessary — transfer failed without latency modeled); injection of observation noise; empirical estimation of contact/friction/restitution parameters; warm-starting simulation training from real fine-tuned weights; on-robot fine-tuning with humans in the loop; using an ES-based optimizer (BGS) that transferred reliably; and multiple sim↔real iterations to broaden the training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper identifies accurate modeling of sensor/actuation latencies as critical (transfer 'completely failed' without it); contact dynamics and observation noise are important; modeling ball spin was identified as a missing fidelity requirement that likely limits transfer for advanced players; accurate human-behavior coverage (human ball distributions) is important to reduce fine-tuning time.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuning on the real robot with human partner each iteration: 60 updates first cycle, 70 updates subsequent cycles, totalling 200 parameter updates per player (~2 hours wall-clock per cycle). S2R-Oracle+FT used only 35% of i-S2R's fine-tuning budget as an ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iteratively updating a simulated human behavior model using real human-robot interaction data (i-S2R) improves sim-to-real transfer for cooperative HRI tasks: i-S2R yields longer and more frequently enjoyable rallies than single-shot S2R+FT, especially for beginner and intermediate human players, and generalizes better to new players. Modeling sensor/actuation latency and injecting observation noise were necessary for transfer; missing simulation of ball spin and other unmodeled factors limited performance for very skilled (advanced) play. Much of i-S2R's benefit comes from progressively improving the human behavior model; if the final human model were known upfront (S2R-Oracle), similar final performance could be reached with less real fine-tuning time.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The human behavior model represents per-trajectory initial position/velocity and landing ranges (16 numbers) derived by fitting simulated drag-based trajectories (Nelder-Mead) and removing outliers (DBSCAN); the simulation samples uniformly from these ranges, losing within-range probability structure and temporal correlation between rally shots.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1630.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1630.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BGS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Blackbox Gradient Sensing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolutionary-search (ES) style black-box optimizer introduced in this paper that uses orthogonal perturbation ensembles and a novel elite-choice selection (ranking perturbation directions by positive/negative reward difference) to estimate policy gradients robustly for training RL policies that transfer to a real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>same ABB IRB 120T + Festo table tennis robot (as used with i-S2R)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>See i-S2R entry; BGS was the only policy optimization method in this work that yielded policies that transferred to the physical robot reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics reinforcement learning / sim-to-real policy optimization</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet (training in-simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>See i-S2R entry; BGS was applied to rollouts collected in the PyBullet simulation and also during on-robot fine-tuning using real rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics (same as i-S2R use case); BGS is an optimizer and does not change fidelity but its sampling/perturbation strategy affected training stability and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Irrelevant to optimizer (see i-S2R for simulation modeling); BGS leverages repeated rollouts per perturbation to reduce variance and used orthogonal perturbations to improve convergence under the given simulated fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Not applicable (optimizer-level).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Used for on-robot fine-tuning with humans in the loop; same hardware and human evaluation described under i-S2R.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>policy parameters trained in simulation using BGS were transferred to the robot and then fine-tuned on-robot; BGS-enabled policies successfully transferred to perform cooperative table tennis control.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Evolutionary search (ES) with antithetic sampling, orthogonal Gaussian perturbation ensembles, reward-normalization, perturbation filtering, and a novel elite-choice algorithm ranking perturbations by the reward difference between positive and negative directions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>same rally-length metrics as i-S2R; empirically BGS-trained policies transferred reliably while policies trained with PPO, QT-OPT, SAC did not transfer well to the robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Other RL algorithms (PPO, SAC, QT-OPT) failed to produce on-robot-deployable policies in this work, indicating that optimizer stability and variance reduction (orthogonal samples, elite-choice based on reward deltas, repeated rollouts) were key to obtaining policies robust to the sim-to-real gap. The sim-to-real gap factors themselves (missing spin, latency, perception mismatch) remain relevant but BGS improved convergence despite those gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>BGS's orthogonal perturbations (variance reduction) and novel elite-choice ranking by reward differences (curvature-aware selection) enabled stable training in simulation and produced policies that could be successfully fine-tuned on the robot; repeating rollouts per perturbation and normalization techniques further reduced variance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>While BGS improved optimization robustness, the paper emphasizes that proper simulation modeling (latency, noise) remains necessary; BGS alone does not eliminate the need for accurate latency/noise modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>BGS hyperparameters reported: simulation used 200 perturbations with 15 rollouts per perturbation; real fine-tuning used 5 perturbations with 3 rollouts per perturbation, step size 0.00375, perturbation std 0.025, with top-x% selection (simulation kept 30%, real 60%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BGS, via orthogonal perturbations and a reward-difference-based elite selection, was crucial for producing policies in simulation that could be transferred and fine-tuned on the real robot; other standard RL algorithms tried (PPO, QT-OPT, SAC) did not transfer well in this work. Robust optimization and variance-reduction methods in ES matter for sim-to-real success when simulation fidelity is imperfect.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 1)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 1)</em></li>
                <li>Humanoid robots learning to walk faster: From the real world to simulation and back <em>(Rating: 1)</em></li>
                <li>Sim2real in robotics and automation: Applications and challenges <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1630",
    "paper_id": "paper-250526228",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "i-S2R",
            "name_full": "Iterative Sim-to-Real",
            "brief_description": "An iterative pipeline that bootstraps a coarse human behavior model from initial real-world data, trains robot policies in simulation against that model, fine-tunes the policy with humans in the loop on the real robot, updates the human model with newly collected real interactions, and repeats this loop to improve sim-to-real transfer for cooperative human-robot tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ABB IRB 120T + Festo linear actuator table tennis robot",
            "agent_system_description": "An 8-DOF system: ABB IRB 120T 6-DOF robotic arm mounted on a 2D Festo linear actuator with a table-tennis paddle end-effector; controlled at high frequency (policy outputs 8 joint velocities at 75Hz, ABB EGM at ~248Hz and Festo at up to 125Hz). Used to play cooperative table tennis rallies with human players.",
            "domain": "human-robot interaction / robotic table tennis (general robotics)",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "Custom PyBullet simulation replicating the robot, contact dynamics and table environment; ball trajectories are simulated with Newtonian dynamics including gravity and drag (no spin); simulation incorporates modeled sensor/action latencies and injected observation noise.",
            "simulation_fidelity_level": "approximate physics simulation (robot and contact dynamics modeled in PyBullet; ball dynamics with drag modeled; sensor/actuator latency and observation noise simulated) — not high-fidelity (spin and some perception/physics details omitted).",
            "fidelity_aspects_modeled": "robot rigid-body dynamics and contact dynamics (via PyBullet), ball Newtonian flight with air drag (Cd, density, mass, cross-sectional area), restitution/friction empirically estimated, sensor observation noise (uniform noise), and component latencies modeled as Gaussians.",
            "fidelity_aspects_simplified": "ball spin is not modeled; perception (vision) imperfections other than added noise not fully modeled; some surface/material effects, wear-and-tear of balls, precise calibration mismatches, and correlation between subsequent human shots were simplified/omitted; human behavior modeled as independent uniform sampling from estimated start/velocity/landing ranges rather than a full probabilistic or sequential model.",
            "real_environment_description": "Physical table tennis setup with the 8-DOF ABB+Festo robot, stereo Ximea cameras tracking ball at 125Hz feeding a 3D tracker (interpolated to 75Hz), human participants (non-professional) playing cooperative rallies; real-world fine-tuning done live with humans in the loop (~2 hours per cycle).",
            "task_or_skill_transferred": "closed-loop high-frequency joint control for cooperative table tennis rallying (returning balls to human reliably, maximizing rally length)",
            "training_method": "reinforcement learning using an Evolutionary Search method (Blackbox Gradient Sensing, BGS) in simulation with iterative sim/real loops; policies warm-started between rounds and fine-tuned on-robot with humans.",
            "transfer_success_metric": "rally length (number of successive paddle touches in a rally); additional metrics: robot hit/return rate and normalized rally length (z-scored per-player).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "On the physical robot i-S2R policies achieved average rallies of ~22 successive hits (mean) with best observed rallies up to 150 hits; aggregated improvement over S2R+FT was ~9% in mean rally length, and for 80% of players rally lengths were 70% to 175% longer compared to the S2R+FT baseline. Performance varied by player skill (strong gains for beginner/intermediate, negative or mixed for an advanced player).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Not standard broad dynamics randomization, but the simulation injected: uniform position noise on ball observations (2x ball diameter), parameterized Gaussian sensor/action latencies sampled per episode (measured µ and σ per component), empirically estimated variability in restitution/friction parameters, and removal of detection outliers when building human ball distributions; human throw distribution was sampled uniformly across inferred min/max ranges.",
            "sim_to_real_gap_factors": "Missing ball spin modeling, perception and calibration mismatches (vision detection limits, tracker errors), variability in estimated delays, surface/material differences and wear (balls/table/paddle), simplified human behavior (independent uniform sampling, no sequential correlation), and residual differences in dynamics not captured by PyBullet.",
            "transfer_enabling_conditions": "iteratively improving a human behavior model from real interaction data; explicit modeling of sensor and action latency in simulation (necessary — transfer failed without latency modeled); injection of observation noise; empirical estimation of contact/friction/restitution parameters; warm-starting simulation training from real fine-tuned weights; on-robot fine-tuning with humans in the loop; using an ES-based optimizer (BGS) that transferred reliably; and multiple sim↔real iterations to broaden the training distribution.",
            "fidelity_requirements_identified": "Paper identifies accurate modeling of sensor/actuation latencies as critical (transfer 'completely failed' without it); contact dynamics and observation noise are important; modeling ball spin was identified as a missing fidelity requirement that likely limits transfer for advanced players; accurate human-behavior coverage (human ball distributions) is important to reduce fine-tuning time.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Fine-tuning on the real robot with human partner each iteration: 60 updates first cycle, 70 updates subsequent cycles, totalling 200 parameter updates per player (~2 hours wall-clock per cycle). S2R-Oracle+FT used only 35% of i-S2R's fine-tuning budget as an ablation.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Iteratively updating a simulated human behavior model using real human-robot interaction data (i-S2R) improves sim-to-real transfer for cooperative HRI tasks: i-S2R yields longer and more frequently enjoyable rallies than single-shot S2R+FT, especially for beginner and intermediate human players, and generalizes better to new players. Modeling sensor/actuation latency and injecting observation noise were necessary for transfer; missing simulation of ball spin and other unmodeled factors limited performance for very skilled (advanced) play. Much of i-S2R's benefit comes from progressively improving the human behavior model; if the final human model were known upfront (S2R-Oracle), similar final performance could be reached with less real fine-tuning time.",
            "additional_notes": "The human behavior model represents per-trajectory initial position/velocity and landing ranges (16 numbers) derived by fitting simulated drag-based trajectories (Nelder-Mead) and removing outliers (DBSCAN); the simulation samples uniformly from these ranges, losing within-range probability structure and temporal correlation between rally shots.",
            "uuid": "e1630.0"
        },
        {
            "name_short": "BGS",
            "name_full": "Blackbox Gradient Sensing",
            "brief_description": "An evolutionary-search (ES) style black-box optimizer introduced in this paper that uses orthogonal perturbation ensembles and a novel elite-choice selection (ranking perturbation directions by positive/negative reward difference) to estimate policy gradients robustly for training RL policies that transfer to a real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "same ABB IRB 120T + Festo table tennis robot (as used with i-S2R)",
            "agent_system_description": "See i-S2R entry; BGS was the only policy optimization method in this work that yielded policies that transferred to the physical robot reliably.",
            "domain": "robotics reinforcement learning / sim-to-real policy optimization",
            "virtual_environment_name": "PyBullet (training in-simulator)",
            "virtual_environment_description": "See i-S2R entry; BGS was applied to rollouts collected in the PyBullet simulation and also during on-robot fine-tuning using real rollouts.",
            "simulation_fidelity_level": "approximate physics (same as i-S2R use case); BGS is an optimizer and does not change fidelity but its sampling/perturbation strategy affected training stability and transfer.",
            "fidelity_aspects_modeled": "Irrelevant to optimizer (see i-S2R for simulation modeling); BGS leverages repeated rollouts per perturbation to reduce variance and used orthogonal perturbations to improve convergence under the given simulated fidelities.",
            "fidelity_aspects_simplified": "Not applicable (optimizer-level).",
            "real_environment_description": "Used for on-robot fine-tuning with humans in the loop; same hardware and human evaluation described under i-S2R.",
            "task_or_skill_transferred": "policy parameters trained in simulation using BGS were transferred to the robot and then fine-tuned on-robot; BGS-enabled policies successfully transferred to perform cooperative table tennis control.",
            "training_method": "Evolutionary search (ES) with antithetic sampling, orthogonal Gaussian perturbation ensembles, reward-normalization, perturbation filtering, and a novel elite-choice algorithm ranking perturbations by the reward difference between positive and negative directions.",
            "transfer_success_metric": "same rally-length metrics as i-S2R; empirically BGS-trained policies transferred reliably while policies trained with PPO, QT-OPT, SAC did not transfer well to the robot.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Other RL algorithms (PPO, SAC, QT-OPT) failed to produce on-robot-deployable policies in this work, indicating that optimizer stability and variance reduction (orthogonal samples, elite-choice based on reward deltas, repeated rollouts) were key to obtaining policies robust to the sim-to-real gap. The sim-to-real gap factors themselves (missing spin, latency, perception mismatch) remain relevant but BGS improved convergence despite those gaps.",
            "transfer_enabling_conditions": "BGS's orthogonal perturbations (variance reduction) and novel elite-choice ranking by reward differences (curvature-aware selection) enabled stable training in simulation and produced policies that could be successfully fine-tuned on the robot; repeating rollouts per perturbation and normalization techniques further reduced variance.",
            "fidelity_requirements_identified": "While BGS improved optimization robustness, the paper emphasizes that proper simulation modeling (latency, noise) remains necessary; BGS alone does not eliminate the need for accurate latency/noise modeling.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "BGS hyperparameters reported: simulation used 200 perturbations with 15 rollouts per perturbation; real fine-tuning used 5 perturbations with 3 rollouts per perturbation, step size 0.00375, perturbation std 0.025, with top-x% selection (simulation kept 30%, real 60%).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "BGS, via orthogonal perturbations and a reward-difference-based elite selection, was crucial for producing policies in simulation that could be transferred and fine-tuned on the real robot; other standard RL algorithms tried (PPO, QT-OPT, SAC) did not transfer well in this work. Robust optimization and variance-reduction methods in ES matter for sim-to-real success when simulation fidelity is imperfect.",
            "uuid": "e1630.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 1,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 1,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Humanoid robots learning to walk faster: From the real world to simulation and back",
            "rating": 1,
            "sanitized_title": "humanoid_robots_learning_to_walk_faster_from_the_real_world_to_simulation_and_back"
        },
        {
            "paper_title": "Sim2real in robotics and automation: Applications and challenges",
            "rating": 1,
            "sanitized_title": "sim2real_in_robotics_and_automation_applications_and_challenges"
        }
    ],
    "cost": 0.014554249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>i-Sim2Real: Reinforcement Learning of Robotic Policies in Tight Human-Robot Interaction Loops</p>
<p>Saminda Abeyruwan saminda@google.com 
Robotics at Google</p>
<p>Laura Graesser lauragraesser@google.com 
Robotics at Google</p>
<p>David B D&apos;ambrosio 
Robotics at Google</p>
<p>Avi Singh singhavi@google.com 
Robotics at Google</p>
<p>Anish Shankar 
Robotics at Google</p>
<p>Alex Bewley bewley@google.com 
Robotics at Google</p>
<p>Deepali Jain jaindeepali@google.com 
Robotics at Google</p>
<p>Krzysztof Choromanski 
Robotics at Google</p>
<p>Pannag R Sanketi psanketi@google.com 
Robotics at Google</p>
<p>i-Sim2Real: Reinforcement Learning of Robotic Policies in Tight Human-Robot Interaction Loops
sim-to-realhuman-robot interactionreinforcement learning
Sim-to-real transfer is a powerful paradigm for robotic reinforcement learning. The ability to train policies in simulation enables safe exploration and large-scale data collection quickly at low cost. However, prior works in sim-to-real transfer of robotic policies typically do not involve any human-robot interaction because accurately simulating human behavior is an open problem. In this work, our goal is to leverage the power of simulation to train robotic policies that are proficient at interacting with humans upon deployment. But there is a chicken and egg problem -how to gather examples of a human interacting with a physical robot so as to model human behavior in simulation without already having a robot that is able to interact with a human? Our proposed method, Iterative-Sim-to-Real (i-S2R), attempts to address this. i-S2R bootstraps from a simple model of human behavior and alternates between training in simulation and deploying in the real world. In each iteration, both the human behavior model and the policy are refined. For all training we apply a new evolutionary search algorithm called Blackbox Gradient Sensing (BGS). We evaluate our method on a real world robotic table tennis setting, where the objective for the robot is to play cooperatively with a human player for as long as possible. Table tennis is a high-speed, dynamic task that requires the two players to react quickly to each other's moves, making for a challenging test bed for research on human-robot interaction. We present results on an industrial robotic arm that is able to cooperatively play table tennis with human players, achieving rallies of 22 successive hits on average and 150 at best. Further, for 80% of players, rally lengths are 70% to 175% longer compared to the sim-to-real plus fine-tuning (S2R+FT) baseline. For videos of our system in action please see https://sites.google.com/view/is2r.</p>
<p>Introduction</p>
<p>Sim-to-real transfer has emerged as a dominant paradigm for learning-based robotics. Real world training is often slow, cost-prohibitive, and poses safety-related challenges, so training in simulation is an attractive alternative and has been explored for a number of real world tasks, including object manipulation [1,2,3,4], legged robot locomotion [5,6], and aerial navigation [7,8]. However, one element that is missing in this prior work is that the policies are not trained to be proficient at interacting with humans upon deployment. The utility of sim-to-real learning can be greatly increased if we extend it to settings where the trained policies need to interact with humans in a close, tight-loop fashion upon deployment. One of the major promises of learning-based robotics is to deploy robots in human-occupied settings, since non-learning robots already work well in deterministic, non-human occupied settings, such as factory floors. However, simulating human behavior is non-trivial (and indeed, one of the primary goals of artificial intelligence research), making it a major bottleneck in sim-to-real research for tasks involving human-robot interaction.</p>
<p>One approach to simulating human behavior is imitation learning. Given a few examples of human behavior, we can use techniques such as behavior cloning [9,10], or inverse reinforcement learning [11,12] to distill that behavior into a policy, and then use these policies to generate human * Indicates equal contribution. behavior in simulation. However, this approach presents a chicken and egg problem: in order to obtain useful examples of human behavior (in the context of human-robot interaction), we need a robot policy that already knows how to interact with humans in the real world, but we cannot learn such a policy without the ability to simulate human behaviors in the first place. The primary contribution of this paper is a practical solution to this problem.</p>
<p>Our proposed method involves learning a coarse model of human behavior from initial data collected in the real world to bootstrap reinforcement learning of robotic policies in simulation. Deploying this learned policy in the real world now allows us to collect data in which the human subjects meaningfully interact with the robot. We then use this real world experience to improve our human behavior model, and continue training the robot policy in simulation under this updated model. We repeat this iterative process until a desired level of performance is achieved. We present results on a task involving a robot playing table tennis with non-professional human players (see Figure 1). The goal for the robot is to maximize rally length, i.e. the number of successive hits by the robot and human before the ball goes out of play and policies are evaluated using rally length. Table  tennis is a high-speed, dynamic task that requires close, tightloop interactions between two players (in this case, a human and a robot). Further, maximizing rally length requires the robot to cooperate with a human, and vice versa. Thus we believe it to be a good instantiation of our problem setting. We build an initial model of the human player's ball trajectories without a robot present and iteratively refine the robot and player models as they play together, ultimately resulting in a robot policy that can hold rallies of 22 successive hits on average and 150 at best.</p>
<p>While we demonstrate our approach on table tennis, we believe that our overall pipeline can be applied to a broad range of tasks, and take into account the various nuances of those tasks. The two characteristics a human behavior model needs to be compatible with our approach are (a) it can be updated using human data that is gathered whilst a human or humans are interacting with a robot, and (b) the model can be used to sample human behavior in simulation.</p>
<p>In summary, the primary contributions of this paper are: (a) a framework for training robotic policies in simulation that would need to interact with human subjects upon deployment, (b) a real world instantiation of this framework on a high-speed, dynamic task requiring tight, closed-loop interactions between humans and robots, (c) a detailed assessment of how our method, which we call Iterative-Sim-to-Real (i-S2R), compares with a baseline sim-to-real approach in the domain of cooperative robotic table tennis, and (d) the first robotic table tennis policy trained to control robot joints using reinforcement learning (RL) that can handle a wide variety of balls and can rally consistently with non-professional humans. i-S2R can apply any RL method, however the only policy-optimization algorithm that so far led to the on-robot-deployable policies is the so-called Blackbox Gradient Sensing (BGS) that we introduce here. For videos of our system in action please see https://sites.google.com/view/is2r.</p>
<p>(1) system ID with a physical robot in the loop, (2) dynamics randomization, (3) simulated latency, and (4) more complex networks. Similarly to Lee et al. [13], we use a 1D CNN to represent control policies. Yet a sim-to-real gap persists. Continuing to train in the real world [24,25,26,27] (known as fine-tuning) is an effective way to bridge the remaining gap since the policy can adapt to changes in the environment. We also utilize fine-tuning in this work, but unlike most past work, our learned policy is expected to interact cooperatively with a real human during this fine-tuning phase.</p>
<p>The closest sim-to-real approaches in prior work are Chebotar et al. [2] and Farchy et al. [23] since they update simulation parameters based on multiple iterations of real world data collection interleaved with simulated training. However, both of these prior works focus on using real world interaction data to learn improved physical parameters for the simulator, whereas our method focuses on learning better human behavior models. Unlike these prior works, our learned policies are proficient at interacting with humans upon deployment in the real world.</p>
<p>Reinforcement Learning for Table Tennis Robotic table tennis is a challenging, dynamic task [28] that has been a test bed for robotics research since the 1980s [29,30,31,32,33]. The current exemplar is the Omron robot [34]. Until recently, most methods tackled the problem by identifying a virtual hitting point for the racket [35,36,37,38,39,40,41,42]. These methods depend on being able to predict the ball state at time t either from a ball dynamics model which may be parameterized [35,36,43,44] or by learning to predict it [33,38,39]. This results in a target paddle state or states and various methods are used to generate robot joint trajectories given these targets [33,35,36,43,44,45,46,47,48,49,50]. More recently, Tebbe et al. [51] learned to predict the paddle target using RL.</p>
<p>An alternative line of research seeks to do away with hitting points and ball prediction models, instead focusing on high frequency control of a robot's joints using either RL [28,39,52] or learning from demonstrations [46, 53,54]. Of these, Büchler et al. [28] is the most similar, training RL policies to control robot joints from scratch at high frequencies given ball and robot states as policy inputs. However Büchler et al. [28] restricts the task to playing with a ball thrower on a single setting, whereas we focus on the harder problem of cooperative play with different humans.</p>
<p>Most prior work simplifies the problem by focusing on play with a ball thrower. Only a few [46, 49, 51, 55] focus on cooperative rallying with a human. Of these, Tebbe et al. [51], is the most similar, evaluating policies on various styles of human-robot cooperative play. However, Tebbe et al. [51] simplify the environment to a single-step bandit and the policy learns to predict the paddle state given the ball state at a pre-determined hit time t. In contrast, we learn closed-loop policies that operate at a high frequency (75Hz), removing the need for a learned policy to accurately predict where the ball will be in the future, increasing the robustness of the system, and enabling more dynamic play.</p>
<p>Human Robot Interaction</p>
<p>Although not a typical HRI benchmark, cooperative robotic table tennis exhibits many of the features studied in the field: a human and robot working together, complex interactions between the two, inferring actions based on non-explicit cues, and so on. A major challenge in HRI is effectively modeling the complexities of human behavior in simulation [56] in order to learn without requiring an actual human. We employ several common techniques from HRI to learn in simulation such as simplifying the human model [57], specialized models for specific players [58], and refining our model based on real world interactions. Finally we note that like us, Paleja et al. [59] found policy performance varied depending on the skill of the human player.</p>
<p>Preliminaries</p>
<p>Problem Setting We consider the problem of cooperative human-robot table tennis as a singleagent sequential decision making problem in which the human is a part of the environment. We formalize the problem as a Markov Decision Process (MDP) [60] consisting of a of a 4-tuple (S, A, R, p), whose elements are the state space S, action space A, reward function R : S × A → R, and transition dynamics p : S × A → S. An episode (s 0 , a 0 , r 0 , ..., s n , a n , r n ) is a finite sequence of s ∈ S, a ∈ A, r ∈ R elements, beginning with a start state s 0 and ending when the environment terminates. We define a parameterized policy π θ : S → A with parameters θ. The objective is to maximize E N t=1 r(s t , π θ (s t )) , the expected cumulative reward obtained in an episode under π θ .</p>
<p>We make two simplifications to our problem. First, we focus on rallies starting with a hit instead of a table tennis serve to make the data more uniform. Second, an episode consists of a single ball throw and return. Policies are therefore rewarded based on their ability to return balls to the opposite side of 3  Figure 2: Iterative-Sim-to-Real. left We start with a coarse bootstrap model of human behavior (shown in yellow), and use it to train an initial robot policy in simulation. We then fine-tune this policy in the real world against a human player, and the human interaction data collected during this period is used to update the human behavior model used in simulation. We then take the fine-tuned policy back to simulation to further train it against the improved human behavior model, and this process is repeated until robot and human behaviors converge. right Specific i-S2R details used in this work. x-axis represents the training iterations in sim, y-axis represents the fine-tuning iterations in real with human-in-the-loop. Model names are in italics.</p>
<p>the table. This reward structure encourages longer rallies, as an agent that can return any ball can also rally indefinitely provided the simulated single shots overlap with the real rally shots.</p>
<p>BGS &amp; Evolutionary Search (ES)</p>
<p>i-S2R is compatible with any RL algorithm. In initial experiments we tried a range of methods -PPO [61], QT-OPT [62], SAC [63], and Blackbox Gradient Sensing (BGS) that we introduce here. Only BGS transferred well to a physical robot, hence we continued with this approach and we leave to future work more exhaustive research on other RL algorithms. BGS is an ES-method [64,65,66,67,68,69] which have been shown to be an effective strategy for solving MDPs [66,68]. ES methods aim to optimize the smoothened version F σ (θ) of the original RL-objective F (θ), where θ stands for the policy parameters, given (for the parameter σ &gt; 0) as:
F σ (θ) = E δ∼N (0,I d ) [F (θ + σδ)].(1)
Different ES algorithms apply different Monte-Carlo strategies to approximate the gradient of F σ (θ).</p>
<p>In BGS, following [64] we choose Monte Carlo samples δ i to form orthogonal-ensembles (to reduce the variance of the estimation) and apply a novel technique for choosing a final collection of samples δ i for gradient estimation (the so-called elite-choice process). The former technique improved convergence in training and the latter was crucial for the overall effectiveness of training -training in simulation failed without it. See Appendix B for details.</p>
<p>Method</p>
<p>i-S2R consists of two core components: (1) an iterative procedure for progressively updating and learning from a human behavior model -the human ball distribution in this setting -and (2) a method for modeling human behavior in simulation given a dataset of human play gathered in the real world (see Figure 2 for an overview). We first describe our iterative training procedure, and then discuss how we model human ball distributions.</p>
<p>Iterative Training Procedure An overview of the method can be seen in Figure 2. First we gather an initial dataset, D 0 , from player P hitting table tennis balls across the table without a robot doing anything. From D 0 , we build our first human behavior model M 0 that defines a ball distribution (see below). A robot policy is trained in simulation to return balls sampled from M 0 . Once the policy has converged, we transfer the parameters, θ 0S , to a real robotic system. The model is fine-tuned whilst player P plays cooperatively (i.e. trying to maximize rally length) with the robot for a fixed number of parameter updates to produce θ 0R . All of the human hits during this fine-tuning phase are added to D 0 to form D 1 , which is used to define M 1 . The policy weights, θ 0R , are then transferred back to simulation and training is continued with the new distribution M 1 . After training in simulation, the policy weights θ 1S are transferred back to the real world. The fine-tuning process is repeated to produce the next set of policy parameters θ 1R , dataset D 2 , and human model M 2 . This process can be repeated as many times as needed.</p>
<p>A useful check for assessing convergence was found by looking at the delta in our human behavior model from one iteration to the next. We found the delta between M 1 and M 2 was substantially smaller than between M 0 and M 1 indicating that three iterations were enough for this task. For details on the ball distribution parameters for different players see subsection C.3.</p>
<p>Modeling Human Ball Distributions One of our primary goals is to simulate human player behaviors from a set of real world ball trajectories that have been subjected to air drag, gravity, and spin. Due to perception challenges in the real world, we do not explicitly model spin. The input to this procedure is a dataset of ball trajectories, where each trajectory consists of a sequence of ball positions. The output is a uniform ball distribution defined by 16 numbers: the minimum and maximum initial ball position (6), velocity (6), and x and y ball landing locations on robot side (4).</p>
<p>The ball distribution is derived from the dataset in two stages. The first step is to estimate a ball's initial position and velocity for each trajectory. We do this by selecting the free flight part of the trajectory (before the first bounce) and minimize the Euclidean distance between the simulated and real trajectory using the Nelder-Mead method [70]. Please see subsection C.4 for details on the model used to simulate a ball trajectory.</p>
<p>Next we remove outliers using DBSCAN [71] and take the minimum and maximum per dimension to define the ball distribution. We sample an initial position and velocity from this distribution and generate a ball trajectory in simulation subject to the drag force. Other parameters needed for the simulation, such as the coefficient of restitution, friction between the table and ball and the robot paddle and the ball, and so on have been empirically estimated following [72,73].</p>
<p>System, Simulation, and MDP Details</p>
<p>Our real world robotic system (see Figure 1) is a combination of an ABB IRB 120T 6-DOF robotic arm mounted to a two-dimensional Festo linear actuator, creating an 8-DOF system, with a table tennis paddle mounted on the end-effector. The 3D ball position is estimated via a stereo pair of Ximea MQ013CG-ON cameras from which we process 2D detections, triangulate to 3D, and filter through a 3D tracker. See Appendix D for more details. We concatenate the ball position with the 8-DOF robot joint angles to form an 11-dimensional observation space. Along with the current observation, we pass the past seven observations (a state space of 8 × 11) as the input to the policy. The policy controls the robot by outputting eight individual joint velocities at 75Hz. Following Gao et al.</p>
<p>[52] we use a 3-layer 1-D dilated gated convolutional neural network as our policy architecture. Details of the policy architecture can be found in Appendix E.</p>
<p>Our simulation is built on the PyBullet [74] physics engine replicating our real environment. We use PyBullet to model robot and contact dynamics whilst balls are modeled as described in section 4. We add random uniform noise of 2× the diameter of a table tennis ball to the ball observation per timestep to aid transfer to a physical system. We also found it necessary to simulate sensor latency, otherwise sim-to-real transfer completely failed. Robot actions as well as ball and robot observation latencies are modeled as parameterized Gaussians based on measurements from the real system. Policies are rewarded for hitting balls and for returning balls in a cooperative manner. See Appendix G for details.</p>
<p>Experimental Results</p>
<p>Experimental Setup To evaluate our method, we completed the procedure described in section 4 for five different non-professional table tennis players, thus training five independent i-S2R policies. We compare i-S2R with two baselines. First, the standard sim-to-real (S2R) baseline in which a policy is transferred zero-shot from simulation [1,3,5,6,7,8]. Second, a stronger baseline of S2R plus fine-tuning (S2R+FT) in which a policy is transferred in simulation and training is continued in the real world. For fair comparison, S2R+FT is given the same real world training budget as i-S2R. We follow the approach in [24] using the same training algorithm throughout and implement an automatic reset for autonomous training. Finally, each player trained a S2R-Oracle+FT policy which was trained in simulation on the penultimate human behavior model obtained through i-S2R and fine-tuned in the real world for 35% of the i-S2R training budget. This is equivalent to the last round of fine-tuning for i-S2R. (See Figure 2 right). S2R-Oracle+FT is intended to isolate the effect of the human behavior modeling on final performance, enabling us to better understand what aspects of the i-S2R process matter. Each policy was evaluated by the model's trainer. Select policies were cross-evaluated by two other players. All policies were tested in random order and the identity of the model was kept hidden from the evaluator ("blind eval"). Further details can be found in Appendix H. left When aggregated across all players, i-S2R rally length is higher than S2R+FT by about 9%. However, note that simple aggregation puts extra weight on higher skilled players that are able to hold a longer rally. center The normalized rally length distribution (see Appendix J for normalization details) shows a bigger improvement between i-S2R and S2R+FT in terms of the mean, median and 25th and 75th percentiles. right The histogram of rally lengths for i-S2R and S2R+FT (250 rallies per model) shows that a large fraction of the rallies for S2R+FT are shorter (i.e. less than 5), while i-S2R achieves longer rallies more frequently.  : Results by player skill. When broken down by player skill, we notice that i-S2R has a substantially longer rally length than S2R+FT and is comparable to S2R-Oracle for beginner and intermediate players. The advanced player is an exception. Note, S2R-Oracle+FT gets just 35% of i-S2R and S2R+FT fine-tuning budget.</p>
<p>Due to the time needed to train and evaluate i-S2R, S2R+FT, and S2R-Oracle+FT (roughly 20 hours per person) we note that 4 of the 5 players are authors on this paper. The non-author player's results appear consistent with our overall findings (see Appendix K for details).</p>
<p>(1) Does i-S2R improve over S2R+FT in a human-robot interactive setting? Figure 3 presents rally length distributions aggregated across all players whilst Figure 4 splits the data by skill. Players are grouped into beginner (40% players), intermediate (40% of players) and advanced (20% players). The non-author player was classified as beginner. Please see Appendix I for skill level definitions. When aggregated over all players, we see that i-S2R is able to hold longer rallies (i.e. rallies that are longer than length 5) at a much higher rate than S2R+FT, as shown in Figure 3. When the players are split by skill level, i-S2R significantly outperforms S2R+FT for both beginner and intermediate players (80% of the players). The improvement differs between the two groups, with i-S2R yielding a ≈ 70% and ≈ 175% improvement for beginner and intermediate players respectively.</p>
<p>The policy trained by the advanced player has a different trend. Here, S2R+FT dramatically outperforms i-S2R. We hypothesize that a good S2R model plays a large part in the strong performance of S2R+FT since better transfer from simulation improves the efficiency of subsequent fine-tuning (see Figure 5). One possible explanation for the poor performance of i-S2R is that the policy played fast. During evaluations, we observe the initial robot return is fast with top spin, likely due to a combination of changes in the behavior model from iteration 1 to 2 and 3 and inherent randomness in the training process. In response, the advanced player returns the ball even faster, also with top spin. This appears challenging for the robot to return. During evaluation, most of the errors are made by the robot, where the rally ends with the ball going over the human player's end of the table. This suggests that fine-tuning was not able to adjust in time to the top spin and fast speed of play, causing the robot to hit over the table. One way to mitigate this would be to model spin in simulation, so the policy could learn to respond to spin throughout training, not just during fine-tuning. However, due to the time consuming nature of repeating experiments on the physical system it is difficult to fully explain this result, especially since both the training methodology and involvement of humans introduces a high degree of variance. . "S2R-Oracle-sim-3" here is same as "S2R-Oracle" in Figure 4.
X vel Y vel Z vel X start Y start Z start X land Y land 0 1 2 3
Delta between sim1 and sim2 parameter ranges
X vel Y vel Z vel X start Y start Z start X land Y land
Delta between sim2 and sim3 parameter ranges The key distribution parameters change substantially from initial ball distribution (sim1) to that after 1st round of sim training (sim2). This is to be expected given we start from a simple human model (hits across the table). The change in parameters between 1st and 2nd round of sim training is much less (sim2 vs. sim3).</p>
<p>(2) How many sim-to-real iterations does the human behavior model take to converge? For beginners we find that it only took two iterations for i-S2R to converge (see Figure 5). In the leftmost chart showing beginner policy data, i-S2R achieves comparable levels of performance at the end of the 2nd (fine-tune-65%) and final (fine-tune-100%) iterations. However, for intermediate skilled players this is not the case. The human behavior model from iteration to iteration ( Figure 6) offers a clue. For beginner players, the distribution barely changes after the 2nd round as evidenced by the difference between the left and right charts. Whereas for intermediate players the distribution continues to change substantially from round 2 to 3 (specifically in y and z velocities), which is perhaps why we see the strongest performance of i-S2R after the 2nd iteration for beginners but after the 3rd iteration for intermediate players.</p>
<p>The advanced player's distribution hardly changes between the 2nd and 3rd round and the performance of i-S2R is comparable across both. However this does not explain why we observed the best i-S2R performance at the end of the 1st round for this player. Investigating the effect of playing style on changes in ball distribution every iteration and hence on the sim-to-real gap or training for more iterations for advanced players can shed light on this in future work.</p>
<p>(3) What is the impact of the human behavior model? For beginner and intermediate players, S2R-Oracle+FT is in line with i-S2R performance. However S2R-Oracle+FT also achieved this level of performance with just 35% of the real world training time compared to i-S2R and S2R+FT. Therefore much of the benefit of i-S2R likely comes from improving the human behavior model from iteration to iteration. It also suggests that if we had access to the final human behavior model at the beginning of training, the iterative sim-to-real training would not be needed. We could simply fine-tune in the real world and achieve comparable performance with substantially less human training time. S2R-Oracle+FT's strong performance also validates our motivation for this work, in which we hypothesized that the difficulty of defining a good human behavior model a priori for human-robot cooperative rallies was limiting performance.</p>
<p>This result indicates that i-S2R does not benefit from additional training iterations in simulation over and above the improvements to the human behavior model. The evaluations at earlier stages in training (shown in Figure 5) suggest the remaining sim-to-real gap could be responsible. Figure 5 shows that, in all cases, after both the second (sim-2) and third (sim-3) rounds of simulated training, rally length drops noticeably. Reducing the sim-to-real gap might improve i-S2R's performance due to better starting points for the last two rounds of fine-tuning. (4) Does i-S2R offer any generalization benefits in this setting?</p>
<p>We evaluate the generalization capabilities of models trained with i-S2R, and how they compare against models trained using S2R+FT by conducting cross-evaluations. A "cross-evaluation" of a policy is an evaluation conducted by a human who did not play with the policy during training. Each of the 5 policies was cross-evaluated by randomly selecting 2 other humans from the human-subject pool and averaging the results. As shown in Figure 7, i-S2R substantially outperforms S2R+FT when the models are cross-evaluated by other players (with similar blind evaluations as earlier) including for the advanced player where S2R+FT was best in self evaluation (see Appendix K for details by player). This observation holds whether we look at absolute or normalized rally length (see Appendix J for normalization methodology). Performance with other players is lower for all models, however i-S2R maintains around 70% of performance on average compared to 30% for S2R+FT. We hypothesize that the broader training distribution obtained by iterating between simulation and reality leads to policies that can deal with a wider range of ball throws, leading to better generalization to new players. Our confidence in this hypothesis is strengthened by the fact that S2R-Oracle+FT also outperforms S2R+FT in this setting.</p>
<p>Limitations</p>
<p>Having a human in the loop poses numerous challenges to robotic reinforcement learning. It slows down the overall learning process to accommodate human participants, and limits the scale at which one can experiment. As one example, while we tested our method on five subjects, time limitations prevented us from training with multiple random seeds for each subject. There is significant variation in how people interact with robots (or sometimes even the same person over time), which introduces extra variance into our experiments. In our experiments, the trends we saw for one particular subject were substantially different from all other subjects, and we could not fully explain why.</p>
<p>It is possible for an expert human player to achieve long rallies by keeping the ball in a very narrow distribution without really improving the inherent capability of the agent to play beyond those balls. In our studies, since we used non-professional players, this was not an issue.</p>
<p>Another limitation arising from training a policy with a human in the loop is the possibility that some performance improvements are attributable to human learning and not policy learning. We did our best to mitigate this by asking players to evaluate all models "blind" (i.e. the player is unaware of what model they are evaluating) and at the end of training, after which the majority of human learning was likely to have occurred. Consequently, we think that differences between models reflect differences in policy capability and not human capability.</p>
<p>Finally, we represent humans in simulation in a simple way -by capturing all initial position and velocity ranges during their play -and then we sample each ball in simulation uniformly and independently. This ignores the probability distribution of balls within those ranges and also results in a loss of correlation between subsequent balls in a rally. The behavior model also omits spin and human attributes such as stamina, skill level, intention, and curiosity. These could be addressed by developing a more sophisticated behavior model that takes these factors into account.</p>
<p>Conclusion</p>
<p>We present i-S2R to learn RL policies that are able to interact with humans by iteratively training in simulation and fine-tuning in the real world with humans in the loop. The approach starts with a coarse model of human behavior and refines it over a series of fine-tuning iterations. The effectiveness of this method is demonstrated in the context of a table tennis rallying task. Extensive "blind" experiments shed light on various aspects of the method and compare it against a baseline where we train and fine-tune in real only once (S2R). We show that i-S2R outperforms S2R in aggregate, and the difference in performance is particularly significant for beginner and intermediate players (4/5). Moreover, i-S2R generalizes much better than S2R to other players.</p>
<p>[31] J. Hartley. Toshiba progress towards sensory control in real time. The Industrial Robot 14-1, pages 50-52, 1983.</p>
<p>[32] H. Hashimoto, F. Ozaki, and K. Osuka. Development of ping-pong robot system using 7 degree of freedom direct drive robots. In Industrial Applications of Robotics and Machine Vision, 1987.</p>
<p>[33] K. Muelling, J. Kober, and J. Peters. A biomimetic approach to robot table tennis. Adaptive Behavior, 2010.</p>
<p>[34] A. Kyohei, N. Masamune, and Y. Satoshi. The ping pong robot to return a ball precisely. 2020.</p>
<p>[ </p>
<p>A Author Contributions</p>
<p>• Saminda Abeyruwan co-led the project, developed i-S2R, ran initial i-S2R experiments, helped build and maintain robotic infrastructure, introduced a set of additional rewards to help with fine-tuning, helped with analyzing the sim-to-real discrepancies, organized the human training and evaluation protocols, and helped to write parts of the paper related to human behavior modeling via ball trajectories. Was one of the test subjects. • Laura Graesser co-led the project, ran initial i-S2R experiments, advised on experimental design, helped build and maintain robotic infrastructure, analyzed the results, wrote the paper. • David B. D'Ambrosio helped build and maintain robotic infrastructure and vision system.</p>
<p>Analysis of human behavior parameters. Literature review. Post-hoc evaluation of humanrobot rallies. Discussion and writing of paper. • Avi Singh wrote the introduction and helped craft the overall narrative for the paper. Made the project website. Was one of the test subjects. • Anish Shankar worked on the system's hardware and software implementation, data infrastructure used in analysis and overall system performance. Was one of the test subjects. </p>
<p>B Details on the BGS Algorithm</p>
<p>As described in section 3, the ES objective is given by:
F σ (θ) = E δ∼N (0,I d ) [F (θ + σδ)],(2)
where σ &gt; 0 controls the precision of the smoothing, and δ is a random normal vector with the same dimension as the policy parameters θ.</p>
<p>ES does not use derivatives or back-propagation to update policy parameters. Instead, the gradient of the policy parameters θ with respect to the objective is estimated with various Monte Carlo techniques. In this work we apply Monte Carlo leveraging in addition the antithetic sampling trick, widely applied by the community.</p>
<p>Specifically, θ is perturbed either by adding or subtracting Gaussian perturbations δ Ri and completing environment rollouts using the perturbed parameters. As a result each perturbation is associated with a reward, one for each direction R + i and R − i . Assuming the perturbations, δ Ri , are rank ordered with δ R1 being the top performing direction, then the policy update can therefore expressed as follows.
θ = θ + α 1 σ R k i=1 1 m m j=1 R + i,j − 1 m m j=1 R − i,j δ Ri ,(3)
where α is the step size, σ R is the standard deviation of each distinct reward (positive and negative direction), k is the number of top directions (elites), N is the number of directions sampled per parameter update, and k &lt; N . m is the number of repeats per direction and R + i,j is the reward corresponding to the j-th repeat of i-th in the positive direction. R − i,j is the same but in the negative direction.</p>
<p>Our BGS algorithm is built on two pillars that we describe in detail below.</p>
<p>Novel Elite-Choice Algorithm: One of the key features of BGS is the novel algorithm of selecting top directions (the elites). In ARS [68], the ranking of the elites is determined by treating each antithetic direction separately. All rewards are ranked yielding an ordering of directions based on the absolute rewards of either the positive or negative directions (Equation 4). Whereas in BGS we take the difference in rewards between the positive and negative directions and rank the differences to yield an ordering over directions (Equation 5).</p>
<p>ARS : Sort δ Ri by max{R + 1 , ...,
R + i , R − 1 , ..., R − i }.(4)
BGS : Sort δ Ri by max
1 m m j=1 R + 1,j − 1 m m j=1 R − 1,j , ..., 1 m m j=1 R + i,j − 1 m m j=1 R − i,j .(5)
ARS can be interpreted as ranking directions in absolute reward space, whereas BGS ranks directions according to reward curvature because it ranks based on reward deltas. The new elite-choice algorithm was the game changer for all policy-training experiments. We could not train efficient policies with ARS (even in the simulator). The feasibility of such a construction comes from the isotropic property of the Gaussian distribution (see: [64] for details). We observed that orthogonal perturbations led to faster convergence in training.</p>
<p>In addition to our novel-elite choice algorithm and orthogonal perturbations, we apply a number of common approaches used in ES methods; state normalization [66,75], reward normalization [68], and perturbation filtering [66]. We also repeat and average rollouts with the same parameters to reduce variance.</p>
<p>C Iterative-Sim-to-Real Procedure</p>
<p>For the table tennis rallying task, we found 3 iterations to be sufficient. The policy was trained for 30k to 45k updates for the first round of training in simulation since it has to learn everything from scratch. For subsequent simulation rounds, the policy was only trained for 5k updates, since we warm start from latest real world policy weights and its primary task here is adaptation to a change in human behavior. Due to the human cost of real world fine-tuning and evaluation, we did not experiment with shorter or longer training cycles. In the real world, the policy was fine-tuned for 70 parameter updates per cycle for the last two cycles and 60 updates for the first cycle to make 200 updates in total. This is equivalent to approximately 2 hours of wall clock time per cycle, which was our budget per player.</p>
<p>C.1 Seed Selection for Rounds of Simulated Training</p>
<p>We have used the following methodology when training in simulation. When training in simulation is required, whether it is training from scratch or intermediate steps of i-S2R, we train 3 models with 3 different random seeds. Different random seeds were used for different players. When transferring to the physical robot, each model is evaluated for 50 episodes according to the training and evaluation instructions provided in subsection H.1. The model with the highest average return is selected for fine-tuning and further experiments. We have used a simple, sparse reward structure for evaluation: if the robot hits the ball, a reward of +1 is given, and if the ball lands on the human side, an additional +1 is given reward. Therefore, the maximum episode reward is +2. If the robot misses the ball, there is no reward, and if the robot faulted or stopped during an episode, a -2 reward is assigned to the episode.</p>
<p>C.2 Bringing the Fine-Tuned Model from the Real World Back to Simulation</p>
<p>In i-S2R the fine-tuned model from the real world is brought back to simulation in the next iteration for two reasons. First, the fine-tuned model has been trained on the most up to date human behavior. As a result it is likely better adapted to play with the updated human behavior model than the latest set of policy weights from simulated training which were trained on the prior human behavior model.</p>
<p>Second, there are aspects of our real world system which we have not been able to model accurately in simulation on top of the challenges in modeling human behavior. Our vision system does not detect spin, there are calibration defects, variability in estimated delays, conditions of the surface materials, wear and tear (of table tennis balls), physical robot properties mismatched with the simulated robot. Therefore, our policies are subject to a sim2real gap and real world fine-tuning adapts the policy to real world conditions. When we transfer the fine-tuned policy weights back to simulation we observe that some adaptation to real world conditions persist from iteration to iteration, reducing the adaptation time in subsequent fine-tuning iterations.</p>
<p>However it is possible that this approach makes training in simulation more difficult. It would be interesting to compare our approach with a variant in which the policy weights are not transferred back to simulation from the real world. Instead training in simulation would continue using the latest policy weights from the previous iteration but using the latest human model after real fine-tuning. We leave this to future work.   </p>
<p>C.3 Human Behavior Models</p>
<p>C.4 Details on Modeling Human Ball Distributions</p>
<p>We use the model,ẍ t = g −K d ||ẋ t ||ẋ t , x t+1 = x t + ∆t(ẋ t + ∆tẍt 2 ),ẋ t+1 =ẋ t + ∆tẍ t to simulate a trajectory, where (1) x t ,ẋ t , andẍ t denote the position, velocity, and acceleration of the ball at time t, (2) g = −9.81m/s 2 [0, 0, 1] T is the gravity, and (3) K d = C d ρ A 2m . m = 0.0027kg is the ball's mass, ρ = 1.29kg/m 3 is the air density, C d = 0.47 is the the drag coefficient, and A = 1.256 × 10 −3 m 2 is the cross-sectional area for a standard table tennis ball.</p>
<p>D Hardware Details</p>
<p>D.1 Robot Hardware Overview</p>
<p>Player Robot: The player robot ( Figure 1) is a combination of an ABB IRB 120T 6-DOF robotic arm mounted to a two-dimensional Festo linear actuator, creating an 8-DOF system. The robot arm's end effector is a standard table tennis paddle with the handle removed attached to a 174.3mm extension. The arm is controlled with ABB's Externally Guided Motion (EGM) interface at approximately 248Hz by specifying joint position and speed targets [76]. The 2D linear actuator is independently controlled at up to 125Hz with position target commands for each axis at a fixed velocity through Festo's custom Modbus interface. Position feedback from the robots is received at the command rate. The policy outputs individual joint velocity commands which are converted by a safety layer (to prevent collisions / stay within performance limits) into raw hardware commands. The robot starts from a forehand-pose as the home position and is controlled by the learned policy as soon as a ball is in play. As soon as the policy either makes contact with the ball returning it or misses it, the robot is returned to the home position and continues the rally with the next or returned ball as fresh inputs to the policy.</p>
<p>D.2 Ball Vision Model</p>
<p>The ball location is determined through a stereo pair of Ximea MQ013CG-ON cameras positioned above and to the side of the table and running at 125Hz. A recurrent 2D detector model detects the ball position in each camera independently. This detector was trained with ≈ 2 hours of ball video data with an additional ≈ 15 minutes of humans pretending to play without a ball which is used for hard negative mining. During training, horizontal flipping augmentation are applied to video sequences to balance detection performance across both directions. The 2D detections from each camera are fed to standard OpenCV triangulation to produce 3D coordinates, which are in turn run filtered through a 3D tracker and interpolated to the 75Hz frequency that the policy does inference on. There is roughly ≈ 15ms of lag between image capture and 3D coordinate availability.</p>
<p>E Model Architecture</p>
<p>We represent our policy using a three layer 1D fully convolutional gated dilated CNN with 976 parameters. Details are given in    </p>
<p>18</p>
<p>F Training Hyperparameters</p>
<p>G Simulation Details</p>
<p>Our simulation handles robot dynamics and contact dynamics (via PyBullet), and we model the ball using Newtonian dynamics, incorporating air drag but not spin. At the beginning of an episode, a ball throw is sampled according the the parameterized distribution described in section 4.</p>
<p>One major difference between simulated and real world robotic systems is the existence of sensor latency and noise in the latter but not the former. We seek to minimize this difference by measuring the latency of the major system components and modeling them in our simulation. These components include     </p>
<p>G.1 Sensor Latency Model</p>
<p>G.2 Rewards in Simulation and the Real World</p>
<p>H Evaluation Methodology</p>
<p>Each model was evaluated by (a) the model's trainer and (b) two other players. In each evaluation, 50 rallies (defined as a sequence of consecutive hits ending when one player fails to return the ball) were played with the human always starting and the rally length calculated as the number of paddle touches for both the human and robot. While the human can be responsible for a rally ending, almost all ended with the robot failing to return the ball or returning it such that the human could not easily continue the rally. The model trainer also evaluated intermediate checkpoints (see Figure 2) using the same methodology to shed light on the training dynamics. To ensure fair evaluation, all models were tested in random order and the identity of the model was kept hidden from the evaluator ("blind eval").</p>
<p>We introduced a bijective model for anonymization to make it easier for the players to evaluate the models fairly. Each player evaluated all their ten models and three models trained by two other randomly selected players in the roster. The identity of the models is revealed once all the evaluations have been completed. A successful evaluation must contain at least 50 valid rally balls (see subsection H.2 for further instruction on determining a valid rally ball). In addition to rally length, we have also collected statistics such as whether the player or robot is at fault for ending the rally.</p>
<p>All players trained and evaluated the following ten models:</p>
<ol>
<li>
<p>i-S2R sim 1 20 2. i-S2R fine-tuned 35% 3. i-S2R sim 2 4. i-S2R fine-tuned 65% 5. i-S2R sim 3 6. i-S2R fine-tuned 100% 7. S2R fine-tuned 65% 8. S2R fine-tuned 100% 9. S2R-Oracle sim 10. S2R-Oracle fine-tuned Each player cross evaluated three models each from two other players:</p>
</li>
<li>
<p>i-S2R fine-tuned 100% 2. S2R-Oracle fine-tuned 3. S2R fine-tuned 100% Table 6 shows the trainer and evaluator combinations for cross evaluations.</p>
</li>
</ol>
<p>Trainer Evaluators player 1 player 4 player 5 player 2 player 1 player 3 player 3 player 1 player 4 player 4 player 2 player 5 player 5 player 2 player 3 </p>
<p>H.1 Instructions for Human Players</p>
<p>We have provided the following instructions while gathering initial ball trajectories and rallying with the robot.</p>
<p>Initial Ball Distribution: The player lobs the ball over the net from the left hand quadrant of the opponent side to the right hand quadrant of the robot side. All the players used the same standard table tennis racket.</p>
<p>Training and Evaluation: The player always starts a rally. The player lobs the ball from the left hand quadrant of the opponent side to the right hand quadrant of the robot side as naturally as possible. During the play, for all the return balls from the robot, the player tries to return the ball to the right hand quadrant of the robot. In all cases, we have instructed the player to cooperate with robot as much as possible. Table 7 contains the rally length evaluation and end-of-rally attribution instructions for raters. For each evaluation, the cases marked as "Filter" are removed. Then, the top 60 rallies are selected and sorted by rally length. For reporting, we have selected the top 50 rallies from this set.</p>
<p>H.2 Details on Rally Score Evaluations</p>
<p>Description did-robot-endrally Instruction</p>
<p>Human hit the first ball to the net.</p>
<ul>
<li>Filter  Human hit the first ball over the table. -Filter Human hit the first ball out of distribution, robot did not return.</li>
</ul>
<p>Yes</p>
<p>Human hit the first valid ball and the robot did not react.</p>
<p>Filter</p>
<p>Human returning a ball out of distribution, robot did not return.</p>
<p>Yes</p>
<p>Human returns a ball that bounces multiple times on the human side (robot has returned the ball).</p>
<p>No</p>
<p>Human returning a ball over the table.</p>
<p>No Human returning a ball to the edge of the table, robot did not return.</p>
<p>Yes</p>
<p>Human hit a ball that graces the net which robot did not contact.</p>
<p>-Filter</p>
<p>Human hit a ping pong type service and the robot did not return.</p>
<p>-Filter</p>
<p>Robot returns a ball which graces the net, but the human cannot return.</p>
<p>No</p>
<p>Robot returns a ball which lands at the corner of the table and the human cannot return.</p>
<p>No</p>
<p>Rally ends due to the robot cannot contact the ball and/or the encoder diff is high (obvious behavior change from a previous rally, if applicable)</p>
<p>Yes</p>
<p>Robot is in ABB home pose, not the episode start state. You throw a dummy ball by hand so that the robot moves to episode start state.</p>
<p>-Filter</p>
<p>Robot is in ABB home pose, not the episode start state. You throw with a paddle so that the robot moves to episode start state.</p>
<p>-Filter Table 7: Rally score evaluation and end-of-rally attribution. Table 8 contains further details on player rally length, calculated over all 10 models that a player evaluated (see Appendix H). This data was used to group players into three skill levels; beginner (players 3 and 5), intermediate (players 4 and 2), and advanced (player 1). Note that player 5 was the non author player.</p>
<p>I Player Skill Level</p>
<p>We grouped players according to empirical skill (i.e. how they actually played) as opposed to using self-reported skill because non-professional players' perception of their skill level may not be well calibrated across players. In future it would be interesting to consider self-reported skill in addition to empirical skill.</p>
<p>J Rally Length Normalization Details</p>
<p>Let x be the rally length, µ x the mean rally length, and σ x the standard deviation of the rally lengths, then rally length is normalized as follows:
x − µ x σ x(6)
Evaluations Here µ x and σ x are calculated over all 10 evaluations (see Appendix H), making 500 (10 x 50) rallies in total. The values per player are given in Table 8. This can be interpreted  as normalizing for player skill and is intended to make rally length comparable between players of different skill levels (e.g. beginner, advanced). This approach was used in Figure 3 and Figure 8.</p>
<p>Cross-Evaluations</p>
<p>Here µ x and σ x are calculated per model (50 rallies in total) and rallies are normalized with respect to the player who trained the model. This is intended to make rally length comparable across models and players (e.g. S2R+FT player 3, i-S2R player 1). This approach was used in Figure 7 and Figure 12 to estimate the % difference in performance when a model is evaluated by different players (cross-evaluations) who did not train the model.</p>
<p>K Additional Results</p>
<p>Here we present additional results. Figure 8 contains additional presentations of the data aggregated over all five players; (a) mean normalized rally length, (b) distribution of normalized rally length, and (c) mean rally length. Figure 9 presents mean rally length by player skill level. Figure 10 contains additional presentations of the data aggregated over 4/5 players with the outlier (advanced player) excluded; (a) mean normalized rally length, (b) distribution of normalized rally length, and (c) mean rally length. Figure 11 and Figure 12 break out results per player. Note that player 5 was the non-author player and was categorized as a beginner. Figure 11 shows the mean and distribution of rally length for each player, ordered from top to bottom by skill level, beginner to advanced. Figure 12 shows cross evaluation data by player with the same ordering by player skill. Figure 13, Figure 14, and Figure 15 present additional details on ball distributions per player during training and evaluation. Figure 16 and Figure 17 present additional data on the robot return rate per player in the form of heatmaps. The color of each square represents the robot return rate (darker = higher return rate) and the number in each square represents the percentage of balls. The grid operates on two scales, a large 3 x 3 grid, and within each cell, a smaller 3 x 3 grid. In each heatmap, the large scale grid represents where the incoming ball bounced on the robot side of the table.</p>
<p>In Figure 16 the small scale grid represents the position on the player side where the ball originated. So, Figure 16 shows the conditional return rate given the start position of the incoming ball and where the ball bounced on the robot side of the In Figure 17 the small scale grid represents the position on the player side where the ball landed (i.e. where the robot returned the ball to). So, Figure 17 shows the conditional return rate given the landing position of the returned ball (i.e. where the robot hit it to) and where the incoming ball bounced on the robot side of the table. As an example, if we look at the player 3 i-S2R middle grid, it accounts for 53.4% of the balls, out of which 17.4% of the returns are to the middle of the table.</p>
<p>Statistical Significance We note that the un-normalized mean rally lengths for i-S2R, S2R+FT and S2R-Oracle+FT are not statistically significantly different, since the 95% confidence intervals overlap (Appendix K, Figure 8 (c)). However, the histogram of rally lengths for i-S2R and S2R+FT (Figure 3, right) shows that a large fraction of the rallies for S2R+FT are shorter (i.e. less than 5), while i-S2R achieves longer rallies more frequently. This suggests i-S2R yields policies that are more fun to play with on average.</p>
<p>When rally length is normalized to account for differences in skill level between players (Appendix K, Figure 8 (a)), the mean rally length for S2R-Oracle+FT is statistically significantly higher than S2R+FT, although the difference is small.</p>
<p>Finally, when the advanced player (outlier) is excluded (Figure 10), the mean rally length (normalized and un-normalized) for i-S2R is statistically significantly higher than S2R+FT and the difference is large. The mean rally length for i-S2R and S2R-Oracle+FT are not statistically significantly different.      Figure 13: Evolution of ball distributions for each player projected to 2D using t-SNE [77] (up to 500 random ball trajectory are used for ∆D1 and ∆D2, and Di = D0 + i j=1 ∆Dj.)
D 0 (player3) D 0 D 1 D 0 D 1 D 2 D 0 (player5) D 0 (player4) D 0 (player2) D 0 (player1)D0 i-S2R-out-of-sim (player3) D0 D1 i-S2R-fine-tune-30% D0 D1 D2 i-S2R-fine-tune-65% D0 D1 D2 D3 i-S2R-fine-tune-100% D0 i-S2R-out-of-sim (player5) D0 i-S2R-out-of-sim (player4) D0 i-S2R-out-of-sim (player2) D0
i-S2R-out-of-sim (player1) Figure 14: Evolution of ball distribution for each player and the overlapping evaluation distributions for i-S2R(2D projected using t-SNE [77] and up to 500 random ball trajectory are sample from each round).
D0 D1 D2 D3 i-S2R (player3) D0 D1 S2R S2R+FT (player3) D0 D1 D2 D3 i-S2R (player5) D0 D1 S2R S2R+FT (player5) D0 D1 D2 D3 i-S2R (player4) D0 D1 S2R S2R+FT (player4) D0 D1 D2 D3 i-S2R (player2) D0 D1 S2R S2R+FT (player2) D0 D1 D2 D3 i-S2R (player1) D0
D1 S2R S2R+FT (player1) Figure 15: Evolution of ball distribution for each player and the overlapping evaluation distributions for i-S2R left and S2R+FT on right (2D projected using t-SNE [77] and up to 500 random ball trajectory are sample from each round).  </p>
<p>K.1 Oracle Ball Distribution Ablation</p>
<p>To assess the contributions of the human behavior model, we hand-designed large, medium, and narrow ball distributions as shown in Table 9. The medium distribution is restricted to throwing balls towards the forehand hand side of the robot with a restricted velocity range, and the narrow distribution is modeled based on our ball thrower machine. Figure 18 compares these distributions with S2R-Oracle. Evaluations were done using a single human subject and are zero-shot from simulation with no fine-tuning. The large model performed ≈ 40% lower then S2R-Oracle. Further, we observe that lower zero-shot scores substantially increase the fine-tuning time required to match final performance, which is costly when a human is in the loop. We also observe that zero-shot transfer with the medium distribution is comparable to S2R-Oracle. This is because there is a high overlap between the two human behavior models. This indicates that human behavior modeling via ball distributions plays an important part in the ability for cooperative interaction with a table tennis playing robot.</p>
<p>Parameter</p>
<p>Large Medium Narrow S2R-Oracle min z velocity (ms −1 ) -10 -0.  Rally Length Figure 18: Zero-shot transfer of rally length for different ball distributions as defined in Table 9.</p>
<p>Figure 1 :
1Robot setup An ABB IRB 120T 6-DOF robotic arm is mounted to a two-dimensional Festo linear actuator, creating an 8-DOF system.</p>
<p>Figure 3 :
3Aggregated results Boxplot details: white circle: mean, horizontal line: median, box bounds: 25th and 75th percentiles.</p>
<p>Figure 4
4Figure 4: Results by player skill. When broken down by player skill, we notice that i-S2R has a substantially longer rally length than S2R+FT and is comparable to S2R-Oracle for beginner and intermediate players. The advanced player is an exception. Note, S2R-Oracle+FT gets just 35% of i-S2R and S2R+FT fine-tuning budget.</p>
<p>Figure 5 :
5Policy performance at key checkpoints during training. For beginner players i-S2R performance converges after just two iterations (see fine-tune-65%). For intermediate players i-S2R takes three iterations to converge (see fine-tune-100%)</p>
<p>Figure 6 :
6Figure 6: The key distribution parameters change substantially from initial ball distribution (sim1) to that after 1st round of sim training (sim2). This is to be expected given we start from a simple human model (hits across the table). The change in parameters between 1st and 2nd round of sim training is much less (sim2 vs. sim3).</p>
<p>Figure 7 :
7Cross-evaluations mean rally lengths (with 95% CI) aggregated across all players. i-S2R generalizes better to new players compared to S2R+FT.</p>
<p>•
Alex Bewley helped build the vision system, curate ball detection data and trained the perception module used in this work. Contributed to writing the vision related sections of the paper. • Deepali Jain co-developed with Krzysztof Choromanski: (a) the ES algorithm used to learn policies presented in this paper and (b) distributed optimization infrastructure to apply ES-based training. Conducted extensive tests of ES in the simulator in the first phase of the project. Edited the paper. • Krzysztof Choromanski co-developed with Deepali Jain: (a) the ES algorithm used to learn policies presented in this paper and (b) distributed optimization infrastructure to apply ES-based training. Conducted extensive tests of ES in the simulator in the first phase of the project. Edited the paper. • Pannag R. Sanketi managed the team. Set the research direction, co-led the project, and wrote the paper. Was one of the test subjects.</p>
<p>Orthogonal
Perturbations (Samples): The other key feature of the BGS algorithm is the use of the orthogonal ensembles of perturbations (samples) δ Ri . This technique was originally introduced in [64] and relies on constructing perturbations δ Ri in blocks, where each block consists of pairwise orthogonal samples. Those samples are still of Gaussian marginal distributions, matching those of the regular non-orthogonal variant.</p>
<p>(a) ABB and Festo action latency, (b) ball observation latency, (c) ABB and Festo observation latency. The latency of each component is modeled by N (µ, σ 2 ) where µ and σ 2 were measured empirically. The details are given in subsection G.1. At the beginning of each episode during training in simulation the latency of each component is sampled and remains fixed throughout the episode.</p>
<p>Figure 8 :
8Aggregated results across all 5 players after learning. Vertical lines are 95% confidence intervals (CIs).</p>
<p>Figure 9 :
9Mean rally length by player skill level. Vertical lines are 95% confidence intervals (CIs). Note: S2R-Oracle+FT is only getting 35% of i-S2R and S2R+FT fine-tuning budget. ) Mean rally length.</p>
<p>Figure 10 :
10Aggregated results across 4/5 players, excluding the outlier (advanced) player, after learning. Vertical lines are 95% confidence intervals (CIs).</p>
<p>Figure 11 :Figure 12 :
1112Breakdown by player, order top to bottom from lowest to highest overall rally mean. left: Mean rally length. Vertical lines are 95% CIs. right: Rally length distribution per model. Cross evaluations. Ordered by model trainer, top to bottom from lowest to highest overall rally mean. left: Mean rally length. right: Mean normalized rally length. Vertical lines are 95% CIs.</p>
<p>Figure 16 :
16.762--0.593 -0.593--0.424 -0.424--0.254 -0.254--0.085 -0.085-0.085 0.085-0.254 0.254-0.424 0.424-0.593 0.593-.762--0.593 -0.593--0.424 -0.424--0.254 -0.254--0.085 -0.085-0.085 0.085-0.254 0.254-0.424 0.424-0.593 0.593-Heatmaps of the robot hit rate with respect to the (x, y) position where the episode initiated from. left: i-S2R right: S2R+FT. The outermost block represents the robot side. Each 3x3 blue block represents the human (opponent) side of the table. Each block shows, if the human throw landed on the robot side, where would the human throw initiated from. The block color represents the robot hit rate.</p>
<p>Figure 17 :
17.762--0.593 -0.593--0.424 -0.424--0.254 -0.254--0.085 -0.085-0.085 0.085-0.254 0.254-0.424 0.424-0.593 0.593-.762--0.593 -0.593--0.424 -0.424--0.254 -0.254--0.085 -0.085-0.085 0.085-0.254 0.254-0.424 0.424-0.593 0.593-Heatmaps of the robot hit rate with respect to the (x, y) position where the episode ends. left: i-S2R right: S2R+FT. The outermost block represents the robot side. Each 3x3 blue block represents the human (opponent) side of the table. Each block shows, if the human throw landed on the robot side, where would the robot hit the ball such that it lands on the opponent side. The block color represents the robot hit rate.</p>
<p>6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand. arXiv:2207.06572v4 [cs.RO] 22 Nov 2022</p>
<p>3 .
3Update Human Behavior Model… </p>
<p>Bootstrap 
model M0 </p>
<p>Latest 
Human 
Interaction 
Data Dk </p>
<p>Deploy on 
robot θkS </p>
<p>Latest 
human 
model 
Mk 
Latest Policy θkR </p>
<p>M1 </p>
<p>M2 </p>
<p>…. </p>
<ol>
<li>
<p>Real Fine-tuning </p>
</li>
<li>
<p>Sim Training </p>
</li>
</ol>
<p>Initial Human Data </p>
<p>(No agent) D0 </p>
<p>Mk </p>
<p>60 </p>
<p>130 </p>
<p>200 </p>
<p>Sim Training Iterations [Updates policy, no change in human model] 
Real Fine-tuning (FT) iterations 
[Updates policy and Human model] </p>
<p>45K 
50K 
55K </p>
<p>Fine-tune 
30% </p>
<p>"i-s2r" 
"s2r + FT" </p>
<p>"s2r" </p>
<p>"s2r-Oracle + FT" </p>
<p>(only 1 round 
fine-tuning) </p>
<p>"s2r-Oracle" </p>
<p>(Train from scratch till 
Sim 3 iterations) </p>
<p>Fine-tune 
65% </p>
<p>Fine-tune 
100% </p>
<p>Use built up 
human model as 
oracle for 
"s2r-Oracle". </p>
<p>0,0 </p>
<p>Sim 1 </p>
<p>Sim 2 </p>
<p>Sim 3 </p>
<p>35] F. Miyazaki, M. Takeuchi, M. Matsushima, T. Kusano, and T. Hashimoto. Realization of the table tennis task based on virtual targets. ICRA, 2002. [36] F. Miyazaki et al. Learning to dynamically manipulate: A table tennis robot controls a ball and rallies with a human being. In Advances in Robot Control, 2006. [37] R. Anderson. A Robot Ping-Pong Player: Experiments in Real-Time Intelligent Control. MIT Press, 1988. [38] K. Muelling et al. Simulating human table tennis with a biomimetic robot setup. In Simulation of Adaptive Behavior, 2010. [39] Y. Zhu, Y. Zhao, L. Jin, J. Wu, and R. Xiong. Towards high level skill learning: Learn to return table tennis ball using monte-carlo based policy gradient method. IEEE International Conference on Real-time Computing and Robotics, 2018. [40] Y. Huang, B. Schölkopf, and J. Peters. Learning optimal striking points for a ping-pong playing robot. IROS, 2015. [41] Y. Sun, R. Xiong, Q. Zhu, J. Wu, and J. Chu. Balance motion generation for a humanoid robot playing table tennis. IEEE-RAS Humanoids, 2011. [42] R. Mahjourian, N. Jaitly, N. Lazic, S. Levine, and R. Miikkulainen. Hierarchical policy design for sample-efficient learning of robot table tennis through self-play. arXiv:1811.12927, 2018. [43] M. Matsushima, T. Hashimoto, and F. Miyazaki. Learning to the robot table tennis taskball control and rally with a human. IEEE International Conference on Systems, Man and Cybernetics, 2003. [44] M. Matsushima, T. Hashimoto, M. Takeuchi, and F. Miyazaki. A learning approach to robotic table tennis. IEEE Transactions on Robotics, 2005. [45] K. Muelling, J. Kober, and J. Peters. Learning table tennis with a mixture of motor primitives. IEEE-RAS Humanoids, 2010. [46] K. Muelling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 2012.[47] Y. Huang, D. Buchler, O. Koç, B. Schölkopf, and J. Peters. Jointly learning trajectory generation 
and hitting point prediction in robot table tennis. IEEE-RAS Humanoids, 2016. </p>
<p>[48] O. Koç, G. Maeda, and J. Peters. Online optimal trajectory generation for robot table tennis. 
Robotics &amp; Autonomous Systems, 2018. </p>
<p>[49] J. Tebbe, Y. Gao, M. Sastre-Rienietz, and A. Zell. A table tennis robot system using an industrial 
kuka robot arm. GCPR, 2018. </p>
<p>[50] Y. Gao, J. Tebbe, J. Krismer, and A. Zell. Markerless racket pose detection and stroke classifica-
tion based on stereo vision for table tennis robots. IEEE Robotic Computing, 2019. </p>
<p>[51] J. Tebbe, L. Krauch, Y. Gao, and A. Zell. Sample-efficient reinforcement learning in robotic 
table tennis. ICRA, 2021. </p>
<p>[52] W. Gao, L. Graesser, K. Choromanski, X. Song, N. Lazic, P. Sanketi, V. Sindhwani, and N. Jaitly. 
Robotic table tennis with model-free reinforcement learning. IROS, 2020. </p>
<p>Table 1
1shows the changes of the ball behavior models, M 0 , M 1 , and M 2 , for each player. Skill levels: players 3 and 5 are beginners, players 2 and 4 are intermediate, and player 1 is advanced.player 1 </p>
<p>player 2 </p>
<p>player 3 </p>
<p>player 4 </p>
<p>player 5 
M </p>
<p>1 </p>
<p>M </p>
<p>2 </p>
<p>min z velocity (ms −1 </p>
<p>) </p>
<p>1.25 
-1.47 -1.56 0.88 </p>
<p>-1.14 -1.27 0.64 </p>
<p>-1.23 -1.39 0.04 </p>
<p>-1.31 -1.72 0.52 </p>
<p>-0.70 -0.87 </p>
<p>max z velocity (ms −1 </p>
<p>) </p>
<p>2.71 </p>
<p>2.95 </p>
<p>2.95 </p>
<p>2.84 </p>
<p>2.84 </p>
<p>3.07 </p>
<p>2.49 </p>
<p>2.79 </p>
<p>2.79 </p>
<p>2.25 </p>
<p>2.73 </p>
<p>2.73 </p>
<p>2.59 </p>
<p>2.75 </p>
<p>2.75 </p>
<p>max x velocity (|ms −1 </p>
<p>|) </p>
<p>1.70 </p>
<p>3.05 </p>
<p>3.05 </p>
<p>1.41 </p>
<p>2.81 </p>
<p>2.89 </p>
<p>0.79 </p>
<p>2.59 </p>
<p>2.78 </p>
<p>1.50 </p>
<p>3.40 </p>
<p>3.45 </p>
<p>0.68 </p>
<p>2.30 </p>
<p>2.68 </p>
<p>min y velocity (|ms −1 </p>
<p>|) </p>
<p>4.12 </p>
<p>2.17 </p>
<p>2.17 </p>
<p>3.97 </p>
<p>3.52 </p>
<p>2.74 </p>
<p>2.95 </p>
<p>2.19 </p>
<p>2.19 </p>
<p>4.44 </p>
<p>3.33 </p>
<p>2.96 </p>
<p>4.20 </p>
<p>2.73 </p>
<p>2.70 </p>
<p>max y velocity (|ms −1 </p>
<p>|) </p>
<p>6.31 </p>
<p>6.63 </p>
<p>6.63 </p>
<p>6.38 </p>
<p>8.05 </p>
<p>8.82 </p>
<p>6.03 </p>
<p>7.11 </p>
<p>7.11 </p>
<p>7.33 </p>
<p>7.33 </p>
<p>7.36 </p>
<p>6.34 </p>
<p>6.94 </p>
<p>6.94 </p>
<p>x start min (m) </p>
<p>-0.19 -0.79 -0.83 0.02 </p>
<p>-0.86 -0.87 0.10 
-0.93 -0.93 -0.09 -0.8 </p>
<p>-0.83 0.25 </p>
<p>-0.64 -0.80 </p>
<p>x start max (m) </p>
<p>0.19 </p>
<p>0.63 </p>
<p>0.70 </p>
<p>0.73 </p>
<p>0.65 </p>
<p>0.67 </p>
<p>0.61 </p>
<p>0.79 </p>
<p>0.81 </p>
<p>0.68 </p>
<p>0.78 </p>
<p>0.83 </p>
<p>0.42 </p>
<p>0.55 </p>
<p>0.64 </p>
<p>y start min (m) </p>
<p>1.05 </p>
<p>0.04 </p>
<p>0.04 </p>
<p>0.85 </p>
<p>0.37 </p>
<p>0.08 </p>
<p>0.61 </p>
<p>0.05 </p>
<p>0.04 </p>
<p>1.01 </p>
<p>0.21 </p>
<p>0.04 </p>
<p>1.08 </p>
<p>0.17 </p>
<p>0.17 </p>
<p>y start max (m) </p>
<p>2.51 </p>
<p>1.87 </p>
<p>1.92 </p>
<p>1.68 </p>
<p>1.89 </p>
<p>1.95 </p>
<p>1.35 </p>
<p>1.83 </p>
<p>1.92 </p>
<p>1.88 </p>
<p>1.58 </p>
<p>1.58 </p>
<p>1.44 </p>
<p>1.81 </p>
<p>1.82 </p>
<p>z start min (m) </p>
<p>0.07 </p>
<p>0.19 </p>
<p>0.19 </p>
<p>0.15 </p>
<p>0.08 </p>
<p>0.01 </p>
<p>0.18 
-0.15 -0.29 0.15 </p>
<p>0.24 </p>
<p>0.19 </p>
<p>0.33 </p>
<p>0.26 </p>
<p>0.26 </p>
<p>z start max (m) </p>
<p>Table 1 :
1Ball distribution changes, M0, M1,and M2, 
per player for i-S2R. </p>
<p>Table 2 .
2The observation space is 2-dimensional (timesteps x [ball position, robot joint position]) which is an (8 x 11) matrix. The networks outputs a vector(8,)   representing joint velocities.Layer 
Parameter 
1 
2 
3 
Convolution dimension 
1D 
1D 
1D 
Number of filters 
8 
12 
8 
Stride 
1 
1 
1 
Dilation 
1 
2 
4 
Activation function 
tanh tanh tanh 
Padding 
valid valid valid </p>
<p>Table 2 :
2CNN model architecture.</p>
<p>Table 3
3presents the ES hyper-parameters used for both simulated and real world training.Parameter 
Simulation Real fine-tuning 
Step size 
0.00375 
0.00375 
Perturbation standard deviation 
0.025 
0.025 
Number of perturbations 
200 
5 
Number of rollouts per perturbation 
15 
3 
Percentage to keep (top x% rollouts) 
30% 
60% 
Maximum environment steps per rollout 
200 
200 
Use orthogonal perturbations 
True 
True 
Use observation normalization 
True 
True </p>
<p>Table 3 :
3ES hyperparameters.</p>
<p>Table 4
4details the parameters used in the simulated sensor latency model described above.Latencies (ms) 
Component 
µ 
σ 2 
Ball observation 
40 
8.2 
ABB observation 
29 
8.2 
Festo observation 
33 
9 
ABB action 
71 
5.7 
Festo action 
64.5 
11.5 </p>
<p>Table 4 :
4Sensor latency model parameters per component.</p>
<p>Table 5
5describes the rewards used in simulation to train and fine-tune in the real world.Rewards 1 </p>
<p>Table 5 :
5Rewards used in simulation to train and fine-tune in the real world.</p>
<p>Table 6 :
6Trainer and evaluator combinations.</p>
<p>Table 8 :
8Rally length statistics by player. Values were calculated over all 10 models that a player evaluated (see 
Appendix H), making 500 (10 x 50) rallies in total </p>
<p>. </p>
<p>Table 9 :
9The ball distribution parameters for each of the ablated distributions. land here implies landing on the robot side.Large 
Medium 
Narrow 
S2R-Oracle </p>
<p>Model </p>
<p>0 </p>
<p>1 </p>
<p>2 </p>
<p>3 </p>
<p>4 </p>
<p>5 </p>
<p>6 </p>
<p>AcknowledgmentsWe thank Pete Florence, Kamyar Ghasemipour, Andrew Silva, Ellie Sanoubari, and Vincent Vanhoucke for their helpful and insightful feedback on earlier versions of this manuscript. We are grateful to Michael Ahn, Sherry Moore, Ken Oslund, and Grace Vesom for all their work on the robot control stack, for Omar Cortes' help in training models and for Justin Boyd and Khem Holden's help in calibrating our vision system. We thank Jon Abelian, Gus Kouretas, Thinh Nguyen, and Krista Reymann for all that they do to help maintain our robotic system. We would also like to thank Navdeep Jaitly, Peng Xu, Nevena Lazic, and Reza Mahjourian for their work on early versions of this system. Finally, we would like to thank Jon Abelian, Justin Boyd, Omar Cortes, Khem Holden, Gus Kouretas and Thinh Nguyen for their help evaluating models.
Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE International Conference on Robotics and Automation. Brisbane, AustraliaIEEEX. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pages 1-8. IEEE, 2018.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N D Ratliff, D Fox, International Conference on Robotics and Automation, ICRA 2019. Montreal, QC, CanadaIEEEY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. D. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In International Conference on Robotics and Automation, ICRA 2019, Montreal, QC, Canada, May 20-24, 2019, pages 8973-8979. IEEE, 2019.</p>
<p>Learning dexterous in-hand manipulation. M Andrychowicz, B Baker, M Chociej, R Józefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, J Schneider, S Sidor, J Tobin, P Welinder, L Weng, W Zaremba, Int. J. Robotics Res. 3912020M. Andrychowicz, B. Baker, M. Chociej, R. Józefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. Int. J. Robotics Res., 39(1), 2020.</p>
<p>Bi-manual manipulation and attachment via sim-to-real reinforcement learning. S Kataoka, S K S Ghasemipour, D Freeman, I Mordatch, S. Kataoka, S. K. S. Ghasemipour, D. Freeman, and I. Mordatch. Bi-manual manipulation and attachment via sim-to-real reinforcement learning, 2022. URL https://arxiv.org/abs/ 2203.08277.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Sci. Robotics. 426J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learning agile and dynamic motor skills for legged robots. Sci. Robotics, 4(26), 2019.</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T E Lee, J Tan, S Levine, Robotics: Science and Systems XVI, Virtual Event / Corvalis. M. Toussaint, A. Bicchi, and T. HermansOregon, USAX. B. Peng, E. Coumans, T. Zhang, T. E. Lee, J. Tan, and S. Levine. Learning agile robotic locomotion skills by imitating animals. In M. Toussaint, A. Bicchi, and T. Hermans, editors, Robotics: Science and Systems XVI, Virtual Event / Corvalis, Oregon, USA, July 12-16, 2020, 2020.</p>
<p>CAD2RL: real single-image flight without a single real image. F Sadeghi, S Levine, Robotics: Science and Systems XIII, Massachusetts Institute of Technology. N. M. Amato, S. S. Srinivasa, N. Ayanian, and S. KuindersmaCambridge, Massachusetts, USAF. Sadeghi and S. Levine. CAD2RL: real single-image flight without a single real image. In N. M. Amato, S. S. Srinivasa, N. Ayanian, and S. Kuindersma, editors, Robotics: Science and Systems XIII, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, July 12-16, 2017, 2017.</p>
<p>Learning high-speed flight in the wild. A Loquercio, E Kaufmann, R Ranftl, M Müller, V Koltun, D Scaramuzza, Sci. Robotics. 6592021A. Loquercio, E. Kaufmann, R. Ranftl, M. Müller, V. Koltun, and D. Scaramuzza. Learning high-speed flight in the wild. Sci. Robotics, 6(59), 2021.</p>
<p>ALVINN: an autonomous land vehicle in a neural network. D Pomerleau, Advances in Neural Information Processing Systems. D. S. TouretzkyDenver, Colorado, USA1NIPS ConferenceD. Pomerleau. ALVINN: an autonomous land vehicle in a neural network. In D. S. Touretzky, editor, Advances in Neural Information Processing Systems 1, [NIPS Conference, Denver, Colorado, USA, 1988], pages 305-313.</p>
<p>Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. T Zhang, Z Mccarthy, O Jow, D Lee, K Goldberg, P Abbeel, arXiv:1710.04615arXiv preprintT. Zhang, Z. McCarthy, O. Jow, D. Lee, K. Goldberg, and P. Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. arXiv preprint arXiv:1710.04615, 2017.</p>
<p>Apprenticeship learning via inverse reinforcement learning. P Abbeel, A Y Ng, Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004). C. E. BrodleyBanff, Alberta, CanadaACM69P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In C. E. Brodley, editor, Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004), Banff, Alberta, Canada, July 4-8, 2004, volume 69 of ACM International Conference Proceeding Series. ACM, 2004.</p>
<p>Maximum entropy inverse reinforcement learning. B D Ziebart, A L Maas, J A Bagnell, A K Dey, Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI. D. Fox and C. P. Gomesthe Twenty-Third AAAI Conference on Artificial Intelligence, AAAIChicago, Illinois, USAAAAI PressB. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In D. Fox and C. P. Gomes, editors, Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 1433- 1438. AAAI Press, 2008.</p>
<p>Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, CoRRJ. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter. Learning quadrupedal locomotion over challenging terrain. CoRR, 2020.</p>
<p>Solving rubik's cube with a robot hand. I Openai, M Akkaya, M Andrychowicz, M Chociej, B Litwin, A Mcgrew, A Petron, M Paino, G Plappert, R Powell, J Ribas, N Schneider, J Tezak, P Tworek, L Welinder, Q Weng, W Yuan, L Zaremba, Zhang, OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang. Solving rubik's cube with a robot hand. 2019.</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, abs/1804.10332CoRRJ. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. CoRR, abs/1804.10332, 2018. URL http://arxiv.org/abs/1804.10332.</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, CoRRW. Zhao, J. P. Queralta, and T. Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. CoRR, 2020.</p>
<p>Sim2real in robotics and automation: Applications and challenges. S Höfer, K Bekris, A Handa, J C Gamboa, M Mozifian, F Golemo, C Atkeson, D Fox, K Goldberg, J Leonard, C Karen Liu, J Peters, S Song, P Welinder, M White, 10.1109/TASE.2021.3064065IEEE Transactions on Automation Science and Engineering. 182S. Höfer, K. Bekris, A. Handa, J. C. Gamboa, M. Mozifian, F. Golemo, C. Atkeson, D. Fox, K. Goldberg, J. Leonard, C. Karen Liu, J. Peters, S. Song, P. Welinder, and M. White. Sim2real in robotics and automation: Applications and challenges. IEEE Transactions on Automation Science and Engineering, 18(2):398-400, 2021. doi:10.1109/TASE.2021.3064065.</p>
<p>Why off-the-shelf physics simulators fail in evaluating feedback controller performance -a case study for quadrupedal robots. M Neunert, T Boaventura, J Buchli, M. Neunert, T. Boaventura, and J. Buchli. Why off-the-shelf physics simulators fail in evaluating feedback controller performance -a case study for quadrupedal robots. 2016.</p>
<p>Model identification via physics engines for improved policy search. S Zhu, A Kimmel, K E Bekris, A Boularias, S. Zhu, A. Kimmel, K. E. Bekris, and A. Boularias. Model identification via physics engines for improved policy search. CoRR, 2017.</p>
<p>Sim2real transfer for reinforcement learning without dynamics randomization. CoRR, abs. M Kaspar, J D M Osorio, J Bock, M. Kaspar, J. D. M. Osorio, and J. Bock. Sim2real transfer for reinforcement learning without dynamics randomization. CoRR, abs/2002.11635, 2020.</p>
<p>Simulation-based design of dynamic controllers for humanoid balancing. J Tan, Z Xie, B Boots, C K Liu, 10.1109/IROS.2016.7759424IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). J. Tan, Z. Xie, B. Boots, and C. K. Liu. Simulation-based design of dynamic controllers for humanoid balancing. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2729-2736, 2016. doi:10.1109/IROS.2016.7759424.</p>
<p>Towards human-level learning of complex physical puzzles. CoRR, abs. K Ota, D K Jha, D Romeres, J Van Baar, K A Smith, T Semitsu, T Oiki, A Sullivan, D Nikovski, J B Tenenbaum, K. Ota, D. K. Jha, D. Romeres, J. van Baar, K. A. Smith, T. Semitsu, T. Oiki, A. Sullivan, D. Nikovski, and J. B. Tenenbaum. Towards human-level learning of complex physical puzzles. CoRR, abs/2011.07193, 2020. URL https://arxiv.org/abs/2011.07193.</p>
<p>Humanoid robots learning to walk faster: From the real world to simulation and back. A Farchy, S Barrett, P Macalpine, P Stone, Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS '13. the 2013 International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS '13A. Farchy, S. Barrett, P. MacAlpine, and P. Stone. Humanoid robots learning to walk faster: From the real world to simulation and back. In Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS '13, page 39-46, 2013.</p>
<p>Legged robots that keep on learning: Fine-tuning locomotion policies in the real world. L M Smith, J C Kew, X B Peng, S Ha, J Tan, S Levine, abs/2110.05457CoRRL. M. Smith, J. C. Kew, X. B. Peng, S. Ha, J. Tan, and S. Levine. Legged robots that keep on learning: Fine-tuning locomotion policies in the real world. CoRR, abs/2110.05457, 2021.</p>
<p>Transfer learning for reinforcement learning on a physical robot. S Barrett, M E Taylor, P Stone, AAMAS 2010. S. Barrett, M. E. Taylor, and P. Stone. Transfer learning for reinforcement learning on a physical robot. In AAMAS 2010, 2010.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerík, T Rothörl, N Heess, R Pascanu, R Hadsell, CoRRA. A. Rusu, M. Vecerík, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell. Sim-to-real robot learning from pixels with progressive nets. CoRR, 2016.</p>
<p>Rapidly adaptable legged robots via evolutionary meta-learning. X Song, Y Yang, K Choromanski, K Caluwaerts, W Gao, C Finn, J Tan, 10.1109/IROS45743.2020.9341571IEEE/RSJ International Conference on Intelligent Robots and Systems. Las Vegas, NV, USAIEEE20202020X. Song, Y. Yang, K. Choromanski, K. Caluwaerts, W. Gao, C. Finn, and J. Tan. Rapidly adaptable legged robots via evolutionary meta-learning. In IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2020, Las Vegas, NV, USA, October 24, 2020 - January 24, 2021, pages 3769-3776. IEEE, 2020. doi:10.1109/IROS45743.2020.9341571. URL https://doi.org/10.1109/IROS45743.2020.9341571.</p>
<p>Learning to play table tennis from scratch using muscular robots. CoRR, abs. D Büchler, S Guist, R Calandra, V Berenz, B Schölkopf, J Peters, D. Büchler, S. Guist, R. Calandra, V. Berenz, B. Schölkopf, and J. Peters. Learning to play table tennis from scratch using muscular robots. CoRR, abs/2006.05935, 2020. URL https: //arxiv.org/abs/2006.05935.</p>
<p>J Billingsley, Robot ping pong. Practical Computing. J. Billingsley. Robot ping pong. Practical Computing, 1983.</p>
<p>Pingpong-playing robot controlled by a microcomputer. Microprocessors and Microsystems -Embedded Hardware Design. J Knight, D Lowery, J. Knight and D. Lowery. Pingpong-playing robot controlled by a microcomputer. Microproces- sors and Microsystems -Embedded Hardware Design, 1986.</p>
<p>Learning from suboptimal demonstration via self-supervised reward regression. CoRL. L Chen, R R Paleja, M C Gombolay, L. Chen, R. R. Paleja, and M. C. Gombolay. Learning from suboptimal demonstration via self-supervised reward regression. CoRL, 2020.</p>
<p>Joint goal and strategy inference across heterogeneous demonstrators via reward network distillation. CoRR, abs. L Chen, R R Paleja, M Ghuy, M C Gombolay, L. Chen, R. R. Paleja, M. Ghuy, and M. C. Gombolay. Joint goal and strategy inference across heterogeneous demonstrators via reward network distillation. CoRR, abs/2001.00503, 2020.</p>
<p>Design of a humanoid ping-pong player robot with redundant joints. Z Yu, Y Liu, Q Huang, X Chen, W Zhang, J Li, G Ma, L Meng, T Li, W Zhang, IEEE International Conference on Robotics and Biomimetics (ROBIO). Z. Yu, Y. Liu, Q. Huang, X. Chen, W. Zhang, J. Li, G. Ma, L. Meng, T. Li, and W. Zhang. Design of a humanoid ping-pong player robot with redundant joints. 2013 IEEE International Conference on Robotics and Biomimetics (ROBIO), pages 911-916, 2013.</p>
<p>Metrics and benchmarks in human-robot interaction: Recent advances in cognitive robotics. A Aly, S Griffiths, F Stramandinoli, Cognitive Systems Research. 43A. Aly, S. Griffiths, and F. Stramandinoli. Metrics and benchmarks in human-robot interaction: Recent advances in cognitive robotics. Cognitive Systems Research, 43:313-323, 2017.</p>
<p>Evaluation of a novel biologically inspired trajectory generator in human-robot interaction. M Huber, H Radrich, C Wendt, M Rickert, A Knoll, T Brandt, S Glasauer, RO-MAN 2009-The 18th IEEE International Symposium on Robot and Human Interactive Communication. IEEEM. Huber, H. Radrich, C. Wendt, M. Rickert, A. Knoll, T. Brandt, and S. Glasauer. Evaluation of a novel biologically inspired trajectory generator in human-robot interaction. In RO-MAN 2009-The 18th IEEE International Symposium on Robot and Human Interactive Communication, pages 639-644. IEEE, 2009.</p>
<p>Fedembed: Personalized private federated learning. A Silva, K Metcalf, N Apostoloff, B.-J Theobald, A. Silva, K. Metcalf, N. Apostoloff, and B.-J. Theobald. Fedembed: Personalized private federated learning, 2022. URL https://arxiv.org/abs/2202.09472.</p>
<p>The utility of explainable ai in ad hoc human-machine teaming. R Paleja, M Ghuy, N Arachchige, R Jensen, M Gombolay, Advances in Neural Information Processing Systems. M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. VaughanCurran Associates, Inc34R. Paleja, M. Ghuy, N. Ranawaka Arachchige, R. Jensen, and M. Gombolay. The utility of explainable ai in ad hoc human-machine teaming. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 610-623. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/file/05d74c48b5b30514d8e9bd60320fc8f6-Paper.pdf.</p>
<p>Markov decision processes: discrete stochastic dynamic programming. M L Puterman, John Wiley &amp; SonsM. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley &amp; Sons, 2014.</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, abs/1707.06347CoRRJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.</p>
<p>Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, S Levine, Proceedings of The 2nd Conference on Robot Learning. The 2nd Conference on Robot LearningPMLRD. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakr- ishnan, V. Vanhoucke, and S. Levine. Scalable deep reinforcement learning for vision-based robotic manipulation. In Proceedings of The 2nd Conference on Robot Learning, pages 651-673. PMLR, 2018.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLRT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning, pages 1861-1870. PMLR, 2018.</p>
<p>Structured Evolution with Compact Architectures for Scalable Policy Optimization. K Choromanski, M Rowland, V Sindhwani, R E Turner, A Weller, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLRK. Choromanski, M. Rowland, V. Sindhwani, R. E. Turner, and A. Weller. Structured Evolution with Compact Architectures for Scalable Policy Optimization. In Proceedings of the 35th International Conference on Machine Learning, pages 969-977. PMLR, 2018.</p>
<p>D Wierstra, T Schaul, T Glasmachers, Y Sun, J Schmidhuber, Natural evolution strategies. D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, and J. Schmidhuber. Natural evolution strategies, 2011.</p>
<p>T Salimans, J Ho, X Chen, S Sidor, I Sutskever, arXiv:1703.03864Evolution strategies as a scalable alternative to reinforcement learning. T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017.</p>
<p>Random gradient-free minimization of convex functions. Y Nesterov, V Spokoiny, FoCMY. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. FoCM, 2017.</p>
<p>Simple random search provides a competitive approach to reinforcement learning. H Mania, A Guy, B Recht, NeurIPS. H. Mania, A. Guy, and B. Recht. Simple random search provides a competitive approach to reinforcement learning. NeurIPS, 2018.</p>
<p>Provably robust blackbox optimization for reinforcement learning. K Choromanski, A Pacchiano, J Parker-Holder, Y Tang, D Jain, Y Yang, A Iscen, J Hsu, V Sindhwani, PMLR3rd Annual Conference on Robot Learning. L. P. Kaelbling, D. Kragic, and K. SugiuraOsaka, Japan100ProceedingsK. Choromanski, A. Pacchiano, J. Parker-Holder, Y. Tang, D. Jain, Y. Yang, A. Iscen, J. Hsu, and V. Sindhwani. Provably robust blackbox optimization for reinforcement learning. In L. P. Kaelbling, D. Kragic, and K. Sugiura, editors, 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 -November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pages 683-696. PMLR, 2019. URL http://proceedings. mlr.press/v100/choromanski20a.html.</p>
<p>A simplex method for function minimization. J A Nelder, R Mead, Computer Journal. 7J. A. Nelder and R. Mead. A simplex method for function minimization. Computer Journal, 7: 308-313, 1965.</p>
<p>Dbscan revisited, revisited: Why and how you should (still) use dbscan. E Schubert, J Sander, M Ester, H.-P Kriegel, X Xu, ACM Transactions on Database Systems. 3E. Schubert, J. Sander, M. Ester, H.-P. Kriegel, and X. Xu. Dbscan revisited, revisited: Why and how you should (still) use dbscan. ACM Transactions on Database Systems, (3), 2017.</p>
<p>Ball speed and spin estimation in table tennis using a racket-mounted inertial sensor. P Blank, B H Groh, B M Eskofier, 978-1-4503-5188-1S. C. Lee, L. Takayama, K. N. Truong, J. Healey, and T. PloetzACMP. Blank, B. H. Groh, and B. M. Eskofier. Ball speed and spin estimation in table tennis using a racket-mounted inertial sensor. In S. C. Lee, L. Takayama, K. N. Truong, J. Healey, and T. Ploetz, editors, ISWC, pages 2-9. ACM, 2017. ISBN 978-1-4503-5188-1. URL http://dblp.uni-trier.de/db/conf/iswc/iswc2017.html#BlankGE17.</p>
<p>Optimal stroke learning with policy gradient approach for robotic table tennis. Y Gao, J Tebbe, A Zell, abs/2109.03100CoRRY. Gao, J. Tebbe, and A. Zell. Optimal stroke learning with policy gradient approach for robotic table tennis. CoRR, abs/2109.03100, 2021. URL https://arxiv.org/abs/2109.03100.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2021.</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. A Nagabandi, In ICRA. A. Nagabandi et al. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In ICRA, 2018.</p>
<p>Abb application manual -externally guided motion. 2020VasterasAbb application manual -externally guided motion., Vasteras, 2020.</p>
<p>Visualizing data using t-SNE. L Van Der Maaten, G Hinton, Journal of Machine Learning Research. 9L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Ma- chine Learning Research, 9:2579-2605, 2008. URL http://www.jmlr.org/papers/v9/ vandermaaten08a.html.</p>            </div>
        </div>

    </div>
</body>
</html>
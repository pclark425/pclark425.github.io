<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1115 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1115</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1115</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-54060739</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.06276v1.pdf" target="_blank">Safe Active Learning for Time-Series Modeling with Gaussian Processes</a></p>
                <p><strong>Paper Abstract:</strong> Learning time-series models is useful for many applications, such as simulation and forecasting. In this study, we consider the problem of actively learning time-series models while taking given safety constraints into account. For time-series modeling we employ a Gaussian process with a nonlinear exogenous input structure. The proposed approach generates data appropriate for time series model learning, i.e. input and output trajectories, by dynamically exploring the input space. The approach parametrizes the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations. We analyze the proposed algorithm and evaluate it empirically on a technical application. The results show the effectiveness of our approach in a realistic technical use case.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1115.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1115.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAL-NX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Safe Active Learning for Time-Series Modeling with Gaussian Processes (SAL-NX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive, safety-constrained active-learning algorithm for time-series (NX-structure) models that uses Gaussian processes for both the regression model and a safety indicator; it plans piecewise input trajectories by maximizing an information-gain criterion (determinant of the predictive covariance) under a probabilistic safety constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAL-NX (Safe Active Learning - NX)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two Gaussian process models: a regression GP for the time-series mapping f (GP with nonlinear exogenous / NX input structure) and a safety GP for the unknown safety function g. Trajectories are parametrized as consecutive piecewise sections (ramps in experiments); at each iteration the algorithm updates both GPs with new trajectory-wise observations and solves a constrained optimization for the next trajectory endpoint using gradient-based methods. Safety probability for a candidate trajectory is approximated (Monte Carlo or EP) from the safety GP.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning (information-gain maximization using GP predictive covariance, D-optimality / determinant criterion) with safe exploration constraints</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each step SAL-NX: (1) updates regression GP (f) and safety GP (g) with observed trajectory inputs/outputs and safety indicators; (2) parametrizes candidate next piecewise trajectory (endpoint η for a ramp); (3) computes predictive covariance Σ(η) for the candidate trajectory and the safety probability ξ(η) = P(z_1..z_m >= 0 | safety GP); (4) solves constrained optimization η* = argmax_{η in Π} I(Σ(η)) s.t. ξ(η) > 1−α, where I is typically det(Σ) (D-optimality). Optimization is greedy/iterative and uses past data (GP posterior mean/covariance) and gradient-based solvers; safety threshold α controls exploration vs risk.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Time-series dynamical systems (experiments: toy 2D function; NX-structured synthetic time-series; real high-pressure fluid injection system)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown/partially known continuous input space; time-series (dynamical) behavior captured by NX or NARX structures; stochastic observation noise (Gaussian); unknown safety boundary g mapping input/trajectory to scalar safety indicator; trajectory-wise (correlated) observations (piecewise trajectories), continuous actions (endpoints of ramps), safety-critical (violations can damage system).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by experiment: toy 2D (d=2, m=5 points per ramp); synthetic NX experiment d=4 (two manipulated vars plus one-step history), m=5; real high-pressure system d=7 (history + inputs), m=5; number of planned trajectories up to 250 in the real use case; continuous action space (endpoints η ∈ Π); GP covariance matrix dimension scales with n·m (can be large).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical: SAL-NX yields faster RMSE reduction and faster coverage of the safe region than random selection (plots over 5 repetitions, e.g. substantial improvement observed within O(10^2) planned trajectories — experiments used up to 250 trajectories); in the real high-pressure system SAL-NX reduced model RMSE and increased safe-region coverage while operating in closed-loop with safety interrupts. Theoretical: safety failure probability bounded (Theorem 1): if α = δ/N then probability of any unsafe trajectory among next N planned ≤ δ; predictive uncertainty (average determinant of Σ) decays as O(log(n)^{d+1}/n) (Theorems 2/3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline random selection with safety constraints shows slower RMSE decrease and slower safe-region coverage; Fisher-information-based baseline (no safety) performed worse for uncertainty reduction. No single scalar performance numbers given in paper; comparisons reported via RMSE and coverage plots (boxplots over 5 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically reaches noticeable improvements within a few tens to a few hundreds of trajectories (examples: initialized with n0=10 (synthetic) or n0=25 (real), then planned up to 250 trajectories); theoretical decay of predictive uncertainty scales like O(log(n)^{d+1}/n). Exact sample counts to reach a target RMSE are not numerically specified.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled explicitly by probabilistic safety threshold α: lower α (more conservative) reduces unsafe trials but slows exploration; higher α increases exploration speed but raises risk of safety violations. The selection criterion (determinant of predictive covariance) is purely exploratory (maximizes uncertainty reduction); NARX extensions may use predictive mean as surrogate for output feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Random selection with safety constraints; Fisher information matrix–based adaptive design (Deflorian et al.) implemented and compared (without safety constraint for fairness in that comparison); also compared conceptually to prior GP-based safe exploration literature.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) SAL-NX safely plans trajectory-wise experiments by maximizing trajectory-level information (determinant of predicted covariance) under a probabilistic safety constraint computed from a safety GP. 2) Safety guarantee (Theorem 1): by setting α=δ/N the total failure probability across next N planned trajectories is ≤ δ, assuming safety GP quantifies uncertainty correctly. 3) Predictive uncertainty (average determinant) provably decays as O(log(n)^{d+1}/n) during exploration (Theorems 2/3). 4) Empirically SAL-NX outperforms random selection in RMSE reduction and safe-region coverage, and is competitive vs. other non-safe methods; shown to be usable on a real high-pressure fluid system with up to 250 planned trajectories and real-time performance claims.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires an initial set of safe trajectories (n0) chosen a priori; theoretical safety guarantee assumes the safety GP correctly quantifies uncertainty (calibrated posterior); computational cost of GP (K_n + σ^2 I inversion) can be high for large n·m (requires sparse/approximate GP methods or rank-one updates); computing trajectory safety probability ξ(τ) is in general intractable and requires Monte Carlo or EP approximations; overly conservative α (e.g., near 0) can prevent expansion beyond initial safe region; empirical results are shown mostly via plots without numeric summary tables; performance depends on GP hyperparameter choices and their training schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Safe Active Learning for Time-Series Modeling with Gaussian Processes', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1115.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1115.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fisher-Info</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fisher Information Matrix based adaptive experimental design (Deflorian et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive-design baseline that selects trajectories by maximizing the Fisher information matrix (sensitivity of GP mean w.r.t. trajectory endpoints), intended to place measurements where model parameters (endpoints) are most identifiable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Fisher-information based design (reimplementation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs an approximate Fisher Information matrix IFIM(η) whose entries are inner products of derivatives of the GP predictive mean with respect to trajectory endpoint parameters η (including contributions from previously observed trajectories); next trajectory endpoint selected by maximizing an objective I(IFIM(η)). This was reimplemented to compare to SAL-NX (without safety constraints for that comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive experimental design via Fisher information maximization (sensitivity-based criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses GP predictive mean sensitivity ∂µ(τ(η))/∂η to build IFIM(η) and greedily chooses η maximizing an information measure derived from IFIM; updates model after measurements and repeats. Focus is on parameter (endpoint) sensitivity rather than directly minimizing predictive posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental settings as SAL-NX: synthetic NX time-series tasks and toy examples (comparison runs without safety constraint in that benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous, dynamical time-series environments with trajectory-wise correlated observations and Gaussian measurement noise; unknown dynamics and safety signals (safety not enforced in the direct Fisher-info comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same dimensionalities as SAL-NX experiments (e.g., synthetic NX d=4,m=5; higher-d real system d=7 when applied) though the Fisher-info experiment reported was primarily synthetic and without the safety constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirically, the Fisher-information criterion was less effective at reducing predictive uncertainty (RMSE) compared to SAL-NX: the paper states the comparison 'does not look promising for the Fisher information matrix' and that Fisher-info focuses on slopes of GP mean instead of reducing posterior variance. No detailed scalar metrics provided in text; plotted comparisons show SAL-NX improves RMSE faster.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not applicable (method itself is an adaptive design baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported as less sample-efficient than SAL-NX for reducing posterior RMSE in experiments reported; specific sample counts to reach thresholds are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit: Fisher-info focuses on information about parameters (sensitivity) which can bias sampling toward regions with large gradient of the mean (may act like exploration of informative slopes but not directly uncertainty-driven). No explicit probabilistic safety trade-off mechanism in the reimplemented comparison (safety constraint was removed to match original method's assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to SAL-NX (without safety) and random selection in numerical experiments (SAL-NX reduced RMSE more effectively).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Fisher-information based design prioritized mean-slope sensitivity (endpoint-parameter identifiability) and, in the reported experiments, was less effective than SAL-NX at reducing predictive uncertainty (RMSE). Additionally, when safety was not enforced both methods without safety incurred many safety violations (>40% on average in 5 repetitions).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not provide safety guarantees and, when used without safety constraints, resulted in high safety violation rates in experiments. Tends to focus on mean sensitivity rather than directly minimizing posterior uncertainty, which can make it less effective as an active-learning strategy for reducing predictive error in GP time-series models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Safe Active Learning for Time-Series Modeling with Gaussian Processes', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1115.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1115.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-Safe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random selection with safety constraint (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that randomly samples trajectory endpoints η and selects the first candidate that satisfies the probabilistic safety constraint ξ(η) > 1−α, used to benchmark SAL-NX.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random selection (with safety filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Randomly samples trajectory parametrizations (endpoints η) uniformly in the domain and accepts the first candidate that the safety GP predicts as safe (ξ(η) > 1−α). The regression GP is updated after execution of accepted trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>None (random exploration subject to safety constraint); baseline for adaptive design</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No active selection based on information gain; randomness is filtered by safety GP to avoid obviously unsafe candidates; model updates depend on collected data but do not influence the candidate-generation process except via safety filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental environments used in SAL-NX comparisons (toy 2D, synthetic NX, and high-pressure fluid injection system).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous, stochastic, time-series dynamical environments with unknown safety boundary; trajectory-wise correlated observations; safety-critical constraints present.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as SAL-NX experiments (toy d=2; NX d=4,m=5; real system d=7,m=5; up to 250 trajectories planned in real use-case evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Empirically inferior to SAL-NX: slower RMSE decrease, slower safe-region coverage, higher variance and outliers in RMSE across runs (plots over 5 repetitions). When compared in the high-pressure system SAL-NX achieved faster model error reduction and better coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower than SAL-NX; required many more random samples to reach comparable RMSE/coverage; exact sample counts not provided (experiments used hundreds of trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>No explicit mechanism; pure exploration subject to safety-filtering parameter α; tuning α trades off accepting fewer random candidates (conservative) vs potentially unsafe but informative ones.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to SAL-NX and Fisher-information baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Random selection with safety produces significantly slower learning (higher RMSE over iterations) and slower expansion of the estimated safe region compared to SAL-NX; used to demonstrate the benefit of information-driven adaptive design in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient sample usage (low sample efficiency), high variance between runs; performance limited by the chance of randomly sampling informative trajectories (even if safe), and cannot exploit posterior structure of GP to focus exploration where uncertainty is highest.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Safe Active Learning for Time-Series Modeling with Gaussian Processes', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting <em>(Rating: 2)</em></li>
                <li>Safe and Automatic Parameter Tuning in Robotics (Bayesian Optimization with Safety Constraints) <em>(Rating: 2)</em></li>
                <li>Safe Exploration in Markov Decision Processes <em>(Rating: 2)</em></li>
                <li>Safe Exploration for Active Learning with Gaussian Processes <em>(Rating: 2)</em></li>
                <li>Online dynamic black box modelling and adaptive experiment design in combustion engine calibration <em>(Rating: 2)</em></li>
                <li>Near-Optimal Sensor Placements in Gaussian Processes <em>(Rating: 1)</em></li>
                <li>Exploration vs Exploitation vs Safety: Risk-Aware Multi-Armed Bandits <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1115",
    "paper_id": "paper-54060739",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "SAL-NX",
            "name_full": "Safe Active Learning for Time-Series Modeling with Gaussian Processes (SAL-NX)",
            "brief_description": "An adaptive, safety-constrained active-learning algorithm for time-series (NX-structure) models that uses Gaussian processes for both the regression model and a safety indicator; it plans piecewise input trajectories by maximizing an information-gain criterion (determinant of the predictive covariance) under a probabilistic safety constraint.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAL-NX (Safe Active Learning - NX)",
            "agent_description": "Two Gaussian process models: a regression GP for the time-series mapping f (GP with nonlinear exogenous / NX input structure) and a safety GP for the unknown safety function g. Trajectories are parametrized as consecutive piecewise sections (ramps in experiments); at each iteration the algorithm updates both GPs with new trajectory-wise observations and solves a constrained optimization for the next trajectory endpoint using gradient-based methods. Safety probability for a candidate trajectory is approximated (Monte Carlo or EP) from the safety GP.",
            "adaptive_design_method": "Active learning (information-gain maximization using GP predictive covariance, D-optimality / determinant criterion) with safe exploration constraints",
            "adaptation_strategy_description": "At each step SAL-NX: (1) updates regression GP (f) and safety GP (g) with observed trajectory inputs/outputs and safety indicators; (2) parametrizes candidate next piecewise trajectory (endpoint η for a ramp); (3) computes predictive covariance Σ(η) for the candidate trajectory and the safety probability ξ(η) = P(z_1..z_m &gt;= 0 | safety GP); (4) solves constrained optimization η* = argmax_{η in Π} I(Σ(η)) s.t. ξ(η) &gt; 1−α, where I is typically det(Σ) (D-optimality). Optimization is greedy/iterative and uses past data (GP posterior mean/covariance) and gradient-based solvers; safety threshold α controls exploration vs risk.",
            "environment_name": "Time-series dynamical systems (experiments: toy 2D function; NX-structured synthetic time-series; real high-pressure fluid injection system)",
            "environment_characteristics": "Unknown/partially known continuous input space; time-series (dynamical) behavior captured by NX or NARX structures; stochastic observation noise (Gaussian); unknown safety boundary g mapping input/trajectory to scalar safety indicator; trajectory-wise (correlated) observations (piecewise trajectories), continuous actions (endpoints of ramps), safety-critical (violations can damage system).",
            "environment_complexity": "Varies by experiment: toy 2D (d=2, m=5 points per ramp); synthetic NX experiment d=4 (two manipulated vars plus one-step history), m=5; real high-pressure system d=7 (history + inputs), m=5; number of planned trajectories up to 250 in the real use case; continuous action space (endpoints η ∈ Π); GP covariance matrix dimension scales with n·m (can be large).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical: SAL-NX yields faster RMSE reduction and faster coverage of the safe region than random selection (plots over 5 repetitions, e.g. substantial improvement observed within O(10^2) planned trajectories — experiments used up to 250 trajectories); in the real high-pressure system SAL-NX reduced model RMSE and increased safe-region coverage while operating in closed-loop with safety interrupts. Theoretical: safety failure probability bounded (Theorem 1): if α = δ/N then probability of any unsafe trajectory among next N planned ≤ δ; predictive uncertainty (average determinant of Σ) decays as O(log(n)^{d+1}/n) (Theorems 2/3).",
            "performance_without_adaptation": "Baseline random selection with safety constraints shows slower RMSE decrease and slower safe-region coverage; Fisher-information-based baseline (no safety) performed worse for uncertainty reduction. No single scalar performance numbers given in paper; comparisons reported via RMSE and coverage plots (boxplots over 5 runs).",
            "sample_efficiency": "Empirically reaches noticeable improvements within a few tens to a few hundreds of trajectories (examples: initialized with n0=10 (synthetic) or n0=25 (real), then planned up to 250 trajectories); theoretical decay of predictive uncertainty scales like O(log(n)^{d+1}/n). Exact sample counts to reach a target RMSE are not numerically specified.",
            "exploration_exploitation_tradeoff": "Controlled explicitly by probabilistic safety threshold α: lower α (more conservative) reduces unsafe trials but slows exploration; higher α increases exploration speed but raises risk of safety violations. The selection criterion (determinant of predictive covariance) is purely exploratory (maximizes uncertainty reduction); NARX extensions may use predictive mean as surrogate for output feedback.",
            "comparison_methods": "Random selection with safety constraints; Fisher information matrix–based adaptive design (Deflorian et al.) implemented and compared (without safety constraint for fairness in that comparison); also compared conceptually to prior GP-based safe exploration literature.",
            "key_results": "1) SAL-NX safely plans trajectory-wise experiments by maximizing trajectory-level information (determinant of predicted covariance) under a probabilistic safety constraint computed from a safety GP. 2) Safety guarantee (Theorem 1): by setting α=δ/N the total failure probability across next N planned trajectories is ≤ δ, assuming safety GP quantifies uncertainty correctly. 3) Predictive uncertainty (average determinant) provably decays as O(log(n)^{d+1}/n) during exploration (Theorems 2/3). 4) Empirically SAL-NX outperforms random selection in RMSE reduction and safe-region coverage, and is competitive vs. other non-safe methods; shown to be usable on a real high-pressure fluid system with up to 250 planned trajectories and real-time performance claims.",
            "limitations_or_failures": "Requires an initial set of safe trajectories (n0) chosen a priori; theoretical safety guarantee assumes the safety GP correctly quantifies uncertainty (calibrated posterior); computational cost of GP (K_n + σ^2 I inversion) can be high for large n·m (requires sparse/approximate GP methods or rank-one updates); computing trajectory safety probability ξ(τ) is in general intractable and requires Monte Carlo or EP approximations; overly conservative α (e.g., near 0) can prevent expansion beyond initial safe region; empirical results are shown mostly via plots without numeric summary tables; performance depends on GP hyperparameter choices and their training schedule.",
            "uuid": "e1115.0",
            "source_info": {
                "paper_title": "Safe Active Learning for Time-Series Modeling with Gaussian Processes",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Fisher-Info",
            "name_full": "Fisher Information Matrix based adaptive experimental design (Deflorian et al. style)",
            "brief_description": "An adaptive-design baseline that selects trajectories by maximizing the Fisher information matrix (sensitivity of GP mean w.r.t. trajectory endpoints), intended to place measurements where model parameters (endpoints) are most identifiable.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Fisher-information based design (reimplementation)",
            "agent_description": "Constructs an approximate Fisher Information matrix IFIM(η) whose entries are inner products of derivatives of the GP predictive mean with respect to trajectory endpoint parameters η (including contributions from previously observed trajectories); next trajectory endpoint selected by maximizing an objective I(IFIM(η)). This was reimplemented to compare to SAL-NX (without safety constraints for that comparison).",
            "adaptive_design_method": "Adaptive experimental design via Fisher information maximization (sensitivity-based criterion)",
            "adaptation_strategy_description": "Uses GP predictive mean sensitivity ∂µ(τ(η))/∂η to build IFIM(η) and greedily chooses η maximizing an information measure derived from IFIM; updates model after measurements and repeats. Focus is on parameter (endpoint) sensitivity rather than directly minimizing predictive posterior uncertainty.",
            "environment_name": "Same experimental settings as SAL-NX: synthetic NX time-series tasks and toy examples (comparison runs without safety constraint in that benchmark).",
            "environment_characteristics": "Continuous, dynamical time-series environments with trajectory-wise correlated observations and Gaussian measurement noise; unknown dynamics and safety signals (safety not enforced in the direct Fisher-info comparison).",
            "environment_complexity": "Same dimensionalities as SAL-NX experiments (e.g., synthetic NX d=4,m=5; higher-d real system d=7 when applied) though the Fisher-info experiment reported was primarily synthetic and without the safety constraint.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirically, the Fisher-information criterion was less effective at reducing predictive uncertainty (RMSE) compared to SAL-NX: the paper states the comparison 'does not look promising for the Fisher information matrix' and that Fisher-info focuses on slopes of GP mean instead of reducing posterior variance. No detailed scalar metrics provided in text; plotted comparisons show SAL-NX improves RMSE faster.",
            "performance_without_adaptation": "Not applicable (method itself is an adaptive design baseline).",
            "sample_efficiency": "Reported as less sample-efficient than SAL-NX for reducing posterior RMSE in experiments reported; specific sample counts to reach thresholds are not given.",
            "exploration_exploitation_tradeoff": "Implicit: Fisher-info focuses on information about parameters (sensitivity) which can bias sampling toward regions with large gradient of the mean (may act like exploration of informative slopes but not directly uncertainty-driven). No explicit probabilistic safety trade-off mechanism in the reimplemented comparison (safety constraint was removed to match original method's assumptions).",
            "comparison_methods": "Compared to SAL-NX (without safety) and random selection in numerical experiments (SAL-NX reduced RMSE more effectively).",
            "key_results": "Fisher-information based design prioritized mean-slope sensitivity (endpoint-parameter identifiability) and, in the reported experiments, was less effective than SAL-NX at reducing predictive uncertainty (RMSE). Additionally, when safety was not enforced both methods without safety incurred many safety violations (&gt;40% on average in 5 repetitions).",
            "limitations_or_failures": "Does not provide safety guarantees and, when used without safety constraints, resulted in high safety violation rates in experiments. Tends to focus on mean sensitivity rather than directly minimizing posterior uncertainty, which can make it less effective as an active-learning strategy for reducing predictive error in GP time-series models.",
            "uuid": "e1115.1",
            "source_info": {
                "paper_title": "Safe Active Learning for Time-Series Modeling with Gaussian Processes",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Random-Safe",
            "name_full": "Random selection with safety constraint (baseline)",
            "brief_description": "A baseline method that randomly samples trajectory endpoints η and selects the first candidate that satisfies the probabilistic safety constraint ξ(η) &gt; 1−α, used to benchmark SAL-NX.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random selection (with safety filtering)",
            "agent_description": "Randomly samples trajectory parametrizations (endpoints η) uniformly in the domain and accepts the first candidate that the safety GP predicts as safe (ξ(η) &gt; 1−α). The regression GP is updated after execution of accepted trajectories.",
            "adaptive_design_method": "None (random exploration subject to safety constraint); baseline for adaptive design",
            "adaptation_strategy_description": "No active selection based on information gain; randomness is filtered by safety GP to avoid obviously unsafe candidates; model updates depend on collected data but do not influence the candidate-generation process except via safety filtering.",
            "environment_name": "Same experimental environments used in SAL-NX comparisons (toy 2D, synthetic NX, and high-pressure fluid injection system).",
            "environment_characteristics": "Continuous, stochastic, time-series dynamical environments with unknown safety boundary; trajectory-wise correlated observations; safety-critical constraints present.",
            "environment_complexity": "Same as SAL-NX experiments (toy d=2; NX d=4,m=5; real system d=7,m=5; up to 250 trajectories planned in real use-case evaluations).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Empirically inferior to SAL-NX: slower RMSE decrease, slower safe-region coverage, higher variance and outliers in RMSE across runs (plots over 5 repetitions). When compared in the high-pressure system SAL-NX achieved faster model error reduction and better coverage.",
            "sample_efficiency": "Lower than SAL-NX; required many more random samples to reach comparable RMSE/coverage; exact sample counts not provided (experiments used hundreds of trajectories).",
            "exploration_exploitation_tradeoff": "No explicit mechanism; pure exploration subject to safety-filtering parameter α; tuning α trades off accepting fewer random candidates (conservative) vs potentially unsafe but informative ones.",
            "comparison_methods": "Compared directly to SAL-NX and Fisher-information baseline in experiments.",
            "key_results": "Random selection with safety produces significantly slower learning (higher RMSE over iterations) and slower expansion of the estimated safe region compared to SAL-NX; used to demonstrate the benefit of information-driven adaptive design in this setting.",
            "limitations_or_failures": "Inefficient sample usage (low sample efficiency), high variance between runs; performance limited by the chance of randomly sampling informative trajectories (even if safe), and cannot exploit posterior structure of GP to focus exploration where uncertainty is highest.",
            "uuid": "e1115.2",
            "source_info": {
                "paper_title": "Safe Active Learning for Time-Series Modeling with Gaussian Processes",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting",
            "rating": 2,
            "sanitized_title": "informationtheoretic_regret_bounds_for_gaussian_process_optimization_in_the_bandit_setting"
        },
        {
            "paper_title": "Safe and Automatic Parameter Tuning in Robotics (Bayesian Optimization with Safety Constraints)",
            "rating": 2,
            "sanitized_title": "safe_and_automatic_parameter_tuning_in_robotics_bayesian_optimization_with_safety_constraints"
        },
        {
            "paper_title": "Safe Exploration in Markov Decision Processes",
            "rating": 2,
            "sanitized_title": "safe_exploration_in_markov_decision_processes"
        },
        {
            "paper_title": "Safe Exploration for Active Learning with Gaussian Processes",
            "rating": 2,
            "sanitized_title": "safe_exploration_for_active_learning_with_gaussian_processes"
        },
        {
            "paper_title": "Online dynamic black box modelling and adaptive experiment design in combustion engine calibration",
            "rating": 2,
            "sanitized_title": "online_dynamic_black_box_modelling_and_adaptive_experiment_design_in_combustion_engine_calibration"
        },
        {
            "paper_title": "Near-Optimal Sensor Placements in Gaussian Processes",
            "rating": 1,
            "sanitized_title": "nearoptimal_sensor_placements_in_gaussian_processes"
        },
        {
            "paper_title": "Exploration vs Exploitation vs Safety: Risk-Aware Multi-Armed Bandits",
            "rating": 1,
            "sanitized_title": "exploration_vs_exploitation_vs_safety_riskaware_multiarmed_bandits"
        }
    ],
    "cost": 0.01464425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Safe Active Learning for Time-Series Modeling with Gaussian Processes
9 Feb 2024</p>
<p>Christoph Zimmer christoph.zimmer@de.bosch.com 
Mona Meister mona.meister@de.bosch.com 
Duy Nguyen-Tuong duy.nguyen-tuong@de.bosch.com </p>
<p>Bosch Center for Artificial Intelligence
RenningenGermany</p>
<p>32nd Conference on Neural Information Processing Systems
2018)MontréalNeurIPSCanada</p>
<p>Safe Active Learning for Time-Series Modeling with Gaussian Processes
9 Feb 2024D0E4167EFD7191B27F9F46B274E27FBFarXiv:2402.06276v1[cs.LG]
Learning time-series models is useful for many applications, such as simulation and forecasting.In this study, we consider the problem of actively learning time-series models while taking given safety constraints into account.For time-series modeling we employ a Gaussian process with a nonlinear exogenous input structure.The proposed approach generates data appropriate for time series model learning, i.e. input and output trajectories, by dynamically exploring the input space.The approach parametrizes the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations.We analyze the proposed algorithm and evaluate it empirically on a technical application.The results show the effectiveness of our approach in a realistic technical use case.</p>
<p>Introduction</p>
<p>Active model learning deals with the problem of sequential data labeling for learning an unknown function.Data points are sequentially selected for labeling such that the information required for approximating the unknown function is maximized, according to some measures.The overall goal is to create an accurate model without having to supply more data than necessary and, thereby reducing the annotation effort and measurement costs.Active learning has been well studied for classification tasks, e.g. for image labeling [12], but in the field of regression, the active learning approach, related to the optimal experimental design problem [8], is not yet widespread.</p>
<p>For actively learning time-series models representing physical systems, the data has to be generated such that the relevant dynamics can be captured.In practice, the physical system needs to be excited by dynamically moving around in the input space using input trajectories, such that the collected data, i.e. input and output trajectories, contain as much information about the dynamics as possible.Commonly used input trajectories include sinusoidal functions, ramps and step functions, white noise, etc. [13,17].When employing input excitation on physical systems, however, additional aspects of safety need to be considered.The excitation must not damage the physical system while dynamically exploring the input space, making it crucial to identify safe regions where dynamic excitation can be performed.</p>
<p>In this paper we consider the problem of safe exploration for active learning of time-series models.The goal is to generate input trajectories and output measurements which are informative for learning time-series models.To do so, our input trajectories are parametrized in consecutive sections, e.g. as consecutive piecewise ramps or splines.These consecutive sections of the input trajectory are determined stepwise in an explorative approach.Given observations, the next trajectory sections are determined by maximizing an information gain criterion with respect to the model.In this paper, we employ a Gaussian process with a nonlinear exogenous structure as the time-series model for which an appropriate exploration criterion is desired.An additional Gaussian process model is simultaneously used for predicting safe input regions, given safety requirements.The sections of The remainder of the paper is organized as follows.In Section 2, we provide an overview on related work.In Section 3, we introduce the algorithm for safe active learning of time-series models.Section 4 provides a theoretical analysis, and in Section 5, we highlight our empirical evaluations in learning time-series model in several settings.The Appendix contains the proofs of the theoretical analysis section and some more experimental investigations.</p>
<p>Related Work</p>
<p>Most existing work for safe exploration in unknown environments is in the reinforcement learning setting [16,10,9].For example, the safe exploration in finite MDP relies on the restriction of suitable policies, ensuring ergodicity at a user-defined safety level [16].In [10], the ergodic assumption for the MDPs is dropped by introducing fatal absorbing states.In [9], the authors consider the use of a multi-armed, risk-aware bandit setting to prevent hazards when exploring different tasks.Strategies for exploring unknown environments have also been reflected in the framework of global optimization with Gaussian processes [1,23,11].For example, [11] propose an efficient submodular exploration criterion for near-optimal sensor placements, i.e. for discrete input spaces.In [1], a framework is presented which yields a compromise between exploration and exploitation through confidence bounds.In [23], the authors show that under reasonable assumptions, strong exploration guarantees can be given for Bayesian optimization with Gaussian processes.Safe exploration using Gaussian processes (GP) has also been considered in the past, such as for safe active learning [20] and safe Bayesian optimization [2,24].In [24], for example, a two-steps process is proposed for a safe exploration and efficient exploitation of identified safe areas.In safe active learning, [20] proposes a method for safe exploration based on the GP variance for stationary, i.e. pointwise, measurements.In contrast to the work by [20], we consider the setting of safe dynamical exploration, i.e. using trajectory-wise measurements.This setting is especially useful when actively learning time-series models.</p>
<p>The problem of active learning for time-series models has not yet been considered extensively in the machine learning literature.Work on related topics is mostly in the field of online design of experiments, e.g.[6].In [6], the authors employ a parametric model for learning dynamical processes, in which the data for model learning is obtained by exploring using the Fisher information matrix.In contrast to their work, we explore unknown environments by employing a criterion defined for the non-parametric GP model, while also taking into account safety requirements.Furthermore, our proposed exploration scheme is rigorously analyzed providing further algorithmic insights.</p>
<p>Safe Active Learning for Time-Series Modeling</p>
<p>Our goal is to approximate an unknown function f : X ⊂ R d → Y ⊂ R. In the case of time-series models, e.g. the well-established nonlinear exogenous (NX) model, the input space consists of discretized values of the so-called manipulated variables [3].Thus, x k at time k can be given as
x k = (u k , u k−1 , . . . , u k−d2+1 ) ,
where (u k ) k , u k ∈ R d1 , [and not d 2 ] represents the discretized manipulated trajectory.Here, d 1 is the dimension of the system's input space, d 2 the dimension of the NX structure, and d = d 2 • d 1 .In practice, the elements u k are measured from physical systems and need not be equidistant, however, for notational convenience we assume equidistance in this setting.In general, the manipulated trajectories are continuous signals and can be explicitly controlled.In the model learning setting, we observe data in the form of n consecutive piecewise trajectories D f n = {τ i , ρ i } n i=1 , where the input trajectory τ i is a matrix and consists of m input points of dimension d, i.e. τ i = (x i 1 , . . ., x i m ) ∈ R d×m .The output trajectory ρ i contains m corresponding output measurements, i.e. ρ i = (y i 1 , . . ., y i m ) ∈ R m .The considered problem is to determine the next piecewise trajectory τ n+1 as input excitation to the physical system such that the information gain of D f n+1 -with respect to modeling f -is increased.At the same time, τ n+1 should be determined subject to given safety constraints.In this section we elaborate on the setting and describe the algorithm.The definition of the considered information gain and corresponding analysis are provided in Section 4.</p>
<p>Modeling Trajectories with Gaussian Processes</p>
<p>We employ a Gaussian Process (GP) model to approximate the function f (see [19] for more details).A GP is specified by its mean function µ(x) and covariance function k(x i , x j ), i.e. f (x i ) ∼ GP(µ(x i ), k(x i , x j )).Given noisy observations of input and output trajectories, the joint distribution according to the GP prior is given as
p (P n |T n ) = N P n |0, K n +σ 2 I ,
where P n ∈ R n•m is a vector concatenating output trajectories and T n ∈ R n•m×d a matrix containing input trajectories.The covariance matrix is represented by K n ∈ R n•m×n•m .In this paper, we employ the Gaussian kernel as the covariance function, i.e. k(x i ,
x j ) = σ 2 f exp(− 1 2 (x i −x j ) T Λ 2 f (x i −x j )), which is parametrized by θ f = (σ 2 f , Λ 2 f )
. Furthermore, we have a zero vector 0 ∈ R n•m as mean, an n • m-dimensional identity matrix I, and σ 2 as output noise variance (see [19]).Given the joint distribution, the predictive distribution p(ρ * |τ * , D f n ) for a new piecewise trajectory τ * can be expressed as
p(ρ * |τ * , D f n ) = N (ρ * |µ(τ * ), Σ(τ * )) ,(1) with µ(τ * ) = k(T n , τ * ) T (K n +σ 2 I) −1 P n , Σ(τ * ) = k * * (τ * , τ * ) − k(T n , τ * ) T (K n +σ 2 I) −1 k(T n , τ * ) ,(2)
where k * * ∈ R m×m is a matrix with k * * ij = k(x i , x j ).The matrix k ∈ R n•m×m contains kernel evaluations relating τ * to the previous n input trajectories.As the covariance matrix K n is fully occupied, the input points x are fully correlated within a piecewise trajectory, as well as across different trajectories; this enables the exploitation of high capacity correlations.However, due to the potentially large dimension n•m, inverting the matrix K n +σ 2 I can be infeasible.GP approximation techniques can be employed, e.g. using sparse inducing inputs or variational approaches [18,22,26].</p>
<p>Modeling the Safety Condition</p>
<p>The safety status of the system is described by an unknown function g : X ⊂ R d → Z ⊂ R, mapping an input point x to a safety value z, which acts as a safety indicator.The values z are computed using information from the system, and are designed such that all values equal or greater than zero are considered safe for the corresponding input x.Example 1 shows a construction for computing z values.More examples can be found in the evaluation in Section 5. Example 1 (A safety indicator for a high-pressure fluid system).In a high-pressure fluid system, we can measure the pressure ψ for a given input state x.Additionally, we know the value of the maximal pressure ψ max which can act on the physical system.Given the current pressure ψ, the safety values z can be computed as
z(ψ) = 1 − exp((ψ − ψ max )/λ p ) ,(3)
where λ p describes the decline, when ψ increases towards ψ max .</p>
<p>Note that z is continuous and, intuitively, indicates the distance of a given point x from the unknown safety boundary in the input space.Thus, given the function g -or an estimate of it -we can evaluate the level of safety for a trajectory τ .We consider a trajectory as safe, if the probability that its safety values z are greater than zero is sufficiently large, i.e.
) = N ζ * |µ g (τ * ), Σ g (τ * ) ,(4)
with µ g (τ * ) and Σ g (τ * ) being the corresponding mean and covariance.The quantities µ g and Σ g are computed as shown in Eq. ( 2), then with Z n ∈ R n•m as the target vector concatenating all ζ i .By employing a GP for approximating g, the safety condition ξ(τ ) for a trajectory τ can be computed as
ξ(τ ) = z1,...,zm≥0 N ζ|µ g (τ ), Σ g (τ ) dz 1 , . . . , z m &gt; 1 − α .(5)
In general, the computation of ξ(τ ) is analytically intractable, and thus needs to rely on some approximation, such as Monte-Carlo sampling or expectation propagation [15].</p>
<p>The Algorithm</p>
<p>In the previous sections, we elaborated on the modeling of the predictive distribution and the safety condition for a given piecewise trajectory τ in the input space.For efficiently choosing an optimal τ , the trajectory needs to be appropriately parametrized.The most straightforward possibility is to parametrize in the input space.We illustrate the trajectory parametrization in the following Example 2, using ramp parameterization.</p>
<p>Example 2 (Consecutive ramps as piecewise trajectory).A ramp can be parametrized with its start and end point.As the start point is the last point of the previous trajectory, the end point η is the only free quantity, and therefore a ramp can be parametrized as
τ (η) = (x 1 (η), . . . , x m (η)) with for 1 ≤ k ≤ m : x k (η) = u 0 + k m (η − u 0 ), . . . , u 0 + k − d 2 + 1 m (η − u 0 )(6)
where u 0 is the start point of the ramp.For k −i ≥ 0, the manipulated input variable is on the currently planned trajectory, and for k − i &lt; 0 it can be read from the list of already executed trajectories.</p>
<p>Given a trajectory parametrization with its predictive distribution in Eq. ( 1) and safety condition in Eq. ( 5), the next piecewise trajectory τ n+1 (η * ) can be obtained by solving the following constrained optimization problem
η * = argmax η∈Π I (Σ(η))(7)s.t. ξ (η) &gt; 1 − α ,(8)
where η represents our trajectory parametrization, Π is domain of the manipulated variable, and I an optimality criterion we will discuss later.As shown in Eq. ( 7), we employ the predictive variance Σ from Eq. (1) for the exploration, which is common in the active learning setting, especially in combination with a GP model [20,14].In contrast to previous work, due to the nature of the considered trajectory, we have a covariance matrix Σ instead of the variance value usually employed in the active learning and Bayesian optimization setting [23].The covariance matrix is mapped by an optimality criterion I to a real number, as indicated by Eq. ( 7).Various optimality criteria can be used for I, as discussed in the system identification literature [8].For example, I can be the determinant, equivalent to maximizing the volume of the predictive confidence ellipsoid of the multi-normal distribution, the trace, equivalent to maximizing the average predictive variance, or the maximal eigenvalue, equivalent to maximizing the largest axis of the predictive confidence ellipsoid [8].</p>
<p>The constraint in Eq. ( 8) represents a probabilistic safety criterion, motivated by our probabilistic modeling approach for the safety.The probabilistic approach flexibly allows us to control the trade-off between exploration speed and safety consideration.For example, a 100% safe exploration would keep the algorithm from leaving the initial safe area and, hence, would not lead to an exploration of new areas.On the other hand, a 0% safe exploration will explore without any safe considerations which will result in many safety violations.This trade-off provides the users an additional degree of freedom, depending on how much they "trust" the behavior of their physical systems.</p>
<p>Algorithm 1 Safe Active Learning for Time-Series Modeling
1: Input: Safety threshold 0 ≤ α ≤ 1 2: Initialization: Collect n 0 safe trajectories, i.e. D f,g 0 = {τ i , ρ i , ζ i } n i=1 with n = n 0 . 3: for k = 1 to N do 4: Update regression model approximating f using D f k−1 = {τ i , ρ i } n i=1
, according to Eq. ( 1)</p>
<p>5:</p>
<p>Update safety model approximating g using
D g k−1 = {τ i , ζ i } n i=1
, according to Eq. (</p>
<p>Determine new piecewise trajectory τ n+1 , by optimizing η according to Eq. (7 and 8)</p>
<p>7:</p>
<p>Execute τ n+1 on the physical system, while measuring ρ n+1 and ζ n+1 8:</p>
<p>Include new trajectories into D f k−1 and D g k−1 with n = n+1.9: end for 10: Update and return regression model and safety model Algorithm 1 summarizes the basic steps of the proposed algorithm, which needs to be initialized by n 0 safe trajectories.In practice, the initial trajectories are located in a small, safe region chosen beforehand using prior knowledge.The incremental updates of the GP models for new data, i.e. steps 4 and 5 in Algorithm 1, can be efficiently performed, e.g. through rank-one updates [21].The optimization problem in Eq. ( 7) can be solved using gradient-based optimization approaches, e.g.[4,5].In this paper, we employ the NX-structure in combination with the GP model for time-series modeling.However, this approach can also be extended to the general nonlinear auto-regressive exogenous case [3], i.e. a GP with NARX input structure
x k = ( y k , y k−1 , . . . , y k−q , u k , u k−1 , . . . , u k−d2+1 )
[and not u k−d ] with q determining the length of the output feedback.In this case, for optimization and planning of the next piecewise trajectories, one can use the predictive mean of p(ρ|τ , D f n ) as surrogate for y k .Note that the input excitation is still performed through the manipulated variable u k in the case of NARX.</p>
<p>Theoretical Results</p>
<p>In this section, we provide some results on the theoretical analysis of the proposed approach.First, we investigate the safety aspect of the algorithm.In Section 4.2, we provide a bound on the decay rate of the predictive variances for the case when the criterion I is a determinant, i.e.I (Σ(η)) = det (Σ(η)).The proofs can be found in the Appendix.</p>
<p>Safe Exploration</p>
<p>To satisfy the safety requirements, it is necessary to bound the probability of failures during exploration.Theorem 1 provides an upper bound on the probabilities for unsafe trajectories.Theorem 1.Let us assume that we have recorded n 0 initial safe trajectories, and that their observations are enough to model g well, in the sense that our GP quantifies the uncertainty of predictions for g correctly, i.e.P (µ g − νσ g ≤ z ≤ µ g + νσ g ) = Erf(ν/ √ 2) for all ν ≥ 0. Let δ ∈ [0, 1] be the desired failure probability when determining the next N consecutive piecewise trajectories.Set α = δ/N and let this α be the probability bound for a trajectory being unsafe (as in Eq. ( 5)).Then, the iterative exploration for the next N trajectories is unsafe with probability at most δ, i.e.
P ∪ n0+N i=n0+1 g(x i j ) &lt; 0 for some 1 ≤ j ≤ m|ξ(τ i ) &gt; 1−α ≤ δ.
Theorem 1 supplies us with a useful rule of thumb to select α for sequentially determining the next N trajectories.</p>
<p>Decay of Predictive Variance</p>
<p>The remainder of the analysis is to show that the proposed exploration scheme makes the predictive uncertainty Σ decrease as n increases.In this paper we use the determinant of Σ as an exploration criterion, which has been shown to have a close relationship to the information gain [14,23], defined as the mutual information I.</p>
<p>First, we point out that this relationship still holds true in case of trajectories as observations.Subsequently, we introduce the maximum information gain as an upper bound, which can further be used to show the decrease of the predictive uncertainty.Lemma 1 clarifies the relationship between determinant and mutual information.Let us denote the predictive variance after recording i − 1 trajectories as Σ i−1 (τ i ), Σ 0 (τ 1 ) = k * * (τ 1 , τ 1 ), and set ρi = f (x i 1 ), . . ., f (x i m ) .Lemma 1.The mutual information I({ρ i } n i=1 ; {ρ i } n i=1 ) can be related to the predictive co-variances Σ i−1 (τ i ) as follows
I ({ρ i } n i=1 ; {ρ i } n i=1 ) = 1/2 n i=1 log |I m + σ −2 Σ i−1 (τ i )|
Next, we introduce the maximum information gain after observing n trajectories as
γ n := max {τ i} n i=1 ⊂X m I({ρ i } n i=1 ; {ρ i } n i=1
), (see Srinivas et al. [23] for more details).The maximum information gain is the information which could be gathered when exploring the system in a noniterative way, by optimally designing all trajectories simultaneously (which is in practice hard as it would require a solution of a high dimensional optimization problem, and would not allow us to incorporate safety information from observations during the experiment).According to Srinivas et al. ( [23], Theorem 5) the maximum information gain satisfies γ n = O log(n) d+1 , i.e. the maximum information grows slower than the number of additional trajectories.This will be crucial later on, but first we investigate the relation between the determinant of the covariance and γ n .Using Lemma 1 the determinant of the covariance can be bounded, as given in Lemma 2. Lemma 2. After observing n trajectories {τ i } n i=1 (according to Eq. ( 7)), the determinant of the covariance is upper bounded by
1 n n i=1 |Σ i−1 (τ i )| ≤ C γ n n ,
where Σ i−1 is the predictive variance computed using the previous i − 1 trajectories, γ n is the maximum information gain, and
C = 2σ 2m f /log(1 + σ −2m σ 2m f ) is a constant.
The first step in proving Lemma 2 is to upper bound the predictive variance using the mutual information via Lemma 1. Subsequently, the mutual information is upper bounded by γ n .Using Lemma 2 and Theorem 5 in [23], we can provide a decay rate on the average determinant of predictive variances.Clarification/Errata: Theorem 2 contained a mistake.Corrected version is below and changes are marked in red color.Theorem 2. Let {τ i } n i=1 be n arbitrary trajectories within a compact and convex domain X such that each τ i shares the same start point with τ i , and k be a kernel function such that k(•, •) ≤ 1.If Σ i−1 come from our exploration scheme Eq. ( 7) (i.e.without safety considerations), then we have
1 n n i=1 |Σ i−1 (τ i )| = O log(n) d+1 n .
We sketch the proof here: as our algorithm (without safety considerations) always chooses the trajectory with the highest determinant (D criterion), the average determinant of an actively learned scheme is always higher than or equal to the average determinant of any other trajectory (τ i ) that shares the same start point.an arbitrary scheme.Therefore,
1 n n i=1 |Σ i−1 (τ i )| ≤ 1 n n i=1 |Σ i−1 (τ i )|, which is O(log(n) d+1 /n)
when employing Lemma 2 and the Theorem 5 of [23].</p>
<p>By Theorem 2, for any sequence of trajectories, the average of the determinants of their predictive covariances tends to zero.As the determinant corresponds to the volume of the confidence ellipsoid, we can conclude that the average volume of confidence ellipsoids tends to zero as well, indicating that on average, our predictions become precise.However, as the safety constraint in Eq. ( 8) changes at every iteration, we extend the statement of Theorem 2: Theorem 3. Let us assume that there exists a compact and convex domain X, and a kernel function k such that k(•, •) ≤ 1, that covers the whole area which is explorable (independent of whether it is safe or not).Then, the statement of Theorem 2 still holds for our Algorithm 1 with iteration-dependent safe areas S i .</p>
<p>Theorem 3 guarantees the decay of averaged determinants of covariances during safe exploration.As shown by the results, the current estimation of the safe region (green area) gradually covers the actual safe area (red line), and the approximation error gradually decreases (as shown in the subfigures).An illustrative video showing all iterations can be found in the Appendix.</p>
<p>Evaluations</p>
<p>In section 5.1 we illustrate the proposed approach using synthetic models, comparing our safe active learning approach (SAL-NX) with random selection using safety constraints.Subsequently, we employ the approach to learn a dynamics model of a physical, high-pressure fluid system in Section 5.2.For simplicity we employ ramps for the piecewise trajectory parametrization, but other curve parameterizations could also be used instead, e.g.spline parameterization.The form of the input trajectory has an impact on the excitation of the system, as comprehensively studied in the field of system identification [17].</p>
<p>Simulated Experiments</p>
<p>Experiment 1 In this experiment, a toy example is employed to illustrate the concept of input space exploration with piecewise trajectories and safe region detection.A function f : R 2 → R, f (x) = (x (1) −2) 2 +(x (1) −2)(x (2) −2)+(x (2) −2) 2 with x = (x (1) , x (2) ) is used as the ground-truth.An observation is given by y = f (x) + ϵ with ϵ ∼ N (0, 1).The safe region is characterized by g : R 2 → R with g(x) = (x (1) −5) 2 +(x (1) −5)(x (2) −5)+(x (2) −5) 2 .The safety indicator z is given as z = −0.005• g(x)+1+ς with ς ∼ N (0, 1).It is considered to be safe for z &gt; 0, otherwise unsafe.</p>
<p>We proceed as shown in Algorithm 1, where the piecewise trajectories are parametrized as 2D-ramps with 5 discretization points (i.e.m = 5, see Example 2).We start with 10 initial safe trajectories and consecutively determine new piecewise trajectories in the input space X, while also collecting outputs y and computing safety indicator values z.As the exploration progresses, the approximation of f and g becomes more and more accurate, as shown in Fig. 1.The current estimation of the safe region (green area) gradually covers the actual safe area, and the approximation error gradually decreases (as shown in the subfigures).An illustrative video showing all iterations can be found in the Appendix.</p>
<p>Experiment 2 In this experiment, we learn a time-series model given as a GP with NX-structure.</p>
<p>We have two manipulated variables u</p>
<p>k and u</p>
<p>k at time k.The NX-structure is determined to be
x k = (u (1) k , u (2) k , u (1) k−1 , u(2)
k−1 ), an input space with d = 4.The ground-truth models of f and g are provided in the Appendix.The piecewise trajectory is again parametrized as 4D-ramps with m = 5.We initialize the models using 10 collected piecewise ramps in a safe area, and start exploring in the input space.For a fair comparison, we benchmark the proposed algorithm against a random selection with safe constraints of next piecewise trajectories.Instead of optimizing the ramp parameter η as shown in Eq. ( 7) and ( 8), we randomly select η and pick the first one which fulfills the safety constraint ξ(η) &gt; 1 − α.Fig. 2 shows the results of the comparison of the proposed approach (SAL-NX) with random selection.</p>
<p>The results in Fig. 2 show that SAL-NX continuously improves the model approximation (shown as RMSE) and provides fast coverage of safe regions.The models for f and g are updated after every iteration by including new sample points.The hyperparameters can be estimated beforehand or updated after every iteration.For the required number of initial trajectories, we refer to the lower bound as given in [20].For computing the safety condition ξ(τ ) from Eq. ( 5), we employ Monte-Carlo sampling.Our experiments are performed on a desktop computer.The algorithm is sufficiently fast for real-time applications.</p>
<p>In this experiment, we also compare our exploration approach with the one proposed in [6], however, without safety requirements in order to cope with the setting from [6].We adapt their criterion based on the Fisher information for our GP model by employing the GP mean function.Additionally, we also compare the decrease in RMSE of the Fisher information based criterion to the decrease in RMSE of our algorithm.The results can be found in the Appendix 7.4 and show a competitive performance of our approach.</p>
<p>Learning a Surrogate Model of the High-Pressure Fluid System</p>
<p>The Use Case As a realistic technical use case, we employ the approach to actively learn a surrogate model of a high-pressure fluid injection system, as shown in Figure 3.Such systems are widely used in industry, e.g. in the automotive domain for injection of fuel into the combustion engine [25].The physical injection system is controlled by an actuation signal v k and the speed of an external engine n k , for every time step k.The goal is to obtain a surrogate model predicting the rail pressure ψ k , which determines the amount of fluid coming out of the outlets.Due to the nature of the fluid and the mechanical components, the dynamics of the whole system are nonlinear, and thus model learning is an appropriate alternative compared to analytical models.However, generating the data for learning a time series surrogate model by varying the actuation signal and engine speed is not simple, as an inappropriate combination of them would result in hazardously high rail pressures, damaging the physical system.Learning Time-Series Surrogate Models Due to the safety requirements and the fact that the safety boundary is not known beforehand, our safe active learning approach is very appropriate for approximating the dynamics model.The employed NX-structure is chosen to be
x k = (n k , n k−1 , n k−2 , n k−3 , v k , v k−1 , v k−3
).We again parameterize with piecewise ramps in this 7D input space.The safety indicator value z is computed as shown in Example 1, with ψ max = 18 MPa being the maximally allowed rail pressure.It should be noted that here z is computed as a function of the target output ψ, in constrast to the experiments in Section 5.1, where z is a function of the input.We initialize the model with 25 trajectories sampled around a safe point chosen by a domain expert.Subsequently, we start exploring the input space dynamically, considering both the safety constraint and the model information gain, while measuring the actuation and speed signals as input and the rail pressure as output.</p>
<p>Figure 4 shows the results after exploring the input space with 250 consecutive ramp trajectories, each consisting of m = 5 discretization points.We update the hyperparameters after every iteration.We compare our SAL-NX approach with the random selection, as described in the previous experiment.The figure also shows the impact of varying the threshold value α on both the model approximation error and the percenteage of selected unsafe trajectories.In practice, the execution of the trajectories on the physical system is interrupted, when the system notices a violation of the maximal pressure ψ max .The selected piecewise trajectory is then indicated as unsafe.For the evaluation of the coverage (second picture from the left), the "ground-truth" safe region is estimated beforehand with an extensive procedure.</p>
<p>Conclusions</p>
<p>In this paper we present an approach for active learning of a time-series model, given as a GP model with NX-structure.In this setting, the exploration is performed while taking safety requirements into account.For the successful application of the algorithm, it is crucial that the system can be actively controlled by a set of inputs and a safety signal can be observed during the system's operation.The proposed approach is evaluated on toy examples, as well as on a realistic technical use case.The results show that this approach is appropriate for real-world applications, especially, in the industrial setting, where safety is a key requirement during operation.</p>
<p>Safe Active Learning for Time-Series Modeling with Gaussian Processes</p>
<p>Christoph Zimmer (1) , Mona Meister (1) and Duy Nguyen-Tuong (1)   1 Bosch Center for Artificial Intelligence, Robert Bosch GmbH, Renningen, Germany 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada</p>
<p>Appendix</p>
<p>This Appendix contains the proof of the safety theorem, the proofs of the lemmas and theorems on the variance decay as well as some details on the numerical evaluation section.</p>
<p>Proof for the Safety Theorem</p>
<p>In the following, we prove Theorem 1, providing a safety guarantee for algorithm 1.</p>
<p>Proof of Theorem 1.According to the constraint in Eq. ( 5), each trajectory τ i, i = n0 + 1, . . ., n0 + N is unsafe with probability at most α (if the GP quantifies the uncertainty of predictions for g correctly).Thus, a trajectory τ i is critical (it is unsafe but considered as safe), if ξ(τ i) &gt; 1−α and g(x i j ) &lt; 0 for at least one 1 ≤ j ≤ m.Using the union bound, we have
P ∪ n 0 +N i=n 0 +1 g(x i j ) &lt; 0 for at least one 1 ≤ j ≤ m|ξ(τ i) &gt; 1−α ≤ n 0 +N i=n 0 +1 P g(x i j ) &lt; 0 for at least one 1 ≤ j ≤ m|ξ(τ i) &gt; 1−α ≤ n 0 +N i=n 0 +1 α = N α = N δ/N = δ .
Note that ξ(τ i) is a random variable because ρ is a random variable, and therefore µ g and Σg as well, depending on the previously recorded values for ρ j , 1 ≤ j ≤ i − 1.Additionally, g(x i j ) is a random variable as well (even though g is deterministic) because the choice of trajectories (and therefore points x i j ) depends on the optimization problem (Eq.7-8)), the solution of which depends on g, which itself depends on ρ.</p>
<p>Proofs on Reduction of Uncertainty</p>
<p>Next, we will prove the lemmas and theorems on the variance decay from section 4.2.The proofs for Lemma 1, 3, and 4 are motivated by Lemma 5.3 and 5.4 of [23] but extended to the situation of trajectories.</p>
<p>First, we prove Lemma 1 to relate the information gain I to the covariance Σ.</p>
<p>Proof of Lemma 1.The mutual information can be expressed in terms of entropy as
I ({ρ i } n i=1 ; { ρi } n i=1 ) = H ({ρ i } n i=1 ) − H ({ρ i } n i=1 |{ ρi } n i=1 )
As {ρ i } n i=1 |{ ρi } n i=1 is Gaussian with variance σ 2 Inm, it holds that H({ρ i } n i=1 |{ ρi } n i=1 ) = 1/2 log |2π eσ 2 Inm| = nm/2 log(2πeσ 2 ).Furthermore, using entropy rules, we obtain Alltogether, this leads to Proof.Let e1, . . ., em be the eigenvalues of Σi−1(τi).Then it holds that |Σi−1(τi)| = m i=1 ei and furthermore that trace(Σi−1(τi)) = m i=1 ei.Keeping in mind that the geometric mean is less than or equal to the arithmetic mean, we have
H ({ρ i } n i=1 ) = H {ρ i } n−1 i=1 + H ρ n |{ρ i } n−1I ({ρ i } n i=1 ; {ρ i } n i=1 ) = H ({ρ i } n i=1 ) − H ({ρ i } n i=1 |{ ρi } n i=1 ) = n j=1 H(ρ i |{ρ i } j−1 i=1 ) − nm/2 log(2πeσ 2 ) = n j=11) + (1 − a b ) • 1 ≥ a b log(1 + b) + (1 − a b ) log(1) = a b log(1 + b) ,(9)
where the second step follows with the concavity of the log function.With this in hand, we can now conclude that |Σi−1(τi
)| = σ 2m σ −2m |Σi−1(τi)| ≤ σ 2m σ −2m σ 2m f log(1 + σ −2m σ 2m f ) log 1 + σ −2m |Σi−1(τi)| = σ 2m σ −2m σ 2m f log(1 + σ −2m σ 2m f ) log 1 + |σ −2 Σi−1(τi)| = C1 log 1 + |σ −2 Σi−1(τi)| ,(10)
where C1 := Dividing by n and upper bounding I by γn yields the desired results.</p>
<p>This statement holds for trajectories from a run of our algorithm without safety.To ensure a good model, we are more interested in the behavior for arbirtrary trajectories, not only those that were chosen by our algorithm.Using a result from Srinivas [23], Theorem 2 extends the statement to an asymptotic behavior of the covariances of other arbitrary trajectories.</p>
<p>Proof of Theorem 2. As our exploration scheme in Eq. ( 7) always chooses the trajectory with the highest determinant (D criterion), the average determinant of an actively learned scheme is always greater than or equal to the average determinant of any other trajectory (τ i) that shares the same start point.an arbitrary scheme.Therefore:
1 n n i=1 |Σi−1(τi)| ≤ 1 n n i=1 |Σi−1(τi)| which is O log(n) d+1
n according to Lemma 2 and Theorem 5 of [23].</p>
<p>By Theorem 2, the average of the determinants of the covariance matrices tends to zero.However, it is phrased for a domain X which is invariant over the iteration of our algorithm.This means it does not yet take safety constraints into account.To this end, Theorem 3 extends Theorem 2 such that it still holds for safe exploration.</p>
<p>Proof of Theorem 3. As it holds that Si ⊆ X for all i, it follows that
max τ i ∈S m i |Σi−1(τ i)| ≤ max τ i ∈X m |Σi−1(τ i)| for 1 ≤ i ≤ n.
The remainder of the proof is analogous to the proof of Theorem 2.</p>
<p>Numerical Evaluation Details</p>
<p>For Experiment 2 in Section 5.1 the ground truth regression function f depends on the current state u k = u</p>
<p>k , u</p>
<p>and the previous state
u k−1 = u (1) k−1 , u(2)
k−1 .Using x k = (u k , u k−1 ), the ground truth function is given by
f (x k ) = f (u k ) + f (u k ) − f u (1) k−1 , u(2)k u (1) k − u (1) k−1 + f (u k ) − f u (1) k , u(2) k−1 u (2) k − u (2) k−1(11)
with a base function f :
[−5, 45] 2 ⊂ R 2 → R, where f (u) = (u (1) − 2) 2 − (u (1) − 2)(u (2) − 2) + (u (2) − 2) 2 .
The observations are drawn from the model y = f (x) + ϵ with noise ϵ ∼ N (0, 1).</p>
<p>The ground truth safety is constructed similarly with a base function g : [−5, 45] 2 ⊂ R 2 → R given by
g(u) = (u (1) − 5) 2 − (u (1) − 5)(u (2) − 5) + (u (2) − 5) 2
and a history dependent ground truth g g
(x k ) = g(u k ) − g(u k ) − g (u(1)
k−1 , u
k ) u (1) k − u (1) k−1 − g(u k ) − g (u (1) k , u (2) k−1 ) u (2) k − u (2) k−1 .(2)
The safety observations are drawn from the model z = −0.005* g(x) + 1 + ϵg with normally distributed noise ϵg with zero mean and standard deviation 0.01.</p>
<p>Further remarks on the numerical details:</p>
<p>• To investigate the effects of hyperparameter training, we conducted one experiment (Experiment 2) with fixed hyperparameters set as  • We update the GP for the RPM modell in every 10 th iteration to reduce computational time for training.
θ f = (σ 2 f , Λ 2 f , σ 2 f,n ) = (2.
• The coverage of the safe region, Figure 2 and 4 2nd plot samples 1000 for (Experiment 2) and 2000 (for the High-Pressure Fluid System) trajectories randomly in the input space with a discretization of 5 points.For each of the discretized points (including its history structure), we check whether it is safe and whether the model thinks that it is safe (mean prediction below safety threshold).The fraction of correctly identified points to all points is then the health coverage.</p>
<p>• For the High-Pressure Fluid System, we perform all except the initial training without the initial (non-actively gathered) data to reduce the impact of the initial data gathered in a rather small area of the space.</p>
<p>• We define prior GP mean and standard devation as zero and one for the regression GP as well as -2 and 1 for the safety GP as we want to consider areas without knowlegde as unsafe.</p>
<p>Benchmarking without safety consideration</p>
<p>We have benchmarked our newly proposed SAL-NX algorithm against a random selection approach and shown the superiority of our algorithm with respect to decreasing RMSE and speed of learning the safe area (Figures 2 and 4).Additionally, one might also think of a comparison to a more sophisticated approach than random selection.However, we are not aware of any algorithm that tackels all three challenges: active learning, safe learning and learning of dynamics models.There is an approach that is designed for dynamics model learning based on work of [6] and [7].We reimplemented their approach and code as follows in order to allow for the comparison on model learning without taking safety considerations into account as their algorithm does not consider safety aspects.</p>
<p>The approach of [6] and [7] is based on building a Fisher information matrix.Intuitively speaking, the Fisher information matrix measures the sensitivity of the model response with respect to parameters -in our case the endpoints of the trajectories.By choosing a point with high sensitivity, one can place measurements to locations where they provide much information on the parameter.The Fisher Information matrix is given by
IFIM(η) = (IFIM(η)) m,m j=1,k=1
with entries (IFIM(η)) j,k = ∂µ(τ (η) T ) ∂η (j)   ∂µ(τ (η)) ∂η (k) + n−1 i=1 ∂µ(τ (η i )) T ∂η (j)   ∂µ(τ (η i )) ∂η (k)   where µ(τ (η i )) is the mean vector of the i-th trajectory with endpoint η i .Similarly as in equation ( 7), we choose the next trajectory by optimizing the information gain -now measured by the Fisher Information matrix η * = argmax η∈Π I (IFIM(η)) .</p>
<p>Figure 5 (left panel) shows a comparison similar to Figure 2 (left panel) between our proposed SAL-NX algorithm without safety consideration and the approach of [6] and [7] based on the Fisher information matrix.This first comparison does not look promising for the Fisher information matrix.The main difference of SAL-NX without safety consideration and the Fisher Information matrix is that SAL-NX reduces the predictive uncertainty while the Fisher information focuses on the slope of the GP mean prediction.</p>
<p>Figure 5 (right panel) shows how the safe area is learned.Note, that even though there is no safety restriction (as given by Eq. ( 8)) for the algorithm in exploring the input space, safety measurements z are still recorded and, therefore, a safety model can be learned.Our SAL-NX approach seems to show competitive performance in learning the safety model as well.</p>
<p>None of both approaches without safety considerations leads to safe learning as in both approaches the percentage of safety violations is high: more than 40% in average over 5 repetitions.</p>
<p>Code</p>
<p>The code will be is released on GitHub : https://github.com/ChristophZimmer/Dynamic-Safe-Active-Learning</p>
<p>Figure 1 :
1
Figure 1: The columns show the progress of the approximation of f (inlay) and the identified safety region (main figure) at different iterations.Each iteration corresponds to a consecutive planning of a new piecewise trajectory (here: 2D ramp).As shown by the results, the current estimation of the safe region (green area) gradually covers the actual safe area (red line), and the approximation error gradually decreases (as shown in the subfigures).An illustrative video showing all iterations can be found in the Appendix.</p>
<p>Figure 2 :
2
Figure 2: The first two pictures from the left show the comparison of the SAL-NX (red line) with random selection (blue line).SAL-NX yields faster convergence in model approximation (left picture) and coverage of safe regions (right picture), while having less variance and outliers (indicated as small circles).The last two pictures show the impact of the safety threshold α.The left picture shows the RMSE of SAL-NX for 4 different values of α.The right picture shows the model approximation error as RMSE (red line) and percentage of unsafe trajectories (blue line) as a function of α.All pictures show boxplots over 5 repetitions.The plot contained inconsistencies which are now corrected.The old plot is left above for comparison.</p>
<p>Figure 3 :
3
Figure 3: High-pressure fluid injection system with controllable inputs v k , n k and measured output ψ k (picture taken from [25]).</p>
<p>Figure 4 :
4
Figure 4: The first two pictures from the left show the comparison of the SAL-NX (red line) with random selection with safe constraints (blue line), with respect to model approximation and coverage of safe regions.Here, α =0.5 0.8 and 250 trajectories are planned.The last two pictures show the impact of the safety threshold α on the approximation error, and failures during exploration.The results are displayed as a boxplot over 5 repetitions.The evaluation and plot contained inconsistencies which are now corrected.The old plot is left above for comparison.</p>
<p>i=1 and H(ρ n |{ρ i } n−1 i=1 ) = 1/2 log |2π e(σ 2 Im + Σn−1(τn))| = 1/2 log |2π e σ 2 (Im + σ −2 Σn−1(τn))| = 1/2 log (2π e σ 2 ) m + 1/2 log |(Im + σ −2 Σn−1(τn))| .</p>
<p>log |Im + σ − 2
2
/2 log (2π e σ 2 ) m + n j=1 1/2 log |(Im + σ −2 Σj−1(τj))| − nm/2 log(2πeσ 2 ) Σi−1(τ i)| .Lemma 1 relates the information I to the covariance Σ, but not yet to the determinant of the covariance |Σ|.To link the determinant of the covariance |Σ| to the mutual information in Lemma 4, we first state the auxilliary Lemma 3. Lemma 3.For i = 1, . . ., n, |Σi−1(τi)| ≤ C1 log(1 + |σ −2 Σi−1(τi)|) is satisfied, where C1 = σ 2m f log(1+σ −2m σ 2m f ) .</p>
<p>m</p>
<p>|Σi−1(τi)| ≤ trace(Σi−1(τi)) m .As the trace of Σ contains only predictive variances (and no correlation terms), we can upper bound them by trace(Σi−1(τi)) j , j = 1, . . ., m, i = 1, . . ., n, are the m points of the i-th trajectory.This results in the upper bound |Σi−1(τi)| ≤ σ 2•m f .Now, it holds that a := σ −2m |Σi−1(τi)| ≤ σ −2m σ 2m f =: b.Furthermore, we have log(1 + a) = log a b (1 + b</p>
<p>fC1 log 1 +
1
log(1+σ −2m σ 2m f ) .Lemma 4. The sum of the determinants of variances is upper bounded by the mutual information, i.e.n i=1 |Σi−1(τi)| ≤ C I ((ρ) n i=1 ; {ρ} n i=1 ) .Proof.For C := 2C1, it holds |σ −2 Σi−1(τi)| = n i=1 C1 log |I| + |σ −2 Σi−1(τi)| ≤ n i=1 C1 log |I + σ −2 Σi−1(τi)| = C I ((ρ) n i=1 ; {ρ} n i=1 ) .Here, the first step holds due to Lemma 3, the third as |I| + |A| ≤ |I + A| for a positive semidefinite matrix A as |I| + |A| = m i=1 1 + m i=1 ai ≤ m i=1 (1 + ai) = |I + A| with ai being eigenvalues of A and ai ≥ 0 as A positive semidefinite.The last step holds according to Lemma 1.Note that (A + I)vi = Avi + Ivi = ai • vi + vi = (ai + 1)vi for vi being an eigenvector for eigenvalue ai shows that ai + 1 is indeed an eigenvalue of A + I.As we have now established a link between the determinant of the covariance -our criterion -and the mutual information I, we can upper bound the average of the determinant of the covariances and prove Lemma 2. Proof of Lemma 2. According to Lemma 4 it holds n i=1 |Σi−1(τi)| ≤ C I ({ρ} n i=1 ; {ρ} n i=1 ) .</p>
<p>25, 0.25, 0.25, 0.250.0025,0.25, 0.0025, 0.25, 0.0025) θ up f = (25, 25, 25, 25, 4, 6.25, 6.25, 1, 0.25) θ log g = (0.25, 0.25, 0.25, 0.25, 0.0025, 0.25, 0.0025, 0.01, 0.0025) θ up g = (25, 25, 25, 25, 4, 6.25, 6.25, 25, 0.0625) • Data is normalized before GP hyperparameter training.</p>
<p>Figure 5 :
5
Figure 5: Right panel: As figure 2 but without safety consideration.Blue color representing the approach based on the Fisher information matrix, red color still representing our SAL-NX approach (now without safety consideration).</p>
<p>, ..., z m |τ ) dz 1 , ..., z m &gt; 1 − α , with α ∈ (0, 1] representing the threshold for considering τ unsafe.Given dataD g n = {τ i , ζ i } n i=1 , with ζ i = (z i 1 , . .., z i m ) ∈ R m ,we employ a GP to approximate the function g.The predictive distribution p(ζ * |τ * , D g n ) given a piecewise trajectory τ * is then computed as p(ζ * |τ * , D g n
p(z 1z1,...,zm≥0</p>
<p>25, 2.25, 2.25, 2.25, 1, 0.25) and θg = (σ 2 g , Λ 2 g , σg,n) = (2.25,2.25, 2.25, 2.25, 4, 0.00025) and one experiment (Rail-Pressure) with hyperparameter training where we used the following upper and lower constrained for the hyperparameters</p>
<p>Using Confidence Bounds for Exploitation-Exploration Trade-Offs. P Auer, Journal of Machine Learning Research. 2002</p>
<p>Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics. F Berkenkamp, A Krause, A P Schoellig, arXivFebruary 2016Technical report</p>
<p>S Billings, Nonlinear System Identification: Narmax Methods in the Time, Frequency, and Spatio-Temporal Domains. John Wiley &amp; Sons2013</p>
<p>A trust region method based on interior point techniques for nonlinear programming. R H Byrd, J C Gilbert, J Nocedal, Mathematical Programming. 8912000</p>
<p>On the convergence of reflective newton methods for large-scale nonlinear minimization subject to bounds. T F Coleman, Y Li, 1992Ithaca, NY, USACornell UniversityTechnical report</p>
<p>Online dynamic black box modelling and adaptive experiment design in combustion engine calibration. M Deflorian, F Kloepper, J Rueckert, 6th IFAC Symposium Advances in Automotive Control. Elsevier2010</p>
<p>Design of experiments for nonlinear dynamic system identification. M Deflorian, F Kloepper, J Rueckert, Proceedings of the 18th World Congress. the 18th World CongressThe International Federation of Automatic Control2011. 2011</p>
<p>Model-Oriented Design of Experiments. V Fedorov, P Hackl, Lecture Notes in Statistics. 2012Springer</p>
<p>Exploration vs Exploitation vs Safety: Risk-Aware Multi-Armed Bandits. N Galichet, M Sebag, O Teytaud, Proceedings of the 5th Asian Conference on Machine Learning. the 5th Asian Conference on Machine Learning2013</p>
<p>Reinforcement Learning with Bounded Risk. P Geibel, Proceedings of the 18th International Conference on Machine Learning. C E Brodley, A P Danyluk, the 18th International Conference on Machine Learning2001</p>
<p>Near-Optimal Sensor Placements in Gaussian Processes. C Guestrin, A Krause, A Singh, Proceedings of the 22nd International Conference on Machine Learning. the 22nd International Conference on Machine Learning2005</p>
<p>Multiclass active learning for image classification. A J Joshi, F Porikli, N Papanikolopoulos, IEEE Conf. on Computer Vision and Pattern Recognition. 2009</p>
<p>series in signal processing, optimization, and control. L Ljung, T Söderström, 1985MIT PressTheory and Practice of Recursive Identification</p>
<p>Information-Based Objective Functions for Active Data Selection. D J C Mackay, Neural Computation. 441992</p>
<p>Expectation Propagation for Approximate Bayesian Inference. T P Minka, Uncertainty in Artificial Intelligence. Morgan Kaufmann2001</p>
<p>Safe Exploration in Markov Decision Processes. T M Moldovan, P Abbeel, Proceedings of the 29th International Conference on Machine Learning. the 29th International Conference on Machine Learning2012</p>
<p>System Identification: A Frequency Domain Approach. R Pintelon, J Schoukens, 2012Wiley</p>
<p>A Unifying View of Sparse Approximate Gaussian Process Regression. J Quiñonero-Candela, C E Rasmussen, Journal of Machine Learning Research. 2005</p>
<p>C E Rasmussen, C K I Williams, Gaussian Processes for Machine Learning. The MIT Press2006</p>
<p>Safe Exploration for Active Learning with Gaussian Processes. J Schreiter, D Nguyen-Tuong, M Eberts, B Bischoff, H Markert, M Toussaint, ECML/PKDD. 20159286</p>
<p>Low rank updates for the Cholesky decomposition. M Seeger, 2007</p>
<p>Sparse Gaussian Processes using Pseudo-inputs. E L Snelson, Z Ghahramani, Advances in Neural Information Processing Systems. 2006</p>
<p>Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting. N Srinivas, A Krause, S M Kakade, M W Seeger, Transactions on Information Theory. 2012</p>
<p>Stagewise safe bayesian optimization with gaussian processes. Y Sui, V Zhuang, J Burdick, Y Yue, 35th International Conference on Machine Learning. 2018</p>
<p>Model-based calibration of engine controller using automated transient design of experiment. N Tietze, U Konigorski, C Fleck, D Nguyen-Tuong, Internationales Stuttgarter Symposium. Fachmedien WiesbadenSpringer201414</p>
<p>Variational Learning of Inducing Variables in Sparse Gaussian Processes. M Titsias, Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. the Twelfth International Conference on Artificial Intelligence and Statistics2009</p>            </div>
        </div>

    </div>
</body>
</html>
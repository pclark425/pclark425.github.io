<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2882 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2882</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2882</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-767a7e949ba4520888e7442ee01e6a37c254fc53</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/767a7e949ba4520888e7442ee01e6a37c254fc53" target="_blank">CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> CLIN is presented, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates.</p>
                <p><strong>Paper Abstract:</strong> Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory centered on causal abstractions (rather than general"helpful hints") that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2882.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2882.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual Learning from Interactions (CLIN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented language-agent that continually learns causal abstractions (textual memories) from trial-and-error in a text-based simulator (ScienceWorld), using frozen LLMs for controller, executor and a memory-generator to update a persistent, dynamic NL memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CLIN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>CLIN is a three-module generative agent (controller, executor, memory) with a fourth module (memory-generator) used for learning. The controller is a frozen LLM prompted with the current task, retrieved memory items, and the trial history (sequence of goal-action-observation triples) to generate the next goal. The executor is a frozen LLM that converts a goal into a valid environment action given admissible action templates; generated candidate actions are validated against the simulator's admissible action set and, if needed, mapped via sentence-transformer embeddings (similarity threshold 0.9) or refined iteratively (up to 5 retries). The memory-generator (frozen LLM) is invoked at the end of each trial to reflect on the trial and generate/update a persistent textual memory of causal abstractions that the controller can retrieve and use in future trials. CLIN also supports meta-memory creation across episodes to generalize to unseen environments/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>gpt-4 (used for controller, executor, and memory-generator; model size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Persistent dynamic textual memory of causal abstractions (semantic/episodic hybrid) plus meta-memory (generalized abstractions across episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory is a persistent list of natural-language sentences expressing causal abstractions (templates like "X SHOULD BE NECESSARY to Y" / "X MAY ..." / "X DOES NOT CONTRIBUTE to Y"). At end of each trial the memory-generator (a frozen LLM) is prompted with the trial trace (goal-action-observation tuples plus final reward) and the three most recent memories to produce an updated memory. The memory-generator performs saliency-based pruning (using final trial reward) so only important insights are kept. Retrieval: the controller selects one or more memory items by matching the current task description and trial history, and appends selected learnings to the controller prompt when generating next goals. Meta-memory: for cross-environment/task generalization the system selects the most successful trial per episode (prioritized replay; archive size=10) and the memory-generator synthesizes a meta-memory (generalized causal abstractions) conditioned on target task/environment. Memory entries carry uncertainty markers ("may" vs "should") reflecting confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>No fixed maximum size specified; observed generated memory size is far smaller than the number of actions executed per trial. Meta-memory archive size is given as 10 (hyperparameter). Memory-generator uses the three most recent per-trial memories when updating.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval is done by the controller using the task instruction and trial history to select memory items judged useful; selection is prompt-driven (no learned retrieval model). Action validity mapping (executor) uses sentence-transformer embeddings for matching generated action to admissible actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Memories are produced/updated after each trial by the memory-generator LLM, which is given the trial trace, final reward (converted to NL feedback), and the three most recent memory versions. A saliency-based pruning step keeps high-utility insights (based on reward). Meta-memory is generated from the best trials across episodes (prioritized level replay selecting most successful trial per episode, archive size 10).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>High-fidelity text-based simulator with natural-language actions and observations, partial observability, 10 sub-places (e.g., kitchen, greenhouse, workshop), many objects with states, large action space (admissible action templates + object instantiations), tasks spanning diverse science domains (thermodynamics, genetics, chemistry, friction, etc.), tasks categorized as Short (<37 oracle steps) or Long (>=37), evaluated across many environment configurations (164 task-environment combos in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Multiple reported metrics (final reward scores). Aggregated averages across 18 tasks/environments: BASE (initial / zero-shot) = 48.6 (avg reward), GEN-ENV (zero-shot with meta-memory) = 52.7, GEN-ADAPT (G+A; allow adaptation after transfer) = 69.5. Adaptation (ADAPT) average after retries: 62.2. Per-type: Short tasks ADAPT = 62.8, Long tasks ADAPT = 61.6. Per-task table entries available (see paper Table 1 / Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>BASE (empty/initial memory) performance reported as 48.6 average reward (this is CLIN's zero-shot starting point). Ablations: replacing structured causal memory with free-form/unstructured memory caused average reward drops (paper reports an average drop of 6 points in 10% of cases compared to CLIN with structured memory). Ablating the controller (which mediates use of memory) caused an 18-point drop in average reward, producing behavior equivalent to ReAct baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Key ablations: (1) Structured causal-abstraction memory vs free-form/unstructured memory — unstructured memory reduced average reward by ~6 points in 10% of cases, indicating structured causal templates help; (2) Ablating the controller (goal-generation step that leverages memory) reduced BASE performance by ~18 points and made agent equivalent to ReAct; (3) Meta-memory (compiled from best trials) improves zero-shot start and accelerates later adaptation (GEN-ENV start 52.7 vs BASE 48.6; continued gains to G+A 69.5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper compares structured causal-abstraction memory to free-form advice (unstructured memory) and finds structured memory outperforms unstructured (free-form) memory; no other memory types (e.g., vector DB, knowledge graph) are directly compared in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Persistently storing causal abstractions ("X is/should be necessary to Y" / "X does not contribute to Y") and refining them across trials enables frozen LLM agents to continually improve in text-based tasks; meta-memory (summaries of best memories across episodes) improves zero-shot generalization to new environments/tasks; retrieval quality and correct selection of memory items are critical and retrieval errors can limit performance; structured, task-relevant causal memory outperforms unstructured text advice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2882.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2882.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reflective LLM-based agent that generates language-based insights by reflecting on a failed attempt and uses those reflections to propose improved plans for retries; used as a strong baseline in ScienceWorld experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reflexion generates reflections (language insights) after failed attempts that are then used to modify future plans; it is built on a frozen LLM (uses ReAct-style base) and produces per-trial reflections to guide the next attempt. In this paper Reflexion is used as a baseline: its reflections are largely task- and environment-specific (e.g., "In the next trial, I will go to desk 1 and find the lamp") and it does not maintain a long-term persistent memory across episodes in the manner CLIN does.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>gpt-4 (paper indicates generative agent baselines including Reflexion use gpt-4 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Short-term per-trial reflective notes / dynamic reflections (no long-term persistent abstract memory across episodes as used by CLIN)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Reflexion's reflections are generated from the most recent trial (reflecting on failures) and used to craft a new plan for the next trial; the reflections are not organized as generalized causal abstractions and are not persistently abstracted across multiple episodes in the way CLIN's meta-memory is.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified in this paper beyond being per-trial reflections; effectively limited to the current trial's reflection context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Reflections are generated from the immediate past trial; there is no long-term retrieval mechanism described in this paper for Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Generate reflection after trial failure/completion and use it to alter next trial's plan; no long-term memory update across episodes described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ScienceWorld (used as a baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same ScienceWorld characteristics as above (text-based, partial observability, complex action space).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper's baselines: average reward across tasks for Reflexion = 39.4 (Table 1 / Table 2 aggregate). Per-task values given in the tables (e.g., various task-specific percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct (base agent without Reflexion-style reflections) reported average = 29.6 (paper uses ReAct as base for Reflexion), showing Reflexion's per-trial reflection provides some gains over ReAct in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper contrasts Reflexion's per-trial reflection approach with CLIN's persistent causal-abstraction memory; CLIN's persistent memory yields substantially larger adaptation/generalization gains (CLIN ADAPT = 62.2 vs Reflexion = 39.4 average).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Reflexion-style per-trial reflection helps performance over a base ReAct agent, but lacks a persistent, generalized memory; persistent dynamic memory of causal abstractions (CLIN) produces much larger continual learning and cross-environment/task generalization gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2882.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2882.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven embodied agent (Minecraft) that builds a code-based skill library from experience to expand capabilities over time (skill library functions as a learned memory/skill store).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Voyager is cited as an example of an LLM-driven agent that grows a skill library (code-based) from rich feedback and failures in Minecraft; it incrementally accumulates skills (a form of memory/skill store) enabling open-ended behavior. The paper references Voyager in related work but does not use it in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Code-based skill library (non-textual skill memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not detailed in this paper beyond the high-level description that Voyager builds a skill library from experience and feedback; details are in the Voyager paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Skill library grown from feedback of failures and successes (high-level description only in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Minecraft (embodied environment) — not a text-game; mentioned for contrast to CLIN.</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Open-ended 3D environment (Minecraft), massive action/state space, skill-building through code generation; referenced only in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Voyager is provided as an example showing a learned skill/library approach to accumulate capabilities; the CLIN paper cites it to contrast non-parametric textual memory vs code-skill libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2882.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2882.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenerativeAgents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative agents that model social behavior and use a dynamic memory of experiences; cited as evidence that memory of experiential learnings can improve LLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>These generative agents maintain dynamic memories of interactions to simulate human-like behavior; the CLIN paper references this work to motivate using memories to improve frozen LLM agent behavior, but does not experiment with these agents directly.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Dynamic experiential memory (textual summaries of past interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Cited to support the idea that a memory of useful learnings can improve frozen LLM behavior; CLIN builds on the concept by structuring memory as causal abstractions for sequential decision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2882.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2882.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/architectural approach where a language model interleaves chain-of-thought style reasoning with actions; used as a baseline controller/executor behavior in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ReAct is a prompting paradigm where an LLM alternates reasoning steps and actions; in this paper ReAct serves as the base agent (controller/executor baseline) and as the base for Reflexion. It does not use a persistent memory of causal abstractions in the way CLIN does.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>gpt-4 (used as baseline LLM in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ScienceWorld (used as baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same as ScienceWorld characteristics above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Average reward for ReAct baseline (no long-term memory) reported as 29.6 (paper Table 2 aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Serves as a baseline demonstrating that adding persistent structured memory (CLIN) substantially improves performance vs. ReAct (CLIN ADAPT average 62.2 vs ReAct 29.6).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2882.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2882.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan: Do as I can, not as I say — Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that combines affordance grounding and language models to select actions for robots; cited as a generative-language baseline in experiments (no persistent CLIN-like memory).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SayCan ranks actions by combining affordance likelihoods with a language model's plan suggestions; used as a comparative baseline in the paper's generalization experiments, but it does not maintain the CLIN-style persistent causal memory.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>ScienceWorld (used as baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Same ScienceWorld characteristics as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>SayCan average reported as 32.9 (aggregate Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Included to show that generative language agents without CLIN-style persistent, structured memory perform worse on average than CLIN in ScienceWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
                <li>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2882",
    "paper_id": "paper-767a7e949ba4520888e7442ee01e6a37c254fc53",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [
        {
            "name_short": "CLIN",
            "name_full": "Continual Learning from Interactions (CLIN)",
            "brief_description": "A memory-augmented language-agent that continually learns causal abstractions (textual memories) from trial-and-error in a text-based simulator (ScienceWorld), using frozen LLMs for controller, executor and a memory-generator to update a persistent, dynamic NL memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CLIN",
            "agent_description": "CLIN is a three-module generative agent (controller, executor, memory) with a fourth module (memory-generator) used for learning. The controller is a frozen LLM prompted with the current task, retrieved memory items, and the trial history (sequence of goal-action-observation triples) to generate the next goal. The executor is a frozen LLM that converts a goal into a valid environment action given admissible action templates; generated candidate actions are validated against the simulator's admissible action set and, if needed, mapped via sentence-transformer embeddings (similarity threshold 0.9) or refined iteratively (up to 5 retries). The memory-generator (frozen LLM) is invoked at the end of each trial to reflect on the trial and generate/update a persistent textual memory of causal abstractions that the controller can retrieve and use in future trials. CLIN also supports meta-memory creation across episodes to generalize to unseen environments/tasks.",
            "base_llm": "gpt-4 (used for controller, executor, and memory-generator; model size not specified)",
            "uses_memory": true,
            "memory_type": "Persistent dynamic textual memory of causal abstractions (semantic/episodic hybrid) plus meta-memory (generalized abstractions across episodes)",
            "memory_architecture": "Memory is a persistent list of natural-language sentences expressing causal abstractions (templates like \"X SHOULD BE NECESSARY to Y\" / \"X MAY ...\" / \"X DOES NOT CONTRIBUTE to Y\"). At end of each trial the memory-generator (a frozen LLM) is prompted with the trial trace (goal-action-observation tuples plus final reward) and the three most recent memories to produce an updated memory. The memory-generator performs saliency-based pruning (using final trial reward) so only important insights are kept. Retrieval: the controller selects one or more memory items by matching the current task description and trial history, and appends selected learnings to the controller prompt when generating next goals. Meta-memory: for cross-environment/task generalization the system selects the most successful trial per episode (prioritized replay; archive size=10) and the memory-generator synthesizes a meta-memory (generalized causal abstractions) conditioned on target task/environment. Memory entries carry uncertainty markers (\"may\" vs \"should\") reflecting confidence.",
            "memory_capacity": "No fixed maximum size specified; observed generated memory size is far smaller than the number of actions executed per trial. Meta-memory archive size is given as 10 (hyperparameter). Memory-generator uses the three most recent per-trial memories when updating.",
            "memory_retrieval_method": "Retrieval is done by the controller using the task instruction and trial history to select memory items judged useful; selection is prompt-driven (no learned retrieval model). Action validity mapping (executor) uses sentence-transformer embeddings for matching generated action to admissible actions.",
            "memory_update_strategy": "Memories are produced/updated after each trial by the memory-generator LLM, which is given the trial trace, final reward (converted to NL feedback), and the three most recent memory versions. A saliency-based pruning step keeps high-utility insights (based on reward). Meta-memory is generated from the best trials across episodes (prioritized level replay selecting most successful trial per episode, archive size 10).",
            "text_game_benchmark": "ScienceWorld",
            "game_characteristics": "High-fidelity text-based simulator with natural-language actions and observations, partial observability, 10 sub-places (e.g., kitchen, greenhouse, workshop), many objects with states, large action space (admissible action templates + object instantiations), tasks spanning diverse science domains (thermodynamics, genetics, chemistry, friction, etc.), tasks categorized as Short (&lt;37 oracle steps) or Long (&gt;=37), evaluated across many environment configurations (164 task-environment combos in experiments).",
            "performance_with_memory": "Multiple reported metrics (final reward scores). Aggregated averages across 18 tasks/environments: BASE (initial / zero-shot) = 48.6 (avg reward), GEN-ENV (zero-shot with meta-memory) = 52.7, GEN-ADAPT (G+A; allow adaptation after transfer) = 69.5. Adaptation (ADAPT) average after retries: 62.2. Per-type: Short tasks ADAPT = 62.8, Long tasks ADAPT = 61.6. Per-task table entries available (see paper Table 1 / Table 2).",
            "performance_without_memory": "BASE (empty/initial memory) performance reported as 48.6 average reward (this is CLIN's zero-shot starting point). Ablations: replacing structured causal memory with free-form/unstructured memory caused average reward drops (paper reports an average drop of 6 points in 10% of cases compared to CLIN with structured memory). Ablating the controller (which mediates use of memory) caused an 18-point drop in average reward, producing behavior equivalent to ReAct baseline.",
            "has_ablation_study": true,
            "memory_ablation_results": "Key ablations: (1) Structured causal-abstraction memory vs free-form/unstructured memory — unstructured memory reduced average reward by ~6 points in 10% of cases, indicating structured causal templates help; (2) Ablating the controller (goal-generation step that leverages memory) reduced BASE performance by ~18 points and made agent equivalent to ReAct; (3) Meta-memory (compiled from best trials) improves zero-shot start and accelerates later adaptation (GEN-ENV start 52.7 vs BASE 48.6; continued gains to G+A 69.5).",
            "comparison_with_other_memory_types": "Paper compares structured causal-abstraction memory to free-form advice (unstructured memory) and finds structured memory outperforms unstructured (free-form) memory; no other memory types (e.g., vector DB, knowledge graph) are directly compared in experiments.",
            "key_findings_about_memory_effectiveness": "Persistently storing causal abstractions (\"X is/should be necessary to Y\" / \"X does not contribute to Y\") and refining them across trials enables frozen LLM agents to continually improve in text-based tasks; meta-memory (summaries of best memories across episodes) improves zero-shot generalization to new environments/tasks; retrieval quality and correct selection of memory items are critical and retrieval errors can limit performance; structured, task-relevant causal memory outperforms unstructured text advice.",
            "uuid": "e2882.0",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A reflective LLM-based agent that generates language-based insights by reflecting on a failed attempt and uses those reflections to propose improved plans for retries; used as a strong baseline in ScienceWorld experiments.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "Reflexion generates reflections (language insights) after failed attempts that are then used to modify future plans; it is built on a frozen LLM (uses ReAct-style base) and produces per-trial reflections to guide the next attempt. In this paper Reflexion is used as a baseline: its reflections are largely task- and environment-specific (e.g., \"In the next trial, I will go to desk 1 and find the lamp\") and it does not maintain a long-term persistent memory across episodes in the manner CLIN does.",
            "base_llm": "gpt-4 (paper indicates generative agent baselines including Reflexion use gpt-4 in experiments)",
            "uses_memory": true,
            "memory_type": "Short-term per-trial reflective notes / dynamic reflections (no long-term persistent abstract memory across episodes as used by CLIN)",
            "memory_architecture": "Reflexion's reflections are generated from the most recent trial (reflecting on failures) and used to craft a new plan for the next trial; the reflections are not organized as generalized causal abstractions and are not persistently abstracted across multiple episodes in the way CLIN's meta-memory is.",
            "memory_capacity": "Not specified in this paper beyond being per-trial reflections; effectively limited to the current trial's reflection context.",
            "memory_retrieval_method": "Reflections are generated from the immediate past trial; there is no long-term retrieval mechanism described in this paper for Reflexion.",
            "memory_update_strategy": "Generate reflection after trial failure/completion and use it to alter next trial's plan; no long-term memory update across episodes described in this paper.",
            "text_game_benchmark": "ScienceWorld (used as a baseline in experiments)",
            "game_characteristics": "Same ScienceWorld characteristics as above (text-based, partial observability, complex action space).",
            "performance_with_memory": "Reported in paper's baselines: average reward across tasks for Reflexion = 39.4 (Table 1 / Table 2 aggregate). Per-task values given in the tables (e.g., various task-specific percentages).",
            "performance_without_memory": "ReAct (base agent without Reflexion-style reflections) reported average = 29.6 (paper uses ReAct as base for Reflexion), showing Reflexion's per-trial reflection provides some gains over ReAct in these experiments.",
            "has_ablation_study": false,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": "Paper contrasts Reflexion's per-trial reflection approach with CLIN's persistent causal-abstraction memory; CLIN's persistent memory yields substantially larger adaptation/generalization gains (CLIN ADAPT = 62.2 vs Reflexion = 39.4 average).",
            "key_findings_about_memory_effectiveness": "Reflexion-style per-trial reflection helps performance over a base ReAct agent, but lacks a persistent, generalized memory; persistent dynamic memory of causal abstractions (CLIN) produces much larger continual learning and cross-environment/task generalization gains.",
            "uuid": "e2882.1",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An LLM-driven embodied agent (Minecraft) that builds a code-based skill library from experience to expand capabilities over time (skill library functions as a learned memory/skill store).",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "Voyager is cited as an example of an LLM-driven agent that grows a skill library (code-based) from rich feedback and failures in Minecraft; it incrementally accumulates skills (a form of memory/skill store) enabling open-ended behavior. The paper references Voyager in related work but does not use it in experiments.",
            "base_llm": null,
            "uses_memory": true,
            "memory_type": "Code-based skill library (non-textual skill memory)",
            "memory_architecture": "Not detailed in this paper beyond the high-level description that Voyager builds a skill library from experience and feedback; details are in the Voyager paper itself.",
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": "Skill library grown from feedback of failures and successes (high-level description only in this paper).",
            "text_game_benchmark": "Minecraft (embodied environment) — not a text-game; mentioned for contrast to CLIN.",
            "game_characteristics": "Open-ended 3D environment (Minecraft), massive action/state space, skill-building through code generation; referenced only in related work.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Voyager is provided as an example showing a learned skill/library approach to accumulate capabilities; the CLIN paper cites it to contrast non-parametric textual memory vs code-skill libraries.",
            "uuid": "e2882.2",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GenerativeAgents (Park et al.)",
            "name_full": "Generative agents: Interactive simulacra of human behavior",
            "brief_description": "Generative agents that model social behavior and use a dynamic memory of experiences; cited as evidence that memory of experiential learnings can improve LLM behavior.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al.)",
            "agent_description": "These generative agents maintain dynamic memories of interactions to simulate human-like behavior; the CLIN paper references this work to motivate using memories to improve frozen LLM agent behavior, but does not experiment with these agents directly.",
            "base_llm": null,
            "uses_memory": true,
            "memory_type": "Dynamic experiential memory (textual summaries of past interactions)",
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": null,
            "game_characteristics": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Cited to support the idea that a memory of useful learnings can improve frozen LLM behavior; CLIN builds on the concept by structuring memory as causal abstractions for sequential decision tasks.",
            "uuid": "e2882.3",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A prompting/architectural approach where a language model interleaves chain-of-thought style reasoning with actions; used as a baseline controller/executor behavior in the paper.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "ReAct is a prompting paradigm where an LLM alternates reasoning steps and actions; in this paper ReAct serves as the base agent (controller/executor baseline) and as the base for Reflexion. It does not use a persistent memory of causal abstractions in the way CLIN does.",
            "base_llm": "gpt-4 (used as baseline LLM in experiments)",
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "ScienceWorld (used as baseline in experiments)",
            "game_characteristics": "Same as ScienceWorld characteristics above.",
            "performance_with_memory": null,
            "performance_without_memory": "Average reward for ReAct baseline (no long-term memory) reported as 29.6 (paper Table 2 aggregate).",
            "has_ablation_study": false,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Serves as a baseline demonstrating that adding persistent structured memory (CLIN) substantially improves performance vs. ReAct (CLIN ADAPT average 62.2 vs ReAct 29.6).",
            "uuid": "e2882.4",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan: Do as I can, not as I say — Grounding language in robotic affordances",
            "brief_description": "An approach that combines affordance grounding and language models to select actions for robots; cited as a generative-language baseline in experiments (no persistent CLIN-like memory).",
            "citation_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "mention_or_use": "mention",
            "agent_name": "SayCan",
            "agent_description": "SayCan ranks actions by combining affordance likelihoods with a language model's plan suggestions; used as a comparative baseline in the paper's generalization experiments, but it does not maintain the CLIN-style persistent causal memory.",
            "base_llm": null,
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "ScienceWorld (used as baseline in experiments)",
            "game_characteristics": "Same ScienceWorld characteristics as above.",
            "performance_with_memory": null,
            "performance_without_memory": "SayCan average reported as 32.9 (aggregate Table 1).",
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "Included to show that generative language agents without CLIN-style persistent, structured memory perform worse on average than CLIN in ScienceWorld.",
            "uuid": "e2882.5",
            "source_info": {
                "paper_title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 1
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        },
        {
            "paper_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
            "rating": 1
        }
    ],
    "cost": 0.019678249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CLIN: A CONTINUALLY LEARNING LANGUAGE AGENT FOR RAPID TASK ADAPTATION AND GENERALIZATION</h1>
<p>Bodhisattwa Prasad Majumder ${ }^{1}$, Bhavana Dalvi Mishra ${ }^{1}$, Peter Jansen ${ }^{1,2}$, Oyvind Tafjord ${ }^{1}$, Niket Tandon ${ }^{1}$, Li Zhang ${ }^{3}$, Chris-Callison Burch ${ }^{3}$, Peter Clark ${ }^{1}$<br>${ }^{1}$ Allen Institute of AI<br>${ }^{2}$ University of Arizona<br>${ }^{3}$ University of Pennsylvania<br>Contact: {bodhisattwam, bhavanad}@allenai.org<br>Project page: https://allenai.github.io/clin/</p>
<h4>Abstract</h4>
<p>Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time, beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory, centered on causal abstractions (rather than general "helpful hints"), that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points ( 13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points ( 7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have been increasingly used to interact with external environments (e.g., simulated worlds) as goal-driven agents (Reed et al., 2022). However, it has been challenging for these language agents to efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning (Chen et al., 2021; Ammanabrolu et al., 2020). More recently, new techniques have appeared in which an agent reflects on its own past experience solving a task in a particular environment, and generates languagebased insights to help it retry the task, e.g., Reflexion (Shinn et al., 2023). Such methods have the advantage of not requiring parameter updates (particularly useful given the growing popularity of frozen language models). However, the style of such insights plays a crucial role in performance, and not all insights improve generalization performance. For example, a specific insight such as "In the next trial, I will go to desk 1 and find the lamp" (Shinn et al., 2023) may have limited value (or even hurt) for a different environment or task.</p>
<p>Our goal is a system that will continually improve over time, both while attempting the same task in the same environment, and across different tasks and environments. Our approach builds on prior work on reflection in two ways: First, we conjecture that a specific style of insight will be useful, namely one that captures causal abstractions about agent's actions, e.g., "opening doors</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: CLIN creates (Trial1) or adapts (Trial2+) a memory of causal abstractions to help in future trials by reflecting on the last trial and current memory. It does this using a suitably prompted LLM to generate the updated memory (Section 3.2). Here, reflecting on Trial1, CLIN notes in memory that going to the kitchen helped with finding seeds, enabling it to find the seeds faster in Trial2. From there, it also learns that moving the seeds to the pot helped plant the seeds. To further generalize across episodes (sequences of trials, right figure) for use in new environments, CLIN generates a summary ("meta-memory") of the best (starred) memories from each prior episode, here generating the generalization that moving to different rooms helps finding objects (Section 3.3)
may be necessary for movement between rooms". Causal abstractions can potentially help the agent decide which action to take in the future, and can be viewed as a kind of action model learning (Arora et al., 2018), but placed in the modern context of language models. Second, we maintain these abstractions in a continually evolving, dynamic memory, which is regularly updated as the agent gains experience, allowing useful causal knowledge to persist (and unhelpful knowledge to be dropped) over time and between tasks and environments, as illustrated in Figure 1.</p>
<p>We operationalize and evaluate this approach in a memory-augmented language agent called CLIN (continual learning from interactions). CLIN is an agent that operates in ScienceWorld (Wang et al., 2022), a virtual, text-based environment in which an agent is tasked with science-based goals, e.g., boiling a liquid, growing a plant. We find that CLIN is able to rapidly learn about the environment and its action vocabulary and continually improve on repeated trials on the same task and environment, outperforming state-of-the-art (SOTA) reflective language agents like Reflexion by 23 points. CLIN can also transfer its learning to new environments (or tasks), improving its zero-shot performance by 4 (13 for new tasks) points and can further improve performance through continual memory updates, enhancing performance by an additional 17 ( 7 for new tasks) points. Our contributions are as follows:</p>
<ul>
<li>For memory-based language agents, we show that memory of causal abstractions is effective at helping the agents learn over an extended period and in varying conditions.</li>
<li>We describe and evaluate CLIN, an architecture for a novel nonparametric learning paradigm. We show that CLIN learns faster than prior systems and generalizes better to new tasks and new environments, achieving state-of-the-art.</li>
</ul>
<p>Overall, this work suggests that a dynamically maintained memory, centered around causal knowledge, is a promising way forward for agents built on frozen models to continually improve over time.</p>
<h1>2 Related Work</h1>
<p>There is a long literature of work on agents that can navigate complex environments. A common approach is to use reinforcement learning (RL), e.g., DRRN (He et al., 2015), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), CALM (Yao et al., 2020), where agents learn a task over repeated trials. However, while effective, such agents typically require a large number of trials to learn and have trouble adapting to unexpected changes in the test environment. More recently, (Adaptive-AgentTeam et al., 2023) demonstrated AdA, an agent that could rapidly adapt to open-ended novel 3D problems, using meta-reinforcement learning, essentially being able to change its policy on the fly.</p>
<p>However, AdA required vast amounts of pretraining, and this skill was still limited to the style of environments and problems seen in pretraining.</p>
<p>Recently, LLMs have provided a new tool for building goal-directed agents (Huang et al., 2022). Given a linguistic description of the world state, a task, and a history, the LLM can be prompted to suggest next actions to take to achieve a goal, exploiting their wealth of semantic knowledge about the world and requiring little training, e.g., SayCan (Ahn et al., 2022), ReAct (Yao et al., 2022), and more recently SwiftSage (Lin et al., 2023), which combines a supervised agent and a deliberative agent together. However, while performing reasonably with little training data, such agents are unable to learn and adapt from experience.</p>
<p>Two recent systems have demonstrated how a frozen-model-based agent could improve at a task. Voyager (Wang et al., 2023) operates in the world of Minecraft, growing a (code-based) skill library from rich feedback of its failures. Reflexion (Shinn et al., 2023) improves at a task by reflecting on a failed attempt at that task and devise a new plan that accounted for that mistake, used in the subsequent prompt to retry the task. While Reflexion did not have a long-term memory, and its reflections were task- and environment-specific, e.g., "In the next trial, I will go to desk 1 and find the lamp.", we take inspiration from it to build an agent, CLIN, which continually maintains and adapts a long-term, persistent memory of reflections, useful across different trials, tasks, and environments.</p>
<p>More generally, others have found that a memory of useful learnings can be used to improve frozen LLM behavior, e.g., in QA (Dalvi et al., 2022; Tandon et al., 2022; Madaan et al., 2023), or for modeling social behavior (Park et al., 2023). We apply this finding to goal-directed agents.</p>
<p>Finally, we note that the content of experiential memory is also important. Specifically, CLIN learns a memory of causal abstractions, which can be seen as learning a linguistic form of action model, describing the causal effects of actions. While there has been substantial work in the planning community of learning action models in a fully formalized context (Arora et al., 2018; Aineto et al., 2018), CLIN loosely applies this idea in the linguistic world of LLM agents.</p>
<h1>3 APPROACH</h1>
<p>Problem Formulation. Sequential decision-making tasks require agents to repeatedly observe and then act in an environment until they accomplish a specific goal. At a high level, this can be accomplished by developing beliefs about the world, acting on the environment based on those beliefs, and then updating one's beliefs based on the observed outcome. Here, we investigate constructing an agent that can continually update its beliefs through interaction and observation while exploiting its past experience toward solving unseen parametric variations of tasks.</p>
<p>Setup. We investigate our continual learning agents in simulated environments. Our environments are modeled in a high-fidelity text-based simulator (Wang et al., 2022), where both actions and observations are expressed in natural language. Let's define the task space to be $\mathcal{M}$, a collection of partially observable Markov Decision Processes (POMDPs) that can be executed in a set of environment configurations $\mathcal{E}$. Each task $m \in \mathcal{M}$ has an initial state and a desired winning state, which vary depending on the environment $e \in \mathcal{E}$.</p>
<p>Our setup allows an agent to attempt a task several times; each time is denoted as a trial, $\mathcal{T}$, which consists of a total of $\tau$ steps. Each step comprises an action by the agent (a), and in response, the simulator returns the result of that action in the form of an observation (o) and a reward ( $r$ ). A collection of $K$ trials is called an episode. The environment gets reset when the task reaches an end state (such as completing, failing, or timing out). In our continual learning setup, the agent retains its memory across trials/episodes, reaping the benefits of continued interaction with the environment.</p>
<h3>3.1 CLIN: A GENERATIVE AGENT CONTINUALLY LEARNING FROM INTERACTIONS</h3>
<p>To act in the world, CLIN uses three modules: a memory, a controller, and an executor, illustrated in Figure 2 and which we now describe. Learning then occurs using a fourth module, a memory generator, to generate an updated memory after each trial.</p>
<p>Memory. CLIN's memory $(\mathcal{S})$ is a persistent, dynamic collection of NL sentences that express causal abstractions, generated by reflecting on past experiences in order to help the agent perform</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The architecture of CLIN. A controller takes the current task, retrievals from memory, and the trial so far, to generate the next goal to achieve. The executor then converts this to a valid action to perform towards that goal. The simulator then performs the action and returns an observation of that action's effect. Memory is updated at the end of each trial by the memory generator (Section 3.2).
better in the future. This generation process is described shortly in Section 3.2. For example, having an explicit causal insight, "opening the fridge is necessary to access apple juice", learned from past experiences, can reduce the action search space for CLIN while looking for "apple juice" in the same environment in future trials. To aid in continual learning, the memory also captures mistakes made by the agent in previous trials, similar to reflective agents explored in recent work (Shinn et al., 2023; Park et al., 2023), noting actions that failed to contribute to a task.</p>
<p>Controller. The role of the controller is to generate the next goal to pursue in service of the task. In CLIN it is a frozen LLM, whose prompt includes the current task $m$, e.g., "convert water into steam", retrievals from the current memory $\mathcal{S}$, and the trial so far (the sequence of goal-action-observation triples $\left{g_{1}, a_{1}, o_{1}, \ldots, g_{t}, a_{t}, o_{t}\right}$ ), and is prompted to output the next goal $g_{t+1}$ to pursue, e.g., "find water". Memory items are retrieved using both the task instruction and the trial history. The controller first selects one or more memory items given the current state and if they are useful for the next action to progress in the task. After that, it appends the learning, if selected, in context to generate a goal, otherwise the goal is generated based on the trial history (see full prompt in Figure 6).
Executor. The role of the executor is to convert the generated goal $g_{t+1}$ into a valid action $a_{t+1}$ that can be executed in the environment in pursuit of that goal. Again a (frozen) LLM is used, whose prompt includes the goal $g_{t+1}$, the trial so far, and all the possible actions that can be performed in the current state (provided by the simulator, as is standard practice in current generative agent research (Ahn et al., 2022; Yao et al., 2022; Lin et al., 2023; Park et al., 2023)). The list of possible actions is expressed as possible action templates and available objects that can instantiate them, rather than a combinatorially large enumeration of possible actions. The model is then prompted to generate a candidate action to perform (see prompt in Figure 6). Finally, CLIN checks this candidate action is one of the valid actions. If it is not, it finds the most similar valid action using the pre-trained embeddings from the sentence-transformer model (Reimers \&amp; Gurevych, 2019). If the top-ranked valid action has a similarity score greater than a threshold (here, 0.9 , chosen as a hyperparameter), the action is selected. Otherwise, we perform iterative refinement (Madaan et al., 2023) by suffixing the context with feedback that the generated candidate action is not executable. This allows the executor to retry the generation for up to a maximum number of tries (here, 5).
Finally, upon executing the action $a_{t+1}$, CLIN receives a partial next state, as an observation, from the simulator and the reward $(r) \in[0,1]$. Rewards are nominally given by the simulator for achieving either major task subgoals (e.g., finding water, for the boiling task), or minor and optional subgoals (e.g., being in the kitchen, for the boiling task). Rewards are sparse and generally only supplied after the completion of a task subgoal. A snapshot of a full trial is given in lines 4-10 in Algorithm 1.
Note that CLIN does not make use of any gold data to identify goals and memories. Rather, we expect CLIN to perform a balanced act of exploration-exploitation by interacting, learning, and adapting to unseen tasks or environment configurations-a key difference from few-shot generative agents by previous work (Ahn et al., 2022; Yao et al., 2022; Lin et al., 2023; Park et al., 2023).</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Continual Learning with CLIN
    procedure ADAPTATION(Task: \(m\), Env: \(e\), Mem-
        ory: \(\mathcal{S}\) ):
        Initialize Memory: \(\mathcal{S}_{0}\)
        for \(k \in 1, \cdots, K\) do:
            Intialize Trial \(\mathcal{T}, t\)
            while \(t&lt;\) max. steps or task not complete do:
                \(g_{t}=\operatorname{Controller}\left(m, e, \mathcal{T}_{&lt;t}, \mathcal{S}_{k-1}\right)\)
                \(a_{t}=\) Executor ( \(g_{t}\), admissible actions)
                \(r_{t}, o_{t}=\operatorname{Simulator}\left(\mathcal{T}_{&lt;t}, a_{t}\right)\)
                \(\mathcal{T}_{&lt;t+1}=\mathcal{T}_{&lt;t}+\left(g_{t}, a_{t}, o_{t}, r_{t}\right)\)
            Final reward \(r_{k}=r_{t}\)
            \(\mathcal{S}_{k}=\) memory-generator \(\left(\left\{\mathcal{S}_{&lt;k}\right\}, \mathcal{T}_{k}, r_{k}\right)\)
        procedure Generalization(Task: \(m\), Env: \(e\),
            past \(\left.m^{\prime} / e^{\prime}\right)\)
            \(\left\{\mathcal{S}_{\text {crucial }}, r_{k}\right\}=\) crucial-memories (past \(\left.m^{\prime} / e^{\prime}\right)\)
            \(\mathcal{S}_{\text {meta }}=\operatorname{meta-memory}\left(\left\{\mathcal{S}_{\text {crucial }}, r_{k}\right\}, m\right)\)
            ADAPTATION \(\left(m, e, \mathcal{S}_{\text {meta }}\right)\)
</code></pre></div>

<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (LEFT) CLIN's continual learning algorithm. (RIGHT) Example causal abstractions.</p>
<h1>3.2 MEMORY: A COLLECTION OF ADAPTIVE CAUSAL ABSTRACTIONS</h1>
<p>At the end of each trial (completion or failure), CLIN uses a memory generator to create or update its memory. The memory generator is a (frozen) LLM prompted to reflect on the current trial and memory, and generate a new memory of insights in the form of (English sentences expressing) useful causal abstractions, as we now describe.
Learning about state transitions is essential for sequential decision-making tasks (Mnih et al., 2013), which can be manifested by knowing 1) actions enabling desired state transitions, 2) actions producing undesired or no change in states, and 3) state transitions that contribute to the task progress. To generate these kinds of knowledge, the generator is prompted to generate insights in a particular syntax (see prompt in Figure 7). To capture good actions enabling desired changes and helpful state transitions, we use the template " X is NECESSARY to Y ", and to capture contrastive examples of unsuitable actions and state transitions, we employ "X DOES NOT CONTRIBUTE to Y", as depicted in Section 4.3, where X, Y are related to actions. These abstractions are functionally analogous to hindsight experience replay (Andrychowicz et al., 2017), obtained from CLIN's past self-explorations.
While useful insights can be abstracted from the trials, CLIN's exploration can be limited, especially in the early trials, given an incredibly large action space. Hence, incomplete exploration can pose varying degrees of uncertainty on extracted insights. To capture this, we also include a measure of uncertainty in each abstraction by either of the two linguistic variations in their surface forms: " X may . . " to denote moderate to high uncertainty, and " $X$ should . . ." to indicate low uncertainty (See Section 4.3). In the course of continual learning, as CLIN gathers experience, we expect the level of uncertainty to change depending on the frequency of their use and their fitness to the task.</p>
<p>Updating Memory Across Trials. CLIN continually attempts to solve a task in an environment for multiple trials (in sum, an episode). To update the memory after each trial within an episode, the memory generator is prompted with the most recent trial (a sequence of $\left(g_{t}, a_{t}, o_{t}\right)$ tuples and the final reward $\left.r_{k}{ }^{1}\right)$, and the memories from the three most recent trials $\left{\mathcal{S}<em k-1="k-1">{k-2}, \mathcal{S}</em>}, \mathcal{S<em k_1="k+1">{k}\right}$. It is then prompted to generate an updated memory $\mathcal{S}</em>$, namely a new list of semi-structured causal abstractions in the forms described above, for use in the next trial. Although we do not specify a maximum size for the memory, we observe that size of the generated memory (i.e., the number of causal abstractions generated) is far less than the number of actions executed in the trial, indicating</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the memory-generator additionally performs a saliency-based pruning to keep only important insights based on the success of the trial, as indicated by the final reward $r_{k}$ at the end of the trial $\mathcal{T}_{k}$.</p>
<h1>3.3 Meta-Memory for Better Generalization</h1>
<p>Updating memory based on past insights and the current trial to influence future trials for the same task in the current environment configuration during test-time adaptation. However, to generalize across tasks or environment configurations, the memory needs to contain more generalized causal abstractions than memories used across trials in an episode. We call this as meta-memory, abstracted across multiple episodes of solving different tasks in different environment configurations to be applicable in future episodes.</p>
<p>Auto-curriculum Selection. Before we generate the meta-memory, it is important to choose memories extracted from the best trials from previous episodes because random sampling may not benefit CLIN in zero-shot generalization (Adaptive-Agent-Team et al., 2023). Following the prioritized level replay scheme (Jiang et al., 2021), we choose the most successful trial per episode and retrieve memories abstracted from those trials with a fixed archive of size 10, a hyperparameter.</p>
<p>Generating Meta-Memory. The goal of the meta-memory is to help CLIN generalize to unseen tasks and/or environments. While we keep the format of the causal abstractions the same as memories generated across trials, the prompt to generate the meta-memory is different than those used for generating per-trial memory. When the new memory is to be used for the same task but in a different environment, the prompt instructs for a meta-memory helpful "to solve the same task in a new environment configuration" given the target task description with an expectation that metamemory abstractions will entail generic causal insights about the task irrespective of environment configurations (see Figure 8). Similarly, when the new memory is to be used for a different task, the prompt is modified accordingly to reflect this (Figure 9). Along with the target task description for better memory generation, each past memory selected to generate the meta-memory is attached to the final rewards for the associated trials, allowing the generator to combine insights across episodes and assign the levels of uncertainty using the evidence of success.</p>
<h2>4 Results and Analysis</h2>
<p>Experimental Setup. Test-time adaptation and generalization via continual learning require a variety of complex tasks and environment configurations to allow an agent to explore, learn latent causal insights from interactions, and exploit them in the future. We choose ScienceWorld (Wang et al., 2022), a text-based interactive environment requiring complex interactive reasoning processes to solve a plethora of science-theory-based tasks spanning several diverse classes (e.g., thermodynamics, genetics, friction, etc.). The virtual space presents 10 sub-places: foundry, greenhouse, outside area, an art studio, workshop, kitchen, living room, bedroom, bathroom, and a hallway connecting inside rooms. The presence of several objects, their individual states, and action templates renders the search space intractable for any agent. ScienceWorld presents strikingly different environment configurations across task types, making it a rich testbed for evaluating adaptation and generalization. ScienceWorld tasks are partitioned into Short (S), e.g., pick \&amp; place and Long (L), e.g., grow plant, tasks based on the number of required actions to succeed.</p>
<p>Here, we define our setups for zero-shot adaptation (ADAPT) and generalization (GEN-ENV and GENTASK). For all setups, we test CLIN and competing baselines on 18 tasks (two task instances from 9 classes) in several environment configurations from the test split of the ScienceWorld benchmark resulting in a total of 164 task-environment combinations unless stated otherwise. We evaluate based on the final reward score provided by the ScienceWorld simulator.</p>
<p>ADAPT: This setup focuses on CLIN's ability to adapt to a task by attempting it for several trials in the same environment configuration. Most importantly, CLIN initializes with an empty memory at the beginning of the first trial and generates memory at the end of each trial. While the environment gets reset at the trial boundary, CLIN's memory continues to be updated, capturing informative causal abstractions pertaining to both successful and failed actions. Here, we compare with Reflexion (Shinn et al., 2023), a SOTA, however, CLIN differs from Reflexion by how the memory is abstracted.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Rapid task adaptation with CLIN. (a) Example tasks where CLIN improves scores across trials. For CLIN, Trial-0 is the BASE, Trial-4 is the ADAPT. (b) Comparison of CLIN with Reflexion (Shinn et al., 2023). (c) CLIN improves from BASE to ADAPT (full results in Appendix C).</p>
<p>GEN-ENV: In this setup, we focus on CLIN's ability to transfer its learning from past experiences to solve tasks in an unseen environment. For a task $m$, we run CLIN for 10 different (train) environment settings (with varying objects and starting locations) and then create meta-memories from its exploration to solve the same task in an unseen (test) environment. Here, we compare CLIN with RL methods DRRN (He et al., 2015), KG-A2C (Ammanabrolu \&amp; Hausknecht, 2020), and CALM (Yao et al., 2020) trained on all (large) training variations with simulator reward and Generative Language agents, SayCan (Ahn et al., 2022), ReAct (Yao et al., 2022), and Reflexion (Shinn et al., 2023), prompted with few-shot demonstrations.</p>
<p>GEN-TASK: In this setup, we focus on CLIN's ability to transfer its learning from past experiences to solve a new task in the same environment. For an environment $e$, we run CLIN for to solve a task $m$ and then condense its learning to solve a novel task $m^{\prime}$ in the environment $e$. We took all test examples where we have a different task defined in the same environment configuration. (Adaptive-Agent-Team et al., 2023) suggests that transferring learning from a random task can be very hard; hence we couple tasks that are related (revolve around overlapping task-critical objects/locations such water, kitchen), such as boil and freeze to measure transfer learning from one to the other. This is a novel setup where we do not have any off-the-shelf baselines. However, here, we compare against CLIN-BASE, a strong baseline agent.</p>
<p>GEN-ADAPT (G+A): If CLIN, in GEN-ENV or GEN-TASK setting, does not successfully complete the new task, it can continue learning and retrying that task. We refer to this setup as GEN-ADAPT. CLIN can use any instruction-tuned LLM (Chung et al., 2022) as part of the controller, executor, and memory generator. In this paper, we use gpt-4, the same as our generative agent baselines.</p>
<h1>4.1 CLIN EXHIBITS RAPID TASK ADAPTATION</h1>
<p>Figure 4a demonstrates two example trends where CLIN learns from its own prior attempts (ADAPT) and gets better at solving a given task. Apart from length, the difficulty level of a task also depends on the environment configuration (hence, variance across environment configurations for each task). CLIN quickly adapts to a short task, Pick \&amp; Place, solving it in its 4th attempt, whereas for a longer task, Grow Fruit, it is not solved after 5th (max) attempts. Furthermore, Figure 4a depicts, CLIN becomes more efficient in later trials by solving the tasks with a lower number of (average) steps. Figure 4c shows an average number of attempts ${ }^{2}$ for CLIN to solve a task and $\%$ episodes per task where scores improved compared to its own first trial.</p>
<p>Next, we compare CLIN with Reflexion, the reflective SOTA agent, in Figure 4b. CLIN already starts off with a stronger base performance (see discussion in 4.3), however, CLIN's relative improvement in ADAPT is significantly stronger than Reflexion's gain from its base agent ReAct. Furthermore, CLIN's relative improvement is higher for longer tasks. This can be attributed to CLIN's persistent memory, which gets refined over past trials, whereas Reflexion may fall short of collecting useful learnings from earlier trials as it only focuses on the current trial for its reflections (hence not long-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RL Methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generative Language Agents</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CLIN (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">KGA2C</td>
<td style="text-align: center;">CALM</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">BASE</td>
<td style="text-align: center;">GEN-ENV</td>
<td style="text-align: center;">G+A</td>
</tr>
<tr>
<td style="text-align: center;">Temp $_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">Temp $_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">58.2</td>
</tr>
<tr>
<td style="text-align: center;">Pick\&amp;Place ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Pick\&amp;Place ${ }_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">93.3</td>
</tr>
<tr>
<td style="text-align: center;">Lifespan ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Lifespan ${ }_{2}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">Biology ${ }_{1}$</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: center;">Boil</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: center;">Freeze</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">GrowPlant</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;">GrowFruit</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">94.5</td>
</tr>
<tr>
<td style="text-align: center;">Biology ${ }_{2}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: center;">Force</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Friction</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">94.0</td>
</tr>
<tr>
<td style="text-align: center;">Genetics ${ }_{1}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Genetics ${ }_{2}$</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">71.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">69.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparing CLIN with baselines for generalization across unseen environments
term). Furthermore, CLIN accumulates both useful (for the task) and harmful (for the task) causal learnings, whereas Reflexion only learns from its mistakes, lacking comprehensive learning.</p>
<h1>4.2 CLIN OUTPERFORMS SOTA, GENERALIZING TO NOVEL ENVIRONMENTS AND TASKS</h1>
<p>Generalizing to new environments. Table 1 compares CLIN with baselines that learn from training environmental variants for a task to improve its performance in a novel environment ${ }^{3}$. Language agents (including CLIN) that use NL feedback from the ScienceWorld (e.g., "Door to the kitchen is closed") perform significantly better compared to RL methods that purely rely on (sparse) numeric rewards from the environment to learn a policy. We observe a positive generalization effect in GEN-ENV (average 4 point gain) compared to BASE where CLIN tries to solve the tasks zero-shot. With a strong BASE performance, CLIN beats all baselines in generalization performance. Furthermore, in G+A, CLIN shows a substantial 16 additional improvement, beating the SOTA reflective agent by 23 points. Figure 5a additionally shows trend of improvement compared to when CLIN does not start with a meta-memory. Meta-memory helps CLIN with a stronger start than BASE ( 52.7 vs. 48.6), with a continued gain in scores till the end of Trial-4 (G+A: 69.5 vs. ADAPT: 62.2). The stronger start for CLIN with meta-memory also results in fewer steps to solve a task. Unlike imitation learning-based agents, TDT (Wang et al., 2022) and SwiftSage (Lin et al., 2023), CLIN (and most baselines) does not use any gold trajectories. Refining its memory only from self-generated trajectories, CLIN outperforms TDT on all 18 tasks and SwiftSage on 8/18 (mostly long) tasks.</p>
<p>Generalizing to new tasks. Mirroring trends from GEN-ENV, CLIN demonstrates strong transfer learning to new tasks with 13-point improvement over its BASE performance, being better at $38.8 \%$ of times (Figure 5c). The improvement attributes to critical learning about the environment ("apple juice is in the fridge", required for both boiling and freezing it), leading to improvement in previously low-performing tasks in both ADAPT and GEN-ENV setups. This transfer learning in GEN-TASK and $\mathrm{G}+\mathrm{A}$ helps CLIN to solve the tasks with a lesser number of steps ${ }^{4}$ and achieve higher rewards.</p>
<h3>4.3 DISCUSSION</h3>
<p>Importance of memory structure. CLIN extracts causal abstractions structured around 'necessary' and 'does not contribute' relations. As an ablation study, we modified our memory generator to generate free-form advice for future trials (without any constraint on their formats). We find that the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reward and #steps trends for CLIN in (a) GEN-ENV and (b) GEN-TASK. (c) \% episode improvements and score change than CLIN without meta-memory (GEN-TASK). (d) CLIN ablations.
average reward drops by 6 points (in $10 \%$ cases compared to CLIN) when using the unstructured memory, indicating the usefulness of causal abstractions, as shown in Figure 5d.</p>
<p>Superior BASE performance. Figure 4 depicts a superior BASE performance for CLIN than the final performance of both ReAct and Reflexion despite using the same underlying LLM (here, gpt-4). We find if we ablate for the controller module in CLIN, responsible for generating a goal before outputting the next action, CLIN's BASE performance drops in $44 \%$ cases. With an 18 point drop in average reward (see Figure 5d), Abl-Contoller-BASE version of CLIN becomes equivalent to ReAct, the base agent for Reflexion, demonstrating the importance of controller even in BASE setup.</p>
<p>A qualitative example. Figure 3 depicts how memory items get refined during task adaptation and for generalization for a task boil. Env2 has a working stove, whereas in Env1, the stove is broken, but a lighter is available as an alternative. With a number of trials in these environments, CLIN learns how to use these two devices to generate heat. In an unseen environment with a broken stove, CLIN quickly receives a positive reward by using a lighter to heat a substance. While insights within an episode are often specific, e.g., "Using the lighter on the metal pot should be necessary to heat the water in the pot", CLIN compiles these insights for a new target environment (as meta-memory), e.g., "Using a heat source (stove, lighter) on the container should be necessary to heat a substance." Appendix B contains examples of memories generated during adaptation and generalization.</p>
<p>Limitation: Lack of exploration. CLIN's learnings are dependent on its own past experience. If CLIN never explores a location in the environment or does not perform an action, an insight related to the unobserved activity can never be generated. Hence, exploration becomes important when task-critical location or action in unknown to CLIN from past trials. For example, in task of creating an orange paint, the agent is supposed to find red and yellow paints from the art studio. However, art studio is not visible when CLIN starts from location 'outside'. Unless the CLIN knows that there exists an art studio, it tries alternative method to create orange paints from other irrelevant objects (e.g., an orange) and remains unsuccessful. When a memory related art-studio appears from past exploration, CLIN is able to successfully complete the task. Similarly, in boil or freeze tasks, CLIN is unable to perform well which requires it to consistently measure the temperature of the substance to know its boiling/freezing point-an act it could never perform successfully in past trials resulting into less useful memory insights and subsequent lower performance in future trials.</p>
<p>Limitation: Poor memory retrieval. For a task of boiling gallium, CLIN is supposed to use oven/blast furnace and not a stove. In the meta-memory for boiling tasks, there are two insights regarding the act of boiling: "Activating stove should be necessary to boil a substance" and "Using an alternative heat source (e.g., oven or fire pit) may be necessary if the initial heat source is insufficient." However, CLIN repeatedly retrieves the former and hence failing at the task despite performing other actions (e.g., finding gallium) correctly. This problem intensifies at the initial trial during</p>
<p>generalization due to the presence of insights with varied initial conditions for them to be applied. This can be circumvented by improved memory representation, which we leave as a future work.</p>
<h1>5 CONCLUSION</h1>
<p>Our goal is a system that can continually improve over time, both while rapidly adapting to a task by multiple retries and efficiently generalizing to novel tasks and environments. We propose CLIN, an architecture for language agents that constructs a persistent, dynamic memory of causal abstractions, refines it over time and uses it effectively to improve its performance on future tasks, achieving state-of-the-art performance. Our work systematically evaluates a novel nonparametric learning paradigm, promising never-ending learning abilities to frozen language agents.</p>
<p>Acknowledgement We sincerely thank Aristo team members Tushar Khot, Ashish Sabharwal, Shashank Gupta, Nathaniel Weir, Kyle Richardson, Jiangjie Chen, Archiki Prasad, and other members such as Faeze Brahman, Alexander Koller at the Allen Institute of AI for their generous feedback.</p>
<h2>REFERENCES</h2>
<p>Adaptive-Agent-Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreyaan Pathak, Nicolas Perez Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, and Lei M. Zhang. Human-timescale adaptation in an open-ended task space. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:255998274.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, KuangHuei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022. URL https://api.semanticscholar.org/CorpusID:247939706.</p>
<p>Diego Aineto, Sergio Jiménez, and Eva Onaindía. Learning strips action models with classical planning. In International Conference on Automated Planning and Scheduling, 2018. URL https://api.semanticscholar.org/CorpusID:49405691.</p>
<p>Prithviraj Ammanabrolu and Matthew J. Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In ICLR, 2020.</p>
<p>Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktaschel, and Jason Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In North American Chapter of the Association for Computational Linguistics, 2020. URL https://api.semanticscholar.org/CorpusID:222125301.</p>
<p>Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. ArXiv, abs/1707.01495, 2017. URL https://api.semanticscholar.org/CorpusID:3532908.</p>
<p>Ankuj Arora, Humbert Fiorino, Damien Pellier, Marc Métivier, and Sylvie Pesty. A review of learning planning action models. The Knowledge Engineering Review, 33, 2018. URL https: //api.semanticscholar.org/CorpusID:56483203.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar. org/CorpusID:235294299.</p>
<p>Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkongu Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022. URL https://api.semanticscholar.org/CorpusID:253018554.</p>
<p>Bhavana Dalvi, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. In EMNLP, 2022.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. arXiv: Artificial Intelligence, 2015. URL https://api.semanticscholar.org/CorpusID:15986631.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022.</p>
<p>Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Nicolaus Foerster, Edward Grefenstette, and Tim Rocktaschel. Replay-guided adversarial environment design. In Neural Information Processing Systems, 2021. URL https://api.semanticscholar.org/CorpusID:238408352.</p>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. ArXiv, abs/2305.17390, 2023. URL https: //api.semanticscholar.org/CorpusID:258960143.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with selffeedback. ArXiv, abs/2303.17651, 2023. URL https://api.semanticscholar.org/CorpusID: 257900871 .</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv, abs/1312.5602, 2013. URL https://api.semanticscholar.org/CorpusID:15238391.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Trans. Mach. Learn. Res., 2022, 2022. URL https://api.semanticscholar.org/CorpusID:248722148.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908. 10084 .</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve GPT-3 after deployment. In ACL Workshop on Commonsense Representation and Reasoning (CSRR'22), 2022. (also arxiv:2201.06009).</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv, abs/2305.16291, 2023. URL https://api.semanticscholar.org/CorpusID: 258887849 .</p>
<p>Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID:247451124.</p>
<p>Shunyu Yao, Rohan Rao, Matthew J. Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language models for action generation in text-based games. ArXiv, abs/2010.02903, 2020. URL https://api.semanticscholar.org/CorpusID:222142129.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395.</p>
<h1>A CLIN PROMPTS</h1>
<p>Figures 6 to 9 are the complete prompts for next-action generation (controller + executor), memorygenerator during ADAPT, GEN-ENV, and GEN-TASK.</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">System</span><span class="o">]</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="n">helping</span><span class="w"> </span><span class="k">execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">science</span><span class="w"> </span><span class="n">experiment</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">simulated</span>
<span class="n">environment</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">limited</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">step</span><span class="p">.</span>
<span class="o">[</span><span class="n">User</span><span class="o">]</span><span class="err">:</span>
<span class="n">Possible</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">OBJ</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="p">)</span><span class="err">:</span>
<span class="err">{</span><span class="n">objects_str</span><span class="err">}</span>
<span class="n">Your</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="nl">formats</span><span class="p">:</span>
<span class="n">Possible</span><span class="w"> </span><span class="nl">actions</span><span class="p">:</span>
<span class="err">{</span><span class="n">actions_str</span><span class="err">}</span>
<span class="k">If</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">say</span><span class="w"> </span><span class="err">\</span><span class="ss">&quot;Ambiguous request\&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">things</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">case</span><span class="p">,</span>
<span class="n">respond</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">corresponding</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="p">.</span>
<span class="n">What</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="k">next</span><span class="vm">?</span>
<span class="k">First</span><span class="p">,</span><span class="w"> </span><span class="n">scan</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="p">(</span><span class="n">unordered</span><span class="p">)</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">learnings</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">provided</span><span class="p">.</span><span class="w"> </span><span class="n">Decide</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span>
<span class="n">learnings</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="n">applicable</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">last</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">make</span><span class="w"> </span><span class="n">progress</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">task</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span>
<span class="k">only</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">selected</span><span class="w"> </span><span class="n">learnings</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">,</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">rationale</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">picking</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span>
<span class="k">action</span><span class="p">.</span><span class="w"> </span><span class="k">If</span><span class="w"> </span><span class="k">no</span><span class="w"> </span><span class="n">Learning</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">selected</span><span class="p">,</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rationale</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">last</span>
<span class="n">observation</span><span class="p">.</span><span class="w"> </span><span class="nf">Format</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nl">follows</span><span class="p">:</span>
<span class="k">Write</span><span class="w"> </span><span class="s1">&#39;I used learning id(s):&#39;</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">comma</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">list</span><span class="p">;</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="k">no</span>
<span class="n">learnings</span><span class="w"> </span><span class="n">selected</span><span class="p">.</span><span class="w"> </span><span class="k">Then</span><span class="p">,</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="err">$$$</span><span class="w"> </span><span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rationale</span><span class="p">.</span><span class="w"> </span><span class="n">Finally</span><span class="p">,</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="err">###</span>
<span class="n">followed</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">think</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">completed</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">TASK_COMPLETE</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">requires</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="s1">&#39;focus&#39;</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">something</span><span class="w"> </span><span class="p">(</span><span class="n">OBJ</span><span class="p">),</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">FOCUS</span><span class="w"> </span><span class="k">ON</span><span class="w"> </span><span class="o">&lt;</span><span class="n">OBJ</span><span class="o">&gt;</span><span class="w"> </span><span class="k">as</span>
<span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span><span class="w"> </span><span class="n">FOCUS</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">extremely</span><span class="w"> </span><span class="n">critical</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">used</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span>
<span class="k">of</span><span class="w"> </span><span class="n">times</span><span class="w"> </span><span class="s1">&#39;focus&#39;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">mentioned</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">description</span><span class="p">.</span><span class="w"> </span><span class="k">Using</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="ow">or</span>
<span class="n">inappropriately</span><span class="w"> </span><span class="p">(</span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">wrong</span><span class="w"> </span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="k">terminate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">session</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">will</span>
<span class="n">be</span><span class="w"> </span><span class="n">rendered</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">incomplete</span><span class="p">.</span>
<span class="k">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">performed</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">action</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">requires</span><span class="w"> </span><span class="n">waiting</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">effect</span><span class="p">,</span><span class="w"> </span><span class="n">please</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="s1">&#39;wait&#39;</span>
<span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="k">action</span><span class="p">.</span>
</code></pre></div>

<p>Figure 6: Prompt for the Controller and the Executor</p>
<h2>B EXAMPLE MEMORIES</h2>
<p>Example generated memory for ADAPT, GEN-ENV, and GEN-TASKsetups in Figures 10 to 12.</p>
<p>[System]: You are an expert assistant.
[User]:
You are given CURRENT TRACE, a sequence of actions that an agent made in a world to accomplish a task.</p>
<p>Task is detailed at the beginning.
For each action, there is a rationale why the agent made that action.
There is an observation that provide details about the new state of the world after each action was executed.
The CURRENT TRACE is accompanied by an EVALUATION REPORT indicating the success of the attempt to the task.</p>
<p>You can also be provided with PREVIOUS LEARNINGS which are learnings from the previous attempts by the agent for the same task in the same environment/world. TASK indicates the task description. EPISODE indicates the number of previous attempts of the task.</p>
<p>Generate a summary of learning, as a numbered list, that will help the agent to successfully accomplish the SAME task AGAIN, in the SAME world.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY BE CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
{CURRENT TRACE}
Action: ...
Observation: ...
...
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>Summary of learning as a numbered list:</p>
<p>Figure 7: Prompt for CLIN's memory generator during ADAPT</p>
<p>[System]: You are an expert assistant.
[User]: You are given a collection of learning lists, that are derived from actions made by an agent and subsequent observations from a world to accomplish a TYPE of TASKs. All of these TASKs belong to a same TYPE (such as 'boiling') but they are executed in different ENVIRONMENT configurations. A different ENVIRONMENT configuration means there are presence of a different set of objects (lighter instead of a stove) that are critical for solving the TASK, presence of a different set of distractor objects that are not useful for the TASK, a different floor plan, etc.</p>
<p>For each learning list, the TASK description is provided at the beginning as TASK:
Each learning list indicates a list of learnings from the agent's best attempt to solve the TASK.</p>
<p>Each learning list is associated with an EVALUATION REPORT indicated how sucessful the respective attempt was for solving the task.</p>
<p>Consider all learning lists and combine them in to a summary of learnings, as a numbered list, that will help the agent to successfully accomplish a NEW TASK related to the previous TASKs (such as 'boling') in an ENVIRONMENT configuration that it has not seen before. The NEW TASK description will be provided.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY NOT CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
{PREVIOUS LEARNINGS}
TASK: ...
LEARNINGS:...
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>NEW TASK: ...
Summary of learning as a numbered list:</p>
<p>Figure 8: Prompt for CLIN's memory generator during GEN-ENV</p>
<p>[System]: You are an expert assistant.
[User]: You may be given a list of learnings, that are derived from actions made by an agent and subsequent observations from a world to accomplish a TASK in an ENVIRONMENT CONFIGURATION.</p>
<p>For the learning list, the TASK description is provided at the beginning as TASK:
The learnings are from the agent's best attempt to solve the TASK.
The learning list is associated with an EVALUATION REPORT indicated how sucessful the attempt was for solving the task.</p>
<p>Now, generate a summary of learnings from the existing ones if provided, such that they will be useful to the NEW TASK in the SAME ENVIRONMENT CONFIGURATION. The NEW TASK may require different actions which are not captured in the given learnings but given learnings can be used to infer about the ENVIRONMENT CONFIGURATION. The NEW TASK description will be given. If PREVIOUS LEARNINGS says 'No learnings available', improvise learnings for the NEW TASK.</p>
<p>Each numbered item in the summary can ONLY be of the form:
X MAY BE NECCESSARY to Y.
X SHOULD BE NECCESSARY to Y.
X MAY NOT CONTRIBUTE to Y.
X DOES NOT CONTRIBUTE to Y.
(PREVIOUS LEARNINGS)
TASK: . . .
LEARNINGS: . . .
EVALUATION REPORT:
REWARD_FINAL: 100. This means: The agent has performed exceptionally well and successfully solved the task.</p>
<p>NEW TASK: ...
Summary of learning as a numbered list:</p>
<p>Figure 9: Prompt for CLIN's memory generator during GEN-TASK</p>
<p>Task: Your task is to determine whether round seed shape is a dominant or recessive trait in the pea plant. If the trait is dominant, focus on the blue box. If the trait is recessive, focus on the orange box.</p>
<p>Memory (ADAPT), after Trial5:</p>
<ol>
<li>Looking around MAY BE NECESSARY to gather information about the environment.</li>
<li>Moving to the greenhouse SHOULD BE NECESSARY to find the pea seeds.</li>
<li>Picking the jar SHOULD BE NECESSARY to access the pea seeds.</li>
<li>Moving the round green pea seed to flower pot 2 SHOULD BE NECESSARY to grow the pea plants.</li>
<li>Moving the wrinkly green pea seed to flower pot 3 SHOULD BE NECESSARY to grow the pea plants.</li>
<li>Waiting SHOULD BE NECESSARY for the pea plants to grow and show their traits.</li>
<li>Looking at the adult pea plant in flower pots 2 and 3 MAY BE NECESSARY to observe the traits of the offspring.</li>
<li>Focusing on the blue box SHOULD BE NECESSARY to indicate the trait of the seed shape.</li>
<li>Continuously looking at the same environment DOES NOT CONTRIBUTE to the task of determining the dominant or recessive trait in the pea plant.</li>
<li>Moving to the workshop DOES NOT CONTRIBUTE to determining the dominant or recessive trait in the pea plant.</li>
<li>Picking up the battery or the wires in the workshop DOES NOT CONTRIBUTE to finding the pea seeds.</li>
<li>Waiting for longer periods MAY BE NECESSARY to allow the pea plants to fully grow and show their traits.</li>
</ol>
<p>Figure 10: Memory generated after Trial5 in ADAPT for a Genetics task.</p>
<p>Task: Your task is to determine which of the two inclined planes (aluminum, platinum) has the most friction. After completing your experiment, focus on the inclined plane with the most friction.</p>
<p>Meta-memory (GEN-ENV):</p>
<ol>
<li>Moving to the hallway SHOULD BE NECESSARY to reach the workshop.</li>
<li>Moving to the workshop SHOULD BE NECESSARY to find the block.</li>
<li>Picking up the block SHOULD BE NECESSARY to move it to the inclined planes.</li>
<li>Placing the block on the first inclined plane (either aluminum or platinum) SHOULD BE NECESSARY to measure the friction.</li>
<li>Activating the stopwatch SHOULD BE NECESSARY to time the experiment.</li>
<li>Waiting for a certain period MAY CONTRIBUTE to observing the friction effect.</li>
<li>Deactivating the stopwatch SHOULD BE NECESSARY to stop timing the experiment.</li>
<li>Moving the block to the second inclined plane (either aluminum or platinum) SHOULD BE NECESSARY to compare the friction.</li>
<li>Activating the stopwatch again SHOULD BE NECESSARY to time the second part of the experiment.</li>
<li>Waiting for a certain period again MAY BE NECESSARY to observe the friction effect.</li>
<li>Deactivating the stopwatch again SHOULD BE NECESSARY to stop timing the experiment.</li>
<li>Focusing on the inclined plane with the most friction SHOULD BE NECESSARY to conclude the experiment.</li>
<li>Repeating the experiment multiple times MAY BE NECESSARY for more accurate results.</li>
<li>Looking around in the initial room multiple times DOES NOT CONTRIBUTE to the task.</li>
<li>Moving the block back and forth between the two inclined planes DOES NOT CONTRIBUTE to the task.</li>
</ol>
<p>Figure 11: Meta-memory used in GEN-ENV for a Friction task.</p>
<p>Task: Your task is to determine whether round seed shape is a dominant or recessive trait in the pea plant. If the trait is dominant, focus on the blue box. If the trait is recessive, focus on the orange box.</p>
<p>Meta-memory (GEN-TASK):
Task: Your task is to freeze mercury. First, focus on the substance. Then, take actions that will cause it to change its state of matter.</p>
<p>Meta-memory (GEN-TASK):</p>
<ol>
<li>Looking around MAY BE NECESSARY to identify the available resources and the layout of the environment.</li>
<li>Moving to different rooms SHOULD BE NECESSARY to find the tools and materials needed to change the state of the substance.</li>
<li>Picking up items like glass cups or metal pots SHOULD BE NECESSARY to contain the substance for changing its state.</li>
<li>Focusing on the substance SHOULD BE NECESSARY to understand its properties and how to interact with it.</li>
<li>Picking up the thermometer SHOULD BE NECESSARY to monitor the temperature of the substance.</li>
<li>Using the thermometer on the substance SHOULD BE NECESSARY to monitor the progress of the task.</li>
<li>Puring the substance into the container SHOULD BE NECESSARY to prepare it for cooling.</li>
<li>Moving the container to a cooling device SHOULD BE NECESSARY to cool the substance.</li>
<li>Waiting for a period of time after cooling the substance SHOULD BE NECESSARY to allow the substance to change state.</li>
<li>Repeatedly checking the temperature of the substance SHOULD BE NECESSARY to monitor the progress of the task.</li>
<li>Activating the stove DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Picking up unrelated items like a lighter DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Moving to unrelated rooms like the workshop DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
<li>Teleporting to the kitchen MAY BE NECESSARY for the task as it speeds up the process of moving between rooms.</li>
<li>Using the thermometer multiple times on the substance after it reaches freezing point DOES NOT CONTRIBUTE to the task as it does not progress the task.</li>
</ol>
<p>Figure 12: Meta-memory used in GEN-TASK for a Freeze task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generative L. Agents</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CLIN (ours)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: center;">Type</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">BASE</td>
<td style="text-align: center;">ADAPT</td>
</tr>
<tr>
<td style="text-align: left;">Temp</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">14.3</td>
</tr>
<tr>
<td style="text-align: left;">Temp</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: left;">Pick\&amp;Place</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Pick\&amp;Place</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: left;">Lifespan</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Lifespan</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: left;">Biology</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: left;">Boil</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">15.2</td>
</tr>
<tr>
<td style="text-align: left;">Freeze</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: left;">GrowPlant</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">GrowFruit</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">71.6</td>
</tr>
<tr>
<td style="text-align: left;">Biology</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">81.0</td>
</tr>
<tr>
<td style="text-align: left;">Force</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Friction</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">72.5</td>
</tr>
<tr>
<td style="text-align: left;">Genetics</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Genetics</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">92.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">$\mathbf{6 2 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">$\mathbf{6 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">$\mathbf{6 2 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparing CLIN with baselines for adaptation</p>
<h1>C MORE RESULTS</h1>
<p>Full results for CLIN outperforming Reflexion is in Table 2. For ScienceWorld benchmark, we exclude electricity tasks since they deviate from standard electrical conventions, prohibiting us from fairly using LLM agents. We choose the first 10 test variants for each 18 tasks selected. The full list of 18 tasks from the benchmark, with the number of test variants used in parentheses:
grow-plant (10), identify-life-stages-1 (5), grow-fruit (10), measure-melting-point-known-substance (10), mendelian-genetics-unknown-plant (10), chemistry-mix-paint-secondary-color (9), freeze (9), lifespan-longest-lived (10), inclined-plane-determine-angle (10), boil (9), use-thermometer (10), chemistry-mix (8), lifespan-shortest-lived (10), find-plant (10), find-living-thing (10), identify-life-stages-2 (4), mendelian-genetics-known-plant (10), inclined-plane-friction-named-surfaces (10).
Short tasks have oracle lengths less than 37 steps (median), and Long tasks have oracle lengths more than equal to 37 steps.</p>
<p>The map to the short names used for tasks in the paper:
Temp: use-thermometer, measure-melting-point-known-substance; Pick\&amp;Place: find-plant, find-living-thing; Chemistry: chemistry-mix, chemistry-mix-paint-secondary-color; Lifespan: lifespan-longest-lived, lifespan-shortest-lived; Biology: identify-life-stages-1, identify-life-stages-2, Boil; Freeze; Grow Plant, Grow Fruit; Force: inclined-plane-determine-angle; Friction: inclined-plane-friction-named-surfaces; Genetics: mendelian-genetics-known-plant, mendelian-genetics-unknownplant.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Baseline numbers are derived from Table 1 in (Lin et al., 2023)
${ }^{4} #$ steps in Figure 5a,b are normalized between $0-1,1$ being maximum # steps allowed for a task.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4872 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4872</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4872</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-73207b9fd2dcfeead7fe086cfdb097e4929a7b44</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/73207b9fd2dcfeead7fe086cfdb097e4929a7b44" target="_blank">MoT: Memory-of-Thought Enables ChatGPT to Self-Improve</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference and can lead to consistent improvements across various CoT methods and LLMs.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive abilities in various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory-of-Thought, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4872.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4872.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage framework that lets a frozen LLM self-improve by pre-generating and storing high-confidence chain-of-thought reasoning paths as external memory, then retrieving and conditioning on those memories at test time without parameter updates or annotated labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo-0301) + MoT framework</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A frozen LLM (primarily ChatGPT / gpt-3.5-turbo-0301 in experiments) is used to (1) pre-think on an unlabeled dataset to produce multiple Chain-of-Thought (CoT) reasoning paths per example and store selected high-confidence thoughts externally, and (2) recall relevant stored thoughts at test time (as few-shot CoT demonstrations) to improve reasoning and answers. The framework is model-agnostic and also evaluated with text-davinci-002/003.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external episodic memory (retrieval-augmented store of saved Chain-of-Thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory pool M stores question/rationale/answer triples generated during a pre-thinking stage; memories are filtered by answer-entropy (uncertainty), clustered for diversity, semantically filtered (SBERT) per cluster to top-k candidates, and then an LLM-based retriever (LLM-retrieval) selects one memory per cluster; selected memories are concatenated as few-shot CoT demonstrations for the final LLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-domain multi-step reasoning (arithmetic reasoning, commonsense reasoning, factual reasoning, NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning tasks requiring chain-of-thought: arithmetic word problems and reading-comprehension requiring discrete reasoning (AQuA, DROP), natural language inference (ANLI-A1/A2/A3), commonsense QA (OBQA, ComV), factual claim verification and QA (BoolQ, FactCK, WikiQA). Evaluations use accuracy or F1 depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQuA, DROP, ANLI-A1/A2/A3, OBQA, ComV, BoolQ, FactCK, WikiQA (as reported in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MoT (ChatGPT gpt-3.5-turbo-0301) average: 69.2 (AVG across listed datasets); per-dataset highlights from Table 1: AQuA 54.1, DROP 65.7, ANLI-A1 64.6, ANLI-A2 52.8, ANLI-A3 55.2, OBQA 82.3, ComV 95.5, BoolQ 71.5, FactCK 82.2, WikiQA 68.0. MoT (with gold filter) average: 71.3; also robust improvements reported on text-davinci-002/003 (e.g., OBQA 80.0→MoT 80.0/84.6 on different LLMs per Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines (ChatGPT gpt-3.5) reported in Table 1: Zero-Shot (direct) AVG 56.8; Few-Shot (direct) AVG 57.6; Zero-Shot-CoT AVG 60.1; Few-Shot-CoT AVG 65.5. Example per-dataset Few-Shot-CoT values: AQuA 49.7, DROP 57.6, OBQA 80.0, BoolQ 67.7, WikiQA 65.2. Variants: MoT (no rationale) AVG 62.7, MoT (no thinking) AVG 63.1.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Using MoT's memory yields large and consistent gains over both zero-shot and few-shot baselines (MoT AVG 69.2 vs Few-Shot-CoT 65.5 and Zero-Shot-CoT 60.1). Filtering memories by answer-entropy is important (unfiltered MoT can degrade and even underperform Few-Shot-CoT on some datasets). Variants show that both relevant memory and explicit CoT reasoning contribute: removing rationales or removing thinking reduces but does not eliminate benefits. MoT (with gold labels to filter memory) gives further improvement, indicating room from false-memory removal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Stored memories still contain errors despite answer-entropy filtering; incorrect memories can deteriorate performance if not filtered. Memory usage is constrained by LLM input length (necessitates clustering and candidate selection). Pre-thinking and retrieval incur API/inference cost (budget). Current memory utilization is a simple concatenation of retrieved memories; more sophisticated use (e.g., verification or guided retrieval) is left for future work. Experiments were conducted in a controlled/safe setting (no internet or tool access) and the method may propagate model biases present in self-generated memories.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>1) LLMs can self-improve without parameter updates or annotated data by storing and recalling high-confidence self-generated chain-of-thoughts. 2) Answer-entropy filtering of multiple sampled reasoning paths effectively reduces noisy/uncertain memories. 3) LLM-based retrieval of memory (LLM-retrieval) outperforms pure semantic-embedding retrieval (SBERT variants) on reasoning tasks. 4) MoT consistently boosts performance across multiple CoT prompting methods and multiple LLM families and remains useful even with limited memory size (benefit persists at 10% memory). 5) Benefits diminish when using extensive self-consistency (many sampled decodings), indicating prepared memory helps most when test-time deliberation is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>MoT is introduced and experimentally validated in this paper (instantiation uses Few-Shot-CoT for pre-thinking, answer-entropy filtering, SBERT semantic filtering + LLM-retrieval selection). Detailed per-dataset numbers are given in Tables 1, 2, 3, 4 and 5 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MoT: Memory-of-Thought Enables ChatGPT to Self-Improve', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4872.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4872.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Retrieval for Memory Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage retrieval approach where semantic embedding (SBERT) first filters candidate memories per cluster and then the LLM itself selects the single most helpful memory among candidates for the current test question, leveraging the LLM's deeper logical understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-retrieval (component within MoT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval component that uses semantic clustering + SBERT top-k filtering to produce candidate memories per cluster and then prompts the same LLM to choose the most helpful candidate (with a constrained output format) for each cluster; selections are concatenated to form the few-shot CoT context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented selection (LLM-guided retrieval from an external memory pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory pool is partitioned into clusters for diversity; per cluster SBERT (all-mpnet-base-v2) computes similarity to q_test and top-k candidates are kept; the LLM is then prompted with 'References: [candidates] Target Question: [q_test] ... which one reference would be the most helpful?' The LLM returns the index of the most helpful question; only questions (not full rationales) can be provided to reduce cost in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Memory retrieval to improve few-shot chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select a small set of stored CoT exemplars that are most relevant/helpful to the current test question so they can be used as in-context demonstrations to improve the LLM's answer.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Evaluated within MoT on DROP, ANLI-A3, OBQA, BoolQ, WikiQA (Table 3 and other experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Comparative results in Table 3 (adding LLM-retrieval): DROP 65.7, ANLI-A3 55.2, BoolQ 71.5, WikiQA 68.0 (these are MoT numbers using LLM-retrieval). Using only MPNet (SBERT all-mpnet-base-v2) gives slightly lower: DROP 64.7, ANLI-A3 53.3, BoolQ 70.4, WikiQA 67.1. Baseline Few-Shot-CoT: DROP 57.6, ANLI-A3 52.3, BoolQ 67.7, WikiQA 65.2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not applicable in isolation (LLM-retrieval is a retrieval method); but compared to pure semantic retrieval (+MPNet) it provides measurable gains (see performance_with_memory vs +MPNet).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>LLM-retrieval outperforms semantic-embedding-only retrieval (MPNet, MiniLM, Instructor-base) across evaluated datasets, showing that letting the LLM select among semantically filtered candidates improves downstream reasoning performance. Even semantic-only retrieval (+MPNet) improves over Few-Shot-CoT, but LLM-retrieval provides an additional boost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM-retrieval requires extra LLM calls (costly) and depends on good candidate filtering to limit LLM input size; providing full rationales for all candidates is expensive so authors used just candidate questions (which they found sufficient). The approach relies on the LLM's self-evaluation abilities and may inherit the LLM's biases or blindspots in selection.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Hybrid retrieval (semantic filter for efficiency + LLM selection for deep logical matching) is effective for retrieving helpful chain-of-thought exemplars; using the LLM as a retriever captures complex reasoning similarities that vector similarity alone misses, improving final task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MoT: Memory-of-Thought Enables ChatGPT to Self-Improve', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models can self-improve <em>(Rating: 2)</em></li>
                <li>STAR: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Self-prompting large language models for open-domain QA <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4872",
    "paper_id": "paper-73207b9fd2dcfeead7fe086cfdb097e4929a7b44",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "MoT",
            "name_full": "Memory-of-Thought",
            "brief_description": "A two-stage framework that lets a frozen LLM self-improve by pre-generating and storing high-confidence chain-of-thought reasoning paths as external memory, then retrieving and conditioning on those memories at test time without parameter updates or annotated labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ChatGPT (GPT-3.5-Turbo-0301) + MoT framework",
            "agent_description": "A frozen LLM (primarily ChatGPT / gpt-3.5-turbo-0301 in experiments) is used to (1) pre-think on an unlabeled dataset to produce multiple Chain-of-Thought (CoT) reasoning paths per example and store selected high-confidence thoughts externally, and (2) recall relevant stored thoughts at test time (as few-shot CoT demonstrations) to improve reasoning and answers. The framework is model-agnostic and also evaluated with text-davinci-002/003.",
            "memory_type": "external episodic memory (retrieval-augmented store of saved Chain-of-Thoughts)",
            "memory_description": "Memory pool M stores question/rationale/answer triples generated during a pre-thinking stage; memories are filtered by answer-entropy (uncertainty), clustered for diversity, semantically filtered (SBERT) per cluster to top-k candidates, and then an LLM-based retriever (LLM-retrieval) selects one memory per cluster; selected memories are concatenated as few-shot CoT demonstrations for the final LLM inference.",
            "task_name": "Multi-domain multi-step reasoning (arithmetic reasoning, commonsense reasoning, factual reasoning, NLI)",
            "task_description": "Multi-step reasoning tasks requiring chain-of-thought: arithmetic word problems and reading-comprehension requiring discrete reasoning (AQuA, DROP), natural language inference (ANLI-A1/A2/A3), commonsense QA (OBQA, ComV), factual claim verification and QA (BoolQ, FactCK, WikiQA). Evaluations use accuracy or F1 depending on dataset.",
            "benchmark_name": "AQuA, DROP, ANLI-A1/A2/A3, OBQA, ComV, BoolQ, FactCK, WikiQA (as reported in the paper)",
            "performance_with_memory": "MoT (ChatGPT gpt-3.5-turbo-0301) average: 69.2 (AVG across listed datasets); per-dataset highlights from Table 1: AQuA 54.1, DROP 65.7, ANLI-A1 64.6, ANLI-A2 52.8, ANLI-A3 55.2, OBQA 82.3, ComV 95.5, BoolQ 71.5, FactCK 82.2, WikiQA 68.0. MoT (with gold filter) average: 71.3; also robust improvements reported on text-davinci-002/003 (e.g., OBQA 80.0→MoT 80.0/84.6 on different LLMs per Table 5).",
            "performance_without_memory": "Baselines (ChatGPT gpt-3.5) reported in Table 1: Zero-Shot (direct) AVG 56.8; Few-Shot (direct) AVG 57.6; Zero-Shot-CoT AVG 60.1; Few-Shot-CoT AVG 65.5. Example per-dataset Few-Shot-CoT values: AQuA 49.7, DROP 57.6, OBQA 80.0, BoolQ 67.7, WikiQA 65.2. Variants: MoT (no rationale) AVG 62.7, MoT (no thinking) AVG 63.1.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Using MoT's memory yields large and consistent gains over both zero-shot and few-shot baselines (MoT AVG 69.2 vs Few-Shot-CoT 65.5 and Zero-Shot-CoT 60.1). Filtering memories by answer-entropy is important (unfiltered MoT can degrade and even underperform Few-Shot-CoT on some datasets). Variants show that both relevant memory and explicit CoT reasoning contribute: removing rationales or removing thinking reduces but does not eliminate benefits. MoT (with gold labels to filter memory) gives further improvement, indicating room from false-memory removal.",
            "limitations_or_challenges": "Stored memories still contain errors despite answer-entropy filtering; incorrect memories can deteriorate performance if not filtered. Memory usage is constrained by LLM input length (necessitates clustering and candidate selection). Pre-thinking and retrieval incur API/inference cost (budget). Current memory utilization is a simple concatenation of retrieved memories; more sophisticated use (e.g., verification or guided retrieval) is left for future work. Experiments were conducted in a controlled/safe setting (no internet or tool access) and the method may propagate model biases present in self-generated memories.",
            "key_insights": "1) LLMs can self-improve without parameter updates or annotated data by storing and recalling high-confidence self-generated chain-of-thoughts. 2) Answer-entropy filtering of multiple sampled reasoning paths effectively reduces noisy/uncertain memories. 3) LLM-based retrieval of memory (LLM-retrieval) outperforms pure semantic-embedding retrieval (SBERT variants) on reasoning tasks. 4) MoT consistently boosts performance across multiple CoT prompting methods and multiple LLM families and remains useful even with limited memory size (benefit persists at 10% memory). 5) Benefits diminish when using extensive self-consistency (many sampled decodings), indicating prepared memory helps most when test-time deliberation is limited.",
            "additional_notes": "MoT is introduced and experimentally validated in this paper (instantiation uses Few-Shot-CoT for pre-thinking, answer-entropy filtering, SBERT semantic filtering + LLM-retrieval selection). Detailed per-dataset numbers are given in Tables 1, 2, 3, 4 and 5 of the paper.",
            "uuid": "e4872.0",
            "source_info": {
                "paper_title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-retrieval",
            "name_full": "LLM-based Retrieval for Memory Selection",
            "brief_description": "A two-stage retrieval approach where semantic embedding (SBERT) first filters candidate memories per cluster and then the LLM itself selects the single most helpful memory among candidates for the current test question, leveraging the LLM's deeper logical understanding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM-retrieval (component within MoT)",
            "agent_description": "Retrieval component that uses semantic clustering + SBERT top-k filtering to produce candidate memories per cluster and then prompts the same LLM to choose the most helpful candidate (with a constrained output format) for each cluster; selections are concatenated to form the few-shot CoT context.",
            "memory_type": "retrieval-augmented selection (LLM-guided retrieval from an external memory pool)",
            "memory_description": "Memory pool is partitioned into clusters for diversity; per cluster SBERT (all-mpnet-base-v2) computes similarity to q_test and top-k candidates are kept; the LLM is then prompted with 'References: [candidates] Target Question: [q_test] ... which one reference would be the most helpful?' The LLM returns the index of the most helpful question; only questions (not full rationales) can be provided to reduce cost in practice.",
            "task_name": "Memory retrieval to improve few-shot chain-of-thought reasoning",
            "task_description": "Select a small set of stored CoT exemplars that are most relevant/helpful to the current test question so they can be used as in-context demonstrations to improve the LLM's answer.",
            "benchmark_name": "Evaluated within MoT on DROP, ANLI-A3, OBQA, BoolQ, WikiQA (Table 3 and other experiments)",
            "performance_with_memory": "Comparative results in Table 3 (adding LLM-retrieval): DROP 65.7, ANLI-A3 55.2, BoolQ 71.5, WikiQA 68.0 (these are MoT numbers using LLM-retrieval). Using only MPNet (SBERT all-mpnet-base-v2) gives slightly lower: DROP 64.7, ANLI-A3 53.3, BoolQ 70.4, WikiQA 67.1. Baseline Few-Shot-CoT: DROP 57.6, ANLI-A3 52.3, BoolQ 67.7, WikiQA 65.2.",
            "performance_without_memory": "Not applicable in isolation (LLM-retrieval is a retrieval method); but compared to pure semantic retrieval (+MPNet) it provides measurable gains (see performance_with_memory vs +MPNet).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "LLM-retrieval outperforms semantic-embedding-only retrieval (MPNet, MiniLM, Instructor-base) across evaluated datasets, showing that letting the LLM select among semantically filtered candidates improves downstream reasoning performance. Even semantic-only retrieval (+MPNet) improves over Few-Shot-CoT, but LLM-retrieval provides an additional boost.",
            "limitations_or_challenges": "LLM-retrieval requires extra LLM calls (costly) and depends on good candidate filtering to limit LLM input size; providing full rationales for all candidates is expensive so authors used just candidate questions (which they found sufficient). The approach relies on the LLM's self-evaluation abilities and may inherit the LLM's biases or blindspots in selection.",
            "key_insights": "Hybrid retrieval (semantic filter for efficiency + LLM selection for deep logical matching) is effective for retrieving helpful chain-of-thought exemplars; using the LLM as a retriever captures complex reasoning similarities that vector similarity alone misses, improving final task performance.",
            "uuid": "e4872.1",
            "source_info": {
                "paper_title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models can self-improve",
            "rating": 2
        },
        {
            "paper_title": "STAR: Bootstrapping reasoning with reasoning",
            "rating": 2
        },
        {
            "paper_title": "Self-prompting large language models for open-domain QA",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        }
    ],
    "cost": 0.013921,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MoT: Memory-of-Thought Enables ChatGPT to Self-Improve</h1>
<p>Xiaonan Li, Xipeng Qiu<br>School of Computer Science, Fudan University<br>Shanghai Key Laboratory of Intelligent Information Processing, Fudan University<br>{lixn20, xpqiu}@fudan.edu.cn</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have shown impressive abilities in various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory-ofThought, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have demonstrated surprising abilities on a wide range of Natural Language Processing (NLP) tasks (Chen et al., 2023; Zhang et al., 2022a; Chowdhery et al., 2022; Tay et al., 2022; OpenAI, 2023; Hoffmann et al., 2022; Touvron et al., 2023; Mialon et al., 2023; Zhao et al., 2023; Qiu et al., 2020). Notably, new abilities emerge in LLMs as they are scaled to hundreds of billions of parameters, like in-context few-shot learning (Chen et al., 2023; Dong et al., 2022), simple digit operation and factual knowledge query (Wei et al., 2022b). Especially, the general reasoning ability of the LLM has impressed the NLP community and relevant techniques have achieved a series of new state-of-the-art (Wei et al., 2022c; Kojima et al., 2022; Lampinen et al., 2022;
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The comparison between fine-tuning and MoT: while fine-tuning LLM with labeled datasets is costly and needs powerful computational resources, MoT can make the LLM self-improve via pre-thinking and recalling, without parameter updates and annotated datasets.</p>
<p>Wang et al., 2022b; Huang and Chang, 2022). Specifically, Wei et al. (2022c) and Kojima et al. (2022) propose few-shot CoT and zero-shot CoT, which elicit LLM's reasoning by few-shot demonstrations and simple yet effective "Let's think step by step" prompting, respectively. Based on them, Wang et al. (2022b); Press et al. (2022); Zhou et al. (2022); Wang et al. (2023); Weng et al. (2022) further propose self-consistency, self-ask, least-tomost, plan-and-solve, etc., to achieve more complicated reasoning in various specialized scenarios.</p>
<p>Despite the impressive abilities of the LLM pretrained on the large corpus, fundamentally improving the LLM's performance beyond few-shot / zero-shot baselines highly depends on either highquality annotated datasets or costly fine-tuning of LLMs. In general, these methods can be divided into three categories: 1. Annotated Datasets + Fine-tuning: Wei et al. (2022a) and Sanh et al. (2022) propose FLAN and T0 respectively to enhance the LLM's zero-shot ability by tens of curated NLP benchmark datasets. Based on FLAN, Chung et al. (2022) scale up its training in terms of model size and the number of tasks, and demonstrate that the added CoT examples with rationales improve the LLM's reasoning abilities. InstructGPT (Ouyang et al., 2022) improves the GPT3 's instruction-following ability by fine-tuning on many diverse crowd-sourced instruction-answer</p>
<p>pairs. 2. Retrieving Annotated Data: Liu et al. (2022), Su et al. (2022a) and Agrawal et al. (2022) use SentenceBERT (Reimers and Gurevych, 2019) or BM25 (Robertson and Zaragoza, 2009) to retrieve relevant examples from the annotated dataset, to improve LLM's in-context learning. Rubin et al. (2022) and Shi et al. (2022a) leverage annotated datasets to train retrievers by the LM-feedback to retrieve helpful demonstrations for the test example. 3. Fine-tuning with LLM-generated data: Zelikman et al. (2022) let the LM generate rationales for annotated dataset and train itself to enhance the reasoning ability. Magister et al. (2022),Ho et al. (2022) and Fu et al. (2023a) use the reasoning paths generated by large LM to improve the small LM's reasoning capability. More recently, Huang et al. (2022) demonstrate the effectiveness of self-training on PaLM (Chowdhery et al., 2022).</p>
<p>As annotating high-quality data, especially rationales in CoT data, is expensive, fine-tuning LLM requires extremely powerful computational resources and results in high computational costs. Methods above that rely on fine-tuning also face two challenges: 1. Since the most powerful LLMs, e.g., GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022; Anil et al., 2023), are only publicly available through the inference API, it is not feasible for most of the research community to improve them by these methods. 2. Fine-tuning LLM for specific capability enhancement is costly and not environmentally friendly. As the LLM has massive parameters, fine-tuning them will lead to substantial costs of model storage and deployment. Further studies show that fine-tuning the LLM with specialized data may significantly decrease its general abilities (Fu et al., 2023b).</p>
<p>While considerable efforts were dedicated to collecting high-quality annotated datasets and finetuning the LLM, which is costly and may decrease its general ability, on the contrary, humans can improve their own reasoning abilities through the metacognition process (Dunlosky and Metcalfe, 2008) and the memory mechanisms (Tulving, 2002), and preserve their general abilities. For example, memory helps humans improve themselves in terms of decision-making, reasoning, judgment, etc (Tulving, 2002). Inspired by this, we propose MoT, shown in Figure 1, a pre-think-then-recall framework to let the LLM self-improve through Memory-of-Thoughts, without supervised data and parameter updates. In the pre-thinking stage, the LLM thinks on the unlabeled dataset and saves the
thoughts as external memory. In the test stage, the LLM recalls relevant memory to help reason and answer the given test question. Since we focus on the overall framework and aim to demonstrate its generality and extensibility, we use simple components to instantiate these two stages. Specifically, we use the simple Few-Shot-CoT (Wei et al., 2022c) with multiple-path decoding strategy (Wang et al., 2022b) in the pre-thinking stage and propose answer-entropy to filter out uncertain thoughts. For memory recall, we propose LLM-retrieval, which lets the LLM itself retrieve relevant memory to help answer the test question. Compared with typical semantic retrievers like SBERT (Reimers and Gurevych, 2019), LLM-retrieval can better capture the deep connection of complicated logic and reasoning than semantic embeddings.</p>
<p>We summarize our contribution as follows:</p>
<ul>
<li>To the best of our knowledge, the proposed framework is the first to let LLM improve its own reasoning abilities based on the memory mechanism, without parameter updates and annotated datasets.</li>
<li>We conduct comprehensive experiments on extensive datasets and the results show that MoT can help ChatGPT improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning and natural language inference without parameter updates and annotated datasets. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.</li>
<li>We release the code and generated CoT reasoning paths to facilitate future research ${ }^{1}$. In this paper, we instantiate the proposed framework with simple components and demonstrate its effectiveness. We hope that MoT can inspire researchers of the important design choices about making the LLM self-improve with memory mechanisms and pave the way for further improvements.</li>
</ul>
<h2>2 Background: Chain of Thought</h2>
<p>The large language model has shown impressive reasoning abilities on various tasks. Chain-ofThoughts (CoT) prompting (Wei et al., 2022c; Kojima et al., 2022) is the most prevailing way to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The illustration of Few-Shot-CoT.
let the LLM reason, i.e., generate a series of intermediate reasoning steps that lead to the final answer. As shown in Figure 2, Few-Shot-CoT (Wei et al., 2022c; Lampinen et al., 2022) provides a few demonstrations with rationales, i.e., question/rationale/answer pairs, and prompts the LLM to generate the rationale that leads to the final answer. Zero-Shot-CoT (Kojima et al., 2022) adds the prompt, "Let's think step by step", after the test question and elicits the LLM's reasoning. Specifically, the Few-Shot-CoT gets the answer as:</p>
<p>$$
\begin{aligned}
&amp; s=\operatorname{LLM}\left(d_{1}, d_{2}, \cdots, q_{\text {test }}\right) \
&amp; a=\text { Parse-Answer }(s)
\end{aligned}
$$</p>
<p>where $d_{i}=\left[x_{i}, r_{i}, a_{i}\right]$ is the $i$ th demonstration and consists of the input, rationale and answer. Few-Shot-CoT first decodes $s$ from the LLM given the few-shot CoT demonstrations, and parses $s$ to get the final answer. Since the demonstration is typically in the format: "[input] [rationale] The answer is [answer]", the answer can be easily parsed from $s$ by the trigger "The answer is" (Wei et al., 2022c). Similarly, Zero-Shot-CoT uses answer triggers, e.g., "Therefore, the answer is", to extract the final answer from the zero-shot reasoning path generated by LLM (Kojima et al., 2022).</p>
<h2>3 Method</h2>
<p>We show the overview of our framework in Figure 3. In this paper, we mainly focus on making LLM self-improve in the typical few-shot CoT scenario, where we are given a frozen large language model and an unlabeled dataset with a few CoT demonstrations (Wei et al., 2022c; Huang et al., 2022). We further demonstrate MoT's effectiveness in zero-shot scenarios in section 4.3. Our framework is divided into two stages: 1. Pre-Think Before the test stage, the LLM thinks over the unlabeled dataset and keeps the high-confidence reasoning paths as memory. 2. Recall In the test stage,
given a test question, we propose LLM-retrieval to let the LLM retrieve relevant memory to help itself reason and answer it. Our method does not depend on high-quality labeled datasets and costly finetuning of LLM, and it is feasible when the LLM is frozen or only available through the inference API. Since we let the LLM think over the unlabeled dataset, save the self-generated thoughts as external memory and retrieve relevant memory for itself to help reasoning, we consider our method as making the LLM self-improve with Memory-of-Thought. We introduce these two stages below.</p>
<h3>3.1 Pre-Thinking</h3>
<h3>3.1.1 Let LLM Think before Test Stage</h3>
<p>In this stage, we let the LLM think over the unlabeled dataset and save the resultant question/rationale/answer pairs as external memory. Since we focus on the overall framework and aim to demonstrate its generality and extensibility, we instantiate the "thinking" mechanism here as the simple Few-Shot-CoT (Wei et al., 2022c) with multiple-path decoding strategy (Wang et al., 2022b) in this paper. Specifically, for each example $x$ from the unlabeled dataset $X$, we let the LLM sample $n$ reasoning paths and answers with temperature $T&gt;0$, denoted as $\left[r_{1}, r_{2} \cdots, r_{n}\right]$ and $\left[a_{1}, a_{2} \cdots, a_{n}\right]$. Then we use majority-voting to select the most consistent answer, $\widetilde{a}=\arg \max <em i="i">{a</em>$, as memory. Since we only consider the thought that leads to the most consistent answer, the retained thoughts can be more accurate (Wang et al., 2022b) and better help the test stage. For simplicity and to save memory size, we randomly select one reasoning path of the final answer for each unlabeled example and see saving multiple thoughts for one question as future work.}} \sum_{j=1}^{n} \mathbb{1}\left(a_{i}=a_{j}\right)$, and keep the reasoning path, which leads to $\widetilde{a</p>
<h3>3.1.2 High-Confidence Thought Filtering</h3>
<p>Since the most consistent answer does not necessarily lead to the correct answer and incorrect demonstrations can cause inferior performance (Yoo et al., 2022; Lyu et al., 2022), we further propose to filter the thoughts by uncertainty. Inspired by Liu et al. (2020); Xin et al. (2020), we propose the answerentropy $u(\cdot)$ to filter out high-uncertainty thoughts:</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The overview of MoT.</p>
<p>Figure 4: The relation between accuracy and the filter-out ratio, after multiple-paths decoding on the DROP dataset. Under the same filter-out ratio, filtering by answer entropy leads to slightly higher F1-score than filtering by Max-P proposed by Huang et al. (2022).</p>
<p>$$
\begin{aligned}
&amp; A^<em> = \text{unique}({a_i}_{i=1}^n) \
&amp; p(a_i^</em>) = \sum_{j=1}^n \mathbb{1}(a_i^<em> = a_j)/n \
&amp; u(a_i^</em>) = -\sum_{i=1}^{|A^i|} p(a_i^<em>) \log p(a_i^</em>)
\end{aligned}
$$</p>
<p>where $A^<em> = {a_1^</em>, a_2^*, \ldots}$ is the set of answers. $u(\cdot)$ indicates the answer uncertainty, and the higher $u(\cdot)$ is, the more uncertain the LLM is. We filter out thoughts whose uncertainty is higher than $\tau$ and $\tau$ is a pre-defined threshold. In the exploratory experiment (Figure 4), we find the thought with lower uncertainty is more likely to be correct: the stricter the filtering is, the more accurate the remaining thoughts are. Hence the answer-entropy can filter out noisy thoughts and lead to more accurate thoughts for recalling. Compared with the filtering in Huang et al. (2022), which uses the number of consistent paths (max probability, abbreviated as Max-P) as metric, answer-entropy leads to slightly higher accuracy, under the same filter-out ratio.</p>
<p>After filtering, we obtain the pool of memory-of-thoughts, $M = {m_i}_{i=1}^{|M|}$, where $m_i$ is the concatenation of corresponding input, reasoning path and answer (see Figure 3). $M$ consists of the high-quality thoughts of LLM on various questions and thus contains crucial and valuable information for the LLM to answer the test question. For the coherence in the subsequent content, we will refer to $m_i$ as "memory" or "thought".</p>
<h3>3.2 Recalling</h3>
<p>In the test stage, the relevant memory is retrieved from the memory pool $M$, to help the LLM answer the given test question, $q_{test}$. Although semantic embedders, e.g., SBERT (Reimers and Gurevych, 2019) are capable of retrieving semantically relevant examples for ICL (Liu et al., 2022), for reasoning tasks, it is challenging for them to fully capture the deep logical connections between $q_{test}$ and helpful memory, as a single vector can not directly reflect the intricate logic and reasoning path. Since the LLM, e.g., ChatGPT, has shown impressively powerful and general natural language understanding capability and a certain level of self-awareness (Kadavath et al., 2022), we propose LLM-retrieval to let the LLM retrieve helpful memory for itself.</p>
<p>As the LLM has a limitation of the max length, it is infeasible to let the LLM directly select among the entire memory pool. Inspired by human's memory recall process, where we usually first unconsciously filter the relevant memories and then consciously evaluate them (Schacter and Addis, 2007; Franklin et al., 2005), we divide LLM-retrieval into two stages: 1. filter out semantically irrelevant memory and get memory candidates; 2. let the LLM choose from memory candidates.</p>
<p>Since the diversity of demonstrations has been shown important for LLMs (Ye et al., 2023; Levy et al., 2022; Li and Qiu, 2023), we follow Li et al. (2022) to conduct memory retrieval with diversity-</p>
<p>based clustering, i.e., we partition the entire memory pool into $l$ clusters, $\left{M^{(1)}, M^{(2)} \cdots, M^{(l)}\right}$, and retrieve one memory from each cluster separately. Specifically, for each cluster $M^{(i)}$, we first use an off-the-shelf semantic embedder, e.g., SBERT (Reimers and Gurevych, 2019), to filter out semantically irrelevant memory and get memory candidates as follows:</p>
<p>$$
M_{c}^{(i)}=\operatorname{top-k}<em _test="{test" _text="\text">{m \in M^{(i)}}\left(\operatorname{sim}\left(q</em>, m\right)\right)
$$}</p>
<p>where $\operatorname{sim}(\cdot, \cdot)$ is the cosine similarity of semantic embeddings. $M_{c}^{(i)}$ are the $i$-th cluster's candidates and contain $k$ memories.</p>
<p>Then we further let the LLM select the most helpful memory from each cluster as follows:</p>
<p>$$
m^{(i)}=\operatorname{LLM}\left(q_{\text {test }}, M_{c}^{(i)}, P_{\text {retrieval }}\right)
$$</p>
<p>where $P_{\text {retrieval }}$ is the prompt for the LLM to retrieve helpful memory. We concatenate the test question $q_{\text {test }}$, memory candidates $M_{c}^{(i)}$ and $P_{\text {retrieval }}$ by a specialized template. The resulting input for LLM is like: "References: $\left[M_{c, 1}^{(i)}, M_{c, 2}^{(i)} \cdots, M_{c, k}^{(i)}\right]$ Target Question: [ $q_{\text {test }}$ ] which one reference would be the most helpful for you to answer the target question?".</p>
<p>In this manner, we can utilize the LLM's powerful natural language understanding ability to select the most helpful memory of $M_{c}^{(i)}$ for itself. Since these retrieved memories are from diverse memory clusters, they can be not only helpful for $q_{\text {test }}$ but also comprehensive, thus facilitating the LLM to answer the test question. Meanwhile, the semantical-filtering can filter out semantically irrelevant memories in advance, thus significantly helps save the number of LLM calls.</p>
<p>In exploratory experiments, we find that providing only memory candidates' questions for LLMretrieval almost does not affect the retrieval result. Hence, for each $M_{c}^{(i)}$, we only provide its question for the LLM to select, which can significantly save the inference cost of the LLM. To make LLM better understand the goal of retrieving helpful memory and make its output easy-parsing with a pre-defined format, we append extra instructions like "You must end in the format like "The most helpful question is question [idx]." to the input. We show the complete input of LLM-retrieval in Appendix A.</p>
<h3>3.3 Inference</h3>
<p>Given a test question $q_{\text {test }}$, the LLM can think and then output the answer based on the retrieved memory, $m^{(1)}, m^{(2)} \cdots m^{(k)}$. Specifically, we let the LLM reason in the manner of Few-Shot-CoT:</p>
<p>$$
\begin{aligned}
&amp; s=\operatorname{LLM}\left(m^{(1)}, m^{(2)}, \cdots, m^{(k)}, q_{\text {test }}\right) \
&amp; a=\text { Parse-Answer }(s)
\end{aligned}
$$</p>
<p>In this paper, we focus on the overall framework and instantiate it with simple components, i.e., Few-Shot-CoT, simple uncertainty filtering and LLMretrieval. We further analyze the orthogonality of MoT and different CoT methods in section 4.3. We leave exploring more implementations, e.g., letting the LLM itself filter out uncertain thoughts (Weng et al., 2022; Long, 2023), as the future work.</p>
<h2>4 Experiment</h2>
<h3>4.1 Experimental Settings</h3>
<p>Dataset We conduct experiments on ten datasets, across four task families: Arithmetic reasoning: AQuA (Ling et al., 2017) and DROP (Dua et al., 2019); Natural Language Inference: Adversarial NLI subsets (Nie et al., 2020), including ANLI-A1, ANLI-A2 and ANLI-A3, which cover varying difficulty levels; Commonsense Reasoning: OBQA (Mihaylov et al., 2018) and ComV (Commonsense Validation) (Wang et al., 2019); Factual Reasoning: BoolQ (Clark et al., 2019), FactCK (Fact Checker) and WikiQA (Srivastava et al., 2022). We list dataset overview, statistics, split and evaluation metrics in Appendix B.
Method Comparison Since we focus on whether MoT can help the LLM self-improve, we compare MoT with baselines on the same LLM, ChatGPT (GPT-3.5-Turbo-0301), including zero-shot/fewshot CoT and zero-shot/few-shot direct prompting. To analyze the effect of rationales and thinking in MoT, we additionally compare MoT with its two variants: 1) MoT (no rationale), which removes rationales in the retrieved memory and thus lets the LLM directly output the answer, which can be seen as the few-shot direct version of MoT; 2) MoT (no thinking), which keeps rationales in the retrieved memory but forces the LLM to directly answer the question without CoT. Specifically, we add "The answer is" as the LLM's output prefix to prompt the LLM directly output the answer. Through these two variants, we can analyze the effect of rationales and the thinking in MoT, respectively. Additionally, we conduct experiments of MoT under annotated datasets, MoT (with gold), to see its potential improvement space, where we use the gold labels to filter out incorrect memory. Thus, MoT will not be degraded by the incorrect answer.</p>
<p>Implementation Details We use the public OpenAI language model of "gpt-3.5-turbo-0301" un-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Arithmetic Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ANLI</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CS Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Factual Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">-A1</td>
<td style="text-align: center;">-A2</td>
<td style="text-align: center;">-A3</td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">ComV</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">FactCK</td>
<td style="text-align: center;">WikiQA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">57.6</td>
</tr>
<tr>
<td style="text-align: center;">MoT (no rationale)</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">62.7</td>
</tr>
<tr>
<td style="text-align: center;">MoT (no thinking)</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">63.1</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot-CoT</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot-CoT</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;">MoT</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: center;">MoT (with gold)</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">71.3</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance comparison on ChatGPT (GPT-3.5-Turbo-0301).
less otherwise specified and the experiments on "text-davinci-002/003" (Appendix C) show consistent trends. For recalling, we use SBERT ("all-mpnet-base-v2") (Reimers and Gurevych, 2019) for semantic filtering. And we set the number of clusters $l$ (also the demonstration quantity) and the number of each cluster's memory candidates $k$ as 4 and 10, respectively. We further analyze the number of demonstrations in Appendix D. In the test stage, for the stability of results, we use greedy decoding to generate the output, unless otherwise specified. We list the full implementation details and few-shot demonstrations in Appendix E.</p>
<h3>4.2 Main Results</h3>
<p>We show the results in Table 1. We see that MoT significantly outperforms baselines on most datasets, which shows MoT's best comprehensive reasoning capability on a series of NLP tasks. Specifically, MoT exceeds Few-Shot-CoT and Zero-Shot-CoT by 3.7 and 9.1 points respectively, and this directly demonstrates that MoT can make the LLM improve itself by memory-of-thoughts, without annotated dataset and parameter updates. Notably, Zero-Shot-CoT shows impressive performance on ChatGPT and outperforms Few-ShotCoT on several datasets, e.g., AQuA, DROP, ANLI1 and ANLI-2, which indicates the potential unnecessary of irrelevant CoT demonstration for the LLM with powerful zero-shot reasoning ability. Meanwhile, MoT surpasses Zero-Shot-CoT consistently on all datasets and this indicates the helpfulness of retrieved memory.</p>
<p>As for MoT's two variants, they also show better overall performance than Zero-Shot and FewShot. Meanwhile, despite directly outputting the answer, they outperform Zero-Shot-CoT and Few-Shot-CoT on several datasets, e.g., OBQA, BoolQ and FactCK. This is analogous to a common phenomenon in human beings: when recalling relevant</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">OBQA</th>
<th style="text-align: left;">BoolQ</th>
<th style="text-align: left;">WikiQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Decoding Paths=8</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT ${ }_{T=0.7}$</td>
<td style="text-align: left;">80.4</td>
<td style="text-align: left;">55.9</td>
<td style="text-align: left;">59.2</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT ${ }_{T=1}$</td>
<td style="text-align: left;">82.4</td>
<td style="text-align: left;">54.1</td>
<td style="text-align: left;">59.2</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT ${ }_{T=1.2}$</td>
<td style="text-align: left;">81.0</td>
<td style="text-align: left;">49.1</td>
<td style="text-align: left;">59.4</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT ${ }_{T=0.7}$</td>
<td style="text-align: left;">82.0</td>
<td style="text-align: left;">69.2</td>
<td style="text-align: left;">69.6</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT ${ }_{T=1}$</td>
<td style="text-align: left;">83.6</td>
<td style="text-align: left;">68.2</td>
<td style="text-align: left;">70.8</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT ${ }_{T=1.2}$</td>
<td style="text-align: left;">82.2</td>
<td style="text-align: left;">70.2</td>
<td style="text-align: left;">70.8</td>
</tr>
<tr>
<td style="text-align: left;">MoT $_{T=0.7}$</td>
<td style="text-align: left;">$\mathbf{8 5 . 0}$</td>
<td style="text-align: left;">73.2</td>
<td style="text-align: left;">$\mathbf{7 3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">MoT $_{T=1}$</td>
<td style="text-align: left;">84.4</td>
<td style="text-align: left;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: left;">72.1</td>
</tr>
<tr>
<td style="text-align: left;">MoT $_{T=1.2}$</td>
<td style="text-align: left;">85.0</td>
<td style="text-align: left;">72.8</td>
<td style="text-align: left;">72.1</td>
</tr>
<tr>
<td style="text-align: left;">Decoding Paths=16</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT ${ }_{T=0.7}$</td>
<td style="text-align: left;">81.6</td>
<td style="text-align: left;">58.0</td>
<td style="text-align: left;">63.8</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT ${ }_{T=1}$</td>
<td style="text-align: left;">83.2</td>
<td style="text-align: left;">55.8</td>
<td style="text-align: left;">65.7</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT ${ }_{T=1.2}$</td>
<td style="text-align: left;">83.4</td>
<td style="text-align: left;">53.8</td>
<td style="text-align: left;">64.3</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT ${ }_{T=0.7}$</td>
<td style="text-align: left;">83.2</td>
<td style="text-align: left;">69.6</td>
<td style="text-align: left;">71.5</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT ${ }_{T=1}$</td>
<td style="text-align: left;">83.6</td>
<td style="text-align: left;">69.2</td>
<td style="text-align: left;">71.7</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT ${ }_{T=1.2}$</td>
<td style="text-align: left;">83.6</td>
<td style="text-align: left;">69.7</td>
<td style="text-align: left;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">MoT $_{T=0.7}$</td>
<td style="text-align: left;">84.4</td>
<td style="text-align: left;">$\mathbf{7 3 . 7}$</td>
<td style="text-align: left;">73.7</td>
</tr>
<tr>
<td style="text-align: left;">MoT $_{T=1}$</td>
<td style="text-align: left;">$\mathbf{8 5 . 0}$</td>
<td style="text-align: left;">72.7</td>
<td style="text-align: left;">$\mathbf{7 4 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">MoT $_{T=1.2}$</td>
<td style="text-align: left;">84.8</td>
<td style="text-align: left;">73.3</td>
<td style="text-align: left;">74.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison under selfconsistency strategy across various hyper-parameters.
memory, we can perform well by intuition, without conscious reasoning (Dijksterhuis and Nordgren, 2006; Todd et al., 1999). Additionally, although MoT (no thinking) is provided with the rationales while MoT (no rationale) is not, they show generally similar performance, which indicates that explicit reasoning is necessary for the LLM to fully leverage the retrieved memory. In short, both relevant memory and explicit reasoning are essential for MoT to consistently achieve improvements on extensive datasets.</p>
<p>Additionally, MoT (with gold) shows better performance than MoT, which indicates the potential improvements when MoT applies more advanced CoT methods (Wang et al., 2023; Zhou et al., 2022; Zheng et al., 2023; Long, 2023) and verification methods (Weng et al., 2022; Manakul et al., 2023).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">DROP</th>
<th style="text-align: left;">ANLI-A3</th>
<th style="text-align: left;">BoolQ</th>
<th style="text-align: left;">WikiQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Few-Shot-COT</td>
<td style="text-align: left;">57.6</td>
<td style="text-align: left;">52.3</td>
<td style="text-align: left;">67.7</td>
<td style="text-align: left;">65.2</td>
</tr>
<tr>
<td style="text-align: left;">MiniLM</td>
<td style="text-align: left;">63.0</td>
<td style="text-align: left;">53.7</td>
<td style="text-align: left;">70.2</td>
<td style="text-align: left;">67.0</td>
</tr>
<tr>
<td style="text-align: left;">Instructor-base</td>
<td style="text-align: left;">64.2</td>
<td style="text-align: left;">53.2</td>
<td style="text-align: left;">70.2</td>
<td style="text-align: left;">66.9</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">57.5</td>
<td style="text-align: left;">52.8</td>
<td style="text-align: left;">69.7</td>
<td style="text-align: left;">66.3</td>
</tr>
<tr>
<td style="text-align: left;">+ MPNet</td>
<td style="text-align: left;">64.7</td>
<td style="text-align: left;">53.3</td>
<td style="text-align: left;">70.4</td>
<td style="text-align: left;">67.1</td>
</tr>
<tr>
<td style="text-align: left;">+ LLM-Retrieval</td>
<td style="text-align: left;">$\mathbf{6 5 . 7}$</td>
<td style="text-align: left;">$\mathbf{5 5 . 2}$</td>
<td style="text-align: left;">$\mathbf{7 1 . 5}$</td>
<td style="text-align: left;">$\mathbf{6 8 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: The comparison of retrieval methods.</p>
<h3>4.3 Analyses</h3>
<p>Multiple-Decoding Performance In this section, we evaluate MoT under self-consistency strategy (Wang et al., 2022b) which decodes multiple times and uses majority-voting to get the final answer. We compare MoT with baselines across varying sampling times and temperatures on OBQA, BoolQ and WikiQA, and the results are shown in Table 2. We see that MoT consistently outperforms Zero-Shot-CoT and Few-Shot-CoT across different decoding temperatures and sampling times, which indicates the generality and stability of MoT. We notice that the improvements slightly diminish when using more sampling times. This is similar to the phenomenon in human beings: the more carefully we think about a question, the less our previous preparation matters.</p>
<p>The Effect of LLM-retrieval To evaluate the effect of LLM-retrieval for MoT, we conduct experiments with varying retrieval methods on DROP, ANLI-A3, BoolQ and WikiQA, shown in Table 3. Besides the SBERT ("all-mpnet-base-v2", abbreviated as MPNet) (Reimers and Gurevych, 2019) used in MoT, we further compare two other semantic embedders, SBERT ("all-MiniLM-L6-v2", abbreviated as MiniLM) (Reimers and Gurevych, 2019) and Instructor-base (Su et al., 2022b) which is trained by 330 diverse tasks and supports various scenarios. We observe that using only MPNet for memory retrieval also brings significant improvements over Few-Shot-CoT, which shows MoT's usability under the limited LLM-API budget. After using the LLM to retrieve memory, the performance gets further improvements, which directly demonstrates the effectiveness of LLM-retrieval. Additionally, we see that LLM-retrieval outperforms all compared semantic embedders, which shows that the LLM can better capture the complicated reasoning logic than semantic embeddings.</p>
<p>The Effect of Filtering To evaluate the effect of memory filtering in MoT, we plot the performance</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The effect of filtering. The far left and far right of the x -axis correspond to no filtering, and the strictest filtering, i.e., only the memory that all paths lead to the same answer can be retained.
curve over different filtering thresholds on DROP, ANLI-A3, OBQA and WikiQA. Specifically, we tune the filtering threshold of answer-entropy uniformly and observe the corresponding performance. The results are shown in Figure 5. We find that the MoT without filtering significantly degrades and slightly underperforms Few-shot-COT on some datasets, e.g., OBQA and FactCK, which indicates that the incorrect memory can deteriorate the LLM's reasoning and thus our filtering strategy is necessary. Meanwhile, most filtering thresholds consistently lead to improvements over baselines, which demonstrates that the improvements of MoT exhibit insensitivity to the hyper-parameter of filtering thresholds in general.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Limited Memory-Size Performance.</p>
<p>Limited Memory-Size Performance In realworld scenarios, the number of unlabeled examples or the size of available external memory space may be limited, and these can both lead to the limited memory-size. In this section, we evaluate MoT under different memory sizes. Specifically, we conduct experiments on the randomly sampled subsets with different proportions and plot the corresponding performance curve in Figure 6. We observe that MoT can consistently lead to performance improvements. Even under 10 percent of the original memory pool, MoT can still outperform Zero-ShotCoT and Few-Shot-CoT. These show the usability of MoT when unlabeled examples or available ex-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">DROP</th>
<th style="text-align: center;">ANLI-A1</th>
<th style="text-align: center;">ANLI-A3</th>
<th style="text-align: center;">OBQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Few-Shot-COT</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: left;">+MoT</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">82.3</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-COT</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: left;">+MoT</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: left;">Plan-and-Solve</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">+MoT</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">85.6</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of various CoT methods.
ternal memory space are limited.</p>
<p>Transferability across Different COT Methods In this section, we evaluate the performance of MoT on two additional COT methods: Zero-Shot-COT (Kojima et al., 2022) and Plan-andSolve (Wang et al., 2023). Compared with Zero-Shot-COT which uses "Let's think step by step" to elicit LLM's reasoning, Plan-and-Solve uses a specialized prompt to let the LLM first devise a plan to divide the entire task into sub-tasks and then solve them based on the plan, and thus it can accomplish more complicated reasoning (Wang et al., 2023). For these two CoT methods, we use them to generate the pool of memory-of-thoughts at prethinking stage, respectively. At the test stage, we retrieve thoughts from the corresponding memory pool, concatenate them with the test question, and then use the corresponding prompt, e.g., "Let's think step by step" for Zero-Shot-COT, to elicit the LLM's reasoning. Results on DROP, ANLI-A1, ANLI-A3 and OBQA are shown in Table 4. We observe that MoT leads to consistent improvements, which shows its stability and generality across various CoT methods. Moreover, since these two CoT methods do not rely on manual CoT demonstrations, these results also demonstrate the effectiveness of MoT when the manual CoT demonstration is not available. Meanwhile, when using the more advanced CoT method, Plan-and-Solve, MoT's performance gets further improvements, which shows its potential in the future where the more powerful CoT method is proposed.</p>
<h2>5 Related Work</h2>
<p>Model Augmentation by LLM-generated Data In this section, we introduce previous methods that use the data generated by LLMs for model augmentation. Ye et al. (2022a); Gao et al. (2022); Ye et al. (2022b) propose ZeroGen, ProGen and ZeroGen ${ }^{+}$ to use the LLM to generate the dataset to enhance small models, e.g., LSTM. Fu et al. (2023a); Mag-
ister et al. (2022); Ho et al. (2022) leverage LLM to generate reasoning paths and teach small LMs to reason. Wang et al. (2022c) and Honovich et al. (2022) leverage the LLM to generate instruction data and improve the instruction-following capability of the LLM. Schick et al. (2023) propose ToolFormer, which learns how to use various tools by self-generated data. Zelikman et al. (2022) and Huang et al. (2022) leverage the LLM to generate reasoning paths and improve itself using labeled and unlabeled datasets, respectively. Different from these methods that depend on expensive fine-tuning, MoT can make the LLM selfimprove with memory-of-thoughts and does not depend on parameter updates and is compatible with API-accessing LLM. Recently, Zhang et al. (2022b); Shao et al. (2023) automatically generate COT demonstrations by the LLM itself. Li et al. (2022) leverage the LLM to generate the knowledge base and improve its ability of open-domain QA. These methods can be seen as the specialized case of MoT, with task-level memory selection or task-specialized memory building.</p>
<p>Demonstration Retrieval for LLM In this section, we introduce previous demonstration retrieval methods for ICL, which mainly retrieve relevant input/output pairs, from an annotated dataset, for the LM to predict the test example. Liu et al. (2022) propose to leverage a dense semantic embedder to retrieve relevant examples to improve ICL. Agrawal et al. (2022) leverage BM25 to retrieve examples for machine translation's ICL. Das et al. (2021) and Hu et al. (2022) design specialized target similarities to train demonstration retrievers on ICL of knowledge-based question answering and dialogue state tracking respectively. Rubin et al. (2022); Shi et al. (2022b) use the LM's feedback to train the demonstration retriever for semantic parsing. Lyu et al. (2022) retrieve relevant examples with random labels and propose heuristic methods to reduce the negative effect of false labels. Recently, Li et al. (2023) propose UDR, a unified demonstration retriever for various NLP tasks, which is trained by the unified LM-feedback on about 40 annotated datasets. While most of these methods depend on high-quality annotated datasets and only explore in-context learning without rationales, MoT can make the LLM self-improve without annotated datasets and parameter updates, and to the best of our knowledge, we are the first to explore demonstration retrieval in the challenging</p>
<p>and complicated reasoning scenarios and demonstrate MoT's effectiveness.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we propose MoT, a framework that let the LLM self-improve via Memory-of-Thought, without annotated datasets and parameter updates. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.</p>
<h2>Limitations</h2>
<p>MoT mainly has the following limitations:</p>
<ul>
<li>Although we propose the answer-entropy to filter out uncertain thoughts, the remaining thoughts can still contain certain mistakes. We will explore more methods of false thought filtering (Lin et al., 2023) in the future.</li>
<li>In this paper, we employ a simple strategy to utilize the relevant memory, i.e., concatenate it with the test input $q_{\text {test }}$ and thus help the LLM answer $q_{\text {test }}$. We will explore more strategies to utilize the retrieved memory, e.g., retrieving the memory to verify the current reasoning path for $q_{\text {test }}$.</li>
<li>On the one hand, in this paper, we make the first step to let the LLM self-improve based on the memory mechanism. The conducted experiments are still in a safe setting, i.e., a specific unlabeled dataset, and the LLM cannot access the internet and control external tools. Hence we think our method and experiment are still safe enough, which will not cause serious impact and unrecoverable consequences on society. On the other hand, large language models have shown various kinds of bias (Bender et al., 2021). Since we let the LLM generate thoughts/memory to help itself, the LLM might suffer from the generated biased content. We see LLM debias as an important future research topic.</li>
</ul>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Natural Science Foundation of China (No. 62236004 and No. 62022027).</p>
<h2>References</h2>
<p>Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2022. Incontext examples selection for machine translation. CoRR, abs/2212.02437.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023. Palm 2 technical report. CoRR, abs/2305.10403.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, pages 610-623. ACM.</p>
<p>Zekai Chen, Mariann Micsinai Balan, and Kevin Brown. 2023. Language models are few-shot learners for prognostic prediction. CoRR, abs/2302.12692.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,</p>
<p>Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021. Casebased reasoning for natural language queries over knowledge bases. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 9594-9611. Association for Computational Linguistics.
A. J. Dijksterhuis and Loran F Nordgren. 2006. A theory of unconscious thought. Perspectives on Psychological Science, 1:109 - 95.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2022. A survey for in-context learning.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association for Computational Linguistics.
J. Dunlosky and J. Metcalfe. 2008. Metacognition. SAGE Publications.</p>
<p>Stan Franklin, Bernard Baars, Uma Ramamurthy, and Matthew Ventura. 2005. The role of consciousness in memory. Brains, Minds Media, 1.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023a. Specializing smaller language models towards multi-step reasoning. CoRR, abs/2301.12726.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023b. Specializing smaller language models towards multi-step reasoning. CoRR, abs/2301.12726.</p>
<p>Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, Zhiyong Wu, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. 2022. Zerogen ${ }^{\circledR}$ : Self-guided highquality data generation in efficient zero-shot learning. CoRR, abs/2205.12679.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. CoRR, abs/2212.10071.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. CoRR, abs/2203.15556.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. CoRR, abs/2212.09689.</p>
<p>Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, and Mari Ostendorf. 2022. Incontext learning for few-shot dialogue state tracking. CoRR, abs/2203.08568.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. CoRR, abs/2210.11610.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. CoRR, abs/2212.10403.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. CoRR, abs/2205.11916.</p>
<p>Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory W. Mathewson, Mh Tessler, Antonia Creswell, James L. McClelland, Jane Wang, and Felix Hill. 2022. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022,</p>
<p>pages 537-563. Association for Computational Linguistics.</p>
<p>Itay Levy, Ben Bogin, and Jonathan Berant. 2022. Diverse demonstrations improve in-context compositional generalization. CoRR, abs/2212.06800.</p>
<p>Junlong Li, Zhuosheng Zhang, and Hai Zhao. 2022. Self-prompting large language models for opendomain QA. CoRR, abs/2212.08635.</p>
<p>Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for incontext learning. CoRR, abs/2305.04320.</p>
<p>Xiaonan Li and Xipeng Qiu. 2023. Finding supporting examples for in-context learning. CoRR, abs/2302.13539.</p>
<p>Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. CoRR, abs/2305.19187.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 158-167. Association for Computational Linguistics.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, pages 100-114. Association for Computational Linguistics.</p>
<p>Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 6035-6044. Association for Computational Linguistics.</p>
<p>Jieyi Long. 2023. Large language model guided tree-ofthought. CoRR, abs/2305.08291.</p>
<p>Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. Z-ICL: zero-shot incontext learning with pseudo-demonstrations. CoRR, abs/2212.09865.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adámek, Eric Malmi, and Aliaksei Severyn. 2022. Teaching small language models to reason. CoRR, abs/2212.08410.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. CoRR, abs/2303.08896.</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey. CoRR, abs/2302.07842.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. CoRR, abs/2210.03350.</p>
<p>Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language processing: A survey. CoRR, abs/2003.08271.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 3980-3990. Association for Computational Linguistics.</p>
<p>Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333-389.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of</p>
<p>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 2655-2671. Association for Computational Linguistics.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Daniel L. Schacter and Donna Rose Addis. 2007. The cognitive neuroscience of constructive memory: Remembering the past and imagining the future. In Jon Driver, Patrick Haggard, and Tim Shallice, editors, Mental Processes in the Human Brain. Oxford University Press.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761.</p>
<p>Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. CoRR, abs/2302.00618.</p>
<p>Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022a. XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing. CoRR, abs/2210.13693.</p>
<p>Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022b. XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing. CoRR, abs/2210.13693.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen,</p>
<p>Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615.</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022a. Selective annotation makes language models better few-shot learners. CoRR, abs/2209.01975.</p>
<p>Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022b. One embedder, any task: Instruction-finetuned text embeddings. CoRR, abs/2212.09741.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. CoRR, abs/2205.05131.</p>
<p>Peter Todd, Jean Ortega, Jennifer Davis, Gerd Gigerenzer, Daniel Goldstein, Adam Goodie, Ralph Hertwig, Ulrich Hoffrage, Kathryn Laskey, Laura Martignon, and Geoffrey Miller. 1999. Simple Heuristics That Make Us Smart.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.</p>
<p>Endel Tulving. 2002. Episodic memory: From mind to brain. Annual review of psychology, 53:1-25.</p>
<p>Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? a pilot study for sense making and explanation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4020-4026, Florence, Italy. Association for Computational Linguistics.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. CoRR, abs/2305.04091.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2022a. Rationaleaugmented ensembles in language models. CoRR, abs/2207.00747.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2022b. Selfconsistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022c. Self-instruct: Aligning language model with self generated instructions. CoRR, abs/2212.10560.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022b. Emergent abilities of large language models. CoRR, abs/2206.07682.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022c. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. 2022. Large language models are reasoners with self-verification. CoRR, abs/2212.09561.</p>
<p>Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 2246-2251. Association for Computational Linguistics.</p>
<p>Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong. 2022a. Zerogen: Efficient zero-shot learning via dataset generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11653-11669. Association for Computational Linguistics.</p>
<p>Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2022b. Progen: Progressive zero-shot dataset generation via in-context feedback. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 3671-3683. Association for Computational Linguistics.</p>
<p>Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. CoRR, abs/2302.05698.</p>
<p>Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter:</p>
<p>A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2422-2437. Association for Computational Linguistics.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. CoRR, abs/2203.14465.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022a. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022b. Automatic chain of thought prompting in large language models. CoRR, abs/2210.03493.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR, abs/2303.18223.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023. Progressive-hint prompting improves reasoning in large language models. CoRR, abs/2304.09797.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed H. Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. CoRR, abs/2205.10625.</p>
<h2>A The Example of LLM-Retrieval</h2>
<p>We show the LLM-retrieval example in Table 6.</p>
<h2>B Dataset Details</h2>
<p>Overview We conduct experiments on ten datasets, including four task families:</p>
<ul>
<li>Arithmetic reasoning: AQuA (Ling et al., 2017): A multi-choice dataset of arithmetic questions covering various topics and difficulty levels, and DROP (Dua et al., 2019): A reading comprehension dataset that needs discrete reasoning;</li>
<li>Natural Language Inference: Adversarial NLI subsets (Nie et al., 2020), including ANLI-A1, ANLI-A2 and ANLI-A3, which cover varying difficulty levels respectively;</li>
<li>Commonsense Reasoning: OBQA (OpenBookQA) (Mihaylov et al., 2018): Commonsense-related questions which require the facts and their applications to novel situations, and ComV (Commonsense Validation) (Wang et al., 2019): A dataset that requires for identifying the sentence that does not make sense from two sentences of similar wording;</li>
<li>Factual Reasoning: BoolQ (Clark et al., 2019), FactCK (Fact Checker) (Srivastava et al., 2022): A dataset that tests the ability to evaluate the authenticity of factual claims covering Wikipedia, COVID-19 and Politics. WikiQA (Srivastava et al., 2022): question answering fron randomly-sampled Wikidata fact triples.</li>
</ul>
<p>Split, Evaluation Metric and Statistics For AQuA, DROP, ANLI-A1, ANLI-A2, ANLI-A3, ComV and OBQA, we use their official test set for evaluation. For BoolQ, we follow Wang et al. (2022a) to use the validation set for evaluation, since its test set is not publicly available. For FactCK and WikiQA, we manually split them into a train/test split, and use the questions of the training set as unlabeled dataset, since there is not split version of them released. Limited by the budget, for the DROP dataset, we only use the half of its unlabeled dataset (the questions of training set) for the LLM to pre-think. For the classification or multi-choice datasets, we use the accuracy as evaluation metric. For the abstractive QA dataset
including DROP and WikiQA, we use the F1-score as evaluation metric. For DROP, since its one test example has multiple annoated answer, we follow its original paper (Dua et al., 2019) to take a max over all annotated answers. Limited by budget, for those evaluation datasets that are larger than 1000, we randomly sample a subset of 1000 examples for evaluation. We list the overall dataset satistiscs, the size of memory after filtering and evaluation metrics in Table 7.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: The impact of demonstration quantity.</p>
<h2>C Performance on Different LLMs</h2>
<p>We conduct experiments on Text-Davinci-002 and Text-Davinci-003 (Chen et al., 2023; Ouyang et al., 2022) to evaluate MoT's generality across different LLMs. We show the results in Table 5. We observe that MoT consistently outperforms baselines on these two LLMs, which shows the effectiveness of MoT does not rely on one specific LLM and it can bring further improvements in the future where the more strong LLM is proposed.</p>
<h2>D The Impact of Demonstration Quantity</h2>
<p>We compare MoT and Few-Shot-CoT under varying numbers of demonstrations and the results are shown in Figure 7. We see that MoT consistently outperforms Few-Shot-CoT across varying amounts of demonstrations, which shows the stability of MoT. Additionally, the results show that the demonstrations in retrieved memory are more helpful and informative than manual demonstrations in Few-Shot-CoT: specifically, with 1 or 2 demonstrations, MoT can outperform Few-Shot-CoT with 4 demonstrations on OBQA and DROP.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ANLI-A3</th>
<th style="text-align: center;">OBQA</th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;">FactCK</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Text-Davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">52.2</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">83.6</td>
</tr>
<tr>
<td style="text-align: left;">MoT</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">86.2</td>
</tr>
<tr>
<td style="text-align: left;">Text-Davinci-003</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">84.0</td>
</tr>
<tr>
<td style="text-align: left;">MoT</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">87.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance comparison on various LLMs.</p>
<h2>E Implementation Details</h2>
<p>We use the public OpenAI language model of "gpt-3.5-turbo-0301" unless otherwise specified and the experiments (Appendix C) on "text-davinci002/003" show consistent improvements. Due to the limitation of LLM-API budget, we heuristically set the hyper-parameters in the pre-thinking stage, including the generation temperature $T$, the number of decoded reasoning paths $n$. For the filtering threshold $\tau$ and the number of memory clusters $l$, we conduct exploratory experiments on OBQA and find that $\tau={0.2,0.3,0.4}$ and $l={3,4,5}$ lead to similar performance. Thus we set $\tau=0.3$ and $l=4$, respectively. For pre-thinking, we use the temperature $T=1.2$ to encourage more diverse reasoning paths, and use $n=16$ reasoning path sampling times, unless otherwise specified. For memory recall, we use SBERT ("all-mpnet-basev2") (Reimers and Gurevych, 2019) for semantic filtering. Limited by the LLM's max input length, we fix the the number of each cluster's memory candidates as 10 for each dataset. In the test stage, for the stability of results, we use greedy decoding to generate the output, unless otherwise specified. For simplicity, we separately run MoT on each dataset and regard cross-dataset memory recall as future work. Baselines' points are from our implementation, and share the same templates, answer parsing and evaluation as MoT.</p>
<p>For AQuA, OpenBookQA, BoolQ, DROP, ANLI-A1, ANLI-A2 and ANLI-A3, We use the same few-shot CoT examples as those in Wei et al. (2022c), Zhou et al. (2022), Wang et al. (2022b) and Wang et al. (2022a), respectively. For the left datasets that have no publicly released manual CoT demonstrations, we randomly select questions from the training set and use ChatGPT to generate reasoning paths and get their few-shot CoT examples. We list the used Few-Shot-CoT examples in Ta-
ble $8,9,10,12,11,13,14$ and 15 .</p>
<h2>F Ethics Statement</h2>
<p>In this paper we make the first step to let the LLM self-improve based on the memory mechanism. The conducted experiments are still in a safe setting, i.e., a specific unlabeled dataset, and the LLM cannot access the internet and control external tools. Hence we think our method and experiment are still safe enough, which will not cause serious impact and unrecoverable consequences on society.</p>
<h1>LLM Input</h1>
<p>I will provide you with a target question and 10 reference questions. I need you to choose a reference question from "Reference Questions", whose question, train of thought or answer would be most helpful for you to answer the target question. Please note that the following reference QA pairs are presented in a random order without any prioritization.</p>
<h2>Target Question:</h2>
<p>Machine A puts out a yo-yo every 6 minutes. Machine B puts out a yo-yo every 9 minutes. After how many minutes will they have produced 10 yo-yos? Answer Choices: (A) 24 minutes (B) 32 minutes (C) 36 minutes (D) 64 minutes (E) 72 minutes</p>
<h2>Reference Questions:</h2>
<p>1 .
Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 2 items in the same time Machine Z produces 2 items. If machine Y takes 9 minutes to produce a batch of items, how many minutes does it take for machine $Z$ to produce the same number of items? Answer Choices: (A) 6 (B) 9 (C) $91 / 2$ (D) 22.5 (E) $131 / 2$
2 .
Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 30 items in the same time Machine Z produces 38 items. If machine Y takes 19 minutes to produce a batch of items, how many minutes does it take for machine $Z$ to produce the same number of items? Answer Choices: (A) 6 (B) 9 (C) $91 / 2$ (D) 15 (E) $131 / 2$
3 .
Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 30 items in the same time Machine Z produces 24 items. If machine Y takes 36 minutes to produce a batch of items, how many minutes does it take for machine $Z$ to produce the same number of items? Answer Choices: (A) 60 (B) 90 (C) $91 / 2$ (D) 45 (E) $131 / 2$
4 .
Q: Working alone at its constant rate, machine A produces x boxes in 10 minutes and working alone at its constant rate, machine B produces 2 x boxes in 5 minutes. How many minutes does it take machines A and B, working simultaneously at their respective constant rates, to produce 10x boxes? Answer Choices: (A) 13 minutes (B) 14 minutes (C) 15 minutes (D) 16 minutes (E) 20 minutes
5 .
Q: Two machines, Y and Z, work at constant rates producing identical items. Machine Y produces 23 items in the same time Machine Z produces 21 items. If machine Y takes 21 minutes to produce a batch of items, how many minutes does it take for machine $Z$ to produce the same number of items? Answer Choices: (A) 6 (B) 9 (C) $91 / 2$ (D) 12 (E) 23
6 .
Q: Machines X and Y produce bottles at their respective constant rates. Machine X produces k bottles in 6 hours and machine Y produces k bottles in 12 hours. How many hours does it take machines X and Y , working simultaneously, to produce 12 k bottles? Answer Choices: (A) 8 (B) 12 (C) 15 (D) 48 (E) 24
7 .
Q: Machines X and Y produce bottles at their respective constant rates. Machine X produces k bottles in 6 hours and machine Y produces k bottles in 3 hours. How many hours does it take machines X and Y , working simultaneously, to produce 12 k bottles? Answer Choices: (A) 4 (B) 8 (C) 12 (D) 18 (E) 4
8 .
Q: Machines X and Y produce bottles at their respective constant rates. Machine X produces k bottles in 4 hours and machine Y produces k bottles in 5 hours. How many hours does it take machines X and Y , working simultaneously, to produce 10k bottles? Answer Choices: (A) $82 / 3$ (B) $125 / 3$ (C) 15 (D) 18 (E) $222 / 9$
9 .
Q: Working alone at its constant rate, machine A produces x boxes in 10 minutes and working alone at its constant rate, machine B produces 2 x boxes in 5 minutes. How many minutes does it take machines A and B, working simultaneously at their respective constant rates, to produce 6x boxes? Answer Choices: (A) 3 minutes (B) 4 minutes (C) 5 minutes (D) 6 minutes (E) 12 minutes
10 .
Q: Machine A can make 350 widgets in 1 hour, and machine B can make 250 widgets in 1 hour. If both machines work together, how much time will it take them to make a total of 900 widgets? Answer Choices: (A) 1 hour and 20 minutes (B) 1 hour and 24 minutes (C) 1 hour and 30 minutes (D) 1 hour and 36 minutes (E) 1 hour and 40 minutes</p>
<p>Which one of the above reference questions is the most helpful question for you to answer the target question? You must choose exactly one reference question to you answer the target question. Your response must end in this format: "The most helpful question is question [index].". For example, if question 5 is your answer, you must end in "The most helpful question is question 5."</p>
<h2>LLM output</h2>
<p>The most helpful question is question 10 .
Table 6: The example of LLM-Retrieval.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Family</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Task Format</th>
<th style="text-align: center;">Unlabeled Questions</th>
<th style="text-align: center;">Test Questions</th>
<th style="text-align: center;">Memory Size</th>
<th style="text-align: center;">Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Arithmetic Reasoning</td>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">Multi Choice</td>
<td style="text-align: center;">97467</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">19334</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">Abstractive QA</td>
<td style="text-align: center;">42777</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">17066</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">NLI</td>
<td style="text-align: center;">ANLI-A1</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">16946</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">9721</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI-A2</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">45460</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">25444</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI-A3</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">100459</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">53402</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">ComV</td>
<td style="text-align: center;">Multi Choice</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">9003</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">Multi Choice</td>
<td style="text-align: center;">4957</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">3559</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">Factual</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">9427</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">6422</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">FactCK</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">6654</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">4782</td>
<td style="text-align: center;">Accuracy</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WikiQA</td>
<td style="text-align: center;">Abstractive QA</td>
<td style="text-align: center;">19442</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">9041</td>
<td style="text-align: center;">F1</td>
</tr>
</tbody>
</table>
<p>Table 7: The statistics of used datasets and their evaluation metrics.</p>
<p>Q: John found that the average of 15 numbers is 40 . If 10 is added to each number then the mean of the numbers is? Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64
A: If 10 is added to each number, then the mean of the numbers also increases by 10 . So the new mean would be 50. The answer is (A).</p>
<p>Q: If a / b = 3/4 and $8 \mathrm{a}+5 \mathrm{~b}=22$, then find the value of a. Answer Choices: (A) $1 / 2$ (B) $3 / 2$ (C) $5 / 2$ (D) $4 / 2$ (E) $7 / 2$
A: If a / b = 3/4, then b = 4a / 3. So $8 \mathrm{a}+5(4 \mathrm{a} / 3)=22$. This simplifies to $8 \mathrm{a}+20 \mathrm{a} / 3=22$, which means $44 \mathrm{a} / 3$ $=22$. So a is equal to $3 / 2$. The answer is (B).
Q: A person is traveling at $20 \mathrm{~km} / \mathrm{hr}$ and reached his destiny in 2.5 hr then find the distance? Answer Choices: (A) 53 km (B) 55 km (C) 52 km (D) 60 km (E) 50 km</p>
<p>A: The distance that the person traveled would have been $20 \mathrm{~km} / \mathrm{hr} * 2.5 \mathrm{hrs}=50 \mathrm{~km}$. The answer is (E).
Q: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (A) 1156 (B) 1392 (C) 1480 (D) 1562 (E) 1788</p>
<p>A: There are 9 one-digit numbers from 1 to 9 . There are 90 two-digit numbers from 10 to 99 . There are 401 three-digit numbers from 100 to $500.9+90(2)+401(3)=1392$. The answer is (B).</p>
<p>Table 8: Few-shot CoT prompts for AQuA, from Wang et al. (2022b).</p>
<p>Q: The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. How many yards do the shortest touchdown run and the longest touchdown pass combine for?
A: All the touchdown runs are: a 27 -yard touchdown run, a 9 -yard touchdown run, a 11 -yard touchdown run. The smallest number among $27,9,11$ is 9 . So the shortest touchdown run was 9 yards. All the touchdown passes are: a 12 -yard touchdown pass. So the longest touchdown pass was 12 yards. So the shortest touchdown run and the longest touchdown pass combine for $9+12=21$ yards. So the answer is 21 yards.</p>
<p>Q: In the county, the population was spread out with $23.50 \%$ under the age of $18,8.70 \%$ from 18 to $24,29.70 \%$ from 25 to $44,24.70 \%$ from 45 to 64 , and $13.30 \%$ who were 65 years of age or older. How many more percent are under the age of 18 compared to the 18 to 24 group?
A: According to the passage, $23.5 \%$ are under the age of 18 , and $8.7 \%$ are from ages 18 to $24.23 .5 \%-8.7 \%=$ $14.8 \%$. So the answer is 14.8 .</p>
<p>Q: Since the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North American Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade Agreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European Coal and Steel Community in 1951 which became the European Economic Community in 1958. Two core objectives of the EEC were the development of a common market, subsequently renamed the single market, and establishing a customs union between its member states. How many years did the European Coal and Steel Community exist?
A: According to the passage, the European Coal and Steel Community was established in 1951 and became the EEC in 1958. 1958 - $1951=7$. So the answer is 7 .</p>
<p>Q: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first quarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard TD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42 -yard touchdown pass to wide receiver Steve Smith. How many scoreless quarters were there?
A: The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is 2 .
Table 9: Few-shot CoT examples for DROP, from Zhou et al. (2022).</p>
<p>Premise:
"Conceptually cream skimming has two basic dimensions - product and geography."
Based on this premise, can we conclude the hypothesis "Product and geography are what make cream skimming work." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: Based on "cream skimming has two basic dimensions" we can't infer that these two dimensions are what make cream skimming work. The answer is it is not possible to tell.</p>
<h1>Premise:</h1>
<p>"One of our member will carry out your instructions minutely."
Based on this premise, can we conclude the hypothesis "A member of my team will execute your orders with immense precision." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: "one of" means the same as "a member of", "carry out" means the same as "execute", and "minutely" means the same as "immense precision". The answer is yes.</p>
<h2>Premise:</h2>
<p>"Fun for adults and children."
Based on this premise, can we conclude the hypothesis "Fun for only children." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: "adults and children" contradicts "only children". The answer is no.
Premise:
"He turned and smiled at Vrenna."
Based on this premise, can we conclude the hypothesis "He smiled at Vrenna who was walking slowly behind him with her mother." is true?
OPTIONS:</p>
<ul>
<li>yes</li>
<li>no</li>
<li>it is not possible to tell</li>
</ul>
<p>A: the premise does not say anything about "Vrenna was walking". The answer is it is not possible to tell.
Table 10: Few-shot CoT prompts for NLI tasks, three subsets of ANLI from Wang et al. (2022a).</p>
<p>Q: Poison causes harm to which of the following? (A) a Tree (B) a robot (C) a house (D) a car
A: Poison will harm living things, only a tree is a living thing. The answer is (A).
Q: As you look deeper into a Marbel you can see (A) the future (B) minut defects (C) colors (D) the other side A: Marbel is not transparent, so you can not see the other side. Marbel does not necessarily have multiple colors. You will see minut defects. The answer is (B).</p>
<p>Q: When food is reduced in the stomach (A) the mind needs time to digest (B) take a second to digest what I said (C) nutrients are being deconstructed (D) reader's digest is a body of works
A: The food is being deconstructed in the stomach during digestion. The answer is (C).
Q: The sun is responsible for (A) puppies learning new tricks (B) children growing up and getting old (C) flowers wilting in a vase (D) plants sprouting, blooming and wilting
A: The sun can affect the growing of living things, like plants. The answer is (D).
Table 11: Few-shot CoT examples for OBQA, from Wang et al. (2022a).</p>
<p>Q: Which one of the following statements is against common sense? (A) Roses buds eat caterpillars (B) The caterpillar eats the rose bud
A: Statement (A) is against common sense as it goes against the natural food chain and the known behavior of roses. Roses are plants and cannot eat or consume other organisms, including caterpillars. The answer is A.</p>
<p>Q: Which one of the following statements is against common sense? (A) He threw his house into the trash bin (B) He threw his food waste into the trash</p>
<p>A: It is not physically possible to throw a house into a trash bin. Statement (A) goes against the laws of physics and is therefore illogical. The answer is A.</p>
<p>Q: Which one of the following statements is against common sense? (A) Because his car was damaged, he received RMB 1000 from electricity company (B) Because his car was damaged, he received RMB 1000 from insurance company
A: It does not make logical sense for an electricity company to compensate someone for car damage. It is more reasonable for an insurance company to provide compensation for car damage. The answer is A.</p>
<p>Q: Which one of the following statements is against common sense? (A) Because his car was damaged, he received RMB 1000 from electricity company (B) Because his car was damaged, he received RMB 1000 from insurance company
A: It does not make logical sense for an electricity company to compensate someone for car damage. It is more reasonable for an insurance company to provide compensation for car damage. The answer is A.</p>
<p>Table 12: Few-shot CoT examples for ComV.</p>
<p>Q: does system of a down have 2 singers?
A: System of a Down currently consists of Serj Tankian, Daron Malakian, Shavo Odadjian and John Dolmayan. Serj and Daron do vocals, so the band does have two singers. The answer is yes.</p>
<p>Q: do iran and afghanistan speak the same language?
A: Iran and Afghanistan both speak the Indo-European language Persian. The answer is yes.
Q: is a cello and a bass the same thing?
A: The cello is played sitting down with the instrument between the knees, whereas the double bass is played standing or sitting on a stool. The answer is no.
Q: can you use oyster card at epsom station?
A: Epsom railway station serves the town of Epsom in Surrey and is not in the London Oyster card zone. The answer is no.</p>
<p>Table 13: Few-shot CoT examples for BoolQ, from Wang et al. (2022a).</p>
<p>Q: On June 2017, the following claim was made: David Lloyd George lost every bid to become prime minister. Was this claim true or false?
A: David Lloyd George served as the Prime Minister of the United Kingdom from 1916 to 1922. He also served as the Chancellor of the Exchequer and the Minister of Munitions before becoming Prime Minister. Therefore, the claim that he lost every bid to become Prime Minister is false. The answer is false.</p>
<p>Q: On June 2017, the following claim was made: In 1966, George Harrison got married for the first time. Was this claim true or false?
A: George Harrison married his first wife, model Pattie Boyd, on January 21, 1966. The answer is true.
Q: On June 2017, the following claim was made: Woodrow Wilson did not live during World War I. Was this claim true or false?
A: Woodrow Wilson was the President of the United States during World War I, serving from 1913 to 1921. The answer is false.</p>
<p>Q: On April 17 2008, the following claim was made: Hillary Clinton has taken over \$800,000 from lobbyists. Was this claim true or false?
A: According to OpenSecrets.org, a nonpartisan research group that tracks money in politics, Hillary Clinton received over $\$ 800,000$ in campaign contributions from lobbyists during her 2008 presidential campaign. The answer is true.</p>
<p>Table 14: Few-shot CoT examples for Fact Checker.</p>
<p>Q: On June 2017, the following claim was made: David Lloyd George lost every bid to become prime minister. Was this claim true or false?
A: David Lloyd George served as the Prime Minister of the United Kingdom from 1916 to 1922. He also served as the Chancellor of the Exchequer and the Minister of Munitions before becoming Prime Minister. Therefore, the claim that he lost every bid to become Prime Minister is false. The answer is false.</p>
<p>Q: The native language of Aaron Swartz is?
A: Aaron Swartz was born in Chicago, Illinois, United States. Therefore, his native language is most likely English, as it is the primary language spoken in the United States. The answer is English.</p>
<p>Q: The religion of Prajadhipok is?
A: Prajadhipok was a Buddhist, as Buddhism is the predominant religion in Thailand, where he was the last absolute monarch before the country became a constitutional monarchy. The answer is Buddhism.</p>
<p>Q: The country of Valletta is?
A: Valletta is the capital city of Malta, which is a small island nation located in the Mediterranean Sea. The answer is Malta.</p>
<p>Q: The sport played by Garry Kasparov is?
A: Garry Kasparov is a former world chess champion, therefore the sport played by him is chess. The answer is chess.</p>
<p>Table 15: Few-shot CoT examples for WikiQA.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/LeeSureman/MoT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
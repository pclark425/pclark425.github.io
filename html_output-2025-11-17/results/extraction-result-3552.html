<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3552 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3552</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3552</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-4c8427e61b35b5a4be8a19a6063c19c5f971d682</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4c8427e61b35b5a4be8a19a6063c19c5f971d682" target="_blank">De novo design of protein target specific scaffold-based Inhibitors via Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> 3D-MolGNN$_{RL}$ provides an efficient way to optimize key features by multi-objective reward function within a protein pocket using parallel graph neural network models, and can serve as an interpretable artificial intelligence tool for lead optimization with optimized activity, potency, and biophysical properties.</p>
                <p><strong>Paper Abstract:</strong> Efficient design and discovery of target-driven molecules is a critical step in facilitating lead optimization in drug discovery. Current approaches to develop molecules for a target protein are intuition-driven, hampered by slow iterative design-test cycles due to computational challenges in utilizing 3D structural data, and ultimately limited by the expertise of the chemist - leading to bottlenecks in molecular design. In this contribution, we propose a novel framework, called 3D-MolGNN$_{RL}$, coupling reinforcement learning (RL) to a deep generative model based on 3D-Scaffold to generate target candidates specific to a protein building up atom by atom from the starting core scaffold. 3D-MolGNN$_{RL}$ provides an efficient way to optimize key features by multi-objective reward function within a protein pocket using parallel graph neural network models. The agent learns to build molecules in 3D space while optimizing the activity, binding affinity, potency, and synthetic accessibility of the candidates generated for infectious disease protein targets. Our approach can serve as an interpretable artificial intelligence (AI) tool for lead optimization with optimized activity, potency, and biophysical properties.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3552",
    "paper_id": "paper-4c8427e61b35b5a4be8a19a6063c19c5f971d682",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0044915,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DE NOVO DESIGN OF PROTEIN TARGET SPECIFIC SCAFFOLD-BASED INHIBITORS VIA REINFORCEMENT LEARNING</h1>
<p>Andrew D. McNaughton, Mridula S. Bontha, Carter R. Knutson, Jenna A. Pope, Neeraj Kumar*<br>Pacific Northwest National Laboratory<br>Richland, WA 99354, USA</p>
<h4>Abstract</h4>
<p>Efficient design and discovery of target-driven molecules is a critical step in facilitating lead optimization in drug discovery. Current approaches to develop molecules for a target protein are intuition-driven, hampered by slow iterative design-test cycles due to computational challenges in utilizing 3D structural data, and ultimately limited by the expertise of the chemist - leading to bottlenecks in molecular design. In this contribution, we propose a novel framework, called 3D-MolGNN ${ }<em L="L" R="R">{R L}$, coupling reinforcement learning (RL) to a deep generative model based on 3D-Scaffold to generate target candidates specific to a protein building up atom by atom from the starting core scaffold. 3D-MolGNN ${ }</em>$ provides an efficient way to optimize key features by multi-objective reward function within a protein pocket using parallel graph neural network models. The agent learns to build molecules in 3D space while optimizing the activity, binding affinity, potency, and synthetic accessibility of the candidates generated for infectious disease protein targets. Our approach can serve as an interpretable artificial intelligence (AI) tool for lead optimization with optimized activity, potency, and biophysical properties.</p>
<h2>1 INTRODUCTION</h2>
<p>Recent advancements in machine learning (ML) and artificial intelligence (AI) have demonstrated the potential to revolutionize the drug design process by reducing the initial chemical search space in the early stages of discovery (You et al., 2018; Born et al., 2020; Li et al., 2018; Huang et al., 2020; Chenthamarakshan et al., 2020). Currently, the potential chemical space is composed of $&gt;10^{60}$ molecules, candidates with suitable activity against specific protein targets only narrows the search space significantly based on the critical fragments. The COVID-19 pandemic has brought a surge of interest to explore data-driven methods to better produce efficacious drug candidates (Huang et al., 2020). One of the most important factors in identifying new drug candidates is that the molecules possess optimized properties that allow them to effectively bind with a required target while also having minimal off-target effects. If we consider molecular generation as a controlled and dynamic step-by-step process, it is possible to produce end products that possess these optimized properties. This approach allows us to formulate de novo drug design as a reinforcement learning (RL) problem and utilize algorithms that best learn a molecule's representation space based on the core moiety and its spatial interaction with the protein target. As an alternative, RL would provide a platform to create a highly-efficient inverse molecular design AI-system capable of producing novel high-performance molecules with domain-targeted properties.</p>
<h3>1.1 Related Works</h3>
<p>Our current work presents a novel approach to address the problem of generating molecules optimized for specific 3D protein targets starting from core functional groups so called scaffold. In recent years, RL-based works to generate molecules have been introduced that attempt to tackle the problem in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>different ways. A method by Popova et al. (2018) utilizes a fragment-replacement-based approach to optimize existing SMILES strings for specific drug-like properties, while another method by Ståhl et al. (2019) generates entirely new SMILES. Several current RL methods rely on the use of variational autoencoders (VAEs), which learn the latent space representation of 2D molecules to find the best representation (Lim et al., 2018; Jin et al., 2018; Liu et al., 2018; Sattarov et al., 2019; Joo et al., 2020). However, none of these methods take into consideration the 3D protein structure during the generation phase. Some of these methods do test the drug-likeness post-generation against some given target protein, but the target is never directly involved in the RL loop. When designing drug candidates that target specific proteins, learning how the molecule interacts with the protein's structure is invaluable to the generative process and can greatly assist in accelerating the automation of medicinal chemistry.</p>
<p>Recently, Born et al. (2020) proposed a deep reinforcement learning framework called PaccMann ${ }^{R L}$ for designing antiviral candidates binding against given protein targets using 2D protein sequence data. The generative model is composed of two pre-trained VAEs, namely a protein-VAE and a SELFIES-VAE, for mapping protein and drug molecules to a multi-modal latent space. The critic model is composed of two predictive models for predicting binding affinity of the protein-ligand pair and toxicity for the ligand. These predictions are used to formulate a multi-modal reward function to penalize the generative model (Born et al., 2021). This method still uses VAEs to generate 2D molecules, but it combines the latent space of the SMILES with the embedding of a specific protein to generate SMILES that consider target protein information</p>
<h1>1.2 Problem Formulations and Proposed Method</h1>
<p>We propose a new method, known as 3D-MolGNN ${ }<em L="L" R="R">{R L}$, that not only incorporates the protein structure into the RL loop, but considers the 3D structures of the generated compounds by constructing them atom-by-atom in 3D space. To the best of our knowledge, this is a process that hasn't been explored as most methods simply rely on 2D representations of the molecules in the form of a SMILES string. To overcome the sequence-based representation of the target and the drug, we leverage a 3D-scaffoldbased molecular generation actor over a 2D SMILES generation actor. The 3D-MolGNN ${ }</em>$ method uses our previously formulated 3D-scaffold-based generative model Joshi et al. (2021) to make atom-wise placements on the molecule starting from a desired scaffold.</p>
<p>Our method considers a similar approach to that of the actor-critic method demonstrated in the PaccMann ${ }^{R L}$ model. In our approach, we utilize a scaffold-based generative model, 3D-Scaffold (Joshi et al., 2021) to produce valid 3D compounds as the actor model. For the critic model, we utilize parallel graph neural networks as a binding probability predictor $\left(\mathrm{GNN}<em P="P">{P}\right.$ (Knutson et al., 2021)) to evaluate whether the generated compound actively binds with a target protein. This method takes the 3D structure of the protein pocket and the ligand, and predicts the probability of their interaction without any prior knowledge of the intermolecular interactions. The $\mathrm{GNN}</em>$ model uses both the residue and 3D atomic-level representation of a protein as well as the 3D atomic and bond level representation of the molecule.</p>
<h2>2 EXPERIMENTS and DATASET</h2>
<p>To demonstrate our methods ability to produce molecules for a given target protein, this work was centered around creating drug-like molecules that interacted with the SARS-CoV-2 $\mathrm{M}<em 50="50">{\text {pro }}$. To train the model, We used a dataset of non-covalent inhibitors from the BindingDB dataset (Gilson et al., 2016) and FDA approved drugs FDA. These compounds were filtered based on biophysical and cheminformatics properties like $\mathrm{IC}</em>}$, molecular weight, atom type, and functionality and have almost 10 K unique scaffolds. To represent the definition of scaffold, we used Murcko scaffolds Bemis $\&amp;$ Murcko (1996). A similar dataset was used in the prior 3D-Scaffold model (Joshi et al., 2021). High-throughput experimentation and our computer aided drug design results suggest that compounds containing the functional group piperazine are potent and have a higher affinity towards the $\mathrm{M<em _pro="{pro" _text="\text">{\text {pro }}$ target (Clyde et al., 2021; Joshi et al., 2021). However, there were no piperazine-containing molecules in the initial dataset used for training the 3D-scaffold model, but we were still able to generate some molecules containing piperazine using the trained model. We curated a smaller subset of the BindingDB dataset that possessed affinity towards protease-like targets and combined them with the piperazine dataset. Finally, we filtered these compounds based on their ability to bind with $\mathrm{M}</em>$ by}</p>
<p>doing an initial pass through the $\mathrm{GNN}<em _pro="{pro" _text="\text">{P}$ model. Since our motive is to achieve better binding affinity, potency and easily synthesizable compounds for the $\mathrm{M}</em>$ target, the initial screening helped us to choose proper compounds for training the RL models so that they learned to produce similar, or better compounds.}</p>
<h1>3 METHODS</h1>
<p>In this section, we discuss in detail the architecture and implementation of the RL approach. In particular,we go into detail about how the critic interprets the output from the actor.</p>
<h3>3.1 3D-MOLGNN $_{R L}$ FRAMEWORK</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic RL workflow highlighting the interaction of Actor and Critic in our 3DMolGNN $<em 0="0">{R L}$ model. The agent starts building the molecule from a starting scaffold (state $\mathrm{S}</em>$ ) and subsequently builds the molecule by choosing an atom based on the reward assigned by the critic for that intermediate state. The Agent block serves as the Actor, generating molecules to pass into the Critic block to assess the performance.</p>
<p>The schematic representation of 3D-MolGNN $<em P="P">{R L}$ framework is shown in the figure 1. The RL workflow begins from training a 3D-scaffold molecule generator followed by optimizing the partially built candidates towards the target of interest at every step of training. The neural network used in the 3D-Scaffold framework for de-novo drug candidate design can be broken into two major blocks: feature learning and atom placement as shown in Figure 1. In the feature learning block, the embedding and interaction layers of SchNet (Joshi et al., 2021; Schütt et al., 2019; Schütt et al., 2017; Schütt et al., 2017; Schütt et al., 2018) are used to extract and update rotationally and translationally invariant atom-wise features that capture the chemical environment of an unfinished molecule. The extracted features are used to predict distributions for the type of next atom and its 3D coordinates, where the latter distribution is constructed from predictions of pairwise distances between the next atom and all preceding atoms. The whole procedure is repeated successively to build a complete molecule with the desired scaffold. The partial molecule associated with each step of this atom placement is assessed by the critic $\left(\mathrm{GNN}</em>}\right)$. As a part of our experimentation, we used two critics simultaneously to use the binding probability and binding affinity of the partial molecule and the target of interest along with the synthetic accessibility of the partial molecule. The binding probability is the outcome from the SoftMax layer which predicts the activity/inactivity of a protein-ligand complex. The binding affinity is measured in terms of $\mathrm{K<em _mathrm_d="\mathrm{d">{\mathrm{i}}$ and $\mathrm{K}</em>\right)$.The two critics essentially use the same feature representation, but differ in terms of the data they have been trained on and also the label associated with the data. A more detailed breakdown of the layers and hyperparameters associated with the actor and critic models is included in the Appendix.}}$, which refer to the inhibition and dissociation constants, respectively and is given as $-\log \left(\frac{K_{i}}{K_{d}</p>
<h1>3.2 Reward Function \&amp; Parameters</h1>
<p>The process of building the molecule starts from a desired scaffold associated with the selected core functionality. For every action $t$ which is a random selection and placement of the atom to the partial molecule, the 3D-scaffold model predicts the next possible atom that could be placed close to the center of mass to the partial molecule at step $t-1$ and transitions to state $s_{t}$. At any step, let $Z_{\text {next }}$ be the ground truth type of the next atom and $\hat{p}<em _next="{next" _text="\text">{\text {type }}^{Z</em>}}}$ the probability that the model assigns to that type at the current step. This probability is converted to a negative log-likelihood as $P_{\Theta}\left(s_{t}\right)^{t y p e}$ (equation 3a). Here, $\hat{p<em j="j">{j}^{b}$ is the probability that the model assigns for the distance between position of already placed atom and ground truth of the next atom to fall into distance bin $b \in B$ at the current step. The distance based probability is calculated using Gaussian expanded ground truth distances $q</em>$ s shown in equation 3b. The overall idea is to train the agent to learn the latent space representation of atom type and it's possible placement in 3D-space closest to the center of mass of the molecule generated so far. The overall probability for any action $t$ is the summation of type-probability and distance-probability (equation 2). The ultimate goal is to train the agent such that it learns to generate new compounds while optimizing the policy $\Pi(\Theta)$ (equation 1). The policy is defined as the difference between the action-probabilities and the reward assigned by the critic at that step.}^{b}$ and probability distribution of atom placement $\hat{p}_{j}^{b</p>
<p>$$
\begin{gathered}
\Pi(\Theta)=\sum_{s_{t} \in S}\left(P_{\Theta}\left(s_{t}\right)-R\left(s_{t}\right)\right) \
P_{\Theta}\left(s_{t}\right)=P_{\Theta}\left(s_{t}\right)^{t y p e}+P_{\Theta}\left(s_{t}\right)^{d i s t} \
P_{\Theta}\left(s_{t}\right)^{t y p e}=-\log \left(\hat{p}<em _next="{next" _text="\text">{\text {type }}^{Z</em>}}}\right) \quad \text { (3a) } \quad P_{\Theta}\left(s_{t}\right)^{d i s t}=\sum_{j=1}^{N} \sum_{b \in B} q_{j}^{b} \log \left(\hat{p<em 1="1">{j}^{b}\right) \
R</em>\right)\right) \
R_{2}\left(s_{t}\right)=\alpha \cdot C_{B P}\left(B, C_{t}\right)+\left(1-\beta \cdot C_{S A}\left(C_{t}\right)\right)
\end{gathered}
$$}\left(s_{t}\right)=\alpha \cdot C_{B P}\left(B, C_{t}\right)+\beta \cdot C_{E A}\left(B, C_{t}\right)+\left(1-\gamma \cdot C_{S A}\left(C_{t</p>
<p>We used two different reward functions as $R_{1}\left(s_{t}\right)$ and $R_{2}\left(s_{t}\right)$ associating with two independent experiments for the purposes of ablation/hyperparameter testing. By removing portions of the reward function, we can see which attributes provide the best reward. The first reward is a function of binding probability, binding affinity of the target and the partially built molecule in addition to the synthetic accessibility of the molecule. While the second reward function uses only the binding probability and the synthetic accessibility score. We assigned different weights to each of these components while calculating the rewards. For $R_{1}\left(s_{t}\right)$ (equation 5), we used 0.5,0.25 and 0.25 respectively for binding probability, binding affinity and synthetic accessibility. While for $R_{2}\left(s_{t}\right)$ (equation 4, we used 0.75 and 0.25 for binding probability and synthetic accessibility respectively. We trained the agent for 150 epochs and chose the best trained model to generate compounds. The Synthetic Accessibility and Binding Affinity scores are scaled to be in between 0 and 1 to match the scale of the binding probability. For more on the ablation study, including some reward functions not shown in the main text, see Appendix C.</p>
<h2>4 RESULTS \&amp; DISCUSSION</h2>
<h3>4.1 Model Performance</h3>
<p>To ensure that we are a learning to build molecules based on the training data, we visualize the training and validation error over each epoch. Figure 2 represents the aggregated loss over the 150 epochs for our 3D-MolGNN ${ }_{R L}$ model. As loss decreases over time, we know that our model is able to effectively learn from the training data. This result is strictly evaluating the training and validation loss through the actor model to evaluate training performance, the drug-likeness scoring of the GNN model is discussed further in the next section.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Plot of loss over total epochs, the loss represents how well the model learns the 3D trace of the input data. Loss is calculated as a combination of KL-Divergence and the Reward Function</p>
<h1>4.2 Drug-Likeness Metrics for Generated Compounds</h1>
<p>We examined the effectiveness of our RL methods using different drug-likeliness metrics: quantitative estimate of drug-likeness (QED), water solubility (logS), synthetic accessibility (SA), and the octanolwater partition coefficient (logP). See the appendix for a full explanation of each metric. A desirable drug candidate would score well in each of these metrics. Since the agent for the 3D-MolGNN ${ }_{R L}$ is a scaffold based generative model, we focused on generating compounds with piperazine as the scaffold. To ensure that only valid molecules are compared, we filtered the total list of generated compounds based on a modified Lipinksi rule. The rule suggests that for a candidate to be an acceptable drug-like compound, there should be no more than 5 hydrogen bond donors, no more than 10 hydrogen bond acceptors, no more than 5 rotatable bonds, a molecular mass between 200 and 500 Dalton, an octanol-water partition coefficient less than 5, and finally at least one aromatic ring in the structure. Once filtered, approximately 100 top compounds per method were obtained.</p>
<h3>4.2.1 COMPARISON TO 2D GENERATIVE MODEL</h3>
<p>To show the advantages of using 3D-based generative modeling over conventional 2D-based generative models (SMILES-based), we compare our 3D-MolGNN ${ }<em L="L" R="R">{R L}$ model to that of PaccMann ${ }</em>}$. For better comparison of the two methods, we utilize a slightly modified version of the architecture to include our GNN critic model instead of the default PaccMann critic. This allows us to have a metric of comparison between a 2D and 3D generative model. For purposes of discussion, the modified PaccMann ${ <em L="L" R="R">{R L}$ model will be referred to as PM-GNN ${ }</em>$.
We first wanted to demonstrate that the RL implementation of the methods were essential in producing molecules with a high binding probability. To accomplish this, we generated molecules for the protein target $\mathrm{M}<em _pro="{pro" _text="\text">{\text {pro }}$ (PDB:6WQF), both with and without passing the reward from the critic back to the agent in each method. We designated here the molecules generated without RL optimization as unoptimized. To fairly compare the optimized vs unoptimized methods, we ensured that the unoptimized models were trained and tested on the same datasets used for training the respective RL models. The comparison of binding probabilities of the generated compounds towards the $\mathrm{M}</em>}}$ target from the 3D-MolGNN ${ <em L="L" R="R">{R L}$ and PM-GNN ${ }</em>}$ is shown in figure 3a and 3b, respectively. From this figure, it is evident that the RL mechanism has improved both agent's performance in producing candidates with high binding probability. The unoptimized molecules generated by the agent in 3D-MolGNN ${ <em L="L" R="R">{R L}$ and PM-GNN ${ }</em>$ as the predictions have a low mean probability. On the other hand, when using biased compounds during RL optimization, we see a significant increase in high binding probability compounds, showing the effects of incorporating RL with the generative models.}$ show very low binding probability towards $\mathrm{M}_{\text {pro }</p>
<p>To further assess the novelty and properties of generated compounds, we looked at the four different drug-likeness properties listed above to serve as comparison metrics. Figure 4 compares these metrics for both 3D-MolGNN ${ }<em _pro="{pro" _text="\text">{R L}$ reward functions with known active molecules for $\mathrm{M}</em>$. A full description of each metric is given in the appendix (section A). The first metric, QED, represents a quantification}</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison of generated compounds before and after optimization by 3D-MolGNN ${ }_{R L}$. The unoptimized molecules were generated using the respective agents for each method over the same data but without involvement of the critic. Results visualized as the normalized probability density function of the data</p>
<p>Figure 3: Comparison of generated compounds before and after optimization by 3D-MolGNN ${ }_{R L}$. The unoptimized molecules were generated using the respective agents for each method over the same data but without involvement of the critic. Results visualized as the normalized probability density function of the data</p>
<p>We next look at the distributions of water solubility calculated using the ESOL method (Delaney, 2004). Figure 4b shows that our methods score better than the current $\mathrm{M}<em L="L" R="R">{\text {pro }}$ actives while also showing that these methods produce molecules with desirable solubility in water. The mean of the 3D-MolGNN ${ }</em>}$ 's $R_{1}(s, t)$ model has the highest improvement relative to the mean of the $\mathrm{M<em 2="2">{\text {pro }}$ actives while both the $R</em>$ models improve less.}(s, t)$ and PM-GNN ${ }_{R L</p>
<p>The next metric, synthetic accessibility (SA), is a term included in each of the reward functions of the RL methods tested. Figure 4c illustrates the distributions obtained by each method where a SA score closer to 1 is preferred. The two 3D-MolGNN ${ }<em 1="1">{R L}$ models have a consistent range of SA scores that center around 5, which indicates consistently above average scores. $R</em>}(s, t)$ shows an $8.78 \%$ improvement and $R_{2}(s, t)$ shows a $9.92 \%$ improvement. The PM-GNN ${ <em _pro="{pro" _text="\text">{R L}$ spans a much wider range of SA scores, but the mean is centered around 4. These three models only slightly edge out the existing $\mathrm{M}</em>$ actives.}</p>
<p>The last metric is the octanol-water partition coefficient, or more simply known as $\log P$. Looking at Figure 4d, we can see how each method produces molecules spanning the range of -2 to 6 . The 3D-MolGNN ${ }<em 1="1">{R L}$ model using the $R</em>}(s, t)$ reward yields a $61.21 \%$ improvement to the $\mathrm{M<em 2="2">{\text {pro }}$ actives in $\log \mathrm{P}$ value, whereas using the $R</em>}(s, t)$ reward yields a $53.08 \%$ improvement. In this metric, the PM-GNN ${ <em _pro="{pro" _text="\text">{R L}$ metric produces compounds very similar to the $\mathrm{M}</em>$ actives.}</p>
<p>Table 1: Details about the drug-likeness metrics proposed in this work. We look at the 3D-MolGNN ${ }<em 1="1">{R L}$ model's two separate reward functions $R</em>(s, t)$. For each method, the top 3 candidates are chosen and their metric scores listed.}(s, t)$ and $R_{2</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">QED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\log \mathbf{S}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\log \mathbf{P}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rank</td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
<td style="text-align: center;">1st</td>
<td style="text-align: center;">2nd</td>
<td style="text-align: center;">3rd</td>
</tr>
<tr>
<td style="text-align: center;">3D-MolGNN ${ }_{R L}$ R1</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">-3.32</td>
<td style="text-align: center;">-2.49</td>
<td style="text-align: center;">-2.23</td>
<td style="text-align: center;">4.81</td>
<td style="text-align: center;">4.81</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">-0.14</td>
</tr>
<tr>
<td style="text-align: center;">3D-MolGNN ${ }_{R L}$ R2</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">-2.53</td>
<td style="text-align: center;">-3.82</td>
<td style="text-align: center;">-2.90</td>
<td style="text-align: center;">3.55</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">3.93</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">2.29</td>
<td style="text-align: center;">1.68</td>
</tr>
<tr>
<td style="text-align: center;">PM-GNN ${ }_{R L}$</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">-3.14</td>
<td style="text-align: center;">-3.90</td>
<td style="text-align: center;">-3.31</td>
<td style="text-align: center;">2.86</td>
<td style="text-align: center;">2.22</td>
<td style="text-align: center;">2.27</td>
<td style="text-align: center;">1.53</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">1.78</td>
</tr>
</tbody>
</table>
<p>To compare how each method performed on a per-molecule basis, Table 1 gives the results for the top 3 candidates produced by each of the methods. These molecules were ranked based on their predicted binding probability with the $\mathrm{M}_{\text {pro }}$ target. We can see that molecules that are performing above average in one metric are not guaranteed to perform well in every metric. For example, the top candidates for each method are each above the mean SA for the given method, but produce below</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of the properties of the molecules produced from 3D-MolGNN ${ }<em L="L" R="R">{R L}$ and PMGNN $</em>$ target. Results visualized as the normalized probability density function of the data
the mean for QED. 2D and 3D snapshots of the top 3 candidates from each method are available in appendix section B.}$ against the experimentally identified active compounds for $\mathrm{M}_{\text {pro }</p>
<p>Since our goal is achieve de novo design, additional metrics such as validity, uniqueness, and novelty were considered among the piperazine-based generated compounds as shown in Table 2. Initial validity was determined by processing the molecules using cheminformatics tools. The methods of calculating these properties are described in further detail in appendix subsection A.5. From table 2, we found that 3D-MolGNN ${ }<em L="L" R="R">{R L}$ R1 and 3D-MolGNN ${ }</em>}$ R2 produced greater than $95 \%$ valid compounds while also scoring close to $100 \%$ in uniqueness and novely from this valid set. The PM-GNN ${ <em L="L" R="R">{R L}$ model, while not scoring quite as well in Uniqueness and Novelty, scores the highest in Validity. This shows that by incorporating more parameters into the multi-objective reward function, there is an improvement in the generation of novel drug candidates. Overall, 3D-MolGNN ${ }</em>$ R1 outperformed other methods by generating compounds with high Validity, Uniqueness and Novelty.</p>
<p>Table 2: Table outlining the three metrics used to evaluate the compounds produced by varying reward functions and RL models. Compounds were screened and scored for an overall percentage based on validity, uniqueness, and novelty.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Validity</th>
<th style="text-align: left;">Uniqueness</th>
<th style="text-align: left;">Novelty</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">3D-MolGNN $_{R L}$ R1</td>
<td style="text-align: left;">$99.9 \%$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$99.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">3D-MolGNN $_{R L}$ R2</td>
<td style="text-align: left;">$97 \%$</td>
<td style="text-align: left;">$99.9 \%$</td>
<td style="text-align: left;">$99.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PM-GNN $_{R L}$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$93 \%$</td>
<td style="text-align: left;">$93 \%$</td>
</tr>
</tbody>
</table>
<h1>5 CONCLUSIONS</h1>
<p>In this work, we introduced a new method to include both the 3D structure of protein target and the generated compounds to perform multi-objective lead optimization critical for drug design and discovery. We demonstrated that our novel framework 3D-MolGNN ${ }<em L="L" R="R">{R L}$, which couples RL to a deep generative model based on a 3D-Scaffold, can generate target candidates built up atom by atom that are specific to a protein pocket. 3D-MolGNN ${ }</em>$ active molecules. This was given by a $&gt;50 \%$ increase in QED, a $&gt;40 \%$ increase in solubility, a $&gt;8 \%$ improvement in SA, and a $&gt;50 \%$ improvement in hydrophilicity. We found that our RL integration significantly improved the types of molecules generated by the untrained agents. Throughout this work, we demonstrated that by including more parameters into the multi-objective reward function, there is an improvement in generated novel target specific candidates. This gives us confidence that our RL framework is effective at producing protein target specific hit candidates by leveraging the 3D structures of both the generated molecule and the protein pocket, a consideration not made by other molecular generation methods to date.}$ provides an efficient way to generate target specific candidates by learning to build molecules in 3D space while optimizing the binding affinity, potency, and synthetic accessibility. To accomplish this, we utilized the protein for SARS-CoV-2 M pro as a target for generating optimized inhibitor candidates. We found that our model was able to generate molecules with better druglikeness, synthetic accessibility, water solubility, and hyrdophilicity than current $\mathrm{M}_{\text {pro }</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>This work was supported in part by the U.S. Department of Energy, Office of Science, Laboratory Directed Research Funding (LDRD), Mathematics of Artificial Reasoning for Science (MARS) Initiative, at the Pacific Northwest National Laboratory. Pacific Northwest National Laboratory (PNNL) is a multiprogram national laboratory operated by Battelle for the DOE under Contract DE-AC05-76RLO 1830. This research used computational resources provided by Research Computing at the Pacific Northwest National Laboratory.</p>
<h2>REFERENCES</h2>
<p>Zinc database. URL http://zinc.docking.org/substances/subsets/fda/ ?page $=1$.</p>
<p>Guy W. Bemis and Mark A. Murcko. The properties of known drugs. 1. molecular frameworks. J. Med. Chem., 39(15):2887-2893, 1996. doi: 10.1021/jm9602928. URL https://doi.org/ 10.1021/jm9602928.
G. Richard Bickerton, Gaia V. Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L. Hopkins. Quantifying the chemical beauty of drugs. Nature Chemistry, 4(2):90-98, feb 2012. ISSN 17554330. doi: 10.1038/nchem.1243. URL http://www.nature.com/articles/nchem. 1243 .</p>
<p>Jannis Born, Matteo Manica, Joris Cadow, Greta Markert, Nil Adell Mill, Modestas Filipavicius, and María Rodríguez Martínez. Paccmann ${ }^{\text {RL }}$ on sars-cov-2: Designing antiviral candidates with conditional generative models. arXiv preprint arXiv:2005.13285, 2020.</p>
<p>Jannis Born, Matteo Manica, Joris Cadow, Greta Markert, Nil Adell Mill, Modestas Filipavicius, Nikita Janakarajan, Antonio Cardinale, Teodoro Laino, and María Rodríguez Martínez. Datadriven molecular design for discovery and synthesis of novel ligands: a case study on SARS-CoV-2. Machine Learning: Science and Technology, 2(2):025024, jun 2021. ISSN 2632-2153. doi: 10.1088/2632-2153/abe808. URL https://iopscience.iop.org/article/10. 1088/2632-2153/abe808.</p>
<p>Vijil Chenthamarakshan, Payel Das, Inkit Padhi, Hendrik Strobelt, Kar Wai Lim, Ben Hoover, Samuel C Hoffman, and Aleksandra Mojsilovic. Target-specific and selective drug design for covid-19 using deep generative models. arXiv preprint arXiv:2004.01215, 2020.</p>
<p>Austin Clyde, Stephanie Galanie, Daniel W Kneller, Heng Ma, Yadu Babuji, Ben Blaiszik, Alexander Brace, Thomas Brettin, Kyle Chard, Ryan Chard, et al. High-throughput virtual screening and</p>
<p>validation of a sars-cov-2 main protease noncovalent inhibitor. Journal of chemical information and modeling, 2021.</p>
<p>John S. Delaney. ESOL: Estimating Aqueous Solubility Directly from Molecular Structure. Journal of Chemical Information and Computer Sciences, 44(3):1000-1005, may 2004. ISSN 0095-2338. doi: 10.1021/ci034243x. URL https://pubs.acs.org/doi/10.1021/ci034243x.</p>
<p>Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of Cheminformatics, 1(1):8, dec 2009. ISSN 1758-2946. doi: 10.1186/1758-2946-1-8. URL https://jcheminf. biomedcentral.com/articles/10.1186/1758-2946-1-8.</p>
<p>Michael K. Gilson, Tiqing Liu, Michael Baitaluk, George Nicola, Linda Hwang, and Jenny Chong. BindingDB in 2015: A public database for medicinal chemistry, computational chemistry and systems pharmacology. Nucleic Acids Research, 44(D1):D1045-D1053, jan 2016. ISSN 0305-1048. doi: 10.1093/nar/gkv1072. URL https://academic.oup.com/nar/ article-lookup/doi/10.1093/nar/gkv1072.</p>
<p>Kexin Huang, Tianfan Fu, Cao Xiao, Lucas Glass, and Jimeng Sun. Deeppurpose: a deep learning based drug repurposing toolkit. arXiv preprint arXiv:2004.08919, 2020.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction Tree Variational Autoencoder for Molecular Graph Generation. feb 2018. URL http://arxiv.org/abs/1802.04364.</p>
<p>Sunghoon Joo, Min Soo Kim, Jaeho Yang, and Jeahyun Park. Generative Model for Proposing Drug Candidates Satisfying Anticancer Properties Using a Conditional Variational Autoencoder. ACS Omega, 5(30):18642-18650, aug 2020. ISSN 2470-1343. doi: 10.1021/acsomega.0c01149. URL https://pubs.acs.org/doi/10.1021/acsomega.0c01149.</p>
<p>Rajendra Prashad Joshi, Niklas Gebauer, Neeraj Kumar, and Mridula Bontha. 3d-scaffold: Deep learning framework to generate 3d coordinates of drug-like molecules with desired scaffolds. bioRxiv, 2021.</p>
<p>Carter Knutson, Mridula Bontha, Jenna A Bilbrey, and Neeraj Kumar. Decoding the protein-ligand interactions using parallel graph neural networks. arXiv preprint arXiv:2111.15144, 2021.</p>
<p>Yibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional graph generative model. Journal of Cheminformatics, 10(1):33, 2018. ISSN 1758-2946. doi: 10. 1186/s13321-018-0287-6. URL https://doi.org/10.1186/s13321-018-0287-6.</p>
<p>Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model based on conditional variational autoencoder for de novo molecular design. Journal of Cheminformatics, 10(1):31, dec 2018. ISSN 1758-2946. doi: 10.1186/s13321-018-0286-7. URL https:// jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0286-7.</p>
<p>Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. Constrained graph variational autoencoders for molecule design. In NeurIPS, pp. 7806-7815, 2018.</p>
<p>Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science Advances, 4(7):eaap7885, jul 2018. ISSN 2375-2548. doi: 10. 1126/sciadv.aap7885. URL https://advances.sciencemag.org/lookup/doi/10. 1126/sciadv.aap7885.</p>
<p>Boris Sattarov, Igor I. Baskin, Dragos Horvath, Gilles Marcou, Esben Jannik Bjerrum, and Alexandre Varnek. De Novo Molecular Design by Combining Deep Autoencoder Recurrent Neural Networks with Generative Topographic Mapping. Journal of Chemical Information and Modeling, 59 (3):1182-1196, mar 2019. ISSN 1549-9596. doi: 10.1021/acs.jcim.8b00751. URL https: //pubs.acs.org/doi/10.1021/acs.jcim.8b00751.</p>
<p>Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Müller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Adv. Neural Inf. Process Syst., volume 30, pp. 9911001. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/ 2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.</p>
<p>Kristof T Schütt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Müller, and Alexandre Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nat. Commun., 8(1):13890, 2017. ISSN 2041-1723. doi: 10.1038/ncomms13890. URL https://doi.org/10.1038/ ncomms13890.
K. T. Schütt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, and K.-R. Müller. Schnet - a deep learning architecture for molecules and materials. J. Chem. Phys., 148(24):241722, 2018. doi: 10.1063/1.5019779. URL https://doi.org/10.1063/1.5019779.
K. T. Schütt, P. Kessel, M. Gastegger, K. A. Nicoli, A. Tkatchenko, and K.-R. Müller. Schnetpack: A deep learning toolbox for atomistic systems. J. Chem. Theory Comput., 15(1):448-455, 2019. doi: 10.1021/acs.jctc.8b00908. URL https://doi.org/10.1021/acs.jctc.8b00908.</p>
<p>Niclas Ståhl, Göran Falkman, Alexander Karlsson, Gunnar Mathiason, and Jonas Boström. Deep Reinforcement Learning for Multiparameter Optimization in de novo Drug Design. Journal of Chemical Information and Modeling, 59(7):3166-3176, jul 2019. ISSN 1549-9596. doi: 10.1021/acs.jcim.9b00325. URL https://pubs.acs.org/doi/10.1021/acs.jcim. 9b00325.</p>
<p>Scott A. Wildman and Gordon M. Crippen. Prediction of Physicochemical Parameters by Atomic Contributions. Journal of Chemical Information and Computer Sciences, 39(5):868-873, sep 1999. ISSN 0095-2338. doi: 10.1021/ci9903071. URL https://pubs.acs.org/doi/10. 1021/ci9903071.</p>
<p>Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In Advances in Neural Information Processing Systems, pp. 6410-6421, 2018.</p>
<h1>A Performance Metrics</h1>
<h2>A. 1 Quantitative Estimate of Druglikeness</h2>
<p>The QED metric represents a quantification of the desirability of the drug (Bickerton et al., 2012). The closer the score is to 1 , the more desirable it is as a drug candidate.</p>
<p>The equation for QED is given as:</p>
<p>$$
Q E D=\exp \left(\frac{1}{n} \sum_{i=1}^{n} \ln d_{i}\right)
$$</p>
<p>Where $d_{i}$ is a series of desirability functions (d) belonging to eight widely used molecular descriptors. These are molecular weight (MW), the octanol-water partition coefficient (ALOGP), the number of hydrogen bond donors (HBD), the number of hydrogen bond acceptors (HBA), the molecular polar surface area (PSA), the number of rotatable bonds (ROTB), the number of aromatic rings (AROM), and the number of structural alerts (ALERTS).
The desirability function can be represented by a general asymmetric double sigmoidal function where $\mathrm{d}(\mathrm{x})$ is the desirability function for molecular descriptor x shown as:</p>
<p>$$
d_{i}(x)=a_{i}+\frac{b_{i}}{1+\exp \left(-\frac{x-e_{i}+\frac{d_{i}}{2}}{e_{i}}\right)} \cdot\left[1-\frac{1}{1+\exp \left(-\frac{x-e_{i}+\frac{d_{i}}{2}}{e_{i}}\right)}\right]
$$</p>
<p>where $a_{i}, \ldots, f_{i}$ can be found in the supplementary table of the original publication (Bickerton et al., 2012).</p>
<h2>A. 2 Estimating Aqueous Solubility</h2>
<p>The metric, water solubility, calculates the log solubility $(\log S)$ of the molecule. In this work, the solubility is determined by ESOL (Delaney, 2004). The majority of drugs posses a $\log S$ between -8 and -2 . The more positive the value, the more water soluble the molecule.
ESOL as defined in Delaney (2004) can be calculate as the multiple linear regression of:</p>
<ol>
<li>$\operatorname{clogP}$</li>
<li>Molecular weight (MWT)</li>
<li>Rotatable bonds (RB)</li>
<li>Aromatic proportion (AP)
given as:</li>
</ol>
<p>$$
\log \left(S_{w}\right)=0.16-0.63 \operatorname{clog} P-0.0062 M W T+0.066 R B-0.74 A P
$$</p>
<h2>A. 3 Synthetic Accessibility</h2>
<p>SA is one of the most critical metrics to use in determining the simplicity in experimentally synthesizing a molecule. It is not a score that dictates how effective the molecule is, but rather a practical measure of it's complexity. The SA score is between 1 to 10 , where 1 indicates an easily synthesizable molecule and 10 indicates a complex one.
The algorithm for calculating the SA score of a molecule (as represented in Ertl \&amp; Schuffenhauer (2009)) is given as:</p>
<p>SAscore $=$ fragmentScore - complexityPenalty,</p>
<p>where the fragment score is calculated as a sum of contributions of all fragments in the molecule divided by the number of fragments in this molecule. The contribution of a fragment is obtained from a database of fragment contributions that were generated by statistical analysis of substructures in the PubChem collection.</p>
<p>The complexity penalty is a score given to characterize the presence of complex structural features in the molecules. It is defined as a combination of the following:</p>
<p>$$
\begin{gathered}
\text { ringComplexityScore }=\log (n \text { RingBridgeAtoms }+1)+\log (n \text { SpiroAtoms }+1) \
\text { stereoComplexityScore }=\log (n \text { StereoCenters }+1) \
\text { macrocyclePenalty }=\log (n \text { Macrocycles }+1) \
\text { sizePenalty }=n \text { Atoms }^{1.005}-\text { natoms }
\end{gathered}
$$</p>
<h1>A. 4 HYDROPHILICITY VS. LIPOPHILICITY</h1>
<p>This metric, known as $\log P$, is the calculated octanol-water partition coefficient of a given molecule. The values represents if a drug is either very hydrophilic (-3) or very lipophilic (+10). This specific metric is present in Lipinski's rule as value that needs to be less than 5 to be considered a drug candidate.</p>
<p>To calculate the partition function for octanol-water partition coefficient that dictates whether a molecule is more hydrophilic or lipophilic we utilize the RDKit package implementation of the Crippen approach (Wildman \&amp; Crippen, 1999). It simply calculates the sum of the contributions of each of the atoms in the molecule. Intramolecular interactions are accounted for my classifying atoms into different types based on their attached $a_{i}$ and neighboring atoms $n_{i}$ :</p>
<p>$$
P_{c a l c}=\sum_{i} n_{i} a_{i}
$$</p>
<p>where P can be further calculated into $\log P$. A full list of the atomic descriptors and contributions can be found in the main text of Wildman \&amp; Crippen (1999).</p>
<h2>A. 5 VALIDITY, UNIQUENESS, AND NOVELTY</h2>
<p>To analyze the novelty of our compounds, we need to look at how we calculate the validity and uniqueness of our compounds.</p>
<p>These are given as follows:</p>
<p>$$
\begin{gathered}
\text { Validity }=\frac{\text { Number of valid molecules }}{\text { Number of generated molecules }} \
\text { Unique }=\frac{\text { Number of unique molecules }}{\text { Number of valid molecules }} \
\text { Novelty }=\frac{\text { Number of generated molecules not in training set }}{\text { Number of unique and valid generated molecules }}
\end{gathered}
$$</p>
<h1>B Top Candidates</h1>
<p>Shown below are snapshots of the top 3 candidates from each of the reward functions presented in this work.</p>
<h2>B. 1 2D Representations</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) 3D-MolGNN ${ }<em L="L" R="R">{R L}$ - R2 Top 3 candidates: $94.49 \%, 91.02 \%, 84.88 \%$
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) 3D-MolGNN ${ }</em>$ - R1 Top 3 candidates: $91.31 \%, 91.82 \%, 88.07 \%$</p>
<p>Figure 5: The top 3 candidates for each reward function in this work. The candidates are shown in descending order from left to right. Their associated binding probability is listed in the subcaption also in descending order from left to right.</p>
<h1>B. 2 3D REPRESENTATIONS</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) 3D-MolGNN ${ }<em L="L" R="R">{R L}$ - R1 Top 3 candidates: $91.31 \%, 91.82 \%, 88.07 \%$
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) 3D-MolGNN ${ }</em>$ - R2 Top 3 candidates: $94.49 \%, 91.02 \%, 84.88 \%$</p>
<p>Figure 6: The top 3 candidates for each reward function in this work. The candidates are shown in descending score order from left to right and down. Their associated binding probability is listed in the subcaption also in descending order from left to right.</p>
<h1>C Ablation Study on Reward Function</h1>
<p>In order to see the importance of optimizing for multiple properties in the reward function, we tested different training weight, and reward function combinations for the model. The most important 2 were listed in the main text of the paper, but we tested 2 more reward functions to test performance.</p>
<p>We used the following combinations and compared how their output scored in the drug-like metrics. We see a steady decrease in performance from Reward 1 to Reward 3 showing that the inclusion of Binding Probability is important for generating molecules with a drug-like features. We also compare our 3D method with the 2D protein inclusion using the original PaccMann Method but with our GNN instead of their PaccMann critic to see if the 2D features have an effect.</p>
<p>$$
\begin{aligned}
&amp; R_{1}\left(s_{t}\right)=\alpha \cdot C_{B P}\left(B, C_{t}\right)+\beta \cdot C_{E A}\left(B, C_{t}\right)+\left(1-\gamma \cdot C_{S A}\left(C_{t}\right)\right) \
&amp; R_{2}\left(s_{t}\right)=\alpha \cdot C_{B P}\left(B, C_{t}\right)+\left(1-\beta \cdot C_{S A}\left(C_{t}\right)\right) \
&amp; R_{3}\left(s_{t}\right)=\alpha \cdot C_{E A}\left(B, C_{t}\right)+\left(1-\beta \cdot C_{S A}\left(C_{t}\right)\right)
\end{aligned}
$$</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: Comparison of the properties of the molecules produced from 3D-MolGNN ${ }<em _pro="{pro" _text="\text">{R L}$ against the experimentally identified active compounds for $\mathrm{M}</em>$ target.}</p>
<p>Overall, figure 7 shows that the complete 3 term reward function for the 3D-MolGNN ${ }_{R L}$ method produces the best scoring molecules overall for the given drug-like metrics.</p>
<h2>D 3D-SCAFFOLD HYPERPARAMETERS</h2>
<p>For the 3D Scaffold model we use the following hyperparameters: batch size of 2, split of 1000-500 training, validation set, 150 max epochs, learning rate of 0.0001 , learning rate patience of 10 , learning rate minimum of 1e-6, learning rate decay of 0.5 , atom-wise representation layer size of 32 , cutoff radius of local environment of 10 , interaction layer size of 6,25 gaussians to expand distances with a max distance of 15 and 300 distance bins.</p>
<h1>E GNN HYPERPARAMETERS</h1>
<p>Hyperparameters such as network depth, layer dimension, and learning rate can have a large effect on model training and the weights in the final realized model. Therefore, we performed a number of trainings to examine combinations of learning rate, number of attention heads, and layer dimension. We used the parameters from our best model to create a baseline: a learning rate of 0.0001 , four attention heads, and a layer dimension of 70 produced an average test AUROC of 0.854 . The test parameters and top scoring configuration are outlined and highlighted in Table 3. The hyperparameters that performed best were a learning rate of 0.0001 , two attention heads, and a dimension of 70 . These parameters resulted in an average test AUROC of 0.864 , still scoring lower than our implemented baseline configuration.</p>
<p>Table 3: Values of examined hyperparameters: learning rate (lr), number $(N)$ of attention heads, and dimension $(D)$ of the GAT layer in each attention head. A grid search was performed on each combination of parameters. The optimal combination is shown in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">lr</th>
<th style="text-align: center;">$N$</th>
<th style="text-align: center;">$D$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\mathbf{7 0}$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{0 . 0 0 0 1}$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">140</td>
</tr>
<tr>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">$\mathbf{4}$</td>
<td style="text-align: center;">210</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">280</td>
</tr>
</tbody>
</table>
<p>Trainings were carried out over 200 epochs on a quarter of the data taken from our dataset consisting of 2,000 samples per target with 79 targets, using the same train-test split for all trainings. The decreased performance compared to the models is expected due to the reduced set of data used to train the model. Thirty of the 36 combinations trained without error and are shown in Table 4.</p>
<p>Table 4: Values of examined hyperparameter set with average train and Test ROC .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">HYPERPARAMETER SET</th>
<th style="text-align: center;">TRAIN ROC AVG</th>
<th style="text-align: center;">TEST ROC AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LR_0.001_N5_D70</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.771</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N4_D210</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N4_D140</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.661</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N3_D70</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.835</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N3_D280</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.630</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N3_D210</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.694</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N3_D140</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.742</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N2_D70</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.861</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N2_D140</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.781</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N2_D280</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.755</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.001_N2_D210</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.773</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.0001_N4_D128</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.854</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.0001_N3_D70</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.855</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.0001_N3_D210</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.860</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.0001_N3_D140</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.862</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.0001_N2_D70</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.864</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.0001_N2_D210</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.861</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.0001_N2_D140</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.853</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N4_D70</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.726</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N4_D210</td>
<td style="text-align: center;">0.776</td>
<td style="text-align: center;">0.783</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N4_D140</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.769</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N3_D70</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.758</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N3_D210</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N3_D140</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.755</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N2_D70</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.749</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N2_D210</td>
<td style="text-align: center;">0.776</td>
<td style="text-align: center;">0.783</td>
</tr>
<tr>
<td style="text-align: left;">LR_0.00001_N2_D140</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.782</td>
</tr>
</tbody>
</table>
<h1>F Math Notations</h1>
<p>$\mathrm{S}<em _mathrm_t="\mathrm{t">{\mathrm{t}}$ - state of the molecule at for action t
T - Terminal state/Max length of molecule
S - all possible states
$\mathrm{R}</em>$ - reward for the partial molecule at step t
$\mathrm{M}}<em _mathrm_c="\mathrm{c">{\mathrm{t}}$ - candidate molecule
B - target protein
$\mathrm{X}</em>$ - Target embeddings from protein-VAE for SELFIES-VAE G - generator
$\mathrm{C}}<em _mathrm_SA="\mathrm{SA">{\mathrm{BP}}$ - Critic for Binding Probability prediction
$\mathrm{C}</em>$ - Critic for SA score prediction
$\mathrm{C}}<em _Theta="\Theta">{\mathrm{EA}}$ - Critic for Experimental Affinity prediction
$\Pi(\Theta)$ - optimization policy
N - steps per episode
$P</em>$ - probability associated with the action $t$
$Z_{\text {next }}$ - ground truth type of the next atom
$\hat{p}<em _next="{next" _text="\text">{\text {type }}^{Z</em>$ - probability that the model assigns to that type at the current step.
$\hat{p}}}<em j="j">{j}^{b}$ - probability that the model assigns for the distance between $\mathbf{r}</em>$ to fall into distance bin $b \in B$ at the current step}$ and $\mathbf{r}_{\text {next }</p>
<h2>PM-GNN $_{R L}$ model</h2>
<p>$\Pi(\Theta)=\sum_{s_{t} \in S} P_{\Theta}\left(s_{t}\right) R\left(s_{t}\right)$
$R\left(s_{t}\right)= \begin{cases}\alpha \cdot C_{B P}\left(B, C_{t}\right)+\beta \cdot C_{S A}\left(C_{t}\right), &amp; t=T \ 0, &amp; t&lt;T\end{cases}$</p>
<h2>3D-MolGNN- $_{R L}$ model</h2>
<p>$\Pi(\Theta)=\sum_{s_{t} \in S}\left(P_{\Theta}\left(s_{t}\right)-R\left(s_{t}\right)\right)$
$P_{\Theta}\left(s_{t}\right)=P_{\Theta}\left(s_{t}\right)^{\text {type }}+P_{\Theta}\left(s_{t}\right)^{\text {dist }}$
$P_{\Theta}\left(s_{t}\right)^{\text {type }}=-\log \left(\hat{p}<em _next="{next" _text="\text">{\text {type }}^{Z</em>\right)$
$P_{\Theta}\left(s_{t}\right)^{\text {dist }}=\sum_{j=1}^{N} \sum_{b \in B} q_{j}^{b} \log \left(\hat{p}}}<em 1="1">{j}^{b}\right)$
$R</em>\right)\right)$
$R_{2}\left(s_{t}\right)=\alpha \cdot C_{B P}\left(B, C_{t}\right)+\left(1-\beta \cdot C_{S A}\left(C_{t}\right)\right)$
$R_{3}\left(s_{t}\right)=\alpha \cdot C_{E A}\left(B, C_{t}\right)+\left(1-\beta \cdot C_{S A}\left(C_{t}\right)\right)$}\left(s_{t}\right)=\alpha \cdot C_{B P}\left(B, C_{t}\right)+\beta \cdot C_{E A}\left(B, C_{t}\right)+\left(1-\gamma \cdot C_{S A}\left(C_{t</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Correspondence to neeraj.kumar@pnn1.gov&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3829 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3829</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3829</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-7c9f69848d28e0a7cbb00942ee83dab9773c23e4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7c9f69848d28e0a7cbb00942ee83dab9773c23e4" target="_blank">GPT-NER: Named Entity Recognition via Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, and performs significantly better than supervised models, which demonstrates the capabilities of GPT-ner in real-world NER applications where the number of labeled examples is limited.</p>
                <p><strong>Paper Abstract:</strong> Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text"Columbus is a city"is transformed to generate the text sequence"@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the"hallucination"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3829",
    "paper_id": "paper-7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0069394999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPT-NER: Named Entity Recognition via Large Language Models</h1>
<p>Shuhe Wang ${ }^{\mathbf{1}}$, Xiaofei Sun ${ }^{\mathbf{1}}$, Xiaoya $\mathbf{L i}^{\mathbf{1}}$, Rongbin Ouyang ${ }^{\mathbf{1}}$ Fei Wu ${ }^{\mathbf{1}}$, Tianwei Zhang ${ }^{\mathbf{2}}$, Jiwei Li ${ }^{\mathbf{1}}$, Guoyin Wang ${ }^{\mathbf{*}}$</p>
<h2>Abstract</h2>
<p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.</p>
<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text Columbus is a city is transformed to generate the text sequence @@Columbus## is a city, where special tokens @@## marks the entity to extract. To efficiently address the hallucination issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a selfverification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag.</p>
<p>We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the lowresource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited. ${ }^{12}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1 Introduction</h2>
<p>Large-scale language models (LLMs) (Brown et al., 2020; Smith et al., 2022; Du et al., 2022; Rae et al., 2021; Thoppilan et al., 2022; Hoffmann et al., 2022; Chowdhery et al., 2022; Touvron et al., 2023) have shown an impressive ability for in-context learning: with only a few task-specific examples as demonstrations, LLMs are able to generate results for a new test input. Under the framework of in-context learning, LLMs have achieved promising results in a variety of NLP tasks, include machine translation (MT) (Vilar et al., 2022; Vidal et al., 2022; Moslem et al., 2023), question answering (QA) (Robinson et al., 2022; Li et al., 2022; Lazaridou et al., 2022) and named entity extraction (NEE) (Chowdhery et al., 2022; Brown et al., 2020).</p>
<p>Despite the progress, LLMs' performances on the task of NER are still well below supervised baselines. This is because of the intrinsic gap between the two tasks of NER and LLMs: NER is a sequence labeling task in nature, where the model needs to assign an entity-type label to each token within a sentence, while LLMs are formalized under a text generation task. The gap between the semantic labeling task and the text generation model leads to inferior performance when applying LLMs to resolve the NER task.</p>
<p>In this paper, we propose GPT-NER to resolve this issue. GPT-NER transforms the NER task to a text-generation task that can be easily adapted by LLMs. Specifically, the task of finding location entities in the input text Columbus is a city is transformed to generate the text sequence @@Columbus## is a city, where special tokens @@## marks the entity. We find that, compared with other formalizations, the proposed strategy, can significantly decrease the difficulty in generating text that fully encodes label information of the input sequence, as the model only needs to mark the position for entities and make copies for all the rest tokens. Experi-</p>
<p>ments show that the proposed strategy significantly improves the performance.</p>
<p>Another big problem with LLMs for NER is the hallucination issue, where LLMs have a strong inclination to over-confidently label NULL inputs as entities. To address this issue, we propose a self-verification strategy, which is placed right after the entity extraction stage, prompting LLMs to ask itself whether an extracted entity belongs to a labeled entity tag. The self-verification strategy acts as a regulating function to counteract the excessive confidence of LLMs, which we find effective in addressing the hallucination issue, leading to a significant performance boost.</p>
<p>We conduct experiments on five widely-adopted NER datasets, both flat NER and nested NER. GPTNER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. Additionally, we find that the performance hasn’t plateaued when we reach the GPT-3 token limit with respect to the number of demonstrations. This means that there is still room for improvement when the 4,096 token limits of GPT-3 are released, e.g., using GPT-4 whose token limit is more than 20K. What is particularly noteworthy is that GPT-NER exhibits impressive proficiency in low-resource and few-shot NER setups: when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This illustrates the potential of GPT-NER to be employed in real-world NER applications even when the quantity of labeled samples is scant.</p>
<h2>2 Related Work</h2>
<h3>2.1 Named Entity Recognition</h3>
<p>Named Entity Recognition (NER) is a task to identify key information in the text and classify it into a set of predefined categories. A common approach to resolve NER is to formulate it as a sequence labeling task. <em>Hammerton (2003)</em> used unidirectional LSTMs to obtain token-level representations and feed them to the softmax classifier obtaining the results. <em>Collobert et al. (2011)</em> used CNN to embed each input word and leverage CRF to decode each embedding into a certain entity. <em>Chiu and Nichols (2016)</em> used a character CNN and <em>Devlin et al. (2018)</em> used BERT to obtain token-level representations for classifications. <em>Lample et al. (2016)</em> combined the bidirectional LSTMs with CRFs to augment the prediction. <em>Sarzynska-Wawer et al. (2021)</em> improved the quality of each word via a large-scale pre-training model. <em>Li et al. (2019a, b)</em> formulated the NER task as an MRC task and further leveraged dice loss to improve the performance of the MRC model, and <em>Wang et al. (2022)</em> proposed the GNN-SL model to allow a general NER model to refer to training examples at test time.</p>
<h3>2.2 Large Language Models and In-context Learning</h3>
<p>Large language models (LLMs) <em>Brown et al. (2020); Rae et al. (2021); Smith et al. (2022); Hoffmann et al. (2022); Chowdhery et al. (2022)</em> have obtained significant performance boosts on a variety of natural language processing tasks <em>Hegselmann et al. (2022); Vilar et al. (2022); Perez et al. (2021); Pietrzak et al. (2021); Wei et al. (2021)</em>. Strategies to use LLMs for downstream tasks can be divided into two categories: fine-tuning and in-context learning. The fine-tuning strategy takes a pre-trained model as initialization and runs additional epochs on the downstream supervised data <em>Raffel et al. (2020); Gururangan et al. (2018); Roberts et al. (2020); Guu et al. (2020)</em>.</p>
<p>Different from the fine-tuning strategy, in-context learning (ICL) prompts LLMs to generate texts under few-shot demonstrations. <em>Radford et al. (2019)</em> first reform downstream tasks using prompts containing demonstrations. <em>Brown et al. (2020)</em> performs a systematic analysis for in-context learning and conducts multiple experiments on various tasks by its GPT-3 model. <em>Chowdhery et al. (2022)</em> perform analysis for the NMT task on PaLM. <em>Perez et al. (2021); Lu et al. (2021); Rubin et al. (2021)</em> show that better prompts and demonstrations lead to a performance boost for in-context learning.</p>
<h2>3 Background</h2>
<p>Named entity recognition (NER) is a typical sequence labeling task that assigns an entity type $y \in Y$ to each word $x$ in a given sentence $X=$ $\left{x_{1}, \ldots, x_{n}\right}$, where $Y$ denotes the set of entity labels and $n$ denotes the length of the given sentence.</p>
<h3>3.1 NER as Sequence Labeling</h3>
<p>A common approach to resolve NER is to formulate it as a sequence labeling task, which can be decomposed into the following two steps: (1) representation extraction and (2) classification.</p>
<p>Representation Extraction aims to obtain the high-dimensional representation for each token</p>
<p>within the input sequence. To embed each input word $x$, firstly the input sentence $X$ is fed into an encoder model, e.g., BERT (Devlin et al., 2018). Then the output of the last layer of the word embedding model is used as the high-dimensional representation $h_{i} \in \mathbb{R}^{m \times 1}$, where $n$ denotes the length of the input sentence and $m$ denotes a variable parameter of the dimension of the vector.</p>
<p>Classification. For classification, each embedded high-dimensional vector $h$ is sent to a multilayer perceptron and then generates the distribution over the named entity vocabulary using the softmax function:</p>
<p>$$
p_{\mathrm{NER}}=\operatorname{softmax} \operatorname{MLP}\left(h \in \mathbb{R}^{m \times 1}\right)
$$</p>
<h2>4 GPT-NER</h2>
<p>In this work, we propose GPT-NER, which uses large language models to resolve the NER task. GPT-NER follows the general paradigm of incontext learning and can be decomposed into three steps: (1) Prompt Construction: for a given input sentence $X$, we construct a prompt (denoted by $\operatorname{Prompt}(X)$ ) for $X$; (2) feeding the constructed prompt to the large language model (LLM) to obtain the generated text sequence $W=$ $\left{w_{1}, \ldots, w_{n}\right} ;(3)$ transforming the text sequence $W$ to a sequence of entity labels to obtain the final results.</p>
<p>Straightforward as the in-context learning paradigm is, the task of NER is not readily fit for it as it is a sequence labeling task in nature rather than a generation task. Below we describe different strategies we propose to adapt LLMs to the NER task in detail.</p>
<h3>4.1 Prompt Construction</h3>
<p>Figure 1 is an example of the prompt used in GPTNER, which consists of three parts:</p>
<h3>4.1.1 Task Description</h3>
<p>Task Description gives an overview of the task, which can be further decomposed into three components:
(1) the first sentence of the task description,
"I am an excellent linguist"
is a constant telling LLMs to produce the output using linguistic knowledge;
(2) The second sentence
"The task is to label [Entity Type] entities in the given sentence"
is a variable sentence indicating the category of entities to be extracted, [Entity Type] represents the type of entity to extract, e.g., Location in the example of Figure 1. It is worth noting that, in this way, for each input sentence, we need to iterate over all entity labels, which is equivalent to transforming an N-class classification task to N binary classification tasks. The reason behind this is as follows: for most current LLMs, e.g., GPT-3, there is a hard limit on the length of the prompt (e.g, 4096 tokens for GPT-3 ${ }^{3}$ ) due to the hardware restrictions. Given this limited number of tokens, it is impossible to include descriptions and demonstrations for all entity types in a single prompt. Therefore, for each input sentence, we construct the prompt $N$ times, each of which corresponds to each entity type;
(3) the third sentence
"Below are some examples"
marks the end of the description and points out the position of few-shot demonstrations.</p>
<h3>4.1.2 Few-shot Demonstration</h3>
<p>The few-shot demonstration is appended to the prompt. It serves as the following two purposes: (1) it regulates the format of the LLM outputs for each test input, as LLMs will (very likely) generate outputs that mimic the format of demonstrations. This is vital for the NER task as we need the output format to be consistent so that we can parse the output in the form of natural language to NER results; (2) it provides the LLM with direct evidence about the task and references to make predictions.</p>
<p>The demonstration sequentially packs a list of examples, where each example consists of both the input sequence $X$ and the output sequence $W$ :</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Input</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">Example</span><span class="w"> </span><span class="n">Sentence</span><span class="p">]</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="nl">Output</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">Labeled</span><span class="w"> </span><span class="n">Sentence</span><span class="p">]</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="err">・</span><span class="p">.</span>
<span class="nl">Input</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">Example</span><span class="w"> </span><span class="n">Sentence</span><span class="p">]</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="nl">Output</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">Labeled</span><span class="w"> </span><span class="n">Sentence</span><span class="p">]</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="p">{</span><span class="n">k</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>where $k$ denotes the number of demonstrations.
The Format of LLM Output. The format of each labeled sentence $W$, which is a text sequence, is of vital importance and should satisfy the following conditions: (1) it needs to contain the information for each word label, and can be easily transformed into the entity type sequence; (2) it needs</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">I am an excelent linquist. The task is to label location entities in the given sentence. Below are some examples</th>
<th style="text-align: center;">Task Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input: Only France and Britain backed Fischler's proposal. Example 1 Output: Only @@France## and @@Britain## backed Fischler's proposal .</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Input: Germany imported 47,600 sheep from Britain last year, nearly half of total imports. Example 2 Output: @@Germany## imported 47,600 sheep from @@Britain## last year, nearly half of total imports .</td>
<td style="text-align: center;">Few-shot Demonstrations</td>
</tr>
<tr>
<td style="text-align: center;">Input: It brought in 4275 tonnes of British mutton . some 10 percent of overall imports. Example 3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Output: It brought in 4275 tonnes of British mutton . some 10 percent of overall imports. Example 3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Input: China says Taiwan spoils atmosphere for talks. Output: @@China## says @@Taiwan## spoils atmosphere for talks.</td>
<td style="text-align: center;">Input Sentence</td>
</tr>
</tbody>
</table>
<p>Figure 1: The example of the prompt of GPT-NER. Suppose that we need to recognize location entities for the given sentence: China says Taiwan spoils atmosphere for talks. The prompt consists of three parts: (1) Task Description: It's surrounded by a red rectangle, and instructs the GPT-3 model that the current task is to recognize Location entities using linguistic knowledge. (2) Few-shot Demonstrations: It's surrounded by a yellow rectangle giving the GPT-3 model few-shot examples for reference. (3) Input Sentence: It's surrounded by a blue rectangle indicating the input sentence, and the output of the GPT-3 model is colored green.
to be smoothly and easily generated by LLMs to boost the models' final accuracy.</p>
<p>For illustration purposes, here we first give a few bad examples for the form of $W$ : for a given input sequence "Columbus is a city", "LOC O O O" is an intuitive format for $W$ which satisfies condition (1); But for condition (2), to generate "LOC O O O", the LLM first needs to learn the alignment between each position in the input sequence "Columbus is a city and each position in $W$ : Columbus to $L O C$, is to $O, a$ to $O$, city to $O$, which naturally adds up to the difficulty of the generation task. However, we find that it is difficult for GPT-3 to generate the output with the same length as the input sentence, especially when the input sentence is long.</p>
<p>To resolve this issue, we propose the LLM output takes the following format: if the input sequence does not contain any entity, $W$ just copies the input $X$; for an entity/entities in the input sequence, we use special tokens "@@" and "##" to surround it/them. The following is an example to extract LOC entities:</p>
<p>Input: Columbus is a sailor $<em 1="1">{1}$
Output: Columbus is a sailor $</em>$
Input: Columbus is a city $<em 2="2">{2}$
Output: @@Columbus## is a city $</em>$
The proposed strategy significantly bridges the gap between the format of the sequence labeling task and the generation model: it significantly decreases the difficulty in the generated text that fully encodes label information, as the LLM only needs to mark the position for entities and make copies for all
the rest. As we will show in the ablation study section 6.1, the proposed strategy yields significant performance boosts over other formats.</p>
<h3>4.1.3 Input Sentence</h3>
<p>This part feeds the current input sentence into the LLM and expects the LLM to generate the output sequence according to the defined format in Sec 4.1.2, which is:</p>
<p>Input: [The Input Sentence]
Output:
where "Ouput:" denotes the flag that the LLM begins to generate the labeled sequence.</p>
<p>Shown in the bottom of Figure 1, given the input sentence "China says Taiwan spoils atmosphere for talks", the LLM ${ }^{4}$ generates the labeled sentence "@@China## says @@Taiwan## spoils atmosphere for talks" with the same format as the former demonstrations, where the two words "China" and "Taiwan" is the recognized Location entity.</p>
<p>Here comes the end of the prompt construction.</p>
<h3>4.2 Few-shot Demonstrations Retrieval</h3>
<p>Here we describe strategies to retrieve demonstration examples.</p>
<h3>4.2.1 Random Retrieval</h3>
<p>The most straightforward strategy is randomly select $k$ examples from the training set. The shortcoming is obvious: there is no guarantee that retrieved examples are semantically close to the input.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: An example of the approach entity-level embedding to retrieve few-shot demonstrations. Supposed that we need to retrieve few-shot demonstrations for the input sentence "<em>Obama lives in Washington</em>" with the defined <em>LOC</em> entity in the prompt. <strong>Step 1 Datastore Construction</strong>: We first use the fine-tuned NER model to extract entities for each sentence in the training set, and formulate them as <em>(key, value)</em> pairs, where <em>key</em> is the extracted entity and <em>value</em> is the corresponding sentence. Then we concatenate all the formulated <em>(key, value)</em> pairs to construct the datastore. <strong>Step 2 Representation Extraction</strong>: First the fine-tuned NER model is utilized to embed the input sentence into a sequence of high-dimensional vectors. Then the embedded high-dimensional vectors are classified into labels according to a softmax layer, where "<em>Obama</em>" and "<em>Washington</em>" are two recognized entities. <strong>Step 3</strong> <em>k</em>NN Search: The embedding of the extracted <em>LOC</em> entity "<em>Washington</em>" is used as the query to find <em>k</em> nearest neighbors in the datastore, and the retrieved sentences are viewed as <em>k</em> few-shot demonstrations.</p>
<h3>4.2.2 <em>k</em>NN-based Retrieval</h3>
<p>To resolve the relatedness issue in Sec 4.2.1, we can retrieve <em>k</em> nearest neighbor (<em>k</em>NN) of the input sequence from the training set (Vilar et al., 2022; Liu et al., 2021): we first compute representations for all training examples, based on which we obtain the <em>k</em> nearest neighbors for an input test sequence.</p>
<p><em>k</em>NN based on Sentence-level Representations. To find <em>k</em>NN examples in the training set, one straightforward method is to use text similarity models such as SimCSE (Gao et al., 2021): we first obtain sentence-level representations for training examples and the input sequence, and use cosine similarity to find <em>k</em>NN.</p>
<p>The shortcoming of <em>k</em>NN based on sentence-level representations is obvious: NER is a token-level task that focuses more on local evidence rather than a sentence-level task, which is concerned with sentence-level semantics. A retrieved sentence (e.g., <em>he is a soldier</em>) that is semantically similar to the input (e.g., <em>John is a soldier</em>) might shed no light on the NER the input contains: in the example above, the retrieved sentence contains no NER and thus provides no evidence for tagging the input.</p>
<p>Entity-level Embedding. To resolve the issue above, we need to retrieve <em>k</em>NN examples based on token-level representations rather than sentence-level representations. We first extract entity-level representations for all tokens of all training examples as the datastore using a fine-tuned NER tagging model. For a given input sequence with length <em>N</em>, we first iterate over all tokens within the sequence to find <em>k</em>NNs for each token, obtaining <em>K</em> × <em>N</em> retrieved tokens. Next, we select the top <em>k</em> tokens from the <em>K</em> × <em>N</em> retrieved tokens, and use their associated sentences as demonstrations. We select several examples to better illustrate demonstrations of three retrieval strategies in Appendix C.</p>
<h3>4.3 Self-verification</h3>
<p>LLMs significantly suffer from the <em>hallucination</em> or overprediction issue (Braverman et al., 2020; Jiang et al., 2021; Zhao et al., 2021). Specifically for NER, LLMs have a strong inclination to overconfidently label NULL inputs as entities, even with demonstrations. The following is an example</p>
<p>of overprediction:</p>
<h2>Prompt:</h2>
<p>I am an excellent linguist. The task is to label location entities in the given sentence.
Below are some examples.
Input:Columbus is a city
Output:@@Columbus## is a city
Input:Rare Hendrix song sells for $\$ 17$
Output:</p>
<h2>GPT-3 Output:</h2>
<p>Rare @@Hendrix## song sells for $\$ 17$
where "Hendrix" is recognized as a location entity by the GPT-3, which is obviously incorrect. To address this issue, we propose the self-verification strategy. Given an extracted entity by LLMs, we ask the LLM to further verify whether the extracted entity is correct, answered by yes or no.</p>
<p>We construct the prompt for self-verification shown in Figure 3. Take the extraction of location entities as an example. The prompt starts with the task description:
"The task is to verify whether the word is a location entity extracted from the given sentence".</p>
<p>Again, we need few-shot demonstrations to boost the accuracy of the self-verifier. Shown in the yellow rectangle in Figure 3, each demonstration consists of three lines:
(1) "The input sentence: Only France and Britain backed Fischler's proposal",
(2) "Is the word "France" in the input sentence a location entity? Please answer with yes or no". (3) Yes.</p>
<p>We pack multiple demonstrations in the prompt in the few-shot setup. Demonstrations are followed by the test example, and fed to the LLM to obtain the output.</p>
<p>Demonstration Selection. We need to select demonstrations for the few-shot self-verification. Since the center of the self-verification task is asking about whether an extracted entity is a specific entity type, we need to select training examples that are semantic to the extracted entity rather than overall sentence-level semantics.</p>
<p>Therefore, we use the entity-level embedding described in Sec 4.2.2 for $k \mathrm{NN}$ demonstration search rather than sentence-level representations: (1) firstly, we construct the datastore by extracting
entity-level representations for all training examples using a fine-tuned NER model; (2) then, we use the same fine-tuned NER model to extract representation for the queried word; (3) finally, we use the representation of the queried word to select $k$ examples from the datastore as few-shot demonstrations, whose answer is "Yes" if the retrieved entity belongs to the queried entity type, and "no" otherwise.</p>
<h2>5 Experiments</h2>
<p>We use GPT-3 (Brown et al., 2020) (davinci-003) as the LLM backbone for all experiments. For davinci003 parameters, we set the maximum output length to 512 tokens. Temperature is set to 0 , top_p to 1 , frequency_penalty to 0 , presence_penalty to 0 , and best_of to 1 .</p>
<h3>5.1 Results on the Full Training Set</h3>
<h3>5.1.1 Results on Flat NER</h3>
<p>For flat NER, entities can't overlap with each other. We conduct experiments on the two widelyused flat-NER datasets, English CoNLL2003 and OntoNotes5.0, using span-level precision, recall, and F1 score as evaluation metrics.</p>
<p>CoNLL2003. CoNLL2003 (Sang and De Meulder, 2003) is an English NER dataset containing four types of named entities: Location, Organization, Person, and Miscellaneous, and we followed protocols in Li et al. (2019a); Ma and Hovy (2016) to process the data.</p>
<p>OntoNotes5.0. OntoNotes5.0 (Pradhan et al., 2013) is an English NER dataset containing 18 types of named entities: 11 types (e.g., Person, Organization) and 7 values (e.g., Date, Percent). More details (including entity types, sentence numbers, and examples) of the two flat NER datasets are shown in Appendix A.1.</p>
<p>Due to the fact that accessing davinci-003 can be expensive, in addition to the full test set, we randomly selected 100 test instances to make it easier for the community to replicate our results. We report performances on both the full and the partial test sets.</p>
<p>Baselines. We adopt currently widely-used NER systems as baselines including:</p>
<ul>
<li>BERT-Tagger (Devlin et al., 2018) fine-tunes BERT on the full training dataset.</li>
</ul>
<p>| 1 am an excellent linguistic. The task is to verify whether the word is a Location entity extracted from the given sentence. | Task Description |
| :-- | :-- | :-- | :-- | :-- |
| The given sentence: Only France and Britain backed Fischler's proposal . Example 1 |  |
| Is the word "Britain" in the given sentence a Location entity? Please answer with yes or no. |  |
| Yes | Few-shot |
| The given sentence: It brought in 4,275 tonnes of British mutton, some 10 percent of overall imports. Example 2 |  |
| Is the word "British" in the given sentence a Location entity? Please answer with yes or no. |  |
| No |  |</p>
<p>The given sentence: Rare Hendrix song sells for $\$ 17$
Is the word "Hendrix" in the given sentence a Location entity? Please answer with yes or no.
Input Sentence
No</p>
<p>Figure 3: The example of the prompt of verification using the GPT-3. Supposed that we need to verify whether the word "Hendrix" in the given sentence "Rare Hendrix song sells for $\$ 17$ " is a Location entity. The prompt consists of three parts: (1) Task Description (Red Rectangle): It gives the definition of the current task: to discriminate whether the specified word in the given sentence belongs to Location entity. (2) Few-shot (Yellow Rectangle): It provides several examples for the GPT-3 to reference.(3) Input Sentence (Blue Rectangle): It indicates the current word that needs to be verified and the sentence it belongs to, and the output of the GPT-3 is colored green.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">English CoNLL2003 (Sampled 100)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Baselines (Supervised Model)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ACE+document-context (Wang et al., 2020)</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">98.28</td>
<td style="text-align: center;">98.04 (SOTA)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NER</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + random retrieval</td>
<td style="text-align: center;">88.18</td>
<td style="text-align: center;">78.54</td>
<td style="text-align: center;">83.08</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">90.47</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">92.68</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + entity-level embedding</td>
<td style="text-align: center;">94.06</td>
<td style="text-align: center;">96.54</td>
<td style="text-align: center;">95.3</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (zen-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">88.95</td>
<td style="text-align: center;">79.73</td>
<td style="text-align: center;">84.34</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">91.77</td>
<td style="text-align: center;">96.36</td>
<td style="text-align: center;">94.01</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">94.15</td>
<td style="text-align: center;">96.77</td>
<td style="text-align: center;">95.46</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (few-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">90.04</td>
<td style="text-align: center;">80.14</td>
<td style="text-align: center;">85.09</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">92.92</td>
<td style="text-align: center;">95.45</td>
<td style="text-align: center;">94.17</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">94.73</td>
<td style="text-align: center;">96.97</td>
<td style="text-align: center;">95.85</td>
</tr>
<tr>
<td style="text-align: center;">English OntoNotes5.0 (Sampled 100)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Baselines (Supervised Model)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERT-MRC+DSC (Li et al., 2019b)</td>
<td style="text-align: center;">93.81</td>
<td style="text-align: center;">93.95</td>
<td style="text-align: center;">93.88 (SOTA)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NER</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + random retrieval</td>
<td style="text-align: center;">64.21</td>
<td style="text-align: center;">65.51</td>
<td style="text-align: center;">64.86</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">76.08</td>
<td style="text-align: center;">83.06</td>
<td style="text-align: center;">79.57</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + entity-level embedding</td>
<td style="text-align: center;">78.38</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">81.14</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (zen-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">64.94</td>
<td style="text-align: center;">65.90</td>
<td style="text-align: center;">65.42</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">77.33</td>
<td style="text-align: center;">83.29</td>
<td style="text-align: center;">80.31</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">79.05</td>
<td style="text-align: center;">83.71</td>
<td style="text-align: center;">81.38</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (few-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">65.21</td>
<td style="text-align: center;">66.25</td>
<td style="text-align: center;">65.73</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">77.64</td>
<td style="text-align: center;">83.22</td>
<td style="text-align: center;">80.43</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">79.25</td>
<td style="text-align: center;">83.73</td>
<td style="text-align: center;">81.49</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of sampled 100 pieces of data for two Flat NER datasets: CoNLL2003 and OntoNotes5.0.</p>
<ul>
<li>MRC-NER (Li et al., 2019a) formulates the NER task as a machine reading comprehension (MRC) task and trains the MRC-NER model on the full training dataset.</li>
<li>MRC-NER+DSC (Li et al., 2019b) is the</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">English CoNLL2003 (FULL)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Baselines (Supervised Model)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERT-Tagger (Devlin et al., 2018)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.8</td>
</tr>
<tr>
<td style="text-align: center;">BERT-MRC (Li et al., 2019a)</td>
<td style="text-align: center;">92.33</td>
<td style="text-align: center;">94.61</td>
<td style="text-align: center;">93.04</td>
</tr>
<tr>
<td style="text-align: center;">GNN-SL (Wang et al., 2022)</td>
<td style="text-align: center;">93.02</td>
<td style="text-align: center;">93.40</td>
<td style="text-align: center;">93.2</td>
</tr>
<tr>
<td style="text-align: center;">ACE+document-context (Wang et al., 2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">94.6 (SOTA)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NER</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + random retrieval</td>
<td style="text-align: center;">77.04</td>
<td style="text-align: center;">68.69</td>
<td style="text-align: center;">72.62</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">81.04</td>
<td style="text-align: center;">88.00</td>
<td style="text-align: center;">84.36</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + entity-level embedding</td>
<td style="text-align: center;">88.54</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">89.97</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (zen-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">77.13</td>
<td style="text-align: center;">69.23</td>
<td style="text-align: center;">73.18</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">83.31</td>
<td style="text-align: center;">88.11</td>
<td style="text-align: center;">85.71</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">89.47</td>
<td style="text-align: center;">91.77</td>
<td style="text-align: center;">90.62</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (few-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">77.50</td>
<td style="text-align: center;">69.38</td>
<td style="text-align: center;">73.44</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">83.73</td>
<td style="text-align: center;">88.07</td>
<td style="text-align: center;">85.9</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">89.76</td>
<td style="text-align: center;">92.06</td>
<td style="text-align: center;">90.91</td>
</tr>
<tr>
<td style="text-align: center;">English OntoNotes5.0 (FULL)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">Baselines (Supervised Model)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERT-Tagger (Devlin et al., 2018)</td>
<td style="text-align: center;">90.01</td>
<td style="text-align: center;">88.35</td>
<td style="text-align: center;">89.16</td>
</tr>
<tr>
<td style="text-align: center;">BERT-MRC (Li et al., 2019a)</td>
<td style="text-align: center;">92.98</td>
<td style="text-align: center;">89.95</td>
<td style="text-align: center;">91.11</td>
</tr>
<tr>
<td style="text-align: center;">GNN-SL (Wang et al., 2022)</td>
<td style="text-align: center;">91.48</td>
<td style="text-align: center;">91.29</td>
<td style="text-align: center;">91.59</td>
</tr>
<tr>
<td style="text-align: center;">BERT-MRC+DSC (Li et al., 2019b)</td>
<td style="text-align: center;">91.59</td>
<td style="text-align: center;">92.56</td>
<td style="text-align: center;">92.07 (SOTA)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-NER</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + random retrieval</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">64.36</td>
<td style="text-align: center;">61.58</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">71.87</td>
<td style="text-align: center;">78.77</td>
<td style="text-align: center;">75.32</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 + entity-level embedding</td>
<td style="text-align: center;">79.17</td>
<td style="text-align: center;">84.29</td>
<td style="text-align: center;">81.73</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (zen-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">59.14</td>
<td style="text-align: center;">64.44</td>
<td style="text-align: center;">61.79</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">72.29</td>
<td style="text-align: center;">78.81</td>
<td style="text-align: center;">75.55</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">79.64</td>
<td style="text-align: center;">84.52</td>
<td style="text-align: center;">82.08</td>
</tr>
<tr>
<td style="text-align: center;">Self-verification (few-shot)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + random retrieval</td>
<td style="text-align: center;">59.23</td>
<td style="text-align: center;">64.65</td>
<td style="text-align: center;">61.94</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + sentence-level embedding</td>
<td style="text-align: center;">72.35</td>
<td style="text-align: center;">78.79</td>
<td style="text-align: center;">75.57</td>
</tr>
<tr>
<td style="text-align: center;">+ GPT-3 + entity-level embedding</td>
<td style="text-align: center;">79.89</td>
<td style="text-align: center;">84.51</td>
<td style="text-align: center;">82.20</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of full data for two Flat NER datasets: CoNLL2003 and OntoNotes5.0.
current SOTA model on the OntoNotes5.0 dataset, leveraging dice loss in replacement of</p>
<p>the standard cross-entropy loss during training.</p>
<ul>
<li>GNN-SL (Wang et al., 2022) fine-tunes RoBERTa (Liu et al., 2019) on the full training dataset and using GNN to refer to the whole training examples at test time.</li>
<li>ACE+document-context (Wang et al., 2020) is the current SOTA model on the CoNLL2003 dataset, optimizing the controller to find better concatenations of embeddings on the full training dataset.</li>
</ul>
<p>Main Results. Table 1 and Table 2 respectively show results on the partial and the full test set for flat NER. Observations are as follows:
(1) $k$ NN retrieval is of vital importance for the NER task. For the random retrieval strategy where demonstrations are randomly selected rather than through $k$ NN search, performances are only 72.62 and 61.58 on the full CoNLL2003 and OntoNotes5.0 sets. Results skyrocket to 84.36 and 75.32 on the full CoNLL2003 and OntoNotes5.0 when sentence-level embeddings are used for the $k$ NN demonstration retrieval.
(2) We observe a significant improvement by changing the sentence-level embedding to tokenlevel embedding for the $k$ NN demonstration search: 84.36 v.s. 89.97 on CoNLL2003 dataset and 75.32 v.s. 81.73 on OntoNotes5.0. This phenomenon is because NER is a token-level task that focuses more on local evidence rather than a sentence-level task: the two sentences "he is a soldier" and "John is a soldier" are semantically similar but don't share any identical entities. Using token-level representation for the $k$ NN search help retrieve more similar demonstrations with respect to the specific entity type, leading to better performances.
(3) We observe further improvements by adding self-verification: on the full CoNLL2003 dataset with entity-level embedding, 89.97 v.s. 90.62 respectively for without and with self-verification for zero-shot learning and 84.97 v.s. 85.91 for fewshot learning. The results prove the effectiveness of self-verification in alleviating overprediction of the GPT-3.
(4) LLM-based systems obtain comparable results to supervised baselines using BERT, i.e., 90.91 v.s. 92.8 on the full CoNLL2003 dataset and 82.20 v.s. 89.16 on the full OntoNotes5.0 dataset. We observe that there still remains a gap between the supervised SOTA model: 94.6 v.s. 90.91 on the full CoNLL2003 dataset and 92.07 v.s. 82.20 on the full OntoNotes5.0 dataset. As will be shown in the ablation study section, we find that the performance hasn't plateaued when we reach the GPT-3 token limit with respect to the number of KNN demonstrations. This means that the token limit is released, e.g., using GPT-4 whose token limit is more than 20 K tokens, there is still room for improvement. We will update performances when GPT-4 API is accessible.</p>
<h3>5.1.2 Results on Nested NER</h3>
<p>For nested NER, entities in each sentence may overlap with each other, as in the following example:</p>
<p>Sentence: The Chinese embassy in France
Geographical Political Entities: Chinese, France
Facility Entities: The Chinese embassy in France
where the two geographical political entities "Chinese" and "France" overlap with the facility entity "The Chinese embassy in France".</p>
<p>We conduct experiments on the three widelyused nested NER datasets: ACE2004, ACE2005 and GENIA, and use span-level precision, recall, and F1 score for evaluation.</p>
<p>ACE2004 and ACE2005. ACE2004 (Doddington et al., 2004) and ACE2005 (Christopher et al., 2006) contain seven types of entities (e.g., organization entities and person entities). We follow the commonly adopted protocols in Katiyar and Cardie (2018) to process the two datasets by dividing them into train, dev, and test sets in an 8:1:1 ratio.</p>
<p>GENIA. GENIA (Ohta et al., 2002) is an English nested NER dataset in the molecular biology domain containing five entity types (e.g., DNA and RNA). More details (including entity types, sentence number, and examples) of three nested NER datasets ACE2004, ACE2005, and GENIA can be found in Appendix A.2.</p>
<p>Baselines. For baselines, four widely used supervised models are included:</p>
<ul>
<li>BERT-MRC (Li et al., 2019a): the current SOTA model on the GENIA dataset, formulating the NER task as a machine reading comprehension (MRC) task and training the MRCNER model on the full training dataset.</li>
<li>Triaffine+BERT (Yuan et al., 2021): finetuning BERT (Devlin et al., 2018) on the full</li>
</ul>
<p>training set and fusing heterogeneous factors for span representations and classification.</p>
<ul>
<li>Triaffine+ALBERT (Yuan et al., 2021): finetuning ALBERT (Lan et al., 2019) on the full training set and fusing heterogeneous factors for span representations and classification.</li>
<li>BINDER (Zhang et al., 2022): the current SOTA model on the ACE2004 dataset and ACE2005 dataset, leveraging a bi-encoder framework to apply contrastive learning to map candidate text spans and entity types into the same vector representation space for representation and classification.</li>
</ul>
<p>Main Results. Results are shown in Table 3, and phenomenon is similar to flat NER is observed:
(1) Again, $k \mathrm{NN}$ retrieval is of vital importance: on the full ACE2004 dataset, 48.4 for random retrieval v.s. 73.62 for entity-level embedding retrieval using KNN search.
(2) For the $k \mathrm{NN}$ demonstration search, a significant improvement is observed by changing the sentence-level embedding to entity-level embedding: 60.68 v.s. 73.62 on ACE2004 and 56.68 v.s. 69.06 on GENIA.
(3) Further performance boost is obtained by adding self-verification, i.e., on the full ACE2004 dataset with sentence-level embedding, 60.68 v.s. 62.31 for zero-shot learning and 60.68 v.s. 62.52 for few-shot learning.</p>
<p>We also observe that the gap between GPT-NER and SOTA models is greater than flat NER. This is because:
(1) Nested NER datasets contain more similar entity types, e.g., the location entities (LOC) and the geographical entities (GPE). Since only a limited number of demonstrations is allowed, it is harder for GPT-3 to distinguish between them,
(2) The annotation guidelines for the three nested NER datasets are more complex and less straightforward. For example, the substring of "The bodies of six people" within the sentence "The bodies of six people were found in the region" is annotated as a person entity. It is easier for a supervised model fine-tuned on the full training set to learn these complex rules, while much harder for an LLM model with a limited number of demonstrations.</p>
<h3>5.2 Results on Low-resource Scenario</h3>
<p>We conduct experiments to estimate the performance of GPT-NER in low resource setups on the</p>
<table>
<thead>
<tr>
<th>ACE2004 (FULL)</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Model</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
</tr>
<tr>
<td>Baselines (Supervised Model)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BERT-MRC (Li et al., 2019a)</td>
<td>85.05</td>
<td>86.32</td>
<td>85.98</td>
</tr>
<tr>
<td>Triaffine+BERT (Yuan et al., 2021)</td>
<td>87.13</td>
<td>87.68</td>
<td>87.40</td>
</tr>
<tr>
<td>Triaffine+ALBERT (Yuan et al., 2021)</td>
<td>88.88</td>
<td>88.24</td>
<td>88.56</td>
</tr>
<tr>
<td>BINDER (Zhang et al., 2022)</td>
<td>88.3</td>
<td>89.1</td>
<td>88.7 (SOTA)</td>
</tr>
<tr>
<td>GPT-NER</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>55.04</td>
<td>41.76</td>
<td>48.4</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>65.31</td>
<td>53.67</td>
<td>60.68</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>72.23</td>
<td>75.01</td>
<td>73.62</td>
</tr>
<tr>
<td>Self-verification (zero-shot)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>55.44</td>
<td>42.22</td>
<td>48.83</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>69.64</td>
<td>54.98</td>
<td>62.31</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>73.58</td>
<td>74.74</td>
<td>74.16</td>
</tr>
<tr>
<td>Self-verification (few-shot)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>55.63</td>
<td>42.49</td>
<td>49.06</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>70.17</td>
<td>54.87</td>
<td>62.52</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>73.29</td>
<td>75.11</td>
<td>74.2</td>
</tr>
<tr>
<td>ACE2005 (FULL)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
</tr>
<tr>
<td>Baselines (Supervised Model)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Triaffine+BERT (Yuan et al., 2021)</td>
<td>86.70</td>
<td>86.94</td>
<td>86.82</td>
</tr>
<tr>
<td>BERT-MRC (Li et al., 2019a)</td>
<td>87.16</td>
<td>86.59</td>
<td>86.88</td>
</tr>
<tr>
<td>Triaffine+ALBERT (Yuan et al., 2021)</td>
<td>87.39</td>
<td>90.31</td>
<td>88.83</td>
</tr>
<tr>
<td>BINDER (Zhang et al., 2022)</td>
<td>89.1</td>
<td>89.8</td>
<td>89.5 (SOTA)</td>
</tr>
<tr>
<td>GPT-NER</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>45.5</td>
<td>46.24</td>
<td>45.37</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>58.04</td>
<td>58.97</td>
<td>58.50</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>71.72</td>
<td>74.2</td>
<td>72.96</td>
</tr>
<tr>
<td>Self-verification (zero-shot)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>45.06</td>
<td>46.62</td>
<td>45.84</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>59.49</td>
<td>60.17</td>
<td>59.83</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>72.63</td>
<td>75.39</td>
<td>73.46</td>
</tr>
<tr>
<td>Self-verification (few-shot)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>45.49</td>
<td>46.73</td>
<td>46.11</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>59.69</td>
<td>60.35</td>
<td>60.02</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>72.77</td>
<td>75.51</td>
<td>73.59</td>
</tr>
<tr>
<td>GENIA (FULL)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
</tr>
<tr>
<td>Baselines (Supervised Model)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Triaffine+BERT (Yuan et al., 2021)</td>
<td>80.42</td>
<td>82.06</td>
<td>81.23</td>
</tr>
<tr>
<td>BERT-MRC (Li et al., 2019a)</td>
<td>85.18</td>
<td>81.12</td>
<td>83.75 (SOTA)</td>
</tr>
<tr>
<td>GPT-NER</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>44.1</td>
<td>38.64</td>
<td>41.37</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>63.43</td>
<td>44.17</td>
<td>51.68</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>61.38</td>
<td>66.74</td>
<td>64.06</td>
</tr>
<tr>
<td>Self-verification (zero-shot)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>44.31</td>
<td>38.79</td>
<td>41.55</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>59.54</td>
<td>44.26</td>
<td>51.9</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>61.77</td>
<td>66.81</td>
<td>64.29</td>
</tr>
<tr>
<td>Self-verification (few-shot)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-3 + random retrieval</td>
<td>44.68</td>
<td>38.98</td>
<td>41.83</td>
</tr>
<tr>
<td>GPT-3 + sentence-level embedding</td>
<td>59.87</td>
<td>44.39</td>
<td>52.13</td>
</tr>
<tr>
<td>GPT-3 + entity-level embedding</td>
<td>61.89</td>
<td>66.95</td>
<td>64.42</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of full data for three Nested NER datasets: ACE2004, ACE2005 and GENIA.</p>
<p>English CoNLL2003 dataset. In order to mimic the low-resource scenario, we randomly select a subset of the full training data as the training set: (a) 8 training sentences $(0.063 \%)$; (b) 100 training sentences $(0.788 \%)$; (c) 1 K sentences $(7.880 \%)$;</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Low-resource comparisons on CoNLL2003 dataset.
and (d) 10 K sentences ( $78.808 \%$ ). For the setup with 8 training sentences, the dataset is constructed to ensure that each entity type contains one positive and one negative example. Evaluations are performed on the full test set.</p>
<p>Setups. We use the same GPT parameters as in Sec 5. For baselines, we train the ACE model (Wang et al., 2020) (which is the current SOTA model) on different training subsets. For GPTNER, we use random demonstration retrieval and sentence-level embedding-based demonstration retrieval for demonstration selection in the few-shot learning stage. For the self-verification stage, we only use zero-shot learning where no demonstration is needed.</p>
<h3>5.2.1 Results</h3>
<p>Results are shown in Figure 4. Observations are as follows:
(1) When the size of the training set is extremely small (i.e., 8 or 100 sentences), and the performance of the supervised model is far below GPT-3. Specifically, with only 8 training examples, the F1 score of GPT-NER is already about 60 while the performance of supervised models is around 0 . This demonstrates the significantly better generalization ability of GPT-NER over supervised baselines in the low-resource setup.
(2) With the increase of the training data, the performance of KNN search grows faster than random retrieval, which is in accord with our expectations: for random retrieval, where all demonstrations are randomly selected, the impact of increasing the size of training data is minimal: the outcomes of selecting K demonstration from 100 and 1000 sets
are similar since they are all randomly selected. But for $k \mathrm{NN}$ demonstration search, increasing the size of training data means selected demonstrations are more likely to be related to the input, leading to better performances.
(3) When the amount of data reaches $10 \%$, as the size of training data increases, the performance of the supervised model will significantly improve, while the result of GPT-3 will increase marginally. This phenomenon indicates that for incontext learning, instead of focusing on increasing the amount of training data, it is more effective to focus on improving the quality of retrieved demonstrations (e.g., random retrieval to $k \mathrm{NN}$ based retrieval) and prompt structure (e.g., adding selfverification).</p>
<h2>6 Ablation Study</h2>
<h3>6.1 Varying the Format of LLM Output</h3>
<p>In Sec 4.1.2, we propose to use special tokens "@@" and "##" to regulate the format of the GPT-3 output, e.g., "@@Columbus## is a city" indicates the word "Columbus" is the recognized entity. We compare the proposed output format with the following two formats:</p>
<p>BMES directly outputs the beginning, middle, end, and singleton indicator for each token within the input:</p>
<p>Input:White House is in Washington
Output:B-ORG E-ORG O O O
Entity+Position asks LLMs to output the entity within the sentence along with its position:</p>
<p>Input:White House is in Washington
Output:White House (0)
where "White House (0)" means that "White House" is an entity and its starting position is 0 at the input sentence.</p>
<p>To enable apple-to-apple comparisons, we use the same setup for the three output formats and conduct experiments on the 100-sample CoNLL 2003 dataset with 32 few-shots.</p>
<p>The F1-score for the proposed ##@@ strategy, BMES and Entity+Position are respective 92.68, 29.75 and 38.73, where BMES and Entity+Position significantly underperform the proposed ##@@ strategy. Explanations are as follows: for the BMES strategy, the LLM needs to learn the alignment between each input word and each BMES</p>
<p>label: White to B-ORG, House to E-ORG, is to $O$, in to $O$, Washington to $O$. By analyzing the error samples, we find that it is usually even hard for the LLM to output a BMES string with the correct length, especially when the input sentence is long, leading to poor final evaluation performances.</p>
<p>For the Entity+Position strategy, we find that the LLM usually confuses the meaning of the of position index (e.g., whether it is character index or word index), leading to incorrect entity position. This problem can be partially alleviated by demonstrations but still exists considering the 4096 token limit for GPT-3. Incorrect position indexes make it hard to map the output text to the sequence labeling evaluation format, leading to poor final evaluation performances.</p>
<h3>6.2 The Number of Few-shot Demonstrations</h3>
<p>We conduct experiments to estimate the effect of the number of demonstrations. Experiments are conducted on the 100-sample CoNLL 2003 dataset. Results are shown in Figure 2. We can observe as $k$ increases, all three LLM-based results keep rising. As we approach the 4096 token limit for demonstrations, the result still hasn't plateaued. This means performance will still rise if more demonstrations are allowed.</p>
<p>An interesting phenomenon is observed that when the number of demonstrations is small, i.e., $k=2,4, k$ NN-based strategies underperform the random retrieval strategy. The explanation is as follows: $k$ NN-based retrieval tends to select demonstrations that are very similar to the input sentence. Therefore, if the input sentence does not contain any entity, the retrieved demonstrations are most to contain no entity either. In this case, demonstrations do not contain the output format information we wish to enforce, leading LLMs to output arbitrary format. Here we give an example:</p>
<p>When the number of demonstrations is small and GPT is required to recognize a certain kind of entity (e.g., Location) but few-shot are all sentences without NER, GPT will be confused and output in its
own format, illustrated in the following example:</p>
<h2>Prompt:</h2>
<p>I am an excellent linguist. The task is to label organization entities in the given sentence. Below are some examples.
Input:Korean pro-soccer games
Output:Korean pro-soccer games
Input:Australia defend the Ashes
Output:Australia defend the Ashes
Input:Japan get lucky win
Output:</p>
<h2>GPT-3 Output:</h2>
<p>Japan [Organization Entity] get lucky win
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Comparisons by varying $k$-shot demonstrations.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we propose GPT-NER to adapt LLMs to the NER task. To bridge the gap between the sequence labeling task and the text generation task, we instruct the LLM to generate a labeled sequence by surrounding entities with special tokens. Additionally, we propose a self-verification strategy to alleviate the hallucination issue of the LLM model. We conduct experiments on both flat and nested NER datasets, and achieve comparable performances to fully supervised baselines. Besides that, we find that GPT-NER has a remarkable ability in the low-resource scenario, that when the amount of training data is extremely scarce, the results of GPT-NER are significantly better than that of the supervised model.</p>
<h2>References</h2>
<p>Mark Braverman, Xinyi Chen, Sham Kakade, Karthik Narasimhan, Cyril Zhang, and Yi Zhang. 2020. Calibration, entropy rates, and memory in language models. pages 1089-1099.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Jason PC Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional lstm-cnns. Transactions of the association for computational linguistics, $4: 357-370$.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Walker Christopher, Strassel Stephanie, Medero Julie, and Maeda Kazuaki. 2006. Ace 2005 multilingual training corpus.</p>
<p>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of machine learning research, 12(ARTICLE):2493-2537.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>George R Doddington, Alexis Mitchell, Mark A Przybocki, Lance A Ramshaw, Stephanie M Strassel, and Ralph M Weischedel. 2004. The automatic content extraction (ace) program-tasks, data, and evaluation. 2(1).</p>
<p>Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. pages 5547-5569.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. pages 3929-3938.</p>
<p>James Hammerton. 2003. Named entity recognition with long short-term memory. pages 172-175.</p>
<p>Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. 2022. Tabllm: Few-shot classification of tabular data with large language models. arXiv preprint arXiv:2210.10723.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. 9:962-977.</p>
<p>Arzoo Katiyar and Claire Cardie. 2018. Nested named entity recognition revisited. 1.</p>
<p>Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.</p>
<p>Junlong Li, Zhuosheng Zhang, and Hai Zhao. 2022. Self-prompting large language models for opendomain qa. arXiv preprint arXiv:2212.08635.</p>
<p>Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. 2019a. A unified mrc framework for named entity recognition. arXiv preprint arXiv:1910.11476.</p>
<p>Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. 2019b. Dice loss for data-imbalanced nlp tasks. arXiv preprint arXiv:1911.02855.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.</p>
<p>Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354.</p>
<p>Yasmin Moslem, Rejwanul Haque, and Andy Way. 2023. Adaptive machine translation with large language models. arXiv preprint arXiv:2301.13294.</p>
<p>Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima, and Junichi Tsujii. 2002. The genia corpus: An annotated research abstract corpus in molecular biology domain.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. Advances in neural information processing systems, 34:11054-11070.</p>
<p>Ben Pietrzak, Ben Swanson, Kory Mathewson, Monica Dinculescu, and Sherol Chen. 2021. Story centaur: Large language model few shot learning as a creative writing tool.</p>
<p>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using ontonotes.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910.</p>
<p>Joshua Robinson, Christopher Michael Rytting, and David Wingate. 2022. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.</p>
<p>Erik F Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. arXiv preprint cs/0306050.</p>
<p>Justyna Sarzynska-Wawer, Aleksander Wawer, Aleksandra Pawlak, Julia Szymanowska, Izabela Stefaniak, Michal Jarkiewicz, and Lukasz Okruszek. 2021. Detecting formal thought disorder by deep contextualized word representations. Psychiatry Research, 304:114135.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Blanca Vidal, Albert Llorens, and Juan Alonso. 2022. Automatic post-editing of mt output using large language models. pages 84-106.</p>
<p>David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102.</p>
<p>Shuhe Wang, Yuxian Meng, Rongbin Ouyang, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, and Guoyin Wang. 2022. Gnn-sl: Sequence labeling based on nearest examples via gnn. arXiv preprint arXiv:2212.02017.</p>
<p>Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, and Kewei Tu. 2020. Automated concatenation of embeddings for structured prediction. arXiv preprint arXiv:2010.05006.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Zheng Yuan, Chuanqi Tan, Songfang Huang, and Fei Huang. 2021. Fusing heterogeneous factors with triaffine mechanism for nested named entity recognition. arXiv preprint arXiv:2110.07480.</p>
<p>Sheng Zhang, Hao Cheng, Jianfeng Gao, and Hoifung Poon. 2022. Optimizing bi-encoder for named entity recognition via contrastive learning. arXiv preprint arXiv:2208.14565.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. pages $12697-12706$.</p>
<h2>A Datasets</h2>
<h2>A. 1 Flat NER</h2>
<p>CoNLL2003. CoNLL2003 (Sang and De Meulder, 2003) contains four types of named entities: Location, Organization, Person and Miscellaneous. Table 4 shows annotations for each entity type and Table 6 shows the number of sentences, tokens and entities in CoNLL2003.</p>
<p>OntoNotes5.0. OntoNotes (Pradhan et al., 2013) contains 18 types of named entities, and Table 5 lists each entity and its annotation. The number of sentences, tokens and entities of OntoNotes5.0 is shown in Table 6.</p>
<h2>A. 2 Nested NER</h2>
<p>ACE2004 and ACE2005. ACE2004 (Doddington et al., 2004) and ACE2005 (Christopher et al., 2006) are two English nested NER datasets containing seven entity types: geographical political entities (GPE), organization entities (ORG), person entities (PER), facility entities (FAC), vehicle entities (VEH), location entities (LOC) and weapon entities (WEA). Annotations for each entity type are shown in Table 8, and the number of sentences, entities, nested entities and nested percentages are shown in Table 7.</p>
<p>GENIA. GENIA is an English nested NER dataset within the molecular biology domain and contains five entity types: cell line, cell type, DNA, RNA and protein. Literally, each entity is named according to the biological meaning, and the number of sentences, entities, nested entities and nested percentages are shown in Table 7.</p>
<h2>B Error Cases of Format BMES and Entity-position</h2>
<p>We select several examples on sample-100 CoNLL2003 dataset to better illustrate the ineffectiveness of these two formats BMES and entityposition. For BMES format is shown in Table 9, and for entity-position format is shown in 10.</p>
<p>From these examples, we can obviously observe that (1) for BMES format, it is difficult for GPT3 to generate the output with the same length as the input sentence, especially when the input sentence is long; (2) for entity-position format, it is confused for GPT-3 to generate the correct position information.</p>
<table>
<thead>
<tr>
<th>Entities Annotations of English CoNLL2003</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Entity Type</td>
<td>Annotation</td>
</tr>
<tr>
<td>ORG</td>
<td>organization entities are limited to named corporate, governmental, or other organizational entities</td>
</tr>
<tr>
<td>PER</td>
<td>person entities are named persons or family</td>
</tr>
<tr>
<td>LOC</td>
<td>location entities are the name of politically or geographically defined locations such as cities, provinces, countries, international regions, bodies of water, mountains, etc</td>
</tr>
<tr>
<td>MISC</td>
<td>miscellaneous entities include events, nationalities, products and works of art</td>
</tr>
</tbody>
</table>
<p>Table 4: Entity annotations of the flat NER dataset CoNLL2003.</p>
<table>
<thead>
<tr>
<th>Entities Annotations of English OntoNotes5.0</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Entity Type</td>
<td>Annotation</td>
</tr>
<tr>
<td>PERSON</td>
<td>People, including fictional</td>
</tr>
<tr>
<td>NORP</td>
<td>Nationalities or religious or political groups</td>
</tr>
<tr>
<td>FAC</td>
<td>Buildings, airports, highways, bridges, etc</td>
</tr>
<tr>
<td>ORG</td>
<td>Companies, agencies, institutions, etc</td>
</tr>
<tr>
<td>GPE</td>
<td>Countries, cities, states</td>
</tr>
<tr>
<td>LOC</td>
<td>Non-GPE locations, mountain ranges, bodies of water</td>
</tr>
<tr>
<td>PRODUCT</td>
<td>Vehicles, weapons, foods, etc</td>
</tr>
<tr>
<td>EVENT</td>
<td>Named hurricanes, battles, wars, sports events, etc</td>
</tr>
<tr>
<td>WORK_OF_ART</td>
<td>Titles of books, songs, etc</td>
</tr>
<tr>
<td>LAW</td>
<td>Named documents made into laws</td>
</tr>
<tr>
<td>LANGUAGE</td>
<td>Any named language</td>
</tr>
<tr>
<td>DATE</td>
<td>Absolute or relative dates or periods</td>
</tr>
<tr>
<td>TIME</td>
<td>Times smaller than a day</td>
</tr>
<tr>
<td>PERCENT</td>
<td>Percentage (including "\%")</td>
</tr>
<tr>
<td>MONEY</td>
<td>Monetary values, including unit</td>
</tr>
<tr>
<td>QUANTITY</td>
<td>Measurements, as of weight or distance</td>
</tr>
<tr>
<td>ORDINAL</td>
<td>"first", "second", etc</td>
</tr>
<tr>
<td>CARDINAL</td>
<td>Numerals that do not fall under another type</td>
</tr>
</tbody>
</table>
<p>Table 5: Entity annotations of the flat NER dataset OntoNotes5.0.</p>
<table>
<thead>
<tr>
<th>Statistics on English CoNLL2003</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Sentences</td>
<td>Tokens</td>
<td>Entities</td>
</tr>
<tr>
<td>Training set</td>
<td>14,987</td>
<td>203,621</td>
<td>23,499</td>
</tr>
<tr>
<td>Development set</td>
<td>3,466</td>
<td>51,362</td>
<td>5,942</td>
</tr>
<tr>
<td>Test set</td>
<td>3,684</td>
<td>46,435</td>
<td>5,648</td>
</tr>
<tr>
<td>Statistics on OntoNotes5.0</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Dataset</td>
<td>Sentences</td>
<td>Tokens</td>
<td>Entities</td>
</tr>
<tr>
<td>Training set</td>
<td>59,924</td>
<td>1,088,503</td>
<td>81,828</td>
</tr>
<tr>
<td>Development set</td>
<td>8,528</td>
<td>147,724</td>
<td>11,066</td>
</tr>
<tr>
<td>Test set</td>
<td>8,262</td>
<td>152,728</td>
<td>11,257</td>
</tr>
</tbody>
</table>
<p>Table 6: Number of sentences, tokens and entities of the flat NER dataset English CoNLL2003 and OntoNotes5.0.</p>
<h2>C Examples</h2>
<p>To better illustrate demonstrations of our GPTNER, we select several examples for random retrieval in Table 11, for sentence-level embedding</p>
<table>
<thead>
<tr>
<th>Statistics on ACE2004</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Sentences</td>
<td>Entities</td>
<td>Nested Entities</td>
<td>Nested Percentage</td>
</tr>
<tr>
<td>Training set</td>
<td>6,200</td>
<td>22,204</td>
<td>10,149</td>
<td>$45.71 \%$</td>
</tr>
<tr>
<td>Development set</td>
<td>745</td>
<td>2,514</td>
<td>1,092</td>
<td>$46.69 \%$</td>
</tr>
<tr>
<td>Test set</td>
<td>812</td>
<td>3,035</td>
<td>1,417</td>
<td>$45.61 \%$</td>
</tr>
<tr>
<td>Statistics on ACE2005</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Dataset</td>
<td>Sentences</td>
<td>Entities</td>
<td>Nested Entities</td>
<td>Nested Percentage</td>
</tr>
<tr>
<td>Training set</td>
<td>7,194</td>
<td>24,441</td>
<td>9,389</td>
<td>$38.41 \%$</td>
</tr>
<tr>
<td>Development set</td>
<td>969</td>
<td>3,200</td>
<td>1,112</td>
<td>$34.75 \%$</td>
</tr>
<tr>
<td>Test set</td>
<td>1,047</td>
<td>2,993</td>
<td>1,118</td>
<td>$37.35 \%$</td>
</tr>
<tr>
<td>Statistics on GENIA</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Dataset</td>
<td>Sentences</td>
<td>Entities</td>
<td>Nested Entities</td>
<td>Nested Percentage</td>
</tr>
<tr>
<td>Training set</td>
<td>16,692</td>
<td>50,509</td>
<td>9,064</td>
<td>$17.95 \%$</td>
</tr>
<tr>
<td>Development set</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Test set</td>
<td>1,854</td>
<td>5,506</td>
<td>1,199</td>
<td>$21.78 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Number of sentences, entities, nested entities, and nested percentage of the nested NER dataset ACE2004, ACE2005 and GENIA.</p>
<p>12 and for entity-level embedding 13. From these results, we can observe that:
(1) For random retrieval in Table 11, we can observe that all sentences have the same opportunity to appear as an example in the few-shot demon-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entities Annotations of English ACE2004 and ACE2005</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entity Type</td>
<td style="text-align: left;">Annotation</td>
</tr>
<tr>
<td style="text-align: left;">GPE</td>
<td style="text-align: left;">geographical political entities are geographical regions defined by political and or social groups such as countries, nations, regions, cities, states, government and its people</td>
</tr>
<tr>
<td style="text-align: left;">ORG</td>
<td style="text-align: left;">organization entities are limited to companies, corporations, agencies, institutions and other groups of people</td>
</tr>
<tr>
<td style="text-align: left;">PER</td>
<td style="text-align: left;">a person entity is limited to human including a single individual or a group</td>
</tr>
<tr>
<td style="text-align: left;">FAC</td>
<td style="text-align: left;">facility entities are limited to buildings and other permanent man-made structures such as buildings, airports, highways, bridges</td>
</tr>
<tr>
<td style="text-align: left;">VEH</td>
<td style="text-align: left;">vehicle entities are physical devices primarily designed to move, carry, pull or push the transported object such as helicopters, trains, ship and motorcycles</td>
</tr>
<tr>
<td style="text-align: left;">LOC</td>
<td style="text-align: left;">location entities are limited to geographical entities such as geographical areas and landmasses, mountains, bodies of water, and geological formations</td>
</tr>
<tr>
<td style="text-align: left;">WEA</td>
<td style="text-align: left;">weapon entities are limited to physical devices such as instruments for physically harming such as guns, arms and gunpowder</td>
</tr>
</tbody>
</table>
<p>Table 8: Entity annotations of the dataset ACE2004 and ACE2005.
stration, and the input sentence and each retrieved example usually do not contain similar examples.
(2) For sentence-level embedding in Table 12, we can observe that the retrieved examples are semantically similar to the input sentence, but may not focus on the same local entities as the input sentence.
(3) For entity-level embedding in Table 13, we can observe that the retrieved examples do focus on the same local entities as the input sentence to lead the prediction progress of GPT-3 more easily. This phenomenon emphasizes the effectiveness of the quality of demonstrations in-context learning.</p>
<h1>Example 1 Length Error</h1>
<h2>Task Description</h2>
<p>I am an excellent linguist. The task is to label organization entities. Below are some examples.</p>
<h2>Sentence-level Demonstrations</h2>
<p>Input: Soccer - results of South Korean PRO-SOCCER games .
Output: O O O O O O O O O
Input: Soccer - results of South Korean PRO-SOCCER games .
Output: O O O O O O O O O
Input: Soccer - results of South Korean PRO-SOCCER games .
Output: O O O O O O O O O
Input: Soccer - Italian cup second round results .
Output: O O O O O O O O</p>
<h2>Input Sentence and GPT-3 Output</h2>
<p>Input: Soccer - Japan get lucky win, China in surprise defeat .
Output: O O O O O O O O O O O
Expected Output: O O O O O O O O O O O</p>
<h2>Example 2 Length Error and Entity Error</h2>
<h2>Task Description</h2>
<p>I am an excellent linguist. The task is to label miscellaneous entities. Below are some examples.</p>
<h2>Sentence-level Demonstrations</h2>
<p>Input: Soccer - results of South Korean PRO-SOCCER games .
Output: O O O O B-MISC E-MISC O O O
Input: Soccer - results of South Korean PRO-SOCCER games .
Output: O O O O B-MISC E-MISC O O O
Input: Soccer - results of South Korean PRO-SOCCER games .
Output: O O O O B-MISC E-MISC O O O
Input: Soccer - Italian cup second round results .
Output: O O B-MISC E-MISC O O O O
Input Sentence and GPT-3 Output
Input: Soccer - Japan get lucky win, China in surprise defeat .
Output: O O B-MISC E-MISC O O O O B-MISC E-MISC O O
Expected Output: O O O O O O O O O O O</p>
<h2>Example 3 Length Error</h2>
<h2>Task Description</h2>
<p>I am an excellent linguist. The task is to label person entities. Below are some examples.</p>
<h2>Sentence-level Demonstrations</h2>
<p>Input: Dubai 1996-08-26
Output: O O
Input: Dubai 1996-08-29
Output: O O
Input: Dubai 1996-08-29
Output: O O
Input: Dubai 1996-08-22
Output: O O
Input Sentence and GPT-3 Output
Input: AL-AIN, United Arab Emirates 1996-12-06
Output: O O
Expected Output: O O O O O O</p>
<h2>Example 4 Length Error and Entity Error</h2>
<h2>Task Description</h2>
<p>I am an excellent linguist. The task is to label location entities. Below are some examples.</p>
<h2>Sentence-level Demonstrations</h2>
<p>Input: Azerbaijan beat Switzerland 1-0 ( halftime 1-0 ) in their World Cup soccer European group three qualifying match on Saturday .
Output: S-LOC O S-LOC O O O O O O O O O O O O O O O O
Input: Nijmeh of Lebanon beat Nasr of Saudi Arabia 1-0 ( halftime 1-0 ) in their Asian club championship second round first leg tie on Saturday .
Output: O O S-LOC O O O B-LOC E-LOC O O O O O O O O O O O O O O O O O
Input: Slovakia beat the Faroe Islands 2-1 ( halftime 1-0 ) in their World Cup soccer European group six qualifying match on Saturday .
Output: S-LOC O O B-LOC E-LOC O O O O O O O O O O O O O O O O O
Input: Canada beat Panama 3-1 ( halftime 2-0 ) in their CONCACAF semifinal phase qualifying match for the 1998 World Cup on Friday .
Output: S-LOC O S-LOC O O O O O O O O O O O O O O O O O O</p>
<h2>Input Sentence and GPT-3 Output</h2>
<p>Input: Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .
Output: S-LOC O O O O O O O O O O O B-LOC E-LOC O O O O O O O O O O O O O
Expected Output: S-LOC O O O O O O O O O O O O O S-LOC O O O O O O O O</p>
<p>Table 9: Examples for the BMES output format on sample-100 CoNLL2003 dataset, where the error information is colored red and the expected correct output is colored blue.</p>
<table>
<thead>
<tr>
<th><strong>Example 1</strong> Position Error and Entity Error</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Task Description</em></td>
<td></td>
</tr>
<tr>
<td>I am an excellent linguist. The task is to label miscellaneous entities. Below are some examples.</td>
<td></td>
</tr>
<tr>
<td><em>Sentence-level Demonstrations</em></td>
<td></td>
</tr>
<tr>
<td>Input: Soccer - results of South Korean PRO-SOCCER games.</td>
<td></td>
</tr>
<tr>
<td>Output: South Korean(4)</td>
<td></td>
</tr>
<tr>
<td>Input: Soccer - results of South Korean PRO-SOCCER games.</td>
<td></td>
</tr>
<tr>
<td>Output: South Korean(4)</td>
<td></td>
</tr>
<tr>
<td>Input: Soccer - results of South Korean PRO-SOCCER games.</td>
<td></td>
</tr>
<tr>
<td>Output: South Korean(4)</td>
<td></td>
</tr>
<tr>
<td>Input: Soccer - Italian cup second round results.</td>
<td></td>
</tr>
<tr>
<td>Output: Italian cup(2)</td>
<td></td>
</tr>
<tr>
<td><em>Input Sentence and GPT-3 Output</em></td>
<td></td>
</tr>
<tr>
<td>Input: Soccer - Japan get lucky win, China in surprise defeat.</td>
<td></td>
</tr>
<tr>
<td>Output: Japan(4), China(4)</td>
<td></td>
</tr>
<tr>
<td><em>Expected Output:</em> None</td>
<td></td>
</tr>
<tr>
<td><strong>Example 2</strong> Position Error and Entity Error</td>
<td></td>
</tr>
<tr>
<td><em>Task Description</em></td>
<td></td>
</tr>
<tr>
<td>I am an excellent linguist. The task is to label organization entities. Below are some examples.</td>
<td></td>
</tr>
<tr>
<td><em>Sentence-level Demonstrations</em></td>
<td></td>
</tr>
<tr>
<td>Input: Dubai 1996-08-26</td>
<td></td>
</tr>
<tr>
<td>Output: None</td>
<td></td>
</tr>
<tr>
<td>Input: Dubai 1996-08-29</td>
<td></td>
</tr>
<tr>
<td>Output: None</td>
<td></td>
</tr>
<tr>
<td>Input: Dubai 1996-08-29</td>
<td></td>
</tr>
<tr>
<td>Output: None</td>
<td></td>
</tr>
<tr>
<td>Input: Dubai 1996-08-22</td>
<td></td>
</tr>
<tr>
<td>Output: None</td>
<td></td>
</tr>
<tr>
<td><em>Input Sentence and GPT-3 Output</em></td>
<td></td>
</tr>
<tr>
<td>Input: AL-ADV, United Arab Emirates 1996-12-06</td>
<td></td>
</tr>
<tr>
<td>Output: AL-ADV, United Arab Emirates</td>
<td></td>
</tr>
<tr>
<td><em>Expected Output:</em> None</td>
<td></td>
</tr>
<tr>
<td><strong>Example 3</strong> Position Error</td>
<td></td>
</tr>
<tr>
<td><em>Task Description</em></td>
<td></td>
</tr>
<tr>
<td>I am an excellent linguist. The task is to label location entities. Below are some examples.</td>
<td></td>
</tr>
<tr>
<td><em>Sentence-level Demonstrations</em></td>
<td></td>
</tr>
<tr>
<td>Input: Third seed Arantxa Sanchez Vicario, the 1994 champion, and eighth-seeded Olympic gold medalist Lindsay Davenport dropped three game each en route to the second round.</td>
<td></td>
</tr>
<tr>
<td>Output: None</td>
<td></td>
</tr>
<tr>
<td>Input: Dutch champions Ajax Amsterdam faltered in their second league match of the season on Saturday losing 2-0 away at Heerenveen.</td>
<td></td>
</tr>
<tr>
<td>Output: None</td>
<td></td>
</tr>
<tr>
<td>Input: Soccer - disappointing Ajax slump 2-0 at Heerenveen.</td>
<td></td>
</tr>
<tr>
<td>Output: Heerenveen(7)</td>
<td></td>
</tr>
<tr>
<td>Input: Australian Open runner-up Anke Huber of Germany, the sixth seed, was undone by an unlucky draw that put her against 17th ranked South African Amanda Coetzer in her opening match.</td>
<td></td>
</tr>
<tr>
<td>Output: Germany(6)</td>
<td></td>
</tr>
<tr>
<td><em>Input Sentence and GPT-3 Output</em></td>
<td></td>
</tr>
<tr>
<td>Input: But China saw their luck desert them in the second match of the group, crashing to a surprise 2-0 defeat to newcomers Uzbekistan.</td>
<td></td>
</tr>
<tr>
<td>Output: China(2), Uzbekistan(9)</td>
<td></td>
</tr>
<tr>
<td><em>Expected Output:</em> China(1), Uzbekistan(23)</td>
<td></td>
</tr>
<tr>
<td><strong>Example 4</strong> Position Error and Entity Error</td>
<td></td>
</tr>
<tr>
<td><em>Task Description</em></td>
<td></td>
</tr>
<tr>
<td>I am an excellent linguist. The task is to label miscellaneous entities. Below are some examples.</td>
<td></td>
</tr>
<tr>
<td><em>Sentence-level Demonstrations</em></td>
<td></td>
</tr>
<tr>
<td>Input: Azerbaijan heat Switzerland 1-0 ( halftime 1-0) in their World Cup soccer European group three qualifying match on Saturday.</td>
<td></td>
</tr>
<tr>
<td>Output: World Cup(10), European(13)</td>
<td></td>
</tr>
<tr>
<td>Input: Nijmeks el Lebanon heat Nass of Saudi Arabia 1-0 ( halftime 1-0) in their Asian club championship second round first leg tie on Saturday.</td>
<td></td>
</tr>
<tr>
<td>Output: Asian(15)</td>
<td></td>
</tr>
<tr>
<td>Input: Slovakia heat the Faroe Islands 2-1 ( halftime 1-0) in their World Cup soccer European group six qualifying match on Saturday.</td>
<td></td>
</tr>
<tr>
<td>Output: World Cup(12), European(15)</td>
<td></td>
</tr>
<tr>
<td>Input: Canada heat Panama 3-1 ( halftime 2-0) in their CONCACAF semifinal phase qualifying match for the 1998 World Cup on Friday.</td>
<td></td>
</tr>
<tr>
<td>Output: World Cup(18)</td>
<td></td>
</tr>
<tr>
<td><em>Input Sentence and GPT-3 Output</em></td>
<td></td>
</tr>
<tr>
<td>Input: Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday.</td>
<td></td>
</tr>
<tr>
<td>Output: Asian(14)</td>
<td></td>
</tr>
<tr>
<td><em>Expected Output:</em> Asian Cup(6)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 10: Examples for the entity-position output format on sample-100 CoNLL2003 dataset, where the error information is colored red and the expected correct output is colored blue.</p>
<h4><strong>Example 1</strong></h4>
<p>Text Description</p>
<p>I am an excellent linguist. The task is to label miscellaneous entities. Below are some examples:</p>
<p><em>Sentence-level Demonstrations</em></p>
<table>
<thead>
<tr>
<th>Input: Seattle at Boston</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Output: Seattle at Boston</td>
<td></td>
</tr>
<tr>
<td>Input: 1. Carly Sacramento</td>
<td></td>
</tr>
<tr>
<td>Output: 1-638-96</td>
<td></td>
</tr>
<tr>
<td>Output: 2. Carly Sacramento</td>
<td></td>
</tr>
<tr>
<td>Output: 638-96</td>
<td></td>
</tr>
<tr>
<td>Input: Director Budge Weidman, who has shepherded the project from the beginning, predicts it will take up to a decade to complete.</td>
<td></td>
</tr>
<tr>
<td>Output: Director Budge Weidman, who has shepherded the project from the beginning, predicts it will take up to a decade to complete.</td>
<td></td>
</tr>
<tr>
<td>Input: Half-0 Bu meet</td>
<td></td>
</tr>
<tr>
<td>Output: Half-0 Bu meet</td>
<td></td>
</tr>
<tr>
<td>Input: Scott Hsuptr</td>
<td></td>
</tr>
<tr>
<td>Output: Scott Hsuptr</td>
<td></td>
</tr>
<tr>
<td>Input: Scott Hsuptr</td>
<td></td>
</tr>
<tr>
<td>Input: Standings in the French first</td>
<td></td>
</tr>
<tr>
<td>Output: Standings in the 40-00</td>
<td></td>
</tr>
</tbody>
</table>
<h1>Example 1</h1>
<h2>Test Description</h2>
<p>I am an excellent linguist. The task is to label location entities. Below are some examples.</p>
<h2>Sentence-level Demonstrations</h2>
<p>Input: Dubai 1996-08-26
Output: @ @Dubai## 1996-08-26
Input: Dubai 1996-08-29
Output: @ @Dubai## 1996-08-29
Input: Dubai 1996-08-29
Output: @ @Dubai## 1996-08-29
Input: Dubai 1996-08-22
Output: @ @Dubai## 1996-08-22
Input: Dubai 1996-08-25
Output: @ @Dubai## 1996-08-25
Input: Baghdad 1996-08-24
Output: @ @Baghdad## 1996-08-24
Input: Baghdad 1996-08-27
Output: @ @Baghdad## 1996-08-27
Input: Baghdad 1996-08-28
Output: @ @Baghdad## 1996-08-28</p>
<h2>Input Sentence and GPF-3 Output</h2>
<p>Input: AL-ADV, United Arab Emirates 1996-12-06
Output: @ @AL-ADM#, @ @United Arab Emirates## 1996-12-06
Expected Output: @ @AL-ADM#, @ @United Arab Emirates## 1996-12-06</p>
<h2>Example 2</h2>
<h2>Test Description</h2>
<p>I am an excellent linguist. The task is to label location entities. Below are some examples.</p>
<h2>Sentence-level Demonstrations</h2>
<p>Input: Azerbaijan beat Switzerland 1-0 ( halftime 1-0 ) in their World Cup soccer European group three qualifying match on Saturday.
Output: @ @Azerbaijan## beat @ @Switzerland## 1-0 ( halftime 1-0 ) in their World Cup soccer European group three qualifying match on Saturday .
Input: Nijmeh of Lebanon beat Naor of Saudi Arabia 1-0 ( halftime 1-0 ) in their Asian club championship second round first leg tie on Saturday.
Output: Nijmeh of @ @Lebanon## beat Naor of @ @Saudi Arabia## 1-0 ( halftime 1-0 ) in their Asian club championship second round first leg tie on Saturday .
Input: Slovakia beat the Faroe Islands 2-1 ( halftime 1-0 ) in their World Cup soccer European group six qualifying match on Saturday.
Output: @ @Slovakia## beat the @ @Faroe Islands## 2-1 ( halftime 1-0 ) in their World Cup soccer European group six qualifying match on Saturday.
Input: Canada beat Panama 3-1 ( halftime 2-0 ) in their CONCACAF semifinal phase qualifying match for the 1998 World Cup on Friday.
Output: @ @Canada## beat @ @Panama## 3-1 ( halftime 2-0 ) in their CONCACAF semifinal phase qualifying match for the 1998 World Cup on Friday.
Input: Soccer - Azerbaijan beat Switzerland in world cup Qualifier .
Output: Soccer - @ @Azerbaijan## beat @ @Switzerland## in world cup Qualifier .
Input: Soccer - Wales beat San Marino in world cup Qualifier .
Output: Soccer - @ @Wales# beat @ @San Marino## in world cup Qualifier .
Input: The United States edged Austria in Salzburg 3-2 in the opening round in April, and then blanked Japan 5-0 in Nagoya last month in the semifinals.
Output: The @ @United States## edged @ @Austria# in @ @Salzburg## 3-2 in the opening round in April, and then blanked @ @Japan## 5-0 in @ @Nagoya## last month in the semifinals .
Input: Soccer - Slovakia beat Faroes in world cup Qualifier .
Output: Soccer - @ @Slovakia## beat @ @Faroes## in world cup Qualifier .</p>
<h2>Input Sentence and GPF-3 Output</h2>
<p>Input: Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday.
Output: @ @Japan## began the defence of their Asian Cup title with a lucky 2-1 win against @ @Syria## in a Group C championship match on Friday.
Expected Output: @ @Japan## began the defence of their Asian Cup title with a lucky 2-1 win against @ @Syria## in a Group C championship match on Friday.</p>
<h2>Example 3</h2>
<h2>Test Description</h2>
<p>I am an excellent linguist. The task is to label miscellaneous entities. Below are some examples.</p>
<h2>Sentence-level Demonstrations</h2>
<p>Input: Azerbaijan beat Switzerland 1-0 ( halftime 1-0 ) in their World Cup soccer European group three qualifying match on Saturday.
Output: Azerbaijan beat Switzerland 1-0 ( halftime 1-0 ) in their @ @World Cup## soccer @ @European## group three qualifying match on Saturday .
Input: Nijmeh of Lebanon beat Naor of Saudi Arabia 1-0 ( halftime 1-0 ) in their Asian club championship second round first leg tie on Saturday.
Output: Nijmeh of Lebanon beat Naor of Saudi Arabia 1-0 ( halftime 1-0 ) in their @ @Asian## club championship second round first leg tie on Saturday .
Input: Slovakia beat the Faroe Islands 2-1 ( halftime 1-0 ) in their World Cup soccer European group six qualifying match on Saturday.
Output: Slovakia beat the Faroe Islands 2-1 ( halftime 1-0 ) in their @ @World Cup## soccer @ @European## group six qualifying match on Saturday .
Input: Canada beat Panama 3-1 ( halftime 2-0 ) in their CONCACAF semifinal phase qualifying match for the 1998 World Cup on Friday.
Output: Canada beat Panama 3-1 ( halftime 2-0 ) in their CONCACAF semifinal phase qualifying match for the 1998 @ @World Cup## on Friday .
Input: Soccer - Azerbaijan beat Switzerland in world cup Qualifier .
Output: Soccer - Azerbaijan beat Switzerland in @ @world cup## Qualifier .
Input: Soccer - Wales beat San Marino in world cup Qualifier .
Output: Soccer - Wales beat San Marino in @ @world cup## Qualifier .
Input: The United States edged Austria in Salzburg 3-2 in the opening round in April, and then blanked Japan 5-0 in Nagoya last month in the semifinals.
Output: The United States edged Austria in Salzburg 3-2 in the opening round in April, and then blanked Japan 5-0 in Nagoya last month in the semifinals .
Input: Soccer - Slovakia beat Faroes in world cup Qualifier .
Output: Soccer - Slovakia beat Faroes in @ @world cup## Qualifier .</p>
<h2>Input Sentence and GPF-3 Output</h2>
<p>Input: Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday.
Output: Japan began the defence of their @ @Asian Cup## title with a lucky 2-1 win against Syria in a Group C championship match on Friday.
Expected Output: Japan began the defence of their @ @Asian Cup## title with a lucky 2-1 win against Syria in a Group C championship match on Friday.</p>
<h2>Example 1</h2>
<h3>Test Description</h3>
<p><strong>Lum service/host</strong> - The task is to label location entities. Below are some examples.</p>
<h3>Sentence level Demonstration</h3>
<p><strong>Input:</strong> AL-BAM, West Bank 1996-08-30
<strong>Output:</strong> @@AL-BAM##, @@West Bank## 1996-08-30
<strong>Input:</strong> AL-MOVEM, West Bank 1996-08-30
<strong>Output:</strong> @@AL-MOVEM##, @@West Bank## 1996-08-30
<strong>Input:</strong> Transmitter (3.0, 1, 4cm) New In Table (Tattoo) + 10+ total Rejabs
<strong>Output:</strong> Transmitter (@@U.S.W1, 2mm) New In Table (@@Fuseo##) + 10+ total Rejabs
<strong>Input:</strong> The greatest declines in the volume of help wanted advertising were in the New England, Mountain and West South Central regions.
<strong>Output:</strong> The greatest declines in the volume of help wanted advertising were in the @@New England##, @@Mountain## and @@West South Central## regions.
<strong>Input:</strong> Drug Flash (3.5) + Host Guidance Point (Italy) 7.5 7.4 (7.5) 2.0 7.6 (8.6)
<strong>Output:</strong> Drug Flash (@@U.S.W1, 0mm) Guidance Point (@@Italy##) 7.5 7.4 (7.5) 2.0 7.6 (8.6)
<strong>Input:</strong> IoT Tamiqoo (3.5) + Host Also Buildiness (Romania) 6.7 5.7 (6.4) 6.1 (retest) - host exhaustion
<strong>Output:</strong> IoT Tamiqoo (@@U.S.W1, 0mm) Also Buildiness (@@Romania##) 6.6 1.2 (1.4) 4.6 (retest) - host exhaustion
<strong>Input:</strong> Choleas, 16, was at President Bill Clinton's side as he rode the rails through parts of West Virginia, Kentucky and Ohio, and was introduced at every step.
<strong>Output:</strong> Choleas, 16, was at President Bill Clinton's side as he rode the rails through parts of @@West Virginia##, @@Kentucky## and @@Ohio##, and was introduced at every step.
<strong>Input:</strong> Clinton said on Saturday he had ordered U.S. forces in the Gulf to go on high alert and was reinforcing them in response to Iraqi attacks on Karelek dissidents in northern Iraq.
<strong>Output:</strong> Clinton said on Saturday he had ordered @@U.S.W forces in the @@Gulf## to go on high alert and was reinforcing them in response to Iraqi attacks on Karelek dissidents in northern @@Iraq##.</p>
<h3>Input Sentence and GPS 3 Output</h3>
<p><strong>Input:</strong> AL-API, United Arab Emirates 1996-12-06
<strong>Output:</strong> @@AL-API##, @@United Arab Emirates## 1996-12-06</p>
<h3>Expected Output: @@AL-API##, @@United Arab Emirates## 1996-12-06</h3>
<h2>Example 2</h2>
<h3>Test Description</h3>
<p><strong>Lum service/host</strong> - The task is to label miscellaneous entities. Below are some examples.</p>
<h3>Sentence level Demonstration</h3>
<p><strong>Input:</strong> The armed hijackers of the Airbus 310 Flight 190, which is expected to arrive about 4 a.m. (9/00/10/00) have said they intend to surrender and seek political asylum in Britain.
<strong>Output:</strong> The armed hijackers of the @@Airbus 310## @@Flight 150##, which is expected to arrive about 4 a.m. (10/00/10/00) 1 have said they intend to surrender and seek political asylum in Britain.
<strong>Input:</strong> The air force of the 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 have said they intend to surrender and seek political asylum in Britain.
<strong>Input:</strong> The Air Force 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Output:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Output:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p><strong>Input:</strong> The 190, which is expected to arrive about 4 a.m. (10/00/10/00) 1 has said they intend to surrender and seek political asylum in Britain.</p>
<p>**Input</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ GPT-3 is used in this example.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
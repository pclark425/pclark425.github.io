<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3097 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3097</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3097</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-260444578</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2212.13428v1.pdf" target="_blank">A Survey on Knowledge-Enhanced Pre-trained Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Natural Language Processing (NLP) has been revolutionized by the use of Pre-trained Language Models (PLMs) such as BERT. Despite setting new records in nearly every NLP task, PLMs still face a number of challenges including poor interpretability, weak reasoning capability, and the need for a lot of expensive annotated data when applied to downstream tasks. By integrating external knowledge into PLMs, \textit{\underline{K}nowledge-\underline{E}nhanced \underline{P}re-trained \underline{L}anguage \underline{M}odels} (KEPLMs) have the potential to overcome the above-mentioned limitations. In this paper, we examine KEPLMs systematically through a series of studies. Specifically, we outline the common types and different formats of knowledge to be integrated into KEPLMs, detail the existing methods for building and evaluating KEPLMS, present the applications of KEPLMs in downstream tasks, and discuss the future research directions. Researchers will benefit from this survey by gaining a quick and comprehensive overview of the latest developments in this field.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3097.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3097.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that augments a parametric generative model with a non-parametric retrieval corpus at inference/training time to improve knowledge-intensive generation and QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG (retrieval + seq2seq generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines a document retriever over an external corpus with a sequence generator (BART-like) that conditions on retrieved passages; retrieval is non-parametric so the model can access up-to-date or large factual context external to model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>626M (reported variants: RAG-Token / RAG-Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented generation', 'retrieval-then-generate multi-step conditioning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At test/time the model retrieves candidate documents (non-parametric memory) relevant to the question, conditions the sequence generator on retrieved text, and generates an answer; retrieval reduces dependence on parametric memorization and enables multi-hop or evidence-grounded generation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse (combines non-parametric retrieval with parametric generation). The paper highlights RAG as combining two different reasoning/storage mechanisms (parametric and non-parametric) rather than relying on a single homogeneous style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain question answering / knowledge-intensive NLP (Natural Questions, WebQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Open-domain QA tasks where models must produce factual answers without being provided the supporting passage; retrieval is used to fetch supporting evidence before answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 4 (survey): RAG-Token: Natural Questions 44.1, WebQuestions 45.5; RAG-Seq: Natural Questions 44.5, WebQuestions 45.2 (accuracy/EM or retrieval+generation scores as reported in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Survey reports RAG (combined parametric + non-parametric) outperforms parametric-only and non-parametric-only baselines on three open-domain QA tasks; compared against T5 variants and retrieval-only or param-only models (REALM, EAE), RAG achieves higher QA performance with comparable or modest parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining diverse mechanisms (retrieval + generation) yields better open-domain QA performance than relying on a single reasoning/storage style; retrieval supplements factual knowledge efficiently and improves factuality/diversity of generated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>None numeric in survey beyond noting that RAG requires external corpus storage and introduces inference-time retrieval overhead (space/time tradeoffs).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3097.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Language Model (REALM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that incorporates a document retriever into the pretraining and fine-tuning loop so the language model explicitly attends to retrieved documents for knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval augmented language model pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adds a learned retriever component that retrieves relevant passages from a large corpus during pretraining and downstream tasks; the retriever and encoder are optimized to improve task performance by conditioning predictions on retrieved text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>330M (as reported in the survey table)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented pretraining and inference']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Uses a retrieval module to fetch supporting passages that the model conditions on when predicting answers or masked tokens; retrieval is non-parametric and updated independently of the model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse relative to pure parametric models (adds a retrieval mechanism), but uses a similar generation/prediction style once retrieval provides context.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain QA / knowledge-intensive tasks (Natural Questions / WebQuestions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>QA tasks requiring external factual knowledge; REALM enables direct conditioning on retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Table 4 (survey): REALM: Natural Questions 40.4, WebQuestions 40.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Survey compares REALM to parametric-only generators (T5 variants) and to combined retrieval+generation (RAG); REALM improves over some baselines but is outperformed by RAG in the cited comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval during pretraining and inference provides meaningful gains on open-domain QA versus purely parametric models, but combined designs (RAG) can yield further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Survey notes advantage of updating external knowledge without re-training but also the cost of maintaining large external corpora; no direct counter-performance beyond being inferior to RAG in reported benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3097.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-BERT (adaptive masking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>E-BERT (adaptive hybrid masking strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that adaptively chooses between word-level and phrase/entity-level masking during pretraining to better incorporate multi-granularity knowledge into masked-language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>E-BERT (masking strategy variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proposes an adaptive hybrid masking procedure that tracks losses for word-level and phrase-level masking modes and dynamically shifts masking probabilities to focus training on the mode with higher current loss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['adaptive hybrid masking (word-level vs phrase/entity-level)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>In each pretraining iteration the model probabilistically selects word- or phrase-level masking; losses from each mode are tracked and used to adapt the selection probability so the model concentrates on harder modes over training.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>uses two distinct masking styles and switches adaptively—so it explicitly leverages multiple, similar masking-based reasoning styles (word vs phrase) to learn richer representations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge probing / LAMA-style factual probing and downstream tasks relying on entity/phrase knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that require the model to recall or use entity/phrase-level facts and relations; adaptive masking aims to improve factual and phrase-level representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey does not provide numeric task metrics for E-BERT's adaptive masking within this text (citation blank). It references that adaptive selection favors the mode with higher loss and can balance word- and phrase-level learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared qualitatively to fixed-probability masking (e.g., GLM's fixed 20% vs 80%), E-BERT's adaptive switching is presented as more flexible and able to dynamically emphasize the harder-to-learn granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive hybrid masking can allocate training focus between reasoning/granularity styles (word vs phrase) based on learning signals, potentially improving incorporation of multi-granularity knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No numeric counter-results reported in the survey; citation title not provided in survey's text so quantitative evaluation details are not reproduced here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3097.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLM (KG-informed masking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM (Knowledge-Graph Informed Masking Strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining masking strategy that biases masking probabilities toward knowledge-graph-important entities to nudge the model to learn more KG-relevant facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM (masking variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modifies token masking during masked language model pretraining by sampling entities using importance weights derived from a knowledge graph (e.g., ConceptNet hop connectivity), masking entities at higher probability than other words.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['knowledge-graph-informed entity-level masking']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Entities that are more connected in a KG (reachable within specified hops) receive higher probability to be masked; this forces the model to learn to predict those KG-important entities during MLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>uses a different selection/prioritization strategy for masking (KG-informed) versus uniform/random masking; still a single core MLM-style prediction method but with KG-guided sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge acquisition / KG completion and knowledge probing (e.g., WN18RR, CKBC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that evaluate the model's factual/relational knowledge and ability to complete or reason over KG-style relations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey reports GLM outperforms some translation-based KG embedding models and KG-BERT on WN18RR and CKBC (no numeric values provided in survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>GLM's KG-informed masking is contrasted with random/entity-agnostic masking and with other KG-integration approaches; the survey highlights GLM's advantages on certain KG completion/probing datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Biasing pretraining to emphasize KG-important entities improves downstream KG-related reasoning and completion compared to naive MLM masking or some pure KG embedding baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No detailed negative numeric results are provided in the survey text; exact magnitudes and statistical detail refer to the original GLM publication.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3097.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KEPLER vs LUKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KEPLER (joint KG+LM pretraining) and LUKE (entity-level LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey notes that more complex joint KG+LM pretraining (KEPLER) does not always outperform simpler entity-level augmentation (LUKE) on entity typing and relation classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kepler: A unified model for knowledge embedding and pre-trained language representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KEPLER; LUKE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>KEPLER jointly pretrains knowledge embeddings and language model objectives to produce text-enhanced knowledge embeddings; LUKE incorporates entity-level information into transformer self-attention (entity-aware self-attention) focusing on entity tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['joint KG embedding + MLM pretraining (KEPLER)', 'entity-aware self-attention / entity-level augmentation (LUKE)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>KEPLER jointly optimizes knowledge-embedding objectives with text MLM so the model stores KG structure in parametric embeddings; LUKE augments Transformer attention to explicitly represent entities within contextual text, a more focused entity-level injection.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>KEPLER uses a combined (diverse) joint-training approach integrating KG embedding methods alongside MLM; LUKE uses a single focused entity-level augmentation strategy—survey reports that simpler, more targeted methods can outperform more complex joint methods on some reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Entity typing and relation classification</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks requiring identification of entity types and classification of relations between entities in text, which test entity-centric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey states KEPLER performs worse on entity typing and relation classification than LUKE (no numeric values provided in survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparison reported qualitatively: joint pretraining (KEPLER) did not produce better downstream performance than LUKE's entity-focused method on certain entity/relation tasks, indicating complexity does not guarantee better reasoning effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>More sophisticated multi-component pretraining (joint KG+LM) can underperform simpler, targeted entity-aware designs for entity-centric reasoning tasks; suggests matching method granularity to task needs matters.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>KEPLER underperformed LUKE on entity typing and relation classification (survey notes this as a counterexample to 'more complex = better').</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3097.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-Adapter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-Adapter (adapter-based knowledge injection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that injects distinct adapter modules to learn different kinds of parametric knowledge while keeping the base PLM parameters frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-adapter: Infusing knowledge into pre-trained models with adapters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>K-Adapter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adds lightweight adapter modules that are independently trained to encode specific knowledge types (e.g., linguistic, factual); adapters can be trained in parallel and appended to a frozen PLM to avoid re-training base weights.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['adapter-based modular knowledge encoding', 'parametric external memory via adapters']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each adapter module specializes in a knowledge type and is integrated into forward passes to modulate representations; adapters act as separate parametric stores accessible during reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse in knowledge sources (multiple adapters for different knowledge types) but similar in that each adapter follows the same adapter-interface style—modular diversity rather than algorithmic diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge-intensive tasks (e.g., open-domain QA, entity/relation tasks) and adapter-targeted benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that benefit from injecting specialized knowledge modules without altering the base PLM weights.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey lists K-Adapter among high-performing KEPLMs; specific numeric results are not reproduced in the survey text here (see original K-Adapter paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Survey contrasts adapter-based parametric memory with non-parametric external memory and joint pretraining; highlights adapters' advantage in parallel training and not requiring base model re-training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapter modules provide a practical way to add diverse knowledge types with low disruption to base PLMs; modular/parallel training enables scalable incorporation of multiple knowledge styles.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Hou et al. (survey cites) found some KEPLMs (including K-Adapter) incorporated only small amounts of factual knowledge as detected by graph-convolution-based probes — indicating adapters do not guarantee deep factual integration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3097.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA-GNN / GreaseLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QA-GNN and GreaseLM (graph-reasoning enhanced LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that integrate knowledge graph subgraphs and graph reasoning (GNN-style multi-step interactions) with language models to perform commonsense and QA reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QAGNN: Reasoning with language models and knowledge graphs for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QA-GNN; GreaseLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>QA-GNN builds and runs a graph neural network over a retrieved KG subgraph to perform multi-step graph reasoning for QA; GreaseLM sets up interaction nodes in both text and KG modalities to iteratively exchange and fuse information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['graph neural network multi-hop reasoning over KG subgraphs', 'multi-step text-KG interaction nodes and attention-based fusion']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Constructs a task-specific KG subgraph (entities and relations) and applies GNN message-passing or iterative attention interactions so that multi-hop inferential chains can be formed explicitly in graph space and fused back into textual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse in style relative to text-only LMs: explicitly uses graph-structured reasoning rather than relying only on implicit parametric reasoning; GreaseLM also mixes modalities via interaction nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Commonsense question answering / multi-hop QA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Question-answering datasets requiring reasoning across multiple facts or conceptual hops typically supported by KG structure (CommonsenseQA and similar).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey states QA-GNN and GreaseLM achieved good results on commonsense QA (no numeric values reproduced in survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Survey positions graph-reasoning models as improving over pure text-only PLMs for commonsense QA by enabling explicit multi-hop inference; GreaseLM extends QA-GNN ideas with bilateral interaction nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit graph-structured reasoning (multi-hop GNN over KG subgraphs) provides a complementary and effective reasoning style to standard PLM implicit reasoning for tasks needing multi-step inferencing.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Graph-based methods can introduce inference-time overhead for subgraph construction and multi-step message passing; no explicit failure cases numerically provided in survey text.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3097.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-BART (Knowledge Graph Augmented BART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative model that injects knowledge subgraphs into an encoder-decoder architecture to improve generative commonsense reasoning and constrained text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KG-BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Augments a BART-style seq2seq model with knowledge subgraph encodings fused into encoder/decoder via multi-headed graph attention so generated text can reflect structured commonsense relations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['knowledge-subgraph augmentation + multi-headed graph attention', 'generation conditioned on KG context (multi-hop reasoning reflected in decoder)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Encodes KG subgraphs relevant to an input, fuses graph node/relation embeddings with text representations via attention in the encoder/decoder, enabling generation that leverages structured commonsense paths.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse relative to plain seq2seq: integrates structured KG reasoning into generation, enabling a different reasoning modality (graph-based) to influence generative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Generative commonsense reasoning tasks (e.g., CommonGen, story ending generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks where the model must generate coherent text constrained or informed by commonsense relations/graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey indicates KG-BART designed for generative commonsense reasoning and reports it as effective; no specific numeric scores reproduced in survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Survey notes that KG-BART and other knowledge-subgraph methods improve generative commonsense outputs compared to plain PLM generation, but at cost of subgraph construction and slower inference.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fusing structured commonsense knowledge into generative models improves plausibility and factuality of generated text for knowledge-intensive NLG tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>KG-subgraph construction increases inference time and complexity; no explicit numeric counterexamples in survey text.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3097.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3097.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hou et al. probe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Understanding the integration of knowledge in language models with graph convolutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis probing how much factual knowledge KEPLMs actually embed, using graph-convolutional diagnostics, finding limited incorporation in some KEPLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding the integration of knowledge in language models with graph convolutions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Analysis / probe (applied to KEPLMs such as ERNIE and K-Adapter)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses graph convolutional simulation probes to detect and quantify factual knowledge integrated into PLMs/KEPLMs, showing some KEPLMs contain only modest amounts of factual information despite their architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['graph-convolution-based probing / diagnostic analysis']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Constructs graph convolution tests to trace and measure whether KEPLM parameters actually store KG facts and how those facts are used during inference/representations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Not an LM for reasoning per se; it is an analysis that contrasts claimed diverse knowledge-incorporation methods with measured factual incorporation (negative/diagnostic result).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge probing / diagnostic evaluation of KEPLMs</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Analyses focused on whether KEPLMs truly absorb/verbalize factual KG knowledge and to what extent different integration methods differ in factual incorporation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey reports Hou et al.'s finding that KEPLMs such as ERNIE and K-Adapter have incorporated only a small amount of factual knowledge (no numeric probe metrics provided in survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Provides an explicit negative contrast: complex knowledge-injection architectures do not necessarily guarantee deep factual incorporation; probes find limited factual storage in several KEPLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Caveat that many KEPLMs may appear to integrate knowledge architecturally but, by probing, may contain only modest factual integration, highlighting the need for more effective methods.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Direct negative finding: ERNIE and K-Adapter were detected to have only small amounts of factual knowledge by the graph-convolution simulator probe.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Retrieval augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Kepler: A unified model for knowledge embedding and pre-trained language representation <em>(Rating: 2)</em></li>
                <li>LUKE: deep contextualized entity representations with entity-aware self-attention <em>(Rating: 2)</em></li>
                <li>Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning <em>(Rating: 2)</em></li>
                <li>QAGNN: Reasoning with language models and knowledge graphs for question answering <em>(Rating: 2)</em></li>
                <li>Greaselm: Graph reasoning enhanced language models <em>(Rating: 2)</em></li>
                <li>K-adapter: Infusing knowledge into pre-trained models with adapters <em>(Rating: 2)</em></li>
                <li>Understanding the integration of knowledge in language models with graph convolutions <em>(Rating: 2)</em></li>
                <li>E-bert: Efficient-yet-effective entity embeddings for bert <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3097",
    "paper_id": "paper-260444578",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A hybrid approach that augments a parametric generative model with a non-parametric retrieval corpus at inference/training time to improve knowledge-intensive generation and QA.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "mention",
            "model_name": "RAG (retrieval + seq2seq generator)",
            "model_description": "Combines a document retriever over an external corpus with a sequence generator (BART-like) that conditions on retrieved passages; retrieval is non-parametric so the model can access up-to-date or large factual context external to model parameters.",
            "model_size": "626M (reported variants: RAG-Token / RAG-Seq)",
            "reasoning_methods": [
                "retrieval-augmented generation",
                "retrieval-then-generate multi-step conditioning"
            ],
            "reasoning_methods_description": "At test/time the model retrieves candidate documents (non-parametric memory) relevant to the question, conditions the sequence generator on retrieved text, and generates an answer; retrieval reduces dependence on parametric memorization and enables multi-hop or evidence-grounded generation.",
            "diversity_of_methods": "diverse (combines non-parametric retrieval with parametric generation). The paper highlights RAG as combining two different reasoning/storage mechanisms (parametric and non-parametric) rather than relying on a single homogeneous style.",
            "reasoning_task_name": "Open-domain question answering / knowledge-intensive NLP (Natural Questions, WebQuestions)",
            "reasoning_task_description": "Open-domain QA tasks where models must produce factual answers without being provided the supporting passage; retrieval is used to fetch supporting evidence before answer generation.",
            "performance_by_method": "Table 4 (survey): RAG-Token: Natural Questions 44.1, WebQuestions 45.5; RAG-Seq: Natural Questions 44.5, WebQuestions 45.2 (accuracy/EM or retrieval+generation scores as reported in cited work).",
            "comparison_of_methods": "Survey reports RAG (combined parametric + non-parametric) outperforms parametric-only and non-parametric-only baselines on three open-domain QA tasks; compared against T5 variants and retrieval-only or param-only models (REALM, EAE), RAG achieves higher QA performance with comparable or modest parameter counts.",
            "key_findings": "Combining diverse mechanisms (retrieval + generation) yields better open-domain QA performance than relying on a single reasoning/storage style; retrieval supplements factual knowledge efficiently and improves factuality/diversity of generated answers.",
            "counter_examples_or_negative_results": "None numeric in survey beyond noting that RAG requires external corpus storage and introduces inference-time retrieval overhead (space/time tradeoffs).",
            "uuid": "e3097.0"
        },
        {
            "name_short": "REALM",
            "name_full": "Retrieval-Augmented Language Model (REALM)",
            "brief_description": "A model that incorporates a document retriever into the pretraining and fine-tuning loop so the language model explicitly attends to retrieved documents for knowledge-intensive tasks.",
            "citation_title": "Retrieval augmented language model pre-training",
            "mention_or_use": "mention",
            "model_name": "REALM",
            "model_description": "Adds a learned retriever component that retrieves relevant passages from a large corpus during pretraining and downstream tasks; the retriever and encoder are optimized to improve task performance by conditioning predictions on retrieved text.",
            "model_size": "330M (as reported in the survey table)",
            "reasoning_methods": [
                "retrieval-augmented pretraining and inference"
            ],
            "reasoning_methods_description": "Uses a retrieval module to fetch supporting passages that the model conditions on when predicting answers or masked tokens; retrieval is non-parametric and updated independently of the model parameters.",
            "diversity_of_methods": "diverse relative to pure parametric models (adds a retrieval mechanism), but uses a similar generation/prediction style once retrieval provides context.",
            "reasoning_task_name": "Open-domain QA / knowledge-intensive tasks (Natural Questions / WebQuestions)",
            "reasoning_task_description": "QA tasks requiring external factual knowledge; REALM enables direct conditioning on retrieved documents.",
            "performance_by_method": "Table 4 (survey): REALM: Natural Questions 40.4, WebQuestions 40.7.",
            "comparison_of_methods": "Survey compares REALM to parametric-only generators (T5 variants) and to combined retrieval+generation (RAG); REALM improves over some baselines but is outperformed by RAG in the cited comparisons.",
            "key_findings": "Retrieval during pretraining and inference provides meaningful gains on open-domain QA versus purely parametric models, but combined designs (RAG) can yield further improvements.",
            "counter_examples_or_negative_results": "Survey notes advantage of updating external knowledge without re-training but also the cost of maintaining large external corpora; no direct counter-performance beyond being inferior to RAG in reported benchmarks.",
            "uuid": "e3097.1"
        },
        {
            "name_short": "E-BERT (adaptive masking)",
            "name_full": "E-BERT (adaptive hybrid masking strategy)",
            "brief_description": "An approach that adaptively chooses between word-level and phrase/entity-level masking during pretraining to better incorporate multi-granularity knowledge into masked-language models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "E-BERT (masking strategy variant)",
            "model_description": "Proposes an adaptive hybrid masking procedure that tracks losses for word-level and phrase-level masking modes and dynamically shifts masking probabilities to focus training on the mode with higher current loss.",
            "model_size": null,
            "reasoning_methods": [
                "adaptive hybrid masking (word-level vs phrase/entity-level)"
            ],
            "reasoning_methods_description": "In each pretraining iteration the model probabilistically selects word- or phrase-level masking; losses from each mode are tracked and used to adapt the selection probability so the model concentrates on harder modes over training.",
            "diversity_of_methods": "uses two distinct masking styles and switches adaptively—so it explicitly leverages multiple, similar masking-based reasoning styles (word vs phrase) to learn richer representations.",
            "reasoning_task_name": "Knowledge probing / LAMA-style factual probing and downstream tasks relying on entity/phrase knowledge",
            "reasoning_task_description": "Tasks that require the model to recall or use entity/phrase-level facts and relations; adaptive masking aims to improve factual and phrase-level representation learning.",
            "performance_by_method": "Survey does not provide numeric task metrics for E-BERT's adaptive masking within this text (citation blank). It references that adaptive selection favors the mode with higher loss and can balance word- and phrase-level learning.",
            "comparison_of_methods": "Compared qualitatively to fixed-probability masking (e.g., GLM's fixed 20% vs 80%), E-BERT's adaptive switching is presented as more flexible and able to dynamically emphasize the harder-to-learn granularity.",
            "key_findings": "Adaptive hybrid masking can allocate training focus between reasoning/granularity styles (word vs phrase) based on learning signals, potentially improving incorporation of multi-granularity knowledge.",
            "counter_examples_or_negative_results": "No numeric counter-results reported in the survey; citation title not provided in survey's text so quantitative evaluation details are not reproduced here.",
            "uuid": "e3097.2"
        },
        {
            "name_short": "GLM (KG-informed masking)",
            "name_full": "GLM (Knowledge-Graph Informed Masking Strategy)",
            "brief_description": "A pretraining masking strategy that biases masking probabilities toward knowledge-graph-important entities to nudge the model to learn more KG-relevant facts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GLM (masking variant)",
            "model_description": "Modifies token masking during masked language model pretraining by sampling entities using importance weights derived from a knowledge graph (e.g., ConceptNet hop connectivity), masking entities at higher probability than other words.",
            "model_size": null,
            "reasoning_methods": [
                "knowledge-graph-informed entity-level masking"
            ],
            "reasoning_methods_description": "Entities that are more connected in a KG (reachable within specified hops) receive higher probability to be masked; this forces the model to learn to predict those KG-important entities during MLM pretraining.",
            "diversity_of_methods": "uses a different selection/prioritization strategy for masking (KG-informed) versus uniform/random masking; still a single core MLM-style prediction method but with KG-guided sampling.",
            "reasoning_task_name": "Knowledge acquisition / KG completion and knowledge probing (e.g., WN18RR, CKBC)",
            "reasoning_task_description": "Tasks that evaluate the model's factual/relational knowledge and ability to complete or reason over KG-style relations.",
            "performance_by_method": "Survey reports GLM outperforms some translation-based KG embedding models and KG-BERT on WN18RR and CKBC (no numeric values provided in survey text).",
            "comparison_of_methods": "GLM's KG-informed masking is contrasted with random/entity-agnostic masking and with other KG-integration approaches; the survey highlights GLM's advantages on certain KG completion/probing datasets.",
            "key_findings": "Biasing pretraining to emphasize KG-important entities improves downstream KG-related reasoning and completion compared to naive MLM masking or some pure KG embedding baselines.",
            "counter_examples_or_negative_results": "No detailed negative numeric results are provided in the survey text; exact magnitudes and statistical detail refer to the original GLM publication.",
            "uuid": "e3097.3"
        },
        {
            "name_short": "KEPLER vs LUKE",
            "name_full": "KEPLER (joint KG+LM pretraining) and LUKE (entity-level LM)",
            "brief_description": "Survey notes that more complex joint KG+LM pretraining (KEPLER) does not always outperform simpler entity-level augmentation (LUKE) on entity typing and relation classification.",
            "citation_title": "Kepler: A unified model for knowledge embedding and pre-trained language representation",
            "mention_or_use": "mention",
            "model_name": "KEPLER; LUKE",
            "model_description": "KEPLER jointly pretrains knowledge embeddings and language model objectives to produce text-enhanced knowledge embeddings; LUKE incorporates entity-level information into transformer self-attention (entity-aware self-attention) focusing on entity tokens.",
            "model_size": null,
            "reasoning_methods": [
                "joint KG embedding + MLM pretraining (KEPLER)",
                "entity-aware self-attention / entity-level augmentation (LUKE)"
            ],
            "reasoning_methods_description": "KEPLER jointly optimizes knowledge-embedding objectives with text MLM so the model stores KG structure in parametric embeddings; LUKE augments Transformer attention to explicitly represent entities within contextual text, a more focused entity-level injection.",
            "diversity_of_methods": "KEPLER uses a combined (diverse) joint-training approach integrating KG embedding methods alongside MLM; LUKE uses a single focused entity-level augmentation strategy—survey reports that simpler, more targeted methods can outperform more complex joint methods on some reasoning tasks.",
            "reasoning_task_name": "Entity typing and relation classification",
            "reasoning_task_description": "Tasks requiring identification of entity types and classification of relations between entities in text, which test entity-centric reasoning.",
            "performance_by_method": "Survey states KEPLER performs worse on entity typing and relation classification than LUKE (no numeric values provided in survey text).",
            "comparison_of_methods": "Direct comparison reported qualitatively: joint pretraining (KEPLER) did not produce better downstream performance than LUKE's entity-focused method on certain entity/relation tasks, indicating complexity does not guarantee better reasoning effectiveness.",
            "key_findings": "More sophisticated multi-component pretraining (joint KG+LM) can underperform simpler, targeted entity-aware designs for entity-centric reasoning tasks; suggests matching method granularity to task needs matters.",
            "counter_examples_or_negative_results": "KEPLER underperformed LUKE on entity typing and relation classification (survey notes this as a counterexample to 'more complex = better').",
            "uuid": "e3097.4"
        },
        {
            "name_short": "K-Adapter",
            "name_full": "K-Adapter (adapter-based knowledge injection)",
            "brief_description": "An approach that injects distinct adapter modules to learn different kinds of parametric knowledge while keeping the base PLM parameters frozen.",
            "citation_title": "K-adapter: Infusing knowledge into pre-trained models with adapters",
            "mention_or_use": "mention",
            "model_name": "K-Adapter",
            "model_description": "Adds lightweight adapter modules that are independently trained to encode specific knowledge types (e.g., linguistic, factual); adapters can be trained in parallel and appended to a frozen PLM to avoid re-training base weights.",
            "model_size": null,
            "reasoning_methods": [
                "adapter-based modular knowledge encoding",
                "parametric external memory via adapters"
            ],
            "reasoning_methods_description": "Each adapter module specializes in a knowledge type and is integrated into forward passes to modulate representations; adapters act as separate parametric stores accessible during reasoning.",
            "diversity_of_methods": "diverse in knowledge sources (multiple adapters for different knowledge types) but similar in that each adapter follows the same adapter-interface style—modular diversity rather than algorithmic diversity.",
            "reasoning_task_name": "Knowledge-intensive tasks (e.g., open-domain QA, entity/relation tasks) and adapter-targeted benchmarks",
            "reasoning_task_description": "Tasks that benefit from injecting specialized knowledge modules without altering the base PLM weights.",
            "performance_by_method": "Survey lists K-Adapter among high-performing KEPLMs; specific numeric results are not reproduced in the survey text here (see original K-Adapter paper).",
            "comparison_of_methods": "Survey contrasts adapter-based parametric memory with non-parametric external memory and joint pretraining; highlights adapters' advantage in parallel training and not requiring base model re-training.",
            "key_findings": "Adapter modules provide a practical way to add diverse knowledge types with low disruption to base PLMs; modular/parallel training enables scalable incorporation of multiple knowledge styles.",
            "counter_examples_or_negative_results": "Hou et al. (survey cites) found some KEPLMs (including K-Adapter) incorporated only small amounts of factual knowledge as detected by graph-convolution-based probes — indicating adapters do not guarantee deep factual integration.",
            "uuid": "e3097.5"
        },
        {
            "name_short": "QA-GNN / GreaseLM",
            "name_full": "QA-GNN and GreaseLM (graph-reasoning enhanced LMs)",
            "brief_description": "Models that integrate knowledge graph subgraphs and graph reasoning (GNN-style multi-step interactions) with language models to perform commonsense and QA reasoning.",
            "citation_title": "QAGNN: Reasoning with language models and knowledge graphs for question answering",
            "mention_or_use": "mention",
            "model_name": "QA-GNN; GreaseLM",
            "model_description": "QA-GNN builds and runs a graph neural network over a retrieved KG subgraph to perform multi-step graph reasoning for QA; GreaseLM sets up interaction nodes in both text and KG modalities to iteratively exchange and fuse information.",
            "model_size": null,
            "reasoning_methods": [
                "graph neural network multi-hop reasoning over KG subgraphs",
                "multi-step text-KG interaction nodes and attention-based fusion"
            ],
            "reasoning_methods_description": "Constructs a task-specific KG subgraph (entities and relations) and applies GNN message-passing or iterative attention interactions so that multi-hop inferential chains can be formed explicitly in graph space and fused back into textual representations.",
            "diversity_of_methods": "diverse in style relative to text-only LMs: explicitly uses graph-structured reasoning rather than relying only on implicit parametric reasoning; GreaseLM also mixes modalities via interaction nodes.",
            "reasoning_task_name": "Commonsense question answering / multi-hop QA",
            "reasoning_task_description": "Question-answering datasets requiring reasoning across multiple facts or conceptual hops typically supported by KG structure (CommonsenseQA and similar).",
            "performance_by_method": "Survey states QA-GNN and GreaseLM achieved good results on commonsense QA (no numeric values reproduced in survey text).",
            "comparison_of_methods": "Survey positions graph-reasoning models as improving over pure text-only PLMs for commonsense QA by enabling explicit multi-hop inference; GreaseLM extends QA-GNN ideas with bilateral interaction nodes.",
            "key_findings": "Explicit graph-structured reasoning (multi-hop GNN over KG subgraphs) provides a complementary and effective reasoning style to standard PLM implicit reasoning for tasks needing multi-step inferencing.",
            "counter_examples_or_negative_results": "Graph-based methods can introduce inference-time overhead for subgraph construction and multi-step message passing; no explicit failure cases numerically provided in survey text.",
            "uuid": "e3097.6"
        },
        {
            "name_short": "KG-BART",
            "name_full": "KG-BART (Knowledge Graph Augmented BART)",
            "brief_description": "A generative model that injects knowledge subgraphs into an encoder-decoder architecture to improve generative commonsense reasoning and constrained text generation.",
            "citation_title": "Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning",
            "mention_or_use": "mention",
            "model_name": "KG-BART",
            "model_description": "Augments a BART-style seq2seq model with knowledge subgraph encodings fused into encoder/decoder via multi-headed graph attention so generated text can reflect structured commonsense relations.",
            "model_size": null,
            "reasoning_methods": [
                "knowledge-subgraph augmentation + multi-headed graph attention",
                "generation conditioned on KG context (multi-hop reasoning reflected in decoder)"
            ],
            "reasoning_methods_description": "Encodes KG subgraphs relevant to an input, fuses graph node/relation embeddings with text representations via attention in the encoder/decoder, enabling generation that leverages structured commonsense paths.",
            "diversity_of_methods": "diverse relative to plain seq2seq: integrates structured KG reasoning into generation, enabling a different reasoning modality (graph-based) to influence generative outputs.",
            "reasoning_task_name": "Generative commonsense reasoning tasks (e.g., CommonGen, story ending generation)",
            "reasoning_task_description": "Tasks where the model must generate coherent text constrained or informed by commonsense relations/graphs.",
            "performance_by_method": "Survey indicates KG-BART designed for generative commonsense reasoning and reports it as effective; no specific numeric scores reproduced in survey text.",
            "comparison_of_methods": "Survey notes that KG-BART and other knowledge-subgraph methods improve generative commonsense outputs compared to plain PLM generation, but at cost of subgraph construction and slower inference.",
            "key_findings": "Fusing structured commonsense knowledge into generative models improves plausibility and factuality of generated text for knowledge-intensive NLG tasks.",
            "counter_examples_or_negative_results": "KG-subgraph construction increases inference time and complexity; no explicit numeric counterexamples in survey text.",
            "uuid": "e3097.7"
        },
        {
            "name_short": "Hou et al. probe",
            "name_full": "Understanding the integration of knowledge in language models with graph convolutions",
            "brief_description": "An analysis probing how much factual knowledge KEPLMs actually embed, using graph-convolutional diagnostics, finding limited incorporation in some KEPLMs.",
            "citation_title": "Understanding the integration of knowledge in language models with graph convolutions",
            "mention_or_use": "mention",
            "model_name": "Analysis / probe (applied to KEPLMs such as ERNIE and K-Adapter)",
            "model_description": "Uses graph convolutional simulation probes to detect and quantify factual knowledge integrated into PLMs/KEPLMs, showing some KEPLMs contain only modest amounts of factual information despite their architectures.",
            "model_size": null,
            "reasoning_methods": [
                "graph-convolution-based probing / diagnostic analysis"
            ],
            "reasoning_methods_description": "Constructs graph convolution tests to trace and measure whether KEPLM parameters actually store KG facts and how those facts are used during inference/representations.",
            "diversity_of_methods": "Not an LM for reasoning per se; it is an analysis that contrasts claimed diverse knowledge-incorporation methods with measured factual incorporation (negative/diagnostic result).",
            "reasoning_task_name": "Knowledge probing / diagnostic evaluation of KEPLMs",
            "reasoning_task_description": "Analyses focused on whether KEPLMs truly absorb/verbalize factual KG knowledge and to what extent different integration methods differ in factual incorporation.",
            "performance_by_method": "Survey reports Hou et al.'s finding that KEPLMs such as ERNIE and K-Adapter have incorporated only a small amount of factual knowledge (no numeric probe metrics provided in survey text).",
            "comparison_of_methods": "Provides an explicit negative contrast: complex knowledge-injection architectures do not necessarily guarantee deep factual incorporation; probes find limited factual storage in several KEPLMs.",
            "key_findings": "Caveat that many KEPLMs may appear to integrate knowledge architecturally but, by probing, may contain only modest factual integration, highlighting the need for more effective methods.",
            "counter_examples_or_negative_results": "Direct negative finding: ERNIE and K-Adapter were detected to have only small amounts of factual knowledge by the graph-convolution simulator probe.",
            "uuid": "e3097.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Retrieval augmented language model pre-training",
            "rating": 2,
            "sanitized_title": "retrieval_augmented_language_model_pretraining"
        },
        {
            "paper_title": "Kepler: A unified model for knowledge embedding and pre-trained language representation",
            "rating": 2,
            "sanitized_title": "kepler_a_unified_model_for_knowledge_embedding_and_pretrained_language_representation"
        },
        {
            "paper_title": "LUKE: deep contextualized entity representations with entity-aware self-attention",
            "rating": 2,
            "sanitized_title": "luke_deep_contextualized_entity_representations_with_entityaware_selfattention"
        },
        {
            "paper_title": "Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning",
            "rating": 2,
            "sanitized_title": "kgbart_knowledge_graphaugmented_bart_for_generative_commonsense_reasoning"
        },
        {
            "paper_title": "QAGNN: Reasoning with language models and knowledge graphs for question answering",
            "rating": 2,
            "sanitized_title": "qagnn_reasoning_with_language_models_and_knowledge_graphs_for_question_answering"
        },
        {
            "paper_title": "Greaselm: Graph reasoning enhanced language models",
            "rating": 2,
            "sanitized_title": "greaselm_graph_reasoning_enhanced_language_models"
        },
        {
            "paper_title": "K-adapter: Infusing knowledge into pre-trained models with adapters",
            "rating": 2,
            "sanitized_title": "kadapter_infusing_knowledge_into_pretrained_models_with_adapters"
        },
        {
            "paper_title": "Understanding the integration of knowledge in language models with graph convolutions",
            "rating": 2,
            "sanitized_title": "understanding_the_integration_of_knowledge_in_language_models_with_graph_convolutions"
        },
        {
            "paper_title": "E-bert: Efficient-yet-effective entity embeddings for bert",
            "rating": 1,
            "sanitized_title": "ebert_efficientyeteffective_entity_embeddings_for_bert"
        }
    ],
    "cost": 0.024321999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Knowledge-Enhanced Pre-trained Language Models</p>
<p>Chaoqi Zhen 
Yanlei Shang 
Xiangyu Liu 
Yifei Li 
Yong Chen 
Senior Member, IEEEDell Zhang 
A Survey on Knowledge-Enhanced Pre-trained Language Models
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. <em>, NO. </em>, JAN. 2023 1Index Terms-Natural Language ProcessingPre-trained Language ModelsKnowledge BasesMemory MechanismInterpretability
Natural Language Processing (NLP) has been revolutionized by the use of Pre-trained Language Models (PLMs) such as BERT. Despite setting new records in nearly every NLP task, PLMs still face a number of challenges including poor interpretability, weak reasoning capability, and the need for a lot of expensive annotated data when applied to downstream tasks. By integrating external knowledge into PLMs, Knowledge-Enhanced Pre-trained Language Models (KEPLMs) have the potential to overcome the above-mentioned limitations. In this paper, we examine KEPLMs systematically through a series of studies. Specifically, we outline the common types and different formats of knowledge to be integrated into KEPLMs, detail the existing methods for building and evaluating KEPLMS, present the applications of KEPLMs in downstream tasks, and discuss the future research directions. Researchers will benefit from this survey by gaining a quick and comprehensive overview of the latest developments in this field.</p>
<p>INTRODUCTION</p>
<p>P Re-trained language models (PLMs) are first trained on a large dataset and then directly transferred to downstream tasks, or further fine-tuned on another small dataset for specific NLP tasks. Early PLMs, such as Skip-Gram [1] and GloVe [2], are shallow neural networks, and their word embeddings (learned from window-sized contexts) are static semantic vectors, which makes them unable to deal with the problems of polysemy in dynamic environments. With the development of deep learning, researchers have tried to leverage deep neural networks to boost tasks' performances with dynamic semantic embeddings. At first, people were still limited to the paradigm of supervised learning and thought without enough labeled data it would be difficult to unleash the potential of deep learning. However, with the emergence of self-supervised learning, big language models such as BERT [3] can learn a lot of knowledge from large-scale unlabeled text data by predicting tokens that have been covered up in advance. Thus they have made breakthrough progress in a number of downstream NLP tasks. Since then, many large models have started to adopt Transformer [4] structures and self-supervised learning to solve NLP problems, and gradually PLMs have entered a phase of rapid development. The latest phenomenal success for PLMs is OpenAI's ChatGPT 1 .</p>
<p>As research has progressed, it has been found that PLMs still struggle with poor interpretability, weakness in robustness, and a lack of reasoning ability. Specifically, PLMs are widely recognized as black boxes whose decision process is opaque, thus making them difficult to interpret. Additionally, Manuscript received Dec. 27, 2022; revised May <strong>, </strong>**.</p>
<ol>
<li>https://chat.openai.com/chat PLMs may not be sufficiently robust as deep neural models are susceptible to adversarial examples. Furthermore, PLMs are also limited in their reasoning abilities because they are purely data-driven. All these shortcomings of PLMs can be improved by incorporating external knowledge, which leads to what we call Knowledge-Enhanced Pre-trained Language Models (KEPLMs). Fig. 1 shows the advantages of KEPLMs in the words of ChatGPT.</li>
</ol>
<p>BACKGROUND</p>
<p>In this section, we first introduce the concept of PLMs, and then talk about the recent trend of combining PLMs and knowledge.</p>
<p>Pre-trained Language Models</p>
<p>In 2013, word2vec [50] opened the era of pre-trained language models. First-generation PLMs such as Skip-Gram [1] and GloVe [2] aim to get good word embeddings for downstream tasks directly, and their model architectures are typically shallow neural networks to allow for computational efficiency. [51]. Second-generation PLMs, e.g., LSTM [52] based CoVe [53] and ELMo [54] as well as Transformer [4] based BERT [3] and GPT [55] focus on learning word embeddings in dynamic contexts. During that period, Transformers became most successful in almost all downstream NLP tasks and brought significant changes to the NLP field. Today, PLMs generally refer to models based on the Transformer architecture, under the pre-trainthen-fine-tune paradigm. The representative PLMs include GPT [55] (an auto-regressive language model based on Transformer Decoder), BERT [3] (an auto-encoding language model based on Transformer Encoder), and BART [56] (a sequence-to-sequence model based on both Transformer Encoder and Decoder). Very recently, prompt learning, a new paradigm in NLP, is getting more and more popular [57]. It can help to make better use of knowledge in PLMs and hence empower PLMs with the ability to perform few-shot or even zero-shot learning for challenging scenarios with little or none labeled data.</p>
<p>Knowledge and PLMs</p>
<p>There are two lines of research on the interaction between knowledge and PLMs: one is to use PLMs as Knowledge Bases (KBs), and the other is to enhance PLMs with knowledge. In this paper, we focus on the latter.</p>
<p>Using PLMs as Knowledge Bases</p>
<p>KBs (e.g., Wikidata [58] and ATOMIC [26]) store entities and their relationships, usually in the form of relation triplets. PLMs are considered as a possible alternative to structured KBs, which has attracted many researchers. Beginning with LAMA [59], many researchers explored whether PLMs could serve as structured KBs. Wan et al. [60] explored how to construct KBs using pre-trained language models automatically.</p>
<p>Heinzerling et al. [61] investigated the relationship between accuracy and memory capacity of neural networks, arguing that PLMs can be used as KBs. Safavi et al. [62] argued that relational KBs represent knowledge with high accuracy but lack flexibility. In contrast, Razniewski et al. [63] dived into the strengths and limitations of both PLMs and KBs. They believed that KBs with explicit knowledge could not be completely replaced by PLMs with latent knowledge. Wang et al. [64] found that closed-book question answering is still a challenge for generative models, and therefore generative models are not suitable to serve as KBs. AlKhamissi et al. [65] argued that there are five aspects at which a PLM needs to excel to qualify as a KB and found that three of them (i.e. consistency, reasoning, and interpretability) are better obtained in KBs than in PLMs.</p>
<p>Enhancing PLMs with Knowledge</p>
<p>On the other way around, we can use knowledge to improve or extend PLMs. In many knowledge-intensive downstream tasks, taking question-answering tasks as an example, the amount of knowledge learned by the pre-trained language model can be increased by adding parameters; however, it is far less effective than directly integrating knowledge [66]. Therefore, it is necessary to inject knowledge into PLMs to obtain better performance. Methods such as ERNIE [67], KnowBert [68], K-BERT [69] are early attempts at incorporating knowledge into PLMs, and they have achieved great success especially on knowledge-intensive NLP tasks. Many subsequent models were inspired by them and improved upon them. Nowadays, more and more KEPLMs are emerging, integrating different kinds of knowledge in different ways and dealing with a variety of NLP tasks. In what follows, we will present a comprehensive overview of KEPLMs.</p>
<p>KNOWLEDGE SOURCES FOR KEPLMS</p>
<p>In this section, we elaborate on the common types and formats of knowledge that are incorporated into PLMs.</p>
<p>Types of Knowledge</p>
<p>There are five types of knowledge that are often integrated into PLMs: linguistic knowledge, semantic knowledge, commonsense knowledge, encyclopedic knowledge, and domain knowledge.</p>
<p>Linguistic Knowledge</p>
<p>Part-of-Speech Tags. Commonly used part-of-speech tags include pronouns, verbs, nouns, pre-positions, conjunctions, adverbs, and adjectives. The could help the understanding of natural language text data, e.g., for sentiment analysis. SentiLARE [70] exploits part-of-speech tags to promote sentiment analysis.</p>
<p>Syntactic Structures. Models acquire the structure of sentences via syntactic parsing, mainly including constituency and dependency. Syntax-BERT [71] uses syntax-related masks to incorporate information from constituency and dependency trees. K-Adapter [72] integrates dependency parsing information, which has improved the performance of the dependency relation prediction task.</p>
<p>Cross-lingual Transferability. Sometimes PLMs could obtain cross-lingual transferability through learning from multilingual corpora. For example, as demonstrated by XLM-K [73], the linguistic knowledge about one language might help the processing of another language.</p>
<p>Semantic Knowledge</p>
<p>Semantic knowledge aims to help models catch the meaning of texts. For example, KT-NET [74], SenseBERT [75], and LIB-ERT [76] introduce semantic knowledge from WordNet [77] and perform well on machine reading comprehension, word sense disambiguation, and lexical simplification, respectively.</p>
<p>Basu et al. [78] converted the syntax tree of the text to its corresponding semantic meanings with the help of VerbNet [79] so that the model could understand the text. SemBERT [80] incorporates semantic knowledge from semantic role labeling for better reading comprehension and language inference.</p>
<p>Commonsense Knowledge</p>
<p>Commonsense knowledge is the routine knowledge people have of their everyday world and activities [81]. Commonly used knowledge bases are listed in Table 1.</p>
<p>Commonsense knowledge is represented as triples in KBs, where head and tail entities are more often phrases than just words, which are different from encyclopedic knowledge. For example, a commonsense triple is like (having no food, CauseDesire, go to a store), while an encyclopedic triple is like (China, capital, Beijing).</p>
<p>As shown in Table 1, ConceptNet [9] is the most widely used open-domain commonsense knowledge graph containing 34 relations, such as RelatedTo, IsA, Causes, etc. It's helpful in commonsense question answering, commonsense validation, and commonsense story generation.</p>
<p>ATOMIC [26] focuses on inferential knowledge organized by "if-then" structure, e.g., "if X pays Y a compliment, then Y will likely return the compliment", covering causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states, which is helpful for commonsense reasoning. It can be employed in commonsense question generation and commonsense question-answering tasks, such as Guan et al. [23] and Mitra et al. [27].</p>
<p>ATOMIC 20 20 [28] covers more accurate and diverse commonsense knowledge than the aforementioned commonsense knowledge sources, including three categories of social interaction, physical and event-centered. Hosseini et al. [29] convert triples (in ATOMIC 20 20 [28]) into natural language sentences for pre-training, which improves the performance on causal pair classification and commonsense of answering questions tasks.</p>
<p>ASER [30] is a large-scale eventuality knowledge graph, with events as nodes and discourse relations as edges. It provides more complicated commonsense knowledge, such as the cause-effect relation between "Jim yells at Bob" and "Bob is upset".</p>
<p>Encyclopedic Knowledge</p>
<p>Encyclopedic knowledge covers widespread information in the open domain, in the form of texts or triples. Wikipedia 2 is a multilingual encyclopedia that is unstructured. BERT utilizes Wikipedia as pre-training data to learn contextual representations; while other methods usually leverage encyclopedic knowledge via knowledge triples.</p>
<p>Wikidata [58] is the most widely used knowledge graph when incorporating encyclopedic knowledge. KEPLMs such as K-Adapter [71], ERNIE [67], KgPLM [82], and ERICA [83], use Wikidata [58] as knowledge sources. Other commonly used English encyclopedic knowledge graphs include Freebase [84], DBpedia [85], and NELL [86]. CN-DBpedia [87] is a widely used Chinese encyclopedic knowledge graph. KEPLMs designed for Chinese downstream tasks, such as K-BERT [69], use CN-DBpedia as knowledge sources.</p>
<p>Wikidata5M is a large-scale knowledge graph proposed by KEPLER [88] that contains high-quality descriptions of entities and relations in addition to triples. KEPLER [88] used these descriptions to initialize knowledge embeddings, and CoLAKE [89] also adopted this approach.</p>
<p>Domain Knowledge</p>
<p>In contrast to encyclopedic knowledge, domain knowledge is knowledge of a specific, specialized field discipline, such as biomedical, e-commerce, and sentiment, which are explored a lot, as shown in Table 2.</p>
<p>Biomedical knowledge is usually represented as triples containing symptoms or diseases as head or tail entities, e.g., (bacterial pneumonia, with associated morphology, inflammation). E-commerce knowledge is formed with product names, while their descriptions are represented by a set of phrases. For example, the product "iPhone XS" is described as "iOS; 4G signal; T-Mobile service; OLED screen; ...". Sentiment knowledge could be represented in many ways, including sentiment words, word polarity, etc.</p>
<p>Formats of Knowledge</p>
<p>There are four formats of knowledge that are often incorporated into PLMs, i.e., entity lexicon, knowledge graph, plain text, and labeled images.</p>
<p>Entity Lexicon</p>
<p>To incorporate knowledge through entities, we need to integrate knowledge embeddings of entities into aligned token embeddings. Existing models propose two methods to obtain initial entity embeddings, obtained through traditional knowledge embedding algorithms, such as TransE used by ERNIE [67] and CokeBERT [90], or through encoding entity descriptions, such as KEPLER [88].</p>
<p>The first approach can fuse the information of neighboring nodes of entities in the knowledge base. But it must face the challenge of heterogeneous embedding space because the embedding vector space of words in the text and entities in KG is inconsistent. The second approach fuses the information in the same embedding space, but the entity embedding may not fully express the meaning of the entities.</p>
<p>It's simple and intuitive to inject knowledge in the form of entity embeddings. However, entity embeddings need to be retrained when the knowledge graph is updated, and the model parameters have to be retrained if injected in the pre-training phase.</p>
<p>Knowledge Graph</p>
<p>Triples. Knowledge stored in the knowledge graphs is commonly in the form of semantic (RDF) triples. To incorporate triples, we can append triples to the proper position in the text, such as K-BERT [69], ERNIE 3.0 [91], Zhang et al. [11], and Bian et al. [92], or integrate their embeddings into text embeddings, such as Liu et al. [93]. More details are in Section 4.2.</p>
<p>Subgraphs. Knowledge subgraphs are part of knowledge graphs that take entities as nodes and relationships as edges. KEPLMs such as QA-GNN [12], GreaseLM [13], KG-BART [17], and KALA [94] incorporated knowledge in the form of knowledge subgraphs, which are described in detail in Section 4.2.2.</p>
<p>Plain Text</p>
<p>To integrate knowledge in texts, we can convert knowledge triples to sentences as the pre-training corpus or add related entity definitions to texts. Commonsense knowledge triples are suitable to be converted into sentences. or instance, Hosseini et al. [29] convert the triples (PersonX accidentally fell, xEffect, PersonX breaks an arm) in ATOMIC 20 20 into the sentence "Tracy accidentally fell. As a result, Tracy breaks an arm.", and fed them to the model for continually pre-training. This method is suitable for commonsense knowledge bases because the triples in them are usually phrases and only need to add conjunctions to obtain sentences.</p>
<p>Dict-BERT [95] append the definitions of the rare words to the end of the text as model input, which facilitates the model's understanding and learning of rare words, but does not apply to polysemous words.  </p>
<p>Captioned Images</p>
<p>Unlike the knowledge presented above in the form of texts, visual knowledge refers to knowledge observed through the eyes, including the shape, size, and color of objects, which may not be mentioned in the text and need to be learned through images. To incorporate visual knowledge, models can first retrieve context-related images, encode them, and then integrate the embeddings of images to text embeddings, such as VALM [96], as shown in Fig. 2. Visual knowledge can also be incorporated through text-image alignment pretraining objectives, such as Vokenization [97].</p>
<p>BUILDING KEPLMS</p>
<p>When we construct KEPLMs, external knowledge could be incorporated into PLMs implicitly and/or explicitly.</p>
<p>Implicit Incorporation of Knowledge</p>
<p>Knowledge-Guided Masking Strategies</p>
<p>PLMs represented by BERT generally use unstructured text documents from Wikipedia etc. as the corpus for pre-training. The unstructured text data contain rich contextual semantic information from which BERT could learn the contextual knowledge of words through Masked Language Modelling (MLM). However, entities and phrases in the text that also contain valuable information have been ignored. By employing a knowledge-guided masking strategy beyond the level of individual words, PLMs are able to incorporate the knowledge about entities and phrases etc., as shown in Fig. 3.  ERNIE [98] adds entity-level and phrase-level masking strategies to BERT, and thus guides the pre-training of BERT to incorporate the entity and phrase information from text. SKEP [47] proposes to mask not entities or phrases but sentiment words so as to inject sentiment knowledge into text representations.</p>
<p>Different from the simple random selection of entities or phrases for masking (as in ERNIE), GLM [15] uses knowledge-graph informed sampling that assigns higher weights to more important entities. Specifically, GLM's masking strategy would mask a general word 20% of the time and an entity 80% of the time. When GLM needs to mask an entity, those entities which can reach other entities in the sentence within a specific number of hops in ConceptNet [9] are considered more critical and given higher probabilities to be chosen. In this way, GLM can nudge the construction of KEPLMs towards more critical entities in the knowledge graph. As illustrated in Fig. 4, assuming that among the four entities in the given sentence, three of them "sick", "baby" and "cry" could be reached within a specific number of hops in ConceptNet while the other one "sometimes" could not, GLM would give the former more chances than the latter to be masked for pre-training when entity-level masking is activated ; the other non-entity words in the sentence would be sampled only when word-level masking is activated.</p>
<p>Instead of using a predefined probability to choose between the two modes of masking (as in GLM), E-BERT [40] proposes an adaptive hybrid masking strategy that allows the model to switch between word-level and phrase-level masking in an adaptive fashion during its pre-training. As illustrated in Fig. 5, E-BERT [40] enters the mode of wordlevel masking when r &lt; α t and the mode of phrase-masking otherwise, where r is a randomly generated number in each iteration. The loss functions L w and L p of the two modes in each iteration are used to track the fitting progress of the learned word-level information and the learned phrase-level information, represented by η t w and η t p , respectively. The relative importance of word-level masking with respect to phrase-level masking, r t , is used further to calculate α t+1 as in Eq. (1), so that the mode with higher loss in the current iteration is more likely to be selected in the next iteration. Thus, E-BERT can switch between the two modes of masking adaptively and strike a balance between them.
η t w = ∆ t,t−1 w /∆ t,1 w = L t−1 w − L t w + / (L 1 w − L t w ), η t p = ∆ t,t−1 p /∆ t,1 p = L t−1 p − L t p + / (L 1 p − L t p ), r t = η t+1 w /η t+1 p , α t+1 = tanh r t .(1)</p>
<p>Knowledge-Related Pre-training Tasks</p>
<p>Some methods for building KEPLMs incorporate knowledge implicitly by adding knowledge-related pre-training tasks, as shown in Fig. 6. For example, KALM [99] enriches the input sequence with entity signals and then adds an entity prediction task to the pre-training objective in order to help the model learn entity information better. KEPLER [88] adds the knowledge embedding pre-training task which shares a Transformer Encoder with the MLM, obtaining text-enhanced knowledge embeddings and knowledge-enhanced PLMs simultaneously. Vokenization [97] proposes the concept of voken (visualized token), i.e., token-related images; it adds a voken classification task that predicts the image corresponding to each token so as to enhance the PLMs with visual knowledge which has been shown to help some downstream NLP tasks.</p>
<p>Explicit Incorporation of Knowledge</p>
<p>There are mainly three ways for PLMs to incorporate external knowledge explicitly: modifying the model input, adding knowledge fusion modules, and utilizing external memory. The first two approaches insert relevant knowledge into PLMs, in the form of either additional input for the model or additional components in the model, as shown in Fig. 7 x and y. The third approach keeps the text and knowledge spaces independent which can facilitate knowledge updates. There exist a few different ways to incorporate knowledge in the form of triples. ERNIE 3.0 [91] prepends related triples to the sentences as the expanded model input. K-BERT [69] injects relevant triples into each sentence to generate a sentence tree for model input. To be specific, if the input sentence has an entity "apple", K-BERT [69] will find the triples whose head entity is "apple" in the knowledge graph and then append the relation and tail entity of these triples to "apple" to generate a new sentence tree. A visible matrix is created to control the level of knowledge noise. Zhang et al. [11] improve the visible matrix of K-BERT to further minimize the introduction of knowledge noise. CoLAKE [89] also introduces triples to the input text, treats the text as a fully connected word graph, and integrates knowledge to form a word-knowledge graph. It takes inspiration from K-BERT and makes some improvements in the reduction of knowledge noise. For the question answering task, Bian et al. [92] convert multiple question-related knowledge triples into text according to predefined templates and feed them into the model together with the question and alternative answers for training, which obtains excellent performance on commonsense question answering. For all the above methods that insert knowledge triples to the model input, the introduction of external knowledge may damage the original sentence structure, and therefore we must try to reduce knowledge noise in this process.</p>
<p>Modifying the Model Input</p>
<p>There are also a few different ways to incorporate knowledge in the form of entities. Dict-BERT [95] obtains the definitions of rare words in a sentence from Wiktionary [100] and appends them to the end of the sentence. Similarly, DKPLM [101] focuses on long-tail entities and uses pseudo token representations from relevant triples to replace their embeddings. Unlike the above methods, WKLM [102] replaces entities in the text with other entities of the same type, which are then fed into the model. Then the model is asked to determine which entities in the sentence are correct and which are replaced. This method does not modify the model, only the input data during its pre-training. A few high-performance KEPLMs constructed using this method are described in detail below.</p>
<p>CoLAKE [89] modifies the model input to incorporate knowledge of entities, as shown in Fig. 8. Specifically, CoLAKE regards each input sentence as a fully connected graph. It takes the entity in the input sentence as the anchor node and introduces a subgraph (composed of triples with that anchor node as the head entity in the knowledge graph) to obtain the word knowledge graph. Then the newly added nodes from the word knowledge graph are appended behind the original input text and fed into the PLMs together for pretraining. CoLAKE distinguishes the node types in the newly obtained input statement and initializes different nodes differently. These nodes include word nodes, entity nodes, and relation nodes. CoLAKE achieves a 5.2% improvement on the relation classification task in comparison to BERT without knowledge integration.</p>
<p>DKPLM [101] proposes the concept of long-tail entities which represent the entities not been fully learned by the model from the corpus. Strengthening the learning of such long-tail entities in the pre-training stage can enhance the model's understanding of semantic context and eventually the language representation. For this purpose, a measurement method KLT has been proposed to identify long-tail entities: the entities with a KLT score below the average in each sentence are regarded as the long-tail entities of that sentence. The KLT score of an entity e is calculated as
KLT (e) = I F req(e)&lt;R f req · SI (e) · KC (e) ,(2)
where the three terms in the equation represent the occurrence frequency of the entity in the corpus, the semantic importance, and the number of neighboring nodes within a certain number of hops in KG, respectively. As illustrated in Fig. 9, DKPLM replaces the embeddings of long-tail entities detected in the text with pseudo token embedding as new input to the model. For example, suppose that the input sentence is "Yao, was selected to start for the Western Conference in the NBA All-Star Game eight times" where "Western Conference" and "All-Star Game" have been  identified as long-tail entities, so the embeddings of them will be replaced by pseudo token embeddings shown as "[LTE]". A pseudo token embedding is encoded by related triples in the knowledge graph and the entity's description in a certain way. The F 1 scores of DKPLM [101] on entity classification and relation classification are 2.1% and 2.87% higher than RoBERTa [103] respectively, confirming that knowledge about long-tail entities could be incorporated into PLMs to obtain better language representation and higher model performance.</p>
<p>Adding Knowledge Fusion Modules</p>
<p>Different from the methods introduced in Section 4.2.1, the methods presented in this section all involve the fusion of different modal spaces. Specifically, the text and knowledge modalities are encoded differently, and additional modules are constructed for inter-modal fusion. As illustrated in The method shown in Fig. 10 (a) can be further divided into two categories. One is the T-K structure represented by ERNIE [67] which mainly incorporates knowledge in the form of entity embeddings: a T-Encoder is followed by a K-Encoder, where T-Encoder encodes the text corpus and K-Encoder integrates the entity embeddings in the knowledge space into the entity embeddings in the text space. Many KEPLMs follow this structure but differ in how they get entity embeddings. The entity embedding in ERNIE is obtained by TransE which takes a single triple as a training sample and does not contain the information of that entity's  neighbor nodes. Developed on top of this architecture, BERT-MK [32] fully considers the information of neighbor nodes when learning the entity embedding in the knowledge space, incorporating more semantic information. CokeBERT [90] found that the entity embeddings in the former method cannot change dynamically according to the textual context. To overcome this limitation, the closer the meaning of the neighbor node is to the text, the more its information will be incorporated into the entity embedding by CokeBERT. The second class of methods attaches other knowledge fusion structures after the PLM. Some KEPLMs use the attention mechanism to fuse the information in the text and knowledge modalities. Kwon et al. exploited an attention mechanism to incorporate sentence-related triples into textual embedding representations [104]. JointLK [14] lets each question token attend on KG nodes and each KG node attend on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. KET [46] ... ...  Fig. 11. The interaction nodes for text-knowledge information fusion [13].</p>
<p>LM</p>
<p>adopts a hierarchical self-attention mechanism to incorporate sentiment knowledge into text representations. Besides, Liu et al. [93] encode the relevant triples in the context and then fused them with the embeddings of the text using a gate mechanism. There are other works based on interaction nodes. Both modalities exchange information through interaction nodes. QA-GNN [12] incorporated information from the text space into the knowledge space through interaction nodes and achieved good results in commonsense question answering. Inspired by this, GreaseLM [13] set up interaction nodes in both modalities to learn the knowledge of that modality separately and then exchange information at the fusion layer to learn the knowledge of the other modality, as shown in Fig. 11. The approach represented in Fig. 10 (b) is to add a knowledge fusion module between the Transformer layers of PLM. KnowBERT [68] adds new modules between Transformer Encoder blocks to incorporate knowledge about entities in sentences. It considers the problem of polysemy that is ignored by ERNIE [67]: for an entity that exhibits different semantic meanings in different contexts, the knowledge about it is incorporated according to its specific meaning. KG-BART [17] added knowledge fusion modules between the Encoder and Decoder layers to integrate information from knowledge subgraphs into the textual representation through a multi-headed graph attention mechanism. JAKET [105] divides the pre-trained language model into the first six layers and the last six layers. After the text passes through the first six layers of the encoder, the hidden layer representation is obtained, and so is the entity embedding representation. At each entity position in the text, the corresponding entity embedding representation is added and then input to the last six layers of the model for subsequent training. The knowledge space and the text space can cyclically reinforce each other for the learning of better representations. The approach represented in Fig. 10 (c) is to add a fusion module inside the Transformer Layer. For example, KALA [94] inserts the knowledge fusion module inside the Transformer block layer, which is inspired by the idea of modulation, i.e., to modulate the embeddings in the text space with the knowledge in the knowledge space. Adding knowledge fusion modules in this way is intuitive, and the incorporated knowledge is mainly entity representation. Some methods consider the context of entities in the knowledge graph, e.g., BERT-MK [32]; some others filter entity neighbor nodes for embedding based on text context, e.g., CokeBERT [90].</p>
<p>Utilizing External Memory</p>
<p>The third method for building KEPLMs explicitly uses external memory, and thus keeps the knowledge space and text space separate.</p>
<p>In Fig. 12, x illustrates the method to apply nonparametric knowledge from external memory to downstream NLP tasks. KGLM [106] selects and copies the facts from a related knowledge graph to generate factual sentences. In other words, it uses a knowledge base to expand the vocabulary to supply information it has never seen before. REALM [107] introduces a knowledge retriever to help the model retrieve and process documents from the knowledge corpus, and thus improves the performance of open-domain question answering. It only needs to update the knowledge corpus if the world knowledge changes.</p>
<p>In Fig. 12, y illustrates the method of learning parametric knowledge using an additional module independent of the PLM. K-Adapter [72] adds adapters to learn parametric knowledge, and the parameters of the PLM itself remain unchanged during pre-training. Such adapters are independent of each other and can be trained in parallel. In addition, more adapters can be added when needed.</p>
<p>RAG [108] that combines nonparametric and parametric memory outperforms other parametric-only and nonparametric-only models in three open domain questionanswering tasks. Furthermore, for text generation, it can create more specific, diverse, and factual text than other parameter-only baselines. Wilmot et al. [109] extend RAG [108] by adding a memory module to improve the models' predictive performance.</p>
<p>When the knowledge base has undergone some changes, keeping the knowledge in external memory has the big advantage that the KEPLM does not require re-training, which is particularly helpful for the application domains where knowledge is updated frequently.</p>
<p>EVALUATING KEPLMS</p>
<p>This section presents methods for evaluating KEPLMs in terms of knowledge capacity, effectiveness, and efficiency.</p>
<p>Knowledge Capacity</p>
<p>The amount of knowledge incorporated into KEPLMs could be assessed using knowledge probes such as LAMA [59] and LAMA-UHN [110]. Intuitively, KEPLMs containing more knowledge would be more powerful for downstream NLP tasks.</p>
<p>LAMA</p>
<p>LAnguage Model Analysis (LAMA) probe [59] provides a series of completion statements that assess how much knowledge is stored in the model by the average accuracy of the model predictions. Knowledge sources for LAMA include Google-RE [111], T-Rex [112], ConceptNet [9], and SQuAD [113]. The Google-RE corpus [111] contains five kinds of relational triples, among which "place of birth", "date of birth", and "place of death" were selected by LAMA and transformed into fill-in-the-blank sentences according to the artificially constructed templates. For example, the triple "place of birth" is built as "[S] was born in [O]", where S represents the head entity and O represents the tail entity. T-Rex [112] is a subset of Wikidata containing 41 relations. The triples in it were also manually transformed into fill-in-theblank sentences. LAMA also selects triples from ConceptNet, covering 16 relations. For these triples, it finds the OMCS sentence containing both the head entity and the tail entity, then masks the tail entity within the sentence to construct a fill-in-the-blank sentence. LAMA [59] selected 305 questionanswer pairs from SQuAD and manually constructed fill-inthe-blank sentences. For example, the question "Who developed the theory of relativity?" was rewritten as "The theory of relativity was developed by ". LAMA is generally recognized, and many existing work utilize LAMA [59] to measure how much knowledge the model has learned, as shown in Table 3.</p>
<p>In Table 3, DKPLM [101] performs better overall, which shows that long-tail entity-based learning helps the model remember factual knowledge. EAE [114] learns entity representations directly from text rather than integrating entity knowledge into the model and performs well on all three datasets related to factual knowledge, which illustrates the effectiveness of the method. [110] found that for the fill-in-the-blank sentences of LAMA [59], the model may answer depending on the surface form of the entity name; for example, in predicting the language spoken by a person with an Italian-sounding name, the model would predict that the person speaks Italian.  To prevent the model obtaining answers from helpful entity names, E-BERT [110] proposes LAMA-UHN (UnHelp-fulNames), a subset of LAMA [59] that focuses on factual knowledge, which deletes sentences with overly helpful entity names. Models such as CoLAKE [89], KEPLER [88], DKPLM [101], KgPLM [82], and K-Adapter [72] are also evaluated on LAMA-UHN, as shown in Table 3. The performances of the models on LAMA-UHN are much lower than those on LAMA, indicating that LAMA-UHN is more challenging to the model and it can better detect how much knowledge the model can actually learn.</p>
<p>LAMA-UHN E-BERT</p>
<p>LAMA-UHN LAMA-Google-RE LAMA-T-REx ConceptNet SQuAD LAMA-UHN-Google-RE LAMA-UHN-T-REx</p>
<p>Other Knowledge Probes</p>
<p>In addition to LAMA and LAMA-UHN, there are other new knowledge probes. LPAQA [115] considered that some sentences in LAMA and LAMA-UHN might be constructed inappropriately so that they only provide a lower bound estimate of the knowledge contained in an LM. That is, the model might know the answer but could not give the correct answer because of the inappropriate way of questioning. For example, for the sentence "Obama is a by profession", the model is asked about Obama's profession, but the expression is unclear. If it is replaced by "Obama worked as a ", it may predict more accurately. LPAQA aims to estimate the knowledge contained in LMs more accurately. Dolphs et al. [116] applied example queries to LAMA probes, and the model's performance improved significantly, which also shows that we need to detect the knowledge contained in the language model properly to avoid underestimating the model. Unlike the above methods, AutoPrompt [117] proposes an automated way to create prompts for measuring the amount of knowledge contained in LMs. This method saves time and effort. Moreover, prompts created by AutoPrompt can estimate knowledge in LM more accurately than manually created ones. In addition to general domain knowledge exploration, Meng et al. propose a biomedical knowledge exploration benchmark named MedLAMA [118].</p>
<p>Effectiveness</p>
<p>We assess the effectiveness of a method by analyzing whether it maintains the original language representation capabilities and how many tasks' performances it can improve. We choose GLUE [119] and KILT [120] as corresponding benchmarks.</p>
<p>General Language Understanding Tasks</p>
<p>We choose the General Language Understanding Evaluation (GLUE) dataset [119] as the benchmark to assess the general language representation capabilities maintained by KEPLMs. It is a benchmark used to measure the performance of language models, containing nine natural language understanding tasks. It is the primary evaluation benchmark used by BERT. Many KEPLMs based on BERT or ROBERTa [103] were tested on GLUE to explore whether incorporating knowledge affects the model's performance in natural language processing tasks. ERNIE [67] has been tested on eight datasets of GLUE with essentially the same performance as BERT-base. It found that no additional knowledge is needed to process the tasks in GLUE, and the model does not cause a loss of textual information after incorporating knowledge. CoLAKE [89], SenseBERT [75], AMS [19], and other models have also been tested on GLUE and found that the way they incorporated knowledge did not affect the original language representation capabilities of the models. CoLAKE [89] and AMS [19] found that solving tasks in GLUE does not require encyclopedic and commonsense knowledge, respectively. Although we do not know whether all models incorporating knowledge affect the language representation capabilities of the models, the experiments done by the above models suggest that maintaining the original language representation capabilities of the model while incorporating knowledge is the goal pursued by the researchers.</p>
<p>Knowledge-Intensive Language Tasks</p>
<p>Most KEPLMs are designed for specific tasks, such as KG-BART [17] for generative commonsense reasoning, ExBERT [21] for natrual language inferance, GreaseLM [13] for question answering, and so forth. These models may perform well in one task but fail in others. KEPLMs that can elevate performance on more tasks at the same time are of higher value, such as KGI [121], which can simultaneously improve the performance of fact checking, slot filling, opendomain QA, and dialog generation.</p>
<p>We choose the Knowledge Intensive Language Task (KILT) dataset [120] as the benchmark to analyze the effectiveness of KEPLMs on different knowledge-intensive tasks. It contains 11 datasets in 5 categories of tasks, including Factchecking, Entity linking, Slot filling, Open-domain QA, and Dialog generation. All tasks are based on the same Wikipedia snapshot, which aims to facilitate the development of generalpurpose models and enable their comparisons.</p>
<p>Efficiency</p>
<p>We assess the efficiency of a KEPLM by considering its model size and required computational resources.</p>
<p>Model Size</p>
<p>Incorporating more knowledge would necessarily mean the expansion of the PLM. Usually the performance of a language model increases with its size (i.e., the number of the model parameters) [66], as shown in Table 4. Briefly speaking, for the same level of performance, the smaller the model size, the more efficient the KEPLM. In Table 4, the performance of T5 on these two questionanswering tasks increases with model parameters. That is to say, adding parameters can increase knowledge to some extent. However, the improvement of tasks is far less than the increment of parameters, which is expensive. KEPLMs such as EAE [114], REALM [107], and RAG [108] adopt different methods to incorporate knowledge, which has far fewer parameters than T5-11B but better performance, indicating that incorporating knowledge properly can significantly improve performance efficiently.</p>
<p>Computational Resources</p>
<p>Computational resources usually increase with the model size and can also be a metric for evaluating the efficiency of KEPLMs. Precisely, we assess KEPLMs mainly through the training time and GPU or TPU used, as shown in Table 5.</p>
<p>We take Table 5 as an example to show how to compare the efficiency of KEPLMs by computing resources. First, we compare the GPUs used by the models. NVIDIA V100 GPU outperforms NVIDIA RTX 2080 Ti GPU. Then, we multiply the number of GPUs by the training time to roughly calculate the training time required for the model to run on just one GPU. SentiLARE takes 80 hours; DKPLM takes 96 hours; and CoLAKE takes 304 hours. The GPU used by SentiLARE is not as good as DKPLM and CoLAKE, and SentiLARE requires less training time, so SentiLARE requires the least computing resources. The GPU memory capacity used by DKPLM is smaller than CoLAKE, and the training time of DKPLM is smaller than CoLAKE, so DKPLM requires less computing resources than CoLAKE. Therefore, the computing resources required by these three models from low to high correspond to SentiLARE, DKPLM, and CoLAKE. The efficiency from high to low corresponds to SentiLARE, DKPLM, CoLAKE.</p>
<p>To sum up, we first look at the GPUs used by the models; then, we multiply the number of GPUs by the training time to roughly calculate the total training time using only one GPU. When GPU performance is close, the model with less total training time is more efficient.</p>
<p>APPLYING KEPLMS</p>
<p>KEPLMs are able to boost the performance of knowledgeintensive downstream tasks which can be grouped into two categories according to whether there is new natural language content created by the model.</p>
<p>Knowledge-Enhanced NLU</p>
<p>KEPLMs based on Transformer encoder only or encoderdecoder could be used for natural language understanding (NLU) tasks, such as entity typing, entity recognition, relationship extraction, sentiment analysis, question answering, language-based reasoning, and knowledge graph completion.</p>
<p>Entity Typing</p>
<p>Given an entity mention and its context, entity typing requires the model to classify the semantic type of the entity mention. FIGER [122] and Open Entity [123] are the most commonly used datasets. We found that models such as CoLAKE [89], KnowBERT [68], KEPLER [88], DKPLM [101], LUKE [124] are only tested on Open Entity; ERICA [83] is only tested on FIGER; ERNIE [67], CokeBERT [90], and K-Adapter [72] experiments on both datasets. Open Entity is more widely used, and we think the reasons are as follows. The training set of FIGER is annotated by remote supervision, and the testing set is manually annotated. Open Entity uses manual annotation for both datasets and has more types and finer-grained classification than FIGER. In addition, BERT-MK [32] performs entity typing in the medical domain, using the datasets 2010 i2b2/VA [125], JNLPBA [126], and BC5CDR [127].</p>
<p>Most of the above methods insert special tokens before and after entity mentions in a given sentence to mark entity mentions (e.g., "he had a differential diagnosis of [E] asystole [/E]") and then use the embeddings of the special symbol preceding the entity mention (i.e., [E]) to predict the entity type.</p>
<p>Li et al. [128] proposed a new dataset WikiWiki, containing 10 million Wikipedia articles with each entity connected to the knowledge graph of Wikidata [58]. Compared with the existing fine-grained type recognition datasets, Wikiwiki is larger and more accurate, which can also be used for entity typing tasks.</p>
<p>Entity Recognition</p>
<p>The Named Entity Recognition (NER) task requires the model to identify the entity mentioned in a given text. In has been the basis for many NLP applications in both general and specific domains (like biomedical), as shown in Table 6.</p>
<p>Relation Extraction</p>
<p>KEPLMs could help to improve the extraction (and classification) of the relations between entities in a given text document. In addition to the public field, this task is more commonly used in the biomedical domain. As can be seen from Table 7, the commonly used datasets in the general field are TACRED [143] and FewRel [145], and TACRED is used more than FewRel. We find that all methods using these two datasets perform better on FewRel than on TACRED. That is to say, TACRED is more challenging than FewRel, so more and more methods tend to test model performance on TACRED. There are many work explicitly designed for the biomedical domain, and just like NER, relation classification tasks are helpful for models to learn domainspecific knowledge.</p>
<p>Sentiment Analysis</p>
<p>There are two kinds of sentiment analysis tasks: sentencelevel sentiment analysis and aspect-level sentiment analysis. Sentence-level sentiment analysis requires models to determine the sentiment polarity of sentences, and commonly used datasets are Stanford Sentiment Treebank SST-2 [151] and Amazon-2 [152]. Aspect-level sentiment analysis requires the model to analyze the sentiment polarity in different aspects of the context, and commonly used datasets are SemEval-2014 Task 4 [153]. SentiLARE [70] and SKEP [47] obtained better results than pure PLM on both sentencelevel and aspect-level tasks by incorporating sentiment knowledge. Through sentiment analysis, REMOTE [48] could detect hate speech, and KET [46] could detect sentiment in dialogues, which helps question-answering robots make better responses.</p>
<p>Question Answering</p>
<p>Question answering tasks include machine reading for question answering (MRQA), open-domain question answering (Open-domain QA), and multiple-choice question answering (Multiple-choice QA) according to the question form. We present commonly used datasets for each task in Table 8.</p>
<p>In Table 8, MRQA, also known as Extractive Question Answering, provides questions and related articles requires the model to find answers from the provided articles. The most commonly used dataset is SQuAD 1.1 [113].</p>
<p>The Open-domain QA task gives the questions without the articles containing answers, requiring models to retrieve relevant articles. Methods such as REALM [107], K-ADAPTER [71], WKLM [102], and EAE [114] are tested using some of the corresponding datasets in Table 8, and they achieve better results than the competitive baselines after incorporating encyclopedic knowledge.</p>
<p>The multiple-choice QA task requires the model to select the correct answer based on the question and the options given. The three datasets listed in Table 8 are all commonsense question-answering tasks. Among them, Com-monsenseQA [159] has five options, OpenBookQA [160] and CosmosQA [161] have four options, and CommonsenseQA is the most widely used.</p>
<p>From Table 8, we can see that the datasets of Opendomain QA and MRQA overlap. Open-domain QA only removes the articles provided to the model based on MRQA and asks the model to retrieve them by itself so that they can share datasets.</p>
<p>Language-based Reasoning</p>
<p>Representative KEPLMs used for reasoning tasks include SMedBERT [34] and Li et al. [162] for natural language inference, KMLMs [132] for logical reasoning, Andor et al. [163] for mathematical reasoning, Chang et al. [24] and Vokenization [97] for commonsense reasoning, CoCoLM [31] for reasoning about the temporal order of events, and VALM [96] for reasoning about object color and size.</p>
<p>Knowledge Graph Completion</p>
<p>Knowledge graphs often suffer from incompleteness, and many relationships between entities are missing. KEPLMs can help infer missing links and complement knowledge graphs to a certain degree.</p>
<p>Models such as GLM [15] were tested on the WN18RR [164] and CKBC [165] sets. It outperforms some translation-based graph embedding models and graphs convolutional networks on WN18RR. CKBC is a generic knowledge graph derived from OMCS [166], and GLM [15] outperforms KG-BERT [167], a model specifically designed for the knowledge graph completion task, on this dataset. K-PLUG [42] performs the e-commerce knowledge graph completion task on MEPAVE [168], which gives a textual description of a product and asks the model to output the attribute values of the product.</p>
<p>Knowledge-Enhanced NLG</p>
<p>KEPLMs based on Transformer decoder only or Transformer encoder-decoder could be used for natural language generation (NLG) tasks, such as sentence generation, dialogue generation, question generation, and answer generation.</p>
<p>Sentence Generation</p>
<p>The sentence generation task requires models to generate reasonable sentences, and commonly used datasets are CommonGen [169] and ROCStories [170]. CommonGen requires models to generate a coherent, proper sentence based on 3-5 given concepts, while KG-BART [17] does so by incorporating commonsense knowledge subgraphs. Models such as GRF [20], Guan et al. [171], and Guan et al. [23] can generate plausible story endings with the help of commonsense knowledge.</p>
<p>Dialogue Generation</p>
<p>Dialogue generation tasks require the model to generate responses based on the context of the dialogue. Knowl-edGPT [172] chose to conduct experiments on Wizard [173] and CMU DoG [174] datasets. Wizard has a wide range of topics, while CMU DoG only focuses on the movie domain.  [131] KALA [92], LUKE [124], KMLMs [132] Biomedical English i2b2 [123], [133], [134] UmlsBERT [ [127], NCBI-disease [137], BC2GM [138] KeBioLM [35], BioBERT [139] Finance</p>
<p>Finance NER [140] K-BERT [69] Social media WNUT-17 [141] KALA [92] Cross-lingual WikiAnn NER [142] KMLMs [132]  SMedBERT [34] GAD [146], DDI [149], ChemProt [150] KeBioLM [35] [113], NewsQA [154], TriviaQA [155], SearchQA [156] KT-NET [74], KgPLM [82] Open-Domain Natural Questions [157], Web Questions [158], TriviaQA [155], SearchQA [156] REALM [107], K-ADAPTER [71], WKLM [102], and EAE [114] Multi-Choice CommonsenseQA [159], OpenBookQA [160], CosmosQA [161] QA-GNN [12], GreaseLM [13], JointLK [14] 6.</p>
<p>Question Generation</p>
<p>The question generation task requires the model to generate questions based on the answers. RAG [108] proposes the Jeopardy Question Generation task, where Jeopardy consists of trying to guess an entity from the facts about it. For example, given the answer "The World Cup", models need to generate relevant fact that points to the answer, like "In 1986, Mexico scored as the first country to host this international sports competition twice."</p>
<p>Answer Generation</p>
<p>Unlike standard question-answering (QA) tasks mentioned above in Section 6. </p>
<p>FUTURE DIRECTIONS</p>
<p>In the above sections, we have presented KEPLMs from multiple perspectives, but there are still some other opportunities. Here we outline and discuss a few promising research directions for KEPLMs. Utilizing More Types of Knowledge. As mentioned in Section 3, existing KEPLMs have considered many types of knowledge, but there are other types of knowledge worth investigating. For example, temporal knowledge graphs such as HyTE [176] contain events that reflect the relationships between different entities over time, so incorporating them into PLMs could help to perform time-related reasoning tasks. Moreover, the phenomenal success of ChatGPT has demonstrated the power of incorporating the knowledge of human intentions and preferences into PLMs directly through Reinforcement Learning from Human Feedback (RLHF) [177].</p>
<p>Improving the Effectiveness of Knowledge Incorporation. As described in Section 4, a variety of technical approaches to incorporating knowledge into PLMs have been proposed. Some of those methods such as KEPLER [88] and Coke-BERT [90] rely on sophisticated joint pre-training of PLMs and KG embeddings. However, KEPLER [88] performs worse on entity typing and relation classification tasks than LUKE [124] which contains only entity-level knowledge. CokeBERT [90] is slightly better than LUKE [124] on some datasets, but it is not as efficient as LUKE [124]. Hou et al. [178] proposed the Graph Convolution Simulator to detect knowledge integrated into PLMs. Their examination of ERNIE [67] and K-Adapter [72] revealed that those KEPLMS have only incorporated a small amount of factual knowledge. There still seems to be much room for more effective incorporation of knowledge into PLMs.</p>
<p>Improving the Efficiency of Knowledge Incorporation. Most existing work about KEPLMs only report improvements with respect to model performance, and only a few assess the costs of knowledge incorporation as well. More time-efficient and space-efficient solutions to KEPLMs are desired. Many methods, such as CoLAKE [89] and ERNIE [67], carry out knowledge incorporation in the pre-training stage, while some others like K-BERT [69], K-Adapter [71], and Syntax-BERT [71] carry out knowledge incorporation in the finetuning stage. The time cost of knowledge incorporation in the pre-training stage is greater than doing that in the finetuning stage. It deserves more investigation to minimize the overhead in the pre-training stage while maintaining good performance. Besides, knowledge incorporation may also increase the inference overhead of the model. For example, GRF [20] and KG-BART [17] involve the construction of knowledge sub-graphs, which makes their inference time much longer. More efficient inference strategies need to be developed for KEPLMs to facilitate their practical applications. The additional space consumption of KEPLMs must also be carefully considered before their deployment. For example, FaE [179] needs an external entity memory and a factual memory containing millions of knowledge triples. RAG [108] relies on a non-parametric knowledge corpus containing tens of millions of documents. Not all of these integrated entities or facts are equally useful: some of them probably play more important roles than others in enhancing the PLM. Therefore, selecting and storing only the most critical subset of knowledge may significantly reduce the space overhead with a small sacrifice in performance. In addition to avoiding the incorporation of less important knowledge, model compression techniques [180] can be used to reduce the computational overhead of KEPLMs. For example, quantization [181], knowledge distillation [182], and parameter sharing [183] can all be applied to KEPLMs to improve their time and space efficiency.</p>
<p>Exploring Other Knowledge-Intensive Tasks. In addition to the downstream NLP tasks listed in Section 6, some other less-explored applications may also benefit from KEPLMs. For example, KEPLMs are likely to improve the correctness of machine translation and the factualness of text summarization [184].</p>
<p>Building A Unified KEPLM for Multiple Tasks. Most of the existing KEPLMs are designed for specific knowledgeintensive NLP tasks. Currently to achieve SOTA performance for different tasks, one often needs to train a different KEPLM for each of them. It is desirable to develop a unified KEPLM for multiple tasks so as to avoid the costly proliferation of KEPLMs. There have been some early attempts towards this direction, such as KGI [121] which is trained to improve the performance on four different tasks in the KILT [120] benchmark.</p>
<p>Performing Zero/Few-shot Learning. In some application domains, there are little quality labelled data, therefore zeroshot learning or few-shot learning will be particularly useful. Thanks to the knowledge built into KEPLMs, they are more able than standard PLMs to overcome the data scarcity problem and tackle many zero/few-shot learning tasks. KALM [99] signals the existence of entities to the input in pre-training to integrate knowledge, significantly improving zero-shot question-answering tasks. Li et al. [128] introduced fine-grained type knowledge of entities, achieving superior performance in zero-shot dialog state tracking. Other than incorporating knowledge into PLMs, one can also exploit external knowledge in prompt engineering [185], [186], [187], [188]. It would be interesting to investigate how to maximize the combined effect of knowledge-enhanced PLMs and knowledge-enhanced prompt engineering together.</p>
<p>Achieving Better Interpretability and Robustness. The interpretability of a model measures how easily a human can understand its results and predictions. Schuff et al. [189] investigated whether incorporating external knowledge can help to explain natural language inference tasks. They have argued that there is a discrepancy between the automatic evaluation method of models and manual scoring, and the effectiveness of automatic evaluation needs to be reconsidered. Akyürek et al. [190] attempted to trace the predictions made by the model back to training data. Cao et al. [191] and LEFA [192], on the other hand, attempted to locate the knowledge stored in the model. The robustness of a model refers to its resistance to input disturbances or adversarial attacks etc. Li et al. [162] improved the robustness of the model by introducing external lexical knowledge into the attention mechanisms. Glass et al. [144] demonstrated adaptive capabilities on new datasets to illustrate the model's robustness. There are not many existing studies which try to improve the interpretability or robustness of KEPLMs. More in-depth investigations on these aspects would be helpful.</p>
<p>CONCLUSION</p>
<p>In summary, this survey provides a comprehensive view of current advances in the rapidly evolving field of KEPLMs. We begin by briefly introducing KEPLMs and describing the knowledge types/formats along with the methods for knowledge incorporation. PLMs can be enhanced by a wide range of knowledge, with encyclopedic and commonsense knowledge being most widely used, and domain-specific knowledge being increasingly explored. Different types of knowledge come in various forms, e.g., knowledge graphs could be integrated into PLMs directly as triples or indirectly through embeddings. The methods for knowledge incorporation can be classified into two main categories, implicit and explicit. Implicit incorporation does not put external knowledge into the model but employs knowledgeguided masking strategies or knowledge-related pre-training tasks to mine and learn knowledge from the pre-traineing corpus. Explicit incorporation can be adding knowledge to the input, integrating knowledge through fusion structures, or storing knowledge in external memory and retrieving it when needed. We then introduce some off-the-shelf methods for assessing the effectiveness of KEPLMs by detecting the amount of knowledge learned by the model, propose metrics for assessing model efficiency, and suggest assessing the generality of the model based on whether it can simultaneously boost performance on various tasks. After that, we present a list of knowledge-intensive tasks and some application areas worth considering. A final discussion of KEPLM research directions concludes this paper. We hope this will inspire researchers to explore KEPLMs further in the future. Finally, we discuss potential research directions for KEPLMs, which we hope will inspire future research in this area.</p>
<p>Fig. 1 .
1The benefits of integrating external knowledge into PLMs, according to ChatGPT -one of the largest PLMs today.</p>
<p>Fig. 3 .
3Using knowledge-guided masking strategies to build KEPLMs.</p>
<p>if a baby cries, he may be sick</p>
<p>Fig. 4 .
4GLM's knowledge-graph informed sampling of entities for masking.</p>
<p>Fig. 9 .
9Replacing the embeddings of long-tail entities with pseudo token embeddings.</p>
<p>Fig. 10 ,
10such knowledge fusion modules mainly appear in three positions: (a) on top of the entire PLM, (b) between the Transformer layers of PLM, (c) inside the Transformer layers of PLM.</p>
<p>Fig. 10 .
10Three different ways to add knowledge fusion modules to a PLM: (a) on top of the entire PLM, (b) between the Transformer layers of PLM, and (c) inside the Transformer layers of PLM.</p>
<p>Fig. 12 .
12Explicit incorporation of knowledge into PLMs via the utilization of external memory.</p>
<p>Some KEPLMs insert relevant knowledge triples or entity descriptions into the input for the model during its pretraining.Fig. 7. Explicit incorporation of knowledge into PLMs via modifying the model input or adding knowledge fusion modules.① Add Knowledge to the Input </p>
<p>h 1 h 2 h 3 h 4 h 5 
Token 
Embeddings </p>
<p>Entity 
Embeddings </p>
<p>Knowledge Enhanced 
Embeddings </p>
<p>Knowledge-intensive Tasks </p>
<p>② Add new fusion Module </p>
<p>Pre-trained Language Model </p>
<p>m 1 </p>
<p>hm 5 
hm 5 
hm 4 
hm 4 
hm 2 
hm 2 
hm 1 
hm 1 
hm 3 
hm 3 </p>
<p>e 1 x 2 x 3 x 4 e 5 </p>
<p>Original Input </p>
<p>e 1 
x 2 x 3 x 4 e 5 r 2 y 2 
r 1 y 1 
e 1 
x 2 x 3 x 4 e 5 d 2 
d 1 
e 1 
x 2 x 3 x 4 e 5 d 2 
d 1 
or </p>
<p>Knowledge Fusion Module 
Knowledge Fusion Module </p>
<p>Entity Description </p>
<p>Knowledge Source </p>
<p>(e 1 ,relation r 1 ,y 1 ) 
(e 5 ,relation r 2 ,y 2 ) 
Triples </p>
<p>e 1 : description d 1 
e 5 : description d 2 </p>
<p>e 1 : description d 1 
e 5 : description d 2 
Entity Description </p>
<p>Knowledge Source </p>
<p>(e 1 ,relation r 1 ,y 1 ) 
(e 5 ,relation r 2 ,y 2 ) 
Triples </p>
<p>e 1 : description d 1 
e 5 : description d 2 </p>
<p>Knowledge Encoder </p>
<p>m 2 
m 2 </p>
<p>Fig. 8. Adding knowledge triples into the model input.Yao, was selected to start for the Western Conference in the NBA All-Star Game eight timesMr. Darcy gives Elizabeth a letter 
Original input </p>
<p>Concat </p>
<p>Pre-trained Language Model </p>
<p>Word-Knowledge 
Graph </p>
<p>Knowledge Subgraph </p>
<p>Elizabeth Elizabeth </p>
<p>sister sister 
father father </p>
<p>Mr. Darcy 
Mr. Darcy </p>
<p>Mr. 
Bennet </p>
<p>Mr. 
Bennet </p>
<p>gives gives </p>
<p>a a 
letter letter </p>
<p>Word Graph </p>
<p>Mr. 
Darcy </p>
<p>Mr. 
Darcy 
[CLS] [CLS] </p>
<p>[SEP] [SEP] </p>
<p>Jane Jane </p>
<p>Wikidata </p>
<p>beloved beloved </p>
<p>Elizabeth Elizabeth </p>
<p>gives gives </p>
<p>a a 
letter letter </p>
<p>Mr. 
Darcy </p>
<p>Mr. 
Darcy 
[CLS] [CLS] </p>
<p>[SEP] [SEP] 
Elizabeth Elizabeth </p>
<p>sister sister </p>
<p>father father </p>
<p>Mr. 
Bennet </p>
<p>Mr. 
Bennet </p>
<p>Jane Jane </p>
<p>beloved beloved </p>
<p>sister 
Jane 
father 
Elizabeth 
[CLS] 
gives 
[SEP] 
Mr. 
Darcy </p>
<p>Mr. 
Darcy </p>
<p>Mr. 
Darcy </p>
<p>Mr. 
Bennet </p>
<p>Mr. 
Bennet 
letter 
a 
beloved </p>
<p>Token 
Embbedings </p>
<p>Replaced 
Embeddings </p>
<p>Input </p>
<p>Pre-trained Language Model </p>
<p>Yao Yao 
Western Western Conf. Conf. 
... ... 
All-Star All-Star Game Game 
... ... 
... ... </p>
<p>Yao Yao 
[LTE] [LTE] 
[LTE] [LTE] 
... ... 
[LTE] [LTE] 
[LTE] [LTE] 
... ... 
... ... </p>
<p>TABLE 3
3Evaluation of Some KEPLMs on LAMA and LAMA-UHN DatasetsModel 
LAMA </p>
<p>TABLE 4
4Comparison between KEPLMs and T5 in Terms of Model Size and 
Task Performance </p>
<p>Model 
Params 
Task Performance 
Natural Q. 
Web Q. </p>
<p>T5-Base 
220M 
25.9 
29.1 
T5-Large 
770M 
28.5 
32.2 
T5-3B 
3B 
30.4 
34.4 
T5-11B 
11B 
34.5 
37.4 </p>
<p>EAE [114] 
367M 
-
39.0 
REALM [107] 
330M 
40.4 
40.7 
RAG-Token [108] 
626M 
44.1 
45.5 
RAG-Seq [108] 
626M 
44.5 
45.2 </p>
<p>TABLE 5 
Comparison of Computational Resourced Required by KEPLMs </p>
<p>Model 
GPU Type 
GPU Num Training </p>
<p>SentiLARE [70] NVIDIA RTX 2080 Ti 
4 
20 hours 
DKPLM [101] 
NVIDIA V100 16GB 
8 
12 hours 
CoLAKE [89] 
NVIDIA V100 32GB 
8 
38 hours </p>
<p>TABLE 6
6KEPLMs for Entity RecognitionDomain 
Dataset 
Model </p>
<p>General 
MSRA-NER [129] 
K-BERT [69], ERNIE [98], ERNIE 2.0 [130] 
CoNLL-2003 </p>
<p>JNLPBA [126], BC5-chem &amp; BC5-disease33] 
DXY-NER [135] 
SMedBERT [34] 
Medicine NER [136] 
K-BERT [69] </p>
<p>TABLE 7
7KEPLMs for Relation Extraction NER [135], CHIP-RE [148]Domain 
Dataset 
Model </p>
<p>General 
TACRED [143] 
K-Adapter [72], ERNIE [67], ERICA [83], KEPLER [88], CokeBERT [90], 
LUKE [124], Glass et al. [144], DKPLM [101], EAE [114] 
FewRel [145] 
ERNIE [67], KEPLER [88], CoLAKE [89], JAKET [105], CokeBERT [90] </p>
<p>Biomedical </p>
<p>2010 i2b2/VA [125], GAD [146], EU-ADR [147] 
BERT-MK [32] 
DXY-</p>
<p>TABLE 8
8KEPLMs for Question AnsweringDomain 
Dataset 
Model </p>
<p>MRQA 
SQuAD 1.1 </p>
<p>1.5, open-domain abstractive QA, aka zeroshot QA or closed-book QA, requires the model to generate answers by itself rather than finding answers from passages or selecting from options. RAG[108] only uses questions and answers from the dataset of MSMARCO NLGT task v2.1[175], treating it as the open-domain abstractive QA task and outperforming the baseline model BART[56].
. https://www.wikipedia.org/
ACKNOWLEDGMENTSHer re-search interests lie in natural language processing, and knowledge acquisition.
Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst26T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their compositionality," in Proc. Int. Conf. Neural Inf. Process. Syst, vol. 26, 2013.</p>
<p>Glove: Global vectors for word representation. J Pennington, R Socher, C D Manning, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essJ. Pennington, R. Socher, and C. D. Manning, "Glove: Global vectors for word representation," in Proc. Conf. Empir. Methods Natural Lang. Process., 2014, pp. 1532-1543.</p>
<p>Bert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proc. Conf. North Amer. Conf. North AmerJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre- training of deep bidirectional transformers for language un- derstanding," in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., 2019, pp. 4171-4186.</p>
<p>Attention is all you need. A Vaswani, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. SystA. Vaswani et al., "Attention is all you need," in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 6000-6010.</p>
<p>Combining pre-trained language models and structured knowledge. P Colon-Hernandez, C Havasi, J Alonso, M Huggins, C Breazeal, arXiv:2101.12294arXiv preprintP. Colon-Hernandez, C. Havasi, J. Alonso, M. Huggins, and C. Breazeal, "Combining pre-trained language models and struc- tured knowledge," arXiv preprint arXiv:2101.12294, 2021.</p>
<p>A survey of knowledge-intensive nlp with pre-trained language models. D Yin, L Dong, H Cheng, X Liu, K.-W Chang, F Wei, J Gao, arXiv:2202.08772arXiv preprintD. Yin, L. Dong, H. Cheng, X. Liu, K.-W. Chang, F. Wei, and J. Gao, "A survey of knowledge-intensive nlp with pre-trained language models," arXiv preprint arXiv:2202.08772, 2022.</p>
<p>Knowledge enhanced pretrained language models: A compreshensive survey. X Wei, arXiv:2202.08772arXiv preprintX. Wei et al., "Knowledge enhanced pretrained language models: A compreshensive survey," arXiv preprint arXiv:2202.08772, 2022.</p>
<p>A survey of knowledge enhanced pre-trained models. J Yang, arXiv:2110.00269arXiv preprintJ. Yang et al., "A survey of knowledge enhanced pre-trained models," arXiv preprint arXiv:2110.00269, 2021.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. R Speer, J Chin, C Havasi, Proc. 31th AAAI Conf. Artif. Intell. 31th AAAI Conf. Artif. IntellR. Speer, J. Chin, and C. Havasi, "Conceptnet 5.5: An open multilingual graph of general knowledge," in Proc. 31th AAAI Conf. Artif. Intell., 2017.</p>
<p>Incorporating explicit knowledge in pretrained language models for passage re-ranking. Q Dong, arXiv:2204.11673arXiv preprintQ. Dong et al., "Incorporating explicit knowledge in pre- trained language models for passage re-ranking," arXiv preprint arXiv:2204.11673, 2022.</p>
<p>Cn-hit-it. nlp at semeval-2020 task 4: Enhanced language representation with multiple knowledge triples. Y Zhang, J Lin, Y Fan, P Jin, Y Liu, B Liu, Proc. 14th Workshop on Semantic Eval. 14th Workshop on Semantic EvalY. Zhang, J. Lin, Y. Fan, P. Jin, Y. Liu, and B. Liu, "Cn-hit-it. nlp at semeval-2020 task 4: Enhanced language representation with multiple knowledge triples," in Proc. 14th Workshop on Semantic Eval., 2020, pp. 494-500.</p>
<p>Qagnn: Reasoning with language models and knowledge graphs for question answering. M Yasunaga, H Ren, A Bosselut, P Liang, J Leskovec, Proc. 2021 Conf. North Amer. 2021 Conf. North AmerM. Yasunaga, H. Ren, A. Bosselut, P. Liang, and J. Leskovec, "Qa- gnn: Reasoning with language models and knowledge graphs for question answering," in Proc. 2021 Conf. North Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., 2021, pp. 535-546.</p>
<p>Greaselm: Graph reasoning enhanced language models. X Zhang, A Bosselut, M Yasunaga, H Ren, P Liang, C D Manning, J Leskovec, ICLR. X. Zhang, A. Bosselut, M. Yasunaga, H. Ren, P. Liang, C. D. Manning, and J. Leskovec, "Greaselm: Graph reasoning enhanced language models," in ICLR, 2022, pp. 1-16.</p>
<p>Jointlk: Joint reasoning with language models and knowledge graphs for commonsense question answering. Y Sun, Q Shi, L Qi, Y Zhang, arXiv:2112.02732arXiv preprintY. Sun, Q. Shi, L. Qi, and Y. Zhang, "Jointlk: Joint reasoning with language models and knowledge graphs for commonsense question answering," arXiv preprint arXiv:2112.02732, 2021.</p>
<p>Exploiting structured knowledge in text via graph-guided representation learning. T Shen, Y Mao, P He, G Long, A Trischler, W Chen, Proc. 2020 Conf. Empir. Methods Natural Lang. Process. 2020 Conf. Empir. Methods Natural Lang. essT. Shen, Y. Mao, P. He, G. Long, A. Trischler, and W. Chen, "Exploit- ing structured knowledge in text via graph-guided representation learning," in Proc. 2020 Conf. Empir. Methods Natural Lang. Process., 2020, pp. 8980-8994.</p>
<p>Kagnet: Knowledge-aware graph networks for commonsense reasoning. B Y Lin, X Chen, J Chen, X Ren, Proc. 2019 Conf. 2019 ConfB. Y. Lin, X. Chen, J. Chen, and X. Ren, "Kagnet: Knowledge-aware graph networks for commonsense reasoning," in Proc. 2019 Conf.</p>
<p>Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process. Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process., 2019, pp. 2829-2839.</p>
<p>Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning. Y Liu, Y Wan, L He, H Peng, S Y Philip, Proc. AAAI Conf. AAAI Conf35Y. Liu, Y. Wan, L. He, H. Peng, and S. Y. Philip, "Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning," in Proc. AAAI Conf. Artif. Intell., vol. 35, no. 7, 2021, pp. 6418-6425.</p>
<p>Comet: Commonsense transformers for automatic knowledge graph construction. A Bosselut, H Rashkin, M Sap, C Malaviya, A Celikyilmaz, Y Choi, PProc. 57th Annu. Meeting Assoc. Comput. Linguistics. A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi, "Comet: Commonsense transformers for automatic knowledge graph construction," in PProc. 57th Annu. Meeting Assoc. Comput. Linguistics, 2019, pp. 4762-4779.</p>
<p>Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models. Z.-X Ye, Q Chen, W Wang, Z.-H Ling, arXiv:1908.06725arXiv preprintZ.-X. Ye, Q. Chen, W. Wang, and Z.-H. Ling, "Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models," arXiv preprint arXiv:1908.06725, 2019.</p>
<p>Language generation with multi-hop reasoning on commonsense knowledge graph. H Ji, P Ke, S Huang, F Wei, X Zhu, M Huang, Proc. 2020 Conf. Empir. Methods Natural Lang. Process. 2020 Conf. Empir. Methods Natural Lang. essH. Ji, P. Ke, S. Huang, F. Wei, X. Zhu, and M. Huang, "Language generation with multi-hop reasoning on commonsense knowledge graph," in Proc. 2020 Conf. Empir. Methods Natural Lang. Process., 2020, pp. 725-736.</p>
<p>Exbert: An external knowledge enhanced bert for natural language inference. A Gajbhiye, N A Moubayed, S Bradley, Int. Conf. Artif. Neural Netw. SpringerA. Gajbhiye, N. A. Moubayed, and S. Bradley, "Exbert: An external knowledge enhanced bert for natural language inference," in Int. Conf. Artif. Neural Netw. Springer, 2021, pp. 460-472.</p>
<p>Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. A Lauscher, O Majewska, L F Ribeiro, I Gurevych, N Rozanov, G Glavaš, Proc. Deep Learn. Inside Out. Deep Learn. Inside OutA. Lauscher, O. Majewska, L. F. Ribeiro, I. Gurevych, N. Rozanov, and G. Glavaš, "Common sense or world knowledge? investigat- ing adapter-based knowledge injection into pretrained transform- ers," in Proc. Deep Learn. Inside Out, 2020, pp. 43-49.</p>
<p>A knowledgeenhanced pretraining model for commonsense story generation. J Guan, F Huang, Z Zhao, X Zhu, M Huang, Trans. Assoc. Comput. Linguistics. 8J. Guan, F. Huang, Z. Zhao, X. Zhu, and M. Huang, "A knowledge- enhanced pretraining model for commonsense story generation," Trans. Assoc. Comput. Linguistics, vol. 8, pp. 93-108, 2020.</p>
<p>Incorporating commonsense knowledge graph in pretrained models for social commonsense tasks. T.-Y. Chang, Proc. Deep Learn. Inside Out. Deep Learn. Inside OutT.-Y. Chang et al., "Incorporating commonsense knowledge graph in pretrained models for social commonsense tasks," in Proc. Deep Learn. Inside Out, 2021, pp. 74-79.</p>
<p>Unsupervised pre-training with structured knowledge for improving natural language inference. X Yang, X Zhu, Z Shi, T Li, arXiv:2109.03941arXiv preprintX. Yang, X. Zhu, Z. Shi, and T. Li, "Unsupervised pre-training with structured knowledge for improving natural language inference," arXiv preprint arXiv:2109.03941, 2021.</p>
<p>Atomic: An atlas of machine commonsense for if-then reasoning. M Sap, Proc. 33th AAAI Conf. 33th AAAI Conf33M. Sap et al., "Atomic: An atlas of machine commonsense for if-then reasoning," in Proc. 33th AAAI Conf. Artif. Intell., vol. 33, no. 01, 2019, pp. 3027-3035.</p>
<p>How additional knowledge can improve natural language commonsense question answering. A Mitra, P Banerjee, K K Pal, S Mishra, C Baral, arXiv:1909.08855arXiv preprintA. Mitra, P. Banerjee, K. K. Pal, S. Mishra, and C. Baral, "How ad- ditional knowledge can improve natural language commonsense question answering?" arXiv preprint arXiv:1909.08855, 2019.</p>
<p>(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. J D Hwang, Proc. 35th AAAI Conf. 35th AAAI Conf35J. D. Hwang et al., "(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs," in Proc. 35th AAAI Conf. Artif. Intell., vol. 35, no. 7, 2021, pp. 6384-6392.</p>
<p>Knowledgeaugmented language models for cause-effect relation classification. P Hosseini, D A Broniatowski, M Diab, Proc. First Workshop on Commonsense Representation Reasoning. First Workshop on Commonsense Representation ReasoningP. Hosseini, D. A. Broniatowski, and M. Diab, "Knowledge- augmented language models for cause-effect relation classifi- cation," in Proc. First Workshop on Commonsense Representation Reasoning, 2022, pp. 43-48.</p>
<p>Aser: A large-scale eventuality knowledge graph. H Zhang, X Liu, H Pan, Y Song, C W , -K Leung, Proc. Web Conf., 2020. Web Conf., 2020H. Zhang, X. Liu, H. Pan, Y. Song, and C. W.-K. Leung, "Aser: A large-scale eventuality knowledge graph," in Proc. Web Conf., 2020, pp. 201-211.</p>
<p>Cocolm: Complex commonsense enhanced language model with discourse relations. C Yu, H Zhang, Y Song, W Ng, Findings Assoc. Comput. Linguistics. C. Yu, H. Zhang, Y. Song, and W. Ng, "Cocolm: Complex commonsense enhanced language model with discourse relations," in Findings Assoc. Comput. Linguistics, 2022, pp. 1175-1187.</p>
<p>Bert-mk: Integrating graph contextualized knowledge into pre-trained language models. B He, Findings Assoc. Comput. Linguistics. B. He et al., "Bert-mk: Integrating graph contextualized knowledge into pre-trained language models," in Findings Assoc. Comput. Linguistics, 2020, pp. 2281-2290.</p>
<p>Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus. G Michalopoulos, Y Wang, H Kaka, H Chen, A Wong, Proc. Conference North Amer. Conference North AmerG. Michalopoulos, Y. Wang, H. Kaka, H. Chen, and A. Wong, "Umlsbert: Clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathe- saurus," in Proc. Conference North Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., 2021, pp. 1744-1753.</p>
<p>Smedbert: A knowledge-enhanced pre-trained language model with structured semantics for medical text mining. T Zhang, Z Cai, C Wang, M Qiu, B Yang, X He, Proc. 59th Annu. Meeting Assoc. Comput. Linguistics and 11th Int. Joint Conf. 59th Annu. Meeting Assoc. Comput. Linguistics and 11th Int. Joint ConfT. Zhang, Z. Cai, C. Wang, M. Qiu, B. Yang, and X. He, "Smed- bert: A knowledge-enhanced pre-trained language model with structured semantics for medical text mining," in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics and 11th Int. Joint Conf. Artif. Intell., 2021, pp. 5882-5893.</p>
<p>Improving biomedical pretrained language models with knowledge. Z Yuan, Y Liu, C Tan, S Huang, F Huang, Proc. 20th Workshop Biomed. Lang. Process, 2021. 20th Workshop Biomed. Lang. ess, 2021Z. Yuan, Y. Liu, C. Tan, S. Huang, and F. Huang, "Improving biomedical pretrained language models with knowledge," in Proc. 20th Workshop Biomed. Lang. Process, 2021, pp. 180-190.</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, J Kang, Bioinform. 364J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, "Biobert: a pre-trained biomedical language representation model for biomedical text mining," Bioinform., vol. 36, no. 4, pp. 1234- 1240, 2020.</p>
<p>Med-bert: A pretraining framework for medical records named entity recognition. N Liu, Q Hu, H Xu, X Xu, M Chen, IEEE Trans. Ind. Informatics. 188N. Liu, Q. Hu, H. Xu, X. Xu, and M. Chen, "Med-bert: A pre- training framework for medical records named entity recognition," IEEE Trans. Ind. Informatics, vol. 18, no. 8, pp. 5600-5608, 2022.</p>
<p>Scibert: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, EMNLP-IJCNLP. I. Beltagy, K. Lo, and A. Cohan, "Scibert: A pretrained language model for scientific text," in EMNLP-IJCNLP, 2019, pp. 3613-3618.</p>
<p>The impact of domain-specific pre-training on named entity recognition tasks in materials science. W Nicholas, T Amalie, H Haoyan, L Sanghoon, C Kevin, D John, D Alexander, P Kristin, C Gerbrand, J Anubhav, 10.2139/ssrn.3950755W. Nicholas, T. Amalie, H. Haoyan, L. Sanghoon, C. Kevin, D. John, D. Alexander, P. Kristin, C. Gerbrand, and J. Anubhav, "The impact of domain-specific pre-training on named entity recognition tasks in materials science," in http://dx.doi.org/10.2139/ssrn.3950755, 2021, pp. 1-43.</p>
<p>Ebert: a phrase and product knowledge enhanced language model for e-commerce. D Zhang, Z Yuan, Y Liu, F Zhuang, H Chen, H Xiong, arXiv:2009.02835arXiv preprintD. Zhang, Z. Yuan, Y. Liu, F. Zhuang, H. Chen, and H. Xiong, "E- bert: a phrase and product knowledge enhanced language model for e-commerce," arXiv preprint arXiv:2009.02835, 2020.</p>
<p>K-aid: Enhancing pre-trained language models with domain knowledge for question answering. F Sun, F.-L Li, R Wang, Q Chen, X Cheng, J Zhang, Proc. 30th ACM Int. Conf. Inf. Knowl. Manage, 2021. 30th ACM Int. Conf. Inf. Knowl. Manage, 2021F. Sun, F.-L. Li, R. Wang, Q. Chen, X. Cheng, and J. Zhang, "K-aid: Enhancing pre-trained language models with domain knowledge for question answering," in Proc. 30th ACM Int. Conf. Inf. Knowl. Manage, 2021, pp. 4125-4134.</p>
<p>K-plug: Knowledge-injected pre-trained language model for natural language understanding and generation in ecommerce. S Xu, Findings Assoc. Comput. Linguistics. S. Xu et al., "K-plug: Knowledge-injected pre-trained language model for natural language understanding and generation in e- commerce," in Findings Assoc. Comput. Linguistics, 2021, pp. 1-17.</p>
<p>LEGAL-BERT: preparing the muppets for court. I Chalkidis, M Fergadiotis, P Malakasiotis, N Aletras, I Androutsopoulos, EMNLP (Findings). I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras, and I. Androutsopoulos, "LEGAL-BERT: preparing the muppets for court," in EMNLP (Findings), 2020, pp. 2898-2904.</p>
<p>When does pretraining help?: assessing self-supervised learning for law and the casehold dataset of 53. L Zheng, N Guha, B R Anderson, P Henderson, D E Ho, 000+ legal holdings," in ICAIL, 2021L. Zheng, N. Guha, B. R. Anderson, P. Henderson, and D. E. Ho, "When does pretraining help?: assessing self-supervised learning for law and the casehold dataset of 53, 000+ legal holdings," in ICAIL, 2021, pp. 159-168.</p>
<p>Lawformer: A pretrained language model for chinese legal long documents. C Xiao, X Hu, Z Liu, C Tu, M Sun, in AI Open, 2021C. Xiao, X. Hu, Z. Liu, C. Tu, and M. Sun, "Lawformer: A pre- trained language model for chinese legal long documents," in AI Open, 2021, pp. 79-84.</p>
<p>Knowledge-enriched transformer for emotion detection in textual conversations. P Zhong, D Wang, C Miao, Proc. nullP. Zhong, D. Wang, and C. Miao, "Knowledge-enriched trans- former for emotion detection in textual conversations," in Proc.</p>
<p>Conf. Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process. Conf. Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process., 2019, pp. 165-176.</p>
<p>Skep: Sentiment knowledge enhanced pre-training for sentiment analysis. H Tian, Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020H. Tian et al., "Skep: Sentiment knowledge enhanced pre-training for sentiment analysis," in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020, pp. 4067-4076.</p>
<p>Refining language models with compositional explanations. H Yao, Y Chen, Q Ye, X Jin, X Ren, Proc. 34th Int. Conf. Neural Inf. 34th Int. Conf. Neural Inf34H. Yao, Y. Chen, Q. Ye, X. Jin, and X. Ren, "Refining language models with compositional explanations," in Proc. 34th Int. Conf. Neural Inf. Process. Syst., vol. 34, 2021, pp. 8954-8967.</p>
<p>D Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, M Tufano, S K Deng, C B Clement, D Drain, N Sundaresan, J Yin, D Jiang, M Zhou, Graphcodebert: Pre-training code representations with data flow. in ICLR, 2021D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. B. Clement, D. Drain, N. Sundaresan, J. Yin, D. Jiang, and M. Zhou, "Graphcodebert: Pre-training code representations with data flow," in ICLR, 2021, pp. 1-18.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient esti- mation of word representations in vector space," arXiv preprint arXiv:1301.3781, 2013.</p>
<p>Pre-trained models for natural language processing: A survey. X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang, Sci. China Technol. Sc. 6310X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, "Pre-trained models for natural language processing: A survey," Sci. China Technol. Sc., vol. 63, no. 10, pp. 1872-1897, 2020.</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 98S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Comput., vol. 9, no. 8, pp. 1735-1780, 1997.</p>
<p>Learned in translation: Contextualized word vectors. B Mccann, J Bradbury, C Xiong, R Socher, Proc. 30th Int. Conf. Neural Inf. Process. Syst. 30th Int. Conf. Neural Inf. ess. SystB. McCann, J. Bradbury, C. Xiong, and R. Socher, "Learned in translation: Contextualized word vectors," in Proc. 30th Int. Conf. Neural Inf. Process. Syst., 2017, pp. 6294-6305.</p>
<p>Deep contextualized word representations. M E Peters, Proc. Conference North Amer. Conference North AmerNew Orleans, LouisianaM. E. Peters et al., "Deep contextualized word representations," in Proc. Conference North Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., New Orleans, Louisiana, 2018, pp. 2227-2237.</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, OpenAI Blog. OnlineA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, "Improving language understanding by generative pre-training," OpenAI Blog, 2018. [On- line]. Available: https://cdn.openai.com/research-covers/ language-unsupervised/language understanding paper.pdf</p>
<p>Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. M Lewis, Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020M. Lewis et al., "Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and compre- hension," in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020, pp. 7871-7880.</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, arXiv:2107.13586arXiv preprintP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompt- ing methods in natural language processing," arXiv preprint arXiv:2107.13586, 2021.</p>
<p>Wikidata: a free collaborative knowledgebase. D Vrandečić, M Krötzsch, Commun. ACM. 5710D. Vrandečić and M. Krötzsch, "Wikidata: a free collaborative knowledgebase," Commun. ACM, vol. 57, no. 10, pp. 78-85, 2014.</p>
<p>Language models as knowledge bases. F Petroni, arXiv:2101.12294arXiv preprintF. Petroni et al., "Language models as knowledge bases?" arXiv preprint arXiv:2101.12294, 2019.</p>
<p>Language models are open knowledge graphs. C Wang, X Liu, D Song, arXiv:2010.11967arXiv preprintC. Wang, X. Liu, and D. Song, "Language models are open knowledge graphs," arXiv preprint arXiv:2010.11967, 2020.</p>
<p>Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries. B Heinzerling, K Inui, arXiv:2008.09036arXiv preprintB. Heinzerling and K. Inui, "Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries," arXiv preprint arXiv:2008.09036, 2020.</p>
<p>Relational world knowledge representation in contextual language models: A review. T Safavi, D Koutra, arXiv:2104.05837arXiv preprintT. Safavi and D. Koutra, "Relational world knowledge represen- tation in contextual language models: A review," arXiv preprint arXiv:2104.05837, 2021.</p>
<p>Language models as or for knowledge bases. S Razniewski, A Yates, N Kassner, G Weikum, arXiv:2110.04888arXiv preprintS. Razniewski, A. Yates, N. Kassner, and G. Weikum, "Language models as or for knowledge bases," arXiv preprint arXiv:2110.04888, 2021.</p>
<p>Can generative pre-trained language models serve as knowledge bases for closed-book qa. C Wang, P Liu, Y Zhang, arXiv:2106.01561arXiv preprintC. Wang, P. Liu, and Y. Zhang, "Can generative pre-trained language models serve as knowledge bases for closed-book qa?" arXiv preprint arXiv:2106.01561, 2021.</p>
<p>B Alkhamissi, M Li, A Celikyilmaz, M Diab, M Ghazvininejad, arXiv:2204.06031A review on language models as knowledge bases. arXiv preprintB. AlKhamissi, M. Li, A. Celikyilmaz, M. Diab, and M. Ghazvinine- jad, "A review on language models as knowledge bases," arXiv preprint arXiv:2204.06031, 2022.</p>
<p>How much knowledge can you pack into the parameters of a language model. A Roberts, C Raffel, N Shazeer, arXiv:2002.08910arXiv preprintA. Roberts, C. Raffel, and N. Shazeer, "How much knowledge can you pack into the parameters of a language model?" arXiv preprint arXiv:2002.08910, 2020.</p>
<p>Ernie: Enhanced language representation with informative entities. Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, Proc. 57th Annu. Meeting Assoc. Comput. Linguistics. 57th Annu. Meeting Assoc. Comput. LinguisticsZ. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, "Ernie: Enhanced language representation with informative entities," in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, 2019, pp. 1441- 1451.</p>
<p>Knowledge enhanced contextual word representations. M E Peters, Proc. 2019 Conf. Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process. 2019 Conf. Empir. Methods Natural Lang. ess. and 9th Int. Joint Conf. Natural Lang. essM. E. Peters et al., "Knowledge enhanced contextual word representations," in Proc. 2019 Conf. Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process., 2019, pp. 43-54.</p>
<p>K-bert: Enabling language representation with knowledge graph. W Liu, P Zhou, Z Zhao, Z Wang, Q Ju, H Deng, P Wang, Proc. AAAI Conf. AAAI Conf34W. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang, "K-bert: Enabling language representation with knowledge graph," in Proc. AAAI Conf. Artif. Intell., vol. 34, no. 03, 2020, pp. 2901-2908.</p>
<p>Sentilare: Sentimentaware language representation learning with linguistic knowledge. P Ke, H Ji, S Liu, X Zhu, M Huang, arXiv:1911.02493arXiv preprintP. Ke, H. Ji, S. Liu, X. Zhu, and M. Huang, "Sentilare: Sentiment- aware language representation learning with linguistic knowl- edge," arXiv preprint arXiv:1911.02493, 2019.</p>
<p>Syntax-bert: Improving pre-trained transformers with syntax trees. J Bai, arXiv:2103.04350arXiv preprintJ. Bai et al., "Syntax-bert: Improving pre-trained transformers with syntax trees," arXiv preprint arXiv:2103.04350, 2021.</p>
<p>K-adapter: Infusing knowledge into pre-trained models with adapters. R Wang, arXiv:2002.01808arXiv preprintR. Wang et al., "K-adapter: Infusing knowledge into pre-trained models with adapters," arXiv preprint arXiv:2002.01808, 2020.</p>
<p>Xlm-k: Improving cross-lingual language model pre-training with multilingual knowledge. X Jiang, Y Liang, W Chen, N Duan, Proc. 36th AAAI Conf. 36th AAAI Conf36X. Jiang, Y. Liang, W. Chen, and N. Duan, "Xlm-k: Improving cross-lingual language model pre-training with multilingual knowledge," in Proc. 36th AAAI Conf. Artif. Intell., vol. 36, no. 10, 2022, pp. 10 840-10 848.</p>
<p>Enhancing pre-trained language representations with rich knowledge for machine reading comprehension. A Yang, Q Wang, J Liu, K Liu, Y Lyu, H Wu, Q She, S Li, Proc. 57th. 57thA. Yang, Q. Wang, J. Liu, K. Liu, Y. Lyu, H. Wu, Q. She, and S. Li, "Enhancing pre-trained language representations with rich knowledge for machine reading comprehension," in Proc. 57th</p>
<p>. Annu. Meeting Assoc. Comput. Linguistics. Annu. Meeting Assoc. Comput. Linguistics, 2019, pp. 2346-2357.</p>
<p>Sensebert: Driving some sense into bert. Y Levine, B Lenz, O Dagan, O Ram, D Padnos, O Sharir, S Shalev-Shwartz, A Shashua, Y Shoham, arXiv:1908.05646arXiv preprintY. Levine, B. Lenz, O. Dagan, O. Ram, D. Padnos, O. Sharir, S. Shalev-Shwartz, A. Shashua, and Y. Shoham, "Sensebert: Driving some sense into bert," arXiv preprint arXiv:1908.05646, 2019.</p>
<p>Specializing unsupervised pretraining models for word-level semantic similarity. A Lauscher, I Vulić, E M Ponti, A Korhonen, G Glavaš, arXiv:1909.02339arXiv preprintA. Lauscher, I. Vulić, E. M. Ponti, A. Korhonen, and G. Glavaš, "Specializing unsupervised pretraining models for word-level semantic similarity," arXiv preprint arXiv:1909.02339, 2019.</p>
<p>Wordnet: a lexical database for english. G A Miller, Commun. ACM. 3811G. A. Miller, "Wordnet: a lexical database for english," Commun. ACM, vol. 38, no. 11, pp. 39-41, 1995.</p>
<p>Knowledge-driven natural language understanding of english text and its applications. K Basu, S C Varanasi, F Shakerin, J Arias, G Gupta, Proc. 35th AAAI Conf. 35th AAAI Conf35K. Basu, S. C. Varanasi, F. Shakerin, J. Arias, and G. Gupta, "Knowledge-driven natural language understanding of english text and its applications," in Proc. 35th AAAI Conf. Artif. Intell., vol. 35, no. 14, 2021, pp. 12 554-12 563.</p>
<p>A large-scale classification of english verbs. K Kipper, A Korhonen, N Ryant, M Palmer, Lang. Resour. Eval. 421K. Kipper, A. Korhonen, N. Ryant, and M. Palmer, "A large-scale classification of english verbs," Lang. Resour. Eval., vol. 42, no. 1, pp. 21-40, 2008.</p>
<p>Semantics-aware bert for language understanding. Z Zhang, Y Wu, H Zhao, Z Li, S Zhang, X Zhou, X Zhou, Proc. AAAI Conf. AAAI Conf34Z. Zhang, Y. Wu, H. Zhao, Z. Li, S. Zhang, X. Zhou, and X. Zhou, "Semantics-aware bert for language understanding," in Proc. AAAI Conf. Artif. Intell., vol. 34, no. 05, 2020, pp. 9628-9635.</p>
<p>A dictionary of sociology. J Scott, G Marshall, Oxford University PressUSAJ. Scott and G. Marshall, A dictionary of sociology. Oxford University Press, USA, 2009.</p>
<p>Kgplm: Knowledge-guided language model pre-training via generative and discriminative learning. B He, X Jiang, J Xiao, Q Liu, arXiv:2012.03551arXiv preprintB. He, X. Jiang, J. Xiao, and Q. Liu, "Kgplm: Knowledge-guided language model pre-training via generative and discriminative learning," arXiv preprint arXiv:2012.03551, 2020.</p>
<p>Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning. Y Qin, Proc. 59th Annu. Meeting Assoc. Comput. Linguistics and 11th Int. Joint Conf. Natural Lang. Process. 59th Annu. Meeting Assoc. Comput. Linguistics and 11th Int. Joint Conf. Natural Lang. essY. Qin et al., "Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning," in Proc. 59th Annu. Meeting Assoc. Comput. Linguistics and 11th Int. Joint Conf. Natural Lang. Process., 2021, pp. 3350-3363.</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. K Bollacker, C Evans, P Paritosh, T Sturge, J Taylor, Proc. ACM SIGMOD Int. Conf. Manage. Data. ACM SIGMOD Int. Conf. Manage. DataK. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, "Freebase: a collaboratively created graph database for structuring human knowledge," in Proc. ACM SIGMOD Int. Conf. Manage. Data, 2008, pp. 1247-1250.</p>
<p>Dbpedia: A nucleus for a web of open data. S Auer, C Bizer, G Kobilarov, J Lehmann, R Cyganiak, Z Ives, The semantic web. SpringerS. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives, "Dbpedia: A nucleus for a web of open data," in The semantic web. Springer, 2007, pp. 722-735.</p>
<p>Toward an architecture for never-ending language learning. A Carlson, J Betteridge, B Kisiel, B Settles, E R Hruschka, T M Mitchell, Proc. 24th AAAI Conf. 24th AAAI ConfA. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka, and T. M. Mitchell, "Toward an architecture for never-ending language learning," in Proc. 24th AAAI Conf. Artif. Intell., 2010.</p>
<p>Cndbpedia: A never-ending chinese knowledge extraction system. B Xu, Y Xu, J Liang, C Xie, B Liang, W Cui, Y Xiao, Int. Conf. Ind., Eng. and Other Appl. of Appl. Intell. Syst. SpringerB. Xu, Y. Xu, J. Liang, C. Xie, B. Liang, W. Cui, and Y. Xiao, "Cn- dbpedia: A never-ending chinese knowledge extraction system," in Int. Conf. Ind., Eng. and Other Appl. of Appl. Intell. Syst. Springer, 2017, pp. 428-438.</p>
<p>Kepler: A unified model for knowledge embedding and pre-trained language representation. X Wang, Trans. Assoc. Comput. Linguistics. 9X. Wang et al., "Kepler: A unified model for knowledge embedding and pre-trained language representation," Trans. Assoc. Comput. Linguistics, vol. 9, pp. 176-194, 2021.</p>
<p>Colake: Contextualized language and knowledge embedding. T Sun, Proc. 28th Int. Conf. Comput. Linguistics, 2020. 28th Int. Conf. Comput. Linguistics, 2020T. Sun et al., "Colake: Contextualized language and knowledge embedding," in Proc. 28th Int. Conf. Comput. Linguistics, 2020, pp. 3660-3670.</p>
<p>Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models. Y Su, AI Open. 2Y. Su et al., "Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models," AI Open, vol. 2, pp. 127-134, 2021.</p>
<p>Ernie 3.0: Large-scale knowledge enhanced pretraining for language understanding and generation. Y Sun, arXiv:2107.02137arXiv preprintY. Sun et al., "Ernie 3.0: Large-scale knowledge enhanced pre- training for language understanding and generation," arXiv preprint arXiv:2107.02137, 2021.</p>
<p>Benchmarking knowledgeenhanced commonsense question answering via knowledge-totext transformation. N Bian, X Han, B Chen, L Sun, Proc. 35th AAAI Conf. 35th AAAI Conf35N. Bian, X. Han, B. Chen, and L. Sun, "Benchmarking knowledge- enhanced commonsense question answering via knowledge-to- text transformation," in Proc. 35th AAAI Conf. Artif. Intell., vol. 35, no. 14, 2021, pp. 12 574-12 582.</p>
<p>Relational memoryaugmented language models. Q Liu, D Yogatama, P Blunsom, Trans. Assoc. Comput. Linguistics. 10Q. Liu, D. Yogatama, and P. Blunsom, "Relational memory- augmented language models," Trans. Assoc. Comput. Linguistics, vol. 10, pp. 555-572, 2022.</p>
<p>Kala: Knowledge-augmented language model adaptation. M Kang, J Baek, S J Hwang, arXiv:2204.10555arXiv preprintM. Kang, J. Baek, and S. J. Hwang, "Kala: Knowledge-augmented language model adaptation," arXiv preprint arXiv:2204.10555, 2022.</p>
<p>Dict-bert: Enhancing language model pre-training with dictionary. W Yu, Findings Assoc. Comput. Linguistic. W. Yu et al., "Dict-bert: Enhancing language model pre-training with dictionary," in Findings Assoc. Comput. Linguistic, 2022, pp. 1907-1918.</p>
<p>Visually-augmented language modeling. W Wang, arXiv:2205.10178arXiv preprintW. Wang et al., "Visually-augmented language modeling," arXiv preprint arXiv:2205.10178, 2022.</p>
<p>Vokenization: Improving language understanding with contextualized, visual-grounded supervision. H Tan, M Bansal, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essH. Tan and M. Bansal, "Vokenization: Improving language understanding with contextualized, visual-grounded supervision," in Proc. Conf. Empir. Methods Natural Lang. Process., 2020, pp. 2066- 2080.</p>
<p>Ernie: Enhanced representation through knowledge integration. Y Sun, arXiv:1904.09223arXiv preprintY. Sun et al., "Ernie: Enhanced representation through knowledge integration," arXiv preprint arXiv:1904.09223, 2019.</p>
<p>Knowledge-aware language model pretraining. C Rosset, C Xiong, M Phan, X Song, P Bennett, S Tiwary, arXiv:2007.00655arXiv preprintC. Rosset, C. Xiong, M. Phan, X. Song, P. Bennett, and S. Tiwary, "Knowledge-aware language model pretraining," arXiv preprint arXiv:2007.00655, 2020.</p>
<p>Wiktionary: A new rival for expert-built lexicons? exploring the possibilities of collaborative lexicography. C M Meyer, I Gurevych, 10.1093/acprof:oso/9780199654864.003.0013Electronic Lexicography. Oxford University PressC. M. Meyer and I. Gurevych, "Wiktionary: A new rival for expert-built lexicons? exploring the possibilities of collaborative lexicography," in Electronic Lexicography. Oxford University Press, 11 2012. [Online]. Available: https://doi.org/10.1093/acprof: oso/9780199654864.003.0013</p>
<p>Dkplm: Decomposable knowledge-enhanced pre-trained language model for natural language understanding. T Zhang, Proc. 36th AAAI Conf. 36th AAAI Conf36T. Zhang et al., "Dkplm: Decomposable knowledge-enhanced pre-trained language model for natural language understanding," in Proc. 36th AAAI Conf. Artif. Intell., vol. 36, no. 10, 2022, pp. 11 703-11 711.</p>
<p>Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. W Xiong, J Du, W Y Wang, V Stoyanov, arXiv:1912.09637arXiv preprintW. Xiong, J. Du, W. Y. Wang, and V. Stoyanov, "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model," arXiv preprint arXiv:1912.09637, 2019.</p>
<p>Y Liu, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintY. Liu et al., "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Why do masked neural language models still need common sense knowledge. S Kwon, C Kang, J Han, J Choi, arXiv:1911.03024arXiv preprintS. Kwon, C. Kang, J. Han, and J. Choi, "Why do masked neural language models still need common sense knowledge?" arXiv preprint arXiv:1911.03024, 2019.</p>
<p>Jaket: Joint pre-training of knowledge graph and language understanding. D Yu, C Zhu, Y Yang, M Zeng, Proc. 36th AAAI Conf. 36th AAAI Conf36D. Yu, C. Zhu, Y. Yang, and M. Zeng, "Jaket: Joint pre-training of knowledge graph and language understanding," in Proc. 36th AAAI Conf. Artif. Intell., vol. 36, no. 10, 2022, pp. 11 630-11 638.</p>
<p>Barack's wife hillary: Using knowledge-graphs for fact-aware language modeling. R L Logan, I V , N F Liu, M E Peters, M Gardner, S Singh, arXiv:1906.07241arXiv preprintR. L. Logan IV, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh, "Barack's wife hillary: Using knowledge-graphs for fact-aware language modeling," arXiv preprint arXiv:1906.07241, 2019.</p>
<p>Retrieval augmented language model pre-training. K Guu, K Lee, Z Tung, P Pasupat, M Chang, Proc. 37th Int. Conf. Mach. Learn. PMLR, 2020. 37th Int. Conf. Mach. Learn. PMLR, 2020K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, "Retrieval augmented language model pre-training," in Proc. 37th Int. Conf. Mach. Learn. PMLR, 2020, pp. 3929-3938.</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, Proc. Int. Conf. Neural Inf. Int. Conf. Neural Inf33P. Lewis et al., "Retrieval-augmented generation for knowledge- intensive nlp tasks," in Proc. Int. Conf. Neural Inf. Process. Syst., vol. 33, 2020, pp. 9459-9474.</p>
<p>Memory and knowledge augmented language models for inferring salience in long-form stories. D Wilmot, F Keller, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essD. Wilmot and F. Keller, "Memory and knowledge augmented language models for inferring salience in long-form stories," in Proc. Conf. Empir. Methods Natural Lang. Process., 2021, pp. 851-865.</p>
<p>E-bert: Efficient-yeteffective entity embeddings for bert. N Poerner, U Waltinger, H Schütze, Findings Assoc. Comput. Linguistics. N. Poerner, U. Waltinger, and H. Schütze, "E-bert: Efficient-yet- effective entity embeddings for bert," in Findings Assoc. Comput. Linguistics, 2020, pp. 803-818.</p>
<p>T-rex: A large scale alignment of natural language with knowledge base triples. H Elsahar, Proc. 11th Int. Conf. Lang. Resour. 11th Int. Conf. Lang. ResourH. Elsahar et al., "T-rex: A large scale alignment of natural language with knowledge base triples," in Proc. 11th Int. Conf. Lang. Resour. Eval., 2018. [Online]. Available: https://aclanthology.org/L18-1544.pdf</p>
<p>Squad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, "Squad: 100,000+ questions for machine comprehension of text," in Proc. Conf. Empir. Methods Natural Lang. Process., 2016, pp. 2383-2392.</p>
<p>Entities as experts: Sparse memory access with entity supervision. T Févry, L B Soares, N Fitzgerald, E Choi, T Kwiatkowski, Proc. 2020 Conf. Empir. Methods Natural Lang. Process. 2020 Conf. Empir. Methods Natural Lang. essT. Févry, L. B. Soares, N. FitzGerald, E. Choi, and T. Kwiatkowski, "Entities as experts: Sparse memory access with entity supervision," in Proc. 2020 Conf. Empir. Methods Natural Lang. Process., 2020, pp. 4937-4951.</p>
<p>How can we know what language models know?. Z Jiang, F F Xu, J Araki, G Neubig, Trans. Assoc. Comput. Linguistics. 8Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, "How can we know what language models know?" Trans. Assoc. Comput. Linguistics, vol. 8, pp. 423-438, 2020.</p>
<p>How to query language models. L Adolphs, S Dhuliawala, T Hofmann, arXiv:2108.01928arXiv preprintL. Adolphs, S. Dhuliawala, and T. Hofmann, "How to query language models?" arXiv preprint arXiv:2108.01928, 2021.</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. T Shin, Y Razeghi, R L Logan, I V , E Wallace, S Singh, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essT. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, "Autoprompt: Eliciting knowledge from language models with automatically generated prompts," in Proc. Conf. Empir. Methods Natural Lang. Process., 2020, pp. 4222-4235.</p>
<p>Rewire-then-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models. Z Meng, F Liu, E Shareghi, Y Su, C Collins, N Collier, Proc. 60th Annu. Meeting Assoc. Comput. Linguistics, 2022. 60th Annu. Meeting Assoc. Comput. Linguistics, 2022Z. Meng, F. Liu, E. Shareghi, Y. Su, C. Collins, and N. Collier, "Rewire-then-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models," in Proc. 60th Annu. Meeting Assoc. Comput. Linguistics, 2022, pp. 4798-4810.</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proc. EMNLP Workshop BlackboxNLP. EMNLP Workshop BlackboxNLPA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, "Glue: A multi-task benchmark and analysis platform for natural language understanding," in Proc. EMNLP Workshop BlackboxNLP, 2018, pp. 353-355.</p>
<p>Kilt: A benchmark for knowledge intensive language tasks. F Petroni, Proc. Conference North Amer. Conference North AmerF. Petroni et al., "Kilt: A benchmark for knowledge intensive language tasks," in Proc. Conference North Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., 2021, pp. 2523-2544.</p>
<p>Kgi: An integrated framework for knowledge intensive language tasks. M F M Chowdhury, M Glass, G Rossiello, A Gliozzo, N Mihindukulasooriya, arXiv:2204.03985arXiv preprintM. F. M. Chowdhury, M. Glass, G. Rossiello, A. Gliozzo, and N. Mi- hindukulasooriya, "Kgi: An integrated framework for knowledge intensive language tasks," arXiv preprint arXiv:2204.03985, 2022.</p>
<p>Design challenges for entity linking. X Ling, S Singh, D S Weld, Trans. Assoc. Comput. Linguistics. 3X. Ling, S. Singh, and D. S. Weld, "Design challenges for entity linking," Trans. Assoc. Comput. Linguistics, vol. 3, pp. 315-328, 2015.</p>
<p>Ultra-fine entity typing. E Choi, O Levy, Y Choi, L Zettlemoyer, Proc. 56th Annu. Meeting Assoc. 56th Annu. Meeting AssocE. Choi, O. Levy, Y. Choi, and L. Zettlemoyer, "Ultra-fine entity typing," in Proc. 56th Annu. Meeting Assoc. Comput. Linguistics, 2018, pp. 87-96.</p>
<p>Luke: deep contextualized entity representations with entityaware self-attention. I Yamada, A Asai, H Shindo, H Takeda, Y Matsumoto, Proc. 2020 Conf. Empir. Methods Natural Lang. Process. 2020 Conf. Empir. Methods Natural Lang. essI. Yamada, A. Asai, H. Shindo, H. Takeda, and Y. Matsumoto, "Luke: deep contextualized entity representations with entity- aware self-attention," in Proc. 2020 Conf. Empir. Methods Natural Lang. Process., 2020, pp. 6442-6454.</p>
<p>2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. Ö Uzuner, B R South, S Shen, S L Duvall, J. Amer. Med. Inform. Assoc. 185Ö. Uzuner, B. R. South, S. Shen, and S. L. DuVall, "2010 i2b2/va challenge on concepts, assertions, and relations in clinical text," J. Amer. Med. Inform. Assoc., vol. 18, no. 5, pp. 552-556, 2011.</p>
<p>Introduction to the bio-entity recognition task at jnlpba. J.-D Kim, T Ohta, Y Tsuruoka, Y Tateisi, N Collier, Proc. Int. Joint Workshop Natural Lang. Process. Int. Joint Workshop Natural Lang. essJ.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier, "In- troduction to the bio-entity recognition task at jnlpba," in Proc. Int. Joint Workshop Natural Lang. Process. Biomedicine and its Appl. Citeseer, 2004, pp. 70-75.</p>
<p>Biocreative v cdr task corpus: A resource for chemical disease relation extraction. J Li, Database(Oxford). J. Li et al., "Biocreative v cdr task corpus: A resource for chemical disease relation extraction," Database(Oxford), vol. 2016, 2016.</p>
<p>Instilling type knowledge in language models via multi-task qa. S Li, M Sridhar, C S Prakash, J Cao, W Hamza, J Mcauley, Findings Assoc. Comput. Linguistics: NAACL, 2022. S. Li, M. Sridhar, C. S. Prakash, J. Cao, W. Hamza, and J. McAuley, "Instilling type knowledge in language models via multi-task qa," in Findings Assoc. Comput. Linguistics: NAACL, 2022, pp. 594-603.</p>
<p>The third international chinese language processing bakeoff: Word segmentation and named entity recognition. G.-A Levow, Proc. Fifth SIGHAN Workshop Chinese Lang. Process. Fifth SIGHAN Workshop Chinese Lang. essG.-A. Levow, "The third international chinese language processing bakeoff: Word segmentation and named entity recognition," in Proc. Fifth SIGHAN Workshop Chinese Lang. Process., 2016, pp. 108- 117.</p>
<p>Ernie 2.0: A continual pre-training framework for language understanding. Y Sun, Proc. 34th AAAI Conf. 34th AAAI Conf34Y. Sun et al., "Ernie 2.0: A continual pre-training framework for language understanding," in Proc. 34th AAAI Conf. Artif. Intell., vol. 34, no. 05, 2020, pp. 8968-8975.</p>
<p>Introduction to the conll-2003 shared task: Language-independent named entity recognition. E F Sang, F. De Meulder, Proc. Conference North Amer. Assoc. Comput. Linguistics. Conference North Amer. Assoc. Comput. LinguisticsE. F. Sang and F. De Meulder, "Introduction to the conll-2003 shared task: Language-independent named entity recognition," in Proc. Conference North Amer. Assoc. Comput. Linguistics, 2003, pp. 142-147.</p>
<p>Knowledge based multilingual language model. L Linlin, L Xin, H Ruidan, B Lidong, J Shafiq, S Luo, arXiv:2111.10962arXiv preprintL. Linlin, L. Xin, H. Ruidan, B. Lidong, J. Shafiq, and S. Luo, "Knowledge based multilingual language model," arXiv preprint arXiv:2111.10962, 2021.</p>
<p>Evaluating the state-of-theart in automatic de-identification. Ö Uzuner, Y Luo, P Szolovits, J. Amer. Med. Inform. Assoc. 145Ö. Uzuner, Y. Luo, and P. Szolovits, "Evaluating the state-of-the- art in automatic de-identification," J. Amer. Med. Inform. Assoc., vol. 14, no. 5, pp. 550-563, 2007.</p>
<p>Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1. A Stubbs, C Kotfila, Uzuner, J. Biomed. Inform. 58A. Stubbs, C. Kotfila, andÖ. Uzuner, "Automated systems for the de-identification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared task track 1," J. Biomed. Inform., vol. 58, pp. S11-S19, 2015.</p>
<p>Ncbi disease corpus: a resource for disease name recognition and concept normalization. R I Dogan, R Leaman, Z Lu, J. Biomed. Inform. 47R. I. Dogan, R. Leaman, and Z. Lu, "Ncbi disease corpus: a resource for disease name recognition and concept normalization," J. Biomed. Inform., vol. 47, pp. 1-10, 2014.</p>
<p>Overview of biocreative ii gene mention recognition. L Smith, Genome Biol. 92L. Smith et al., "Overview of biocreative ii gene mention recogni- tion," Genome Biol., vol. 9, no. 2, pp. 1-19, 2008.</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. J Lee, Bioinformatics. 364J. Lee et al., "Biobert: a pre-trained biomedical language represen- tation model for biomedical text mining," Bioinformatics, vol. 36, no. 4, pp. 1234-1240, 2020.</p>
<p>Results of the wnut2017 shared task on novel and emerging entity recognition. L Derczynski, E Nichols, M Van Erp, N Limsopatham, Proc. 3rd Workshop Noisy User-generated Text. 3rd Workshop Noisy User-generated TextL. Derczynski, E. Nichols, M. van Erp, and N. Limsopatham, "Results of the wnut2017 shared task on novel and emerging entity recognition," in Proc. 3rd Workshop Noisy User-generated Text, 2017, pp. 140-147.</p>
<p>Crosslingual name tagging and linking for 282 languages. X Pan, B Zhang, J May, J Nothman, K Knight, H Ji, Proc. 55th Annu. Meeting Assoc. Comput. Linguistics. 55th Annu. Meeting Assoc. Comput. Linguistics1X. Pan, B. Zhang, J. May, J. Nothman, K. Knight, and H. Ji, "Cross- lingual name tagging and linking for 282 languages," in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, vol. 1, 2017, pp. 1946-1958.</p>
<p>Position-aware attention and supervised data improve slot filling. Y Zhang, V Zhong, D Chen, G Angeli, C D Manning, Proc. 2017 Conf. Empir. Methods Lang. Process. 2017 Conf. Empir. Methods Lang. essY. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning, "Position-aware attention and supervised data improve slot filling," in Proc. 2017 Conf. Empir. Methods Lang. Process., 2017, pp. 35-45.</p>
<p>Robust retrieval augmented generation for zero-shot slot filling. M Glass, G Rossiello, M F M Chowdhury, A Gliozzo, Proc. 2021 Conf. Emplir. Methods Natural Lang. Process. 2021 Conf. Emplir. Methods Natural Lang. essM. Glass, G. Rossiello, M. F. M. Chowdhury, and A. Gliozzo, "Robust retrieval augmented generation for zero-shot slot filling," in Proc. 2021 Conf. Emplir. Methods Natural Lang. Process., 2021, pp. 1939-1949.</p>
<p>Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. X Han, Proc. nullX. Han et al., "Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation," in Proc. 2018</p>
<p>. Conf, Empir, Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. Process., 2018, pp. 4803-4809.</p>
<p>Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research. À Bravo, J Piñero, N Queralt-Rosinach, M Rautschka, L I Furlong, BMC Bioinformatics. 161À. Bravo, J. Piñero, N. Queralt-Rosinach, M. Rautschka, and L. I. Furlong, "Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research," BMC Bioinformatics, vol. 16, no. 1, pp. 1-17, 2015.</p>
<p>The eu-adr corpus: annotated drugs, diseases, targets, and their relationships. E M Van Mulligen, J. Biomed. Inform. 455E. M. Van Mulligen et al., "The eu-adr corpus: annotated drugs, diseases, targets, and their relationships," J. Biomed. Inform., vol. 45, no. 5, pp. 879-884, 2012.</p>
<p>The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions. M Herrero-Zazo, I Segura-Bedmar, P Martínez, T Declerck, J. Biomed. Inform. 465M. Herrero-Zazo, I. Segura-Bedmar, P. Martínez, and T. Declerck, "The ddi corpus: An annotated corpus with pharmacological substances and drug-drug interactions," J. Biomed. Inform., vol. 46, no. 5, pp. 914-920, 2013.</p>
<p>Overview of the biocreative vi chemicalprotein interaction track. M Krallinger, Proc. sixth BioCreative Challenge Eval. Workshop. sixth BioCreative Challenge Eval. Workshop1M. Krallinger et al., "Overview of the biocreative vi chemical- protein interaction track," in Proc. sixth BioCreative Challenge Eval. Workshop, vol. 1, 2017, pp. 141-146.</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. R Socher, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essR. Socher et al., "Recursive deep models for semantic composi- tionality over a sentiment treebank," in Proc. Conf. Empir. Methods Natural Lang. Process., 2013, pp. 1631-1642.</p>
<p>Character-level convolutional networks for text classification. X Zhang, J Zhao, Y Lecun, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. SystX. Zhang, J. Zhao, and Y. LeCun, "Character-level convolutional networks for text classification," in Proc. Int. Conf. Neural Inf. Process. Syst., 2015.</p>
<p>Semeval-2014 task 4: Aspect based sentiment analysis. M Pontiki, D Galanis, J Pavlopoulos, H Papageorgiou, I Androutsopoulos, S Manandhar, Proc. 8th Int Workshop Semantic Eval. 8th Int Workshop Semantic EvalM. Pontiki, D. Galanis, J. Pavlopoulos, H. Papageorgiou, I. An- droutsopoulos, and S. Manandhar, "Semeval-2014 task 4: Aspect based sentiment analysis," in Proc. 8th Int Workshop Semantic Eval., 2014.</p>
<p>Mrqa 2019 shared task: Evaluating generalization in reading comprehension. A Fisch, A Talmor, R Jia, M Seo, E Choi, D Chen, Proc. 2nd Workshop Mach. Reading Question Answering. 2nd Workshop Mach. Reading Question AnsweringA. Fisch, A. Talmor, R. Jia, M. Seo, E. Choi, and D. Chen, "Mrqa 2019 shared task: Evaluating generalization in reading comprehension," in Proc. 2nd Workshop Mach. Reading Question Answering, 2019, pp. 1-13.</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D S Weld, L Zettlemoyer, Proc. 55th Annu. Meeting Assoc. Comput. Linguistics. 55th Annu. Meeting Assoc. Comput. LinguisticsM. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension," in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, 2017, pp. 1601-1611.</p>
<p>Searchqa: A new q&amp;a dataset augmented with context from a search engine. M Dunn, L Sagun, M Higgins, V U Guney, V Cirik, K Cho, arXiv:1704.05179arXiv preprintM. Dunn, L. Sagun, M. Higgins, V. U. Guney, V. Cirik, and K. Cho, "Searchqa: A new q&amp;a dataset augmented with context from a search engine," arXiv preprint arXiv:1704.05179, 2017.</p>
<p>Natural questions: A benchmark for question answering research. T Kwiatkowski, Trans. Assoc. Comput. Linguistics. 7T. Kwiatkowski et al., "Natural questions: A benchmark for question answering research," Trans. Assoc. Comput. Linguistics, vol. 7, pp. 452-466, 2019.</p>
<p>Semantic parsing on freebase from question-answer pairs. J Berant, A Chou, R Frostig, P Liang, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essJ. Berant, A. Chou, R. Frostig, and P. Liang, "Semantic parsing on freebase from question-answer pairs," in Proc. Conf. Empir. Methods Natural Lang. Process., 2013, pp. 1533-1544.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. A Talmor, J Herzig, N Lourie, J Berant, Proc. Conference North Amer. Chapter Assoc. Conference North Amer. Chapter AssocA. Talmor, J. Herzig, N. Lourie, and J. Berant, "Commonsenseqa: A question answering challenge targeting commonsense knowledge," in Proc. Conference North Amer. Chapter Assoc. Comput. Linguistics, 2019, pp. 4149-4158.</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, Proc. Conf. Empir. Methods Natural Lang. Conf. Empir. Methods Natural LangT. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, "Can a suit of armor conduct electricity? a new dataset for open book question answering," in Proc. Conf. Empir. Methods Natural Lang. Process., 2018, pp. 2381-2391.</p>
<p>Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. L Huang, R Le Bras, C Bhagavatula, Y Choi, Proc. 57th Annu. Meeting Assoc. Comput. Linguistics and 9th Int. Joint Conf. 57th Annu. Meeting Assoc. Comput. Linguistics and 9th Int. Joint ConfL. Huang, R. Le Bras, C. Bhagavatula, and Y. Choi, "Cosmos qa: Machine reading comprehension with contextual commonsense reasoning," in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics and 9th Int. Joint Conf. Artif. Intell., 2019, pp. 2391-2401.</p>
<p>Knowledge enhanced attention for robust natural language inference. A H Li, A Sethy, arXiv:1909.00102arXiv preprintA. H. Li and A. Sethy, "Knowledge enhanced attention for robust natural language inference," arXiv preprint arXiv:1909.00102, 2019.</p>
<p>Giving bert a calculator: Finding operations and arguments with reading comprehension. D Andor, L He, K Lee, E Pitler, Proc. 2019 Conf. Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process. 2019 Conf. Empir. Methods Natural Lang. ess. and 9th Int. Joint Conf. Natural Lang. essD. Andor, L. He, K. Lee, and E. Pitler, "Giving bert a calculator: Finding operations and arguments with reading comprehension," in Proc. 2019 Conf. Empir. Methods Natural Lang. Process. and 9th Int. Joint Conf. Natural Lang. Process., 2019, pp. 5947-5952.</p>
<p>Convolutional 2d knowledge graph embeddings. T Dettmers, P Minervini, P Stenetorp, S Riedel, Proc. 32th AAAI Conf. 32th AAAI ConfT. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel, "Convolu- tional 2d knowledge graph embeddings," in Proc. 32th AAAI Conf. Artif. Intell., 2018.</p>
<p>Commonsense knowledge base completion. X Li, A Taheri, L Tu, K Gimpel, Proc. 57th Annu. Meeting Assoc. Comput. Linguistics. 57th Annu. Meeting Assoc. Comput. LinguisticsX. Li, A. Taheri, L. Tu, and K. Gimpel, "Commonsense knowledge base completion," in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, 2016, pp. 1445-1455.</p>
<p>Open mind common sense: Knowledge acquisition from the general public. P Singh, T Lin, E T Mueller, G Lim, T Perkins, W L Zhu, OTM Confederated Int. Conf., "On the Move to Meaningful Internet Systems. Berlin, GermanySpringerP. Singh, T. Lin, E. T. Mueller, G. Lim, T. Perkins, and W. L. Zhu, "Open mind common sense: Knowledge acquisition from the general public," in OTM Confederated Int. Conf., "On the Move to Meaningful Internet Systems". Berlin, Germany: Springer, 2002, pp. 1223-1237.</p>
<p>Kg-bert: Bert for knowledge graph completion. L Yao, C Mao, Y Luo, arXiv:1909.03193arXiv preprintL. Yao, C. Mao, and Y. Luo, "Kg-bert: Bert for knowledge graph completion," arXiv preprint arXiv:1909.03193, 2019.</p>
<p>Multimodal joint attribute prediction and value extraction for ecommerce product. T Zhu, Y Wang, H Li, Y Wu, X He, B Zhou, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essT. Zhu, Y. Wang, H. Li, Y. Wu, X. He, and B. Zhou, "Multimodal joint attribute prediction and value extraction for ecommerce product," in Proc. Conf. Empir. Methods Natural Lang. Process., 2020, pp. 2129-2139.</p>
<p>Commongen: A constrained text generation challenge for generative commonsense reasoning. B Y Lin, Findings Assoc. Comput. Linguistics. B. Y. Lin et al., "Commongen: A constrained text generation challenge for generative commonsense reasoning," in Findings Assoc. Comput. Linguistics, 2020, pp. 1823-1840.</p>
<p>A corpus and cloze evaluation for deeper understanding of commonsense stories. N Mostafazadeh, Proc. Conference North Amer. Conference North AmerN. Mostafazadeh et al., "A corpus and cloze evaluation for deeper understanding of commonsense stories," in Proc. Conference North Amer. Chapter Assoc. Comput. Linguistics, 2016, pp. 839-849.</p>
<p>Story ending generation with incremental encoding and commonsense knowledge. J Guan, Y Wang, M Huang, Proc. 33th AAAI Conf. 33th AAAI Conf33J. Guan, Y. Wang, and M. Huang, "Story ending generation with incremental encoding and commonsense knowledge," in Proc. 33th AAAI Conf. Artif. Intell., vol. 33, no. 01, 2019, pp. 6473-6480.</p>
<p>Knowledgegrounded dialogue generation with pre-trained language models. X Zhao, W Wu, C Xu, C Tao, D Zhao, R Yan, arXiv:2010.08824arXiv preprintX. Zhao, W. Wu, C. Xu, C. Tao, D. Zhao, and R. Yan, "Knowledge- grounded dialogue generation with pre-trained language models," arXiv preprint arXiv:2010.08824, 2020.</p>
<p>Wizard of wikipedia: Knowledge-powered conversational agents. E Dinan, S Roller, K Shuster, A Fan, M Auli, J Weston, arXiv:1811.01241arXiv preprintE. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston, "Wizard of wikipedia: Knowledge-powered conversational agents," arXiv preprint arXiv:1811.01241, 2018.</p>
<p>A dataset for document grounded conversations. K Zhou, S Prabhumoye, A W Black, Proc. 2018 Conf. Empir. Methods Natural Lang. 2018 Conf. Empir. Methods Natural LangK. Zhou, S. Prabhumoye, and A. W. Black, "A dataset for document grounded conversations," in Proc. 2018 Conf. Empir. Methods Natural Lang. Process., 2018, pp. 708-713.</p>
<p>Ms marco: A human generated machine reading comprehension dataset. T Nguyen, M Rosenberg, X Song, J Gao, S Tiwary, R Majumder, L Deng, Proc. Workshop on Cognitive Comput.: Integrating neural and symbolic approaches 2016 co-located with the 30th Annu. Conf. Neural Inf. Process. Syst. Workshop on Cognitive Comput.: Integrating neural and symbolic approaches 2016 co-located with the 30th Annu. Conf. Neural Inf. ess. SystT. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, "Ms marco: A human generated machine reading comprehension dataset," in Proc. Workshop on Cognitive Comput.: Integrating neural and symbolic approaches 2016 co-located with the 30th Annu. Conf. Neural Inf. Process. Syst., 2016. [Online]. Available: http://ceur-ws.org/Vol-1773/CoCoNIPS 2016 paper9.pdf</p>
<p>Hyte: Hyperplanebased temporally aware knowledge graph embedding. S S Dasgupta, S N Ray, P Talukdar, Proc. nullS. S. Dasgupta, S. N. Ray, and P. Talukdar, "Hyte: Hyperplane- based temporally aware knowledge graph embedding," in Proc.</p>
<p>. Conf, Empir, Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. Process., 2018, pp. 2001-2011.</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2203.02155arXiv preprintL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Understanding the integration of knowledge in language models with graph convolutions. Y Hou, G Fu, M Sachan, arXiv:2202.00964arXiv preprintY. Hou, G. Fu, and M. Sachan, "Understanding the integration of knowledge in language models with graph convolutions," arXiv preprint arXiv:2202.00964, 2022.</p>
<p>Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge. P Verga, H Sun, L B Soares, W W Cohen, arXiv:2007.00849arXiv preprintP. Verga, H. Sun, L. B. Soares, and W. W. Cohen, "Facts as ex- perts: Adaptable and interpretable neural memory over symbolic knowledge," arXiv preprint arXiv:2007.00849, 2020.</p>
<p>Model compression and acceleration for deep neural networks: The principles, progress, and challenges. Y Cheng, D Wang, P Zhou, T Zhang, IEEE Signal Process. Mag. 351Y. Cheng, D. Wang, P. Zhou, and T. Zhang, "Model compres- sion and acceleration for deep neural networks: The principles, progress, and challenges," IEEE Signal Process. Mag., vol. 35, no. 1, pp. 126-136, 2018.</p>
<p>Q-bert: Hessian based ultra low precision quantization of bert. S Shen, Proc. 34th AAAI Conf. 34th AAAI Conf34S. Shen et al., "Q-bert: Hessian based ultra low precision quantiza- tion of bert," in Proc. 34th AAAI Conf. Artif. Intell., vol. 34, no. 05, 2020, pp. 8815-8821.</p>
<p>Distilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.025312arXiv preprintG. Hinton, O. Vinyals, J. Dean et al., "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, vol. 2, no. 7, 2015.</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.11942arXiv preprintZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, "Albert: A lite bert for self-supervised learning of language representations," arXiv preprint arXiv:1909.11942, 2019.</p>
<p>Evaluating the factual consistency of abstractive text summarization. W Kryściński, B Mccann, C Xiong, R Socher, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essW. Kryściński, B. McCann, C. Xiong, and R. Socher, "Evaluating the factual consistency of abstractive text summarization," in Proc. Conf. Empir. Methods Natural Lang. Process., 2020, pp. 9332-9346.</p>
<p>Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. S Hu, N Ding, H Wang, Z Liu, J Wang, J Li, W Wu, M Sun, ACL. S. Hu, N. Ding, H. Wang, Z. Liu, J. Wang, J. Li, W. Wu, and M. Sun, "Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification," in ACL, 2022, pp. 2225- 2240.</p>
<p>Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. X Chen, N Zhang, X Xie, S Deng, Y Yao, C Tan, F Huang, L Si, H Chen, WWWX. Chen, N. Zhang, X. Xie, S. Deng, Y. Yao, C. Tan, F. Huang, L. Si, and H. Chen, "Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction," in WWW, 2022, pp. 2778-2788.</p>
<p>Ontology-enhanced prompt-tuning for few-shot learning. H Ye, N Zhang, S Deng, X Chen, H Chen, F Xiong, X Chen, H Chen, WWWH. Ye, N. Zhang, S. Deng, X. Chen, H. Chen, F. Xiong, X. Chen, and H. Chen, "Ontology-enhanced prompt-tuning for few-shot learning," in WWW, 2022, pp. 778-787.</p>
<p>Improving language model predictions via prompts enriched with knowledge graphs. B Ryan, D Minh-Hoang, H Fabian, H Yuan, M.-P Albert, S Vijay, ISWC. B. Ryan, D. Minh-Hoang, H. Fabian, H. Yuan, M.-P. Albert, and S. Vijay, "Improving language model predictions via prompts enriched with knowledge graphs," in ISWC, 2022, pp. 1-10.</p>
<p>Does external knowledge help explainable natural language inference? automatic evaluation vs. human ratings. H Schuff, H.-Y Yang, H Adel, N T Vu, Proc. 4th BlackboxNLP Workshop Analyzing and interpreting Neural Netw for NLP. 4th BlackboxNLP Workshop Analyzing and interpreting Neural Netw for NLPH. Schuff, H.-Y. Yang, H. Adel, and N. T. Vu, "Does external knowledge help explainable natural language inference? auto- matic evaluation vs. human ratings," in Proc. 4th BlackboxNLP Workshop Analyzing and interpreting Neural Netw for NLP, 2021, pp. 26-41.</p>
<p>Tracing knowledge in language models back to the training data. E Akyürek, arXiv:2205.11482arXiv preprintE. Akyürek et al., "Tracing knowledge in language models back to the training data," arXiv preprint arXiv:2205.11482, 2022.</p>
<p>Editing factual knowledge in language models. N De Cao, W Aziz, I Titov, Proc. Conf. Empir. Methods Natural Lang. Process. Conf. Empir. Methods Natural Lang. essN. De Cao, W. Aziz, and I. Titov, "Editing factual knowledge in language models," in Proc. Conf. Empir. Methods Natural Lang. Process., 2021, pp. 6491-6506.</p>
<p>Locating and editing factual associations in gpt. K Meng, D Bau, A Andonian, Y Belinkov, arXiv:2202.05262arXiv preprintK. Meng, D. Bau, A. Andonian, and Y. Belinkov, "Locating and editing factual associations in gpt," arXiv preprint arXiv:2202.05262, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
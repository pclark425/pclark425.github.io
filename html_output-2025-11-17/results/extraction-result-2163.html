<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2163 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2163</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2163</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-276767120</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.01508v1.pdf" target="_blank">E NABLING AI S CIENTISTS TO R ECOGNIZE I NNOVATION : A D OMAIN -A GNOSTIC A LGORITHM FOR A SSESSING N OVELTY</a></p>
                <p><strong>Paper Abstract:</strong> In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery. This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea’s local density with its adjacent neighbors’ densities. We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment. Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820) and biomedical research (AUROC=0.765) domains. Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain evaluation. These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2163.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2163.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Neighbor Density</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic algorithm introduced in this paper that scores the novelty of a research idea by comparing the idea's local semantic neighbor density to the local densities of its adjacent neighbors (quantile-based score). It is validated at scale using automated temporal test-set construction and shown to be domain-invariant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Relative Neighbor Density (RND) algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>non-LLM statistical / embedding-based novelty metric</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific literature (computer science and biomedical)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Does not generate hypotheses; assesses novelty of generated research ideas/abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automated temporal validation: positive samples = recent high-quality papers from top venues (NeurIPS 2024 acceptances, Nature Medicine 2024–Feb 2025 articles); negative samples = older highly-cited papers from the same venues; evaluation metric = AUROC on constructed test sets (NeurIPS, Nature Medicine, Mixed).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Percentile of the idea's neighbor-density relative to the empirical distribution of neighbor-densities among its P nearest neighbors (score = 100 * ECDF_P(ND_idea)). Uses P nearest neighbors and Q neighbors to compute local ND.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (RND is an evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>AUROC reported: NeurIPS = 0.820; Nature Medicine = 0.765; Mixed (cross-domain) = 0.795. Shows consistent high performance across single-domain and cross-domain test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>RND is designed to be invariant to domain and produces score distributions that are stable across domains; its validation performance remains high for both familiar (single-domain) and novel / cross-domain examples (AUROC ~0.78–0.82).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Not applicable in the sense of generation vs validation for RND itself; RND is presented as a potential mechanism to reduce asymmetry by providing reliable novelty signals for generators (e.g., reasoning models).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Robust: Mixed (cross-domain) AUROC = 0.795, indicating strong OOD / multi-domain performance compared to absolute density baselines that degrade markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Empirically shows near-identical score distributions across domains (claimed domain-invariant quantile behavior); theoretical analysis via probability integral transform supports uniformity of quantile scores.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Computational complexity O(P * Q) per idea; authors chose P=100 and Q=50 for experiments (trade-off between statistical stability and compute).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Provides domain-invariant quantile scores that can be used as reward signals in RL for reasoning/generation models; reduces need for manual labeled validation sets via automated temporal test construction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>contradicts</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RND provides a domain-invariant, scalable automated novelty score validated on large literature corpora (PubMed + arXiv) with AUROCs ≈0.76–0.82, and it resists cross-domain degradation seen in absolute-density and LLM-judge methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2163.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model with retrieved literature (LLM + literature search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A judgment pipeline that supplies an LLM with the titles/abstracts of the top-k retrieved relevant papers (here k=10) and asks it to decide whether the core concepts overlap with the candidate idea; used both as a novelty-assessor baseline and discussed as a component of larger AI-scientist systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM + literature search</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (LLM) plus retrieval augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific literature (CS and biomedical evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>When used in AI scientist workflows, LLMs can generate research ideas/hypotheses; in this paper the configuration is used as a novelty judge rather than a generator.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Provide the LLM with the 10 most relevant papers (retrieved by semantic embedding) and ask whether the idea is substantively overlapping; determination is binary (0 non-novel if overlap found, 1 novel otherwise); evaluated using AUROC on test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>LLM's internal judgment about overlap with retrieved literature (binary / probabilistic judgment aggregated across repeated runs).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not reported for generation in this paper; focus is on judging novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported AUROC varies by domain and model: with Sonnet-3.7 as the inference model + literature search, AUROC ≈ 0.80 on NeurIPS (computer-science) but degrades to ≈0.60 on Nature Medicine and cross-domain mixed set. Authors report running LLM baselines multiple times and averaging due to output variability.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance degrades on unfamiliar / out-of-domain content (e.g., biomedical) relative to familiar domain (computer science); internal model knowledge biases judgments even when external papers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Evidence of asymmetry: while LLMs can be strong text generators, their reliability as validators is brittle and domain-dependent; judgments are sensitive to phrasing and internal knowledge gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degraded — AUROC drops from ~0.80 (in-domain CS) to ~0.60 (biomed / cross-domain), indicating lowered OOD reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantitatively reported; paper cites known sensitivity of autoregressive LLMs to input perturbations, implying poor calibration and stability in judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Higher than pure embedding metrics because of retrieval + multiple LLM queries and repeated runs (authors ran LLM-based baselines three times and averaged results).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Augmenting LLMs with retrieved literature substantially improves judgment accuracy compared to guideline-only prompting; still requires robust retrieval and likely human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented LLM judging improves novelty assessment in familiar domains but suffers substantial domain transfer degradation; outputs are variable and require multiple runs and external knowledge to approach competitive accuracy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2163.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+guideline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM with review-guideline prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an LLM prompted with an explicit review guideline (NeurIPS 2024 review guideline used here) to judge novelty; evaluated as a baseline for novelty assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM with guideline prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model with structured prompt</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>computer science (NeurIPS) primarily</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>LLMs in this setup act as validators (not used to generate ideas in reported experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Prompt the LLM with formal review criteria (NeurIPS review guideline) and request overall novelty score; evaluated on test sets where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>LLM's assessment following the guideline as scoring rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Poor without literature context: authors report LLM-with-guideline settings provide highly inaccurate novelty judgments (performance comparable to random guessing when no external literature provided). Sonnet-3.7 with guideline performed poorly on Nature Medicine and Mixed (not applicable or low).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>High sensitivity to domain and absence of external evidence; cannot reliably judge novel vs familiar without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Shows that prompting alone does not close the validation gap — reasoning-capable models still fail to robustly validate novelty without external evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor; not robust cross-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; qualitative claim of unreliability and sensitivity to input phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Lower than retrieval-augmented LLMs (no external search) but provides poor accuracy, so cost/benefit negative.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Not effective by itself; needs retrieval or further tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM judgement guided only by a review template is insufficient for reliable novelty assessment — external literature is critical for performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2163.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+tournament</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Swiss-tournament judging (pairwise comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Swiss-system tournament approach where an LLM evaluates standardized project proposals pairwise in rounds to produce ranked novelty scores; used as an LLM-based baseline (Si et al. approach).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM Swiss-tournament judge</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model with pairwise comparison protocol</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>computer science evaluation (ICLR style) and general idea ranking</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Used for judging/ranking ideas rather than producing them in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Transform ideas into a standardized proposal format, run iterative pairwise LLM comparisons (Swiss tournament), and assign scores by wins; evaluated on novelty test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Relative wins in tournament; pairwise judgments of novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>When run without literature augmentation, tournament-based LLM judgments performed poorly and were often no better than random; Sonnet-3.7 with tournament was unreliable absent external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Tournament approach does not overcome domain knowledge gaps; still sensitive to phrasing and domain-specific familiarity.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Tournament judging exposes variability and instability in LLM judgments; generation may be fluent while validation via pairwise comparisons remains brittle.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degraded absent retrieval; not robust across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; implied poor calibration for novelty classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Higher due to multiple pairwise LLM comparisons per idea (iterative tournament rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combining tournaments with literature retrieval or expert-in-the-loop may improve outcomes but not evaluated fully here.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pairwise LLM tournament methods are insufficient alone for reliable novelty assessment and are sensitive to missing external literature/context.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2163.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Historical Dissimilarity (Absolute Local Density)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An absolute local density metric (from prior work) measuring novelty as average embedding distance between a candidate abstract and the 5 most similar historical abstracts (used in ON metric); treats absolute distance as a novelty score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Historical Dissimilarity (HD)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>embedding-distance / local density metric</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific literature (computer science and biomedical evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Not a generator — a novelty metric used to assess generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Measure average Euclidean distance between idea embedding and k (e.g., 5) nearest historical abstracts; used directly or combined into Overall Novelty (ON) with a contemporary impact metric; validated against human labels in prior work and against automated test sets here using AUROC.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Absolute local distance (dissimilarity) to nearest neighbors in historical and contemporary databases (HD and CD), optionally combined with citation-based Contemporary Impact to form ON.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Per-paper results: NeurIPS AUROC ≈ 0.851; Nature Medicine ≈ 0.757; Mixed (cross-domain) AUROC ≈ 0.395 (severe cross-domain degradation). Authors note HD closely matches RND within single domains but fails in cross-domain evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance highly domain-dependent — good in-domain but poor for mixed/domain-shift situations, indicating sensitivity to corpus size, temporal boundaries, and domain-specific semantic densities.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>As a validator metric, HD's asymmetry manifests as good single-domain discrimination but inability to provide comparable scores across domains, exacerbating generator-validation mismatch in multi-disciplinary settings.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor: Mixed AUROC drops to 0.395, demonstrating large OOD failure.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Score distributions are domain-dependent (non-uniform across domains), i.e., poorly calibrated between domains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Lower than LLM-based judging; requires retrieval of a small fixed number of nearest neighbors (e.g., 5).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>RND is proposed explicitly to address HD's cross-domain shortcomings by using relative neighbor densities (quantile) instead of absolute distances.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Absolute local-density metrics can perform well within domains but fail to generalize across domains, producing inconsistent score distributions and large performance drops under domain shift.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2163.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Overall Novelty (ON) metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined metric introduced in prior work that multiplies historical dissimilarity (HD) with a citation-based Contemporary Impact (CI) term and contemporary dissimilarity (CD) to produce an Overall Novelty score; validated previously to correlate with human-labeled novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Overall Novelty (ON)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>composite embedding-distance + citation-impact metric</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific literature novelty assessment</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Evaluator (not a generator).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Combine Historical Dissimilarity, Contemporary Dissimilarity, and Contemporary Impact (citation) to produce a novelty score; prior work reported correlation with human labels; in this paper ON is used as a baseline conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Product/composition of HD × (CI / CD) per Su et al. (2024) formulation referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Prior work reported correlation with human-labeled novelty (paper cites Su et al. validation), but ON inherits HD's domain sensitivity and arbitrary-decision dependence (k, temporal splits).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Susceptible to domain-specific citation practices and temporal window choices; may misclassify across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>As a metric, ON can misalign with human judgments when corpus choices vary, contributing to validation unreliability for generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Authors assert ON and absolute-density metrics generalize poorly across domains; no explicit ON AUROC numbers given here.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not uniform across domains: depends on corpus selection and citation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Requires nearest-neighbor retrieval and citation statistics computation; moderate.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Not presented as closing the gap; RND offered as an alternative to avoid ON's arbitrary choices.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ON correlates with human labels in prior small-scale studies but is sensitive to arbitrary choices (k, time windows, citation patterns) and thus limited in generalizability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2163.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sonnet-3.7</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.7 Sonnet (Anthropic Sonnet-3.7)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent high-performing reasoning model used as an LLM judge baseline in experiments; shows better reasoning capabilities than GPT-4o but performance depends strongly on external literature input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sonnet-3.7</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning and novelty judgment (evaluated on CS and biomedical novelty tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Capable of generating text and research ideas (not evaluated as generator here), used primarily as a novelty judge in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>As a judge, tested under prompting variants: (1) no external literature + guideline/tournament, (2) with retrieved literature (top-10). Evaluation via AUROC on constructed test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>LLM scoring output when asked to judge novelty using provided inputs and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not reported here for generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>With literature search: AUROC ≈ 0.80 on NeurIPS (in-domain CS); degrades to ≈0.60 on Nature Medicine and cross-domain. Without literature, judgments are highly inaccurate (near-random).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Significant degradation on unfamiliar (biomedical) domain versus familiar (computer science) domain; performance heavily driven by internal model knowledge plus retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Model can perform strong reasoning in-domain when augmented by retrieval, but its validation capability is brittle out-of-domain, evidencing generation/validation asymmetry under domain shift.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degraded OOD performance: drop from AUROC ~0.8 to ~0.6 in other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; paper reports sensitivity and instability in LLM judgments generally.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High if retrieval used; also multiple runs were averaged due to variability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Augment with literature retrieval and possibly RND as reward signal in RL training to improve novelty generation/validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sonnet-3.7 achieves high novelty-judgment AUROC in a familiar domain when provided retrieved papers, but its accuracy degrades substantially in other domains, demonstrating brittleness of LLM-as-judge.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2163.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-r1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning-for-reasoning model (Guo et al. 2025) that aims to incentivize reasoning ability in LLMs using rule-based reward systems; mentioned as an improved reasoning model with potential for integrating novelty reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Incentivizing reasoning capability in llms via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deepseek-r1</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>reinforcement-learned reasoning model (LLM fine-tuned with RL)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general reasoning for tasks including novelty generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates reasoning chains and potentially research ideas; paper references it as a reasoning model rather than providing experimental generation results here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not directly validated in this paper; authors suggest RND could be used as a reward mechanism for Deepseek-style RL training to encourage novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not detailed in this paper; potential measure would be external reward signals (e.g., RND scores) during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not reported here; authors cite Deepseek-r1 as exhibiting improved reasoning capabilities in other work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Not applicable in-paper; suggested that adding domain-invariant novelty rewards could improve generation of novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Paper posits that RND could reduce the generation-validation gap when used as an RL reward, but no empirical results are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Proposed: use RND as a reward mechanism in RL training to align generation with validated novelty signals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deepseek-r1 exemplifies recent RL-for-reasoning advances; the paper proposes using RND as a reward signal to guide such models toward more genuinely novel idea generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2163.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (autoregressive LLM system card cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive LLM cited in the paper as an example of models whose outputs are sensitive to input perturbations and who show lower accuracy for novelty judgment compared to newer reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>autoregressive large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general text generation and reasoning; evaluated here as a novelty judge baseline</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates text and research ideas; used here as a judge in comparative baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>LLM-as-judge baseline; sensitive to prompt/phrasing and unstable; authors note GPT-4o achieved near-random AUROC historically in their tests (≈0.5) without robust retrieval/augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>LLM output judgments when prompted about novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not quantified here for idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported as low/inferior: GPT-4o produced outputs sensitive to perturbations and had poorer AUROC compared to newer reasoning models (authors mention GPT-4o around random baseline in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Highly sensitive to input rephrasing and lacks reliability for novelty judgment, especially without retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Shows asymmetry: fluent generation but unreliable validation judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor and unstable; not robust cross-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Described as poorly calibrated in terms of judgment stability (sensitive to perturbations); no numeric calibration reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Lower than some retrieval-augmented complex pipelines but unreliable; repeated runs may be needed to reduce variance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Retrieval augmentation and improved reasoning models recommended; RND suggested as a non-LLM validation mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Autoregressive LLMs like GPT-4o are sensitive to input perturbations and provide unreliable novelty judgments absent strong augmentation, reinforcing the fabrication-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2163.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (AI Scientist framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concept and framework (Lu et al. 2024) integrating idea generation, evaluation, and refinement with LLMs and external tools (e.g., Semantic Scholar API) to automate research workflows; used as motivating prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist framework (Lu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-component automated research pipeline (LLM generation + retrieval + evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated scientific discovery / idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates research ideas/hypotheses, iteratively refines them using retrieval and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In prior work, integrates review guidelines and semantic search (Semantic Scholar API) to help LLMs determine novelty; validation often relies on human assessments in prior literature and is described as a motivation for RND.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>LLM-informed judgments augmented by retrieved literature and review-guideline prompts; prior systems used human or LLM-based judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Prior approaches are sensitive to LLM reliability and domain; the current paper argues these frameworks need better, scalable novelty validators like RND.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Frameworks integrate generation and validation but rely heavily on LLM judgment and human labels; the paper identifies a validation bottleneck and proposes RND to alleviate it.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Authors propose integrating RND into AI Scientist workflows to provide automated, domain-invariant novelty signals and reduce reliance on expert labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI Scientist frameworks provide end-to-end automation potential but are constrained by brittle LLM judgment and lack scalable, domain-invariant novelty validators—RND is proposed as a solution.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2163.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2163.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM-Generated Ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system (Hu et al. 2024) that iteratively plans and searches to improve novelty/diversity of ideas generated by LLMs; cited as related work exploring LLMs in idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nova</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based generation framework with planning/search</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM-generated research ideas and creativity optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates diverse and novel research ideas by iterative planning and search in idea space.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Prior work used LLM-judges and/or human evaluation to rate novelty; not evaluated directly in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Diversity- and novelty-promoting planning objectives internal to the generation pipeline; external validation was done in prior work via human labels/LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Prior systems emphasize generation-side mechanisms; this paper highlights the need for scalable and robust validators like RND.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Could benefit from integrating RND for automatic novelty scoring during iterative generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generation-focused systems (e.g., Nova) improve idea generation but rely on human/LLM judgments for validation; RND could supply scalable validation to complement such generators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers <em>(Rating: 2)</em></li>
                <li>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas <em>(Rating: 2)</em></li>
                <li>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation <em>(Rating: 2)</em></li>
                <li>Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2163",
    "paper_id": "paper-276767120",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "RND",
            "name_full": "Relative Neighbor Density",
            "brief_description": "A domain-agnostic algorithm introduced in this paper that scores the novelty of a research idea by comparing the idea's local semantic neighbor density to the local densities of its adjacent neighbors (quantile-based score). It is validated at scale using automated temporal test-set construction and shown to be domain-invariant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Relative Neighbor Density (RND) algorithm",
            "system_type": "non-LLM statistical / embedding-based novelty metric",
            "domain": "general scientific literature (computer science and biomedical)",
            "generation_capability": "Does not generate hypotheses; assesses novelty of generated research ideas/abstracts",
            "validation_method": "Automated temporal validation: positive samples = recent high-quality papers from top venues (NeurIPS 2024 acceptances, Nature Medicine 2024–Feb 2025 articles); negative samples = older highly-cited papers from the same venues; evaluation metric = AUROC on constructed test sets (NeurIPS, Nature Medicine, Mixed).",
            "novelty_measure": "Percentile of the idea's neighbor-density relative to the empirical distribution of neighbor-densities among its P nearest neighbors (score = 100 * ECDF_P(ND_idea)). Uses P nearest neighbors and Q neighbors to compute local ND.",
            "generation_performance": "Not applicable (RND is an evaluator).",
            "validation_performance": "AUROC reported: NeurIPS = 0.820; Nature Medicine = 0.765; Mixed (cross-domain) = 0.795. Shows consistent high performance across single-domain and cross-domain test sets.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "RND is designed to be invariant to domain and produces score distributions that are stable across domains; its validation performance remains high for both familiar (single-domain) and novel / cross-domain examples (AUROC ~0.78–0.82).",
            "generation_validation_asymmetry": "Not applicable in the sense of generation vs validation for RND itself; RND is presented as a potential mechanism to reduce asymmetry by providing reliable novelty signals for generators (e.g., reasoning models).",
            "out_of_distribution_performance": "Robust: Mixed (cross-domain) AUROC = 0.795, indicating strong OOD / multi-domain performance compared to absolute density baselines that degrade markedly.",
            "calibration_quality": "Empirically shows near-identical score distributions across domains (claimed domain-invariant quantile behavior); theoretical analysis via probability integral transform supports uniformity of quantile scores.",
            "validation_computational_cost": "Computational complexity O(P * Q) per idea; authors chose P=100 and Q=50 for experiments (trade-off between statistical stability and compute).",
            "human_validation_required": false,
            "gap_closing_mechanisms": "Provides domain-invariant quantile scores that can be used as reward signals in RL for reasoning/generation models; reduces need for manual labeled validation sets via automated temporal test construction.",
            "evidence_type": "contradicts",
            "key_findings": "RND provides a domain-invariant, scalable automated novelty score validated on large literature corpora (PubMed + arXiv) with AUROCs ≈0.76–0.82, and it resists cross-domain degradation seen in absolute-density and LLM-judge methods.",
            "uuid": "e2163.0"
        },
        {
            "name_short": "LLM+search",
            "name_full": "Large Language Model with retrieved literature (LLM + literature search)",
            "brief_description": "A judgment pipeline that supplies an LLM with the titles/abstracts of the top-k retrieved relevant papers (here k=10) and asks it to decide whether the core concepts overlap with the candidate idea; used both as a novelty-assessor baseline and discussed as a component of larger AI-scientist systems.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM + literature search",
            "system_type": "large language model (LLM) plus retrieval augmentation",
            "domain": "general scientific literature (CS and biomedical evaluated)",
            "generation_capability": "When used in AI scientist workflows, LLMs can generate research ideas/hypotheses; in this paper the configuration is used as a novelty judge rather than a generator.",
            "validation_method": "Provide the LLM with the 10 most relevant papers (retrieved by semantic embedding) and ask whether the idea is substantively overlapping; determination is binary (0 non-novel if overlap found, 1 novel otherwise); evaluated using AUROC on test sets.",
            "novelty_measure": "LLM's internal judgment about overlap with retrieved literature (binary / probabilistic judgment aggregated across repeated runs).",
            "generation_performance": "Not reported for generation in this paper; focus is on judging novelty.",
            "validation_performance": "Reported AUROC varies by domain and model: with Sonnet-3.7 as the inference model + literature search, AUROC ≈ 0.80 on NeurIPS (computer-science) but degrades to ≈0.60 on Nature Medicine and cross-domain mixed set. Authors report running LLM baselines multiple times and averaging due to output variability.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance degrades on unfamiliar / out-of-domain content (e.g., biomedical) relative to familiar domain (computer science); internal model knowledge biases judgments even when external papers are provided.",
            "generation_validation_asymmetry": "Evidence of asymmetry: while LLMs can be strong text generators, their reliability as validators is brittle and domain-dependent; judgments are sensitive to phrasing and internal knowledge gaps.",
            "out_of_distribution_performance": "Degraded — AUROC drops from ~0.80 (in-domain CS) to ~0.60 (biomed / cross-domain), indicating lowered OOD reliability.",
            "calibration_quality": "Not quantitatively reported; paper cites known sensitivity of autoregressive LLMs to input perturbations, implying poor calibration and stability in judgments.",
            "validation_computational_cost": "Higher than pure embedding metrics because of retrieval + multiple LLM queries and repeated runs (authors ran LLM-based baselines three times and averaged results).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Augmenting LLMs with retrieved literature substantially improves judgment accuracy compared to guideline-only prompting; still requires robust retrieval and likely human oversight.",
            "evidence_type": "supports",
            "key_findings": "Retrieval-augmented LLM judging improves novelty assessment in familiar domains but suffers substantial domain transfer degradation; outputs are variable and require multiple runs and external knowledge to approach competitive accuracy.",
            "uuid": "e2163.1"
        },
        {
            "name_short": "LLM+guideline",
            "name_full": "LLM with review-guideline prompting",
            "brief_description": "Using an LLM prompted with an explicit review guideline (NeurIPS 2024 review guideline used here) to judge novelty; evaluated as a baseline for novelty assessment.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM with guideline prompting",
            "system_type": "large language model with structured prompt",
            "domain": "computer science (NeurIPS) primarily",
            "generation_capability": "LLMs in this setup act as validators (not used to generate ideas in reported experiments).",
            "validation_method": "Prompt the LLM with formal review criteria (NeurIPS review guideline) and request overall novelty score; evaluated on test sets where applicable.",
            "novelty_measure": "LLM's assessment following the guideline as scoring rubric.",
            "generation_performance": "Not applicable here.",
            "validation_performance": "Poor without literature context: authors report LLM-with-guideline settings provide highly inaccurate novelty judgments (performance comparable to random guessing when no external literature provided). Sonnet-3.7 with guideline performed poorly on Nature Medicine and Mixed (not applicable or low).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "High sensitivity to domain and absence of external evidence; cannot reliably judge novel vs familiar without retrieval.",
            "generation_validation_asymmetry": "Shows that prompting alone does not close the validation gap — reasoning-capable models still fail to robustly validate novelty without external evidence.",
            "out_of_distribution_performance": "Poor; not robust cross-domain.",
            "calibration_quality": "Not quantified; qualitative claim of unreliability and sensitivity to input phrasing.",
            "validation_computational_cost": "Lower than retrieval-augmented LLMs (no external search) but provides poor accuracy, so cost/benefit negative.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Not effective by itself; needs retrieval or further tooling.",
            "evidence_type": "supports",
            "key_findings": "LLM judgement guided only by a review template is insufficient for reliable novelty assessment — external literature is critical for performance.",
            "uuid": "e2163.2"
        },
        {
            "name_short": "LLM+tournament",
            "name_full": "LLM Swiss-tournament judging (pairwise comparisons)",
            "brief_description": "A Swiss-system tournament approach where an LLM evaluates standardized project proposals pairwise in rounds to produce ranked novelty scores; used as an LLM-based baseline (Si et al. approach).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM Swiss-tournament judge",
            "system_type": "large language model with pairwise comparison protocol",
            "domain": "computer science evaluation (ICLR style) and general idea ranking",
            "generation_capability": "Used for judging/ranking ideas rather than producing them in this paper's experiments.",
            "validation_method": "Transform ideas into a standardized proposal format, run iterative pairwise LLM comparisons (Swiss tournament), and assign scores by wins; evaluated on novelty test sets.",
            "novelty_measure": "Relative wins in tournament; pairwise judgments of novelty.",
            "generation_performance": "Not applicable.",
            "validation_performance": "When run without literature augmentation, tournament-based LLM judgments performed poorly and were often no better than random; Sonnet-3.7 with tournament was unreliable absent external knowledge.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Tournament approach does not overcome domain knowledge gaps; still sensitive to phrasing and domain-specific familiarity.",
            "generation_validation_asymmetry": "Tournament judging exposes variability and instability in LLM judgments; generation may be fluent while validation via pairwise comparisons remains brittle.",
            "out_of_distribution_performance": "Degraded absent retrieval; not robust across domains.",
            "calibration_quality": "Not quantified; implied poor calibration for novelty classification tasks.",
            "validation_computational_cost": "Higher due to multiple pairwise LLM comparisons per idea (iterative tournament rounds).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combining tournaments with literature retrieval or expert-in-the-loop may improve outcomes but not evaluated fully here.",
            "evidence_type": "supports",
            "key_findings": "Pairwise LLM tournament methods are insufficient alone for reliable novelty assessment and are sensitive to missing external literature/context.",
            "uuid": "e2163.3"
        },
        {
            "name_short": "HD",
            "name_full": "Historical Dissimilarity (Absolute Local Density)",
            "brief_description": "An absolute local density metric (from prior work) measuring novelty as average embedding distance between a candidate abstract and the 5 most similar historical abstracts (used in ON metric); treats absolute distance as a novelty score.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Historical Dissimilarity (HD)",
            "system_type": "embedding-distance / local density metric",
            "domain": "scientific literature (computer science and biomedical evaluated)",
            "generation_capability": "Not a generator — a novelty metric used to assess generated ideas.",
            "validation_method": "Measure average Euclidean distance between idea embedding and k (e.g., 5) nearest historical abstracts; used directly or combined into Overall Novelty (ON) with a contemporary impact metric; validated against human labels in prior work and against automated test sets here using AUROC.",
            "novelty_measure": "Absolute local distance (dissimilarity) to nearest neighbors in historical and contemporary databases (HD and CD), optionally combined with citation-based Contemporary Impact to form ON.",
            "generation_performance": "Not applicable.",
            "validation_performance": "Per-paper results: NeurIPS AUROC ≈ 0.851; Nature Medicine ≈ 0.757; Mixed (cross-domain) AUROC ≈ 0.395 (severe cross-domain degradation). Authors note HD closely matches RND within single domains but fails in cross-domain evaluation.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance highly domain-dependent — good in-domain but poor for mixed/domain-shift situations, indicating sensitivity to corpus size, temporal boundaries, and domain-specific semantic densities.",
            "generation_validation_asymmetry": "As a validator metric, HD's asymmetry manifests as good single-domain discrimination but inability to provide comparable scores across domains, exacerbating generator-validation mismatch in multi-disciplinary settings.",
            "out_of_distribution_performance": "Poor: Mixed AUROC drops to 0.395, demonstrating large OOD failure.",
            "calibration_quality": "Score distributions are domain-dependent (non-uniform across domains), i.e., poorly calibrated between domains.",
            "validation_computational_cost": "Lower than LLM-based judging; requires retrieval of a small fixed number of nearest neighbors (e.g., 5).",
            "human_validation_required": null,
            "gap_closing_mechanisms": "RND is proposed explicitly to address HD's cross-domain shortcomings by using relative neighbor densities (quantile) instead of absolute distances.",
            "evidence_type": "supports",
            "key_findings": "Absolute local-density metrics can perform well within domains but fail to generalize across domains, producing inconsistent score distributions and large performance drops under domain shift.",
            "uuid": "e2163.4"
        },
        {
            "name_short": "ON",
            "name_full": "Overall Novelty (ON) metric",
            "brief_description": "A combined metric introduced in prior work that multiplies historical dissimilarity (HD) with a citation-based Contemporary Impact (CI) term and contemporary dissimilarity (CD) to produce an Overall Novelty score; validated previously to correlate with human-labeled novelty.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Overall Novelty (ON)",
            "system_type": "composite embedding-distance + citation-impact metric",
            "domain": "scientific literature novelty assessment",
            "generation_capability": "Evaluator (not a generator).",
            "validation_method": "Combine Historical Dissimilarity, Contemporary Dissimilarity, and Contemporary Impact (citation) to produce a novelty score; prior work reported correlation with human labels; in this paper ON is used as a baseline conceptually.",
            "novelty_measure": "Product/composition of HD × (CI / CD) per Su et al. (2024) formulation referenced in the paper.",
            "generation_performance": "Not applicable.",
            "validation_performance": "Prior work reported correlation with human-labeled novelty (paper cites Su et al. validation), but ON inherits HD's domain sensitivity and arbitrary-decision dependence (k, temporal splits).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Susceptible to domain-specific citation practices and temporal window choices; may misclassify across domains.",
            "generation_validation_asymmetry": "As a metric, ON can misalign with human judgments when corpus choices vary, contributing to validation unreliability for generated ideas.",
            "out_of_distribution_performance": "Authors assert ON and absolute-density metrics generalize poorly across domains; no explicit ON AUROC numbers given here.",
            "calibration_quality": "Not uniform across domains: depends on corpus selection and citation behavior.",
            "validation_computational_cost": "Requires nearest-neighbor retrieval and citation statistics computation; moderate.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Not presented as closing the gap; RND offered as an alternative to avoid ON's arbitrary choices.",
            "evidence_type": "supports",
            "key_findings": "ON correlates with human labels in prior small-scale studies but is sensitive to arbitrary choices (k, time windows, citation patterns) and thus limited in generalizability.",
            "uuid": "e2163.5"
        },
        {
            "name_short": "Sonnet-3.7",
            "name_full": "Claude-3.7 Sonnet (Anthropic Sonnet-3.7)",
            "brief_description": "A recent high-performing reasoning model used as an LLM judge baseline in experiments; shows better reasoning capabilities than GPT-4o but performance depends strongly on external literature input.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Sonnet-3.7",
            "system_type": "large language model / reasoner",
            "domain": "general scientific reasoning and novelty judgment (evaluated on CS and biomedical novelty tasks)",
            "generation_capability": "Capable of generating text and research ideas (not evaluated as generator here), used primarily as a novelty judge in experiments.",
            "validation_method": "As a judge, tested under prompting variants: (1) no external literature + guideline/tournament, (2) with retrieved literature (top-10). Evaluation via AUROC on constructed test sets.",
            "novelty_measure": "LLM scoring output when asked to judge novelty using provided inputs and prompts.",
            "generation_performance": "Not reported here for generation tasks.",
            "validation_performance": "With literature search: AUROC ≈ 0.80 on NeurIPS (in-domain CS); degrades to ≈0.60 on Nature Medicine and cross-domain. Without literature, judgments are highly inaccurate (near-random).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Significant degradation on unfamiliar (biomedical) domain versus familiar (computer science) domain; performance heavily driven by internal model knowledge plus retrieved evidence.",
            "generation_validation_asymmetry": "Model can perform strong reasoning in-domain when augmented by retrieval, but its validation capability is brittle out-of-domain, evidencing generation/validation asymmetry under domain shift.",
            "out_of_distribution_performance": "Degraded OOD performance: drop from AUROC ~0.8 to ~0.6 in other domains.",
            "calibration_quality": "Not quantified; paper reports sensitivity and instability in LLM judgments generally.",
            "validation_computational_cost": "High if retrieval used; also multiple runs were averaged due to variability.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Augment with literature retrieval and possibly RND as reward signal in RL training to improve novelty generation/validation.",
            "evidence_type": "supports",
            "key_findings": "Sonnet-3.7 achieves high novelty-judgment AUROC in a familiar domain when provided retrieved papers, but its accuracy degrades substantially in other domains, demonstrating brittleness of LLM-as-judge.",
            "uuid": "e2163.6"
        },
        {
            "name_short": "Deepseek-r1",
            "name_full": "Deepseek-R1",
            "brief_description": "A reinforcement-learning-for-reasoning model (Guo et al. 2025) that aims to incentivize reasoning ability in LLMs using rule-based reward systems; mentioned as an improved reasoning model with potential for integrating novelty reward signals.",
            "citation_title": "Incentivizing reasoning capability in llms via reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "Deepseek-r1",
            "system_type": "reinforcement-learned reasoning model (LLM fine-tuned with RL)",
            "domain": "general reasoning for tasks including novelty generation",
            "generation_capability": "Generates reasoning chains and potentially research ideas; paper references it as a reasoning model rather than providing experimental generation results here.",
            "validation_method": "Not directly validated in this paper; authors suggest RND could be used as a reward mechanism for Deepseek-style RL training to encourage novelty.",
            "novelty_measure": "Not detailed in this paper; potential measure would be external reward signals (e.g., RND scores) during RL.",
            "generation_performance": "Not reported here; authors cite Deepseek-r1 as exhibiting improved reasoning capabilities in other work.",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Not applicable in-paper; suggested that adding domain-invariant novelty rewards could improve generation of novel ideas.",
            "generation_validation_asymmetry": "Paper posits that RND could reduce the generation-validation gap when used as an RL reward, but no empirical results are presented.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Proposed: use RND as a reward mechanism in RL training to align generation with validated novelty signals.",
            "evidence_type": "mixed",
            "key_findings": "Deepseek-r1 exemplifies recent RL-for-reasoning advances; the paper proposes using RND as a reward signal to guide such models toward more genuinely novel idea generation.",
            "uuid": "e2163.7"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (autoregressive LLM system card cited)",
            "brief_description": "An autoregressive LLM cited in the paper as an example of models whose outputs are sensitive to input perturbations and who show lower accuracy for novelty judgment compared to newer reasoning models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "GPT-4o",
            "system_type": "autoregressive large language model",
            "domain": "general text generation and reasoning; evaluated here as a novelty judge baseline",
            "generation_capability": "Generates text and research ideas; used here as a judge in comparative baselines.",
            "validation_method": "LLM-as-judge baseline; sensitive to prompt/phrasing and unstable; authors note GPT-4o achieved near-random AUROC historically in their tests (≈0.5) without robust retrieval/augmentation.",
            "novelty_measure": "LLM output judgments when prompted about novelty.",
            "generation_performance": "Not quantified here for idea generation.",
            "validation_performance": "Reported as low/inferior: GPT-4o produced outputs sensitive to perturbations and had poorer AUROC compared to newer reasoning models (authors mention GPT-4o around random baseline in some settings).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Highly sensitive to input rephrasing and lacks reliability for novelty judgment, especially without retrieval augmentation.",
            "generation_validation_asymmetry": "Shows asymmetry: fluent generation but unreliable validation judgments.",
            "out_of_distribution_performance": "Poor and unstable; not robust cross-domain.",
            "calibration_quality": "Described as poorly calibrated in terms of judgment stability (sensitive to perturbations); no numeric calibration reported.",
            "validation_computational_cost": "Lower than some retrieval-augmented complex pipelines but unreliable; repeated runs may be needed to reduce variance.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Retrieval augmentation and improved reasoning models recommended; RND suggested as a non-LLM validation mechanism.",
            "evidence_type": "supports",
            "key_findings": "Autoregressive LLMs like GPT-4o are sensitive to input perturbations and provide unreliable novelty judgments absent strong augmentation, reinforcing the fabrication-validation gap.",
            "uuid": "e2163.8"
        },
        {
            "name_short": "AI Scientist (framework)",
            "name_full": "The AI Scientist (AI Scientist framework)",
            "brief_description": "A concept and framework (Lu et al. 2024) integrating idea generation, evaluation, and refinement with LLMs and external tools (e.g., Semantic Scholar API) to automate research workflows; used as motivating prior work.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": "AI Scientist framework (Lu et al.)",
            "system_type": "multi-component automated research pipeline (LLM generation + retrieval + evaluation)",
            "domain": "automated scientific discovery / idea generation",
            "generation_capability": "Generates research ideas/hypotheses, iteratively refines them using retrieval and chain-of-thought prompting.",
            "validation_method": "In prior work, integrates review guidelines and semantic search (Semantic Scholar API) to help LLMs determine novelty; validation often relies on human assessments in prior literature and is described as a motivation for RND.",
            "novelty_measure": "LLM-informed judgments augmented by retrieved literature and review-guideline prompts; prior systems used human or LLM-based judgments.",
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Prior approaches are sensitive to LLM reliability and domain; the current paper argues these frameworks need better, scalable novelty validators like RND.",
            "generation_validation_asymmetry": "Frameworks integrate generation and validation but rely heavily on LLM judgment and human labels; the paper identifies a validation bottleneck and proposes RND to alleviate it.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Authors propose integrating RND into AI Scientist workflows to provide automated, domain-invariant novelty signals and reduce reliance on expert labeling.",
            "evidence_type": "mixed",
            "key_findings": "AI Scientist frameworks provide end-to-end automation potential but are constrained by brittle LLM judgment and lack scalable, domain-invariant novelty validators—RND is proposed as a solution.",
            "uuid": "e2163.9"
        },
        {
            "name_short": "Nova",
            "name_full": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM-Generated Ideas",
            "brief_description": "A prior system (Hu et al. 2024) that iteratively plans and searches to improve novelty/diversity of ideas generated by LLMs; cited as related work exploring LLMs in idea generation.",
            "citation_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
            "mention_or_use": "mention",
            "system_name": "Nova",
            "system_type": "LLM-based generation framework with planning/search",
            "domain": "LLM-generated research ideas and creativity optimization",
            "generation_capability": "Generates diverse and novel research ideas by iterative planning and search in idea space.",
            "validation_method": "Prior work used LLM-judges and/or human evaluation to rate novelty; not evaluated directly in this paper's experiments.",
            "novelty_measure": "Diversity- and novelty-promoting planning objectives internal to the generation pipeline; external validation was done in prior work via human labels/LLM judges.",
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": "Prior systems emphasize generation-side mechanisms; this paper highlights the need for scalable and robust validators like RND.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Could benefit from integrating RND for automatic novelty scoring during iterative generation.",
            "evidence_type": "neutral",
            "key_findings": "Generation-focused systems (e.g., Nova) improve idea generation but rely on human/LLM judgments for validation; RND could supply scalable validation to complement such generators.",
            "uuid": "e2163.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "rating": 2
        },
        {
            "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
            "rating": 2
        },
        {
            "paper_title": "Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation",
            "rating": 2
        },
        {
            "paper_title": "Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2
        }
    ],
    "cost": 0.020566249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025</p>
<p>Yao Wang wang-yao24@mails.tsinghua.edu.cn 
Department of Automation
Tsinghua University</p>
<p>Mingxuan Cui mingxuan.cui@mail.nankai.edu.cn 
Nankai University</p>
<p>Arthur Jiang arthursjiang@gmail.com 
Yidu Technology 
ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025014BE4AA0DE10575620E8518F64631CFarXiv:2503.01508v2[cs.AI]
In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery.This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea's local density with its adjacent neighbors' densities.We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment.Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820)and biomedical research (AUROC=0.765)domains.Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s.0.597) on cross-domain evaluation.These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
<p>Introduction</p>
<p>In the pursuit of Artificial General Intelligence (AGI), automating scientific research and knowledge discovery presents both a formidable challenge and an exciting opportunity, as it will be groundbreaking to expand the boundaries of human knowledge by leveraging scalable computing resources.Therefore, as the capabilities of large language models (LLMs) continue to improve, researchers have started to explore their use in automating various aspects of the research process, including the generation of novel ideas, as exemplified by the AI scientist concept (Lu et al. [2024]).</p>
<p>A key task for any AI-based scientist is the generation of novel research ideas, a task traditionally performed by human scientists during their brainstorming phase.While LLMs have shown promise in generating a large pool of ideas quickly and cost-effectively, akin to the initial stages of human research, the real challenge lies in evaluating these ideas for their novelty.Traditionally, novelty in scientific research has been assessed through peer review and expert evaluations, where domain specialists judge the originality of an idea based on their experience and familiarity with existing literature.However, such assessments are inherently subjective, time-consuming, and inconsistent across reviewers.Moreover, as the volume of scientific output grows exponentially, manual novelty assessment struggles to keep pace.Automated methods are therefore crucial for filtering out redundant ideas and promoting genuinely innovative directions.</p>
<p>Existing approaches primarily fall into two categories: (1) leveraging large language models (LLMs) as judges and (2) using absolute local density-based novelty metrics.</p>
<p>The most straightforward approach is to use LLMs as judges to evaluate the novelty of ideas.Si. et al. adopted a Swiss system tournament design to evaluate ideas by using LLM as judge (Si et al. [2024]), which was further applied in Nova: An Iterative Planning and Search Approach to enhance Novelty and Diversity of LLM-Generated Ideas (Hu et al. [2024]).To improve LLM's accuracy of judgment, Lu include NeurIPS review guideline and Semantic Scholar API as tools (Lu et al. [2024], Su et al. [2024]): the NeurIPS review guideline was served as both chain-of-thoughts prompts and few-shot examples, while search API enabled LLM to search top 10 relevant papers to determine the novelty of given idea.</p>
<p>An alternative approach relies on the absolute local density in semantic embedding space to measure novelty.Su. et al. introduced the concept of Historical Dissimilarity and Contemporary Dissimilarity, calculated as the average Euclidean distance (local density) between a generated abstract's embedding and the embeddings of the five most similar abstracts from a historical database and a contemporary database, respectively.In combination with the citation-based Contemporary Impact metric, they developed the Overall Novelty (ON) metric (Su et al. [2024]).Their study validated that ON correlates well with human-labeled novelty, demonstrating its effectiveness as a novelty assessment measure.</p>
<p>Though the aforementioned approaches provide potential solutions on idea novelty evaluation, there are major challenges when considering practical issues.</p>
<p>First, the reliability of using-LLM-as-judge remains questionable, even with external knowledge or tools.Studies have demonstrated that auto-regressive LLMs like GPT-4o (Hurst et al. [2024]) produce outputs sensitive to input perturbations (Zhuo et al. [2023], Singh et al. [2024]).In novelty assessment specifically, this means identical research ideas phrased differently might receive contradictory novelty ratings.While recent reasoning models, such as DeepSeek-r1 (Guo et al. [2025]) and Sonnet-3.7 (Anthropic [2025]), show improved reasoning capabilities, their reliability for scientific novelty judgment remains unvalidated.</p>
<p>Second, the absolute local density-based metric from (Su et al. [2024]) shows significant limitations across diverse research contexts.By relying on just 5 most similar abstracts from history and contemporary databases as reference points, the metric's validity becomes highly dependent on arbitrary choices: the size of paper collections, the temporal boundaries defining 'history' versus 'contemporary,' and the selection criteria for inclusion.Different research domains also exhibit varying citation patterns, publication velocities, and semantic densities, which undermined the metric's generalizability across research domains.</p>
<p>Last but not least, the validation methodology used to assess novelty evaluators themselves was significantly lacking.In most cases, the validation of novelty metric depends on small test sets manually labeled by human experts within a specific domain (Hu et al. [2024], Lu et al. [2024], Su et al. [2024], Si et al. [2024]).Such validations are difficult to scale across different research areas, as they are often highly specialized and tailored to particular fields of study.What's worse, manually produced novelty labels rapidly become outdated: as scientific research advances continuously, ideas labeled "novel" today quickly become established knowledge, rendering static human-labeled validation sets increasingly inaccurate over time.</p>
<p>To address these challenges, in this paper, we establish comprehensive semantic embedding databases for novelty assessment.These databases incorporate over 30 million publications from two distinct domains: Pubmed, the leading biomedical literature search engine with nearly 36 million articles (Jin et al. [2024]), and Arxiv, which contains more than 2.3 million scholarly articles across eight subject areas (Cornell Tech [2023]).</p>
<p>Based on these resources, we propose the Relative Neighbor Density (RND) algorithm, which measures novelty by analyzing the distribution patterns of semantic neighbors rather than simple absolute local density.This approach proves more reliable than LLM-based judgments and more generalizable than existing absolute local density-based metrics across different research domains.We also develop an automated validation methodology that leverages temporal publication patterns to evaluate novelty without requiring expert manual labeling.Our extensive evaluations using test sets from computer science, biomedical science, and cross-domain contexts demonstrate that our proposed algorithm maintains accuracy within specific domains while scaling effectively across diverse research areas.</p>
<p>Our main contributions are:</p>
<p>• A novel neighbor density-based Relative Neighbor Density (RND) algorithm for assessing research idea novelty that is robust across domains, which holds domain-invariant property</p>
<p>Related Works</p>
<p>Assessing the novelty of research ideas is a fundamental challenge in automating scientific discovery.Various approaches have been explored in recent years, leveraging Large Language Models (LLMs) and semantic similarity measures to evaluate idea originality.This section reviews existing methods, highlighting their strengths and limitations.</p>
<p>LLMs for Novelty Assessment</p>
<p>Recent work has demonstrated promising results in using LLMs as autonomous judges for research novelty.Si et al. (Si et al. [2024]) evaluated this approach using ICLR submissions, converting them into standardized project proposals and conducting pairwise comparisons between accepted and rejected papers(Table 7).Their Swiss tournament system iteratively paired proposals based on accumulated scores, with Claude-3.5-Sonnetachieving 71.4% accuracy in predicting paper acceptance.As a control measure, they included human expert reranking, which revealed notable discrepancies between automated and human judgments.</p>
<p>Lu et al. (Lu et al. [2024]) expanded this concept with their AI Scientist framework, integrating idea generation, evaluation, and refinement.Their system employs chain-of-thought prompting and external knowledge retrieval via Semantic Scholar API to enhance assessment quality.While showing promise in matching human-level performance, these LLM-based approaches face fundamental challenges in reliability and consistency, as highlighted by studies showing their sensitivity to input variations (Zhuo et al. [2023], Singh et al. [2024]).</p>
<p>Absolute Local Density-based Metrics</p>
<p>An alternative approach focuses on semantic local density to evaluate novelty.Su et al. (Su et al. [2024]) used the Historical Dissimilarity (HD), which is the average Euclidean distance between the generated abstract embedding and embeddings of the 5 most similar abstracts in historical literature base.We denote it as "Absolute Local Density" because the average distance is a metric of local density, and they use the value of density directly.In addition to HD, they also used Contemporary Dissimilarity (CD), which calculate in same algorithm in contemporary literature base, which is also an absolute local density-based metric.</p>
<p>Based on HD and CD, they developed Overall Novelty (ON) metric as below,
ON = HD × CI CD (1)
The main challenge of local density-based metric is that the density values vary across different domains.In case of evaluating ideas from mixed research domains, the variance would cause a severe degrade in the accuracy.</p>
<p>Validating Novelty Metrics</p>
<p>A critical limitation in existing research is the lack of scalable validation methodologies.Current approaches (Lu et al. [2024], Su et al. [2024], Hu et al. [2024], Si et al. [2024]) typically rely on small-size literature database and manually labeled test set created by domain experts.For instance, Su et al. (Su et al. [2024]) constructed an "ecosystem" for computer science (CS) using information extracted from 85,217 papers-a dataset that represents only a small fraction of the CS literature available on platforms like arXiv.While their analysis demonstrated promising correlations between novelty scores and human labels across 100 manually evaluated abstracts, the methodology's reliance on domain-specific expertise significantly constrains its generalizability, which is echoed by reviewer's comments that only validating CS is "relatively simple" (Openreview Reviewer 2X9t [2025]).</p>
<p>Furthermore, the scalability challenge persists across all existing frameworks.The creation of test sets currently requires expert labeling, making large-scale evaluation prohibitively resource-intensive.This limitation underscores the need for automated approaches to test set creation that maintain fidelity while eliminating the requirement for extensive manual labeling.</p>
<p>Method</p>
<p>Problem Description</p>
<p>Given a set of ideas I,
I = {idea i }, i ∈ <a href="2">1, N </a>
Where idea i is a sequence of words or characters in nature language.N ≥ 1 represents the number of ideas whose novelty needs to be assessed.</p>
<p>The objective is to design a mapping F from idea space to a score in real value space
F (idea i ) = score i , where idea i ∈ I, score i ∈ R (3)
The novelty score score should be monotonic, meaning that for any two ideas idea i and idea j , if idea i is more novel than idea j , then their corresponding scores must satisfy:
∀ idea i , idea j ∈ I, idea i ≻ idea j ⇒ F (idea i ) &gt; F (idea j )(4)
where idea i ≻ idea j denotes that idea i is considered more novel than idea j based on a given novelty criterion.</p>
<p>Semantic Embedding &amp; Literature Database</p>
<p>Each published literature's abstract, which is also a sequence of words or characters in natural language, is denoted as
a j .
The semantic embedding model is a mapping function G , which maps ideas and abstracts into embedding vectors:
G (idea i ) = v i , where v i ∈ R dims ,(5)G (a j ) = v j , where v j ∈ R dims (6)
Thus, the preprocessed literature semantic database is represented as a set A:
A = {(a j , v j ) | j ∈ [1, M ]}(7)
We collected 36 million academic articles from the PubMed Download API (National Library of Medicine [2025]) and 2.6 million papers from the ArXiv dataset (Cornell University [2025]).Among all fetched documents, only those with both a non-empty title and abstract were considered valid for the experiment, resulting in 25,360,114 papers from PubMed and 2,643,057 papers from ArXiv.</p>
<p>For each paper, two semantic embedding vectors were generated-one from its title and another from its abstract-using the M3-Embedding model Chen et al. [2024].The embedding vector dimension, denoted as dims, is 1024.All texts and embedding vectors were stored in Elasticsearch Version 8 for efficient retrieval.</p>
<p>Algorithm</p>
<p>For each idea idea i and its embedding v i , we first find its P nearest neighbors using k-Nearest Neighbors (KNN) search:
{v 1 , v 2 , . . . , v P } = KN N (v i , P, A) (8) where v j is the j-th nearest neighbor of v i , j ∈ [1, P ].
For the idea itself v i and each neighbor v j , the neighbor density (ND) is defined as below
N D = 1 Q Q k=1 d(v, v k )(9)
where v k denotes the k-th nearest article's embedding vector in the literature corpus A and d(•, •) is the cosine distance between two vectors.</p>
<p>We define the set S i that contains the neighbor density values of idea i 's neighbors:
S i = {N D j | j ∈ [1, P ]}(10)
Finally, we compute the novelty score score i for idea i as:
score i = |{N D ∈ S i | N D ≤ N D i }| |S i | × 100 (11)
The selection of values of P and Q is a trade-off between reliability of estimation and other cost.A lower values of P will cause biased estimation of novelty score; while higher value of P will increase computing cost in O(P • Q) complexity.Similarly, lower Q will also cause unreliable estimation of local density.Meanwhile, a higher Q value will not only be computational costly, but also sacrifice sensitivity as ND converges to its expectation as Q increase, losing information local density.Refer to Appendix A for detailed analysis on the effects of P and Q.</p>
<p>Based on analysis and empirical experiments, we set P = 100 and Q = 50.</p>
<p>The pseudoscope implement calculation of ND is presented in Algorithm 1; and the pseudoscope of complete algorithm is provided in Algorithm 2.</p>
<p>Algorithm</p>
<p>Validation without Human Labeling</p>
<p>As a novelty evaluation algorithm, the most challenging point in past research is to find a reliable labeled test set to evaluate the algorithm.Therefore, we propose a new method to construct a convincing test set instead of relying on human experts to annotate it.</p>
<p>For the positive samples (a.k.a novel ideas) in the test set, we select recent articles from top journals or conferences.For the negative samples (a.k.a.non-novel ideas), highly cited articles published before the last few years were selected, also from the research domain's top journals or conferences.The fundamental principles behind such methodology were: high-quality novel ideas are more likely to be published in recent issues and top journals or conferences; while after time passes, the at-the-time novel ideas were more likely to attract attention and related works, thus become non-innovative at present.</p>
<p>In this way, we can make positive and negative samples have a more obvious difference in novelty in a relatively objective and recognized way.We have two test sets: NeurIPS, which represents the most advanced research results in the field of computer science, and Nature Medicine, which represents the most cutting-edge papers in the medical field.The sample year distribution of the test sets can be found in Table 1 NeurIPS test set: The initial corpus consists of papers that are Accept (oral) or Accept (spotlight) by Program Chairs at the 2024 NeurIPS conference, which represents the latest research results in computer science.Furthermore, we select articles from the initial corpus that explicitly mention that the papers have obvious novelty in the comments of Program Chairs to form the positive samples of the NeurIPS test set.The comments and decision information of Program Chairs can be obtained on the OpenReview.netwebsite.At the same time, we use the Semantic Scholar API to obtain the 99 most cited papers published in the NeurIPS conference from 2015 to 2020 to form the negative samples of the test set.The titles of all samples are presented in Table 4 Nature Medicine test set: The positive samples of the Nature Medicine test set consist of articles classified as "Article" type, published in Nature Medicine from August 2024 to February 2025, according to the classification on the nature.comwebsite.Articles related to phase 2 or phase 3 trials were excluded.And we used the same method as the negative samples of the NeurIPS test set to obtain 99 articles of Nature Medicine with the highest citation count in the past 15 years as negative samples of the test set.The titles of all samples are presented in</p>
<p>Baseline</p>
<p>To evaluate our algorithm, we selected all existing novelty assessment algorithms as baselines, categorized into two groups: LLM-based and non-LLM-based.Non-LLM-based algorithms, including Relative Neighbor Density(Ours), Historical Dissimilarity(HD), and Overall Novelty(ON), rely solely on literature search and mathematical calculations.Since the output of the literature search for the same query remains consistent, we conducted a single test to assess the algorithm's performance.In contrast, for LLM-based algorithms, due to the inherent variability of LLM outputs, we ran three tests for each algorithm, calculated the average result, and included the standard deviation in the table.The full experimental results of the LLM-based method are provided in Table D.1.</p>
<p>For all methods, we use the abstracts of the papers in the test set as "ideas" for testing.</p>
<p>Historical Dissimilarity: Identify the five most relevant papers based on their embeddings and compute the Euclidean distance between the embedding of the idea and the embeddings of the abstracts of these five papers.The final novelty score is obtained by averaging these distance values.</p>
<p>Overall Novelty: The historical database contains papers from 2011 to 2021, and the contemporary database contains papers from 2021 to 2025.The score calculation method refers to equation 1.</p>
<p>LLM + literature search: Provide LLM with the titles and abstracts of the 10 most relevant papers to the given idea.The model then assesses whether the core concepts of these papers significantly overlap with the idea(Table 8).If substantial overlap is detected, the idea is deemed non-novel and assigned a score of 0. If no significant overlap is found, the idea is considered novel and assigned a score of 1.</p>
<p>LLM with guideline: Utilize the NeurIPS 2024 review guidelines to assist LLM in evaluating the novelty of ideas(Table 6).The final score is determined based on the "Overall" score provided in the review assessment.</p>
<p>LLM with tournament: First, the idea is transformed into the Standardized Project Proposal format(Table 7).Next, the novelty of all standardized ideas is assessed using the Swiss tournament method, where ideas are iteratively compared in a structured competition.Finally, each idea is assigned a score based on the number of wins it accumulates throughout the tournament.</p>
<p>Accuracy Evaluation</p>
<p>As shown in Table 2, our enhanced neighbor density-based novelty measurement algorithm outperforms all baseline models on both the Nature Medicine and Mixed test sets, while also demonstrating strong performance on the NeurIPS test set.2: Validation of Different methods, measured by AUROC.HD: Historical Dissimilarity (section 2.2).ON: Overall Novelty (section 2.2).LLM + literature search: supplementing LLM with 10 relevant papers, which were searched by idea's embedding from our literature database using semantic embedding.LLM with guideline: using NeurIPS 2024 review guideline to help LLM judge the novelty of ideas, which is not applicable to Nature Medicine.Therefore, the results of Nature Medicine and Mixed are marked as not applicable.LLM with tournament: a Swiss system tournament design to evaluate ideas by using LLM as judge.</p>
<p>Model</p>
<p>By comparing the results of various LLM-related algorithms, we observe a key similarity between Sonnet-3.7 with guideline and Sonnet-3.7 with tournament: both methods provide very limited external knowledge to the LLM, with no existing literature being fed into the model.As a result, the model's judgment of novelty is highly inaccurate.In contrast, the LLM + literature search method inputs the 10 most relevant papers to the idea, significantly improving the accuracy of the model's judgment.Moreover, the accuracy of the LLM + literature search method is much higher in the field of computer science compared to the field of biomedicine, highlighting the significant impact of the model's internal knowledge on the judgment outcomes, even with the addition of external knowledge.Additionally, Sonnet-3.7 (Anthropic [2025]) and Deepseek-r1 (Guo et al. [2025]) show much higher AUROC scores than GPT-4o, indicating that when external knowledge is provided, the performance of the inference model greatly surpasses that of the autoregressive model.However, we observed that the Historical Dissimilarity (HD) metric closely matched the performance of our proposed method on the Nature Medicine and NeurIPS test set.In contrast, on the Mixed test set, there was a significant disparity, with our method achieving an AUROC of 0.795, while HD only reached 0.362.This prompted us to further investigate the underlying reasons for this substantial difference.</p>
<p>The score distributions provided by the Historical Dissimilarity (HD) metric on the NeurIPS and Nature Medicine test set, as shown in Figure 1, are markedly different.This disparity implies that some negative samples from NeurIPS would be evaluated as more novel than some positive samples from Nature Medicine under this evaluation system, highlighting HD's limited generalization ability across domains.In contrast, the score distributions of our method on both test sets are nearly identical, indicating that our scores are absolute and unaffected by the specific discipline or field.This means that our scores are universally comparable across domains.This result underscores the robust cross-domain evaluation capability of our method, making it applicable for researchers in any field.</p>
<p>Sensitivity Study</p>
<p>Sensitivity of Hyper-Parameter</p>
<p>Since our method has two key parameters: P and Q, we conducted experiments to understand the contribution of each parameter to our algorithm.As illustrated in the left panel of Figure 2, the AUROC on each test set increases as P grows.However, when P &gt; 50, the improvement in AUROC becomes marginal compared to the significant gain observed when increasing P from 10 to 50.This suggests that the marginal benefit of further increasing P diminishes while simultaneously incurring substantial computational costs, given that the algorithm's time complexity is O(P • Q).Additionally, the poor performance observed when P = 10 can be attributed to the biased estimation of novelty scores when P is too small, a phenomenon influenced by multiple factors.For a more detailed explanation, please refer to Appendix A.1.</p>
<p>The right panel of Figure 2 demonstrates that when P remains constant, both excessively small and large values of Q negatively impact the algorithm's performance.This is due to the inaccuracy in local density estimation when Q is too small and the significant reduction in algorithm sensitivity when Q is too large.For further details, please refer to Appendix A.</p>
<p>Sensitivity of Design</p>
<p>In our Relative Neighbor Density algorithm, the notion "relative", i.e. comparing idea's local density with its neighbor's local densities, plays an important role.Moreover, other distance metric, such as Euclidean distance, could also be used in our algorithm.To understand the sensitivity of the current design, we conducted experiments by changing the design of the "relative" notion, and distance metric.The result presented in Table 3</p>
<p>Case Study</p>
<p>We visualize the neighbors of both a novel and a non-novel idea in the embedding vector space to demonstrate the superiority of our algorithm.Figures 3 and 4 show the visualization results of the embedding vectors of an idea and its neighbors on a two-dimensional plane, after dimensionality reduction using t-Distributed Stochastic Neighbor Embedding (t-SNE).While t-SNE excels at preserving the local structure of the data, it does not reliably retain the global structure(Van der Maaten and Hinton [2008]).As a result, the distance between the idea and its P adjacent neighbors is not accurately preserved, but distances between these P neighbors and their Q nearest neighbors is well preserved.</p>
<p>We first use Attention is All You Need (Vaswani et al. [2017]), a highly cited article, as a non-novel idea from the current perspective.Figure 3 clearly illustrates that there is a dense cluster of neighbors around the idea.In contrast, the neighbors around the idea's P neighbors are relatively few and sparse.</p>
<p>Next, we use Evaluating the World Model Implicit in a Generative Model (Vafa et al. [2025]), an article considered highly novel by the NeurIPS 2024 Program Chairs, as an example of a novel idea, based on their comments (openreview [2025]).In Figure 4, it is evident that the idea's local neighbor density is much sparser than its P nearest neighbors.</p>
<p>The experimental results demonstrate that the novelty of an idea is reflected in the local structure of the most similar documents to the idea within the embedding vector space, which supports the correctness of our algorithm in principle.Furthermore, a key difference between Figures 3 and 4 is that Figure 3 shows multiple neighboring clusters centered around the idea's P nearest neighbors, suggesting that the vector density of the two images in the embedding space is notably different.This highlights that the novelty of an idea cannot be determined solely by local density of the idea but must also take into account the vector density surrounding the idea.This is also clearly reflected in the experimental results for the Mixed test set in Table 3.</p>
<p>Discussion</p>
<p>In this work, we proposed a novel neighbor density-based metric for assessing research idea novelty, addressing the limitations of LLM judgment and absolute local density-based metrics.By leveraging large-scale literature embeddings from both biomedical sciences and computer science, our approach ensures robust reliability and cross-domain generalizability.Additionally, we introduced a scalable validation framework that eliminates reliance on expert labeling, enabling objective and reproducible novelty assessment.</p>
<p>Why a Non-LLM Novelty Assessment Algorithm is Necessary?</p>
<p>Assessing the novelty of a research idea is inherently difficult, subjective, and resource-intensive.While LLMs have the potential to assist in this process, their effectiveness is limited by the challenges outlined in the Introduction.Our experiments (see Table 2) highlight these issues: without an integrated search tool, even the most advanced reasoning models' performance was comparable to random guessing (AUROC =0.5).When a search tool was introduced, Sonnet-3.7 achieved similar accuracy on the NeurIPS test set (AUROC =0.8) but experienced significant degradation (AUROC =0.6) on both the Nature Medicine and cross-domain test sets.In contrast, our proposed RND algorithm can produce more reliable and consistent results, as seen in Table 2. Our algorithm is better at distinguishing genuinely novel ideas from the large pool of candidates from mixing research domains (AUROC =0.78 v.s Other's AUROC&lt;=0.6).Such cross-domain novelty assessment capability is crucial to AI scientist, as more and more innovation happened in inter-discipline of research domains.</p>
<p>Recent advancements in reinforcement learning for reasoning models, such as those demonstrated in Deepseek-R1 (Guo et al. [2025]), suggest the potential of rule-based reward systems to guide model development.Our RND algorithm could serve as a sophisticated rewarding mechanism, potentially enhancing reasoning model's capabilities in generating novel scientific ideas and advancing the role of AI in scientific innovation.</p>
<p>Accuracy in Each Domain</p>
<p>In</p>
<p>Domain-invariant Accuracy</p>
<p>However, when tested in the cross-domain test set (the Mixed test set), which includes ideas from both computer science and biomedicine, the performance of HD significantly degraded, with its AUROC dropping to 0.362.In contrast, the AUROC of our proposed algorithm remained robust at 0.795, similar to its performance in the single-domain test sets.</p>
<p>As demonstrated in the Table 3, the relative position of idea's local density among all of its neighbor's local density is crucial for comparing novelty across different domains.</p>
<p>We argue that RND algorithm hold domain-invariant property, i.e. the distribution of novelty scores produced by RND is identical regardless of the tested domain, which explained why our relative density-based approach succeeds in cross-domain scenarios.According to the mathematical reasoning in Appendix A.3, we concluded the distribution of novelty score S is only subject to P (the number of neighbors considered); thus it is invariant to the validation domain.Furthermore, in Figure 1 (right panel) and 5, the actual distribution of scores echoed the theoretical analysis.Such domain-invariant property is crucial for conducting multi-disciplinary scientific research, where ideas from diverse fields must be compared and evaluated effectively.</p>
<p>Why Validation Methods Differ Between Novel and Non-Novel?</p>
<p>When building our test set, an obvious approach might be to use symmetrical sources -for example, using accepted NeurIPS papers as novel samples and rejected NeurIPS papers (specifically those rejected for lacking novelty) as non-novel samples.However, this approach presents significant limitations.Firstly, very few top-tier venues publicly release review comments with explicit novelty assessments, making such data scarce and difficult to generalize across domains.Secondly, papers may be rejected for "lack of novelty" due to incremental advances or methodological similarities, even when addressing previously unexplored topics.</p>
<p>Instead, our definition of novelty relies on how extensively similar ideas have been studied in the literature.Following this definition, we selected highly-cited papers from recent years as our non-novel samples, as these papers represent ideas that have been thoroughly explored and extended by numerous subsequent works.While high citation count itself can indicate either novelty or utility, papers that are both recent and highly-cited typically represent research areas that have quickly become crowded with similar work, making the original contributions less novel by our working definition.Further details on our sampling methodology can be found in Section 4.1.</p>
<p>Limitations &amp; Future Work</p>
<p>Several limitations of our work warrant further exploration.</p>
<p>First, the algorithm relies heavily on large-scale literature databases with semantic embeddings.Biases in the literature database could potentially influence novelty assessments, especially if certain areas of research are underrepresented or if publication biases exist within fields.</p>
<p>Second, the algorithm's performance is also dependent on the quality of semantic embeddings for representing complex scientific concepts.While the M3 model demonstrated effectiveness, domain-specific fine-tuning could potentially improve performance.Future work should investigate specialized embedding models for scientific literature that better capture the complex semantics of scientific abstracts, particularly for technical terminology and methodological nuances.</p>
<p>Third, our validation methodology, while avoiding the need for expert labeling, relied on non-novel samples that may be too easily distinguishable from novel ones.By using historical highly-cited papers as non-novel examples, rather than borderline cases such as recently rejected papers or incremental work from current journals, we created a simplified assessment scenario compared to the subtle distinctions scientists face in real research settings.However, the fact that none of the tested algorithms achieved saturated AUROCs even in this relatively straightforward scenario demonstrates the fundamental challenge of novelty assessment and validates our comparative analysis.</p>
<p>Looking ahead, we envision several promising directions for future work:</p>
<ol>
<li>
<p>Integration with AI Research Workflows: Incorporating our novelty evaluation algorithm into end-to-end AI scientist workflows would enable autonomous research ideation and evaluation.This integration would allow AI systems to independently generate research hypotheses, assess their novelty using our domain-invariant RND algorithm, and prioritize the most promising directions for further investigation.Such integration could accelerate scientific discovery by efficiently navigating complex multi-disciplinary research landscapes where human intuition about novelty is often limited.</p>
</li>
<li>
<p>Enhancing Reasoning Model: As highlighted in our discussion, current reasoning models struggle with reliable novelty assessment across domains.We propose utilizing our RND algorithm as a sophisticated reward mechanism within reinforcement learning frameworks for training reasoning models for AI research.By providing domain-invariant novelty signals during training, we could potentially guide models to generate more innovative scientific ideas while maintaining scientific validity.</p>
</li>
</ol>
<p>These advancements would further enhance AI's role in scientific research by accelerating idea generation, refining research hypotheses, and potentially uncovering interdisciplinary connections that might otherwise remain unexplored.</p>
<p>Appendix A Algorithm Analysis</p>
<p>A.1 Effects of Parameter P</p>
<p>The novelty score is computed as
score i = |{N D ∈ S i | N D ≤ N D i }| P × 100,(12)
where S i = {N D j | j = 1, 2, . . ., P } is the set of neighbor densities for the P nearest neighbors of the idea i, and N D i is the neighbor density for idea i itself.</p>
<p>Empirical Cumulative Distribution Function (ECDF) Interpretation</p>
<p>Define the empirical cumulative distribution function (ECDF) for the set S i as
F P (x) = 1 P P j=1 1 {N Dj ≤x} ,
where 1 {N Dj ≤x} is the indicator function that is 1 if N D j ≤ x and 0 otherwise.Then, by definition, the novelty score can be written as score i = 100
• F P (N D i ). (13)</p>
<p>Consistency of the ECDF</p>
<p>Let F (x) be the true cumulative distribution function of the neighbor densities (assumed to be i.i.d.samples from a distribution F ).By the Glivenko-Cantelli theorem, the ECDF F P (x) converges uniformly to F (x) as P → ∞:
sup x |F P (x) − F (x)| P →∞ − −−− → 0.
Thus, for a sufficiently large P , we have
F P (N D i ) ≈ F (N D i ).
(14) This shows that the score, being proportional to F P (N D i ), converges to 100 • F (N D i ), which is the true quantile of N D i in the distribution of neighbor densities.</p>
<p>Variance and Sensitivity with Finite P</p>
<p>For a finite sample size P , F P (N D i ) is a random variable whose variance depends on P .Under the assumption of i.i.d.sampling,
Var(F P (N D i )) = F (N D i )(1 − F (N D i )) P .
Thus, the standard deviation is proportional to 1 √ P .This quantifies that:</p>
<p>• Smaller P : The variance Var(F P (N D i )) is larger, leading to a noisier (less reliable) estimation of the quantile, and hence of the novelty score.• Larger P : The variance decreases, yielding a more accurate estimation of the true quantile F (N D i ).</p>
<p>The novelty score becomes less sensitive to random fluctuations when P is large, as the empirical quantile is a better estimator of the true quantile.</p>
<ol>
<li>Discreteness of the Score for Small P When P is small, the possible values of F P (N D i ) are discrete, specifically:
F P (N D i ) ∈ 0, 1 P , 2 P , . . . , 1 .
For instance, if P = 1, then F 1 (N D i ) can only be 0 or 1, corresponding to a score of either 0% or 100%.This coarse granularity can result in a biased or uninformative measure of novelty.As P increases, the steps 1 P become finer, allowing the score to capture more subtle differences in the density distribution.</li>
</ol>
<p>Conclusion</p>
<p>The parameter P affects the final novelty score in two major ways:</p>
<ol>
<li>Accuracy: As P increases, the empirical cumulative distribution F P (x) better approximates the true cumulative distribution F (x), leading to a more accurate quantile estimate F (N D i ). 2. Variance: The variance of the estimate F P (N D i ) is proportional to 1 P .Thus, a larger P reduces the variability of the score, making it less sensitive to random noise.In summary, a higher P leads to a more robust and sensitive measure of novelty, while a smaller P results in a discrete and noisier estimate.</li>
</ol>
<p>A.2 Effects of Parameter Q</p>
<p>The neighbor density (ND) is given by:
N D = 1 Q Q k=1 d k ,(15)
where
d k = d(v, v k )
represents the distance between the point v and its k-th nearest neighbor.</p>
<p>Assuming that d k are independent and identically distributed (i.i.d.) random variables with mean E[d k ] = µ d , we compute the expectation of ND:
E[N D] = E 1 Q Q k=1 d k = 1 Q Q k=1 E[d k ] = 1 Q Qµ d = µ d .(16)
The variance of ND is given by:
Var(N D) = Var 1 Q Q k=1 d k . (17)
Using the property that the variance of the mean of Q i.i.d.random variables is:
Var 1 Q Q k=1 d k = 1 Q 2 Q k=1 Var(d k ).(18)
Since each d k has variance σ 2 d , we obtain:
Var(N D) = Qσ 2 d Q 2 = σ 2 d Q .(19)</p>
<p>A.2.1 Interpretation of Variance Scaling</p>
<p>The derived formula:
Var(N D) = σ 2 d Q (20)
shows that:</p>
<p>• As Q increases, the variance of ND decreases.</p>
<p>• Specifically, variance scales inversely with Q, meaning that larger Q results in a more stable estimate of ND.</p>
<p>• When Q → ∞, Var(N D) → 0, indicating that ND converges to its expected value µ d , which would cause lost of information on local density.• For small Q, ND exhibits higher variability, making it more sensitive to local fluctuations.</p>
<p>A.3 Domain-Invariant</p>
<p>A.3.1 Theoretical Analysis</p>
<p>Consider a test set in which each idea is assigned a neighborhood density defined as
N D = 1 Q Q i=1 d(idea, a i ),(21)
where a i denotes the ith nearest article in the literature corpus and d(•, •) is the cosine distance.</p>
<p>Let F (x) be the cumulative distribution function (CDF) of the neighborhood densities in the literature corpus.The percentile score for an idea is then defined by S = F (N D).</p>
<p>(22) By the probability integral transform, if N D is drawn from a distribution with CDF F (x), then S ∼ U(0, 1).</p>
<p>(
)23
In practice, F (x) is estimated empirically using P articles from the neighborhood.The empirical CDF is given by
FP (x) = 1 P P j=1 1{N D j ≤ x},(24)
where N D j is the neighborhood density of the jth article, and 1{•} is the indicator function.Since
P j=1 1{N D j ≤ x} ∼ Binomial(P, F (x)),(25)
we have
E[ FP (x)] = F (x) and Var[ FP (x)] = F (x)(1 − F (x)) P .(26)
Now, consider two literature corpora: a medical corpus with density distribution F M (x) and a computer science corpus with density distribution F C (x).For an idea in the test set, define its scores as
ŜM = FM (N D M ) and ŜC = FC (N D C ),(27)
where N D M and N D C are the neighborhood densities computed using the respective corpora.According to equation 26, we have
E[ Ŝ] = F (N D) and Var[ Ŝ] = F (N D)(1 − F (N D)) P . (28)
where F (N D) ∼ U(0, 1), which implies
ŜM d = ŜC .(29)
Furthermore, note that the variance of the empirical estimate FP (x) is solely a function of P :
Var[ FP (x)] = F (x)(1 − F (x)) P .(30)
Thus, when P changes (e.g., P = 50, 100, or 500), the change in variance-and hence the fluctuation in the score-is proportional to 1 P and is independent of the corpus.In other words,
∆ Var ∝ 1 P ,(31)
which holds for both the medical and the computer science datasets.</p>
<p>Therefore, we conclude that:
ŜM d = ŜC and ∆ Ŝ ∝ 1 P . (32)
This establishes that the scoring distributions for the test set are identical across corporas, and the effect of changing P on the score variation is equivalent for all datasets.Task description: You are a researcher who is reviewing a paper that was submitted to a computer science venue.Be critical and cautious in your decision.If a paper is bad or you are unsure, give it bad scores and reject it.Below is a description of the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.Reviewer guidelines: 1. Summary: Briefly summarize the paper and its contributions.This is not the place to critique the paper; the authors should generally agree with a well-written summary.2. Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: -Originality: Are the tasks or methods new?Is the work a novel combination of well-known techniques?(This can be valuable!)Is it clear how this work differs from previous contributions?-Quality: Is the submission technically sound?Are claims well-supported (e.g., by theoretical analysis or experimental results)?Are the methods used appropriately?Is this a complete piece of work or a work in progress?Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?-Clarity: Is the submission clearly written?Is it well organized?(If not, please make constructive suggestions for improving its clarity.)Does it adequately inform the reader?(Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)-Significance: Are the results important?Are others (researchers or practitioners) likely to use the ideas or build on them?Does the submission address a difficult task in a better way than previous work?Does it advance the state of the art in a demonstrable way?Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?3. Questions: Please list and carefully describe any questions and suggestions for the authors.Think of the things where a response from the author can change your opinion, clarify confusion, or address a limitation.This can be very important for a productive rebuttal and discussion phase with the authors.4. Ethical concerns: If there are ethical issues with this paper, please flag the paper for an ethics review.5. Overall: Please provide an "overall score" for this submission.Choices: -10: Award quality: Technically flawless paper with groundbreaking impact on one or more areas, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations.</p>
<p>A.3.2 Experimental Evidence</p>
<p>-9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area and excellent impact on multiple areas, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area or high-toexcellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-7: Accept: Technically solid paper, with high impact on at least one sub-area or moderate-to-high impact on more than one area, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.</p>
<p>-6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, and ethical considerations.</p>
<p>-5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation.Please use sparingly.</p>
<p>-4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation.Please use sparingly.</p>
<p>-3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility, and incompletely addressed ethical considerations.</p>
<p>-2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility, and mostly unaddressed ethical considerations.</p>
<p>-1: Very Strong Reject: For instance, a paper with trivial results or unaddressed ethical considerations Provided paper: Here is the paper you are asked to review: {paper} Output: Return a JSON object: <JSON> template <JSON> Role: You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.Task description: You have an idea and you want to check if it is novel or not.I.e., not overlapping significantly with existing literature or already well explored.Be a harsh critic for novelty, ensure there is a sufficient contribution in the idea for a new conference or workshop paper.You will be given the titles and abstracts of the 10 papers most relevant to your idea.Decide a paper idea is novel if after sufficient searching, you have not found a paper that significantly overlaps with your idea.Decide a paper idea is not novel, if you have found a paper that significantly overlaps with your idea.Set your decision to True if you think the idea is novel, set it to False if you think the idea is not novel.Your Idea: This is the idea you need to judge for novelty: {Idea} Top 10 relevant papers: {papers} Output: Return only True or False, dont return any other words.</p>
<p>Figure 1 :
1
Figure 1: Comparison of HD &amp; Our score distributions in different domains.1: In the right panel, the upper and lower bounds of the score exceeded the actual score range ([0, 100]) because of linear interpolation.2: to make the horizontal axis comparable, we scaled the Historical Dissimilarity scores by ×100.</p>
<p>Figure 2 :
2
Figure 2: Comparison of AUROC of RND algorithm with different parameters.left: AUROC with different P value when Q=50. right: AUROC with different Q value when P=100</p>
<p>Figure 3 :
3
Figure 3: Neighbor Distribution of a Non-novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 4 :
4
Figure 4: Neighbor Distribution of a Novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 5 :
5
Figure 5: Score Distribution of RND algorithm with different P value.</p>
<ol>
<li>
<p>The Consensus Molecular Subtypes of Colorectal Cancer4.Fratricide-resistant CD7-CAR T cells in T-ALL.4. High-performance medicine: the convergence of human and artificial intelligence 5. International multicenter validation of AI-driven ultrasound detection of ovarian cancer.5.Understanding the tumor immune microenvironment (TIME) for effective therapy 6. Donor-derived GD2-specific CAR T cells in relapsed or refractory neuroblastoma.6.Intestinal microbiota metabolism of L-carnitine, a nutrient in red meat, promotes atherosclerosis 7. Single-nucleus chromatin accessibility and transcriptomic map of breast tissues of women of diverse exosomes educate bone marrow progenitor cells toward a pro-metastatic phenotype through MET 9. Echocardiographic screening for heart failure and optimization of the care pathway for individuals with pacemakers: a randomized controlled trial.9. Signatures of T cell dysfunction and exclusion predict cancer immunotherapy response Continued on next page Nature Medicine Test Set Positive Negative 10.Population-based, first-tier genomic newborn screening in the maternity ward.10.Neutralizing antibody levels are highly predictive of immune protection from symptomatic SARS-CoV-2 infection 11.Allogeneic CD5-specific CAR-T therapy for relapsed/refractory T-ALL: a phase 1 trial.11.Ischemia and reperfusion-from mechanism to translation 12. Transplantation of a genetically modified porcine heart into a live human.12. Mechanisms of fibrosis: therapeutic translation for fibrotic disease 13.A multi-modal single-cell and spatial expression map of metastatic breast cancer biopsies across clinicopathological features.13.Metabolite profiles and the risk of developing diabetes 14. ctDNA-based molecular residual disease and survival in resectable colorectal cancer.14.Mechanisms of NAFLD development and therapeutic strategies 15.Antifungal heteroresistance causes prophylaxis failure and facilitates breakthrough Candida parapsilosis infections.15.Inflammasomes: mechanism of action, role in disease, and therapeutics 16.Subcutaneous weekly semaglutide with automated insulin delivery in type 1 diabetes: a doubleblind, randomized, crossover trial.16.Chronic inflammation in the etiology of disease across the life span 17.Combined endurance and resistance exercise training in heart failure with preserved ejection fraction: a randomized controlled trial.17.Mutational Landscape of Metastatic Cancer Revealed from Prospective Clinical Sequencing of 10,000 Patients 18. Multi-omic profiling a defined bacterial consortium for treatment of recurrent Clostridioides difficile infection.18. Antibody responses to SARS-CoV-2 in patients with COVID-19 19.An organotypic atlas of human vascular cells.19.ABT-199, a potent and selective BCL-2 inhibitor, achieves antitumor activity while sparing platelets 20.Lipid profiling identifies modifiable signatures of cardiometabolic risk in children and adolescents with obesity.20.Clinical and immunological assessment of asymptomatic SARS-CoV-2 infections 21.Ferric carboxymaltose for anemia in late pregnancy: a randomized controlled trial.21.Extrapulmonary manifestations of COVID-19 22. Effects of conditional cash transfers on tuberculosis incidence and mortality according to race, ethnicity and socioeconomic factors in the 100 Million Brazilian Cohort.22.A guide to deep learning in healthcare 23.Phenome-wide associations of sleep characteristics in the Human Phenotype Project.23.A global survey of potential acceptance of a COVID-19 vaccine 24.Proteomic signatures improve risk prediction for common and rare diseases.24.The emerging role of lncRNAs in cancer 25.Remotely delivered weight management for people with long COVID and overweight: the randomized wait-list-controlled ReDIRECT trial.25.SARS-CoV-2 Entry Genes Are Most Highly Expressed in Nasal Goblet and Ciliated Cells within Human Airways 26.Sustained effect of prasinezumab on Parkinson's disease motor progression in the open-label extension of the PASADENA trial.26.Gut microbiota metabolism of dietary fiber influences allergic airway disease and hematopoiesis 27.Collaboration between clinicians and visionlanguage models in radiology report generation.27.The immunology of stroke: from mechanisms to translation 28.Oral obeldesivir provides postexposure protection against Marburg virus in nonhuman primates.28.Asthma phenotypes: the evolution from clinical to molecular approaches Continued on next page Nature Medicine Test Set Positive Negative 29.Digital consults in heart failure care: a randomized controlled trial.</p>
</li>
<li>
<p>Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy 58.Semaglutide in patients with overweight or obesity and chronic kidney disease without diabetes: a randomized double-blind placebo-controlled clinical trial.58.Identification of the molecular basis of doxorubicin-induced cardiotoxicity 59.Intracerebroventricular B7-H3-targeting CAR T cells for diffuse intrinsic pontine glioma: a phase 1 trial.59.New from NPG: Genome-wide association study identifies five new schizophrenia loci 60.AI-based selection of individuals for supplemental MRI in population-based breast cancer screening: the randomized ScreenTrustMRI trial.60.Senolytics Improve Physical Function and Increase Lifespan in Old Age 61.A toolbox for surfacing health equity harms and biases in large language models.61.Subtypes of Pancreatic Ductal Adenocarcinoma and Their Differing Responses to Therapy 62. Partitioned polygenic risk scores identify distinct types of metabolic dysfunction-associated steatotic liver disease.62.A purified membrane protein from Akkermansia muciniphila or the pasteurized bacterium improves metabolism in obese and diabetic mice 63.Multi-omics-based mapping of decidualization resistance in patients with a history of severe preeclampsia.63.The NALP3/NLRP3 Inflammasome Instigates Obesity-Induced Autoinflammation and Insulin Resistance 64.Electronic nudges for sustained influenza vaccination uptake in older adults: the nationwide randomized NUDGE-FLU-2 trial.64.IgE and mast cells in allergic disease 65.A time-stratified, case-crossover study of heat exposure and perinatal mortality from 16 hospitals in sub-Saharan Africa.65. Brown adipose tissue activity controls triglyc-bonemarrow-derived stromal cells to pulmonary alveoli protects against acute lung injury 87.A single-cell atlas of the peripheral immune response in patients with severe COVID-19 88.Determinants of response and resistance to CD19 chimeric antigen receptor (CAR) T cell therapy of chronic lymphocytic leukemia 89.Cancer epigenetics reaches mainstream oncology 90.Real-time tracking of self-reported symptoms to predict potential COVID-19 91.Metformin alters the gut microbiome of individuals with treatment-naive type 2 diabetes, contributing to the therapeutic effects of the drug 92.Synaptic plasticity and depression: new insights from stress and rapid-acting antidepressants 93.Matrix-embedded cells control osteoclast formation 94. Targeting EZH2 in cancer 95.Comprehensive molecular characterization of clinical responses to PD-1 inhibition in metastatic gastric cancer 96.Identification of miR-34a as a potent inhibitor of prostate cancer progenitor cells and metastasis by directly repressing CD44 97.Phenotype molding of stromal cells in the lung tumor microenvironment 98. Key roles of adjuvants in modern vaccines 99.AI in health and medicine Table 5: Titles of Novel (Positive) and Non-novel (Negative) Papers in Nature Medicine Test Set C Prompt C.1 Prompt for LLM with NeurIPS 2024 Review Guideline Prompt</p>
</li>
</ol>
<p>score ← |{N D∈D|N D≤N D Idea }| |D| × 100 10: Return score
1 Find Neighbors and Calculate Neighbor Density1: function NEIGHBOR(Input, P, Q)2:v Input ← GET_EMBEDDING(Input)▷ Using M3-Embedding model3:C ← []4:neighbors ← GET_NEIGHBORS(v Input , max(P, Q))▷ Find max(P, Q) nearest neighbors5:neighbors_f or_count ← neighbors[: Q]▷ Only use the Q nearest neighbors to calculate density6:neighbors_f or_distribution ← neighbors[: P ] ▷ The P nearest neighbors are used to calculate distribution7:for each paper in neighbors_f or_count do8:v paper ← GET_EMBEDDING(paper)9:distance ← 1 -COSINE_SIMILARITY(v Input , v paper )10:C.Append(distance)11:end for12:N D Input ← MEAN(C)13:return N D Input , neighbors_f or_distribution14: end functionAlgorithm 2 Calculate Novelty Score of Given Idea1: Input: Idea2: Output: A score in the range of 0 to 1003: D ← []4: N D Idea , neighbors ← NEIGHBOR(Idea, P, Q)5: for paper in neighbors do6:N D paper , _ ← NEIGHBOR(paper, P, Q)7:D.Append(N D paper )8: end for9:</p>
<p>Table 5
5Test setLabelCounttotal 2024-2025 2019-2023 2014-2018 -2014NeurIPSPositive Negative80 9980 00 310 680 0Nature MedicinePositive Negative66 9966 00 290 320 38</p>
<p>Table 1 :
1
Count of Data in Different Time Ranges for NeurIPS and Nature Medicine Test Sets.Positive: novel samples, Negative: non-novel samples.</p>
<p>Table 3 :
3
validate our statement.AUROC Comparison for Different Design .Absolute Local Density: Use the idea's local density as novelty score.(density calculated by mean distances between idea and idea's P first level neighbors).Euclidean distance: replace the cosine distance with Euclidean in RND
Test setAUROCRelative Neighbor Density(Ours) Absolute Local Density Euclidean distanceNeurIPS0.8200.8510.815Nature Medicine0.7650.7570.753Mixed0.7950.3950.78</p>
<p>NeurIPS test set, the neighbor density-based RND algorithm, absolute local density-based HD algorithm and Sonnet-3.7 with literature search tools achieved AUROC better than 0.8.When it comes to another domain (Nature Medicine
Idea First level neighbor Second level neighborNeighbor: Learning Knowledge Graph-based World Models of Textual Environments60Idea: Evaluating the World Model Implicit in a Generative Model40Neighbor: Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation20Neighbor: DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning0204060801007550250255075
test set for biomedical research), only RND and HD achieved AUROC at approximately 0.7; the other algorithms, including reasoning model such as Sonnet-3.7,degraded to AUROC 0.6.The strong performance of HD in the two respective domains suggests that measuring the local density (average semantic distance between a given idea and its nearest neighbors) in the historical literature database can effectively indicate the novelty of that idea in a single research domain.Since our proposed algorithm also incorporates semantic density, it exhibited similar accuracy level.</p>
<p>Table 6 :
6
Prompt for LLM with NeurIPS 2024 Review Guideline C.2 Prompt for Standardized Project Proposals For model selection, if any version of Claude is mentioned, change it to the latest version of Claude (Claude-3.5);if any version of LLaMA is mentioned, change it to the latest version LLaMA-3.Do not make any other model changes.Now directly generate the edited student idea to match the format of the template.</p>
<p>Table 7 :
7
Prompt for Standardized Project Proposals C.3 Prompt for LLM with Literature Search Prompt</p>
<p>Table 8 :
8
Prompt for LLM with Literature Search</p>
<p>Long-term cardiovascular outcomes of COVID-19 83.Ketone body β-hydroxybutyrate blocks the NLRP3 inflammasome-mediated inflammatory disease 84.Large language models in medicine 85.In vivo photodynamic therapy using upconversion nanoparticles as remote-controlled nanotransducersContinued on next pagePromptRole: You are a writing assistant specialized in editing academic writing.Task: I will give you a student's research idea and an idea template.Your task is to edit the student's idea to follow the template's format.Student idea: Title {title} Main Idea {paper} Template: 1. Title: A concise statement of the main research question to be used as the paper title.2. Problem Statement: Clearly define the problem your research intends to address.Explain clearly why this problem is interesting and important.3. Motivation: Explain why existing methods are not good enough to solve the problem, and explain the inspiration behind the new proposed method.You should also motivate why the proposed method would work better than existing baselines on the problem.4. Proposed Method: Explain how the proposed method works, describe all the essential steps. 5.Step-by-Step Experiment Plan: Break down every single step of the experiments, make sure every step is executable.Cover all essential details such as the datasets, models, and metrics to be used.If the project involves prompting, give some example prompts for each step.6. Test Case Examples: Give at least two concrete examples.The first example should show how the baseline method fails on the test case.If there are multiple baselines, give examples for all of them.The second example should show how the proposed method succeeds on the test case.For each test case, include the input (test example and the full prompt) and the expected output.You should also provide an explanation for why the outputs from the proposed prompt are better.If the proposed method has multiple steps, break them down into intermediate steps.7. Fallback Plan: Propose some alternative plans for what should the students do if the proposed method doesn't manage to satisfy the success criteria.For example, you can suggest additional analysis to help debug why the proposed method didn't work, which could inform alternative new methods, or just turn the project into an analysis paper instead by offering some interesting ablation and insights.Requirement: Make sure that you only edit the wording and formatting, including things like punctuation, capitalization, linebreaks, and bullet points.Also make sure to edit any informal wording and phrasing to use vocabulary that sounds like the template's writing style.No other changes are allowed beyond these.You should use tab as indentation and make sure to use appropriate nested indentation for sub-bullets.All bullets should have a clear hierarchy so people can easily differentiate the sub-bullets.Only leave empty lines between sections and remove any extra line breaks.If many bullet points are clustered together in a paragraph, separate them clearly with indentation and appropriate bullet point markers.Change to a new line for each new bullet point.For the fallback plan, do not list a bunch of bullet points.Instead, condense them into one coherent paragraph.For line breaks, avoid Raw String Literals or Double Backslashes when using " n", and change them to spaces or tabs.For in-line citations, if the citation mentioned the author's last name (like "(Si et al., 2023)" or "(An et al., 2024)"), you should keep them there; but if the citation is just a number (like "[1]" or "[3,4,5]"), you should just remove it and do some necessary rephrasing to make the sentence still sound coherent without the references.Apart from minor rephrasing and changing formatting, do not change any content of the idea.You must preserve the exact meaning of the original idea, do not change, remove, or add any other details.Do not drop any sections (including test case examples).Do not rename any models, datasets, or methods.Do not drop clarification or examples in brackets and do not drop any data source mentions (e.g., Chatbot Arena or Wildchat)!Note that when indexing test case examples, each test case example could have multiple steps of inputs and outputs and you shouldn't give separate indices to them.Each test case example should be a whole set of input-output pairs for the baseline(s) and proposed method.For the proposed method section, avoid any big changes.If the section comes in as a coherent paragraph, you don't have to break it down into bullet points.If the section is already in bullet points, you should keep it that way.If the section is a mix of both, you should keep the bullet points and the coherent paragraph as they are.Keep all the clarification and examples mentioned in all the sections and do not remove any of them (including those in brackets).D Result in Detail
Claude 3.7 sonnet and claude code. Anthropic, 2025. Feb 28th, 2025</p>
<p>Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu, 2024</p>
<p>Cornell Tech, Arxiv annual report 2023. 2023. Feb 25th, 2025</p>
<p>Arxiv dataset. 2025. Jan 10, 2025Cornell University</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.14255October 2024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Pubmed and beyond: biomedical literature search in the age of artificial intelligence. Qiao Jin, Robert Leaman, Zhiyong Lu, EBioMedicine. 1002024</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. September 2024</p>
<p>Evaluating the world model implicit in a generative model. 2025. Jan 10, 2025. 2025. Feb 26th, 2025National Library of Medicine. Pubmed download</p>
<p>Official review of submission1763. 2025. March 7th, 2025Openreview Reviewer 2X9t</p>
<p>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109September 2024</p>
<p>Robustness of LLMs to Perturbations in Text. Ayush Singh, Navpreet Singh, Shubham Vatsal, arXiv:2407.08989July 2024</p>
<p>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.09403October 2024</p>
<p>Evaluating the world model implicit in a generative model. Keyon Vafa, Justin Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan, Advances in Neural Information Processing Systems. 372025</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li, arXiv:2301.12868March 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4300 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4300</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4300</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-281843726</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.04749v1.pdf" target="_blank">LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows</a></p>
                <p><strong>Paper Abstract:</strong> The increasing volume of scholarly publications requires advanced tools for efficient knowledge discovery and management. This paper introduces ongoing work on a system using Large Language Models (LLMs) for the semantic extraction of key concepts from scientific documents. Our research, conducted within the German National Research Data Infrastructure for and with Computer Science (NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing. We outline our explorative work, which uses in-context learning with various LLMs to extract concepts from papers, initially focusing on the Business Process Management (BPM) domain. A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields. We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study. Additionally, we gathered insights from the computer science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services. These services aim to support structured literature reviews, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4300.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4300.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NFDIxCS LLM demo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NFDIxCS LLM-Based Concept Extraction Demo (Gradio UI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source demo system that uses large language models with in-context learning (zero- and few-shot) and chain-of-thought prompting to extract predefined semantic concepts (e.g., research questions, methods, findings) from full-text scientific papers; evaluated on a 122-paper BPM corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NFDIxCS LLM-based concept extraction pipeline (demo)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Users upload a paper and select predefined or custom extraction questions; the system constructs prompts that include the full text (or chunks), an instruction template, and optional few-shot examples (three in-domain examples in 3-shot mode). Prompts request a JSON return with fields for reasoning, extracted context, and concise answer. Chain-of-thought style instructions encourage the model to produce stepwise reasoning alongside the extracted context and final answer. The demo UI (Gradio) orchestrates calls to remote LLMs and returns extracted structured outputs for downstream indexing or knowledge-graph prefill.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen 2.5-72B Instruct; Llama 3.3-70B Instruct; Google Gemini 1.5 Flash variants (Flash_002, Flash_8B_001) — accessed via openrouter.ai for Qwen/Llama and Google API for Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science — Business Process Management (BPM) conference papers</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>122 papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not specified — primary targets were semantic concepts (research questions, methods, findings); extraction could include numeric findings but system was not focused on extracting formal mathematical laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>structured JSON with fields: reasoning (text), context (text passages), answer (concise textual answer); outputs intended for mapping into knowledge-graph templates or structured fields (textual / categorical / binary)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison against a manually annotated gold-standard dataset of 122 papers; metrics per target type (ExactAcc for categorical targets, macro-averaged F1 for binary targets, BERTScore F1 for free-text targets); also pilot user study and workshop feedback</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported per-model/table: ExactAcc up to 0.249 (qwen-2.5-72b 3-shot), BinF1 up to 0.594 (qwen-2.5-72b 3-shot), BERT_F1 up to 0.897 (gemini-1.5-flash-8b 3-shot). Example overall scores: qwen-2.5-72b (3-shot) overall 0.569; qwen-2.5-72b (0-shot) overall 0.540; llama-3.3-70b (0-shot) overall 0.548.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline was the model in zero-shot mode; few-shot (3-shot) was compared to zero-shot for the same models showing mixed effects: few-shot improved categorical and free-text extraction in some models but decreased binary-task performance (class bias). No non-LLM supervised baselines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not focused on extracting formal quantitative / mathematical laws; extraction quality varied by target type (low categorical ExactAcc <0.25, moderate binary F1, strong free-text semantic alignment via BERT_F1 ~0.88–0.90). Full-text few-shot examples are costly and may introduce noise; traceability to source passages needs improvement; few-shot can induce class bias; commercial models' size/cost limit performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4300.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4300.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning (zero-/few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning with LLMs (zero-shot and few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy where LLMs are guided to perform extraction tasks by providing task instructions and optionally a small number of example input-output pairs in the prompt, enabling rapid domain adaptation without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero-shot / Few-shot in-context extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Extraction is performed by sending the full text plus an instruction prompt to the LLM. In few-shot mode, three full-text examples with question, instruction, and manually crafted ideal answers are included in the prompt to align model output style; zero-shot uses only instructions and text. Prompts request structured JSON responses containing reasoning, context, and answer. Models produce outputs directly without supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen 2.5-72B, Llama 3.3-70B, Gemini 1.5 Flash variants (as evaluated in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Business Process Management (BPM) papers for evaluation; generalizable to other scientific fields</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>122 papers (evaluation set)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not explicitly targeted; method extracts semantic targets and free-text findings which may include quantitative statements but not formalized laws</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual/JSON structured answers (reasoning, context, concise answer), suitable for mapping into KG fields or structured metadata</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to manual annotations (gold standard) using ExactAcc, macro F1, and BERTScore F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See model-specific metrics reported for the demo (ExactAcc, BinF1, BERT_F1). Few-shot vs zero-shot comparisons reported (e.g., qwen-2.5-72b 3-shot ExactAcc 0.249 vs 0-shot 0.219; BinF1 0.594 vs 0.514; BERT_F1 0.863 vs 0.887).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Zero-shot served as baseline; few-shot compared to zero-shot showed mixed improvements (improved categorical and free-text in some cases, decreased binary performance due to class bias). No classical supervised baseline included.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Including full-text examples in prompts is costly and may add noise; few-shot can introduce class biases; performance depends on model size and quality; not designed to produce formal symbolic/mathematical expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4300.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4300.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that encourages models to output intermediate reasoning steps along with final answers, improving the traceability and reasoning quality of extracted information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Chain-of-thought style extraction prompts</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompts instruct the model to 'think step by step' and return a reasoning field in the requested JSON, producing intermediate reasoning text alongside selected context and a concise answer; intended to improve traceability and quality of extracted semantic items.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied with Qwen 2.5-72B, Llama 3.3-70B, and Gemini 1.5 Flash variants in the demo</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>BPM papers (demo); generalizable</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>122 papers (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not targeted specifically</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual reasoning plus extracted context and concise textual answer in JSON</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated as part of the demo against manual annotations and user feedback; no separate ablation reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No isolated metrics for chain-of-thought alone; included within overall model performance reported in Table 1</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not isolated; compared implicitly to prompts without chain-of-thought (zero-shot baseline described elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Chain-of-thought increases output verbosity and token usage; traceability still requires explicit linking/highlighting of source passages; possible cost increase</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4300.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4300.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that grounds LLM outputs in retrieved external documents to reduce hallucination and improve factual grounding of answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval Augmentation Reduces Hallucination in Conversation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The paper mentions RAG as a general technique to ground LLM knowledge in relevant texts for better question answering and factuality; RAG systems retrieve relevant passages and include them in the LLM context or use them to post-process model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not applicable in-paper (general technique mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / multiple scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not specified (RAG can be used to support extraction of numeric facts or relationships but the paper does not detail such use)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>not specified in this paper (cited work evaluates hallucination reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as a strategy to ground models; in practice RAG depends on retrieval quality and chunking; paper notes chunking/semantic relevance limits in vector search as ongoing work</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4300.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4300.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that combines open-access scholarly content with LLMs to provide grounded question answering across multiple scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CORE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as an example of retrieval-grounded QA across scholarly literature; integrates open-access corpora (CORE) with an LLM for domain-spanning question answering, likely using retrieval to ground responses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not specified in this paper (referenced work likely details models)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-domain scholarly literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as related work; no details in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4300.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4300.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit: AI literature review research assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform that uses LLMs in a systematic-review style workflow to synthesize findings from multiple papers in response to a user's query.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Elicit: AI literature review research assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Elicit-style literature synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cited as a platform that leverages LLMs to retrieve relevant literature and synthesize findings into answers to user-specified queries, inspired by workflows for systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / literature reviews across domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>may synthesize quantitative findings but not described here</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>synthesized narrative answers (not formalized laws)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>not described in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not described in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not described</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as related work; paper notes such platforms do not typically support predefined domain-specific question templates without subscription</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4300.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4300.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ORKG Ask: A Neuro-symbolic Scholarly Search and Exploration System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid neuro-symbolic system that enables ad-hoc comparison tables and information extraction from scholarly works using LLMs tied to knowledge-graph structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ORKG Ask (neuro-symbolic IE for scholarly search)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as a recent system that uses LLM-based information extraction to populate comparison tables and support knowledge-graph backed scholarly search; the authors propose to build on ORKG Ask by using curated domain questions to prefill KG templates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not specified in this paper (ORKG Ask paper details models)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scholarly literature / knowledge graph population</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not specified here (ORKG focuses on contributions/methods/findings in structured form)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>comparison tables and structured KG entries</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper positions ORKG Ask as a baseline to extend toward prefilling KG templates with curated questions; manual curation remains effortful</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4300.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4300.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scite.ai</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scite.ai citation classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A platform that uses AI to classify citation contexts as supporting, disputing, or mentioning a claim, facilitating analysis of how scientific claims are used across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite.ai) data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Citation-context classification (Scite)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned as an example of an AI-driven service that highlights and classifies citation contexts; useful for identifying support/dispute patterns across literature but not described here as extracting formal quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general across disciplines</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>citation classification labels and contexts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not described here</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as related work; not central to the paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows', 'publication_date_yy_mm': '2025-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering. <em>(Rating: 2)</em></li>
                <li>ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System. <em>(Rating: 2)</em></li>
                <li>Elicit: AI literature review research assistant. <em>(Rating: 2)</em></li>
                <li>Retrieval Augmentation Reduces Hallucination in Conversation. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Construction of the literature graph in semantic scholar. <em>(Rating: 1)</em></li>
                <li>Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite.ai) data. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4300",
    "paper_id": "paper-281843726",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "NFDIxCS LLM demo",
            "name_full": "NFDIxCS LLM-Based Concept Extraction Demo (Gradio UI)",
            "brief_description": "An open-source demo system that uses large language models with in-context learning (zero- and few-shot) and chain-of-thought prompting to extract predefined semantic concepts (e.g., research questions, methods, findings) from full-text scientific papers; evaluated on a 122-paper BPM corpus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "NFDIxCS LLM-based concept extraction pipeline (demo)",
            "method_description": "Users upload a paper and select predefined or custom extraction questions; the system constructs prompts that include the full text (or chunks), an instruction template, and optional few-shot examples (three in-domain examples in 3-shot mode). Prompts request a JSON return with fields for reasoning, extracted context, and concise answer. Chain-of-thought style instructions encourage the model to produce stepwise reasoning alongside the extracted context and final answer. The demo UI (Gradio) orchestrates calls to remote LLMs and returns extracted structured outputs for downstream indexing or knowledge-graph prefill.",
            "llm_model_used": "Qwen 2.5-72B Instruct; Llama 3.3-70B Instruct; Google Gemini 1.5 Flash variants (Flash_002, Flash_8B_001) — accessed via openrouter.ai for Qwen/Llama and Google API for Gemini",
            "scientific_domain": "Computer Science — Business Process Management (BPM) conference papers",
            "number_of_papers": "122 papers",
            "type_of_quantitative_law": "not specified — primary targets were semantic concepts (research questions, methods, findings); extraction could include numeric findings but system was not focused on extracting formal mathematical laws",
            "extraction_output_format": "structured JSON with fields: reasoning (text), context (text passages), answer (concise textual answer); outputs intended for mapping into knowledge-graph templates or structured fields (textual / categorical / binary)",
            "validation_method": "Comparison against a manually annotated gold-standard dataset of 122 papers; metrics per target type (ExactAcc for categorical targets, macro-averaged F1 for binary targets, BERTScore F1 for free-text targets); also pilot user study and workshop feedback",
            "performance_metrics": "Reported per-model/table: ExactAcc up to 0.249 (qwen-2.5-72b 3-shot), BinF1 up to 0.594 (qwen-2.5-72b 3-shot), BERT_F1 up to 0.897 (gemini-1.5-flash-8b 3-shot). Example overall scores: qwen-2.5-72b (3-shot) overall 0.569; qwen-2.5-72b (0-shot) overall 0.540; llama-3.3-70b (0-shot) overall 0.548.",
            "baseline_comparison": "Baseline was the model in zero-shot mode; few-shot (3-shot) was compared to zero-shot for the same models showing mixed effects: few-shot improved categorical and free-text extraction in some models but decreased binary-task performance (class bias). No non-LLM supervised baselines reported.",
            "challenges_limitations": "Not focused on extracting formal quantitative / mathematical laws; extraction quality varied by target type (low categorical ExactAcc &lt;0.25, moderate binary F1, strong free-text semantic alignment via BERT_F1 ~0.88–0.90). Full-text few-shot examples are costly and may introduce noise; traceability to source passages needs improvement; few-shot can induce class bias; commercial models' size/cost limit performance comparisons.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4300.0",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "In-context learning (zero-/few-shot)",
            "name_full": "In-context learning with LLMs (zero-shot and few-shot prompting)",
            "brief_description": "A prompting strategy where LLMs are guided to perform extraction tasks by providing task instructions and optionally a small number of example input-output pairs in the prompt, enabling rapid domain adaptation without task-specific fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Zero-shot / Few-shot in-context extraction",
            "method_description": "Extraction is performed by sending the full text plus an instruction prompt to the LLM. In few-shot mode, three full-text examples with question, instruction, and manually crafted ideal answers are included in the prompt to align model output style; zero-shot uses only instructions and text. Prompts request structured JSON responses containing reasoning, context, and answer. Models produce outputs directly without supervised fine-tuning.",
            "llm_model_used": "Qwen 2.5-72B, Llama 3.3-70B, Gemini 1.5 Flash variants (as evaluated in the paper)",
            "scientific_domain": "Business Process Management (BPM) papers for evaluation; generalizable to other scientific fields",
            "number_of_papers": "122 papers (evaluation set)",
            "type_of_quantitative_law": "not explicitly targeted; method extracts semantic targets and free-text findings which may include quantitative statements but not formalized laws",
            "extraction_output_format": "textual/JSON structured answers (reasoning, context, concise answer), suitable for mapping into KG fields or structured metadata",
            "validation_method": "Comparison to manual annotations (gold standard) using ExactAcc, macro F1, and BERTScore F1",
            "performance_metrics": "See model-specific metrics reported for the demo (ExactAcc, BinF1, BERT_F1). Few-shot vs zero-shot comparisons reported (e.g., qwen-2.5-72b 3-shot ExactAcc 0.249 vs 0-shot 0.219; BinF1 0.594 vs 0.514; BERT_F1 0.863 vs 0.887).",
            "baseline_comparison": "Zero-shot served as baseline; few-shot compared to zero-shot showed mixed improvements (improved categorical and free-text in some cases, decreased binary performance due to class bias). No classical supervised baseline included.",
            "challenges_limitations": "Including full-text examples in prompts is costly and may add noise; few-shot can introduce class biases; performance depends on model size and quality; not designed to produce formal symbolic/mathematical expressions.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4300.1",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Chain-of-thought prompting",
            "name_full": "Chain-of-thought prompting for LLMs",
            "brief_description": "A prompting technique that encourages models to output intermediate reasoning steps along with final answers, improving the traceability and reasoning quality of extracted information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Chain-of-thought style extraction prompts",
            "method_description": "Prompts instruct the model to 'think step by step' and return a reasoning field in the requested JSON, producing intermediate reasoning text alongside selected context and a concise answer; intended to improve traceability and quality of extracted semantic items.",
            "llm_model_used": "Applied with Qwen 2.5-72B, Llama 3.3-70B, and Gemini 1.5 Flash variants in the demo",
            "scientific_domain": "BPM papers (demo); generalizable",
            "number_of_papers": "122 papers (evaluation)",
            "type_of_quantitative_law": "not targeted specifically",
            "extraction_output_format": "textual reasoning plus extracted context and concise textual answer in JSON",
            "validation_method": "Evaluated as part of the demo against manual annotations and user feedback; no separate ablation reported",
            "performance_metrics": "No isolated metrics for chain-of-thought alone; included within overall model performance reported in Table 1",
            "baseline_comparison": "Not isolated; compared implicitly to prompts without chain-of-thought (zero-shot baseline described elsewhere)",
            "challenges_limitations": "Chain-of-thought increases output verbosity and token usage; traceability still requires explicit linking/highlighting of source passages; possible cost increase",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4300.2",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "An approach that grounds LLM outputs in retrieved external documents to reduce hallucination and improve factual grounding of answers.",
            "citation_title": "Retrieval Augmentation Reduces Hallucination in Conversation.",
            "mention_or_use": "mention",
            "method_name": "Retrieval-Augmented Generation",
            "method_description": "The paper mentions RAG as a general technique to ground LLM knowledge in relevant texts for better question answering and factuality; RAG systems retrieve relevant passages and include them in the LLM context or use them to post-process model outputs.",
            "llm_model_used": "not applicable in-paper (general technique mentioned)",
            "scientific_domain": "general / multiple scientific domains",
            "number_of_papers": null,
            "type_of_quantitative_law": "not specified (RAG can be used to support extraction of numeric facts or relationships but the paper does not detail such use)",
            "extraction_output_format": "not specified in this paper",
            "validation_method": "not specified in this paper (cited work evaluates hallucination reduction)",
            "performance_metrics": "not reported here",
            "baseline_comparison": "not reported here",
            "challenges_limitations": "Mentioned as a strategy to ground models; in practice RAG depends on retrieval quality and chunking; paper notes chunking/semantic relevance limits in vector search as ongoing work",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4300.3",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "CORE-GPT",
            "name_full": "CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering",
            "brief_description": "A system that combines open-access scholarly content with LLMs to provide grounded question answering across multiple scientific domains.",
            "citation_title": "CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering.",
            "mention_or_use": "mention",
            "method_name": "CORE-GPT",
            "method_description": "Mentioned as an example of retrieval-grounded QA across scholarly literature; integrates open-access corpora (CORE) with an LLM for domain-spanning question answering, likely using retrieval to ground responses.",
            "llm_model_used": "not specified in this paper (referenced work likely details models)",
            "scientific_domain": "cross-domain scholarly literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "not specified here",
            "extraction_output_format": "not specified here",
            "validation_method": "not specified here",
            "performance_metrics": "not specified here",
            "baseline_comparison": "not specified here",
            "challenges_limitations": "Mentioned as related work; no details in this paper",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4300.4",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Elicit",
            "name_full": "Elicit: AI literature review research assistant",
            "brief_description": "A platform that uses LLMs in a systematic-review style workflow to synthesize findings from multiple papers in response to a user's query.",
            "citation_title": "Elicit: AI literature review research assistant.",
            "mention_or_use": "mention",
            "method_name": "Elicit-style literature synthesis",
            "method_description": "Cited as a platform that leverages LLMs to retrieve relevant literature and synthesize findings into answers to user-specified queries, inspired by workflows for systematic reviews.",
            "llm_model_used": "not specified in this paper",
            "scientific_domain": "general / literature reviews across domains",
            "number_of_papers": null,
            "type_of_quantitative_law": "may synthesize quantitative findings but not described here",
            "extraction_output_format": "synthesized narrative answers (not formalized laws)",
            "validation_method": "not described in this paper",
            "performance_metrics": "not described in this paper",
            "baseline_comparison": "not described",
            "challenges_limitations": "Mentioned as related work; paper notes such platforms do not typically support predefined domain-specific question templates without subscription",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4300.5",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "ORKG Ask",
            "name_full": "ORKG Ask: A Neuro-symbolic Scholarly Search and Exploration System",
            "brief_description": "A hybrid neuro-symbolic system that enables ad-hoc comparison tables and information extraction from scholarly works using LLMs tied to knowledge-graph structures.",
            "citation_title": "ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System.",
            "mention_or_use": "mention",
            "method_name": "ORKG Ask (neuro-symbolic IE for scholarly search)",
            "method_description": "Referenced as a recent system that uses LLM-based information extraction to populate comparison tables and support knowledge-graph backed scholarly search; the authors propose to build on ORKG Ask by using curated domain questions to prefill KG templates.",
            "llm_model_used": "not specified in this paper (ORKG Ask paper details models)",
            "scientific_domain": "scholarly literature / knowledge graph population",
            "number_of_papers": null,
            "type_of_quantitative_law": "not specified here (ORKG focuses on contributions/methods/findings in structured form)",
            "extraction_output_format": "comparison tables and structured KG entries",
            "validation_method": "not described here",
            "performance_metrics": "not described here",
            "baseline_comparison": "not described here",
            "challenges_limitations": "Paper positions ORKG Ask as a baseline to extend toward prefilling KG templates with curated questions; manual curation remains effortful",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4300.6",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        },
        {
            "name_short": "Scite.ai",
            "name_full": "Scite.ai citation classification",
            "brief_description": "A platform that uses AI to classify citation contexts as supporting, disputing, or mentioning a claim, facilitating analysis of how scientific claims are used across the literature.",
            "citation_title": "Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite.ai) data.",
            "mention_or_use": "mention",
            "method_name": "Citation-context classification (Scite)",
            "method_description": "Mentioned as an example of an AI-driven service that highlights and classifies citation contexts; useful for identifying support/dispute patterns across literature but not described here as extracting formal quantitative laws.",
            "llm_model_used": "not specified in this paper",
            "scientific_domain": "general across disciplines",
            "number_of_papers": null,
            "type_of_quantitative_law": "not applicable",
            "extraction_output_format": "citation classification labels and contexts",
            "validation_method": "not described here",
            "performance_metrics": "not described here",
            "baseline_comparison": "not described here",
            "challenges_limitations": "Mentioned as related work; not central to the paper's experiments",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4300.7",
            "source_info": {
                "paper_title": "LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows",
                "publication_date_yy_mm": "2025-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering.",
            "rating": 2,
            "sanitized_title": "coregpt_combining_open_access_research_and_large_language_models_for_credible_trustworthy_question_answering"
        },
        {
            "paper_title": "ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System.",
            "rating": 2,
            "sanitized_title": "orkg_ask_a_neurosymbolic_scholarly_search_and_exploration_system"
        },
        {
            "paper_title": "Elicit: AI literature review research assistant.",
            "rating": 2,
            "sanitized_title": "elicit_ai_literature_review_research_assistant"
        },
        {
            "paper_title": "Retrieval Augmentation Reduces Hallucination in Conversation.",
            "rating": 2,
            "sanitized_title": "retrieval_augmentation_reduces_hallucination_in_conversation"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Construction of the literature graph in semantic scholar.",
            "rating": 1,
            "sanitized_title": "construction_of_the_literature_graph_in_semantic_scholar"
        },
        {
            "paper_title": "Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite.ai) data.",
            "rating": 1,
            "sanitized_title": "examining_the_use_of_supportive_and_contrasting_citations_in_different_disciplines_a_brief_study_using_scite_sciteai_data"
        }
    ],
    "cost": 0.01166025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows ⋆
6 Oct 2025</p>
<p>Samy Ateia samy.ateia@ur.de 0009-0000-2622-9194
University of Regensburg
Universitätsstraße 3193053RegensburgGermany</p>
<p>Udo Kruschwitz udo.kruschwitz@ur.de 0000-0002-5503-0341
University of Regensburg
Universitätsstraße 3193053RegensburgGermany</p>
<p>Melanie Scholz melanie.scholz@uni-bayreuth.de 0000-0002-5503-0341
University of Bayreuth
Universitätsstraße 3095447BayreuthGermany</p>
<p>Agnes Koschmider agnes.koschmider@uni-bayreuth.de 0000-0001-8206-7636
University of Bayreuth
Universitätsstraße 3095447BayreuthGermany</p>
<p>Moayad Almohaishi moayad.almohaishi@uni-bayreuth.de 0009-0002-1758-3153
University of Bayreuth
Universitätsstraße 3095447BayreuthGermany</p>
<p>LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows ⋆
6 Oct 2025AC0C24461439B6430FA196E65643357510.1007/978-arXiv:2510.04749v1[cs.DL]sponding to the accepted manuscript and supersedes the submitted version that was inadvertently published as the version of record.Large Language ModelsInformation ExtractionScientific PublishingDigital LibrariesFAIR PrinciplesKnowledge Graphs
The increasing volume of scholarly publications requires advanced tools for efficient knowledge discovery and management.This paper introduces ongoing work on a system using Large Language Models (LLMs) for the semantic extraction of key concepts from scientific documents.Our research, conducted within the German National Research Data Infrastructure for and with Computer Science (NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing.We outline our explorative work, which uses in-context learning with various LLMs to extract concepts from papers, initially focusing on the Business Process Management (BPM) domain.A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields.We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study.Additionally, we gathered insights from the computer science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services.These services aim to support structured literature reviews, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs.</p>
<p>Introduction</p>
<p>The scientific publication landscape is booming with global annual publication growing by 59% according to the NSF 3 and more than one million articles being published per year in biomedicine and life sciences alone [6].This rise in publications makes it harder for scientists to stay on top of their field, while also limiting the discoverability of their publications as they have to compete with others for visibility.Digital transformation and new tools such as LLMs can accelerate that problem, but also have the potential to assist scientists and publishing platforms in managing these challenges.</p>
<p>In computer science, publications are often accompanied by software artifacts and datasets for reproducibility, but their management frequently lacks standardization and fails to meet FAIR principles.The National Research Data Infrastructure for and with Computer Science (NFDIxCS) project 4 addresses this by creating an infrastructure to implement FAIR principles [21] for CS research outputs in Germany [5].But to make these artifacts findable it is necessary to link them to relevant semantic information from the publication text itself, for example, research questions or methods so that they are discoverably as related work by other scientists.</p>
<p>Our project, situated within the NFDIxCS initiative, aims to develop tools to exactly address this problem.We leverage Large Language Models (LLMs) for the semantic analysis of scientific text, with the goal of enhancing the FAIR principles for scholarly literature.Specifically, we aim to:</p>
<p>-Develop robust methods for automatically extracting key semantic concepts (e.g., research questions, methodologies, findings) from scientific papers.-Explore mechanisms for structuring these extracted concepts to improve the organization and distribution of digital content, potentially linking them to knowledge graphs.-Design and prototype services, informed by community needs, that use this structured information to support researchers in their workflows.</p>
<p>This short paper presents our preliminary findings and outlines how user-driven requirements are shaping the trajectory of our research towards practical applications.We publish a demo system alongside its source code under a permissive license5 alongside the results of our user workshops, and plan to maintain this practice for future services.</p>
<p>Related Work</p>
<p>In this work, we explore the use of LLMs for knowledge extraction to improve scientific workflows within digital libraries and beyond.We review related efforts in three key areas: platforms for scientific literature analysis, knowledge graphs for structuring scientific information, and the use of LLMs for information extraction.</p>
<p>Platforms for Scientific Literature Analysis</p>
<p>Several platforms exist that use natural language processing (NLP) based on language models to highlight relevant information from scientific text, therefore assisting in navigating the considerable volume of publications.Semantic Scholar [2], for example, employs AI to provide summaries (TLDRs) and identify influential citations, while Elicit uses a systematic review inspired workflow, leveraging LLMs to synthesize findings from multiple papers in response to a user's query [20].Scite.aifocuses specifically on citation context, classifying whether a citation supports, disputes, or merely mentions a claim [9].While these platforms offer similar flexible LLM-based question answering tools, they do not currently offer the use of predefined domain-specific questions and mostly require a paid subscription to be fully utilized.With our approach, we want to potentially offer higher accuracy and user guidance through curated extraction targets and examples serving specialized communities.</p>
<p>Knowledge Graphs for Structuring Scientific Knowledge</p>
<p>Structuring scientific knowledge in a machine-readable format has long been a goal of the scientific community.The Open Research Knowledge Graph (ORKG) [8] is a prominent initiative aiming to represent the content of research papers as structured data.By describing papers through their contributions, methods, and findings, the ORKG facilitates systematic comparisons and reviews.Other notable examples are the discontinued Microsoft Academic Graph (MAG) [18] that was succeeded by OpenAlex [13] or SciKGraph [17].However, curating such knowledge graphs often requires significant manual effort from researchers.Most recently, ORKG Ask was introduced, which offers the possibility to create ad-hoc comparison tables using information extraction with LLMs [11].In our work, we want to build on that approach and take it a step further.Instead of just having users query questions on a set of retrieved papers, we explore how curated questions from domain-experts can be leveraged to prefill knowledge graph input templates for users.This complements the KG vision by lowering the barrier to entry and scaling up content acquisition.Embedding and indexing the extracted information in separate fields could lead to improved semantic search, by enabling users to search for papers with similar research questions or algorithms.</p>
<p>LLMs for Information Extraction in Science</p>
<p>In-context learning with LLMs describes the ability of these models to solve problems that they have not explicitly been trained on, by just giving the model an abstract description of the problem (Zero-Shot) or several examples (Few-Shot) in their input context.These approaches were first popularized with LLMs like GPT-3 [3] and enable their use in domains where limited, or no training data is available.Recent LLMs such as the Google Gemini series 6 or OpenAIs GPT-4.1 7 have pushed the size of the available context up to 1 million input tokens.Which makes it feasible to extract information from large text sources in a single step.These properties can be used in retrieval augmented generation (RAG) [16] systems that ground the knowledge of these models in relevant texts.Systems such as CORE-GPT have shown the usefulness of such approaches in questions answering across multiple scientific domains [12].</p>
<p>In our work, we explore both zero-and few-shot learning for extracting predefined semantic information from scientific texts, that can then be used to facilitate scientific knowledge discovery and publication workflows.</p>
<p>Methodology: LLM-Based Concept Extraction</p>
<p>Our system uses an LLM to extract semantic information from scientific documents.The demo UI allows a user to upload a paper and pose predefined or custom questions.The LLM then processes the document and a prompt to identify and return relevant information or synthesized answers (Figure 1).</p>
<p>In-context Learning for Rapid Domain Adaptation</p>
<p>We leverage the in-context learning of modern LLMs for rapid domain adaptation.By providing instructions and a few examples to guide extraction, our method avoids the need for the extensive, domain-specific datasets required by traditional supervised techniques.</p>
<p>In our evaluations we tested two modes: A few-shot mode where we supplied three in-domain examples each consisting of 1. the full text of the document; 2. the domain-specific information extraction questions, 3. the instructions, 4. the manually crafted ideal answer from the document.In addition, we tested a zeroshot mode where we only supplied the instruction and the full text of the PDF.</p>
<p>The zero-shot mode formed our baseline for evaluation, but could also be used to offer the flexibility to the user to pose their own extraction questions against a document or a set of retrieved documents.The few-shot mode aligns the model with the style of the manual annotators, overcoming limited or missing instructions in the way the question was posed.This mode could be used in settings where predefined questions and predictable answer formats are important, for example when prefilling forms for later knowledge graph mapping.</p>
<p>We tested four different models: Qwen 2.5 72B instruct [14], Llama 3.3 70B instruct 8 [7], Gemini 1.5 Flash 002, Gemini 1.5 Flash 8B 001 [4].Llama and Qwen were accessed via openrouter.aiwhile the Gemini models were accessed via the official Google API.</p>
<p>The instructions used chain of thought prompting [19] to generate a reasoning beside relevant context and the final answer.</p>
<p>The exact zero-shot prompt can be seen in Listing 1.1.</p>
<p>Listing 1.1. Zero-shot prompt example in Python</p>
<p>Extract the information answering the following question from the text : Question : ' ' '{ question } ' ' ' Text : ' ' '{ text } ' ' ' Return a JSON object in the following format : {{ " reasoning ": " &lt; think step by step and write down your reasoning &gt;" , " context ": " &lt; contains all relevant context from the text &gt;" , " answer ": " &lt; one concise answer to the question for example : yes / no / none , or a word or multiple words &gt;" }} Try to be concise and limit your reasoning , answer , and the extracted context to max 500 words .</p>
<p>Dataset and Domain</p>
<p>The initial development and a preliminary evaluation were carried out on a corpus of 122 scientific papers from the Business Process Management (BPM) conferences (2019-2023) 9 .This domain was selected due to the availability of domain experts who are actively constructing a knowledge graph in this area.Key concepts were manually annotated in the papers to establish a gold standard for evaluating extraction performance.</p>
<p>Extraction Example</p>
<p>An example of the extraction process for the Target Concept of a "Research Question" would involve posing the Query: "What is the explicitly stated research question for the paper?".The system might then return an Example Extracted Answer like: "How to decide which processes need to be analyzed in detail to determine if changes are necessary."</p>
<p>This methodology is highly flexible, allowing us to target a wide array of semantic information within scientific texts with high adaptability, as only limited domain expert involvement is needed to create a few examples for each information item.</p>
<p>Demo System</p>
<p>To showcase the ability of the tool and collect initial user feedback, we built a demo UI using the Gradio framework [1] around our approach.The demo system is available online 10 (user:demo, pw:demo) and the source code for this system is available on GitHub 11 .</p>
<p>User Feedback</p>
<p>In a pilot user study, we collected initial user-feedback with the demo system through a questionnaire after instructing a panel of users to choose one paper from a selection of business processing domain papers, upload it to the tool and select any questions that they were interested in.</p>
<p>At a separate workshop with around 30 participants from different computerscience fields, we collected user-stories that they would like to be solved by the offered and demonstrated technology.</p>
<ol>
<li>For free-text targets (4 targets, 488 annotations), we assessed semantic equivalence with the F1-score from BERTScore [22].</li>
</ol>
<p>The Overall score in Table 1 is the unweighted arithmetic mean of these three metrics.BERT_F1 scores near 0.90 indicate strong semantic alignment on free-text fields, whereas binary indicators show moderate performance (best BinF1 = 0.59) and exact categorical extraction remains limited (ExactAcc &lt; 0.25).</p>
<p>User Study &amp; Workshop</p>
<p>Feedback from our pilot user study on the prototype demo UI was positive (88% satisfaction with extracted concepts), indicating the potential utility of the approach.While some feedback was UI related (hiding advanced configuration like few-shot examples, wanting more expert configuration), a main point was that the traceability of the extracted information should be improved.</p>
<p>In a separate workshop with around 30 computer-science researchers, we collected 56 user-stories.38 of these focused on the task of literature research and comparison, 8 on assistance while writing papers, 3 on support in the review process, 3 were directed towards software development and 4 were unique.Overall, it became clear that the users want to go beyond just extracting concepts from one specific paper and compare the extracted information from multiple papers instead.The full categorized list is available in our repository 12 .</p>
<p>Discussion</p>
<p>The results of our technical evaluation and user-studies, while preliminary, provide valuable direction for the development of our future services.The open-weight models Qwen 2.5-72B-Instruct and Llama 3.3-70B-Instruct seemed to perform better than the commercial models that we tested.For the Gemini 1.5-flash-8b model, this is most likely explained by the difference in size.The size of the normal Gemini 1.5-flash model is unknown but given our results and the cost and speed we suspect that it is also smaller than the 70 billion parameter models that we compared them to.</p>
<p>From the feedback that we collected through our pilot user study and the discussion in a later workshop, it became clear that there is a need for better transparency and traceability.Ideally, highlighting the text passages that inform an extracted information items in the source document.</p>
<p>Even though there are commercial services available that are similar to our tool, our contribution can inform researchers and professionals that want to offer customizable domain-specific services to their users.We demonstrate the feasibility of in-context learning and open-weights models for these use-cases and publish our code to boost independent development of transparent services.</p>
<p>Conclusion and Ongoing Work</p>
<p>We confirmed the potential of current LLMs to summarize and extract domainspecific information from scientific text.Through in-context learning, these models can be quickly adapted to specific scientific domains and facilitate the transfer of expert knowledge between researchers by highlighting and comparing key aspects of their work.</p>
<p>Through our user-centric approach, we collected valuable feedback and userstories that can guide the development of current and future services.Notable transparency and the need that services enable the user to verify LLM generated output by tracing summarized information back to the source text.</p>
<p>Our ongoing work will focus on exploring embedding-based retrieval on the extracted structured information, therefore overcoming the arbitrary chunking issue that limits semantic relevance in vector search.We're also exploring how our approach can be integrated in the publishing process, prefilling templates for knowledge graph mapping e.g., for ORKG.Making it easier for authors to fill out forms that facilitate the discoverability of their work.</p>
<p>Overall, our work highlights the potential of LLMs to improve the publishing process and discoverability of scientific information in digital libraries and beyond.Its main contribution is demonstrating the practical integration of these models into a user-focused, open-source system designed to tackle real-world challenges, rather than proposing a novel extraction algorithm itself.</p>
<p>Fig. 1 .
1
Fig. 1.LLM-based demo extraction pipeline.</p>
<p>Few-shot examples seemed to improve the performance of the models in tasks where specific categorial answers were needed and on the free-text extractions measured by BERTScore.But on the binary classification task, the performance decreased.This could be explained by a class bias introduced via the few-shot examples, while on the textual extractions the examples might have informed the model better about the expected format of the answers.Using the full-text of documents in few-shot examples is costly and potentially increases noise.We are working on exploring the impact of more and shorter examples and selecting ideal examples for specific extraction target types.</p>
<p>Table 1 .
1
Model comparison on the paper-coding benchmark (higher = better)
ModelExactAcc BinF1 BERT_F1 Overallqwen-2.5-72b (3-shot)0.2490.5940.8630.569qwen-2.5-72b (0-shot)0.2190.5140.8870.540llama-3.3-70b (0-shot)0.2120.5560.8770.548gemini-1.5-flash-002 (3-shot)0.2460.3300.8930.490gemini-1.5-flash-002 (0-shot)0.1830.3900.8830.486gemini-1.5-flash-8b-001 (0-shot)0.1710.3450.8810.466gemini-1.5-flash-8b-001 (3-shot)0.1800.1480.8970.408
https://web.archive.org/web/20250507134337/https://www.ncses.nsf.gov/ pubs/nsb202333/executive-summary
https://nfdixcs.org/
CC BY 4.0
https://web.archive.org/web/20250607225206/https://blog.google/ technology/ai/google-gemini-next-generation-model-february-2024/
https://web.archive.org/web/20250612080402/https://openai.com/index/ gpt-4-1/
https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/
https://bpm-conference.org/conferences/
https://github.com/SamyAteia/nfdixcs-d3-knowledge-extraction-demo
ResultsWe evaluated model performance against a gold-standard dataset of 122 manually annotated papers.We used three metrics based on the target type:Acknowledgments.We thank the anonymous reviewers for their valuable feedback.This work is funded by the German Research Foundation (DFG) as part of the NFDIxCS consortium (Grant number: 501930651).Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article.
For categorical targets (15 targets, 1121 annotations), we used ExactAcc. 15</p>
<p>For binary targets (13 targets, 984 annotations), we used the macro-averaged F1-score. BinF1</p>
<p>Gradio: Hasslefree sharing and testing of ml models in the wild. A Abid, A Abdalla, A Abid, D Khan, A Alfozan, J Zou, 2019</p>
<p>W Ammar, D Groeneveld, C Bhagavatula, I Beltagy, M Crawford, D Downey, J Dunkelberger, A Elgohary, S Feldman, V Ha, arXiv:1805.02262Construction of the literature graph in semantic scholar. 2018arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. P Georgiev, 2024</p>
<p>Research Data Management in Computer Science-NFDIxCS Approach. M Goedicke, U Lucke, INFORMATIK 20222022BonnGesellschaft für Informatik</p>
<p>The landscape of biomedical research. R González-Márquez, L Schmidt, B M Schmidt, P Berens, D Kobak, 10.1016/j.patter.2024.100968Patterns. 561009682024</p>
<p>A Grattafiori, The Llama 3 Herd of Models. 2024</p>
<p>Open research knowledge graph: a system walkthrough. M Y Jaradeh, A Oelen, M Prinz, M Stocker, S Auer, Digital Libraries for Open Knowledge: 23rd International Conference on Theory and Practice of Digital Libraries, TPDL 2019. Proceedings. Oslo, NorwaySpringerSeptember 9-12, 2019. 201923</p>
<p>Examining the use of supportive and contrasting citations in different disciplines: a brief study using Scite (scite. ai) data. B Lund, A Shamsi, Scientometrics. 12882023</p>
<p>An empirical evaluation of set similarity join techniques. W Mann, N Augsten, P Bouros, Proceedings of the VLDB Endowment. the VLDB Endowment20169</p>
<p>A Oelen, M Y Jaradeh, S Auer, arXiv:2412.04977ORKG ASK: A Neuro-symbolic Scholarly Search and Exploration System. 2024arXiv preprint</p>
<p>CORE-GPT: Combining open access research and large language models for credible, trustworthy question answering. D Pride, M Cancellieri, P Knoth, International Conference on Theory and Practice of Digital Libraries. Springer2023</p>
<p>OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. J Priem, H Piwowar, R Orr, arXiv:2205.018332022arXiv preprint</p>
<p>. Yang Qwen, A Yang, B Zhang, B Hui, B Zheng, B Yu, B Li, C Liu, D Huang, F Wei, H Lin, H Yang, J Tu, J Zhang, J Yang, J Yang, J Zhou, J Lin, J Dang, K Lu, K Bao, K Yang, K Yu, L Li, M Xue, M Zhang, P Zhu, Q Men, R Lin, R Li, T Tang, T Xia, T Ren, X Ren, X Fan, Y Su, Y Zhang, Y Wan, Y Liu, Y Cui, Z Zhang, Z Qiu, Z , 2025Qwen2.5 Technical Report</p>
<p>Data extraction methods for systematic review (semi) automation: Update of a living systematic review. L Schmidt, A N F Mutlu, R Elmore, B K Olorisade, J Thomas, J P Higgins, F1000Research. 202510401</p>
<p>Retrieval Augmentation Reduces Hallucination in Conversation. K Shuster, S Poff, M Chen, D Kiela, J Weston, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>SciKGraph: A knowledge graph approach to structure a scientific field. M D L Tosi, J C Reis, 10.1016/j.joi.2020.101109Journal of Informetrics. 1511011092021</p>
<p>Microsoft academic graph: When experts are not enough. K Wang, Z Shen, C Huang, C H Wu, Y Dong, A Kanakia, Quantitative Science Studies. 112020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems. NIPS '22. the 36th International Conference on Neural Information Processing Systems. NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Elicit: AI literature review research assistant. S Whitfield, M A Hofmann, Public Services Quarterly. 1932023</p>
<p>The FAIR Guiding Principles for scientific data management and stewardship. M D Wilkinson, M Dumontier, I J Aalbersberg, G Appleton, M Axton, A Baak, N Blomberg, J W Boiten, L B Da Silva Santos, P E Bourne, Scientific data. 312016</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
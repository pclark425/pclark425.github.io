<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2372 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2372</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2372</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-10bd4160b44803ada6a3d2e366c44b7e2a4ffe90</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/10bd4160b44803ada6a3d2e366c44b7e2a4ffe90" target="_blank">An Explanation of In-context Learning as Implicit Bayesian Inference</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper studies how in-context learning can emerge when pretraining documents have long-range coherence, and proves when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2372",
    "paper_id": "paper-10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00666775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>An Explanation of In-context Learning as Implicit Bayesian Inference</h1>
<p>Sang Michael Xie<br>Stanford University<br>xie@cs.stanford.edu<br>Percy Liang<br>Stanford University<br>pliang@cs.stanford.edu</p>
<p>Aditi Raghunathan<br>Stanford University<br>aditir@stanford.edu<br>Tengyu Ma<br>Stanford University<br>tengyuma@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning ${ }^{1}$. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) such as GPT-3 (Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021) are pretrained on massive text corpora to predict the next word given previous words. They demonstrate the surprising ability to do in-context learning, where an LM "learns" to do a task simply by conditioning on a prompt containing input-output pairs, achieving SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18\% and $3 \%$ over previous SOTA (Brown et al., 2020)). For example, consider the task of predicting nationalities from names. A prompt (Figure 1) is constructed by concatenating independent "training" examples (e.g., "Albert Einstein was German") followed by a "test example" ("Marie Curie was"). Conditioning on this prompt, GPT-3 places the largest probability on the correct output $p($ "Polish" | "Albert Einstein was German $\backslash \mathrm{n}$ Mahatma Gandhi was Indian $\backslash \mathrm{n}$ Marie Curie was")</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ol>
<li>Pretraining documents are conditioned on a latent concept (e.g., biographical text)
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
<li>Concatenate examples into a prompt and predict next word(s). Language model (LM) implicitly infers the shared concept across examples despite the unnatural concatenation</li>
</ol>
<p>Albert Einstein was German \n Mahatma Gandhi was Indian \n Marie Curie was
$\longrightarrow$| LM | $\longrightarrow$ | Polish |
| :-- | :-- | :-- |</p>
<p>Figure 1: In-context learning can emerge from modeling long-range coherence in the pretraining data. During pretraining, the language model (LM) implicitly learns to infer a latent concept (e.g., wiki bios, which typically transition between name (Albert Einstein) $\rightarrow$ nationality (German) $\rightarrow$ occupation (physicist) $\rightarrow$...) shared across sentences in a document. Although prompts are unnatural sequences that concatenate independent examples, in-context learning occurs if the LM can still infer the shared concept across examples to do the task (name $\rightarrow$ nationality, which is part of wiki bios).
by inferring the task from examples. Intruigingly, GPT-3 was not explicitly pretrained to learn from examples, and the distribution of prompts (which concatenate independent examples) is quite different from natural language. Our understanding of in-context learning is limited since (i) real pretraining data is messy and (ii) in-context learning has so far required large-scale datasets and models.
In this paper, we introduce a simple pretraining distribution where in-context learning emerges. To generate a document, we first draw a latent concept $\theta$, which parameterizes the transitions of a Hidden Markov Model (HMM) (Baum and Petrie, 1966), then sample a sequence of tokens from the HMM (Figure 9). This latent variable structure is common in topic models such as LDA (Blei et al., 2003, Gruber et al., 2007). During pretraining, the LM must infer the latent concept across multiple sentences to generate coherent continuations. When conditioning on a prompt, in-context learning occurs when the LM also infers a shared prompt concept across examples to make a prediction. We assume the LM fits the pretraining distribution $p$ exactly with enough data and expressivity, so that the question of in-context learning becomes characterizing the conditional distribution of completions given prompts $p$ (output|prompt) under the pretraining distribution, where the prompt is generated from a different distribution $p_{\text {prompt }}$. This conditional distribution, which is the posterior predictive distribution, marginalizes out the latent concepts:</p>
<p>$$
p(\text { output } \mid \text { prompt })=\int_{\text {concept }} p(\text { output } \mid \text { concept, prompt }) p(\text { concept } \mid \text { prompt }) d(\text { concept })
$$</p>
<p>If $p$ (concept|prompt) concentrates on the prompt concept with more examples, then the LM learns via marginalization by "selecting" the prompt concept. Thus, in-context learning can be viewed as the LM implicitly performing Bayesian inference.
The main challenge is that prompts are sampled from a different distribution than the pretraining distribution. The canonical Bayesian asymptotic tool is the Bernstein-von Mises theorem (Gunst and Shcherbakova, 2008, Kleijn and van der Vaart, 2012, van der Vaart, 1998), which asserts (under regularity conditions) that the posterior distribution of a latent variable concentrates on the maximum likelihood estimate. However, Bernstein-von Mises typically assumes observations are independent and/or drawn from the same distribution as the model, both of which are not satisfied. We prove that despite the distribution mismatch, the asymptotic prediction error of in-context learning is optimal when the signal about the latent concept in each prompt example is larger than the error due to the distribution mismatch. Additionally, we prove that the in-context learning error decreases with the length of each example-thus, information in the inputs, not just the input-output mapping, can be useful for in-context learning.
As a companion to this theory, we created the Generative IN-Context learning dataset (GINC), which is a small-scale synthetic dataset for studying in-context learning. We find that both Transformers (Vaswani et al., 2017) and LSTMs (Hochreiter and Schmidhuber, 1997) trained on GINC exhibit in-context learning. We verify intuitions from the theory, showing that the accuracy of incontext learning improves with the number of examples and example length. Ablations of the GINC dataset show that the latent concept structure in the pretraining distribution is crucial to the emergence of in-context learning.
The experiments also bring up open questions which go beyond our theory, which only studies the pretraining distribution. We find that scaling up the number of model parameters steadily improves the in-context accuracy despite achieving the same pretraining loss, showing that larger models may improve in-context learning beyond increasing the capacity for memorizing the training data better. Previously observed in-context learning phenomena such as sensitivity to example ordering (Zhao et al., 2021) and the existence of settings where zero-shot is better than one/fewshot learning (Brown et al., 2020) are also mirrored in GINC.</p>
<h1>2 In-context learning setting</h1>
<p>Pretraining distribution. In our framework, a latent concept $\theta$ from a family of concepts $\Theta$ defines a distribution over observed tokens $o$ from a vocabulary $\mathcal{O}$. To generate a document, we first sample a concept from a prior $p(\theta)$ and then sample the document given the concept. Each pretraining document is a length $T$ sequence:</p>
<p>$$
p\left(o_{1}, \ldots, o_{T}\right)=\int_{\theta \in \Theta} p\left(o_{1}, \ldots, o_{T} \mid \theta\right) p(\theta) d \theta
$$</p>
<p>We assume $p\left(o_{1}, \ldots, o_{T} \mid \theta\right)$ is defined by a Hidden Markov Model (HMM). The concept $\theta$ determines the transition probability matrix of the HMM hidden states $h_{1}, \ldots, h_{T}$ from a hidden state set $\mathcal{H}$.</p>
<p>Prompt distribution. The prompt distribution $p_{\text {prompt }}$ generates prompts for in-context learning. The prompt is a concatenation of $n$ independent training examples and 1 test input $x_{\text {test }}$, which are all conditioned on a shared prompt concept $\theta^{*}$. The goal is to predict the test output $y_{\text {test }}$ by predicting the next token.
A prompt example is composed of an input token sequence $x$ (e.g., Albert Einstein was) followed by an output token $y$ (e.g., German). In particular, the $i$-th training example $O_{i}$ consists of an input</p>
<p>$x_{i}=O_{i}[1: k-1]$ (the first $k-1$ tokens) followed by an output token $y_{i}=O_{i}[k]$ at the end ${ }^{2}$. The $i$-th training example is independently generated as follows:</p>
<ol>
<li>Generate a start hidden state $h_{i}^{\text {start }}$ from a prompt start distribution $p_{\text {prompt }}$.</li>
<li>Given $h_{i}^{\text {start }}$, generate the example sequence $O_{i}=\left[x_{i}, y_{i}\right]$ from $p\left(O_{i} \mid h_{i}^{\text {start }}, \theta^{<em>}\right)$, the pretraining distribution conditioned on a prompt concept $\theta^{</em>}$.</li>
</ol>
<p>The test input $x_{\text {test }}=x_{n+1}$ is sampled similarly. Between each example, there is a special delimiter token $o^{\text {delim }}$. The prompt consists of a sequence of training examples $\left(S_{n}\right)$ followed by the test example $x_{\text {test }}$ :</p>
<p>$$
\left[S_{n}, x_{\text {test }}\right]=\left[x_{1}, y_{1}, o^{\text {delim }}, x_{2}, y_{2}, o^{\text {delim }}, \ldots, x_{n}, y_{n}, o^{\text {delim }}, x_{\text {test }}\right] \sim p_{\text {prompt }}
$$</p>
<p>Mismatch between prompt and pretraining distributions. Since transitions between independent examples can be unnatural, the prompts are low probability sequences under the pretraining distribution. We provide a simple illustration using the names to nationalities example. Suppose that wiki bio documents in the pretraining data typically transition between name $\rightarrow$ nationality $\rightarrow$ occupation $\rightarrow \ldots$ In the prompt, the examples transition between name $\rightarrow$ nationality $\rightarrow$ name $\rightarrow$ nationality $\rightarrow \ldots$, which contains low-probability transitions such as "German" $\rightarrow$ "Mahatma Gandhi". The prompt formatting (e.g., choice of delimiter) can also be a source of mismatch. We aim to show that despite this mismatch, large LMs can infer the prompt concept from examples.</p>
<p>In-context predictor and task. For in-context learning, the output target $y$ for each example $x$ is sampled according to $p_{\text {prompt }}(y \mid x)$ :</p>
<p>$$
y_{\text {test }} \sim p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)=\mathbb{E}<em _test="{test" _text="\text">{h</em>\right)\right]
$$}}^{\text {start }} \sim p_{\text {prompt }}\left(h_{\text {test }}^{\text {start }} \mid x_{\text {test }}\right)}\left[p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta^{*</p>
<p>where $h_{\text {test }}^{\text {start }}$ denotes the hidden state corresponding to the first token of $x_{\text {test }}$.
We analyze the in-context predictor $f_{n}\left(x_{\text {test }}\right)=\arg \max <em n="n">{y} p\left(y \mid S</em>}, x_{\text {test }}\right)$, which outputs the most likely prediction over the pretraining distribution conditioned on the prompt from the prompt distribution ${ }^{3}$. We study the in-context predictor and its expected $0-1$ error with $n$ examples $L_{0-1}\left(f_{n}\right)=\mathbb{E<em _test="{test" _text="\text">{x</em>\right]\right]$.}}, y_{\text {test }} \sim p_{\text {prompt }}}\left[\mathbf{1}\left[f_{n}\left(x_{\text {test }}\right) \neq y_{\text {test }</p>
<h1>2.1 Assumptions</h1>
<p>We detail the assumptions in our framework, including the structure of delimiters and regularity assumptions. We first assume that there exists a subset of delimiter hidden states $\mathcal{D}$ which generates the special delimiter token $o^{\text {delim }}$ deterministically.</p>
<p>Assumption 1 (Delimiter hidden states). Let the delimiter hidden states $\mathcal{D}$ be a subset of $\mathcal{H}$. For any $h^{\text {delim }} \in \mathcal{D}$ and $\theta \in \Theta, p\left(o^{\text {delim }} \mid h^{\text {delim }}, \theta\right)=1$ and for any $h \notin \mathcal{D}, p\left(o^{\text {delim }} \mid h, \theta\right)=0$.</p>
<p>Thus, observing the delimiter $o^{\text {delim }}$ reveals that the corresponding hidden state is in $\mathcal{D}$, but does not reveal which element of $\mathcal{D}$ it is. The delimiter is usually a token that can appear in a broad range of contexts (e.g., newline). The delimiter ideally does not distract from the examples - for example, an adversarial delimiter could look like part of the input $x$. To mitigate these scenarios, we assume that no delimiter (e.g., newline) is significantly more likely under one concept rather than another.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Assumption 2 (Bound on delimiter transitions). For any delimiter state $h^{\text {delim }} \in \mathcal{D}$ and any hidden state $h \in \mathcal{H}$, the probability of transitioning to a delimiter hidden state under $\theta$ is upper bounded $p\left(h^{\text {delim }} \mid h, \theta\right)&lt;$ $c_{2}$ for any $\theta \in \Theta \backslash\left{\theta^{<em>}\right}$, and is lower bounded $p\left(h^{\text {delim }} \mid h, \theta^{</em>}\right)&gt;c_{1}&gt;0$ for $\theta^{*}$. Additionally, the start hidden state distribution for delimiter hidden states is bounded as $p\left(h^{\text {delim }} \mid \theta\right) \in\left[c_{3}, c_{4}\right]$.</p>
<p>The choice of prompt start distribution can be a source of distribution shift which is separate from the distribution shift from concatenating independent examples. We make an assumption that limits how much distribution shift is introduced by the prompt start distribution.</p>
<p>Assumption 3 (Distribution shift from prompt start distribution). We assume that the prompt start distribution $p_{\text {prompt }}$ is close in TV distance to all hidden transition distributions (under $\theta^{<em>}$ ) starting from a delimiter hidden state: $\max <em _prompt="{prompt" _text="\text">{h^{\text {delim }} \in \mathcal{D}} T V\left(p</em>, \theta^{}}(h) | p\left(h \mid h^{\text {delim }</em>}\right)\right)&lt;\Delta / 4$. Here, $\Delta=p_{\text {prompt }}\left(y_{\text {max }} \mid x_{\text {test }}\right)-$ $\max <em _max="{max" _text="\text">{y \neq y</em>=\arg \max }}} p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)$ is the margin between the most likely label $y_{\max <em _prompt="{prompt" _text="\text">{y} p</em>\right)$ and the second most likely label.}}\left(y \mid x_{\text {test }</p>
<p>Note that even when the maximum TV distance is 0 , there is still distribution shift from concatenating independent examples.
We also assume the prompt concept $\theta^{<em>}$ is in the family $\Theta$, which is a broad set of concepts.
Assumption 4 (Well-specification). The prompt concept $\theta^{</em>}$ is in $\Theta$.
Even though the pretraining distribution is broad, the prompt is still low probability under the pretraining distribution since it concatenates independent examples.
Finally, if the prompt has zero probability under the prompt concept $\theta^{<em>}$, then Bayesian inference will not be able to infer the prompt concept as in Section 3.1. The following are regularity assumptions which mainly ensure that the prompt is not zero probability under $\theta^{</em>}$.</p>
<p>Assumption 5 (Regularity). The pretraining distribution $p$ satisfies: 1) Lower bound on transition probability for the prompt concept $\theta^{<em>}$ : for any pair of hidden states $h, h^{\prime} \in \mathcal{H}, p\left(h \mid h^{\prime}, \theta^{</em>}\right)&gt;c_{5}&gt;0$. 2) Start hidden state is lower bounded: for any $h \in \mathcal{H}, p\left(h \mid \theta^{<em>}\right) \geq c_{8}&gt;0$. 3) All tokens can be emitted: for every symbol o, there is some hidden state $h \in \mathcal{H}$ such that $p\left(o \mid h, \theta^{</em>}\right)&gt;c_{6}&gt;0,4$ ) The prior $p(\theta)$ has support over the entire concept family $\Theta$ and is bounded above everywhere.</p>
<h1>3 Theoretical analysis</h1>
<p>We prove that in the limit of infinite examples, the error of the in-context predictor is optimal if a distinguishability condition holds - the prompt concept $\theta^{*}$ is distinct enough from the other concepts in $\Theta$ (e.g., when $\Theta$ is a discrete set). When distinguishability does not hold (e.g, $\Theta$ is continuousvalued), we show that the expected error still decreases with the length of each example, showing that information in both the inputs and the input-output mapping contribute to in-context learning.</p>
<h3>3.1 High-level approach</h3>
<p>Our goal is to show that $\arg \max <em n="n">{y} p\left(y \mid S</em>\right) \rightarrow \arg \max }, x_{\text {test }<em _prompt="{prompt" _text="\text">{y} p</em>\right)$ as the number of examples $n$ grows. In the following, assume that the prompt has non-zero probability under the pretraining distribution $p$ given $\theta^{}}\left(y \mid x_{\text {test }<em>}$, meaning that $p\left(S_{n}, x_{\text {test }} \mid \theta^{</em>}\right)&gt;0$. We expand $p\left(y \mid S_{n}, x_{\text {test }}\right)$ to analyze its</p>
<p>limit:</p>
<p>$$
\begin{aligned}
p\left(y \mid S_{n}, x_{\text {test }}\right) &amp; =\int_{\theta} p\left(y \mid S_{n}, x_{\text {test }}, \theta\right) p\left(\theta \mid S_{n}, x_{\text {test }}\right) d \theta \
&amp; \propto \int_{\theta} p\left(y \mid S_{n}, x_{\text {test }}, \theta\right) p\left(S_{n}, x_{\text {test }} \mid \theta\right) p(\theta) d \theta \quad \text { (Bayes' rule, drop the constant } \left.\frac{1}{p\left(S_{n}, x_{\text {test }}\right)}\right) \
&amp; =\int_{\theta} \sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta\right) \frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{*}\right)} p(\theta) d \theta
\end{aligned}
$$</p>
<p>(Law of total prob, Markov property, divide by $p\left(S_{n}, x_{\text {test }} \mid \theta^{*}\right)$ (a constant))</p>
<p>$$
=\int_{\theta} \sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta\right) \exp \left(n \cdot r_{n}(\theta)\right) p(\theta) d \theta
$$</p>
<p>where $r_{n}(\theta)=\frac{1}{n} \log \frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{<em>}\right)}$. In Theorem 1, we prove that under a distinguishability condition, $\exp \left(n \cdot r_{n}(\theta)\right) \rightarrow 0$ for all concepts $\theta$ except the prompt concept $\theta^{</em>}$, where $\exp \left(n \cdot r_{n}\left(\theta^{<em>}\right)\right)=1$. The only nonzero term in the integral is when $\theta=\theta^{</em>}$, and thus the prompt concept is "selected" as a consequence of Bayesian inference ${ }^{4}$. Lemma 1 shows that the argmax after restricting to $\theta^{<em>}$ is the same as the most likely label under $p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)$ (using Assumption 3). Putting these together with Equation 6, the in-context predictor infers the prompt concept $\theta^{</em>}$ :</p>
<p>$$
\underset{y}{\arg \max } p\left(y \mid S_{n}, x_{\text {test }}\right) \rightarrow \underset{y}{\arg \max } p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)
$$</p>
<p>Thus, the in-context predictor is optimal as the number of in-context examples increases.</p>
<h1>3.2 Heuristic derivation</h1>
<p>Recall from Section 3.1 that if $\exp \left(n \cdot r_{n}(\theta)\right) \rightarrow 0$ for all $\theta \neq \theta^{<em>}$, then Bayesian inference "selects" the prompt concept through marginalization. To do this, we focus on showing that $r_{n}(\theta)$, the average log-likelihood ratio between $\theta$ and $\theta^{</em>}$, converges to a negative constant, and thus $n r_{n}$ goes to $-\infty$.
The main technical challenge is to handle the sequence-of-examples structure of the prompt, which makes all the examples dependent with respect to the pretraining distribution. Our approach uses properties of delimiter tokens to approximately factorize the examples, with constant error per example. We let $O_{i}^{\mathrm{ex}}=\left[\sigma_{i-1}^{\text {delim }}, O_{i}\right]$ be the $i$-th input-output pair and the previous delimiter together for $i&gt;1$ and define $O_{1}^{\mathrm{ex}}=O_{1}$. Expanding the likelihood term inside $r_{n}(\theta)$, our goal is to show</p>
<p>$$
p\left(S_{n}, x_{\text {test }} \mid \theta\right)=p\left(x_{\text {test }} \mid S_{n}, \theta\right) p\left(S_{n} \mid \theta\right) \approx \prod_{i=1}^{n} O(1) p\left(O_{i} \mid \theta\right)
$$</p>
<p>To show this, we expand $p\left(S_{n} \mid \theta\right)$ with the chain rule, and with Assumption 5 (to bound $p\left(x_{\text {test }} \mid S_{n}, \theta\right)$ by $O(1)$ ) it can be shown that</p>
<p>$$
p\left(x_{\text {test }} \mid S_{n}, \theta\right) p\left(S_{n} \mid \theta\right) \approx \prod_{i=1}^{n} O(1) p\left(O_{i}^{\mathrm{ex}} \mid O_{1: i-1}^{\mathrm{ex}}, \theta\right)
$$</p>
<p>We then marginalize $p\left(O_{i}^{\mathrm{ex}} \mid O_{1: i-1}^{\mathrm{ex}}, \theta\right)$ over the hidden state $h_{i-1}^{\text {delim }}$ corresponding to the delimiter in $O_{i}^{\mathrm{ex}}=\left[\sigma_{i-1}^{\text {delim }}, O_{i}\right]$ :</p>
<p>$$
\prod_{i=1}^{n} O(1) p\left(O_{i}^{\mathrm{ex}} \mid O_{1: i-1}^{\mathrm{ex}}, \theta\right)=\prod_{i=1}^{n} O(1) \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right) p\left(h_{i-1}^{\text {delim }} \mid O_{1: i-1}^{\mathrm{ex}}, \theta\right) \approx \prod_{i=1}^{n} O(1) p\left(O_{i} \mid \theta\right)
$$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>While summing over $\mathcal{H}$ above would be a trivial equality, we can replace $\mathcal{H}$ with the set of delimiter hidden states $\mathcal{D}$ since $p\left(h \mid O_{1: i-1}^{\text {ex }}, \theta\right)=0$ for non-delimiter hidden states $h \notin \mathcal{D}$ (Assumption 1). We used in the first equality that $O_{1: i-1}^{\text {ex }} \rightarrow h_{i-1}^{\text {delim }} \rightarrow O_{i}^{\text {ex }}$ forms a Markov chain and $p\left(o_{i-1}^{\text {delim }} \mid h_{i-1}^{\text {delim }}\right)=1$ (Assumption 1) to change $O_{i}^{\text {ex }}$ to $O_{i}$. Finally, we can show using properties of delimiter hidden states (Assumption 2) that $p\left(h_{i-1}^{\text {delim }} \mid O_{1: i-1}^{\text {ex }}, \theta\right)=O(1)$ and $\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right) \approx O(1) p\left(O_{i} \mid \theta\right)$ in the second step. Therefore, we can upper bound $r_{n}(\theta)$ as</p>
<p>$$
r_{n}(\theta) \leq \frac{1}{n}\left(O(n)+\sum_{i=1}^{n} \log \frac{p\left(O_{i} \mid \theta\right)}{p\left(O_{i} \mid \theta^{<em>}\right)}\right) \rightarrow O(1)+\mathbb{E}<em _prompt="{prompt" _text="\text">{O \sim p</em>{p\left(O \mid \theta^{}}}\left[\log \frac{p(O \mid \theta)</em>}\right)}\right]
$$</p>
<p>The expectation term can be written as the difference of two KL divergences, $K L\left(p_{\text {prompt }}(O) | p\left(O \mid \theta^{<em>}\right)\right)-K L\left(p_{\text {prompt }}(O) | p(O \mid \theta)\right)$. We bound the first KL term by a constant using Assumption 5 - intuitively for one example, $p_{\text {prompt }}$ and $p\left(\cdot \mid \theta^{</em>}\right)$ are close. We break the second term into a sum of negative KL divergences over $k$ tokens. There are $O(k)$ KL terms and only $O(1)$ other error terms, which come from the distribution mismatch between the prompt and pretraining distributions. If the KL terms are larger than the error terms, then $r_{n}(\theta)$ has a negative limit. If this holds for all $\theta \neq \theta^{<em>}$, then we have $\exp \left(n \cdot r_{n}(\theta)\right) \rightarrow 0$ for all $\theta \neq \theta^{</em>}$, enabling in-context learning.</p>
<h1>3.3 Formal results</h1>
<h3>3.3.1 In-context learning under distinguishability</h3>
<p>We define a distinguishability condition which formalizes when in-context learning occurs. Letting $p_{\theta}^{j}(o):=p(O[j]=o \mid O[1: j-1], \theta)$ be the output distribution of the $j$-th token given the previous tokens and $p_{\text {prompt }}^{j}(o):=p_{\text {prompt }}(O[j]=o \mid O[1: j-1])$ be the analogous distribution under the prompt distribution, the distinguishability condition depends on the KL divergence between $p_{\text {prompt }}^{j}$ (which represents $\theta^{*}$ ) and $p_{\theta}^{j}$ as well as error terms $\epsilon_{\text {start }}^{\theta}$ and $\epsilon_{\text {delim }}^{\theta}$ coming from the distribution mismatch between the prompt and pretraining distributions at the start and delimiter token for each example:</p>
<p>$$
\begin{gathered}
K L_{j}\left(\theta^{*} | \theta\right):=\mathbb{E}<em _prompt="{prompt" _text="\text">{O[1: j-1] \sim p</em>\right)\right] \
\epsilon_{\text {delim }}^{\theta}:=2\left(\log \left(c_{2}\right)-\log \left(c_{1}\right)\right)+\log \left(c_{4}\right)-\log \left(c_{3}\right), \quad \epsilon_{\text {start }}^{\theta}:=\log \left(1 / c_{8}\right)
\end{gathered}
$$}}}\left[K L\left(p_{\text {prompt }}^{j} | p_{\theta}^{j</p>
<p>Condition 1 (Distinguishability). We define $\theta^{<em>}$ to be distinguishable if for all $\theta \in \Theta, \theta \neq \theta^{</em>}$,</p>
<p>$$
\sum_{j=1}^{k} K L_{j}\left(\theta^{*} | \theta\right)&gt;\epsilon_{\text {start }}^{\theta}+\epsilon_{\text {delim }}^{\theta}
$$</p>
<p>When the signal from KL divergence (LHS) is larger than the error terms, Equation 14 is satisfied (Figure 2). For larger example lengths $k$, the LHS increases, improving distinguishability. Intuitively, larger example lengths increase the proportion of the prompt sampled from the pretraining distribution by providing more evidence for Bayesian inference. Under Condition 1, the in-context predictor asymptotically achieves the optimal expected error.
Theorem 1. Assume the assumptions in Section 2.1 hold. If Condition 1 holds, then as $n \rightarrow \infty$ the prediction according to the pretraining distribution is</p>
<p>$$
\underset{y}{\arg \max } p\left(y \mid S_{n}, x_{\text {test }}\right) \rightarrow \underset{y}{\arg \max } p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)
$$</p>
<p>Thus, the in-context predictor $f_{n}$ achieves the optimal 0-1 risk: $\lim <em 0-1="0-1">{n \rightarrow \infty} L</em>\right)=\inf }\left(f_{n<em 0-1="0-1">{f} L</em>(f)$.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: When the signal about the prompt concept within each example (green) is greater than the error from low-probability transitions between examples, in-context learning succeeds in our latent concept setting (Theorem 1). Increasing the example length $k$ increases the signal. The signal for in-context learning comes from tokens in both the inputs and the input-output mapping.</p>
<h1>3.3.2 Non-distinguishable case</h1>
<p>The distinguishability condition (Condition 1) fails when there is some $\theta \neq \theta^{<em>}$ for which the KL divergence between $\theta$ and $\theta^{</em>}$ is less than the error terms. However, this also means that the output distributions of $\theta$ and $\theta^{*}$ are close in KL. We leverage this to prove that the expected 0-1 error decreases with the example length $k$ under two different settings where distinguishability does not hold.</p>
<p>Continuity. Our first result relies on a continuity assumption between the concept parameter and its corresponding output distribution. Our assumption is based on prior works (Kleijn and van der Vaart, 2012), where the KL divergence is assumed to have a 2nd-order Taylor expansion.</p>
<p>Theorem 2. Let the set of $\theta$ which does not satisfy Equation 14 in Condition 1 to be $\mathcal{B}$. Assume that KL divergences have a 2nd-order Taylor expansion around $\theta^{*}$ :</p>
<p>$$
\forall j&gt;1, \quad K L_{j}\left(\theta^{<em>} | \theta\right)=\frac{1}{2}\left(\theta-\theta^{</em>}\right)^{\top} I_{j, \theta^{<em>}}\left(\theta-\theta^{</em>}\right)+O\left(\left|\theta-\theta^{*}\right|^{3}\right)
$$</p>
<p>where $I_{j, \theta^{<em>}}$ is the Fisher information matrix of the $j$-th token distribution with respect to $\theta^{</em>}$. Let $\gamma_{\theta^{<em>}}=$ $\frac{\max <em _max="{max" _text="\text">{j} \lambda</em>\left(I_{j, \theta^{}</em>}}\right)}{\min <em _min="{min" _text="\text">{j} \lambda</em>$ is bounded as}}\left(I_{j, \theta^{*}}\right)}$ where $\lambda_{\text {max }}, \lambda_{\text {min }}$ return the largest and smallest eigenvalues. Then for $k \geq 2$ and as $n \rightarrow \infty$, the 0-1 risk of the in-context learning predictor $f_{n</p>
<p>$$
\lim <em 0-1="0-1">{n \rightarrow \infty} L</em>\right) \leq \inf }\left(f_{n<em 0-1="0-1">{f} L</em> \sup }(f)+g^{-1}\left(O\left(\frac{\gamma_{\theta^{*}<em _start="{start" _text="\text">{\theta \in \mathcal{B}}\left(\epsilon</em>\right)\right)
$$}}^{\theta}+\epsilon_{\text {delim }}^{\theta}\right)}{k-1</p>
<p>where $g(\delta)=\frac{1}{2}((1-\delta) \log (1-\delta)+(1+\delta) \log (1+\delta))$ is a calibration function (Steinwart, 2007, Ávila Pires and Szepesvári, 2016) for the multiclass logistic loss for $\delta \in[0,1)$, assuming that the minimizers of the 0-1 risk and multiclass logistic risk are the same.</p>
<p>Since the inverse calibration function $g^{-1}$ is roughly linear in $\epsilon$ for $\epsilon \leq 0.7$, the excess risk roughly decreases as $O(1 / k)$. When the "worst-case condition number" $\gamma_{\theta^{<em>}}$ of the Fisher information matrices is smaller (well-conditioned), the error decreases. Intuitively, this means that there is no direction to vary $\theta^{</em>}$ in which the output distribution will sharply change. As a consequence, the concepts $\theta$ that are not distinguishable from the prompt concept $\theta^{*}$ parameterize distributions that produce similar outputs to the prompt concept and thus achieve a small error.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: In-context accuracy (95% intervals) of Transformers (left) and LSTMs (right) on the GINC dataset. Accuracy increases with number of examples $n$ and length of each example $k$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablation studies for 4 layer Transformers on the GINC dataset with vocab size 50. (Left) When pretrained with only one concept, in-context learning fails. (Middle) When the pretraining data has random transitions, the model sees all token transitions but in-context learning fails. (Right) When prompts are from random unseen concepts, in-context learning fails to extrapolate.</p>
<p><strong>Varying-length test examples.</strong> In the setting where the length of $x_{\text{test}}$ is random (uniformly from 2 to $k$), we can give a similar error guarantee without continuity.</p>
<p><strong>Theorem 3.</strong> <em>Let the set of $\theta$ which does not satisfy Equation 14 in Condition 1 to be $\mathcal{B}$. Let the length of the test example $x_{\text{test}}$ be uniformly distributed between 2 and $k$, for $k \geq 2$. Then for $k \geq 2$ and as $n \to \infty$, the 0-1 risk of the in-context learning predictor $f_n$ is bounded as</em></p>
<p>$$\lim_{n \to \infty} L_{0-1}(f_n) \leq \inf_{f} L_{0-1}(f) + g^{-1} \left( O\left(\frac{\sup_{\theta \in \mathcal{B}}(\epsilon_{\text{start}}^t + \epsilon_{\text{delim}}^t)}{k - 1} \right) \right), \tag{18}$$</p>
<p><em>assuming that the minimizers of the 0-1 risk and multiclass logistic risk are the same.</em></p>
<p>Instead of measuring only the error at the $k$-th token, we average the prediction error on the 2nd to $k$-th tokens. However, we leave bridging the mismatch between training examples, which are consistently length $k$, and test examples, which have random length, to future work.</p>
<h2>4 Simulations</h2>
<p>We generate the GINC dataset and show that Transformers (Vaswani et al., 2017) and LSTMs (Hochreiter and Schmidhuber, 1997) trained on GINC exhibit in-context learning. In the theory, we assumed that the pretrained LM fits the pretraining distribution exactly. Here, we pretrain LMs to approximate the pretraining distribution, showing that the in-context learning properties of the pretraining distribution transfer to the LM.</p>
<p>GINC dataset. We construct the GINC dataset according to our theory (see Appendix F.1). For pretraining, we define a uniform mixture of HMMs over a family $\Theta$ of 5 concepts to generate 1000 pretraining documents with $\sim 10$ million tokens total. For prompting, we generate prompts with 0 to 64 training examples and example lengths $k \in{3,5,8,10}$ (2500 prompts for each setting). The target token $y_{\text {test }}$ is taken to be the most likely output $\arg \max <em _prompt="{prompt" _text="\text">{y} p</em>\right)$ instead of sampling so that the intrinsic error is 0 .}}\left(y \mid x_{\text {test }</p>
<p>Main result. We train GPT-2-based Transformers (Radford et al., 2019) and LSTMs on three versions of the GINC dataset with vocabulary sizes 50, 100, and 150, then evaluate the in-context accuracy (see Appendix F.2, F.3). We average all results over 5 pretraining runs. Figure 3 shows that for both Transformer and LSTMs, in-context accuracy improves as the number of prompt examples $n$ and the example length $k$ increase, verifying our theory.</p>
<p>Ablations on the latent concept structure. We ablate the role of the mixture-of-concepts structure in GINC. In Figure 4 (left), we pretrain a 4 layer Transformer on data with only one concept (removing the prior) from $\Theta$, resulting in flat in-context learning curves. Figure 4 (middle) shows that pretraining on random pretraining data, which contains all possible token transitions, in-context learning also fails. Therefore, the mixture-of-concepts structure is important and simply seeing diverse token transitions does not enable in-context learning.</p>
<p>Extrapolation to unseen concepts. Full generative control of GINC allows for experimentation with latent variables in the pretraining distribution. For example, in large-scale datasets, it is difficult to test whether a concept or task is in the pretraining data. We test this in GINC by testing the in-context accuracy of a 4 layer Transformer on prompts generated from 5 random concepts that are not in the pretraining family of concepts. Figure 4 (right) shows that in-context learning also fails for these novel concepts.</p>
<p>Effect of model size and architecture. Figure 5 shows that increasing the size of the Transformer (4, 12, 16 layers) steadily increases the in-context accuracy, corroborating the results of Brown et al. (2020). Table 6 shows that even though larger Transformers may have the same pretraining loss (e.g., 12 and 16 layer Transformers both get 1.33 validation loss for vocab size 50), the in-context accuracy still improves ( $81 \%$ to $85 \%$ from 12 to 16 layers), suggesting that larger models can improve in-context learning beyond improving pretraining perplexity. This may be related to phenomena from overparameterization and overtraining (Power et al., 2021, Zhang et al., 2017). Finally, the model architecture also plays a role - LSTMs consistently outperform Transformers on GINC despite having fewer parameters, perhaps due to the similarity between HMMs and LSTMs. We leave analysis of the effect of model scaling and model architecture as open questions.</p>
<p>Sensitivity to example ordering. In Figure 7 (left), we test the sensitivity of in-context accuracy on GINC to the ordering of the prompt examples, following Zhao et al. (2021). For this experiment, we consider prompts generated from a single concept and prompt start distribution. We sample 10 different sets (leading to 10 training set IDs) of 4 examples and generate all 24 possible permutations for each example set. We consider the in-context accuracy of the 4 layer Transformer trained on GINC with vocabulary size 50. Similarly to the behavior of GPT-3 (Zhao et al., 2021), there is a significant variation ( $10-40 \%$ difference) between permutations of the same set of examples.</p>
<p>Zero-shot is sometimes better than few-shot. In some settings in GINC, we find that zero-shot performance can be better than few-shot performance. This mirrors GPT-3 on some datasets (e.g., LAMBADA, HellaSwag, PhysicalQA, RACE-m, CoQA/SAT analogies for smaller models (Brown</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: In-context accuracy (95% intervals) of Transformers improves as model size increases on the GINC dataset for vocabulary sizes 50, 100, and 150.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<table>
<thead>
<tr>
<th>Model</th>
<th># Params</th>
<th>Train loss (pretraining)</th>
<th>Val loss (pretraining)</th>
<th>In-context Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vocab size 50, k = 10, n = 64</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Transformer (4 layer)</td>
<td>29M</td>
<td>1.49</td>
<td>1.50</td>
<td>60.2 ± 5.7</td>
</tr>
<tr>
<td>Transformer (12 layer)</td>
<td>85M</td>
<td>1.31</td>
<td>1.33</td>
<td>81.2 ± 7.1</td>
</tr>
<tr>
<td>Transformer (16 layer)</td>
<td>115M</td>
<td>1.31</td>
<td>1.33</td>
<td>84.7 ± 5.4</td>
</tr>
<tr>
<td>LSTM</td>
<td>28M</td>
<td>1.31</td>
<td>1.35</td>
<td>95.8 ± 1.11</td>
</tr>
<tr>
<td>Vocab size 100, k = 10, n = 64</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Transformer (4 layer)</td>
<td>29M</td>
<td>1.58</td>
<td>1.59</td>
<td>67.4 ± 4.7</td>
</tr>
<tr>
<td>Transformer (12 layer)</td>
<td>85M</td>
<td>1.40</td>
<td>1.42</td>
<td>84.6 ± 3.0</td>
</tr>
<tr>
<td>Transformer (16 layer)</td>
<td>115M</td>
<td>1.41</td>
<td>1.43</td>
<td>88.7 ± 1.6</td>
</tr>
<tr>
<td>LSTM</td>
<td>28M</td>
<td>1.43</td>
<td>1.44</td>
<td>95.8 ± 1.54</td>
</tr>
<tr>
<td>Vocab size 150, k = 10, n = 64</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Transformer (4 layer)</td>
<td>29M</td>
<td>1.44</td>
<td>1.45</td>
<td>92.8 ± 1.9</td>
</tr>
<tr>
<td>Transformer (12 layer)</td>
<td>85M</td>
<td>1.27</td>
<td>1.28</td>
<td>98.4 ± 0.4</td>
</tr>
<tr>
<td>Transformer (16 layer)</td>
<td>115M</td>
<td>1.27</td>
<td>1.28</td>
<td>98.1 ± 0.5</td>
</tr>
<tr>
<td>LSTM</td>
<td>28M</td>
<td>1.26</td>
<td>1.31</td>
<td>99.2 ± 1.06</td>
</tr>
</tbody>
</table>
<p>Figure 6: In-context accuracies (95% intervals) on GINC with vocab sizes (50, 100, 150) for Transformers and LSTMs. Accuracy improves with scale even though the pretraining loss may be the same.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: (Left) In-context accuracy varies widely with example ordering. Each training ID refers to a set of training examples. Each dot refers to the in-context learning accuracy of one permutation of the training examples for that particular training ID. (Right) Zero-shot performance can be higher than one/few-shot performance in some settings in GINC, mirroring the behavior of GPT-3 on some datasets such as LAMBADA (Brown et al., 2020). The few-shot setting introduces the distracting prompt structure, which can initially lower accuracy.</p>
<p>et al., 2020)). This occurs especially when the transition probabilities in GINC are lower entropy (controlled via a temperature parameter). For this experiment, we consider GINC with transition matrix temperature parameter 0.01 (instead of 0.1), 12 concepts, and vocabulary size 100. Figure 7 (right) shows that here, few-shot accuracy is initially worse than zero-shot accuracy, but can recover with more examples. We hypothesize that the distracting prompt structure initially decreases the accuracy in this setting.</p>
<h2>5 Discussion and related work</h2>
<p><strong>Learning via Bayesian inference and extrapolation.</strong> The canonical Bernstein-von Mises theorem (van der Vaart, 1998) does not apply for in-context learning since the prompt examples are not independent under the pretraining distribution. Gunst and Shcherbakova (2008) show a Bernstein-von Mises-type result for observations from an HMM, but do not handle observations from a dif</p>
<p>ferent distribution. Future directions include more precise asymptotic results about the posterior distribution and results under misspecification/extrapolation Kleijn and van der Vaart, 2012). A possible avenue for extrapolation to some types of unseen concepts is to factorize the latent concept into semantics and syntax. While the pretraining data may contain only some semantics-syntax pairs, the language model could generalize to unseen pairs if it learns generalizable syntactical operations such as copying or reordering.</p>
<p>Topic models and HMMs. Topic models such as LDA (Blei et al., 2003) also have document-level latent variables, but learning is typically relies on algorithms such as EM (Dempster et al., 1977), variational inference (Jordan et al., 1999), or MCMC (Hastings, 1970, Metropolis et al., 1953). We focus on learning as a natural result of Bayesian inference without an explicit inference algorithm. Wei et al. (2021a) also use an HMM model in their pretraining analysis. However, they analyze how pre-trained representations learned with masked LMs (Clark et al., 2020, Devlin et al., 2019, Lewis et al., 2020, Liu et al., 2019) can improve optimization-based downstream learning (Lester et al., 2021, Li and Liang, 2021) rather than in-context learning.</p>
<p>Bridging the mismatch between pretraining and prompting. Prior works support our theoretical intuitions that reducing the prompt distribution mismatch would improve in-context learning. Finetuning LMs on text with a prompting format improves its zero-shot performance (Sanh et al., 2021, Wei et al., 2021b) and optimizing prompt templates improves few-shot finetuning (Gao et al., 2021, Jiang et al., 2020, Schick and Schütze, 2021, Shin et al., 2020). Holtzman et al. (2021), Zhao et al. (2021) improve in-context accuracy via calibration or renormalization, a form of adaptation to the prompt distribution.</p>
<p>Meta-learning. Meta-learning methods can also train a sequence model to learn from examples (Ravi and Larochelle, 2017). However, meta-learning models are trained to learn, while incontext learning emerges from LM pretraining.</p>
<p>Studying large-scale phenomena at a small scale. We can study in-context learning, a large scale phenomenon, at a small scale in GINC because the complexity of the pretraining distribution (HMM hidden state size, number of latent concepts) is small, such that the data and models are relatively larger. Since GINC is synthetic, we can also control the latent data properties (e.g., unseen concepts) to make predictions about large LMs while working at a small scale.</p>
<h1>6 Conclusion</h1>
<p>We cast in-context learning as implicit Bayesian inference, where the pretrained LM implicitly infers a concept when making a prediction. We show that in-context learning occurs when the pretraining distribution is a mixture of HMMs. Our work provides a first step towards understanding in-context learning, which we hope will provide insight for improving pretraining and prompting.</p>
<h2>Acknowledgements</h2>
<p>We thank Tianyi Zhang, Frieda Rong, Lisa Li, Colin Wei, Shibani Santurkar, Tri Dao, Ananya Kumar, and Shivam Garg for helpful discussions and feedback. SMX is supported by an NDSEG Fellowship. The work is partially supported by an Open Philanthropy Project Award, SDSI, and SAIL at Stanford University. TM acknowledges support of Google Faculty Award, NSF IIS 2045685, the Sloan Fellowship, and JD.com. Toyota Research Institute provided funds to support this work.</p>
<h1>References</h1>
<p>Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554-1563, 1966.
D. Blei, Andrew Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research (JMLR), 3:993-1022, 2003.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations (ICLR), 2020.
A. P. Dempster, Laird N. M., and Rubin D. B. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1-38, 1977.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Association for Computational Linguistics (ACL), pages 4171-4186, 2019.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv, 2021.</p>
<p>Zoubin Ghahramani and Michael Jordan. Factorial hidden Markov models. Machine Learning, 29: 245-273, 1997.</p>
<p>Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. Hidden topic Markov models. In Artificial Intelligence and Statistics (AISTATS), 2007.
M. Gunst and O. Shcherbakova. Asymptotic behavior of Bayes estimators for hidden Markov models with application to ion channels. Mathematical Methods of Statistics, 17, 2008.</p>
<p>Keith W. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97-109, 1970.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735-1780, 1997.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn't always right, 2021.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? In Association for Computational Linguistics (ACL), 2020.</p>
<p>Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37:183-233, 1999.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Association for Computational Linguistics (ACL), 2017.</p>
<p>Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
B.J.K. Kleijn and A.W. van der Vaart. The Bernstein-von mises theorem under misspecification. Electronic Journal of Statistics, 6, 2012.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Association for Computational Linguistics (ACL), 2020.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Association for Computational Linguistics (ACL), 2021.</p>
<p>Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs, August 2021.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019.</p>
<p>Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087-1092, 1953.</p>
<p>Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Association for Computational Linguistics (ACL), 2016.</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. In ICLR MATH AI Workshop, 2021.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.</p>
<p>Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR), 2017.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2021.</p>
<p>Timo Schick and Hinrich Schütze. Exploiting cloze questions for few shot text classification and natural language inference. In European Association for Computational Linguistics (EACL), 2021.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Eliciting knowledge from language models using automatically generated prompts. In Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26, 2007.
A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. arXiv, 2021a.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv, 2021b.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.</p>
<p>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017.</p>
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (ICML), 2021.</p>
<p>Bernardo Ávila Pires and Csaba Szepesvári. Multiclass classification calibration functions. arXiv, 2016.</p>
<h1>A Framework details</h1>
<p>Prompt distribution details. For in-context learning, we sample a prompt from a new distribution $p_{\text {prompt }}$, which consists of $n$ independent training examples and 1 test example. We first sample $n$ hidden segments $H$ of length $k$ by sampling the first element $h^{\text {start }}=H[1]$ from a prompt start distribution $p_{\text {prompt }}$. Then, we sample the rest of the segment $H^{\text {seg }}=H[2: k]$ from the hidden transition distribution of the pretraining distribution $p$ corresponding to a particular concept $\theta^{*}$ :</p>
<p>$$
\begin{aligned}
H_{1}, \ldots, H_{n}, &amp; H_{i}=\left[h_{i, 1}, \ldots, h_{i, k}\right] \
h_{i}^{\text {start }}=H_{i}[1] \sim p_{\text {prompt }}, &amp; H_{i}^{\text {seg }}=H_{i}[2: k] \sim p\left(H_{i}^{\text {seg }} \mid h^{\text {start }}, \theta^{*}\right)
\end{aligned}
$$</p>
<p>To end each example (except the test example), we sample $n$ delimiters $h^{\text {delim }} \in \mathcal{D}$ from $p_{\text {prompt }}^{\text {delim }}$ :</p>
<p>$$
h_{1}^{\text {delim }}, \ldots, h_{n}^{\text {delim }}, \quad h_{i}^{\text {delim }} \sim p_{\text {prompt }}^{\text {delim }}
$$</p>
<p>Conditioned on hidden variables $H_{i}$ and $h_{i}^{\text {delim }}$, we sample the observed tokens $O_{i}=\left[o_{i, 1}, \ldots, o_{i, k}\right]$ and $o_{i}^{\text {delim }}$ respectively from the pre-training distribution:</p>
<p>$$
\begin{aligned}
O_{1}, \ldots, O_{n}, &amp; O_{i} \sim p\left(O_{i} \mid H_{i}\right) \
o_{1}^{\text {delim }}, \ldots, o_{n}^{\text {delim }}, &amp; o_{i}^{\text {delim }} \sim p\left(o_{i}^{\text {delim }} \mid h_{i}^{\text {delim }}, \theta^{*}\right)
\end{aligned}
$$</p>
<p>The "input" for each example is $x_{i}=O_{i}[1: k-1]$ and the "output" is $y_{i}=O_{i}[k]$. Taking $S$ to be the sequence of training examples (without the test example), the resulting prompt sequence is
$\left[S_{n}, x_{\text {test }}\right]=\left[O_{1}, o_{1}^{\text {delim }}, \ldots, O_{n}, o_{n}^{\text {delim }}, x_{\text {test }}\right]=\left[x_{1}, y_{1}, o_{1}^{\text {delim }}, x_{2}, y_{2}, o_{2}^{\text {delim }}, \ldots, x_{n}, y_{n}, o_{n}^{\text {delim }}, x_{\text {test }}\right] \sim p_{\text {prompt }}$
where $x_{\text {test }}=x_{n+1}=O_{n+1}[1: k-1]$ is sampled via the same process but with $k-1$ elements.</p>
<h2>B Propositions for Theorem 1</h2>
<p>The following propositions, which lower bound the probability of a delimiter token and probability of an example under $\theta^{<em>}$, are direct corollaries of the assumptions.
Proposition 1. For all $i$, we have $p\left(h_{i}^{\text {delim }} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta^{</em>}\right)&gt;c_{1}$ and $p\left(h_{i}^{\text {delim }} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta\right)&lt;$ $c_{2}$.</p>
<p>Proof. By Assumption 2,</p>
<p>$$
\begin{aligned}
p\left(h_{i}^{\text {delim }} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta\right) &amp; =\sum_{h_{i, k}} p\left(h_{i}^{\text {delim }} \mid h_{i, k}\right) p\left(h_{i, k} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta\right) \
&amp; &lt;\sum_{h_{i, k}} c_{2} p\left(h_{i, k} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta\right)=c_{2}
\end{aligned}
$$</p>
<p>Similarly,</p>
<p>$$
\begin{aligned}
p\left(h_{i}^{\text {delim }} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta^{<em>}\right) &amp; =\sum_{h_{i, k}} p\left(h_{i}^{\text {delim }} \mid h_{i, k}\right) p\left(h_{i, k} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta^{</em>}\right) \
&amp; &gt;\sum_{h_{i, k}} c_{1} p\left(h_{i, k} \mid O_{1}, o_{1}^{\text {delim }}, \ldots, O_{i}, \theta^{*}\right)=c_{1}
\end{aligned}
$$</p>
<p>Proposition 2. The probability of an example is lower bounded for $\theta^{<em>}$ : there is some $c_{7}&gt;0$ such that $p\left(O_{i} \mid h_{i}^{\text {start }}, h_{j, l}, \theta^{</em>}\right)&gt;c_{7}$ for all $i$ and future hidden states $h_{j, l}$, for any $l$ and $j&gt;i$.</p>
<p>Proof. By Assumption 5, we have</p>
<p>$$
p\left(O_{i} \mid h_{i}^{\text {start }}, h_{j, l}, \theta^{<em>}\right)=\sum_{H_{i}} p\left(O_{i} \mid H_{i}\right) p\left(H_{i} \mid h_{i}^{\text {start }}, h_{j, l}, \theta^{</em>}\right)&gt;\left(c_{6}\right)^{k}
$$</p>
<p>for some $H_{i}$. We have</p>
<p>$$
p\left(H_{i} \mid h_{i}^{\text {start }}, h_{j, l}, \theta^{<em>}\right)=\frac{p\left(h_{j, l} \mid H, h_{i}^{\text {start }}, \theta^{</em>}\right) p\left(H \mid h_{i}^{\text {start }}, \theta^{<em>}\right)}{p\left(h_{j, l} \mid h_{i}^{\text {start }}, \theta^{</em>}\right)}&gt;c_{5}^{2}
$$</p>
<p>which lower bounds the terms in the numerator by $c_{5}$ (marginalizing over previous hidden states), and upper bounding the denominator by 1 . Setting $c_{7}=\left(c_{6}\right)^{k} c_{5}^{2}$ finishes the proof.</p>
<h1>C Convergence of the in-context predictor</h1>
<p>Under Assumption 3, we show that the in-context predictor $f_{n}\left(x_{\text {test }}\right)=\arg \max <em n="n">{y} p\left(y \mid S</em>$ from $\Theta$ ) of the in-context predictor. We will complete the argument for the convergence of the in-context predictor in the proof of Theorem 1.}, x_{\text {test }}\right)$ converges when abstracting away the Bayesian inference component (the selection of $\theta^{*</p>
<p>Lemma 1. Suppose the prompt $S_{n}$ and the test input $x_{\text {test }}$ are given. Under Assumption 3, we show that the argmax of the averaged predictive distribution conditioned on $\theta^{*}$ and a prompt $S_{n}$ is the same as the argmax of the prompt predictive distribution:</p>
<p>$$
\underset{y}{\arg \max } \sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta^{<em>}\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta^{</em>}\right)=\underset{y}{\arg \max } p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)
$$</p>
<p>Proof. First, we note by definition that</p>
<p>$$
p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)=\sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta^{*}\right) p_{\text {prompt }}\left(h_{\text {test }}^{\text {start }} \mid x_{\text {test }}\right)
$$</p>
<p>Expanding the last term, we have</p>
<p>$$
p_{\text {prompt }}\left(h_{\text {test }}^{\text {start }} \mid x_{\text {test }}\right) \propto p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta^{*}\right) p_{\text {prompt }}\left(h_{\text {test }}^{\text {start }}\right)
$$</p>
<p>which is proportional to a constant in $x_{\text {test }}$.
On the other hand, analyzing one term inside the LHS of the lemma statement, we have</p>
<p>$$
p\left(h^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta^{<em>}\right) \propto p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta^{</em>}\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta^{*}\right)
$$</p>
<p>which is proportional to a constant in $x_{\text {test }}$ and $S_{n}$. The quantities differ in the last term, which we expand below and put in matrix form. Let $T \in \mathbb{R}^{|\mathcal{H}| \times|\mathcal{D}|}$ be the matrix that represents the transition probabilities starting from a delimiter state: $p\left(h_{\text {test }}^{\text {start }} \mid h^{\text {delim }}\right)$ for $h_{\text {test }}^{\text {start }} \in \mathcal{H}$ and $h^{\text {delim }} \in \mathcal{D}$. As a result,</p>
<p>$$
\begin{aligned}
p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta^{<em>}\right) &amp; =\sum_{h_{\text {test }}^{\text {delim }}} p\left(h_{\text {test }}^{\text {start }} \mid h_{n}^{\text {delim }}, \theta^{</em>}\right) p\left(h_{n}^{\text {delim }} \mid S_{n}, \theta^{*}\right) \
&amp; =T v
\end{aligned}
$$</p>
<p>where $h_{n}^{\text {delim }}$ is the delimiter hidden state before $h_{\text {test }}^{\text {start }}$.</p>
<p>Let $W \in \mathbb{R}^{|\mathcal{Y}| \times|\mathcal{H}|}$ be the matrix that represents the probabilities $p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta^{<em>}\right) p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta^{</em>}\right)$ for all the possible $y \in \mathcal{Y}$ and $h_{\text {test }}^{\text {start }} \in \mathcal{H}$. Overall, we can write</p>
<p>$$
\begin{aligned}
\sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(\cdot \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta^{<em>}\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta^{</em>}\right) &amp; =W T v \
p_{\text {prompt }}\left(\cdot \mid x_{\text {test }}\right) &amp; =W u
\end{aligned}
$$</p>
<p>where $u \in \mathbb{R}^{|\mathcal{H}|}$ is the vector of probabilities that corresponds to the prompt start distribution $p_{\text {prompt }}$.
Bounding the difference between the two predictive distributions,</p>
<p>$$
\begin{aligned}
|W T v-W u|<em 1="1">{\infty} &amp; \leq|W T v-W u|</em> \
&amp; =\sum_{i=1}^{|\mathcal{Y}|}\left|W_{i}^{\top}(T v-u)\right|<em i="1">{i} \
&amp; =\sum</em>(T v-u)}^{|\mathcal{Y}|}\left|\sum_{j=1}^{|\mathcal{H}|} W_{i j<em i="1">{j}\right| \
&amp; \leq \sum</em>\left|(T v-u)}^{|\mathcal{Y}|} \sum_{j=1}^{|\mathcal{H}|} W_{i j<em i="i" j="j">{j}\right| \quad\left(W</em> \geq 0\right) \
&amp; =\sum_{j=1}^{|\mathcal{H}|}\left(\sum_{i=1}^{|\mathcal{Y}|} W_{i j}\right)\left|(T v-u)<em 1="1">{j}\right| \
&amp; =|T v-u|</em>
\end{aligned}
$$</p>
<p>Using Assumption 3, we can further bound this by $\Delta / 2$ :</p>
<p>$$
\begin{aligned}
|T v-u|<em _operatorname_prompt="\operatorname{prompt">{1} &amp; =2 T V\left(p</em>=i, \theta^{}}(\cdot) | \sum_{i=1}^{|\mathcal{D}|} v_{i} p\left(\cdot \mid h^{\text {delim }<em>}\right)\right) \
&amp; \leq 2 \sum_{i=1}^{|\mathcal{D}|} v_{i} T V\left(p_{\operatorname{prompt}}(\cdot) | p\left(\cdot \mid h^{\text {delim }}=i, \theta^{</em>}\right)\right) \quad \text { (convexity of TV distance) } \
&amp; \leq 2 \max <em _operatorname_prompt="\operatorname{prompt">{h^{\text {delim }} \in \mathcal{D}} T V\left(p</em>\right)\right)&lt;\Delta / 2
\end{aligned}
$$}}(\cdot) | p\left(\cdot \mid h^{\text {delim }}, \theta^{*</p>
<p>Since the probability of any output does not change by more than $\Delta / 2$ and the margin between the most likely label and the second most likely label is $\Delta$, the argmax's are the same, showing the result.</p>
<h1>D Proof of Theorem 1</h1>
<p>Proof. We analyze the most likely prediction over the pretraining distribution conditioned on the prompt $\arg \max <em n="n">{y} p\left(y \mid S</em>\right)$.}, x_{\text {test }</p>
<p>$$
\begin{aligned}
p\left(y \mid S_{n}, x_{\text {test }}\right) &amp; =\int_{\theta} p\left(y \mid S_{n}, x_{\text {test }}, \theta\right) p\left(\theta \mid S_{n}, x_{\text {test }}\right) d \theta \
&amp; \propto \int_{\theta} p\left(y \mid S_{n}, x_{\text {test }}, \theta\right) p\left(S_{n}, x_{\text {test }} \mid \theta\right) p(\theta) d \theta \
&amp; \propto \int_{\theta} p\left(y \mid S_{n}, x_{\text {test }}, \theta\right) \frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{<em>}\right)} p(\theta) d \theta \
&amp; =\int_{\theta} \sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta\right) \frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{</em>}\right)} p(\theta) d \theta
\end{aligned}
$$</p>
<p>Defining the following quantity,</p>
<p>$$
r_{n}(\theta)=\frac{1}{n} \log \frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{*}\right)}
$$</p>
<p>we will show that under distinguishability for all $\theta \neq \theta^{*}, r_{n}(\theta)$ converges to a negative constant such that</p>
<p>$$
\frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{*}\right)}=\exp \left(n \cdot r_{n}(\theta)\right) \rightarrow 0
$$</p>
<p>for $\theta \neq \theta^{<em>}$, whereas this ratio is always 1 for $\theta=\theta^{</em>}$. This will then "select" the desired prompt concept through marginalization.
Supposing that Equation 53 holds, we show that the theorem statement holds. Let</p>
<p>$$
\Delta^{\prime}=\max <em _prompt="{prompt" _text="\text">{h^{\text {delim }} \in \mathcal{D}} T V\left(p</em>\right)\right)&lt;\Delta / 2
$$}}(\cdot) | p\left(\cdot \mid h^{\text {delim }}, \theta^{*</p>
<p>and let $\epsilon&lt;\left(\Delta / 2-\Delta^{\prime}\right) p\left(\theta^{*}\right)$. Then for $n$ large enough (due to Equation 53),</p>
<p>$$
\begin{aligned}
\int_{\theta} \sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} &amp; p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta\right) \frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{<em>}\right)} p(\theta) d \theta \
&amp; =\sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta^{</em>}\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta^{<em>}\right) p\left(\theta^{</em>}\right)+\int_{\theta \neq \theta^{<em>}} \epsilon_{\theta}(y) p(\theta) d \theta \
&amp; \propto \sum_{h_{\text {test }}^{\text {start }} \in \mathcal{H}} p\left(y \mid x_{\text {test }}, h_{\text {test }}^{\text {start }}, \theta^{</em>}\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, x_{\text {test }}, \theta^{<em>}\right)+\frac{1}{p\left(\theta^{</em>}\right)} \int_{\theta \neq \theta^{*}} \epsilon_{\theta}(y) p(\theta) d \theta
\end{aligned}
$$</p>
<p>where $\epsilon_{\theta}(y) \leq \epsilon / 2$ for all $y \in \mathcal{Y}$.
By Lemma 1, the argmax of the first term of Equation 57 is the same as $\arg \max <em _prompt="{prompt" _text="\text">{y} p</em>$. Since}}\left(y \mid x_{\text {test }}\right)$, where the margin between the most likely label and the second most likely is at least $\Delta / 2-\Delta^{\prime</p>
<p>$$
\frac{1}{p\left(\theta^{<em>}\right)} \int_{\theta \neq \theta^{</em>}} \epsilon_{\theta}(y) p(\theta) \leq \frac{\epsilon}{2 p\left(\theta^{*}\right)}&lt;\left(\Delta / 2-\Delta^{\prime}\right) / 2
$$</p>
<p>for all $y \in \mathcal{Y}$, the argmax of Equation 57 is also the same as $\arg \max p_{\text {prompt }}\left(y \mid x_{\text {test }}\right)$.</p>
<p>Now it remains to show that $r_{n}(\theta)$ converges to a negative constant for $\theta \neq \theta^{*}$. Let $O_{i}^{\text {ex }}=\left[o_{i-1}^{\text {delim }}, O_{i}\right]$ be the $i$-th observation segment and the previous delimiter together for $i&gt;1$ and define $O_{1}^{\text {ex }}=O_{1}$. Expanding the numerator of the ratio in $r_{n}(\theta)$, we have</p>
<p>$$
\begin{aligned}
p\left(S_{n}, x_{\text {test }} \mid \theta\right)= &amp; p\left(x_{\text {test }} \mid S_{n}, \theta\right) p\left(S_{n} \mid \theta\right) \
= &amp; \sum_{h_{\text {test }}^{\text {delim }}} p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta\right) p\left(o_{n}^{\text {delim }} \mid O_{1: n}^{\text {ex }}, \theta\right) \prod_{i=1}^{n} p\left(O_{i}^{\text {ex }} \mid O_{1: i-1}^{\text {ex }}, \theta\right) \
= &amp; \sum_{h_{\text {test }}^{\text {delim }}} p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta\right) \
&amp; \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(o_{n}^{\text {delim }} \mid h_{n}^{\text {delim }}\right) p\left(h_{n}^{\text {delim }} \mid O_{1: n}^{\text {ex }}, \theta\right) \prod_{i=1}^{n} \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right) p\left(h_{i-1}^{\text {delim }} \mid O_{1: i-1}^{\text {ex }}, \theta\right) \
= &amp; \sum_{h_{\text {test }}^{\text {delim }}} p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta\right) \
&amp; \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(h_{n}^{\text {delim }} \mid O_{1: n}^{\text {ex }}, \theta\right) \prod_{i=1}^{n} \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right) p\left(h_{i-1}^{\text {delim }} \mid O_{1: i-1}^{\text {ex }}, \theta\right) \
= &amp; \sum_{h_{\text {test }}^{\text {start }}} p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta\right) \prod_{i=1}^{n} \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right) p\left(h_{i-1}^{\text {delim }} \mid O_{1: i-1}^{\text {ex }}, \theta\right)
\end{aligned}
$$</p>
<p>Note that in the last line, the inner sum is over the set of delimiter states $\mathcal{D}$ by using the assumption that observing a delimiter $o^{\text {delim }}$ implies that the corresponding hidden state $h^{\text {delim }}$ must be in $\mathcal{D}$. We also see that $\sum_{h_{i-1}^{\text {delim }}} p\left(h_{n}^{\text {delim }} \mid O_{1: n}^{\text {ex }}, \theta\right)=1$.
We restrict our attention to $\theta$ where $p\left(S_{n}, x_{\text {test }} \mid \theta\right)&gt;0$, since otherwise $\theta$ does not affect the prediction. Expanding $r_{n}(\theta)$, we have the following upper bound:</p>
<p>$$
\begin{aligned}
r_{n}(\theta) &amp; =\frac{1}{n}\left(\log \frac{p\left(S_{n}, x_{\text {test }} \mid \theta\right)}{p\left(S_{n}, x_{\text {test }} \mid \theta^{<em>}\right)}\right) \
&amp; =\frac{1}{n}\left(\log \frac{\sum_{h_{\text {test }}^{\text {start }}} p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta\right)}{\sum_{h_{\text {test }}^{\text {start }}} p\left(x_{\text {test }} \mid h_{\text {test }}^{\text {start }}, \theta^{</em>}\right) p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta^{<em>}\right)}+\sum_{i=1}^{n} \log \frac{\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right) p\left(h_{i-1}^{\text {delim }} \mid O_{1: i-1}^{\text {ex }}, \theta\right)}{\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta^{</em>}\right) p\left(h_{i-1}^{\text {delim }} \mid O_{1: i-1}^{\text {ex }}, \theta^{<em>}\right)}\right) \
&amp; \leq \frac{1}{n}\left(\log \frac{\sum_{h_{\text {test }}^{\text {start }}} 1 \cdot p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta\right)}{\sum_{h_{\text {test }}^{\text {start }}} c_{7} \cdot p\left(h_{\text {test }}^{\text {start }} \mid S_{n}, \theta^{</em>}\right)}+n\left(\log \left(c_{2}\right)-\log \left(c_{1}\right)\right)+\sum_{i=1}^{n} \log \frac{\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right)}{\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta^{<em>}\right)}\right) \
&amp; =\frac{1}{n}\left(-\log \left(c_{7}\right)+n\left(\log \left(c_{2}\right)-\log \left(c_{1}\right)\right)+\sum_{i=1}^{n} \log \frac{\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right)}{\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta^{</em>}\right)}\right)
\end{aligned}
$$</p>
<p>In the above steps, we used both Propositions 1 and 2 in the terms involving $c_{2}, c_{1}$ (bounding the probability of $h^{\text {delim }}$ hidden states) and $c_{7}$ (bounding the probability of $x_{\text {test }}$ ). Note that in the second line, the sum can must be over the set of delimiter states $\mathcal{D}$ by using the assumption that observing a delimiter $o^{\text {delim }}$ implies that the corresponding hidden state $h^{\text {delim }}$ must be in $\mathcal{D}$.</p>
<p>Focusing on the numerator of the ratio term and summing over the start hidden state for the $i$-th example,</p>
<p>$$
\begin{aligned}
\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} p\left(O_{i} \mid h_{i-1}^{\text {delim }}, \theta\right) &amp; =\sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} \sum_{h_{i}^{\text {start }}} p\left(O_{i} \mid h_{i}^{\text {start }}, \theta\right) p\left(h_{i}^{\text {start }} \mid h_{i-1}^{\text {delim }}, \theta\right) \
&amp; =\sum_{h_{i-1}^{\text {start }}} p\left(O_{i} \mid h_{i}^{\text {start }}, \theta\right) p\left(h_{i}^{\text {start }} \mid \theta\right) \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} \frac{p\left(h_{i}^{\text {start }} \mid h_{i-1}^{\text {delim }}, \theta\right)}{p\left(h_{i}^{\text {start }} \mid \theta\right)} \
&amp; =\sum_{h_{i-1}^{\text {start }}} p\left(O_{i} \mid h_{i}^{\text {start }}, \theta\right) p\left(h_{i}^{\text {start }} \mid \theta\right) \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} \frac{p\left(h_{i-1}^{\text {delim }} \mid h_{i}^{\text {start }}, \theta\right)}{p\left(h_{i-1}^{\text {delim }} \mid \theta\right)}
\end{aligned}
$$</p>
<p>where the last step applies Bayes' rule. We can lower and upper bound the following quantity for any $\theta$ using Assumption 2:</p>
<p>$$
\begin{aligned}
&amp; \frac{p\left(h_{i-1}^{\text {delim }} \mid h_{i}^{\text {start }}, \theta\right)}{p\left(h_{i-1}^{\text {delim }} \mid \theta\right)} \leq \frac{p\left(h_{i-1}^{\text {delim }} \mid h_{i}^{\text {start }}, \theta\right)}{c_{3}} \
&amp; \frac{p\left(h_{i-1}^{\text {delim }} \mid h_{i}^{\text {start }}, \theta\right)}{p\left(h_{i-1}^{\text {delim }} \mid \theta\right)} \geq \frac{p\left(h_{i-1}^{\text {delim }} \mid h_{i}^{\text {start }}, \theta\right)}{c_{4}}
\end{aligned}
$$</p>
<p>This implies that</p>
<p>$$
\begin{aligned}
&amp; \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} \frac{p\left(h_{i-1}^{\text {delim }} \mid h_{i}^{\text {start }}, \theta\right)}{p\left(h_{i-1}^{\text {delim }} \mid \theta\right)} \leq \frac{1}{c_{3}} \
&amp; \sum_{h_{i-1}^{\text {delim }} \in \mathcal{D}} \frac{p\left(h_{i-1}^{\text {delim }} \mid h_{i}^{\text {start }}, \theta\right)}{p\left(h_{i-1}^{\text {delim }} \mid \theta\right)} \geq \frac{1}{c_{4}}
\end{aligned}
$$</p>
<p>Plugging in these bounds, we have</p>
<p>$$
\begin{aligned}
r_{n}(\theta) &amp; \leq \frac{1}{n}\left(-\log \left(c_{7}\right)+2 n\left(\log \left(c_{2}\right)-\log \left(c_{1}\right)\right)+n\left(\log \left(c_{4}\right)-\log \left(c_{3}\right)\right)+\sum_{i=1}^{n} \log \frac{\sum_{h_{i}^{\text {start }}} p\left(O_{i} \mid h_{i}^{\text {start }}, \theta\right) p\left(h_{i}^{\text {start }} \mid \theta\right)}{\sum_{h_{i}^{\text {start }}} p\left(O_{i} \mid h_{i}^{\text {start }}, \theta\right) p\left(h_{i}^{\text {start }} \mid \theta^{<em>}\right)}\right) \
&amp; =\frac{1}{n}\left(-\log \left(c_{7}\right)+2 n\left(\log \left(c_{2}\right)-\log \left(c_{1}\right)\right)+n\left(\log \left(c_{4}\right)-\log \left(c_{3}\right)\right)+\sum_{i=1}^{n} \log \frac{p\left(O_{i} \mid \theta\right)}{p\left(O_{i} \mid \theta^{</em>}\right)}\right) \
&amp; \rightarrow_{n \rightarrow \infty} \mathbb{E}<em _prompt="{prompt" _text="\text">{O \sim p</em>
\end{aligned}
$$}}}\left[\log \frac{p(O \mid \theta)}{p\left(O \mid \theta^{*}\right)}\right]+\epsilon_{\text {delim }}^{\theta</p>
<p>where we set</p>
<p>$$
\epsilon_{\text {delim }}^{\theta}=2\left(\log \left(c_{2}\right)-\log \left(c_{1}\right)\right)+\log \left(c_{4}\right)-\log \left(c_{3}\right)
$$</p>
<p>Next, we convert the expectation in the bound into a KL divergence. We have</p>
<p>$$
\begin{aligned}
\mathbb{E}<em _prompt="{prompt" _text="\text">{O \sim p</em>{p\left(O \mid \theta^{}}}\left[\log \frac{p(O \mid \theta)<em>}\right)}\right] &amp; =\mathbb{E}<em _prompt="{prompt" _text="\text">{O \sim p</em>{p\left(O \mid \theta^{}}}\left[\log \frac{p(O \mid \theta)}{p_{\text {prompt }}(O)}+\log \frac{p_{\text {prompt }}(O)</em>}\right)}\right] \
&amp; =K L\left(p_{\text {prompt }} | p\left(\cdot \mid \theta^{*}\right)\right)-K L\left(p_{\text {prompt }} | p(\cdot \mid \theta)\right)
\end{aligned}
$$</p>
<p>We will upper bound the first KL term:</p>
<p>$$
K L\left(p_{\text {prompt }} | p\left(\cdot \mid \theta^{<em>}\right)\right)=\mathbb{E}<em _prompt="{prompt" _text="\text">{O \sim p</em>{p\left(O \mid \theta^{}}}\left[\log \frac{p_{\text {prompt }}(O)</em>}\right)}\right]
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We can exchange limits and integrals since the probabilities are bounded (dominated convergence).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
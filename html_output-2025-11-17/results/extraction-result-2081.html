<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2081 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2081</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2081</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-39983a80f111f7f6e793f02c5725a14bca76b32d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/39983a80f111f7f6e793f02c5725a14bca76b32d" target="_blank">The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The AI Scientist-v2 is introduced, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper, marking the first instance of a fully AI-generated paper successfully navigating a peer review.</p>
                <p><strong>Paper Abstract:</strong> AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2081.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2081.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end agentic system that autonomously generates research ideas, writes experiment code, executes experiments, analyzes results (including visualizations), iterates via an agentic tree-search, and authors complete manuscripts; it integrates LLMs for generation and VLMs for figure feedback and uses an Experiment Progress Manager to coordinate staged experimentation and replications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>agentic system combining large language models, code-generation, and tree-search orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning research / automated scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>scientific hypotheses, experiment code, experimental results (training/validation metrics, figures), and full manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (system produces novel combinations and autonomous manuscripts; generated research ideas can be novel but frequently incremental and sometimes contain hallucinations)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven idea generation followed by agentic tree search over code/experiment nodes (parallel node expansion); LLMs (Claude 3.5 Sonnet v2 for code) generate concrete experiment plans and Python code which are executed automatically; selection/refinement guided by an LLM evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Multi-stage automated validation: (1) automatic execution of generated code in Python (errors mark nodes as 'buggy'), (2) saving and checking training/validation metrics, (3) VLM critique of generated figures (flagging unclear labels/legends/duplication), (4) replication nodes to compute mean and standard deviation across seeds, (5) LLM-based evaluator ranks nodes by performance/plot quality, and (6) external human peer review (blind workshop peer review) for manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Operational outcomes reported: the system generated three full manuscripts autonomously; one of the three achieved an average workshop peer-review score of 6.33 (individual scores 6, 6, 7) and would have been accepted by meta-review; typical total runtime per paper ranges from several hours up to the 15-hour limit set by authors; stage node allocations: Stage1=21 nodes, Stage2=12, Stage3=12, Stage4=12; per-node max runtime 1 hour. No end-to-end success rate (e.g., probability of producing an accepted paper) is quantified beyond the 3-run anecdote.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation elements: replication nodes provide means and standard deviations for experiments; one generated manuscript achieved acceptance-level peer-review scores (avg 6.33). The paper reports internal reviewer findings that identified methodological issues (hallucinated citations, ~57% dataset overlap in one experiment, inaccuracies in figure captions) indicating that automated validation did not catch all correctness issues. No global validation accuracy/precision metrics are reported for the system as a whole.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantitatively measured. Qualitatively, the paper notes that as experimental/task complexity and novelty increased (e.g., more complex arithmetic expressions), generated models' generalization degraded and some automated validations missed issues (e.g., dataset overlap, hallucinated citations), indicating validation reliability declines with output novelty/complexity, but no numeric relationship is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The paper documents an asymmetry: generation capabilities (idea/code/manuscript production) can produce outputs that pass workshop peer review, yet validation pipelines can fail to catch substantive errors (hallucinated citations, dataset overlap, misleading figure captions). Only one of three fully autonomous manuscripts was accepted, and authors performed meta-selection of best runs prior to submission, evidencing a gap between raw generation output and reliable validated output.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Partial: uncertainty about experimental outcomes is captured empirically via multiple replications (replication nodes) and aggregation nodes that compute means and standard deviations. No probabilistic uncertainty estimates or calibrated confidence scores for LLM-generated claims are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported. The paper does not provide calibration metrics for LLM confidence or for system-level confidence in its scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not formally measured; qualitative evidence suggests degraded performance on more complex/unseen arithmetic-expression generalization tasks (authors note generalization worsens as expression complexity increases), but no explicit OOD metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — proxies used include execution success/failure (buggy vs non-buggy node), training/validation metrics produced by experiments (losses, accuracies), LLM evaluator ranking scores (qualitative/LLM-assigned), VLM figure critique (clarity/alignment checks), and external peer-review scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Humans intervened at meta-level: they selected which AI-generated initial ideas to fully pursue (chose 3), ran multiple seeds per idea, and selected the single best complete manuscript for submission; reviewers were informed some submissions could be AI-generated and the accepted manuscript was withdrawn post-review. Frequency: meta-selection occurred once per idea/batch of runs in the described evaluation; the paper states this human selection step could be avoided if desired, but they used it here.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (machine learning experiments); domain is not highly formalized like mathematics, which contributes to a non-trivial generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Several strategies are implemented: automated debugging nodes, replication nodes + aggregation (mean/std), LLM-based evaluators to rank nodes by performance and plot quality, VLM feedback loop to detect visual issues, Experiment Progress Manager with stage-based stopping criteria, literature checks via Semantic Scholar during idea generation, and human meta-selection for final submission. Effectiveness: these mitigations improved robustness (enabled an acceptance-level manuscript) but did not eliminate hallucinations, dataset-overlap errors, or methodological shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Reported hallucinated/inaccurate citations; identified dataset overlap (~57% overlap between train/test) affecting reliability of reported experimental results; inaccuracies in figure captions and interpretation; only 1/3 AI-generated manuscripts achieved acceptance-level peer-review scores; authors and reviewers flagged insufficient methodological rigor despite automated checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Automated code generation and execution with systematic debugging and VLM figure checks successfully produced coherent manuscripts and experimental results; replication/aggregation provided statistical summaries; one manuscript achieved acceptance-level peer-review scores, demonstrating that the validation pipeline can be sufficient for workshop-level quality in at least some runs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported as a numeric ratio. Relevant operational numbers: per-paper end-to-end generation typically several hours up to 15 hours; per-node maximum runtime 1 hour; node counts per stage indicate substantial parallel computation. The paper does not quantify the computational cost of external validation (peer review) vs internal generation/validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2081.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2081.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist-v1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards Fully-Automated Open-Ended Scientific Discovery (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The precursor to v2 that demonstrated a fully automated pipeline (idea-to-manuscript) but relied on human-authored code templates and a linear experimentation process, limiting generalization and depth of exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully-Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist-v1</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>automated scientific discovery pipeline leveraging LLMs with human-provided code templates</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning research / automated scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experiment code, experimental results, visualizations, manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental relative to later v2: demonstrated feasibility of end-to-end automation but constrained by templates</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM edits applied to human-authored template code in a linear, sequential workflow</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>automated execution of generated code and generation of figures/manuscripts; lacked the deeper staged tree-search validation used in v2</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Demonstrated end-to-end generation in prior work (Lu et al., 2024) but required human templates; not submitted to peer review as a fully autonomous paper in that work (according to the current paper)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not evaluated via workshop peer review in the current report; described as having more short-sighted/linear experimentation and limited depth of exploration compared to v2.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported; described qualitatively as more short-sighted and less capable of deep exploration, implying degraded handling of novel research avenues relative to v2.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>v1 generation was constrained by templates and thus less likely to produce radically novel pipelines; validation was simpler/linear and less systematic than v2, suggesting a smaller but less rigorous generation/validation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Execution success; generated figures; no VLM integration in v1 (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High: v1 required human-authored templates to bootstrap experiments, implying substantial human involvement per new topic.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (machine learning experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Not applicable beyond linear debugging and LLM-driven code edits; lacked the tree-search, VLM checks, and manager agent introduced in v2.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Described limitations (template dependence, linear experimentation) that prevented broad autonomy and deeper validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>v1 nevertheless demonstrated that an end-to-end automated pipeline is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2081.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2081.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-Sonnet-v2 (code gen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (v2) — code generation model (as used for experiment code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model used by The AI Scientist-v2 for code generation: prompted to produce concrete experiment plans and Python code for each node in the agentic tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude 3.5 Sonnet (v2)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model specialized for code generation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>software/code generation for machine learning experiments</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Python experiment scripts and textual experiment plans</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel code compositions (recombination of learned code patterns and templates generated on the fly)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompted LLM code generation with temperature 0.5 and max tokens 8192 (per Table 2); used as the primary actor to synthesize experiment code for tree-search nodes</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Generated code is executed automatically; execution errors recorded and flagged as 'buggy'; passing runs produce saved metrics and visuals for downstream VLM/LLM evaluation and replication.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>No quantitative success/failure rates reported for code generation; operational hyperparameters reported (temperature=0.5, max tokens=8192). Many generated nodes succeeded, others produced execution errors triggering debug nodes, but no aggregate success metric is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation of code relies on execution success, downstream metric evaluation, and VLM/LMM critiques; no numeric validation accuracy for the model's code outputs is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper describes practical pipeline where generated code is immediately validated via execution and further vetted by VLM and LLM evaluators; no formal comparison metrics are given.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Execution success/failure, training/validation metrics generated by executed code, and VLM figure critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human meta-selection used at the manuscript selection stage; otherwise generated code is executed and debugged automatically by the system.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>engineering/empirical (programming and ML experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Automatic debugging nodes, replication and aggregation nodes, LLM-based evaluators and VLM checks mitigate incorrect code generation; effectiveness not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>System sometimes produced buggy nodes requiring debugging; no aggregate automated code-correctness metric was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Many generated nodes executed successfully and enabled end-to-end experiments whose results were used in manuscripts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2081.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2081.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (LLM/VLM feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used as LLM and Vision-Language feedback agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal model used within The AI Scientist-v2 as the LLM/VLM feedback agent to critique figures, captions, and textual alignment and to assist in manuscript reflection and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multimodal large language model / vision-language model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>figure and manuscript quality assessment within ML research workflows</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>figure critiques, feedback comments, ranking/assessment signals used by the experiment manager and manuscript reflection stage</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not applicable (used as critic/evaluator rather than novel-output generator in the reported experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Provides multimodal critique by ingesting screenshots of figures plus captions and surrounding text; prompts elicit checks for alignment, clarity, missing legends, duplication, and interpretive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>VLM checks mark nodes as buggy if issues are detected; used during both per-node experimentation and manuscript reflection stages to improve visual clarity and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>No numeric performance metrics reported for the VLM critique (e.g., precision/recall of flagged figure issues). Paper gives qualitative examples of issues VLM flags (unclear labels, missing legends, duplication).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Qualitative: VLM feedback led to iterations that improved figure quality; no quantitative validation performance metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>VLM acts as a validation layer for visual artifacts produced by generation modules; no formal comparison of VLM validation strength vs generation capability is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Figure clarity checks, caption-figure alignment, detection of duplication between main text and appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>VLM feedback is applied automatically in pipeline; humans are still used at meta-level and for post-review inspection. Frequency of human intervention not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (visual/communication quality assessment in scientific manuscripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>VLM feedback loop to detect visual and caption errors which otherwise might survive generation; partially effective but not sufficient to catch all substantive methodological errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Despite VLM checks, the accepted manuscript still contained caption inaccuracies and other substantive issues reported by human reviewers, showing VLM validation alone is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>VLM flagged many low-level visual issues (labels, legends) that were corrected, improving figure quality.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2081.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2081.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentic Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentic Tree Search for LLM-driven Experimentation and Code Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallelized tree-search strategy where each node encapsulates an experiment script, high-level plan, execution trace, performance metrics, and visualizations; nodes are expanded in parallel by LLMs, executed, and ranked by an LLM evaluator to guide further exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentic Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>search/meta-reasoning framework orchestrating LLM-generated code/actions</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>automated experiment design and software-driven research exploration</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>alternative experiment implementations, hyperparameter variants, ablations, debug patches, and manuscripts seeded from best nodes</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>methodological innovation (applied adaptation of LLM+tree-search methods to full experimental pipeline); outputs are recombinations and refinements of generated code/plans</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Best-first parallel node expansion guided by an LLM evaluator; child nodes are generated either for debugging buggy parents or for refining non-buggy parents; node types include hyperparameter, ablation, replication, and aggregation nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Each generated node is automatically executed; execution errors mark 'buggy' nodes and spawn debug attempts; successful nodes undergo plotting and VLM review; replication and aggregation nodes compute statistical summaries to validate results.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Hyperparameters reported: Debug Probability=1.0, Max Debug Depth=3, Node allocation per stage (Stage1:21, Stage2:12, Stage3:12, Stage4:12). No end-to-end quantitative generation quality metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is integrated into the tree-search cycle (execution success, VLM checks, LLM evaluation), enabling selection of best-performing nodes to seed subsequent stages; no numeric validation metrics beyond those produced by individual experiments (means/std) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not reported; framework intended to better explore novel hypotheses than linear pipelines, but no quantitative comparison is given.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Framework narrows generation-validation gap by immediately executing and vetting nodes and by maintaining parallel explorations, but the paper documents remaining failures (e.g., hallucinations, dataset overlap) showing this mitigation is partial.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Replication-based empirical statistics (mean/std) used to quantify variability; no Bayesian or probabilistic uncertainty estimates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Execution success, per-node performance metrics (training/validation curves), VLM/LLM feedback scores, and aggregated statistics from replication nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>System runs multiple seeds and parallel nodes autonomously; authors performed human meta-selection once among the multiple complete manuscripts per idea. The tree search itself is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / engineering (machine learning experiment construction)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Parallel exploration, prioritized debugging, replication and aggregation, LLM evaluators for best-first selection, VLM feedback for visual checks; partial effectiveness demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Even with tree search, some generated experiments produced problematic results (dataset overlap, misleading figures) that automated node-level checks didn't fully catch.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Tree-search plus automated validation enabled more systematic exploration and yielded at least one acceptance-level manuscript, indicating improved alignment of generation and validation capabilities relative to linear approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported (node allocations and per-node runtime limits provided, but no end-to-end cost ratio between generation and validation).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Aide: Ai-driven exploration in the space of code <em>(Rating: 2)</em></li>
                <li>MLEbench: Evaluating machine learning agents on machine learning engineering <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully-Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2081",
    "paper_id": "paper-39983a80f111f7f6e793f02c5725a14bca76b32d",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "AI Scientist-v2",
            "name_full": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
            "brief_description": "An end-to-end agentic system that autonomously generates research ideas, writes experiment code, executes experiments, analyzes results (including visualizations), iterates via an agentic tree-search, and authors complete manuscripts; it integrates LLMs for generation and VLMs for figure feedback and uses an Experiment Progress Manager to coordinate staged experimentation and replications.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "The AI Scientist-v2",
            "system_type": "agentic system combining large language models, code-generation, and tree-search orchestration",
            "scientific_domain": "machine learning research / automated scientific discovery",
            "output_type": "scientific hypotheses, experiment code, experimental results (training/validation metrics, figures), and full manuscripts",
            "novelty_level": "moderately novel (system produces novel combinations and autonomous manuscripts; generated research ideas can be novel but frequently incremental and sometimes contain hallucinations)",
            "generation_method": "LLM-driven idea generation followed by agentic tree search over code/experiment nodes (parallel node expansion); LLMs (Claude 3.5 Sonnet v2 for code) generate concrete experiment plans and Python code which are executed automatically; selection/refinement guided by an LLM evaluator.",
            "validation_method": "Multi-stage automated validation: (1) automatic execution of generated code in Python (errors mark nodes as 'buggy'), (2) saving and checking training/validation metrics, (3) VLM critique of generated figures (flagging unclear labels/legends/duplication), (4) replication nodes to compute mean and standard deviation across seeds, (5) LLM-based evaluator ranks nodes by performance/plot quality, and (6) external human peer review (blind workshop peer review) for manuscripts.",
            "generation_performance": "Operational outcomes reported: the system generated three full manuscripts autonomously; one of the three achieved an average workshop peer-review score of 6.33 (individual scores 6, 6, 7) and would have been accepted by meta-review; typical total runtime per paper ranges from several hours up to the 15-hour limit set by authors; stage node allocations: Stage1=21 nodes, Stage2=12, Stage3=12, Stage4=12; per-node max runtime 1 hour. No end-to-end success rate (e.g., probability of producing an accepted paper) is quantified beyond the 3-run anecdote.",
            "validation_performance": "Validation elements: replication nodes provide means and standard deviations for experiments; one generated manuscript achieved acceptance-level peer-review scores (avg 6.33). The paper reports internal reviewer findings that identified methodological issues (hallucinated citations, ~57% dataset overlap in one experiment, inaccuracies in figure captions) indicating that automated validation did not catch all correctness issues. No global validation accuracy/precision metrics are reported for the system as a whole.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not quantitatively measured. Qualitatively, the paper notes that as experimental/task complexity and novelty increased (e.g., more complex arithmetic expressions), generated models' generalization degraded and some automated validations missed issues (e.g., dataset overlap, hallucinated citations), indicating validation reliability declines with output novelty/complexity, but no numeric relationship is provided.",
            "generation_validation_comparison": "The paper documents an asymmetry: generation capabilities (idea/code/manuscript production) can produce outputs that pass workshop peer review, yet validation pipelines can fail to catch substantive errors (hallucinated citations, dataset overlap, misleading figure captions). Only one of three fully autonomous manuscripts was accepted, and authors performed meta-selection of best runs prior to submission, evidencing a gap between raw generation output and reliable validated output.",
            "uncertainty_quantification": "Partial: uncertainty about experimental outcomes is captured empirically via multiple replications (replication nodes) and aggregation nodes that compute means and standard deviations. No probabilistic uncertainty estimates or calibrated confidence scores for LLM-generated claims are reported.",
            "calibration_quality": "Not reported. The paper does not provide calibration metrics for LLM confidence or for system-level confidence in its scientific claims.",
            "out_of_distribution_performance": "Not formally measured; qualitative evidence suggests degraded performance on more complex/unseen arithmetic-expression generalization tasks (authors note generalization worsens as expression complexity increases), but no explicit OOD metrics are reported.",
            "validation_proxy_metrics": "Yes — proxies used include execution success/failure (buggy vs non-buggy node), training/validation metrics produced by experiments (losses, accuracies), LLM evaluator ranking scores (qualitative/LLM-assigned), VLM figure critique (clarity/alignment checks), and external peer-review scores.",
            "human_validation_required": true,
            "human_validation_frequency": "Humans intervened at meta-level: they selected which AI-generated initial ideas to fully pursue (chose 3), ran multiple seeds per idea, and selected the single best complete manuscript for submission; reviewers were informed some submissions could be AI-generated and the accepted manuscript was withdrawn post-review. Frequency: meta-selection occurred once per idea/batch of runs in the described evaluation; the paper states this human selection step could be avoided if desired, but they used it here.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (machine learning experiments); domain is not highly formalized like mathematics, which contributes to a non-trivial generation-validation gap.",
            "gap_mitigation_strategies": "Several strategies are implemented: automated debugging nodes, replication nodes + aggregation (mean/std), LLM-based evaluators to rank nodes by performance and plot quality, VLM feedback loop to detect visual issues, Experiment Progress Manager with stage-based stopping criteria, literature checks via Semantic Scholar during idea generation, and human meta-selection for final submission. Effectiveness: these mitigations improved robustness (enabled an acceptance-level manuscript) but did not eliminate hallucinations, dataset-overlap errors, or methodological shortcomings.",
            "evidence_supporting_gap": "Reported hallucinated/inaccurate citations; identified dataset overlap (~57% overlap between train/test) affecting reliability of reported experimental results; inaccuracies in figure captions and interpretation; only 1/3 AI-generated manuscripts achieved acceptance-level peer-review scores; authors and reviewers flagged insufficient methodological rigor despite automated checks.",
            "evidence_contradicting_gap": "Automated code generation and execution with systematic debugging and VLM figure checks successfully produced coherent manuscripts and experimental results; replication/aggregation provided statistical summaries; one manuscript achieved acceptance-level peer-review scores, demonstrating that the validation pipeline can be sufficient for workshop-level quality in at least some runs.",
            "computational_cost_ratio": "Not reported as a numeric ratio. Relevant operational numbers: per-paper end-to-end generation typically several hours up to 15 hours; per-node maximum runtime 1 hour; node counts per stage indicate substantial parallel computation. The paper does not quantify the computational cost of external validation (peer review) vs internal generation/validation.",
            "uuid": "e2081.0"
        },
        {
            "name_short": "AI Scientist-v1",
            "name_full": "The AI Scientist: Towards Fully-Automated Open-Ended Scientific Discovery (v1)",
            "brief_description": "The precursor to v2 that demonstrated a fully automated pipeline (idea-to-manuscript) but relied on human-authored code templates and a linear experimentation process, limiting generalization and depth of exploration.",
            "citation_title": "The AI Scientist: Towards Fully-Automated Open-Ended Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": "The AI Scientist-v1",
            "system_type": "automated scientific discovery pipeline leveraging LLMs with human-provided code templates",
            "scientific_domain": "machine learning research / automated scientific discovery",
            "output_type": "experiment code, experimental results, visualizations, manuscripts",
            "novelty_level": "incremental relative to later v2: demonstrated feasibility of end-to-end automation but constrained by templates",
            "generation_method": "LLM edits applied to human-authored template code in a linear, sequential workflow",
            "validation_method": "automated execution of generated code and generation of figures/manuscripts; lacked the deeper staged tree-search validation used in v2",
            "generation_performance": "Demonstrated end-to-end generation in prior work (Lu et al., 2024) but required human templates; not submitted to peer review as a fully autonomous paper in that work (according to the current paper)",
            "validation_performance": "Not evaluated via workshop peer review in the current report; described as having more short-sighted/linear experimentation and limited depth of exploration compared to v2.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported; described qualitatively as more short-sighted and less capable of deep exploration, implying degraded handling of novel research avenues relative to v2.",
            "generation_validation_comparison": "v1 generation was constrained by templates and thus less likely to produce radically novel pipelines; validation was simpler/linear and less systematic than v2, suggesting a smaller but less rigorous generation/validation capability.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Execution success; generated figures; no VLM integration in v1 (per paper).",
            "human_validation_required": true,
            "human_validation_frequency": "High: v1 required human-authored templates to bootstrap experiments, implying substantial human involvement per new topic.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (machine learning experiments)",
            "gap_mitigation_strategies": "Not applicable beyond linear debugging and LLM-driven code edits; lacked the tree-search, VLM checks, and manager agent introduced in v2.",
            "evidence_supporting_gap": "Described limitations (template dependence, linear experimentation) that prevented broad autonomy and deeper validation.",
            "evidence_contradicting_gap": "v1 nevertheless demonstrated that an end-to-end automated pipeline is possible.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2081.1"
        },
        {
            "name_short": "Claude-3.5-Sonnet-v2 (code gen)",
            "name_full": "Claude 3.5 Sonnet (v2) — code generation model (as used for experiment code generation)",
            "brief_description": "A large language model used by The AI Scientist-v2 for code generation: prompted to produce concrete experiment plans and Python code for each node in the agentic tree search.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude 3.5 Sonnet (v2)",
            "system_type": "large language model specialized for code generation",
            "scientific_domain": "software/code generation for machine learning experiments",
            "output_type": "Python experiment scripts and textual experiment plans",
            "novelty_level": "in-distribution to moderately novel code compositions (recombination of learned code patterns and templates generated on the fly)",
            "generation_method": "prompted LLM code generation with temperature 0.5 and max tokens 8192 (per Table 2); used as the primary actor to synthesize experiment code for tree-search nodes",
            "validation_method": "Generated code is executed automatically; execution errors recorded and flagged as 'buggy'; passing runs produce saved metrics and visuals for downstream VLM/LLM evaluation and replication.",
            "generation_performance": "No quantitative success/failure rates reported for code generation; operational hyperparameters reported (temperature=0.5, max tokens=8192). Many generated nodes succeeded, others produced execution errors triggering debug nodes, but no aggregate success metric is provided.",
            "validation_performance": "Validation of code relies on execution success, downstream metric evaluation, and VLM/LMM critiques; no numeric validation accuracy for the model's code outputs is provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported.",
            "generation_validation_comparison": "Paper describes practical pipeline where generated code is immediately validated via execution and further vetted by VLM and LLM evaluators; no formal comparison metrics are given.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Execution success/failure, training/validation metrics generated by executed code, and VLM figure critiques.",
            "human_validation_required": true,
            "human_validation_frequency": "Human meta-selection used at the manuscript selection stage; otherwise generated code is executed and debugged automatically by the system.",
            "formal_verification_used": false,
            "domain_formalization_level": "engineering/empirical (programming and ML experiments)",
            "gap_mitigation_strategies": "Automatic debugging nodes, replication and aggregation nodes, LLM-based evaluators and VLM checks mitigate incorrect code generation; effectiveness not quantified.",
            "evidence_supporting_gap": "System sometimes produced buggy nodes requiring debugging; no aggregate automated code-correctness metric was reported.",
            "evidence_contradicting_gap": "Many generated nodes executed successfully and enabled end-to-end experiments whose results were used in manuscripts.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2081.2"
        },
        {
            "name_short": "GPT-4o (LLM/VLM feedback)",
            "name_full": "GPT-4o (used as LLM and Vision-Language feedback agent)",
            "brief_description": "A multi-modal model used within The AI Scientist-v2 as the LLM/VLM feedback agent to critique figures, captions, and textual alignment and to assist in manuscript reflection and evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o",
            "system_type": "multimodal large language model / vision-language model",
            "scientific_domain": "figure and manuscript quality assessment within ML research workflows",
            "output_type": "figure critiques, feedback comments, ranking/assessment signals used by the experiment manager and manuscript reflection stage",
            "novelty_level": "not applicable (used as critic/evaluator rather than novel-output generator in the reported experiments)",
            "generation_method": "Provides multimodal critique by ingesting screenshots of figures plus captions and surrounding text; prompts elicit checks for alignment, clarity, missing legends, duplication, and interpretive accuracy.",
            "validation_method": "VLM checks mark nodes as buggy if issues are detected; used during both per-node experimentation and manuscript reflection stages to improve visual clarity and alignment.",
            "generation_performance": "No numeric performance metrics reported for the VLM critique (e.g., precision/recall of flagged figure issues). Paper gives qualitative examples of issues VLM flags (unclear labels, missing legends, duplication).",
            "validation_performance": "Qualitative: VLM feedback led to iterations that improved figure quality; no quantitative validation performance metrics reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported.",
            "generation_validation_comparison": "VLM acts as a validation layer for visual artifacts produced by generation modules; no formal comparison of VLM validation strength vs generation capability is provided.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Figure clarity checks, caption-figure alignment, detection of duplication between main text and appendix.",
            "human_validation_required": true,
            "human_validation_frequency": "VLM feedback is applied automatically in pipeline; humans are still used at meta-level and for post-review inspection. Frequency of human intervention not quantified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (visual/communication quality assessment in scientific manuscripts)",
            "gap_mitigation_strategies": "VLM feedback loop to detect visual and caption errors which otherwise might survive generation; partially effective but not sufficient to catch all substantive methodological errors.",
            "evidence_supporting_gap": "Despite VLM checks, the accepted manuscript still contained caption inaccuracies and other substantive issues reported by human reviewers, showing VLM validation alone is insufficient.",
            "evidence_contradicting_gap": "VLM flagged many low-level visual issues (labels, legends) that were corrected, improving figure quality.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2081.3"
        },
        {
            "name_short": "Agentic Tree Search",
            "name_full": "Agentic Tree Search for LLM-driven Experimentation and Code Generation",
            "brief_description": "A parallelized tree-search strategy where each node encapsulates an experiment script, high-level plan, execution trace, performance metrics, and visualizations; nodes are expanded in parallel by LLMs, executed, and ranked by an LLM evaluator to guide further exploration.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Agentic Tree Search",
            "system_type": "search/meta-reasoning framework orchestrating LLM-generated code/actions",
            "scientific_domain": "automated experiment design and software-driven research exploration",
            "output_type": "alternative experiment implementations, hyperparameter variants, ablations, debug patches, and manuscripts seeded from best nodes",
            "novelty_level": "methodological innovation (applied adaptation of LLM+tree-search methods to full experimental pipeline); outputs are recombinations and refinements of generated code/plans",
            "generation_method": "Best-first parallel node expansion guided by an LLM evaluator; child nodes are generated either for debugging buggy parents or for refining non-buggy parents; node types include hyperparameter, ablation, replication, and aggregation nodes.",
            "validation_method": "Each generated node is automatically executed; execution errors mark 'buggy' nodes and spawn debug attempts; successful nodes undergo plotting and VLM review; replication and aggregation nodes compute statistical summaries to validate results.",
            "generation_performance": "Hyperparameters reported: Debug Probability=1.0, Max Debug Depth=3, Node allocation per stage (Stage1:21, Stage2:12, Stage3:12, Stage4:12). No end-to-end quantitative generation quality metrics provided.",
            "validation_performance": "Validation is integrated into the tree-search cycle (execution success, VLM checks, LLM evaluation), enabling selection of best-performing nodes to seed subsequent stages; no numeric validation metrics beyond those produced by individual experiments (means/std) are reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not reported; framework intended to better explore novel hypotheses than linear pipelines, but no quantitative comparison is given.",
            "generation_validation_comparison": "Framework narrows generation-validation gap by immediately executing and vetting nodes and by maintaining parallel explorations, but the paper documents remaining failures (e.g., hallucinations, dataset overlap) showing this mitigation is partial.",
            "uncertainty_quantification": "Replication-based empirical statistics (mean/std) used to quantify variability; no Bayesian or probabilistic uncertainty estimates provided.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Execution success, per-node performance metrics (training/validation curves), VLM/LLM feedback scores, and aggregated statistics from replication nodes.",
            "human_validation_required": true,
            "human_validation_frequency": "System runs multiple seeds and parallel nodes autonomously; authors performed human meta-selection once among the multiple complete manuscripts per idea. The tree search itself is automated.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / engineering (machine learning experiment construction)",
            "gap_mitigation_strategies": "Parallel exploration, prioritized debugging, replication and aggregation, LLM evaluators for best-first selection, VLM feedback for visual checks; partial effectiveness demonstrated.",
            "evidence_supporting_gap": "Even with tree search, some generated experiments produced problematic results (dataset overlap, misleading figures) that automated node-level checks didn't fully catch.",
            "evidence_contradicting_gap": "Tree-search plus automated validation enabled more systematic exploration and yielded at least one acceptance-level manuscript, indicating improved alignment of generation and validation capabilities relative to linear approaches.",
            "computational_cost_ratio": "Not reported (node allocations and per-node runtime limits provided, but no end-to-end cost ratio between generation and validation).",
            "uuid": "e2081.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Aide: Ai-driven exploration in the space of code",
            "rating": 2
        },
        {
            "paper_title": "MLEbench: Evaluating machine learning agents on machine learning engineering",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards Fully-Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.02073025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</h1>
<p>Yutaro Yamada ${ }^{1, <em>}$, Robert Tjarko Lange ${ }^{1, </em>}$, Cong Lu ${ }^{1,2,3, <em>}$, Shengran Hu ${ }^{1,2,3}$, Chris Lu ${ }^{4}$, Jakob Foerster ${ }^{4}$, Jeff Clune ${ }^{2,3,5, \dagger}$ and David Ha ${ }^{1, \dagger}$<br></em>Equal Contribution, ${ }^{1}$ Sakana AI, ${ }^{2}$ University of British Columbia, ${ }^{3}$ Vector Institute, ${ }^{4}$ FLAIR, University of Oxford, ${ }^{5}$ Canada CIFAR AI Chair, ${ }^{\dagger}$ Equal Advising</p>
<h4>Abstract</h4>
<p>AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AIgenerated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.</p>
<h2>1. Introduction</h2>
<p>Automated scientific discovery empowered by artificial intelligence (AI) has garnered considerable attention in recent years (Cornelio et al., 2023; Gil et al., 2014; King et al., 2009; Kitano, 2021; Wang et al., 2023; Xu et al., 2021). The development of end-to-end frameworks capable of autonomously formulating hypotheses, performing experiments, analyzing results, and authoring manuscripts could fundamentally transform the scientific process. A notable recent advance in this direction is The AI Scientist-v1 (Lu et al., 2024), which demonstrated the feasibility of a fully automated scientific workflow and downstream manuscript production. However, significant limitations constrained its broad applicability and autonomy. Specifically, it relied heavily on human-authored code templates requiring manual effort to create a new template for each new topic area. Furthermore, its linear and shallow experimentation approach prevented deeper exploration of scientific hypotheses.</p>
<p>In this paper, we introduce The AI Scientist-v2, a substantially improved successor that directly addresses these limitations. Our contributions are threefold. First, we eliminate the dependency on human-provided code templates, significantly increasing the system's autonomy and ability to be deployed out of the box across multiple machine learning domains. Second, we introduce an experiment manager agent coupled with a novel agentic tree-search algorithm, enabling deeper and</p>
<p>Table 1 | Comparison of AI Scientist Versions. Comparison highlights key advancements in THE AI SCIENTIST-v2, including autonomous code generation via tree search, enhanced VLM integration for feedback during experiments and manuscript review, and evaluation through formal peer review.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: center;">Codebase <br> Drafting</th>
<th style="text-align: center;">Execution <br> Planning</th>
<th style="text-align: center;">Parallel <br> Experiments</th>
<th style="text-align: center;">VLM <br> Reviewer</th>
<th style="text-align: center;">Human Result <br> Evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">THE AI SCIENTIST-V1</td>
<td style="text-align: center;">Topic-Specific</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Not Submitted</td>
</tr>
<tr>
<td style="text-align: left;">THE AI SCIENTIST-V2</td>
<td style="text-align: center;">Domain-General</td>
<td style="text-align: center;">Tree-Based</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Workshop Acceptance-Worthy</td>
</tr>
</tbody>
</table>
<p>more systematic exploration of complex hypotheses. Third, we enhance the reviewing and refinement stages by integrating a Vision-Language Model (VLM)-based feedback mechanism, improving the quality, clarity, and alignment of generated figures, captions, and text interpretation. To rigorously evaluate the capabilities and limitations of fully autonomous manuscript generation, we conducted a controlled experiment: three manuscripts entirely generated by The AI Scientist-v2 were submitted to a peer-reviewed workshop at ICLR. Remarkably, one manuscript achieved an average reviewer score of 6.33 (placing it roughly in the top $45 \%$ of submissions) and would have been accepted after meta-review were it human-generated, thus becoming the first fully AI-generated manuscript to successfully pass a peer-review process.</p>
<p>The accepted paper investigates whether incorporating an explicit compositional regularization term into neural network training can improve compositional generalization. Specifically, it penalizes large deviations between embeddings of successive time steps in sequence models, hypothesizing that this encourages compositionality. The approach is evaluated using synthetic arithmetic expression datasets, but it is found that compositional regularization does not yield significant improvements and occasionally harms performance. The workshop reviewers appreciated the paper for clearly identifying the challenges of effective compositional regularization and reporting on negative results. However, they collectively highlighted shortcomings, including insufficient justification and intuitive explanations for why the chosen regularization method would enhance compositionality. Our personal assessment (detailed further in §4) highlights several additional potential improvements in method description (e.g., making clear exactly which component of the network is being regularized), potential dataset overlap issues, and inaccuracies in figure captions. Overall, reviewers viewed the paper as an interesting and technically sound workshop contribution that needs further development and broader experimentation to reach conference-level rigor.</p>
<p>This report provides an in-depth outline of the developed methodological advances, analysis of the workshop-submitted papers, and a discussion on the ethical and safety considerations of systems like The AI Scientist-v2. Our overall contributions are as follows:</p>
<ol>
<li>We introduce The AI Scientist-v2, an automated scientific discovery framework enhanced by agentic tree search, VLM feedback, and parallel experiment execution. It thereby significantly improves the autonomy, flexibility, and scientific exploration depth of previous systems.</li>
<li>We demonstrate, for the first time, that an AI-generated manuscript can successfully pass peer review at a recognized machine learning workshop, marking a critical milestone for AI science.</li>
<li>We conduct comprehensive internal evaluations and analyses of both peer-review feedback and our system's outputs, providing insights into the strengths, weaknesses, and current status of AI-generated manuscripts relative to traditional human-authored scientific publications.</li>
<li>We open-source the full codebase for The AI Scientist-v2 and the ICLR 2025 workshop experiment data, encouraging further exploration by the research community and advancing a discussion regarding AI's evolving role in science-in the open.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | The AI Scientist-v2 Workflow. The workflow consists of several phases covering automated idea generation, experiment execution, figure visualization, manuscript writing, and reviewing. Unlike the initial version, The AI Scientist-v2 removes the dependency on humancoded templates. Instead, it employs agentic tree search (managed by an Experiment Progress Manager across several stages, orange) to generate and refine code implementations. Subsequent experimentation leverages the best-performing code checkpoints (nodes) from the tree search to iteratively test various research hypotheses.</p>
<h1>2. Background</h1>
<p>The AI Scientist-v1 (Lu et al., 2024) introduced the first AI system that entirely automates scientific discovery and the presentation of its results. Given a baseline code template, it autonomously wrote code, executed experiments, visualized outcomes, and produced a complete scientific manuscript. However, despite representing a significant step forward, The AI Scientist-v1 was subject to limitations. Foremost among these was its reliance on human-crafted baseline code templates, significantly constraining its autonomy and hindering unconstrained out-of-the-box deployability. Instead, human effort was still required to draft an initial base experiment outline in code. Additionally, the experimentation process followed a strictly linear hypothesis-testing routine, limiting depth and exploration flexibility, especially when addressing complex research questions.</p>
<p>Language Model Agent Scaffolding. To further enhance LLM performance on complex reasoning tasks, researchers have developed agentic scaffolding frameworks, each with distinct advantages and limitations. For example, Reflexion (Shinn et al., 2024) enables models to iteratively reflect on previous responses, encouraging self-improvement through critical evaluation of past outputs; it improves robustness, but can introduce computational overhead and slower inference. Another promising direction is the integration of tree-search strategies with LLMs (Jiang et al., 2025), allowing structured exploration of reasoning paths. This approach enhances systematic reasoning and comprehensiveness, though at the cost of increased complexity, higher computational demands, and challenges in scalability.</p>
<p>Tree Search with Large Language Models. We empirically observed that automated research conducted by The AI Scientist-v1 often resulted in short-sighted experimentation. The humandriven scientific process, on the other hand, relies on open-ended hypothesis generation, steppingstone collection, and iterative hypothesis refinement. Recent advances using code generation as an action space have opened new opportunities for LLM-driven automated workflows (Wang et al., 2024). AIDE (Jiang et al., 2025) combines LLM-based code generation with tree search, demon-</p>
<p>strating state-of-the-art performance on the MLEBench benchmark (Chan et al., 2025), designed for machine learning engineering tasks. In AIDE, each node represents a potential solution state with a corresponding scalar evaluation score (e.g., validation accuracy). Nodes are iteratively selected for further debugging or refinement based on these scores. Inspired by this approach, we integrate a similar tree search-based exploration strategy within our automated scientific discovery framework, adapting it specifically to the multi-stage nature of scientific experimentation, as detailed in $\S 3$.</p>
<h1>3. The AI Scientist-v2</h1>
<p>We now describe the major innovations introduced in The AI Scientist-v2 relative to The AI SCIENTIST-v1 (Lu et al., 2024). The most significant improvement is the move towards greater autonomy and generalization, starting a more general idea generation phase ( $\S 3.1$ ) and eliminating the reliance on fixed, human-authored template code for experimentation. This process begins with generalized idea generation, producing an initial concept, which then feeds into the experimentation phase (§3.2). To manage this, we introduce two critical features in the experimentation phase: coarse-grained experiment management and agentic tree search-based exploration. Additionally, we integrate Vision Language Models (VLMs) into the experimental and review phases (§3.4). Finally, we streamline the manuscript writing phase by replacing the incremental, Aider-based (Gauthier, 2024) iterative writing approach of The AI Scientist-v1 with a simpler, single-pass generation followed by a separate reflection stage powered by reasoning models such as ol (OpenAI, 2024). We include a full list of sampling hyperparameters and models used in Appendix A and the prompts used for The AI Scientist-v2 in Appendix B.</p>
<h3>3.1. More General Idea Generation</h3>
<p>A key conceptual shift in The AI Scientist-v2 is the approach to research idea generation. Unlike the predecessor system, which primarily focused on proposing incremental modifications or extensions based on an existing codebase, The AI Scientist-v2 adopts a process that begins at a higher level of abstraction. The system is prompted to engage in more open-ended thinking about potential research directions, hypotheses, and experimental designs, akin to formulating a research abstract or grant proposal before committing to a specific implementation.</p>
<p>This approach encourages the exploration of potentially more novel or foundational ideas, rather than being constrained by the structure and topics of pre-existing code. It aligns more closely with how researchers often develop broader research visions, starting with abstract concepts and assessing novelty and feasibility before diving into specific implementations. Crucially, this generalized idea generation phase integrates literature review tools, such as Semantic Scholar, in the loop. The system can query the literature database during the idea formulation process to assess the novelty of a proposed concept and identify relevant prior work. This allows for more informed decisions about pursuing a particular research avenue, ensuring ideas are grounded in the existing scientific landscape from the outset, rather than relying solely on post-hoc checks.</p>
<h3>3.2. Removing Template Dependency</h3>
<p>Following the improved idea generation phase, The AI Scientist-v2 proceeds with experimentation. Beyond the code-conditioned idea generation, The AI Scientist-v1 also depended on the predefined template code as a starting baseline implementation. The LLM-driven code changes were then limited to sequential code adaptations. We now outline our strategy for eliminating this limitation, thus improving the system's flexibility and autonomy.</p>
<h3>3.2.1. Experiment Progress Manager</h3>
<p>Real-world scientific experimentation typically proceeds through distinct stages, from initial feasibility assessments to detailed ablation analyses. To emulate this structured approach, we introduce an</p>
<p>experiment progress manager agent that coordinates four clearly defined stages of scientific experimentation:</p>
<p>Stage 1 Preliminary Investigation: Establishing initial feasibility and correctness through a minimal working prototype based on the generated research idea.
Stage 2 Hyperparameter Tuning: Refining the initial implementation by optimizing critical hyperparameters (e.g., learning rate, epochs) to create a robust experimental baseline.
Stage 3 Research Agenda Execution: Systematically implementing the core research agenda based on the tuned baseline.
Stage 4 Ablation Studies: Systematically assessing the importance of various research components, providing rigorous support for the main experimental findings.</p>
<p>Each stage has explicit stopping criteria. Stage 1 concludes when a basic working prototype is successfully executed. Stage 2 ends when experiments stabilize, as indicated by convergence in training curves and successful execution across at least two datasets. Stages 3 and 4 conclude when the allocated computational budget is exhausted. Stage 3 also includes a check for experiment duration-if runs finish much faster than the pre-allocated runtime, the system suggests increasing the complexity of experiments.</p>
<p>After each stage, the experiment manager selects the best-performing node using a dedicated LLM evaluator (see next section) based on clearly articulated criteria. This selected node is then carried forward to seed the subsequent experimentation stage. The manager also records checkpoints at each stage's completion. To ensure scientific rigor and reproducibility, the experiment manager launches multiple replications of the selected best experiments at the conclusion of each stage. These repeated runs provide statistics (mean and standard deviation) for figures and reported results.</p>
<h1>3.2.2. Parallelized Agentic Tree Search</h1>
<p>The AI Scientist-v1 operated strictly linearly, where each code refinement directly built on the immediately preceding experiment. In contrast, The AI Scientist-v2 adopts a significantly more flexible and exploratory approach inspired by recent successes in integrating tree search with LLM-driven workflows (Chan et al., 2025; Jiang et al., 2025; Wijk et al., 2024) and research on openendedness (Clune, 2019; Mouret and Clune, 2015). We incorporate this agentic tree search approach across all four experimentation stages outlined in §3.2.1, enabling deeper and more systematic exploration of scientific hypotheses.</p>
<p>Each experimental node within our tree-based framework undergoes the following execution cycle: An LLM first generates both a concrete experimentation plan and the associated Python code to implement the experiment. The generated code is immediately executed in a Python interpreter. If execution encounters an error, the error message is recorded, and the node is marked as buggy, ending the current execution cycle for that node. If execution succeeds, the experiment proceeds to the plotting phase.</p>
<p>During each experiment, the system is instructed to save all relevant experimental outputs (training and validation metrics, losses, etc.) into structured numpy files. In the plotting phase, The AI Scientist-v2 reads these stored results and the code, generating visualizations that summarize and illustrate the findings clearly. These visualizations are subsequently passed to a Vision-Language Model (VLM) for critique. Any issues flagged by the VLM (such as unclear labels, missing legends, or misleading visualizations) result in the node being marked as buggy, and this feedback is recorded for future debugging. Nodes that successfully execute and pass the VLM review without issue are designated as non-buggy.</p>
<p>We define each node as a collection comprising an experiment script (e.g., a Python file), a textual</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | The AI Scientist-v2 workflow showing different stages of tree-based experimentation. Stage 1 begins at the root node, where initial experiment code is generated in parallel. After running the experiment code and visualization scripts, each node is classified based on the outcome: if an error occurs, it is marked as a buggy node; otherwise, it is labeled as a non-buggy node. New child nodes are created differently depending on their parent node's status: For non-buggy nodes, refinement is applied to improve the experiment code for better performance. For buggy nodes, the system attempts to debug them using stored error information. A best-performing node, selected by LLM-based evaluation, is passed down as the root node of Stage 2. From this root node, child nodes are created for hyperparameter tuning. The top-performing node from Stage 2 is then used to initialize Stage 3 , where the system executes the research agenda, applies refinements, and performs debugging as needed. In Stage 4, similar to Stage 2, the root node generates ablation nodes. Additionally, replication nodes repeat the same experiment as their parent node, while aggregation nodes collect results from replication nodes to generate combined visualizations and summaries.</p>
<p>description of the high-level plan implemented in the script, an execution error trace (if applicable), experiment runtime, performance metrics recorded during the experiment, feedback from an LLM after running the script, a visualization script, file paths to the generated figures, feedback from a VLM on those figures, and the node's final status (either buggy or non-buggy).</p>
<p>At each iteration, the system selects several nodes from the existing tree to expand in parallel. With a predefined probability, a buggy node is chosen (thus prioritizing error resolution and debugging); otherwise, a non-buggy node is selected for further refinement and improvement. When choosing between non-buggy nodes, the system uses a best-first search strategy, guided by an LLM that evaluates candidates based on factors like performance metrics, training dynamics, and the quality of generated plots. The selected nodes are expanded by creating a new child node that may either attempt debugging if the parent node was buggy, or refine and improve upon the previous experiment if the parent was non-buggy. An LLM is used to generate the plan and experiment code for each new child node, after which all new nodes are executed concurrently in parallel, significantly accelerating the exploration process. In addition to buggy and non-buggy nodes, we introduce specialized node variants tailored to specific experimental needs:</p>
<ul>
<li>Hyperparameter nodes systematically explore alternative hyperparameter configurations during Stage 2. The system maintains careful records of previously tested hyperparameters, preventing redundant experiments. Errors encountered during hyperparameter tuning trigger the creation of corresponding debug nodes.</li>
<li>Ablation nodes evaluate crucial ablation studies during Stage 4, assessing the importance of various components or assumptions underlying the experiment. Similar to hyperparameter nodes, previously tested ablation conditions are tracked to avoid repetition, and debugging nodes are created in response to any encountered errors.</li>
<li>Replication nodes execute replicates of their parent experiments using different random seeds. Typically, several replication nodes are created to enable the calculation of statistical measures (mean and standard deviation) of experimental outcomes, enhancing result robustness.</li>
<li>Aggregation nodes are special nodes created to consolidate and visualize the combined results of replication nodes. Unlike other node types, aggregation nodes do not conduct new experiments but simply generate a Python script to aggregate and summarize prior results, producing figures that explicitly show means and standard deviations.</li>
</ul>
<p>The structured design of experiment stages and tailored node types facilitates systematic exploration across all stages. Unlike some LLM agents that rigidly follow predefined, fine-grained workflow graphs, our approach adopts a looser structure that guides the entire empirical research cycle, enabling flexible system behavior while maintaining coherence across iterative stages.</p>
<h1>3.3. Dataset Loading via Hugging Face</h1>
<p>Most empirical machine learning research relies heavily on publicly available datasets. Hugging Face Hub provides a convenient and unified framework for accessing a wide variety of commonly used datasets, complete with predefined train, validation, and test splits. In The AI Scientist-v2, we prompt the system to leverage Hugging Face Hub whenever possible, automatically downloading required datasets using the standard one-line function (datasets.load_dataset). While this standardized approach greatly simplifies dataset handling, we acknowledge it is somewhat ad-hoc, as not all dataset repositories support this method.</p>
<h3>3.4. Vision-Language Model Reviewer</h3>
<p>Unlike The AI Scientist-v1, which did not leverage Vision Language Models (VLMs), The AI Scientist-v2 incorporates VLMs at two phases of the research workflow: First, during the tree-based experimentation phase, VLMs provide immediate feedback on generated figures, ensuring</p>
<p>that these visualizations effectively and accurately communicate experimental results. Second, during the manuscript writing reflection stage, VLMs evaluate figures and their captions, enhancing the visual clarity and coherence of the resulting paper.</p>
<p>In the paper-writing process, we extract screenshots of figures alongside their captions and the corresponding text from the paper that references them (identified by the keyword "Figure X"). These images and textual references are then provided to the VLM, which performs multiple quality checks, including verifying the alignment between figures and captions, identifying issues with visual clarity (e.g., missing legends, unclear labels), and detecting potential duplication of figures in the main text and appendix. Through the iterative integration of VLM feedback, we significantly enhance the visual quality and clarity of manuscripts generated by The AI Scientist-v2.</p>
<h1>4. Human Evaluation of Manuscripts Generated by The AI Scientist-v2</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Peer-reviewed ICBINB workshop paper generated by The AI Scientist-v2. The paper investigates the usage of a temporal consistency regularizer on the embeddings of an LSTMbased sequence model. The results discuss the effect of the regularizer on compositional regularization and highlight the difficulty of training models capable of improved generalization. It received peerreview scores of 6 (weak accept), 7 (accept), and 6 (weak accept) before meta-review and ranked among the top $45 \%$ submitted workshop papers.</p>
<p>To rigorously evaluate the capabilities and limitations of our automated scientific discovery system, we conducted a human evaluation study in collaboration with the organizers of the ICLR 2025 workshop, "I Can't Believe It's Not Better" (ICBINB). As detailed below, our evaluation included submitting fully automated manuscripts generated by The AI SCIENTIST-v2 to the official peer-review process of the workshop.</p>
<h3>4.1. Evaluation Methodology</h3>
<p>Our evaluation process involved the following carefully controlled steps:</p>
<ol>
<li>AI-Generated Submissions: We generated three complete manuscripts using only THE AI Scientist-v2, starting from broad topical prompts aligned with the workshop's scope via the generalized idea generation process ( $\S 3.1$ ). After the initial topic definition, the entire scientific workflow-hypothesis formulation, experiment design, coding, data analysis, visualizations, and manuscript writing-was executed autonomously, without any human intervention or editing.</li>
<li>Blind Peer Review: The three AI-generated submissions were included among the 43 total submissions received by the ICBINB workshop. Reviewers were informed in advance that some submissions might be AI-generated, but were not told which submissions were produced by THE AI Scientist-v2. Reviewers could also opt out of reviewing potentially AI-generated manuscripts.</li>
<li>Review Outcomes and Acceptance Decisions: Among the three manuscripts produced by THE AI Scientist-v2, one manuscript achieved a sufficiently high average reviewer score ( 6.33 out of 10 , with individual scores of 6,6 , and 7 ) to surpass the workshop's acceptance threshold. The remaining two submissions received lower scores and were not accepted.</li>
<li>Post-Review Withdrawal: Prior to the workshop submission, we arranged with the workshop organizers and ICLR leadership that any accepted AI-generated manuscripts would be withdrawn after the review process. This decision was made to avoid prematurely setting a precedent for purely AI-generated research entering the official scientific record without broader community discussion and consensus. Reviewers were informed of the experiment only after peer review.</li>
</ol>
<p>In addition to the official workshop peer reviews, we also conducted a thorough internal evaluation of all three AI-generated manuscripts. Our internal review carefully examined the experimental rigor, clarity of presentation, methodological soundness, and novelty of the generated manuscripts. We concluded that none of the manuscripts met the quality standards typical of top-tier main-track conferences. However, we thought that one submission was indeed sufficiently compelling to receive acceptance at the workshop level, and this is the same manuscript the workshop peer review process accepted. This outcome provides encouraging evidence that manuscripts autonomously generated by The AI Scientist-v2 can produce research on par with top-tier Machine Learning workshop papers (see detailed internal analyses in §4.2).
Observations and Insights. Our internal inspection of the generated experiments and code revealed several noteworthy limitations. First, The AI Scientist-v2 occasionally introduced inaccuracies in citations, similar to the well-known "hallucination" issue encountered in large language models. Second, while the system successfully executed standard experimental pipelines, it sometimes lacked the detailed methodological rigor and in-depth analysis typically required for acceptance at leading main conferences. However, such limitations did not prevent acceptance at the workshop level.</p>
<p>Transparency and Ethical Considerations. We believe it is crucial for the scientific community to engage openly and transparently with AI-generated research, subjecting it to the same rigorous peer-review processes applied to human-authored work. However, responsible oversight is essential. In conducting this evaluation, we obtained IRB approval from the University of British Columbia (H24-02652). We ensured full transparency and coordination with ICLR leadership and the workshop organizers. Before the review process, reviewers were explicitly informed that some submissions could be AI-generated and offered the option to opt out. Following acceptance, we withdrew the AI-generated manuscript prior to publication, which is consistent with our commitment to avoid prematurely inserting purely AI-generated works into the official scientific record without broader community discussion. We emphasize that the community has not yet reached a consensus on integrating AI-generated research into formal scientific publications, making careful and transparent experimentation essential at this preliminary stage. Additionally, we believe that all AI-generated papers should be clearly labeled as such in any public arena, and in The AI Scientist-v1 and The AI Scientist-v2 always make sure to do so.</p>
<h1>4.2. The first AI-generated peer-reviewed workshop paper.</h1>
<p>Paper Generation Process. The generation process for the workshop-accepted paper began with the generalized idea generation phase (\$3.1), prompted with the workshop's theme (ICBINB's focus on negative results and unexpected findings) extracted from the official website. In this phase, the system generated around twenty potential research ideas, all centered on core machine learning topics. To encourage a more applied perspective, we then modified the system prompt to focus on ideas involving the use of deep learning in real-world domains such as finance, psychology, agriculture, environmental science, and public health. This second phase produced another set of roughly twenty research ideas. From this combined AI-generated pool, we selected the three most promising initial ideas-two from the first batch, and one from the second batch-based on alignment with the workshop theme and potential interest, focusing on topics representing distinct research directions. This initial idea selection step allowed us to manage computational resources by choosing which distinct, AIgenerated starting points to explore further with the full system. It did not involve modifying the ideas themselves. All three generated ideas resulted in a workshop-submitted paper (included in full in Appendix C). For each selected idea, the system autonomously executed the full experimental pipeline using the parallelized agentic tree search (\$3.2.2) multiple times, each initiated with a different random seed. From the multiple complete manuscripts generated for each initial idea (i.e., one manuscript per seed), we selected the single best-resulting manuscript for submission based on a careful inspection of its overall coherence and scientific quality. This process mimics a professor reviewing the work of many students or teams and deciding which work is ready to be submitted for peer review. Our current study aims to see whether The AI Scientist-v2 can produce at least one paper that survives peer review, and not what fraction of the time it can do so. That is an interesting question for future work and is likely best done after additional improvements are made in the next generation of The AI Scientist. In the reflection stage of the writeup for each run, The AI Scientist-v2 is prompted with the target page lengths (e.g., the 4-page limit for the workshop) alongside the current length of the compiled PDF. This allowed the system to ensure that the final output adhered to submission guidelines without manual text editing within that specific run.</p>
<p>Crucially, while humans initiated the process by providing the high-level workshop theme and selected which initial AI-generated ideas to run multiple times through the full pipeline (akin to deciding which experiments to fund or prioritize), and subsequently selected the most promising complete output from those multiple runs, the entire process within any single run-hypothesis refinement, code generation, execution, analysis, visualization, and writing-was performed autonomously by The AI Scientist-v2. No human edited the generated code, experimental results, figures, or manuscript text of the selected final manuscript. The selection of initial ideas from the AI's output, the execution of multiple seeds, the subsequent selection of the best complete run, and the automated handling of length constraints represent high-level experimental setup and process management (meta-selection from fully autonomous outputs), not human-in-the-loop intervention in the scientific content generation of the chosen manuscript. The system, if run for sufficiently many seeds, would have generated similar outputs, requiring only the final selection step to be performed by humans. Even this could have been avoided were we willing to send all generated papers to peer review, which we did not want to do. Therefore, all submitted content was entirely generated by THE AI SCIENTIST-v2.</p>
<p>Workshop-Accepted Paper Content. The paper investigates the use of compositional regularization to improve generalization in neural networks. The AI Scientist-v2 proposes adding an explicit regularization term to the training loss function, encouraging networks to develop compositional representations to encourage representations to not change much over time while processing inputs. However, contrary to its expectations, experiments using synthetic arithmetic expression datasets revealed that this approach did not significantly enhance generalization performance. In fact, com-</p>
<p>positional regularization sometimes hindered model training. Furthermore, increasing arithmetic expression complexity made generalization even worse, irrespective of regularization. The paper concludes that explicitly enforcing compositional structures via regularization alone may not be sufficient and highlights potential conflicts between compositional regularization and the primary learning objective. It recommends future exploration of alternative regularization methods and different architectural approaches to better address compositional generalization issues. We provide the full annotated paper in Appendix C.</p>
<h1>Initial Idea for the Workshop-Accepted Paper</h1>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;Title&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Enhancing Compositional Generalization in Neural Networks via Compositional</span>
<span class="n">Regularization</span><span class="s2">&quot;,</span>
<span class="s2">&quot;Short Hypothesis&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Introducing a compositional regularization term during training can</span>
<span class="n">encourage</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">networks</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">develop</span><span class="w"> </span><span class="n">compositional</span><span class="w"> </span><span class="n">representations</span><span class="p">,</span><span class="w"> </span><span class="n">thereby</span><span class="w"> </span><span class="n">improving</span><span class="w"> </span><span class="n">their</span>
<span class="n">ability</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">generalize</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">novel</span><span class="w"> </span><span class="n">combinations</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">known</span><span class="w"> </span><span class="n">components</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="s2">&quot;Experiments&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;Implement the compositional regularization term and integrate it into the loss function of</span>
<span class="w">    </span><span class="n">standard</span><span class="w"> </span><span class="n">sequence</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">sequence</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">architectures</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">attention</span><span class="w"> </span><span class="n">mechanisms</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Train models on synthetic datasets like SCAN and COGS, evaluating performance on</span>
<span class="w">    </span><span class="n">compositional</span><span class="w"> </span><span class="n">generalization</span><span class="w"> </span><span class="n">tasks</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="n">term</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Apply the method to real-world tasks such as machine translation using the IWOLT dataset</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="n">semantic</span><span class="w"> </span><span class="n">parsing</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">GeoQuery</span><span class="w"> </span><span class="n">dataset</span><span class="p">,</span><span class="w"> </span><span class="n">assessing</span><span class="w"> </span><span class="n">improvements</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">generalization</span><span class="w"> </span><span class="n">to</span>
<span class="w">    </span><span class="n">new</span><span class="w"> </span><span class="n">language</span><span class="w"> </span><span class="n">constructs</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Analyze the learned representations by visualizing embedding spaces and utilizing</span>
<span class="w">    </span><span class="n">compositionality</span><span class="w"> </span><span class="n">metrics</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">assess</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="n">affects</span><span class="w"> </span><span class="n">internal</span>
<span class="w">    </span><span class="n">representations</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Conduct ablation studies to determine the impact of different strengths of the</span>
<span class="w">    </span><span class="n">regularization</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">identifying</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">optimal</span><span class="w"> </span><span class="n">balance</span><span class="w"> </span><span class="n">between</span><span class="w"> </span><span class="n">enforcing</span><span class="w"> </span><span class="n">compositionality</span><span class="w"> </span><span class="ow">and</span>
<span class="w">    </span><span class="n">maintaining</span><span class="w"> </span><span class="n">overall</span><span class="w"> </span><span class="n">performance</span><span class="o">.</span><span class="s2">&quot;,</span>
<span class="w">    </span><span class="s2">&quot;Compare the proposed method against other approaches aimed at improving compositional</span>
<span class="w">    </span><span class="n">generalization</span><span class="p">,</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">learning</span><span class="w"> </span><span class="n">techniques</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">specialized</span><span class="w"> </span><span class="n">architectures</span><span class="o">.</span><span class="s2">&quot;</span>
<span class="p">],</span>
<span class="p">]</span>
</code></pre></div>

<p>Paper Assessment by the Authors. In our review, we evaluated the technical aspects of this paper and identified several strengths and weaknesses. We appreciated the exploration of temporal consistency regularization-penalizing large changes in embedding representations between successive tokens-as an interesting method to enhance compositional generalization. The synthetic arithmetic task chosen by the authors was appropriate, providing a suitable setting to test their hypothesis across varying levels of complexity. However, we noted several areas requiring improvement. First, the description of the regularization term was unclear and potentially misleading, as readers might incorrectly assume it was applied to the LSTM hidden states rather than input embeddings. We recommended clarifying this explicitly by adding a code appendix or conducting additional ablations applying the regularization to LSTM hidden states. Second, the paper omitted key references, notably Hochreiter and Schmidhuber (1997), and instead relied on general textbook citations. Additionally, we found inaccuracies in some figures and descriptions: specifically, the caption of Figure 3 incorrectly interpreted validation loss, and Figure 5's attention-based model clearly outperformed the LSTM model, contradicting the authors' claims. Furthermore, we found the experimental evaluation limited, as the tasks were restricted to short sequences and synthetic data. We suggested extending the evaluation to include real-world tasks, longer sequences, larger models, and a deeper analysis.</p>
<p>Our examination of the code revealed potential issues with dataset overlap—approximately 57\% overlap between training and test sets-which could significantly affect the reliability of the results. Additionally, we identified confusion in the paper's terminology regarding "embedding states" versus "hidden states," which should be clarified for precision. We also questioned the reported $100 \%$ accuracy of the attention-augmented LSTM model, as our additional tests indicated that this performance was primarily due to task simplicity and significantly decreased when task complexity increased. Overall,</p>
<p>we considered the paper technically sound and a borderline accept for the workshop, acknowledging its valuable insights and intriguing ideas. However, we concluded it lacks sufficient depth and rigor for acceptance into a full conference without addressing the highlighted concerns.</p>
<p>Paper Assessment by Human Workshop Reviewers. The reviewers generally agree that the paper addresses an important topic-compositional generalization in neural networks and appreciate the authors' proposed compositional regularization method, as well as their detailed analysis of unexpected results. All reviewers recognize the paper's strength in clearly presenting why the regularization term does not yield the anticipated improvements, emphasizing its informative negative results. However, the reviewers highlight several areas for improvement:</p>
<p>Justification and Intuition: All reviewers suggest the need for clearer justification or intuition behind why penalizing large changes between successive hidden states might improve compositionality. They recommend adding references to related works, theoretical motivations, or visual explanations to strengthen the rationale.
Network Architecture Generalization: Reviewers emphasize that since only the LSTM architecture was evaluated, the findings should not be generalized across all neural network types. They suggest experimenting with additional architectures, such as transformers, to better understand the impact of the regularization across different neural network models.
Experimental Breadth: Reviewers suggest extending the evaluation to other tasks or datasets beyond synthetic arithmetic expressions to further validate the generalizability of the conclusions.
Overall: The reviewers recommend acceptance to the workshop due to the paper's insightful exploration and clear analysis despite its negative results. They encourage further elaboration on methodological motivations, additional experimental evaluations, and clearer connections between compositional regularization and the complexity of compositional tasks. The paper received scores of 6 (weak accept), 7 (accept), and 6 (weak accept). Below, we include two of the reviews for which we obtained explicit permission from the reviewers to include them in our report. The remaining reviewer did not respond to our request.</p>
<h1>Reviewer # 1: A good paper analyzing the effectiveness of a compositional regularization term for LSTMs</h1>
<p>Summary: The authors propose a regularization term to enhance compositional regularization in neural networks. The idea is to penalize large deviations between subsequent time steps of the hidden state, thus "squeezing" the hidden state to encourage composition and preventing a dominating representation. The authors test their approach on synthetic arithmetic expression with varying operator complexity and length. They show that although the regularization term appears to be working, it counterintuitively does not improve test accuracy. Furthermore, the authors identify a bottleneck regarding network capacity with increasing arithmetic operators.</p>
<p>Strengths:
I find the idea of regularizing or squeezing the hidden representations to encourage compositionally an interesting idea. The authors define a good baseline and ablate their method well against it, revealing why the regularization term does not work as expected. I think the insight that operator complexity is a bottleneck for the neural network is important, as it raises the question whether architectural changes might be more effective for compositionally than regularization.</p>
<p>Weaknesses:
The paper would benefit from more intuition as to why the proposed regularization term should encourage compositionality. This could be either an experiment or simply a visualization for the reader. Only one architecture (LSTM) was tested. It would be interesting to see if transformer architectures fare better with compositionality due to the attention mechanism.
I think the connection between compositional regularization and operator complexity needs to be made more explicit. From reading the introduction both arguments seem a bit disconnected although I can infer the authors intentions.</p>
<p>Conclusion:
Overall, I would accept this paper to the workshop, since it proposes a simple and interesting</p>
<div class="codehilite"><pre><span></span><code><span class="n">idea</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">authors</span><span class="w"> </span><span class="n">providing</span><span class="w"> </span><span class="n">ablations</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">encourage</span><span class="w"> </span><span class="n">further</span><span class="w"> </span><span class="n">analysis</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">problem</span><span class="p">.</span><span class="w"> </span><span class="n">As</span><span class="w"> </span><span class="n">a</span>
<span class="n">suggestion</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">encourage</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">authors</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">intuition</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">why</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proposed</span>
<span class="n">regularization</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">improve</span><span class="w"> </span><span class="n">compositionality</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">network</span><span class="p">.</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">suggest</span>
<span class="n">either</span><span class="w"> </span><span class="n">adding</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">related</span><span class="w"> </span><span class="n">work</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">regularisation</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">elaborating</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span>
<span class="n">intuition</span><span class="w"> </span><span class="n">behind</span><span class="w"> </span><span class="n">penalising</span><span class="w"> </span><span class="n">subsequent</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">hidden</span><span class="w"> </span><span class="n">state</span><span class="p">.</span>
<span class="nl">Rating:</span><span class="w"> </span><span class="mh">7</span><span class="o">:</span><span class="w"> </span><span class="n">Good</span><span class="w"> </span><span class="n">paper</span><span class="p">,</span><span class="w"> </span><span class="n">accept</span>
<span class="nl">Award:</span><span class="w"> </span><span class="n">No</span><span class="w"> </span><span class="n">Award</span>
<span class="nl">Confidence:</span><span class="w"> </span><span class="mh">4</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">reviewer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">confident</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">absolutely</span><span class="w"> </span><span class="n">certain</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">evaluation</span><span class="w"> </span><span class="n">is</span>
<span class="n">correct</span>
</code></pre></div>

<h1>Reviewer #2: Compositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization</h1>
<p>This paper investigates the effectiveness of incorporating a compositional regularization term into the loss function of neural networks to improve compositional generalization. The authors hypothesized that penalizing deviations from compositional structures would enhance the model's ability to generalize to unseen arithmetic expressions. However, their results on synthetic arithmetic datasets showed that compositional regularization did not lead to significant improvements and, in some cases, even hindered learning.</p>
<p>I think this paper greatly contributes to the workshops theme and fits into the scope. Moreover, it is a great example of challenges that occur during such approaches and could be interesting to discuss in the workshop setting. While I think that the authors should further broaden the experiments to other tasks in order to increase the generalizability of the findings, I would still recommend to accept the paper.</p>
<p>Rating: 6: Marginally above acceptance threshold
Award: No Award
Confidence: 2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</p>
<h2>5. Limitations \&amp; Ethical Considerations</h2>
<p>While The AI Scientist-v2 demonstrates significant progress by successfully generating a peer-reviewed workshop paper, it is important to contextualize this achievement clearly. First, the acceptance occurred at a workshop level rather than at the main conference track, and only one of the three AI-generated submissions was accepted. Workshop papers generally report preliminary results and exploratory work, and acceptance rates at workshops (typically 60-80\%) are notably higher than at main conference tracks ( $20-30 \%$ for leading machine learning venues such as ICLR, ICML, and NeurIPS). Thus, the current version of The AI Scientist-v2 does not yet consistently reach the rigorous standard required for top-tier conference publications, nor does it even reach workshop-level consistently.</p>
<p>Moreover, despite the structured agentic tree search and enhanced autonomy introduced in The AI SCIENTIST-v2, certain aspects of scientific inquiry-such as formulating genuinely novel, highimpact hypotheses, designing truly innovative experimental methodologies, or rigorously justifying design choices with deep domain expertise-remain challenging for purely automated systems. Addressing these limitations in future iterations will be essential to move beyond preliminary or incremental scientific results toward consistently high-quality, conference-level contributions.</p>
<p>As LLMs rapidly advance, future versions of our system will likely overcome many current limitations. Therefore, we believe it is important for the scientific community to study the quality of AI-generated research, and one of the best ways to do so is to submit (with appropriate permissions) a small sample of it to the same peer-review processes used to evaluate human work. We conducted this study with full cooperation from both ICLR leadership and the workshop organizers, and received IRB approval from the University of British Columbia (H24-02652). Per agreement with ICLR workshop organizers, our AI-generated papers will not appear on OpenReview's public forum and have already</p>
<p>been withdrawn. As a community, we need to establish norms for AI-generated science-including disclosure requirements and timing. We advocate for transparency about AI-generated content, though questions remain about whether work should first be judged on merit to avoid bias. Going forward, we will continue to exchange opinions with the research community on the state of this technology to ensure it does not evolve solely to game peer review or artificially inflate the CVs of unscrupulous scientists, which would undermine the meaning of the scientific peer review and evaluation processes.</p>
<h1>6. Related Work</h1>
<p>Recent advancements have substantially expanded the field of automated scientific discovery, particularly through approaches leveraging artificial intelligence (AI). Early end-to-end approaches, exemplified by The AI Scientist-v1 (Lu et al., 2024), introduced fully automated frameworks, such as AI-Researcher (Data Intelligence Lab, 2025), capable of autonomously navigating the entire research pipeline. Subsequent works, however, often incorporate varying degrees of human oversight, as demonstrated by Intology (Intology AI, 2025) and Carl (AutoScience AI, 2025). Other systems narrow the scope; for example, CycleResearcher (Weng et al., 2025) focuses specifically on the path from idea generation to manuscript drafting, explicitly excluding experimental execution. Alternative approaches include protocol designs for experiments in self-driving laboratories that do not rely on large language models (LLMs) or use them in complementary roles (Shi et al., 2025). Several concurrent works explore similar territories, including Agent Laboratory (Schmidgall et al., 2025) and agentRxiv (Schmidgall and Moor, 2025), highlighting the rapid development in this area.</p>
<p>LLM-based scientific idea generation has been explicitly investigated in recent studies. Notably, Si et al. (2025) examined the capabilities of LLMs to generate human-level scientific ideas, finding through human evaluations that LLM-generated ideas were typically more novel but often less feasible than those proposed by human experts. GraphEval (Feng et al., 2025) offers graph-based methods for evaluating research ideas, further highlighting the current limitations of LLMs in accurate idea assessment.</p>
<p>Several benchmarks have been established to systematically evaluate AI performance in scientific tasks. MLEBench (Chan et al., 2025) and Aide (Jiang et al., 2025) provide structured environments to assess model capabilities on tasks representative of research engineering workloads. The METR Research Engineer benchmark (Wijk et al., 2024), for instance, demonstrates AI superiority in executing short-duration tasks (sub-2-hour tasks). Comprehensive reviews, such as the one by Eger et al. (2025), document the role and effectiveness of LLMs in scientific workflows. Coding-specific benchmarks such as SciCode (Tian et al., 2024), curated explicitly by domain scientists, address problems across physics, chemistry, and biology, encompassing structured sub-problems to rigorously evaluate research-related programming skills. Similarly, BixBench focuses on computational biology, providing comprehensive evaluations of LLM-based agents (Mitchener et al., 2025). Additionally, independent evaluations specifically target AI scientist frameworks, like the evaluation of The AI Scientist-v1 by Beel et al. (2025), further delineate AI capabilities in this domain.</p>
<p>Industry efforts, including Google's AI Research Copilot (also known as AI Co-Scientist, Gottweis et al., 2025), exemplify contributions from major technology companies to this growing field. Conceptually, Bengio et al. (2025) draws a distinction between agentic AI systems and Scientist AIs, emphasizing that the latter focus primarily on deepening the understanding of data rather than pursuing goaldirected interactions with the world. This distinction underscores the varying philosophical and methodological perspectives driving contemporary automated scientific discovery efforts.</p>
<h1>7. Conclusion</h1>
<p>In this work, we introduced The AI Scientist-v2, a significantly improved automated scientific discovery system featuring enhanced autonomy and exploration capabilities. Compared to its predecessor, The AI Scientist-v1, our system removes reliance on human-crafted templates, incorporates a structured and exploratory agentic tree search methodology supervised by an experiment manager agent, and integrates Vision-Language Model (VLM) feedback loops for iterative refinement of visualizations and manuscript quality. We demonstrated that The AI Scientist-v2 is capable of autonomously generating manuscripts that successfully pass peer review at a workshop of a major machine learning conference.</p>
<p>This achievement, the first instance of a fully AI-generated paper navigating peer review, marks a notable milestone and shows promising early signs of progress, even considering the limitations discussed regarding workshop versus conference standards (§5). While significant challenges remain in consistently achieving top-tier quality and generating truly groundbreaking hypotheses, the capabilities demonstrated here suggest a clear trajectory. We believe that such advancements signal that next-generation AI Scientists will herald a new era in science. This is just the beginning; we expect AI capabilities to continue improving, potentially at an exponential rate. At some point in the future, AI will likely generate papers that match or exceed human quality, even at the highest levels of scientific publishing.</p>
<p>Ultimately, overcoming current limitations and scaling these systems holds immense potential. We believe what matters most is not simply how AI science compares to human science, but whether its discoveries aid in human flourishing, such as curing diseases or expanding our knowledge of the laws that govern our universe. By developing systems like The AI Scientist-v2 and sharing them openly, we look forward to helping usher in this era of AI science contributing to the betterment of humanity, fostering collaboration and accelerating the pace of discovery.</p>
<h2>References</h2>
<p>AutoScience AI. Meet Carl: The First AI System to Produce Academically Peer-Reviewed Research, 2025. URL https://www.autoscience.ai/blog/meet-carl-the-first-ai-system-t o-produce-academically-peer-reviewed-research. Accessed: 2025-03-21.</p>
<p>Joeran Beel, Min-Yen Kan, and Moritz Baumgart. An evaluation of sakana's ai scientist for autonomous research: Wishful thinking or an emerging reality towards' artificial general research intelligence'(agri)? arXiv preprint arXiv:2502.14297, 2025.</p>
<p>Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, and David Williams-King. Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path?, 2025. URL https://arxiv.org/abs/2502.15657.</p>
<p>Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Aleksander Madry, and Lilian Weng. MLEbench: Evaluating machine learning agents on machine learning engineering. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/f orum?id=6s5uXNWGIh.</p>
<p>Jeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artificial intelligence. CoRR, abs/1905.10985, 2019. URL http://arxiv.org/abs/1905.10985.</p>
<p>Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R. Josephson, Joao Goncalves, Kenneth L. Clarkson, Nimrod Megiddo, Bachir El Khadir, and Lior Horesh. Combining data and theory for</p>
<p>derivable scientific discovery with ai-descartes. Nature Communications, 14(1):1777, Apr 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-37236-y. URL https://doi.org/10.1038/s414 $67-023-37236-y$.</p>
<p>The University of Hong Kong Data Intelligence Lab. Ai-researcher: Fully-automated scientific discovery with llm agents, 2025. URL https://github.com/HKUDS/AI-Researcher.</p>
<p>Steffen Eger, Yong Cao, Jennifer D'Souza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li, et al. Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation. arXiv preprint arXiv:2502.05151, 2025.</p>
<p>Tao Feng, Yihang Sun, and Jiaxuan You. Grapheval: A lightweight graph-based LLM framework for idea evaluation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=5RUM1aIdok.</p>
<p>Paul Gauthier. Aider is ai pair programming in your terminal, 2024. URL https://aider. chat/. https://aider.chat/.</p>
<p>Yolanda Gil, Mark Greaves, James Hendler, and Haym Hirsh. Amplify scientific discovery with artificial intelligence. Science, 346(6206):171-172, 2014. doi: 10.1126/science.1259439. URL https://www.science.org/doi/abs/10.1126/science. 1259439.</p>
<p>Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025.</p>
<p>Intology AI. Zochi Tech Report, 2025. URL https://www.intology.ai/blog/zochi-tech-r eport. Accessed: 2025-03-21.</p>
<p>Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide: Ai-driven exploration in the space of code, 2025. URL https://arxiv.org/ abs/2502.13138.</p>
<p>Ross D. King, Jem Rowland, Stephen G. Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria Liakata, Magdalena Markham, Pinar Pir, Larisa N. Soldatova, Andrew Sparkes, Kenneth E. Whelan, and Amanda Clare. The automation of science. Science, 324(5923):85-89, 2009. doi: 10.1126/sc ience.1165620. URL https://www.science.org/doi/abs/10.1126/science. 1165620.</p>
<p>Hiroaki Kitano. Nobel turing challenge: creating the engine for scientific discovery. npj Systems Biology and Applications, 7(1):29, Jun 2021. ISSN 2056-7189. doi: 10.1038/s41540-021-00189-3. URL https://doi.org/10.1038/s41540-021-00189-3.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024.</p>
<p>Ludovico Mitchener, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi P Wellawatte, Andrew White, Lorenzo Sani, and Samuel G Rodriques. Bixbench: a comprehensive benchmark for llm-based agents in computational biology. arXiv preprint arXiv:2503.00096, 2025.</p>
<p>Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. ArXiv, abs/1504.04909, 2015. URL https://api.semanticscholar.org/CorpusID:14759751.</p>
<p>OpenAI. Openai o1 system card, 2024. URL https://api.semanticscholar.org/CorpusID: 272648256 .</p>
<p>Samuel Schmidgall and Michael Moor. Agentrxiv: Towards collaborative autonomous research. arXiv preprint arXiv:2503.18102, 2025.</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025.</p>
<p>Yu-Zhe Shi, Mingchen Liu, Fanxu Meng, Qiao Xu, Zhangqian Bi, Kun He, Lecheng Ruan, and Qining Wang. Hierarchically encapsulated representation for protocol design in self-driving labs. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openrevi ew.net/forum?id=9nUBh4V6SA.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can LLMs generate novel research ideas? a large-scale human study with 100+ NLP researchers. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=M23dTGWCZy.</p>
<p>Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: A research coding benchmark curated by scientists, 2024.</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Veličković, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, Aug 2023. ISSN 1476-4687. doi: 10.1038/s41586-023-06221-2. URL https://doi.org/10.1038/s41586-023-06221-2.</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Proceedings of the 41st International Conference on Machine Learning, ICML'24. JMLR.org, 2024.</p>
<p>Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. Cycleresearcher: Improving automated research via automated review. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/f orum?id=bjcsVLoHYs.</p>
<p>Hjalmar Wijk, Tao R. Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Jun Koba Sato, William Saunders, Maksym Taran, Ben West, and Elizabeth Barnes. Re-bench: Evaluating frontier ai r\&amp;d capabilities of language model agents against human experts. ArXiv, abs/2411.15114, 2024. URL https://api.semanticscholar.org/CorpusID:274192262.</p>
<p>Yongjun Xu, Xin Liu, Xin Cao, Changping Huang, Enke Liu, Sen Qian, Xingchen Liu, Yanjun Wu, Fengliang Dong, Cheng-Wei Qiu, Junjun Qiu, Keqin Hua, Wentao Su, Jian Wu, Huiyu Xu, Yong Han, Chenguang Fu, Zhigang Yin, Miao Liu, Ronald Roepman, Sabine Dietmann, Marko Virta,</p>
<p>Fredrick Kengara, Ze Zhang, Lifu Zhang, Taolan Zhao, Ji Dai, Jialiang Yang, Liang Lan, Ming Luo, Zhaofeng Liu, Tao An, Bin Zhang, Xiao He, Shan Cong, Xiaohong Liu, Wei Zhang, James P. Lewis, James M. Tiedje, Qi Wang, Zhulin An, Fei Wang, Libo Zhang, Tao Huang, Chuan Lu, Zhipeng Cai, Fang Wang, and Jiabao Zhang. Artificial intelligence: A powerful paradigm for scientific research. The Innovation, 2(4), Nov 2021. ISSN 2666-6758. doi: 10.1016/j.xinn.2021.100179. URL https://doi.org/10.1016/j.xinn.2021.100179.</p>
<h1>Author Contributions</h1>
<p>Yutaro Yamada (shared first author): Co-led the project and contributed core ideas. Coded the core tree-search and template-free version of the AI Scientist v2. Ran paper generation experiments. Read and validated the work of many AI-generated papers to select submissions and checked the paper code implementations. Led the writing of the paper. Wrote detailed analyses of the submitted papers for our manuscript.</p>
<p>Robert Tjarko Lange (shared first author): Co-initiated, co-led the project and contributed core ideas. Coded core parts of VLM AI Reviewer, tailored the paper generation pipeline to the workshop and ran the paper generation experiments. Organized the workshop communication process. Read and validated the work of many AI-generated papers to select submissions and checked the paper code implementations. Led the writing of the paper. Wrote detailed analyses of the submitted papers for our manuscript.</p>
<p>Cong Lu (shared first author): Co-initiated, co-led the project and contributed core ideas. Coded core parts of the improved idea generation, tool use, experiment aggregation, and paper writing framework. Evaluated AI-generated paper submissions. Wrote and led the IRB approval process. Led the writing of the paper.</p>
<p>Shengran Hu: Enhanced the iterative AI reviewer with VLM feedback, contributed to the experiment and paper writing framework, helped run paper generation experiments, read and validated the work of many AI-generated papers to select submissions, and checked the paper code implementations. Helped writing and iterating over drafts of the paper. Helped write the IRB approval.</p>
<p>Chris Lu: Co-initiated the project. Provided advice, feedback, and writing.
Jakob Foerster: Provided advice, feedback, and writing.
Jeff Clune (equal advising): Provided overarching guidance for the research project, offering technical insight, advice, feedback, and writing. Oversaw the IRB application process. Evaluated AI-generated paper submissions.</p>
<p>David Ha (equal advising): Provided overarching guidance for the research project, offering technical insight, advice, feedback, and writing. Oversaw the public communication process.</p>
<h1>Supplementary Material</h1>
<p>Table of Contents
A Hyperparameters ..... 21
B Prompts ..... 21
C AI Generated Papers ..... 31
C. 1 Compositional Regularization: Unexpected Obstacles in Enhancing Neural Network Generalization ..... 32
C.1.1 AI Scientist Team Review ..... 41
C.1.2 AI Scientist Team Code Review ..... 42
C. 2 Unveiling the Impact of Label Noise on Model Calibration in Deep Learning ..... 45
C.2.1 The AI Scientist-v2 Idea ..... 45
C.2.2 AI Scientist Team Review ..... 53
C.2.3 AI Scientist Team Code Review ..... 55
C.2.4 Workshop Reviews ..... 57
C. 3 Real-world Challenges in Pest Detection using Deep Learning: an Investigation into Failures and Solutions ..... 58
C.3.1 The AI Scientist-v2 Idea ..... 58
C.3.2 AI Scientist Team Review ..... 65
C.3.3 AI Scientist Team Code Review ..... 66
C.3.4 Workshop Reviews ..... 67</p>
<h1>A. Hyperparameters</h1>
<p>This section details the key hyperparameters used in The AI Scientist-v2. Model configurations for language and vision-language models are listed in Table 2. The hyperparameters governing the agentic tree search (\$3.2.2) and experiment stage (\$3.2.1) progression, including node execution limits, are shown in Table 3.</p>
<p>Table 2 | LLM and VLM Hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component/Task</th>
<th style="text-align: left;">Model Used</th>
<th style="text-align: center;">Max Tokens</th>
<th style="text-align: center;">Temperature</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Code Generation (\$3.2)</td>
<td style="text-align: left;">Claude 3.5 Sonnet (v2)</td>
<td style="text-align: center;">8,192</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">LLM/VLM Feedback Agents (\$3.4)</td>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">8,192</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Summary Report Agent (\$3)</td>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">8,192</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p>Table 3 | Agentic Tree Search \&amp; Execution Hyperparameters (\$3.2.2, §3.2.1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Debug Probability</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Maximum Debug Depth</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">Maximum Experiment Runtime per Node</td>
<td style="text-align: center;">1 hour</td>
</tr>
<tr>
<td style="text-align: left;">Node Allocation per Stage:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Stage 1: Preliminary Investigation</td>
<td style="text-align: center;">21 nodes</td>
</tr>
<tr>
<td style="text-align: left;">Stage 2: Hyperparameter Tuning</td>
<td style="text-align: center;">12 nodes</td>
</tr>
<tr>
<td style="text-align: left;">Stage 3: Research Agenda Execution</td>
<td style="text-align: center;">12 nodes</td>
</tr>
<tr>
<td style="text-align: left;">Stage 4: Ablation Studies</td>
<td style="text-align: center;">12 nodes</td>
</tr>
</tbody>
</table>
<p>The total time required for The AI Scientist-v2 to generate a single paper depends on the complexity of the problems. Based on our experience, this process usually takes anywhere from several hours to a maximum of 15 hours, which is the runtime limit we have set.</p>
<h2>B. Prompts</h2>
<p>In this section, we include the prompts used in all phases of The AI Scientist-v2.</p>
<h2>Idea Generation Prompt</h2>
<p># System prompt
You are an experienced AI researcher who aims to propose high-impact research ideas resembling exciting grant proposals. Feel free to propose any novel ideas or experiments; make sure they are novel. Be very creative and think out of the box. Each proposal should stem from a simple and elegant question, observation, or hypothesis about the topic. For example, they could involve very interesting and simple interventions or investigations that explore new possibilities or challenge existing assumptions. Clearly clarify how the proposal distinguishes from the existing literature.</p>
<p>Ensure that the proposal can be done starting from the provided codebase, and does not require resources beyond what an academic lab could afford. These proposals should lead to papers that are publishable at top ML conferences.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1152 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1152</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1152</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-222124917</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2010.01062v3.pdf" target="_blank">Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> To rapidly learn a new task, it is often essential for agents to explore efficiently -- especially when performance matters from the first timestep. One way to learn such behaviour is via meta-learning. Many existing methods however rely on dense rewards for meta-training, and can fail catastrophically if the rewards are sparse. Without a suitable reward signal, the need for exploration during meta-training is exacerbated. To address this, we propose HyperX, which uses novel reward bonuses for meta-training to explore in approximate hyper-state space (where hyper-states represent the environment state and the agent's task belief). We show empirically that HyperX meta-learns better task-exploration and adapts more successfully to new tasks than existing methods.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1152.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1152.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HyperX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyper-State Exploration (HyperX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-reinforcement learning agent that meta-learns approximate belief inference (via a VAE) and explicitly incentivises meta-exploration by adding two intrinsic bonuses: (1) a novelty bonus on approximate hyper-states using Random Network Distillation (RND) and (2) a VAE reconstruction-error bonus to drive learning of the belief model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HyperX</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy πψ conditioned on environment state s_t and an approximate belief b_t (mean+variance of a VAE latent); jointly trained with a VAE encoder qφ and decoders pθ for rewards/transitions. Intrinsic modules: an RND predictor fω trained against a fixed random prior g to estimate hyper-state novelty, and VAE reconstruction error computed via pθ to estimate belief inaccuracy. PPO is used to optimise the policy with extrinsic + intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>curiosity-driven exploration / intrinsic reward (novelty on hyper-states via RND + prediction-error / model uncertainty bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>During meta-training HyperX augments environment reward with two adaptive bonuses: r_hyper(s+,b)=||f(s+,b)-g(s+,b)||^2 (RND novelty over hyper-states) to encourage trying different task-exploration strategies, and r_error = negative log-likelihood of observed reward/next-state under VAE decoders to encourage visiting states where belief inference is poor. The policy conditions on the evolving belief b_t to choose actions; intrinsic bonus weights (λ_h, λ_e) are annealed over training so at meta-test the policy focuses on online return.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple (Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir, Sparse MuJoCo AntGoal, Sparse 2D Navigation, Meta-World ML1 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown / partially observable task identity across episodes (BAMDP setting); stochastic transitions in some MuJoCo tasks; continuous and discrete state/action spaces depending on environment; strongly sparse rewards in many testbeds; tasks are drawn from a task distribution p(M).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by environment: Treasure Mountain (continuous 4D observations, rollout length 100 steps); Multi-Stage Gridworld (3 rooms, discrete grid, H=50); Sparse HalfCheetahDir (continuous MuJoCo, 200 steps per episode for adaptation, sparse reward only outside interval); Sparse AntGoal (high-dimensional continuous state/action, sparse dense-reward radius =1, multiple episodes); meta-training budgets reported in paper: e7–e8 frames per environment (e.g., Treasure Mountain: 8e7 frames, Sparse HalfCheetahDir: 3e7 frames, Sparse AntGoal: 4e8 frames).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Substantially improved across sparse / partially observable environments versus baselines: Treasure Mountain — HyperX meta-learns the superior strategy (climb mountain to observe treasure) consistently across all 10 seeds (higher cumulative online return than VariBAD); Multi-Stage Gridworld — only HyperX solves the sequential unlocked-goals problem (finds G1→G2→G3) while VariBAD/VariBAD+r(s) fail; Sparse HalfCheetahDir — HyperX successfully meta-learns the correct within-episode adaptation strategy where standard baselines fail (exact numeric returns in paper's Table 1 show HyperX succeeds and baselines have much lower returns); Sparse AntGoal — higher success rate per episode than VariBAD, RL^2 and PEARL (HyperX finds goal often in episode 1 and achieves substantially higher first-episode success rate). Exact numeric values vary by task and are given in the paper's figures and tables (e.g., ML1-reach/push: 100% first-rollout success for HyperX and VariBAD; ML1-pick-place dense: HyperX 44.5% success).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines without HyperX's adaptive meta-exploration (VariBAD, RL^2, Belief Learning, PEARL, ProMP, E-MAML) generally perform worse on sparse tasks: e.g., VariBAD learns suboptimal task-exploration on Treasure Mountain (walk circle strategy), fails to find later-stage goals in Multi-Stage Gridworld, and has lower success rates on Sparse AntGoal; in Sparse HalfCheetahDir many baselines fail to learn the correct behaviour (returns around -150 per episode for RL^2/PEARL in one plot).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>At meta-test the learned HyperX policy often locates sparse signals in the first episode (e.g., AntGoal: finds goal in episode 1 via spiralling search); meta-training requires large sample budgets (tens to hundreds of millions of environment frames reported per task). The VAE reconstruction bonus accelerates learning of useful beliefs early in meta-training; annealing intrinsic bonuses prevents perpetual exploration at meta-test.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit: policy optimises discounted sum of extrinsic + λ_h r_hyper + λ_e r_error; λ_h and λ_e are annealed down so meta-training emphasises data collection for belief learning but meta-test focuses on exploitation of learned Bayes-adaptive behaviour. The policy uses the approximate belief b_t (from VAE) to choose actions that trade short-term cost (information-seeking) vs long-term gain (Bayes-optimal exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>VariBAD (VAE-based belief meta-learner), VariBAD+r(s) (state-novelty), RL^2 (recurrent meta-learner), Belief Learning (Humplik et al., privileged-task inference), PEARL, ProMP, E-MAML, MetaCURE (information-gain intrinsic reward), state-only RND baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Encouraging exploration in approximate hyper-state space (state + belief) is necessary for effective meta-exploration on sparse tasks; 2) Combining a hyper-state novelty bonus (RND) with a VAE reconstruction-error bonus is required when beliefs are learned jointly—r_hyper alone fails early because beliefs are initially meaningless, r_error alone is insufficient to ensure diverse task-exploration; 3) HyperX meta-learns approximately Bayes-optimal adaptation strategies on multiple sparse/partially-observable benchmarks where prior methods fail; 4) Annealing intrinsic bonuses is important to avoid persistent exploration at meta-test.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relying only on hyper-state novelty (r_hyper) fails when the belief inference is poor early in meta-training; relying only on the VAE error (r_error) may not guarantee diverse meta-exploration (70% success on Treasure Mountain when used alone). HyperX requires substantial meta-training data (tens/hundreds of millions of frames) and tuning of RND hyperparameters (sensitivity to prior weight scale). Some tasks remain challenging (e.g., sparse ML1-pick-place: only 9/20 seeds learned something with HyperX on dense version and 1/20 on sparse), indicating horizon length and problem structure can limit effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1152.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1152.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VariBAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VariBAD (Variational Bayes-Adaptive Deep RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-RL method that meta-learns an explicit approximate belief over tasks using a VAE (encoder qφ producing a latent m/belief) and conditions a policy on that belief; used as a baseline and as the belief backbone in HyperX.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Varibad: A very good method for bayes-adaptive deep rl via meta-learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VariBAD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Jointly trains a VAE (encoder qφ(m|τ:t) and decoders pθ for rewards/transitions) and a policy πψ(s_t, b_t) where b_t is the VAE posterior (mean+variance). The RL loss J(ψ) is optimised alongside an ELBO for the trajectory model; the RL loss is not backpropagated into the encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian belief-conditioned policy (approximate Bayes-adaptive RL via variational inference); implicit exploration driven by policy conditioned on posterior, but no explicit meta-exploration bonuses by default</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by maintaining an approximate task belief (VAE posterior) from trajectory τ:t and conditioning the policy on b_t to select actions; exploration emerges implicitly from meta-learned policy that has experienced the task distribution during meta-training.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same set of benchmarks as HyperX when used as a baseline (Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir, Sparse AntGoal, Sparse 2D Navigation, Meta-World ML1)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable task identity across episodes; sparse rewards in many comparisons; continuous or discrete state/action depending on benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by environment; identical to HyperX benchmarks when used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Performs well on many benchmarks with informative/dense rewards and simpler sparse tasks (e.g., ML1-reach/push: 100% first-rollout success), but fails on several hard sparse/partially observable tasks in this paper: Treasure Mountain (learns inferior circle-search strategy rather than climbing mountain), Multi-Stage Gridworld (fails to reach G3), Sparse HalfCheetahDir (fails to learn correct within-episode adaptation), Sparse AntGoal (lower success rates than HyperX).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>When VariBAD is augmented with a simple state-novelty bonus r(s) it still fails to solve some tasks (e.g., Multi-Stage Gridworld cannot reach final goal G3), indicating simple state-space exploration is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Can adapt quickly in some tasks (e.g., finds goals in a few episodes in sparse 2D navigation), but often less sample-efficient than HyperX on the hardest sparse tasks; meta-training budgets similar to HyperX.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit via conditioning on approximate belief; no explicit intrinsic bonuses in default VariBAD, so early meta-training may be dominated by myopic behaviour if sparse reward provides little learning signal.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to HyperX, HyperX ablations, RL^2, Belief Learning (Humplik), PEARL, ProMP, E-MAML, MetaCURE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>VariBAD provides a useful explicit belief representation for meta-RL, but without dedicated meta-exploration bonuses it can converge to suboptimal task-exploration strategies on sparse/partially observable tasks; augmenting with state-only novelty is insufficient in problems where hyper-state novelty matters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails on tasks requiring meta-exploration across different adaptation strategies (e.g., Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir) because beliefs are not enough to drive exploration during meta-training when rewards are sparse. No mechanism to encourage diverse task-exploration during meta-training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1152.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1152.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief Oracle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief Oracle (policy conditioned on exact belief)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental agent in this paper that is given the true (computed) Bayesian belief b_t and trained via standard RL conditioned on the exact hyper-state s+ = (s, b); used to investigate the effect of hyper-state novelty bonuses when beliefs are accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief Oracle</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A policy trained with access to the true task posterior belief b_t (computed analytically for the toy/synthetic tasks), conditioned on the exact hyper-state s+; trained with and without intrinsic bonuses to test their effect in the idealised belief-accurate setting.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>hyper-state novelty via RND (intrinsic reward on exact hyper-states); curiosity-driven intrinsic bonuses</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses exact belief b_t and RND hyper-state novelty to guide exploration: when belief updates make regions of the state space novel for the updated belief, the novelty bonus encourages exploration there; this produces approximate Bayes-optimal behaviour under the oracle belief.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sparse HalfCheetahDir (used for the controlled investigation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Two-task distribution (walk forward / walk backward), sparse reward only outside interval [-5,5], continuous MuJoCo dynamics, 200 environment steps to adapt; belief can be computed exactly in this simplified prior.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Continuous state/action (HalfCheetah), adaptation horizon 200 steps; but belief computation simplified for the two-task prior allowing exact posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>With access to exact belief and adding the hyper-state exploration bonus r_hyper, the Belief Oracle learns approximately Bayes-optimal behaviour for the Sparse HalfCheetahDir task; without the bonus the Belief Oracle still fails to learn correct behaviour for this task, demonstrating the necessity of hyper-state exploration even with exact beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Belief Oracle without hyper-state bonus fails to learn correct exploration strategy in Sparse HalfCheetahDir (even with exact belief), showing that an appropriate exploration incentive is required to discover information-seeking maneuvers.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not precisely quantified beyond experiments; oracle experiments demonstrate that targeted intrinsic bonuses can dramatically reduce required exploration at meta-test (agent can find task-informative regions within adaptation horizon), but training still requires standard RL sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Directly trades off using exact belief to exploit vs taking information-seeking actions encouraged by hyper-state novelty; because belief is exact, the RND hyper-state bonus drives exploration precisely to where it is expected to improve future returns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to same environment policies without hyper-state bonus and to HyperX which learns beliefs jointly; used to show the difference between operating with exact vs approximate beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Demonstrates that (a) novelty on hyper-states (state+belief) can induce approximately Bayes-optimal behaviour if beliefs are accurate, and (b) even an oracle that knows the belief needs explicit hyper-state exploration incentives to discover the information-gathering trajectories on sparse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Oracle setting is idealised and infeasible in general (true belief rarely available); results show that when beliefs are learned jointly they are initially inaccurate, making r_hyper alone ineffective early in meta-training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1152.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1152.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief Learning (Humplik et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief Learning (Humplik et al., 2019) - meta reinforcement learning as task inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that meta-learns to perform task inference using privileged task information during training, producing an explicit belief model used by a policy to adapt online.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Meta reinforcement learning as task inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief Learning (Humplik et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A meta-RL approach that trains an explicit belief/inference model using ground-truth task descriptions available during meta-training (privileged information), then conditions a policy on the inferred belief for fast adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>explicit task-inference + belief-conditioned policy; intrinsic exploration can be added but in the paper this method is used mainly as a baseline (privileged supervision reduces need for r_error)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Learns to infer task identity from experience (privileged during training), then policy uses inferred belief to pick actions; because inference is easier (privileged labels), the hyper-state novelty bonus r_hyper alone can be sufficient for sparse HalfCheetahDir in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as baseline across the same set of benchmarks (Sparse HalfCheetahDir and others)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same partially observable/task-uncertainty settings as other benchmarks; privileged info available at meta-training only.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as corresponding benchmarks; inference is easier due to privileged training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>With privileged task descriptions at meta-training, Belief Learning can succeed on Sparse HalfCheetahDir using only r_hyper (no r_error needed), achieving performance comparable to or better than unsupervised VariBAD+HyperX ablations on that task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not explicitly quantified in all settings within this paper; however, when not augmented with hyper-state novelty bonus it can still be limited by sparse rewards if privileged information is insufficient to guide exploration behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Privileged supervision improves sample efficiency for learning inference; exact numbers not provided in paper but experiments indicate faster/better belief learning compared to unsupervised VAE-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Relies on supervised inference to reduce exploration need; when combined with hyper-state novelty bonus it still balances exploration (try different strategies) vs exploitation via belief-conditioned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to VariBAD, HyperX and RL^2 as baselines in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Privileged training of belief inference can obviate the need for a reconstruction-error bonus (r_error) and permit hyper-state novelty (r_hyper) alone to produce successful meta-training on some sparse tasks; highlights the difficulty of unsupervised belief learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on privileged task information during meta-training, which is often unavailable in realistic settings; not directly comparable to HyperX's unsupervised belief learning scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1152.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1152.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaCURE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaCURE (Meta-trained exploration via information gain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent meta-exploration method that trains a separate exploration policy intrinsically motivated by an information-gain reward (difference in prediction errors with vs without privileged task information), designed for few-episode meta-learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn to effectively explore in context-based meta-rl</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MetaCURE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Trains a dedicated exploration policy that maximises an intrinsic reward based on information gain about the task (computed using privileged task information during meta-training); used as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information-gain maximisation (intrinsic reward), separate exploration/exploitation policies</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Exploration policy seeks transitions that maximise the reduction in task uncertainty (measured as difference in prediction errors when conditioning on current experience vs conditioning on ground-truth task), then exploitation policy uses collected data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Compared on sparse ML1 Meta-World tasks and Sparse HalfCheetahDir (adapted to online setting for comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Few-episode adaptation setting; environments can provide sparse success signal; MetaCURE requires privileged information during meta-training to compute information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Meta-World ML1 tasks (robotic arm manipulation), Sparse HalfCheetahDir (MuJoCo continuous control); episodic adaptation (multiple episodes allowed).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>MetaCURE alone (information-gain bonus) is insufficient on Sparse HalfCheetahDir in this paper's online adaptation formulation—it tends to incentivise going to the boundary of the sparse interval but requires addition of a hyper-state bonus to solve the task; on sparse ML1 Meta-World tasks MetaCURE performs worse than HyperX in some hard variants (HyperX outperforms MetaCURE on sparse ML1-pick-place according to Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not directly reported; MetaCURE typically improves few-episode learning when privileged info available but can fail if the information gain signal can't be measured or if the setting is online adaptation rather than few-episode.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed to improve efficiency in few-episode settings by focusing exploration on information gain, but in online single-episode adaptation as tested here it did not provide sufficient incentive alone.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit via separate exploration policy trained to maximise information gain; exploitation policy then uses data gathered. Relies on privileged meta-training signal to compute intrinsic reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to HyperX (which does not require privileged information) and other baselines; paper shows MetaCURE alone fails on some online sparse tasks without added hyper-state novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Information-gain intrinsic reward can drive exploration but, in online single-episode adaptation settings and without privileged access at meta-test, is insufficient alone for some sparse tasks—the hyper-state novelty bonus complements it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires privileged task information during meta-training to compute information gain; struggles in online single-episode adaptation settings tested in this paper and needs augmentation (e.g., hyper-state novelty) to solve some sparse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1152.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1152.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL^2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL^2 (Reinforcement Learning Squared)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent black-box meta-RL method where a recurrent policy receives past actions, rewards and states and implicitly maintains a task belief in its hidden state; used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RL2 : Fast reinforcement learning via slow reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL^2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent policy (RNN) trained across tasks; hidden state aggregates trajectory history and acts as an implicit belief used to adapt behaviour online. No explicit belief representation or explicit r_error estimation is available.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>implicit meta-learned exploration (RNN hidden-state acts as belief / posterior sampling-like behaviour)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by updating its hidden state with observed transitions/rewards and selecting actions conditioned on this hidden state; exploration emerges from meta-training across tasks rather than from explicit intrinsic bonuses, though the paper experiments with adding state/hyper-state novelty bonuses using the hidden state as proxy for belief.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as baseline across Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir, Sparse AntGoal, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable task identity; sparse rewards in several benchmarks; continuous/discrete depending on environment.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as other benchmarks; RNN must learn to encode belief in its hidden state within episode(s).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Generally underperforms HyperX on the hardest sparse tasks; in Treasure Mountain and Multi-Stage Gridworld RL^2 learns the inferior 'circle-walk' exploration strategy and fails to find superior strategies consistently; on Sparse HalfCheetahDir and Sparse AntGoal RL^2 performs worse (lower returns/success rates) than HyperX and sometimes fails to solve the task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline RL^2 (without additional intrinsic bonuses) commonly converges to suboptimal exploration strategies on sparse tasks in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Can be sample-inefficient on complex sparse tasks since hidden-state belief must be learned implicitly; meta-training budgets similar to others but performance lags.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled implicitly through RNN dynamics and meta-training; lacks explicit mechanisms (like r_error) to incentivise visiting states that improve belief inference accuracy, limiting meta-exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to HyperX, VariBAD, Belief Learning, PEARL, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Implicit RNN-based belief maintenance (RL^2) without explicit mechanisms for meta-exploration can converge to myopic/suboptimal strategies on sparse/partially-observable tasks; using hidden state as a proxy for belief to compute hyper-state novelty is possible, but r_error cannot be computed and performance suffers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Hidden-state belief is implicit so methods that require an explicit belief quality metric (VAE reconstruction error) cannot be applied; struggles on several sparse benchmarks tested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1152.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1152.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEARL (Probabilistic Embeddings for Actor-Critic RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-policy meta-RL method that uses probabilistic context variables to represent task belief and performs posterior sampling for adaptation; used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient off-policy meta-reinforcement learning via probabilistic context variables</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learns a probabilistic context encoder producing latent distribution over tasks conditioned on context transitions; policy and critic conditioned on sampled context latent allow fast adaptation via posterior sampling (Thompson-sampling-like).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>posterior sampling via probabilistic context variables (Thompson-sampling-like adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Collects context data and samples a task embedding from posterior to condition policy for adaptation; adaptation often performed between segments/episodes rather than purely within a single unbroken episode.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Compared on Sparse AntGoal, Sparse HalfCheetahDir and Sparse 2D Navigation as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable task identity across episodes; sparse rewards in some tests; episodic multi-episode adaptation setting often assumed by PEARL.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Continuous MuJoCo and 2D navigation tasks used in paper; PEARL was often disadvantaged in single-episode online adaptation experiments because it relies on episode segmentation / between-episode posterior updating.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In online single-episode adaptation settings PEARL often fails to find goals far from the start and has low first-episode success rates compared to HyperX and VariBAD; in experiments, PEARL does not reach good performance within the constrained online adaptation regime used for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not explicitly separated in the paper; PEARL's design (off-policy and between-episode posterior updates) makes it less suited to single-episode online adaptation tasks evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed to be sample-efficient in few-episode meta-RL scenarios, but in strictly online single-episode settings its assumptions reduce effectiveness; empirical sample-efficiency in this paper lower than HyperX on the hardest sparse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implements posterior-sampling-style exploration via sampling context latents; exploration-exploitation is mediated by how quickly posterior concentrates over tasks across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared with HyperX, VariBAD, RL^2, PEARL variants, ProMP, E-MAML where relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>PEARL and similar few-episode methods are not optimised for single-episode online adaptation; they underperform HyperX on sparse tasks where exploration must succeed within the first episode(s).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Less suitable for strict online single-episode adaptation experiments; often fails to find distant sparse rewards in episode 1, leading to lower success rates compared to HyperX.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Varibad: A very good method for bayes-adaptive deep rl via meta-learning <em>(Rating: 2)</em></li>
                <li>Exploration by random network distillation <em>(Rating: 2)</em></li>
                <li>Meta reinforcement learning as task inference <em>(Rating: 2)</em></li>
                <li>Learn to effectively explore in context-based meta-rl <em>(Rating: 2)</em></li>
                <li>RL2 : Fast reinforcement learning via slow reinforcement learning <em>(Rating: 1)</em></li>
                <li>Efficient off-policy meta-reinforcement learning via probabilistic context variables <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1152",
    "paper_id": "paper-222124917",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "HyperX",
            "name_full": "Hyper-State Exploration (HyperX)",
            "brief_description": "A meta-reinforcement learning agent that meta-learns approximate belief inference (via a VAE) and explicitly incentivises meta-exploration by adding two intrinsic bonuses: (1) a novelty bonus on approximate hyper-states using Random Network Distillation (RND) and (2) a VAE reconstruction-error bonus to drive learning of the belief model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HyperX",
            "agent_description": "Policy πψ conditioned on environment state s_t and an approximate belief b_t (mean+variance of a VAE latent); jointly trained with a VAE encoder qφ and decoders pθ for rewards/transitions. Intrinsic modules: an RND predictor fω trained against a fixed random prior g to estimate hyper-state novelty, and VAE reconstruction error computed via pθ to estimate belief inaccuracy. PPO is used to optimise the policy with extrinsic + intrinsic rewards.",
            "adaptive_design_method": "curiosity-driven exploration / intrinsic reward (novelty on hyper-states via RND + prediction-error / model uncertainty bonus)",
            "adaptation_strategy_description": "During meta-training HyperX augments environment reward with two adaptive bonuses: r_hyper(s+,b)=||f(s+,b)-g(s+,b)||^2 (RND novelty over hyper-states) to encourage trying different task-exploration strategies, and r_error = negative log-likelihood of observed reward/next-state under VAE decoders to encourage visiting states where belief inference is poor. The policy conditions on the evolving belief b_t to choose actions; intrinsic bonus weights (λ_h, λ_e) are annealed over training so at meta-test the policy focuses on online return.",
            "environment_name": "Multiple (Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir, Sparse MuJoCo AntGoal, Sparse 2D Navigation, Meta-World ML1 variants)",
            "environment_characteristics": "Unknown / partially observable task identity across episodes (BAMDP setting); stochastic transitions in some MuJoCo tasks; continuous and discrete state/action spaces depending on environment; strongly sparse rewards in many testbeds; tasks are drawn from a task distribution p(M).",
            "environment_complexity": "Varies by environment: Treasure Mountain (continuous 4D observations, rollout length 100 steps); Multi-Stage Gridworld (3 rooms, discrete grid, H=50); Sparse HalfCheetahDir (continuous MuJoCo, 200 steps per episode for adaptation, sparse reward only outside interval); Sparse AntGoal (high-dimensional continuous state/action, sparse dense-reward radius =1, multiple episodes); meta-training budgets reported in paper: e7–e8 frames per environment (e.g., Treasure Mountain: 8e7 frames, Sparse HalfCheetahDir: 3e7 frames, Sparse AntGoal: 4e8 frames).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Substantially improved across sparse / partially observable environments versus baselines: Treasure Mountain — HyperX meta-learns the superior strategy (climb mountain to observe treasure) consistently across all 10 seeds (higher cumulative online return than VariBAD); Multi-Stage Gridworld — only HyperX solves the sequential unlocked-goals problem (finds G1→G2→G3) while VariBAD/VariBAD+r(s) fail; Sparse HalfCheetahDir — HyperX successfully meta-learns the correct within-episode adaptation strategy where standard baselines fail (exact numeric returns in paper's Table 1 show HyperX succeeds and baselines have much lower returns); Sparse AntGoal — higher success rate per episode than VariBAD, RL^2 and PEARL (HyperX finds goal often in episode 1 and achieves substantially higher first-episode success rate). Exact numeric values vary by task and are given in the paper's figures and tables (e.g., ML1-reach/push: 100% first-rollout success for HyperX and VariBAD; ML1-pick-place dense: HyperX 44.5% success).",
            "performance_without_adaptation": "Baselines without HyperX's adaptive meta-exploration (VariBAD, RL^2, Belief Learning, PEARL, ProMP, E-MAML) generally perform worse on sparse tasks: e.g., VariBAD learns suboptimal task-exploration on Treasure Mountain (walk circle strategy), fails to find later-stage goals in Multi-Stage Gridworld, and has lower success rates on Sparse AntGoal; in Sparse HalfCheetahDir many baselines fail to learn the correct behaviour (returns around -150 per episode for RL^2/PEARL in one plot).",
            "sample_efficiency": "At meta-test the learned HyperX policy often locates sparse signals in the first episode (e.g., AntGoal: finds goal in episode 1 via spiralling search); meta-training requires large sample budgets (tens to hundreds of millions of environment frames reported per task). The VAE reconstruction bonus accelerates learning of useful beliefs early in meta-training; annealing intrinsic bonuses prevents perpetual exploration at meta-test.",
            "exploration_exploitation_tradeoff": "Explicit: policy optimises discounted sum of extrinsic + λ_h r_hyper + λ_e r_error; λ_h and λ_e are annealed down so meta-training emphasises data collection for belief learning but meta-test focuses on exploitation of learned Bayes-adaptive behaviour. The policy uses the approximate belief b_t (from VAE) to choose actions that trade short-term cost (information-seeking) vs long-term gain (Bayes-optimal exploitation).",
            "comparison_methods": "VariBAD (VAE-based belief meta-learner), VariBAD+r(s) (state-novelty), RL^2 (recurrent meta-learner), Belief Learning (Humplik et al., privileged-task inference), PEARL, ProMP, E-MAML, MetaCURE (information-gain intrinsic reward), state-only RND baselines.",
            "key_results": "1) Encouraging exploration in approximate hyper-state space (state + belief) is necessary for effective meta-exploration on sparse tasks; 2) Combining a hyper-state novelty bonus (RND) with a VAE reconstruction-error bonus is required when beliefs are learned jointly—r_hyper alone fails early because beliefs are initially meaningless, r_error alone is insufficient to ensure diverse task-exploration; 3) HyperX meta-learns approximately Bayes-optimal adaptation strategies on multiple sparse/partially-observable benchmarks where prior methods fail; 4) Annealing intrinsic bonuses is important to avoid persistent exploration at meta-test.",
            "limitations_or_failures": "Relying only on hyper-state novelty (r_hyper) fails when the belief inference is poor early in meta-training; relying only on the VAE error (r_error) may not guarantee diverse meta-exploration (70% success on Treasure Mountain when used alone). HyperX requires substantial meta-training data (tens/hundreds of millions of frames) and tuning of RND hyperparameters (sensitivity to prior weight scale). Some tasks remain challenging (e.g., sparse ML1-pick-place: only 9/20 seeds learned something with HyperX on dense version and 1/20 on sparse), indicating horizon length and problem structure can limit effectiveness.",
            "uuid": "e1152.0",
            "source_info": {
                "paper_title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "VariBAD",
            "name_full": "VariBAD (Variational Bayes-Adaptive Deep RL)",
            "brief_description": "A meta-RL method that meta-learns an explicit approximate belief over tasks using a VAE (encoder qφ producing a latent m/belief) and conditions a policy on that belief; used as a baseline and as the belief backbone in HyperX.",
            "citation_title": "Varibad: A very good method for bayes-adaptive deep rl via meta-learning",
            "mention_or_use": "use",
            "agent_name": "VariBAD",
            "agent_description": "Jointly trains a VAE (encoder qφ(m|τ:t) and decoders pθ for rewards/transitions) and a policy πψ(s_t, b_t) where b_t is the VAE posterior (mean+variance). The RL loss J(ψ) is optimised alongside an ELBO for the trajectory model; the RL loss is not backpropagated into the encoder.",
            "adaptive_design_method": "Bayesian belief-conditioned policy (approximate Bayes-adaptive RL via variational inference); implicit exploration driven by policy conditioned on posterior, but no explicit meta-exploration bonuses by default",
            "adaptation_strategy_description": "Adapts by maintaining an approximate task belief (VAE posterior) from trajectory τ:t and conditioning the policy on b_t to select actions; exploration emerges implicitly from meta-learned policy that has experienced the task distribution during meta-training.",
            "environment_name": "Same set of benchmarks as HyperX when used as a baseline (Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir, Sparse AntGoal, Sparse 2D Navigation, Meta-World ML1)",
            "environment_characteristics": "Partially observable task identity across episodes; sparse rewards in many comparisons; continuous or discrete state/action depending on benchmark.",
            "environment_complexity": "Varies by environment; identical to HyperX benchmarks when used for comparison.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Performs well on many benchmarks with informative/dense rewards and simpler sparse tasks (e.g., ML1-reach/push: 100% first-rollout success), but fails on several hard sparse/partially observable tasks in this paper: Treasure Mountain (learns inferior circle-search strategy rather than climbing mountain), Multi-Stage Gridworld (fails to reach G3), Sparse HalfCheetahDir (fails to learn correct within-episode adaptation), Sparse AntGoal (lower success rates than HyperX).",
            "performance_without_adaptation": "When VariBAD is augmented with a simple state-novelty bonus r(s) it still fails to solve some tasks (e.g., Multi-Stage Gridworld cannot reach final goal G3), indicating simple state-space exploration is insufficient.",
            "sample_efficiency": "Can adapt quickly in some tasks (e.g., finds goals in a few episodes in sparse 2D navigation), but often less sample-efficient than HyperX on the hardest sparse tasks; meta-training budgets similar to HyperX.",
            "exploration_exploitation_tradeoff": "Implicit via conditioning on approximate belief; no explicit intrinsic bonuses in default VariBAD, so early meta-training may be dominated by myopic behaviour if sparse reward provides little learning signal.",
            "comparison_methods": "Compared directly to HyperX, HyperX ablations, RL^2, Belief Learning (Humplik), PEARL, ProMP, E-MAML, MetaCURE.",
            "key_results": "VariBAD provides a useful explicit belief representation for meta-RL, but without dedicated meta-exploration bonuses it can converge to suboptimal task-exploration strategies on sparse/partially observable tasks; augmenting with state-only novelty is insufficient in problems where hyper-state novelty matters.",
            "limitations_or_failures": "Fails on tasks requiring meta-exploration across different adaptation strategies (e.g., Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir) because beliefs are not enough to drive exploration during meta-training when rewards are sparse. No mechanism to encourage diverse task-exploration during meta-training.",
            "uuid": "e1152.1",
            "source_info": {
                "paper_title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Belief Oracle",
            "name_full": "Belief Oracle (policy conditioned on exact belief)",
            "brief_description": "An experimental agent in this paper that is given the true (computed) Bayesian belief b_t and trained via standard RL conditioned on the exact hyper-state s+ = (s, b); used to investigate the effect of hyper-state novelty bonuses when beliefs are accurate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief Oracle",
            "agent_description": "A policy trained with access to the true task posterior belief b_t (computed analytically for the toy/synthetic tasks), conditioned on the exact hyper-state s+; trained with and without intrinsic bonuses to test their effect in the idealised belief-accurate setting.",
            "adaptive_design_method": "hyper-state novelty via RND (intrinsic reward on exact hyper-states); curiosity-driven intrinsic bonuses",
            "adaptation_strategy_description": "Uses exact belief b_t and RND hyper-state novelty to guide exploration: when belief updates make regions of the state space novel for the updated belief, the novelty bonus encourages exploration there; this produces approximate Bayes-optimal behaviour under the oracle belief.",
            "environment_name": "Sparse HalfCheetahDir (used for the controlled investigation)",
            "environment_characteristics": "Two-task distribution (walk forward / walk backward), sparse reward only outside interval [-5,5], continuous MuJoCo dynamics, 200 environment steps to adapt; belief can be computed exactly in this simplified prior.",
            "environment_complexity": "Continuous state/action (HalfCheetah), adaptation horizon 200 steps; but belief computation simplified for the two-task prior allowing exact posterior.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "With access to exact belief and adding the hyper-state exploration bonus r_hyper, the Belief Oracle learns approximately Bayes-optimal behaviour for the Sparse HalfCheetahDir task; without the bonus the Belief Oracle still fails to learn correct behaviour for this task, demonstrating the necessity of hyper-state exploration even with exact beliefs.",
            "performance_without_adaptation": "Belief Oracle without hyper-state bonus fails to learn correct exploration strategy in Sparse HalfCheetahDir (even with exact belief), showing that an appropriate exploration incentive is required to discover information-seeking maneuvers.",
            "sample_efficiency": "Not precisely quantified beyond experiments; oracle experiments demonstrate that targeted intrinsic bonuses can dramatically reduce required exploration at meta-test (agent can find task-informative regions within adaptation horizon), but training still requires standard RL sample budgets.",
            "exploration_exploitation_tradeoff": "Directly trades off using exact belief to exploit vs taking information-seeking actions encouraged by hyper-state novelty; because belief is exact, the RND hyper-state bonus drives exploration precisely to where it is expected to improve future returns.",
            "comparison_methods": "Compared to same environment policies without hyper-state bonus and to HyperX which learns beliefs jointly; used to show the difference between operating with exact vs approximate beliefs.",
            "key_results": "Demonstrates that (a) novelty on hyper-states (state+belief) can induce approximately Bayes-optimal behaviour if beliefs are accurate, and (b) even an oracle that knows the belief needs explicit hyper-state exploration incentives to discover the information-gathering trajectories on sparse tasks.",
            "limitations_or_failures": "Oracle setting is idealised and infeasible in general (true belief rarely available); results show that when beliefs are learned jointly they are initially inaccurate, making r_hyper alone ineffective early in meta-training.",
            "uuid": "e1152.2",
            "source_info": {
                "paper_title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Belief Learning (Humplik et al.)",
            "name_full": "Belief Learning (Humplik et al., 2019) - meta reinforcement learning as task inference",
            "brief_description": "A method that meta-learns to perform task inference using privileged task information during training, producing an explicit belief model used by a policy to adapt online.",
            "citation_title": "Meta reinforcement learning as task inference",
            "mention_or_use": "use",
            "agent_name": "Belief Learning (Humplik et al.)",
            "agent_description": "A meta-RL approach that trains an explicit belief/inference model using ground-truth task descriptions available during meta-training (privileged information), then conditions a policy on the inferred belief for fast adaptation.",
            "adaptive_design_method": "explicit task-inference + belief-conditioned policy; intrinsic exploration can be added but in the paper this method is used mainly as a baseline (privileged supervision reduces need for r_error)",
            "adaptation_strategy_description": "Learns to infer task identity from experience (privileged during training), then policy uses inferred belief to pick actions; because inference is easier (privileged labels), the hyper-state novelty bonus r_hyper alone can be sufficient for sparse HalfCheetahDir in this setting.",
            "environment_name": "Used as baseline across the same set of benchmarks (Sparse HalfCheetahDir and others)",
            "environment_characteristics": "Same partially observable/task-uncertainty settings as other benchmarks; privileged info available at meta-training only.",
            "environment_complexity": "Same as corresponding benchmarks; inference is easier due to privileged training signal.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "With privileged task descriptions at meta-training, Belief Learning can succeed on Sparse HalfCheetahDir using only r_hyper (no r_error needed), achieving performance comparable to or better than unsupervised VariBAD+HyperX ablations on that task.",
            "performance_without_adaptation": "Not explicitly quantified in all settings within this paper; however, when not augmented with hyper-state novelty bonus it can still be limited by sparse rewards if privileged information is insufficient to guide exploration behaviours.",
            "sample_efficiency": "Privileged supervision improves sample efficiency for learning inference; exact numbers not provided in paper but experiments indicate faster/better belief learning compared to unsupervised VAE-based approaches.",
            "exploration_exploitation_tradeoff": "Relies on supervised inference to reduce exploration need; when combined with hyper-state novelty bonus it still balances exploration (try different strategies) vs exploitation via belief-conditioned policy.",
            "comparison_methods": "Compared to VariBAD, HyperX and RL^2 as baselines in the paper's experiments.",
            "key_results": "Privileged training of belief inference can obviate the need for a reconstruction-error bonus (r_error) and permit hyper-state novelty (r_hyper) alone to produce successful meta-training on some sparse tasks; highlights the difficulty of unsupervised belief learning.",
            "limitations_or_failures": "Relies on privileged task information during meta-training, which is often unavailable in realistic settings; not directly comparable to HyperX's unsupervised belief learning scenario.",
            "uuid": "e1152.3",
            "source_info": {
                "paper_title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "MetaCURE",
            "name_full": "MetaCURE (Meta-trained exploration via information gain)",
            "brief_description": "A recent meta-exploration method that trains a separate exploration policy intrinsically motivated by an information-gain reward (difference in prediction errors with vs without privileged task information), designed for few-episode meta-learning.",
            "citation_title": "Learn to effectively explore in context-based meta-rl",
            "mention_or_use": "use",
            "agent_name": "MetaCURE",
            "agent_description": "Trains a dedicated exploration policy that maximises an intrinsic reward based on information gain about the task (computed using privileged task information during meta-training); used as a baseline in comparisons.",
            "adaptive_design_method": "information-gain maximisation (intrinsic reward), separate exploration/exploitation policies",
            "adaptation_strategy_description": "Exploration policy seeks transitions that maximise the reduction in task uncertainty (measured as difference in prediction errors when conditioning on current experience vs conditioning on ground-truth task), then exploitation policy uses collected data.",
            "environment_name": "Compared on sparse ML1 Meta-World tasks and Sparse HalfCheetahDir (adapted to online setting for comparisons)",
            "environment_characteristics": "Few-episode adaptation setting; environments can provide sparse success signal; MetaCURE requires privileged information during meta-training to compute information gain.",
            "environment_complexity": "Meta-World ML1 tasks (robotic arm manipulation), Sparse HalfCheetahDir (MuJoCo continuous control); episodic adaptation (multiple episodes allowed).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "MetaCURE alone (information-gain bonus) is insufficient on Sparse HalfCheetahDir in this paper's online adaptation formulation—it tends to incentivise going to the boundary of the sparse interval but requires addition of a hyper-state bonus to solve the task; on sparse ML1 Meta-World tasks MetaCURE performs worse than HyperX in some hard variants (HyperX outperforms MetaCURE on sparse ML1-pick-place according to Appendix).",
            "performance_without_adaptation": "Not directly reported; MetaCURE typically improves few-episode learning when privileged info available but can fail if the information gain signal can't be measured or if the setting is online adaptation rather than few-episode.",
            "sample_efficiency": "Designed to improve efficiency in few-episode settings by focusing exploration on information gain, but in online single-episode adaptation as tested here it did not provide sufficient incentive alone.",
            "exploration_exploitation_tradeoff": "Explicit via separate exploration policy trained to maximise information gain; exploitation policy then uses data gathered. Relies on privileged meta-training signal to compute intrinsic reward.",
            "comparison_methods": "Compared to HyperX (which does not require privileged information) and other baselines; paper shows MetaCURE alone fails on some online sparse tasks without added hyper-state novelty.",
            "key_results": "Information-gain intrinsic reward can drive exploration but, in online single-episode adaptation settings and without privileged access at meta-test, is insufficient alone for some sparse tasks—the hyper-state novelty bonus complements it.",
            "limitations_or_failures": "Requires privileged task information during meta-training to compute information gain; struggles in online single-episode adaptation settings tested in this paper and needs augmentation (e.g., hyper-state novelty) to solve some sparse tasks.",
            "uuid": "e1152.4",
            "source_info": {
                "paper_title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RL^2",
            "name_full": "RL^2 (Reinforcement Learning Squared)",
            "brief_description": "A recurrent black-box meta-RL method where a recurrent policy receives past actions, rewards and states and implicitly maintains a task belief in its hidden state; used as a baseline in this paper.",
            "citation_title": "RL2 : Fast reinforcement learning via slow reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "RL^2",
            "agent_description": "Recurrent policy (RNN) trained across tasks; hidden state aggregates trajectory history and acts as an implicit belief used to adapt behaviour online. No explicit belief representation or explicit r_error estimation is available.",
            "adaptive_design_method": "implicit meta-learned exploration (RNN hidden-state acts as belief / posterior sampling-like behaviour)",
            "adaptation_strategy_description": "Adapts by updating its hidden state with observed transitions/rewards and selecting actions conditioned on this hidden state; exploration emerges from meta-training across tasks rather than from explicit intrinsic bonuses, though the paper experiments with adding state/hyper-state novelty bonuses using the hidden state as proxy for belief.",
            "environment_name": "Used as baseline across Treasure Mountain, Multi-Stage Gridworld, Sparse HalfCheetahDir, Sparse AntGoal, etc.",
            "environment_characteristics": "Partially observable task identity; sparse rewards in several benchmarks; continuous/discrete depending on environment.",
            "environment_complexity": "Same as other benchmarks; RNN must learn to encode belief in its hidden state within episode(s).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Generally underperforms HyperX on the hardest sparse tasks; in Treasure Mountain and Multi-Stage Gridworld RL^2 learns the inferior 'circle-walk' exploration strategy and fails to find superior strategies consistently; on Sparse HalfCheetahDir and Sparse AntGoal RL^2 performs worse (lower returns/success rates) than HyperX and sometimes fails to solve the task.",
            "performance_without_adaptation": "Baseline RL^2 (without additional intrinsic bonuses) commonly converges to suboptimal exploration strategies on sparse tasks in these experiments.",
            "sample_efficiency": "Can be sample-inefficient on complex sparse tasks since hidden-state belief must be learned implicitly; meta-training budgets similar to others but performance lags.",
            "exploration_exploitation_tradeoff": "Handled implicitly through RNN dynamics and meta-training; lacks explicit mechanisms (like r_error) to incentivise visiting states that improve belief inference accuracy, limiting meta-exploration.",
            "comparison_methods": "Compared to HyperX, VariBAD, Belief Learning, PEARL, etc.",
            "key_results": "Implicit RNN-based belief maintenance (RL^2) without explicit mechanisms for meta-exploration can converge to myopic/suboptimal strategies on sparse/partially-observable tasks; using hidden state as a proxy for belief to compute hyper-state novelty is possible, but r_error cannot be computed and performance suffers.",
            "limitations_or_failures": "Hidden-state belief is implicit so methods that require an explicit belief quality metric (VAE reconstruction error) cannot be applied; struggles on several sparse benchmarks tested.",
            "uuid": "e1152.5",
            "source_info": {
                "paper_title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "PEARL",
            "name_full": "PEARL (Probabilistic Embeddings for Actor-Critic RL)",
            "brief_description": "An off-policy meta-RL method that uses probabilistic context variables to represent task belief and performs posterior sampling for adaptation; used as a baseline in this paper.",
            "citation_title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables",
            "mention_or_use": "use",
            "agent_name": "PEARL",
            "agent_description": "Learns a probabilistic context encoder producing latent distribution over tasks conditioned on context transitions; policy and critic conditioned on sampled context latent allow fast adaptation via posterior sampling (Thompson-sampling-like).",
            "adaptive_design_method": "posterior sampling via probabilistic context variables (Thompson-sampling-like adaptation)",
            "adaptation_strategy_description": "Collects context data and samples a task embedding from posterior to condition policy for adaptation; adaptation often performed between segments/episodes rather than purely within a single unbroken episode.",
            "environment_name": "Compared on Sparse AntGoal, Sparse HalfCheetahDir and Sparse 2D Navigation as baseline",
            "environment_characteristics": "Partially observable task identity across episodes; sparse rewards in some tests; episodic multi-episode adaptation setting often assumed by PEARL.",
            "environment_complexity": "Continuous MuJoCo and 2D navigation tasks used in paper; PEARL was often disadvantaged in single-episode online adaptation experiments because it relies on episode segmentation / between-episode posterior updating.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In online single-episode adaptation settings PEARL often fails to find goals far from the start and has low first-episode success rates compared to HyperX and VariBAD; in experiments, PEARL does not reach good performance within the constrained online adaptation regime used for comparisons.",
            "performance_without_adaptation": "Not explicitly separated in the paper; PEARL's design (off-policy and between-episode posterior updates) makes it less suited to single-episode online adaptation tasks evaluated here.",
            "sample_efficiency": "Designed to be sample-efficient in few-episode meta-RL scenarios, but in strictly online single-episode settings its assumptions reduce effectiveness; empirical sample-efficiency in this paper lower than HyperX on the hardest sparse tasks.",
            "exploration_exploitation_tradeoff": "Implements posterior-sampling-style exploration via sampling context latents; exploration-exploitation is mediated by how quickly posterior concentrates over tasks across episodes.",
            "comparison_methods": "Compared with HyperX, VariBAD, RL^2, PEARL variants, ProMP, E-MAML where relevant.",
            "key_results": "PEARL and similar few-episode methods are not optimised for single-episode online adaptation; they underperform HyperX on sparse tasks where exploration must succeed within the first episode(s).",
            "limitations_or_failures": "Less suitable for strict online single-episode adaptation experiments; often fails to find distant sparse rewards in episode 1, leading to lower success rates compared to HyperX.",
            "uuid": "e1152.6",
            "source_info": {
                "paper_title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Varibad: A very good method for bayes-adaptive deep rl via meta-learning",
            "rating": 2,
            "sanitized_title": "varibad_a_very_good_method_for_bayesadaptive_deep_rl_via_metalearning"
        },
        {
            "paper_title": "Exploration by random network distillation",
            "rating": 2,
            "sanitized_title": "exploration_by_random_network_distillation"
        },
        {
            "paper_title": "Meta reinforcement learning as task inference",
            "rating": 2,
            "sanitized_title": "meta_reinforcement_learning_as_task_inference"
        },
        {
            "paper_title": "Learn to effectively explore in context-based meta-rl",
            "rating": 2,
            "sanitized_title": "learn_to_effectively_explore_in_contextbased_metarl"
        },
        {
            "paper_title": "RL2 : Fast reinforcement learning via slow reinforcement learning",
            "rating": 1,
            "sanitized_title": "rl2_fast_reinforcement_learning_via_slow_reinforcement_learning"
        },
        {
            "paper_title": "Efficient off-policy meta-reinforcement learning via probabilistic context variables",
            "rating": 1,
            "sanitized_title": "efficient_offpolicy_metareinforcement_learning_via_probabilistic_context_variables"
        }
    ],
    "cost": 0.021737999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning</p>
<p>Luisa Zintgraf 
Leo Feng 
Cong Lu 
Maximilian Igl 
Kristian Hartikainen 
Katja Hofmann 
Shimon Whiteson 
Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning</p>
<p>To rapidly learn a new task, it is often essential for agents to explore efficiently -especially when performance matters from the first timestep. One way to learn such behaviour is via meta-learning. Many existing methods however rely on dense rewards for meta-training, and can fail catastrophically if the rewards are sparse. Without a suitable reward signal, the need for exploration during meta-training is exacerbated. To address this, we propose HyperX, which uses novel reward bonuses for meta-training to explore in approximate hyper-state space (where hyper-states represent the environment state and the agent's task belief). We show empirically that HyperX metalearns better task-exploration and adapts more successfully to new tasks than existing methods.</p>
<p>Introduction</p>
<p>In meta-learning, the experience of a machine learning model over multiple learning episodes (which can range from single model updates to entire lifetimes), is used to improve future learning performance (Hospedales et al., 2020), e.g., in terms of data or computational efficiency. This "learning to learn" requires a feedback signal on the metalevel, which quantifies how well the model performs after a learning episode. In meta reinforcement learning (meta-RL), this is often measured by episodic (online or final) return of the agent, or learning improvement. If this feedback signal is not present because the agent has not made sufficient learning progress, meta-learning can fail to solve the task. In this case, the agent faces an exploration challenge at the meta-level: how to find a good meta-learning signal in the first place.  In this environment, the agent must find the hidden treasure but cannot see above the grass. Two possible task-exploration strategies are to (s1) search the grass for the treasure, or (s2) climb up the mountain, see the treasure, and go there. The agent is meta-trained across different treasure locations, and should try both s1 and s2 to find out which one leads to higher online return in expectation across tasks. We call this meta-exploration across task-exploration strategies. Due to a lack of sufficient meta-exploration, existing methods often fail to find the (superior) task-exploration strategy s2 (Sec 5.1).</p>
<p>A weakness of existing meta-RL methods is that they often rely on dense rewards or sufficiently small state spaces, such that even with naive exploration during meta-training the agent receives rewards that guide it towards good behaviour. However, we observe empirically (Sec 5) that if the rewards are sparse or if exploratory behaviour is penalised in the short term, existing methods can fail. This is problematic, since defining dense rewards can be tedious and error-prone, and many real-world applications have sparse rewards (e.g., a fail/success criterion). Hence, to make meta-learning practical for such settings, we need methods that can metalearn even when rewards are sparse.</p>
<p>In this paper we consider this challenge in the context of meta-learning fast online adaptation to tasks from a given distribution. In this setting, the agent aims to maximise expected online return when adapting to an unknown task, which requires it to carefully trade off exploration and exploitation. To make this distinction clear, we call this taskexploration in contrast to meta-exploration, illustrated with an example in Figure 1. Task-exploration refers to the exploration behaviour we want to meta-learn: when in a new environment, the agent must explore to learn the task. We want this agent to be Bayes-optimal, i.e., to maximise expected online return that it incurs while learning about the environment. Meta-exploration refers to the challenge of exploring across tasks and adaptation behaviours during meta-training. The agent has to (a) explore across individual tasks since the same state can have different values across tasks, and (b) learn about the shared structure between tasks to extract information about how to adapt, i.e., the agent must try out different task-exploration strategies during meta-training to find the Bayes-optimal one. Contrary to the Bayes-optimal task-exploration we want to meta-learn, we do not care about the rewards incurred during meta-exploration, but rather about efficiently gathering the data needed for meta-learning.</p>
<p>The Bayes-optimal policy can in principle be found by solving a Bayesian formulation of the reinforcement learning (RL) problem that considers both the environment state and the agent's internal belief about the environment, together called hyper-states (Duff &amp; Barto, 2002). This allows the agent to compute how to explore optimally under task uncertainty: it takes information-seeking actions (which can be costly in the short term) if and only if they lead to higher expected long-term returns (by yielding information that can be exploited later). While this computation is intractable for all but the simplest environments, recent work makes significant progress by meta-learning approximately Bayesoptimal behaviour for a given task distribution (Duan et al., 2016;Wang et al., 2016;Ortega et al., 2019;Humplik et al., 2019;Zintgraf et al., 2020;Mikulik et al., 2020).</p>
<p>We propose HyperX (Hyper-State Exploration), a novel method for meta-learning approximately Bayes-optimal exploration strategies when rewards are sparse. Similar to VariBAD (Zintgraf et al., 2020), HyperX simultaneously meta-learns approximate task inference, and trains a policy that conditions on hyper-states, i.e., the environment state and the approximate task belief. To ensure sufficient metaexploration, HyperX combines two exploration bonuses during meta-training. The first is a novelty bonus on approximate hyper-states using random network distillation (Osband et al., 2018;Burda et al., 2019b) that encourages the agent to try out different task-exploration strategies, so that it can better find an approximately Bayes-optimal one. However, as this requires accurate task inference which we aim to meta-learn alongside the policy, the beliefs are inaccurate early in training and this bonus is not useful by itself. We therefore use a second exploration bonus to incentivise the agent to gather the data necessary to learn approximate belief inference. This bonus is computed using the discrepancy between the rewards and transitions that the belief model decoder predicts, and the ground-truth rewards and transitions the agent observes. This exploration bonus en-courages the agent to visit states where the belief inference is incorrect and more data should be collected.</p>
<p>We show empirically that in environments without dense and informative rewards, current state of the art methods either fail to learn, or learn sub-optimal adaptation behaviour. In contrast, we show that HyperX can successfully meta-learn approximately Bayes-optimal strategies on these tasks.</p>
<p>Background</p>
<p>Our goal is to meta-learn policies that maximise expected online return, i.e., optimally trade off exploration and exploitation under task uncertainty. We formally define this problem setting below.</p>
<p>Problem Setting</p>
<p>Task Distribution. We consider a meta-learning setting where we have a distribution p(M ) over MDPs. An MDP
M i ∼p(M ) is defined by a tuple M i =(S, A, R i , T i , γ, H).
S is a set of states, A a set of actions, R(r t+1 |s t , a t , s t+1 ) a reward function, T (s t+1 |s t , a t ) a transition function including the initial state distribution T i (s 0 ), γ a discount factor, and H the horizon. Across tasks, the reward and transition functions can vary so we often express p(M ) as p(R, T ).</p>
<p>Objective. Our objective is to meta-learn a policy that, when deployed in an (unseen) test task drawn from p(M ), maximises the online return achieved during task-learning:
max π E p(M ) [J (π)] where J (π) = E T,R,π [ H−1 t=0 γ t r t ].
Since the agent does not initially know which MDP it is in, maximising this objective requires a good task-exploration strategy to cope with the initially unknown reward and transition functions, and to exploit task information to adapt in this environment. The more an agent can make use of prior knowledge about p, the better it can perform this trade-off.</p>
<p>Meta-Learning. During meta-training we assume access to a task distribution p(M ), from which we sample batches of tasks M = {M i } N i=1 and interact with them to learn good task-exploration strategies. During this phase, we need good meta-exploration, to collect the data necessary for metalearning. At meta-test time, the agent is evaluated based on the expected return it gets while adapting to new tasks from p(M ). This requires good task-exploration strategies.</p>
<p>Bayesian Reinforcement Learning.</p>
<p>In principle, we can compute the optimal solution to the problem describe above by formulating the problem as a Bayes-Adaptive MDP (BAMDP, Duff &amp; Barto (2002)), which is a tuple M + = S + , A, R + , T + , T + 0 , γ, H + . Here, S + = S × B is the hyper-state space, consisting of the underlying MDP environment state space S and a belief space B whose elements are beliefs over the MDP.</p>
<p>This belief is typically expressed as a distribution over the reward and transition function b t (R, T ) = p(R, T |τ :t ), where τ :t = (s 0 , a 0 , r 1 , s 1 , . . . , s t ) is the agent's experience up until the current time step t in the current task. The transition function is defined as T + (s + t+1 |s + t , a t , r t ) = E bt [T (s t+1 |s t , a t )] δ(b t+1 =p(R, T |τ :t+1 )) and the reward function as
R + (s + t , a t , s + t+1 ) = E bt+1 [R(s t , a t , s t+1 )] . T + 0 (s + )
is the initial hyper-state distribution, and H + is the horizon in the BAMDP.</p>
<p>A policy π(s + ) acting in a BAMDP conditions its actions not only on the environment state s, but also on the belief b. This way, it can take task uncertainty into account when making decisions. The agent's objective in a BAMDP is to maximise the expected return in an initially unknown environment, while learning, within the horizon H + :
J + (π) = E b0,T + ,π   H + −1 t=0 γ t R + (r t+1 |s + t , a t , s + t+1 )   .
(1) A policy π(s t , b t ) that maximises this objective is called Bayes-optimal, as it optimally trades off exploration and exploitation in order to maximise expected cumulative return. For an in-depth introduction to BAMDPs, see Duff &amp; Barto (2002) or Ghavamzadeh et al. (2015).</p>
<p>The belief inference and planning in belief space is generally intractable, but we can meta-learn an approximate inference procedure Mikulik et al., 2020). Existing methods meta-learn to maintain a belief either implicitly within the workings of recurrent networks (RL 2 , Duan et al. (2016); Wang et al. (2016)), or explicitly by meta-learning a posterior using privileged (Humplik et al., 2019) or unsupervised (VariBAD, Zintgraf et al. (2020)) information. In this paper we use VariBAD, because it explicitly expresses the belief as a single latent vector, which we need in order to compute the exploration bonus.</p>
<p>VariBAD</p>
<p>VariBAD (Zintgraf et al., 2020) jointly trains a policy π ψ (s t , b t ), and a variational auto-encoder (VAE, Kingma &amp; Welling (2014)) for approximate belief inference. The VAE consists of an encoder q θ (m|τ :t ) to compute an approximate belief b t , and reward and transition decoders p(r i+1 |s i , a i , s i+1 , m t ) and p(s i+1 |s i , a i , m t ) with m ∼ b t which are used only during meta-training. The objective is
L(φ, θ, ψ) = E p(M )   J (ψ) + H + t=0 ELBO t (φ, θ)   (2) where ELBO t = E p(M ) E q φ (m|τ:t) [log p θ (τ :H + |m)] − KL(q φ (m|τ :t )||q φ (m|τ :t−1 ))] ,(3)
with prior q φ (m) = N (0, I). The objective jointly maximises an RL loss J (with the agent conditioned on state s t and approximate belief b t represented by the mean and variance of the VAE's latent distribution) and an evidence lower bound (ELBO) on the environment model, that consists of a reconstruction term for the trajectory and a KL divergence to the previous posterior. Like Zintgraf et al. (2020), we do not backpropagate the RL loss through the encoder (hence J does not depend on the encoder parameters φ).</p>
<p>Method: HyperX</p>
<p>Meta-learning good task-adaptation behaviour requires the agent to, during meta-training, gather the data necessary to learn good task-exploration strategies. If the environment rewards are sparse, they might however not provide enough signal for an agent to learn something if it follows naive exploration during meta-training. In that case, existing methods can fail and special attention needs to be paid to meta-exploration. The agent needs to explore the state space sufficiently during meta-training, which is complicated by the fact that the same state can have different values across tasks. A good meta-exploration strategy also ensures that the agent tries out diverse task-exploration strategies which allow it to find an approximately Bayes-optimal one.</p>
<p>To address the meta-exploration problem, we propose Hy-perX (Hyper-State-Exploration), a method to meta-learn approximately Bayes-optimal behaviour even when rewards are not dense. The two key ideas behind HyperX are:</p>
<ol>
<li>
<p>We can incentivise the agent to try out different taskexploration strategies during meta-training by rewarding novel hyper-states. By exploring the joint space of beliefs and states (i.e., hyper-states), the agent simultaneously (a) explores the state space, while distinguishing between visitation counts in different tasks due to changing beliefs, and (b) tries out different taskexploration strategies because these lead to different beliefs (even in the same state). To achieve this, we add an exploration bonus r hyper (s + ) that rewards visiting novel hyper-states.</p>
</li>
<li>
<p>For the novelty bonus on hyper-states to be meaningful, the beliefs needs to be meaningful. However since the inference procedure is meta-learned alongside the policy, they do not capture task information early in training. We therefore additionally incentivise the agent to explore states where beliefs are inaccurate, by using the VAE reconstruction error (of the current rewards and transitions given the current belief) as a reward bonus, r error (s t , r t ). Since the belief is conditioned on the history including the most recent reward r t and state s t , and the VAE is trained to predict rewards and states given beliefs, this bonus tends to zero over training.</p>
</li>
</ol>
<p>In the following, we describe how to compute these bonuses.</p>
<p>Hyper-State Exploration.</p>
<p>To compute exploration bonuses on the hyper-states, we use random network distillation (see Appendix A.1) given its empirical successes in standard RL problems (Osband et al., 2017;Burda et al., 2019b) and theoretical justifications for deep networks (Pearce et al., 2020;Ciosek et al., 2020). To compute a reward bonus, a predictor network f (s + ) is trained to predict the outputs of a fixed, randomly initialised prior network g(s + ), on all hyper-states s + visited by the agent so far in meta-training. The mismatch between those predictions is low for frequently visited hyper-states and high for novel hyper-states. Formally we define the reward bonus for a hyper-state s + t = (s t , b t ) as
r hyper (s + t ) = ||f (s + t ) − g(s + t )|| 2 .(4)
We parameterise the predictor network f ω with ω and train it alongside the policy and VAE.</p>
<p>Approximate Hyper-State Exploration. To meta-learn an (approximately) Bayes-optimal policy, we need access to the belief over tasks at every timestep t during which the policy interacts with the environment. To this end, we use VariBAD (Zintgraf et al., 2020), because it provides a belief representation using a single vector. VariBAD trains an inference procedure using a VAE alongside the policy to obtain approximate beliefs, which are represented by the mean and variance of a latent Gaussian distribution.</p>
<p>At the beginning of meta-training, the beliefs do not sufficiently capture task information. If the policy does not explore and always gets a sparse reward which is uninformative w.r.t. the task, we fail to meta-learn to perform belief inference. The policy should therefore seek states where the VAE is not yet trained well. As a proxy for this, we use the VAE reconstruction error for the reward and states at the current timestep as a reward bonus:
r error (r t , s t ) = −E q φ (m|τ:t) log p θ (r t |s t−1 , a t−1 , s t , m) + log p θ (s t |s t−1 , a t−1 , m) . (5)
Since r t and s t were observed in τ :t , the encoder q has all data to encode the information needed by the decoder p to predict the current reward and state transition. Early in training, these predictions are inaccurate in states where the rewards/transitions differ a lot across tasks. Therefore this exploration bonus incentivises the agent to visit states that provide crucial training data for the VAE. In practice, this reward bonus is computed using one Monte Carlo sample from q. If only one aspect (reward or transitions) change across tasks, VariBAD only learns the respective decoder, and so we only use the respective reward bonus.</p>
<p>Algorithm 1 HyperX Pseudo-Code</p>
<p>Input: Distribution over MDPs p(M ) Initialise: Encoder q φ , decoder p θ , policy π ψ , RND pre-
dictor network f ω , buffer B = {s 0 , b 0 } for k = 1, . . . , K do Sample environments M = {M i } N i=1 where M i ∼ p for M i ∈ M do Reset s 0 , h 0 , b 0 for t = 0, . . . , T − 1 do Choose action: a t = π ψ (s t , b t )
Step environment:
s t+1 , r t+1 = M i .step(a t ) Update belief: b t+1 = q φ (s t+1 , a t , r t+1 , h t )
Compute exploration bonuses: r hyper (s + t+1 ) using Eq (4) r error (r t+1 , s t+1 ) using Eq (5) Add data to buffer: B p .add(s t+1 , b t+1 , a t , r t+1 , r hyper t+1 , r error t+1 ) end for end for Update VAE, policy, and RND predictor network:
(φ, θ) ← (φ, θ) + α (φ,θ) ∇ (φ,θ) H+ t=0 ELBO t (φ, θ) ψ ← ψ + α ψ ∇ ψĴ (ψ) using Eq (6) ω ← ω − α ω ∇ ω E s + ∼B ||f ω (s + ) − g(s + )|| 2 2 end for
Meta-Training Objective. Putting these bonuses together, the new objective for the agent iŝ
J + (ψ) = E b0,T + ,π ψ H + −1 t=0 γ t R + (r t+1 |s + t , a t , s + t+1 ) + λ h r hyper (s + t+1 ) + λ e r error (r t+1 , s t+1 ) . (6)
While in principle these bonuses tend towards zero during meta-training, we anneal their weights (λ h , λ e ) over time. This prevents the policy to keep meta-exploring at metatest time and ensure that it maximises only the expected online return. Algorithm 1 shows pseudo-code for HyperX. Implementation details are given in Appendix C.</p>
<p>Related Work</p>
<p>Exploration Bonuses. Deep RL has been successful on many tasks, and naive exploration via sampling from a stochastic policy is often sufficient if rewards are dense. For hard exploration tasks this performs poorly, and a variety of more sophisticated exploration methods have been proposed. Many of these reward novel states, often using count-based approaches to measure novelty (Strehl &amp; Littman, 2008;Bellemare et al., 2016;Ostrovski et al., 2017;Tang et al., 2017). A prominent method is Random Network Distillation (RND) for state-space exploration in RL (Osband et al., 2017;Burda et al., 2019b;Ciosek et al., 2020). We use it for hyper-states in this paper. We further use an exploration bonus based on the VAE reconstruction error of rewards and transitions. Prediction errors of environment dynamics are used to explore the MDP state space, by, e.g., Achiam &amp; Sastry (2016) Meta-Learning Task-Exploration. Meta-learning how to adapt quickly to a new tasks often includes learning efficient task-exploration, i.e., how to explore an unknown environment. We distinguish few-episode learning where the agent has several episodes for exploration and maximises final episodic return, and online adaptation where performance counts from the first timestep in a new environment, and the agent has to carefully trade off exploration and exploitation.</p>
<p>A popular approach to few-episode learning is gradientbased meta-learning. Here the agent collects data, then performs a gradient update, and is evaluated afterwards. Examples are MAML (Finn et al., 2017) and follow-up work addressing task-exploration (Rothfuss et al., 2019;Stadie et al., 2018); learning separate exploration and exploitation policies (Gurumurthy et al., 2019;Liu et al., 2020;Zhang et al., 2020); or doing task-exploration based on sampling (Gupta et al., 2018) where in particular PEARL (Rakelly et al., 2019) exhibits behaviour akin to posterior sampling. Few-episode learning methods maximise episodic return, and can often not be Bayes-optimal by design.</p>
<p>In this paper we consider the online adaptation setting where we instead want to maximise online return. While computing the exact solution -the Bayes-optimal policy -is infeasible for all but the simplest environments, approximate solutions can be meta-learned Mikulik et al., 2020). When using recurrent policies that receive rewards and actions as inputs in addition to the states (Duan et al., 2016;Wang et al., 2016), learning how to explore happens within the policy network's dynamics, and can be seen as implicitly maintaining a task belief. Humplik et al. (2019) and Zintgraf et al. (2020) develop methods that represent this belief explicitly, by meta-learning to perform inference either using privileged task information during training (Humplik et al., 2019), or by meta-learning to perform inference in an unsupervised way (Zintgraf et al., 2020). To the best of our knowledge, all existing meta-learning methods for online adaptation rely on myopic exploration during meta-training. As we observe empirically (Sec 5), this can cause them to break down if rewards are too sparse.</p>
<p>Meta-Exploration. Two recent works also study the problem of exploration during meta-training, albeit for fewepisode learning. This setting can be more forgiving when it comes to the task-exploration behaviour since the agent has multiple rollouts to collect data, and is reset to the starting position afterwards. Still, similar considerations about metaexploration apply. Zhang et al. (2020) propose MetaCURE, which meta-learns a separate exploration policy that is intrinsically motivated by an exploration bonus that rewards information gain. Liu et al. (2020) propose DREAM, where a separate exploration policy is trained to collect data from which a task embedding (pre-trained via supervision with privileged information) can be recovered. These methods can, in principle, still suffer from poor meta-exploration if rewards are so sparse that there is no signal to begin with, and information gain / task embedding recovery cannot be measured. We empirically compare to MetaCURE and find that this is indeed true (Sec 5.3). On the challenging sparse ML1 Meta-World tasks (Yu et al., 2019), HyperX outperforms MetaCURE by a large margin even though MetaCURE receives more time to explore (Appendix B.1).</p>
<p>If available, privileged information can be used during metatraining to guide exploration, such as expert trajectories (Dorfman &amp; Tamar, 2020), dense rewards for meta-training but not testing (Rakelly et al., 2019), or ground-truth task IDs / descriptions (Liu et al., 2020;Kamienny et al., 2020). HyperX works well even if such information is not available.</p>
<p>Exploration in POMDPs.</p>
<p>Meta-exploration is related to exploration when learning in partially observable MDPs (POMDPs, Cassandra et al. (1994)), of which BAMDPs are a special case. This topic is mostly studied on small environments. Similar to our work, Cai et al. (2009) incentivize exploration in under-explored regions of belief space. However, they use two separate policies for exploration and exploitation and rely on Bayesian learning to update them, restricting this to small discrete state spaces. Several authors (Poupart &amp; Vlassis, 2008;Ross et al., 2008;Doshi et al., 2008;Ross et al., 2011) explore model-based Bayesian reinforcement learning in partially observable domains. By relying on approximate value iteration to solve the planning problem, they are also restricted to small environments. To our knowledge, only Yordanov (2019) provides some initial results on a simple environment using Random Network Distillation. They propose various ways to deal with the non-stationarity of the latent embedding such as using a random recurrent network that aggregates past trajectories.</p>
<p>Empirical Results</p>
<p>We present four experiments that illustrate how and why HyperX helps agents meta-learn good online adaptation strategies (Sec 5.1-5.3), and results on sparse MuJoCo Ant-Goal to demonstrate that HyperX scales well (Sec 5.4).</p>
<p>Many standard meta-RL benchmarks do not require much exploration, in the sense that there is no room for improvement via better exploration, such as the 2D navigation Pointrobot (Appendix B.2), Meta-World ML1 even with sparse rewards (Appendix B.1), or the otherwise challenging dense AntGoal environment (Appendix B.5). 
(A) (B) (C)</p>
<p>Treasure Mountain</p>
<p>We consider our earlier example (Fig 1), where the agent's task is to find a treasure hidden in tall grass. There are two good task-exploration strategies: (s1) search the grass until the treasure is found, or (s2) climb the mountain, spot the treasure, and go there directly. The latter strategy has higher expected return, but is harder to meta-learn since (a) climbing the mountain is discouraged by negative rewards and (b) the agent must meta-learn to interpret and remember the treasure location it sees from the mountain.</p>
<p>We implement this as follows (details in Appendix C.1.1): the treasure can be anywhere along a circle. The agent gets a sparse reward when it reaches the treasure, and a time penalty otherwise. Within the circle is the mountain, represented by a smaller circle. Walking on it incurs a higher time penalty. The agent's observation is 4D: its x-y position, and the treasure's x-y coordinates, which are only visible from the mountain top. The agent starts at the bottom of the circle and has one rollout of 100 steps to find the treasure. Figure 2 shows the learning curves of VariBAD (which uses no exploration bonuses for meta-training) and HyperX, with HyperX performing significantly better. Figures 2A-2C show the behaviour of HyperX at different times during training. At the beginning (2A), it explores along the circle (but does not stop at the treasure and explores further) and its performance increases. Then, it discovers the mountain top: because the VAE reconstruction error r error is high there, it  initially just stays there (2B). Performance drops since the penalty for climbing the mountain is slightly higher than the time penalty the agent gets otherwise. Finally, at the end of training (2C) it learns the optimal strategy s2 (consistently across all 10 seeds). Inspection of the rollouts show that VariBAD and other methods for online adaptation (RL 2 (Duan et al., 2016;Wang et al., 2016) and Belief Learning (Humplik et al., 2019)) always only meta-learn the inferior task-exploration strategy s1 (see Appendix B.3).</p>
<p>When using only r hyper , the agent only learns the inferior strategy s1: early in training, hyper-states are meaningless and the agent stops exploring the mountain top. When using only r error , the agent learns the superior strategy s2 around 70% of the time. For learning curves see Appendix B.3.</p>
<p>This experiment shows that HyperX tries out different taskexploration strategies during meta-training, and can therefore meta-learn a superior exploration strategy even when it is expensive in the short term, but pays off in the longer run.</p>
<p>Multi-Stage Gridworld</p>
<p>Next, we consider a partially observable multi-stage gridworld which illustrates how, without the appropriate exploration bonuses on hyper-states, existing methods can converge prematurely to a local optimum.</p>
<p>The gridworld is illustrated in Figure 3: three rooms are connected by narrow corridors, and three (initially unknown) goals (G1-G3) are placed in corners of rooms: The goals pro-vide increasing rewards, i.e. r 1 = 1, r 2 = 10 and r 3 = 100, but are only sequentially unlocked; G2 (r 2 ) is only available after G1 has been reached; G3 (r 3 ) is only available after G2 has been reached. The environment is partially observable (Poupart &amp; Vlassis, 2008;Cai et al., 2009) as the agent only observes its position in the environment and not which goals are unlocked. If the agent is not on an (available) goal it gets r = −0.1. G1 and G3 are always in the middle room, G2 always in an outer room on the same side as G1. The agent starts in the center of the middle room and has H = 50 steps. The best strategy is to search the first room for G1, then search the appropriate room for G2, and then return to the middle room to find G3. Figure 4 compares VariBAD, VariBAD with state-novelty bonus, and HyperX. VariBAD learns to reach G1 and remains there, effectively receiving only r 1 at every timestep. VariBAD+r(s) learns to find G2 and stay there, but fails to find G3. Only HyperX solves the problem (see behaviour in Figure 3). Methods which use a purely state-based exploration bonus such as VariBAD+r(s) are unable to find G3 in the middle room as those states s (not hyper-states (s, h)) appear already sufficiently explored. In contrast, a novelty bonus on the hyper-state r(s, h) like in HyperX leads to a high novelty bonus in the middle room once G2 is found because the belief changes.</p>
<p>These results show that without the right exploration bonuses during meta-training, the agent can prematurely converge to a suboptimal solution. Additionally, we see that that HyperX can handle this degree of partial observability.</p>
<p>Sparse HalfCheetahDir</p>
<p>To demonstrate the effect of the different exploration bonuses, we consider the following example for which we can compute exact beliefs. The environment is based on the HalfCheetah-Dir MuJoCo environment, which is commonly used in meta-RL (e.g., Finn et al., 2017;Rakelly et al., 2019). The prior distribution p(M ) is uniform over the two tasks "walk forward" and "walk backward". In the dense version the agent is rewarded according to its (1D) velocity in the correct direction. We consider a sparse version without resets: the agent only receives the dense reward once it walks sufficiently far away from its starting position, outside an interval [−5, 5] (and a control penalty otherwise), and has 200 environment steps to adapt. This makes it much more difficult to find the optimal adaptation strategy, which is to walk far enough in one direction to infer the task, and turn around in case the direction was wrong. ProMP (Rothfuss et al., 2019), E-MAML 1 (Stadie et al., 2018) and VariBAD (Zintgraf et al., 2020), as shown in Table 1a. HyperX in contrast successfully meta-learns the correct task-adaptation strategy. For all baselines, we used the available open source code.</p>
<p>Exploration in Exact Hyper-State Space. To investigate how the exploration bonuses in HyperX help solve this task, we first assume that we have access to the true hyper-state s + t =(s t , b t ), including the true belief which we define as Since we can manually compute this belief, we can train a Belief Oracle using standard reinforcement learning, by conditioning the policy on the exact hyper-state. Table 1b shows the performance of the Belief Oracle, with and without reward bonuses. Without the bonus, even this Belief Oracle does not learn the correct behaviour for this seemingly simple task. When adding the exploration bonus r hyper (b, s) on the hyper-state, the policy learns approximately Bayes-optimal behaviour. Figure 5 shows how the bonuses incentivise the agent to explore, with the red gradient in the background visualising the reward bonus (darker meaning more bonus). When the agent walks outside the sparse-reward interval and updates its belief, the reward bonus in the opposite direction becomes high since it has not yet visited that area with the updated belief very often. HyperX: Exploration in Approximate Hyper-State Space. Above we assumed access to the true belief b t . When meta-learning how to perform approximate belief inference alongside the policy however, these beliefs change over time and are initially inaccurate. As Table 1c (top) shows, using only the hyper-state exploration bonus r hyper , which worked well for the Belief Oracle, performs suboptimally. This is because early in training the belief inference is inaccurate, and the hyper-state bonus is meaningless: the agent prematurely and wrongly assumes it has sufficiently explored. Only when adding the error reward bonus r error as well to incentivise the agent to explore areas where the belief inference makes mistakes, can we meta-learn approximately Bayes-optimal behaviour for this task. Using only the error reward bonus r error performs poorly as well.</p>
<p>Different meta-learners. While we build HyperX on VariBAD, the same exploration bonuses can be used for other meta-learning methods that provide (a) a belief representation, and (b) a measure of how good the belief inference is. One such method is the work by Humplik et al. (2019) who train a belief model using the ground-truth task description. This makes meta-learning inference easier, and the exploration bonus r error may not be necessary: for sparse HalfCheetahDir, using only the hyper-state bonus (r hyper ) is sufficient, as shown in Table 1c (middle). This result is not directly comparable to HyperX since it uses privileged information, whereas HyperX meta-learns inference in an unsupervised way. For the method RL 2 , the RNN hidden state can be used as a belief proxy to compute the hyperstate bonus. The second exploration bonus (r error ) however, cannot be estimated because the hidden state is only used implicitly by the agent. As Table 1c shows, an exploration bonus on the state (r state ) or on the hyper-state (r hyper ) for RL 2 is not sufficient to solve the task.</p>
<p>Comparison to MetaCURE. Recently Zhang et al. (2020) proposed MetaCURE for meta-exploration. They use information gain as an intrinsic reward, defined as the difference between the prediction errors (of states/rewards) given the agent's current experience or given the groundtruth task. For the sparse HalfCheetahDir task this is high when the agent first steps over the interval bound. Even though MetaCure is defined for episodic task-adaptation, we can use its bonus in the online adaptation setting as well. Compared to HyperX it requires training two additional prediction networks, and it relies on privileged task information during meta-training to do so. Table 1c shows that this exploration bonus alone is not sufficient to solve the taskit only incentivises the agent to go to the interval boundary). Adding a hyper-state bonus is required to solve the task. </p>
<p>Sparse MuJoCo AntGoal</p>
<p>To show that HyperX can scale to more complex environments, we evaluate it on a harder MuJoCo task, a sparse version of the common meta-learning learning benchmark Ant-Goal-2D (Rothfuss et al., 2019;Rakelly et al., 2019). The agent must navigate to an initially unknown goal position. In the dense version, the agent gets a reward relative to its distance to the goal. This version can be solved by current meta-learning results (see Appendix B). We make this task significantly harder by sparsifying the rewards, giving the agent the dense reward only if it is within a certain distance of the goal (the blue shaded area in Figure 6B). The best exploration strategy is therefore to spiral outwards until a reward signal is found. Figure 6A shows the success rate of HyperX, VariBAD, RL 2 and PEARL across different episodes, where an agent is successful if it enters the goal circle. For RL 2 , VariBAD, and HyperX, the belief is maintained across episodes by not resetting the RNN (encoder) hidden state. Both RL 2 and PEARL only learn to go to goals close to the starting position, and therefore have low success rate. HyperX has a higher success rate than VariBAD. This result illustrates that a lack of good exploration can crucially affect final performance: the success rate in the 6th episode is good iff early exploration was good. Plots for the returns across episodes and learning curves can be found in Appendix B.5.</p>
<p>Figures 11 and 12 (Appendix B.5) show example behaviours of the meta-trained HyperX and VariBAD agent at test time.</p>
<p>HyperX learns to efficiently search the space of possible goals, occasionally in the shape of a spiral (Fig 6B), finding the goal in the first episode. VariBAD can also learn to search in a spiral but is not as efficient and more likely to fail. Once the agent reaches dense-reward radius around the goal, it is able to determine where the goal is and heads there directly. In subsequent episodes, the agent exploits its knowledge about the goal and returns directly to it.</p>
<p>Overall our empirical results show that HyperX can metalearn excellent adaptation behaviour on challenging sparse reward tasks where existing methods fail. We also evaluated HyperX on sparse environments used in the literature, like sparse PointRobot (Rakelly et al., 2019), and sparse Meta-World ML1 (Yu et al., 2019). However, in both cases, VariBAD can already solve these tasks (Appendix B.1-B.2).</p>
<p>Conclusion</p>
<p>The Meta-Exploration Problem. This paper showed that existing meta-learning methods can fail if the environment rewards are not densely informative with respect to the task, and myopic exploration during meta-training is insufficient. We highlighted that in this case, special attention needs to be paid to meta-exploration. This applies to many different problem settings, but we focused on online adaptation where the agent aims to maximise expected online return. Here, task-exploration is particularly challenging since the agent has to trade off exploration and exploitation.</p>
<p>Our Solution: HyperX. We proposed HyperX, which uses two exploration bonuses to incentivise the agent to explore in approximate hyper-state space during meta-training. This way, it collects the data necessary to learn approximate belief inference (incentivised by r error ), and tries out different task-exploration strategies during meta-training (incentivised by r hyper ). We demonstrated empirically how meta-learning without explicit meta-exploration can fail and why, and showed that HyperX can solve these tasks.</p>
<p>Software and Data</p>
<p>Our source code can be found at https://github. com/lmzintgraf/hyperx.</p>
<p>Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning</p>
<p>Supplementary Material</p>
<p>A. Additional Background</p>
<p>A.1. Randomised Prior Functions</p>
<p>In reinforcement learning, we can use the fact that unseen states can be seen as out-of-distribution data of a model that is trained on all data the agent has seen so far. Getting uncertainty estimates on states can thus quantify our uncertainty about the value of a state and in turn whether we have explored these states sufficiently. We can think about why exploration purely in the state space S (which is shared across tasks) is not enough: if the agent has explored a state many times in one task and is certain of its value, it should not necessarily exploit this knowledge in a different task, because this same state could have a completely different value. We cannot view these as separate exploration problems however, since we also have to try out different deployed exploration strategies and combine the information to meta-learn Bayes-optimal behaviour. Therefore, we want to incentivise the agent to explore in the hyper-state space S + = S × B. Only if an environment state together with a specific belief has been observed sufficiently often to determine its value should the agent trust its value estimate of that belief-state. This therefore amounts to exploration in a BAMDP state space, which essentially means trying out different exploration strategies in the environments of the training distribution. We use Random Network Distillation (RND) (Osband et al., 2018;Burda et al., 2019b;Ciosek et al., 2020) to obtain such uncertainty estimates and review them using the formulation of Ciosek et al. (2020) in the following.</p>
<p>Assume we are given a set of training data
D = {s i } N i=1
of all states the agent has observed. To get uncertainty estimates, we first fit B predictor networks g j (s) (j = 1, . . . , B) to a random prior process f j (s) each (a network with randomly initialised weights, which is fixed and never updated). We then estimate the uncertainty for a state s * as
σ 2 (s * ) = max(0, σ 2 µ (s * ) + βv σ (s * ) − σ 2 A ),(7)
where σ 2 µ (s * ) is the sample mean of the squared errors between the B predictor networks and the prior processes; v σ (s * ) is the sample variance of the squared error. The first quantifies our uncertainty, whereas the second quantifies our uncertainty over what our uncertainty is. In practice, B = 1 is typically sufficient and the second term disappears (Ciosek et al., 2020). The term σ 2 A is the aleatoric noise inherent in the data which is an irreducible constant. In theory, this can be learned as well and depends on how much information can be extracted about the value of states and actions from the data. In practice, we set this term to 0.</p>
<p>Given a hyper-state s + t = (s t , b t ), an ensemble of B prior networks {f i (s + )} B i=1 and corresponding predictor networks {h i (s + )} B i=1 , the reward bonus is defined as
r c (s + t ) = max(0, σ m u 2 (s + t ) + βv σ (s + t ) − σ 2 A ) (8)
where σ m u 2 (s + t ) is the sample mean of the squared error between prior and predictor networks and v σ (s + t ) is the sample variance of that error.</p>
<p>B. Additional Results</p>
<p>In this section we provide additional experimental results. The first two sections are additional environments -in particular sparse environments used in the literature before, but where we found that our baselines already performed very well. In addition, we provide more details and results for the experiments in the main paper. Implementation details, including hyperparameters and environment specifications, are given in Appendix C. The (anonymised) source code is attached as additional supplementary material.</p>
<p>B.1. Meta-World</p>
<p>To test how our method scales up to more challenging problem settings, we evaluate it on the Meta-World benchmark (Yu et al., 2019), where a simulated robot arm has to perform tasks. We evaluate our method on the ML1 benchmark, of which three different versions exist: reach/push/pick-andplace (in increasing order of complexity). In each of these, task distributions are generated by varying the starting position of the agent and the goal/object positions.</p>
<p>Each environment has a dense reward function that was designed such that an agent trained on a single task (i.e., fixed starting/object/goal position) can learn to solve it. Evaluation is done in terms of success rate (rather than return), which is a task-specific binary signal indicating whether the task was accomplished (at any moment during the rollout). Yu et al. (2019) proposed a sparse version of this benchmark that uses this binary success indicator, rather than the dense reward, for training. This sparse version was used in (Zhang et al., 2020), on ML1-reach and ML1-push.</p>
<p>The agent is trained on a set of 50 environments and evaluated on unseen environments from the same task distribution. In all baselines, the agent has 10 episodes to adapt, and performance is measured in the last rollout. Since we consider the online adaptation setting where we want the agent has to perform well from the start, we trained VariBAD and Hy-perX to maximise online return during the first two episodes. This is more challenging since it includes exploratory actions.  Zhang et al. (2020). We ran VariBAD and HyperX for 5 random seeds for dense reach/push, and 20 seeds for dense pick-place. VariBAD and HyperX were trained to maximise expected online return within 2 episodes. The first (few) episodes often includes exploratory actions, yet have higher success rate than existing methods that maximise final episodic return. For the sparse Pick-Place environment, in brackets we report the number of seeds that learned something. Table 2 shows the results for both the dense and sparse versions of ML1.</p>
<p>ML1-reach / ML1-push. VariBAD achieves 100% success rate on both the dense and the sparse version of ML1-reach and ML1-push in the first rollout. Compared to other existing methods -even MetaCURE (Zhang et al., 2018) which explicitly tries to deal with sparsity -this is a significant improvement. We confirm in our experiments that HyperX does not decrease performance and also reaches 100% success rate on these environments.</p>
<p>ML1</p>
<p>-pick-place. The environment ML1-pick-place is more challenging, because the task consists of two steps: picking up an object and placing it somewhere (where both the object and goal location differ across tasks). Even on the dense version, existing methods struggle. HyperX achieves state of the art on this task with 44.5% success rate, suggesting HyperX can help meta-learning even when rewards are dense. For VariBAD and HyperX we found that our agents either learn the task near perfectly (and have close to 100% success rate in the first rollout), or not at all. VariBAD learned something for 6 out of 20 seeds, and HyperX learned something for 9 out of 20 seeds. For the sparse version of this environment, we only saw some success for 1/20 seeds for both VariBAD or HyperX.</p>
<p>We suspect that the main challenge in ML1-Pick-Place is the short horizon (150), which does not give the agent enough time to explore during meta-training. This is why HyperX can give some improvement even in the dense version. In an upcoming version of Meta-World (Yu et al., 2019), the horizon will be increased to 200, opening up interesting opportunities for future research on sparse Pick-Place.</p>
<p>B.2. Sparse 2D Navigation</p>
<p>We evaluate on a Point Robot 2D navigation task used by Gupta et al. (2018); Rakelly et al. (2019); Humplik et al. (2019). The agent must navigate to an unknown goal sampled along the border of a semicircle of radius 1.0, and receives a reward relative to its proximity to the goal when it is within a goal radius of 0.2. Thus far, only Humplik et al. (2019) successfully meta-learn to solve this task by meta-training with sparse rewards, though they rely on privileged information during meta-training (the goal position). The other methods meta-train with dense rewards and evaluate using sparse rewards. We use a horizon of 100 here (instead of 20 as in the papers above) to give VariBAD and HyperX enough time to demonstrate interesting exploratory behaviour. Figure 7A shows the performance of PEARL, VariBAD, and HyperX at test time, when rolling out for 30 episodes. Both VariBAD and HyperX adapt to the task quickly compared to PEARL, but HyperX reaches slightly lower final performance.</p>
<p>To shed light on these performance differences, Figures  7B and 7C visualise representative example rollouts for the meta-trained VariBAD and HyperX agents. We picked examples where the target goals are at the end of the semicircle, which we found are most difficult for the agents. VariBAD (7B) struggles to find the goal, taking several attempts to reach it. Once the goal is found, it does return to it but on a sub-optimal trajectory. By contrast, HyperX searches the space of goals more strategically, and returns to the goal faster in subsequent episodes.</p>
<p>(A) Meta-Test Performance (B) VariBad example rollout (C) HyperX example rollout Figure 7. Meta-test performance on the Sparse 2D Navigation environment. Left: Performance averaged over the task distribution at the end of training. Because PEARL is not optimizing for optimal exploration, it requires many more episodes to find the goal. Both VariBad and HyperX optimise for optimal exploration and are able to quickly find the goal. However, VariBad's exploration is suboptimal, not covering all possible goal locations equally well (see middle plot), explaining the lower performance compared to HyperX.</p>
<p>B.3. Treasure Mountain</p>
<p>Ablations. Figure 8A shows the learning curves for the HyperX, in comparison to ablating different exploration bonuses. When using only the hyper-state novelty bonus r hyper , HyperX learns the inferior strategy of walking in a circle: it has no incentive to go up the mountain early in training (because beliefs there are meaningless because the VAE has not learned yet to interpret the hint) and stars avoiding the mountain. When using only the VAE reconstruction error bonus r error , the agent learns the superior strategy of walking up the mountain to see the goal location 70% of the time (7/10 seeds). In contrast, HyperX, which uses both exploration bonuses, learns the superior strategy for all 10 seeds. Lastly, we tested VariBAD with a simple state novelty exploration bonus: this again learns the inferior circle-walking strategy only, because it quickly learns to avoid the mountain top.</p>
<p>Baselines -Performance. Figure 8B shows the learning curves for HyperX and VariBAD (discussed in Sec 5.1), as well as additional baselines RL 2 (Duan et al., 2016;Wang et al., 2016) (which is a model-free method where the policy is a recurrent network that gets previous actions and rewards as inputs in addition to the environment state) and the Belief Learning method of Humplik et al. (2019) (which uses privileged information -the goal position -during metatraining). Both these baselines also only learn the inferior circle-walking strategy, because the correct incentives for meta-exploration are missing.</p>
<p>Baselines -Behaviour. Figures 8C and 8D show meta-test time behaviour of VariBAD and RL 2 : both methods learn to walk in a circle until the goal is found. This was consistent across all (10) seeds. Figure 9 shows the learning curves for the Sparse Chee-tahDir experiments, with 95% confidence intervals (over 20 seeds). Fig 9A shows Figure 9C shows example behaviour of a suboptimal policy at test time. The agent returns back into the zero-reward zone after realising that the task was not "go left", but stays in there instead of behaving optimally, which is going further to the right and into the dense reward area beyond the sparse interval border.</p>
<p>B.4. Sparse CheetahDir</p>
<p>B.5. Sparse MuJoCo AntGoal</p>
<p>In addition to the main results in the paper (Sec 5.4) we provide additional experimental results here. Figure 10A shows the returns achieved by the agents across different episodes. Figures 10B show the learning curves for the returns during the first episode, with 95% confidence intervals (shaded areas, 10 seeds). Figure 10C shows the combined learning curves, comprising of all 6 episodes, with 95% confidence intervals (shaded areas, 10 seeds). Figures  12 and 11 show example rollouts for VariBAD and HyperX.</p>
<p>Dense AntGoal. We also evaluated HyperX on the dense AntGoal environment. VariBAD and HyperX were trained to maximise performance within a single episode. PEARL was trained with the default hyperparameters provided by the open-sourced code of the authors. The results are:: VariBAD: -123 (Episode 1), HyperX: -127 (Episode 1), PEARL: -200 (Episode 6). This confirms that HyperX does not impact performance, but that there is also not much room for improvement.  (Zintgraf et al., 2020) and RL 2 (Duan et al., 2016;Wang et al., 2016). They follow the inferior exploration strategy of walking around the circle until the treasure is found, instead of climbing the mountain to directly observe the treasure and get there faster.  </p>
<p>C. Implementation Details</p>
<p>The source code is available as additional supplementary material. In this section, we provide the environment specifications (C.1), runtimes (C.5), and hyperparameters (C.6).</p>
<p>C.1. Environment Specifications</p>
<p>In this section we provide additional details on the environments that were used in the main paper. Implementation of these environments are in the provided source code. where (x, y) is the agent's position (the mountain center is 0, 0, and the mountain radius 0.5). If not at the treasure or on the mountain, the agent gets a timestep penalty of at least −5, which increases as the agent walks further outside the outer circle (to discourage it from walking too far). The agent cannot walk outside [−1.5, 1.5] in either direction. Three (initially unknown) goals (G1-G3) are placed in corners of rooms: G1 in the middle room, G2 in the room that is on the side where G1 was placed, and G3 in the middle room (but not where G1 was placed). The agent always starts in the middle of the centre room and has H = 50 steps. The goals provide increasing rewards, i.e. r 1 = 1, r 2 = 10 and r 3 = 100, but are only sequentially unlocked; G2 (r 2 ) is only available after G1 has been reached; G3 (r 3 ) is only available after G2 has been reached. The environment is partially observable (Poupart &amp; Vlassis, 2008;Cai et al., 2009) as the agent only observes its position in the environment and not which goals are unlocked. If the agent is not on an (available) goal it gets r = −0.1. When the agent stands on a goal, it keeps receiving the respective reward while standing there (the goal does not disappear). The best strategy is to search the first room for G1, then search the appropriate room for G2, and then return to the middle room to find G3.</p>
<p>C.2. Sparse HalfCheetahDir</p>
<p>We use the commonly used HalfCheetahDir meta-learning benchmark (based on code of Zintgraf et al. (2020)), and sparsify it as follows. If the agent's x-position is within [−5, 5] it only gets the control penalty; otherwise it gets the standard dense reward comprised of the sum of the control penalty and the 1D velocity in the correct direction.</p>
<p>C.3. Sparse MuJoCo AntGoal</p>
<p>We use the commonly used AntGoal meta-learning benchmark (based on code of Rakelly et al. (2019)), and sparsify it as follows. We extend the environment's state space by including the x and y-position of the agent's torso. In the original AntGoal, the goal is sampled from within a circle of radius of 3 with a higher chance of the goal being sampled away from the centre of the circle. Unlike the dense version where the agent receives a dense goal-related reward at all times, our sparse AntGoal only receives goal-related rewards when within a radius of 1 of the goal.</p>
<p>The agent receives at all time a control penalty and contact forces penalty. When outside the goal circle, the agent receives an additional constant negative reward that is equivalent to the negative goal radius, i.e. −1. When within the goal circle, the agent receives a reward of 1 for being within the goal circle and a penalty equivalent to the negative distance to the goal, essentially encouraging the agent to walk towards the centre of the goal circle.</p>
<p>C.4. Meta-World</p>
<p>We use the official version of Meta-World as provided by Yu et al. (2019) at https://github.com/ rlworkgroup/metaworld. As suggested by Yu et al. (2019) and as tested in Zhang et al. (2020), for the sparse version of this environment, we use the success criterion which the environment returns, and give the agent a reward of 0 if success=False and a reward of 1 if success=True. The success criterion depends on the environment; in 'Reach' for example it is true if the agent put its gripper close to the (initially unknown) goal position, and false otherwise. For evaluation, we report 'Success' if the agent was successful at any moment during an episode, following the evaluation protocol proposed by Yu et al. (2019).  </p>
<p>C.5. Runtimes</p>
<p>C.6. Hyperparameters</p>
<p>We train the policy using PPO, and we add the intrinsic bonus rewards to the extrinsic environment reward and use the sum when learning with PPO. We normalise the intrinsic and extrinsic rewards separately by dividing by a rolling estimate of the standard deviation.</p>
<p>On the next two pages we show the hyperparameters used for the policy, the VAE, and the exploration bonuses. Hyperparameters were selected using a simple (non-exhaustive) gridsearch.</p>
<p>For the MuJoCo environments, we only used the relevant state information for the RND hyper-state bonus (the x-axis for HalfCheetahDir, and the x-y-position for AntGoal).</p>
<p>RND Hyperparameter Sensitivity. To assess how sensitive HyperX to choices of hyperparameters that affect the hyperstate exploration bonus, we evaluated it on a range of different choices, shown in Table 4. There is little sensitivity to architecture depth and batchsize, as well as to the output dimension of the RND networks. Performance is stable for learning rates 10 −3 −10 −6 (possibly because we use the Adam optimiser), but we found that the best frequency (f req) at which the RND network is updated to be environment dependent. Performance is sensitive to the scaling factor (wsi in the table) for the initial prior network weights. We used a scaling factor of 10 in our experiments, and found that too small or too large scaling factors can hurt performance. An interesting direction for future work is to find more principled ways to guide the choice of the hyperparameters that are particularly sensitive to the exploration and across environments.
RND dim out = 32 (default 128) 737 RND dim out = 256 (default 128) 812 RND depth = 1 (default 2) 794 RND depth = 3 (default 2) 814 RND batchsize = 32 (default 128) 856 RND batchsize = 256 (default 128) 867 RND lr = 1e − 2 (default 1e − 4) 108 RND lr = 1e − 3 (default 1e − 4) 883 RND lr = 1e − 5 (default 1e − 4) 845 RND lr = 1e − 6 (default 1e − 4)
766 RND wsi = 1 (default 10) 597 RND wsi = 5 (default 10) 766 RND wsi = 15 (default 10) 533      </p>
<p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</p>
<p>Figure 1 .
1Illustration of the Meta-Exploration Problem.</p>
<p>;Burda et al. (2019a);Pathak et al. (2017);Schmidhuber (1991);Stadie et al. (2015).</p>
<p>Figure 2 .
2Treasure Mountain Results. Top: Learning curves for HyperX and VariBAD (10 seeds, 95% confidence intervals shaded). Bottom: Behaviour of the HyperX agent at different stages of training. HyperX learns the superior task-exploration strategy of climbing the mountain to see the treasure, and going there directly after. VariBAD learns the inferior strategy of walking around the circle until finding the treasure (see rollouts in Appendix B.3).</p>
<p>Figure 3 .
3Multi-Stage Gridworld. Goal 1 (×) unlocks goal 2 ( * ), which unlocks goal 3 (•). Example behaviour of HyperX in blue.</p>
<p>Figure 4 .
4Multi-Stage Gridworld Learning Curves. (3 seeds)</p>
<p>Figure 5 .
5HalfCheetahDir Example Rollouts for the Belief Oracle, early in meta-training (1e6 frames), trained with the hyperstate-bonus r hyper (s + ). The y-axis denotes time in agent steps. The background visualises the hyper-state-bonus: darker means higher bonus. Top: At the beginning of the episode, the agent's belief is the prior, and the exploration bonus incentivises it to explore away from the familiar start position. Bottom: Once the agent enters the dense-reward zone, it infers the task and updates its belief. Now, the states on the right side seem novel since the agent has not seen them together with the posterior belief.follows. The prior belief is b 0 =[0.5, 0.5] and it can be updated to the posterior belief b=[1, 0] (left) or b=[0, 1] (right) once the agent observes a single reward outside of the interval[−5, 5].</p>
<p>Figure 6 .
6Sparse AntGoal Results. 6A shows the success rate per episode (10 seeds, standard error shaded). 6B shows a cherrypicked test rollout of the HyperX agent during the first episode.</p>
<p>Figure 8 .
8Treasure Mountain -Additional Rollouts. Shown are example rollouts for the final agents of VariBAD</p>
<p>of a policy which failed to learn Bayes-optimal behaviour. We observe such behaviour often when training HyperX with the reward bonus on the hyper-states only, r hyper (b, s).</p>
<p>Figure 9 .Figure 10 .Figure 11 .
91011HalfCheetahDir: Additional Results. Learning curves for the Belief Oracle (A) and HyperX (B), with and without reward bonus, averaged over 20 seeds.. (A) Return per episode at meta-test time (standard error shaded). RL2 and PEARL do not learn to solve the task and achieve a reward of around -150 per episode. (B) Learning curve (return in ep 1) (C) Learning Curve (sum of returns in ep 1-6) Sparse AntGoal: Additional Plots. (10 seeds). HyperX Example Rollouts Figure 12. VariBAD Example Rollouts</p>
<p>The observations of the agent are 4D and continuous. The first two dimensions are the agent's (x, y)-position. The last two dimensions are zero if the agent is not on the mountain top, and are the (x, y)-coordinates of the treasure when the agent is on the mountain top (within a radius of 0.1). The agent's actions are the (continuous) stepsize it takes in (x, y)-direction, bounded in [−0.1, 0.1].C.1.2. MULTI-STAGE GRIDWORLDThe layout of this environment is depicted inFig 3.It consist of three rooms which are of size 3 × 3 grid, and corridors that connect the rooms of length 3. The environment state is the (x, y) position of the agent, unnormalised. There are five available actions: no-op, up, right, down, left.</p>
<p>Table 1 (
1top) shows that a policy 
trained with a reward bonus only on the state, r(s), performs 
worse. The reason is that the agent is not incentivised to 
explore states to the far right after its belief has changed. 
Inspection of the learned policies shows that agents trained </p>
<p>this for the Belief Oracle, with different exploration bonuses. Fig 9B shows this for HyperX, with different exploration bonuses.</p>
<p>Table 3
3shows the runtimes for our experiments. Unless otherwise stated, we used a NVIDIA GeForce GTX 1080 GPU. These runtimes should serve as a rough estimate, and can vary depending on hardware and concurrent processes.Environment 
Frames Runtime (ca.) </p>
<p>Treasure Mountain 
8e+7 
35h 
Multi-Stage Gridworld 
1e+8 
65h (CPU) 
Sparse HalfCheetahDir 
3e+7 
20h (CPU) 
Sparse AntGoal 
4e+8 
65h 
Meta-World 
5e+7 
45h 
Sparse 2D Navigation 
5e+7 
12h </p>
<p>Table 3 .
3</p>
<p>Table 4 .
4Additional Sparse CheetahDir Results, for different RND hyperparameter settings (averaged over three seeds). wsi stands for weight scale initialisation of the fixed random prior network.Treasure 
GridWorld CheetahDir AntGoal </p>
<p>University of Oxford, UK. 2 Mila, Université de Montréal, Canada. 3 Microsoft Research, Cambridge, UK. Correspondence to: Luisa Zintgraf <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#117;&#105;&#115;&#97;&#46;&#122;&#105;&#110;&#116;&#103;&#114;&#97;&#102;&#64;&#99;&#115;&#46;&#111;&#120;&#46;&#97;&#99;&#46;&#117;&#107;">&#108;&#117;&#105;&#115;&#97;&#46;&#122;&#105;&#110;&#116;&#103;&#114;&#97;&#102;&#64;&#99;&#115;&#46;&#111;&#120;&#46;&#97;&#99;&#46;&#117;&#107;</a>.
E-MAML/ProMP/PEARL are not designed to adapt within a single episode, so we do the gradient update (E-MAML/ProMP) / posterior sampling (PEARL) after half an episode.
AcknowledgementsWe thank Wendelin Böhmer, Tabish Rashid, Matt Smith, Jelena Luketina, Sebastian Schulze, and Joost van Amersfoort for helpful discussions and feedback. We also thank the anonymous reviewers for their feedback and suggestions that helped improve our paper. Luisa Zintgraf is supported by the 2017 Microsoft Research PhD Scholarship Program, and the 2020 Microsoft Research EMEA PhD Award. Maximilian Igl and Cong Lu are supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems. Kristian Hartikainen is funded by the EPSRC. This work was supported by a generous equipment grant from NVIDIA, and enabled in part by computing resources provided by Compute Canada. This project has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713).
Surprise-based intrinsic motivation for deep reinforcement learning. J Achiam, S Sastry, NeurIPS Deep RL Workshop. Achiam, J. and Sastry, S. Surprise-based intrinsic motivation for deep reinforcement learning. In NeurIPS Deep RL Workshop, 2016.</p>
<p>Unifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying count-based explo- ration and intrinsic motivation. In Advances in Neural Information Processing Systems, pp. 1471-1479, 2016.</p>
<p>Large-scale study of curiosity-driven learning. Y Burda, H Edwards, D Pathak, A Storkey, T Darrell, A A Efros, ICLR. Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A. A. Large-scale study of curiosity-driven learning. In ICLR, 2019a.</p>
<p>Exploration by random network distillation. Y Burda, H Edwards, A Storkey, O Klimov, International Conference on Learning Representation (ICLR). Burda, Y., Edwards, H., Storkey, A., and Klimov, O. Explo- ration by random network distillation. In International Conference on Learning Representation (ICLR), 2019b.</p>
<p>Learning to explore and exploit in pomdps. C Cai, X Liao, Carin , L , Advances in Neural Information Processing Systems. Cai, C., Liao, X., and Carin, L. Learning to explore and exploit in pomdps. In Advances in Neural Information Processing Systems, pp. 198-206, 2009.</p>
<p>Acting optimally in partially observable stochastic domains. A R Cassandra, L P Kaelbling, M L Littman, Twelfth National Conference on Artificial Intelligence. AAAI Classic Paper AwardCassandra, A. R., Kaelbling, L. P., and Littman, M. L. Act- ing optimally in partially observable stochastic domains. In Twelfth National Conference on Artificial Intelligence, 1994. AAAI Classic Paper Award, 2013.</p>
<p>Conservative uncertainty estimation by fitting prior networks. K Ciosek, V Fortuin, R Tomioka, K Hofmann, R Turner, International Conference on Learning Representation (ICLR. 2020Ciosek, K., Fortuin, V., Tomioka, R., Hofmann, K., and Turner, R. Conservative uncertainty estimation by fitting prior networks. In International Conference on Learning Representation (ICLR), 2020.</p>
<p>Offline meta reinforcement learning. R Dorfman, A Tamar, arXiv:2008.02598arXiv preprintDorfman, R. and Tamar, A. Offline meta reinforcement learning. arXiv preprint arXiv:2008.02598, 2020.</p>
<p>Reinforcement learning with limited reinforcement: Using bayes risk for active learning in pomdps. F Doshi, J Pineau, Roy , N , Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningDoshi, F., Pineau, J., and Roy, N. Reinforcement learning with limited reinforcement: Using bayes risk for active learning in pomdps. In Proceedings of the 25th inter- national conference on Machine learning, pp. 256-263, 2008.</p>
<p>RL 2 : Fast reinforcement learning via slow reinforcement learning. Y Duan, J Schulman, X Chen, P L Bartlett, I Sutskever, Abbeel , P , arXiv:1611.02779arXiv preprintDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. RL 2 : Fast reinforcement learn- ing via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.</p>
<p>Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. M O Duff, A Barto, University of Massachusetts at AmherstPhD thesisDuff, M. O. and Barto, A. Optimal Learning: Computa- tional procedures for Bayes-adaptive Markov decision processes. PhD thesis, University of Massachusetts at Amherst, 2002.</p>
<p>Model-agnostic metalearning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, International Conference on Machine Learning (ICML). Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Interna- tional Conference on Machine Learning (ICML), 2017.</p>
<p>Bayesian reinforcement learning: A survey. Foundations and Trends® in Machine Learning. M Ghavamzadeh, S Mannor, J Pineau, A Tamar, 8Ghavamzadeh, M., Mannor, S., Pineau, J., Tamar, A., et al. Bayesian reinforcement learning: A survey. Founda- tions and Trends® in Machine Learning, 8(5-6):359-483, 2015.</p>
<p>Meta-reinforcement learning of structured exploration strategies. A Gupta, R Mendonca, Y Liu, P Abbeel, S Levine, Advances in Neural Processing Systems (NeurIPS). Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. Meta-reinforcement learning of structured exploration strategies. In Advances in Neural Processing Systems (NeurIPS), 2018.</p>
<p>Model agnostic meta-exploration. S Gurumurthy, S Kumar, K Sycara, Mame, Proceedings of (CoRL) Conference on Robot Learning. (CoRL) Conference on Robot Learning100Gurumurthy, S., Kumar, S., and Sycara, K. Mame: Model agnostic meta-exploration. In Proceedings of (CoRL) Conference on Robot Learning, volume 100, pp. 910 - 922, October 2019.</p>
<p>T Hospedales, A Antoniou, P Micaelli, A Storkey, arXiv:2004.05439Meta-learning in neural networks: A survey. arXiv preprintHospedales, T., Antoniou, A., Micaelli, P., and Storkey, A. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020.</p>
<p>J Humplik, A Galashov, L Hasenclever, P A Ortega, Y W Teh, N Heess, arXiv:1905.06424Meta reinforcement learning as task inference. arXiv preprintHumplik, J., Galashov, A., Hasenclever, L., Ortega, P. A., Teh, Y. W., and Heess, N. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424, 2019.</p>
<p>Learning adaptive exploration strategies in dynamic environments through informed policy regularization. P.-A Kamienny, M Pirotta, A Lazaric, T Lavril, N Usunier, L Denoyer, arXiv:2005.02934arXiv preprintKamienny, P.-A., Pirotta, M., Lazaric, A., Lavril, T., Usunier, N., and Denoyer, L. Learning adap- tive exploration strategies in dynamic environments through informed policy regularization. arXiv preprint arXiv:2005.02934, 2020.</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, International Conference on Learning Representation (ICLR). Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In International Conference on Learning Repre- sentation (ICLR), 2014.</p>
<p>Explore then execute: Adapting without rewards via factorized meta-reinforcement learning. E Z Liu, A Raghunathan, P Liang, C Finn, arXiv:2008.02790arXiv preprintLiu, E. Z., Raghunathan, A., Liang, P., and Finn, C. Ex- plore then execute: Adapting without rewards via fac- torized meta-reinforcement learning. arXiv preprint arXiv:2008.02790, 2020.</p>
<p>Meta-trained agents implement bayes-optimal agents. V Mikulik, G Delétang, T Mcgrath, T Genewein, M Martic, S Legg, P Ortega, Advances in Neural Information Processing Systems. 33Mikulik, V., Delétang, G., McGrath, T., Genewein, T., Mar- tic, M., Legg, S., and Ortega, P. Meta-trained agents implement bayes-optimal agents. Advances in Neural Information Processing Systems, 33, 2020.</p>
<p>P A Ortega, J X Wang, M Rowland, T Genewein, Z Kurth-Nelson, R Pascanu, N Heess, J Veness, A Pritzel, P Sprechmann, arXiv:1905.03030Meta-learning of sequential strategies. arXiv preprintOrtega, P. A., Wang, J. X., Rowland, M., Genewein, T., Kurth-Nelson, Z., Pascanu, R., Heess, N., Veness, J., Pritzel, A., Sprechmann, P., et al. Meta-learning of se- quential strategies. arXiv preprint arXiv:1905.03030, 2019.</p>
<p>Deep exploration via randomized value functions. I Osband, D Russo, Z Wen, B Van Roy, Journal of Machine Learning Research. Osband, I., Russo, D., Wen, Z., and Van Roy, B. Deep exploration via randomized value functions. Journal of Machine Learning Research, 2017.</p>
<p>Randomized prior functions for deep reinforcement learning. I Osband, J Aslanides, A Cassirer, Advances in Neural Information Processing Systems (NeurIPS). Osband, I., Aslanides, J., and Cassirer, A. Randomized prior functions for deep reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), pp. 8617-8629, 2018.</p>
<p>Count-based exploration with neural density models. G Ostrovski, M G Bellemare, A Van Den Oord, R Munos, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Exploration in Approximate Hyper-State SpaceOstrovski, G., Bellemare, M. G., van den Oord, A., and Munos, R. Count-based exploration with neural density models. In Proceedings of the 34th International Confer- ence on Machine Learning-Volume 70, pp. 2721-2730. JMLR. org, 2017. Exploration in Approximate Hyper-State Space</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, Darrell , T , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised predic- tion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 16-17, 2017.</p>
<p>Uncertainty in neural networks: Approximately bayesian ensembling. T Pearce, M Zaki, A Brintrup, N Anastassacos, Neely , A , International Conference on Artificial Intelligence and Statistics (AISTATS). 2020Pearce, T., Zaki, M., Brintrup, A., Anastassacos, N., and Neely, A. Uncertainty in neural networks: Approximately bayesian ensembling. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.</p>
<p>Model-based bayesian reinforcement learning in partially observable domains. P Poupart, N Vlassis, Proc Int. Symp. on Artificial Intelligence and Mathematics. Int. Symp. on Artificial Intelligence and MathematicsPoupart, P. and Vlassis, N. Model-based bayesian reinforce- ment learning in partially observable domains. In Proc Int. Symp. on Artificial Intelligence and Mathematics,, pp. 1-2, 2008.</p>
<p>Efficient off-policy meta-reinforcement learning via probabilistic context variables. K Rakelly, A Zhou, D Quillen, C Finn, S Levine, International Conference on Machine Learning (ICML). Rakelly, K., Zhou, A., Quillen, D., Finn, C., and Levine, S. Efficient off-policy meta-reinforcement learning via prob- abilistic context variables. In International Conference on Machine Learning (ICML), 2019.</p>
<p>Bayes-adaptive pomdps. S Ross, B Chaib-Draa, J Pineau, Advances in neural information processing systems. Ross, S., Chaib-draa, B., and Pineau, J. Bayes-adaptive pomdps. In Advances in neural information processing systems, pp. 1225-1232, 2008.</p>
<p>A bayesian approach for learning and planning in partially observable markov decision processes. S Ross, J Pineau, B Chaib-Draa, P Kreitmann, Journal of Machine Learning Research. 12Ross, S., Pineau, J., Chaib-draa, B., and Kreitmann, P. A bayesian approach for learning and planning in partially observable markov decision processes. Journal of Ma- chine Learning Research, 12(May):1729-1770, 2011.</p>
<p>Promp: Proximal meta-policy search. J Rothfuss, D Lee, I Clavera, T Asfour, Abbeel , P , International Conference on Learning Representation (ICLR). Rothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P. Promp: Proximal meta-policy search. In International Conference on Learning Representation (ICLR), 2019.</p>
<p>A possibility for implementing curiosity and boredom in model-building neural controllers. J Schmidhuber, Proc. of the international conference on simulation of adaptive behavior: From animals to animats. of the international conference on simulation of adaptive behavior: From animals to animatsSchmidhuber, J. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pp. 222- 227, 1991.</p>
<p>Incentivizing exploration in reinforcement learning with deep predictive models. B C Stadie, S Levine, Abbeel , P , arXiv:1507.00814arXiv preprintStadie, B. C., Levine, S., and Abbeel, P. Incentivizing ex- ploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.</p>
<p>Some considerations on learning to explore via meta-reinforcement learning. B C Stadie, G Yang, R Houthooft, X Chen, Y Duan, Y Wu, P Abbeel, I Sutskever, Advances in Neural Processing Systems (NeurIPS). Stadie, B. C., Yang, G., Houthooft, R., Chen, X., Duan, Y., Wu, Y., Abbeel, P., and Sutskever, I. Some considerations on learning to explore via meta-reinforcement learning. In Advances in Neural Processing Systems (NeurIPS), 2018.</p>
<p>An analysis of modelbased interval estimation for markov decision processes. A L Strehl, M L Littman, Journal of Computer and System Sciences. 748Strehl, A. L. and Littman, M. L. An analysis of model- based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309- 1331, 2008.</p>
<h1>exploration: A study of count-based exploration for deep reinforcement learning. H Tang, R Houthooft, D Foote, A Stooke, O X Chen, Y Duan, J Schulman, F Deturck, Abbeel , P , Advances in neural information processing systems. Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O. X., Duan, Y., Schulman, J., DeTurck, F., and Abbeel, P. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural informa- tion processing systems, pp. 2753-2762, 2017.</h1>
<p>Learning to reinforcement learn. J X Wang, Z Kurth-Nelson, D Tirumala, H Soyer, J Z Leibo, R Munos, C Blundell, D Kumaran, M Botvinick, Annual Meeting of the Cognitive Science Community (CogSci). Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., and Botvinick, M. Learning to reinforcement learn. In Annual Meeting of the Cognitive Science Community (CogSci), 2016.</p>
<p>Using instrinsic motivation for exploration in partially observable environments. Y Yordanov, Oxford, UKUniversity of OxfordMaster's thesisYordanov, Y. Using instrinsic motivation for exploration in partially observable environments. Master's thesis, University of Oxford, Oxford, UK, 2019.</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Conference on Robot Learning (CoRL). Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and eval- uation for multi-task and meta reinforcement learning. In Conference on Robot Learning (CoRL), 2019. URL https://arxiv.org/abs/1910.10897.</p>
<p>Decoupling dynamics and reward for transfer learning. A Zhang, H Satija, J Pineau, ICLR workshop track. Zhang, A., Satija, H., and Pineau, J. Decoupling dynamics and reward for transfer learning. In ICLR workshop track, 2018.</p>
<p>Learn to effectively explore in context-based meta-rl. J Zhang, J Wang, H Hu, Y Chen, C Fan, C Zhang, arXiv:2006.08170arXiv preprintZhang, J., Wang, J., Hu, H., Chen, Y., Fan, C., and Zhang, C. Learn to effectively explore in context-based meta-rl. arXiv preprint arXiv:2006.08170, 2020.</p>
<p>Varibad: A very good method for bayes-adaptive deep rl via meta-learning. L Zintgraf, K Shiarlis, M Igl, S Schulze, Y Gal, K Hofmann, S Whiteson, International Conference on Learning Representation (ICLR. 2020Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hof- mann, K., and Whiteson, S. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. In Interna- tional Conference on Learning Representation (ICLR), 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5430 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5430</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5430</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-161b3e82567b9a9c6911171fa55f05695bf93217</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/161b3e82567b9a9c6911171fa55f05695bf93217" target="_blank">GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation.</p>
                <p><strong>Paper Abstract:</strong> There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5430.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5430.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Self-Critique</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Self-Critique Iterative Backprompting (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-then-reflect loop where GPT-4 first generates a candidate graph coloring and then the same GPT-4 instance is prompted to act as a verifier and produce feedback (edges that violate coloring constraints); that feedback is appended to the prompt and the generator is asked to produce a revised coloring, repeated up to 15 rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf GPT-4 accessed via the OpenAI API; prompted with a system role "You are a constraint satisfaction solver" and deterministic decoding (temperature=0) for the main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>LLM self-critique (iterative backprompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single LLM acts as both generator and verifier. Workflow: (1) prompt LLM to produce a coloring, (2) prompt same LLM to verify that coloring and list violating edges (if any), (3) append verification feedback to history and re-prompt generator, up to 15 iterations or until verifier says 'correct'. Verification output (natural language) is fed back unchanged regardless of correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph Coloring</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve and verify small graph coloring instances (Erdős–Rényi graphs, 10–17 vertices, ~24 edges on average) with an instruction to use at most 3 colors; also a verification subtask that requires listing violating same-color edges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Final answers: 1 out of 100 instances answered correctly (≈1%) when GPT-4 both generated and verified (i.e., iterative self-critique with GPT-4 verifier produced only a single final correct instance out of 100). If one counts any correct coloring produced at any point in the chain (even if the LLM-verifier failed to stop on it), the chain contains correct generations for roughly 40% of instances (charitable counting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct single-prompt baseline: 16% (16 out of 100 instances correct).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No empirical improvement in final answers when the same LLM self-verifies; in fact, the iterative self-critique setup performed much worse (1% final correctness) than the direct single-answer baseline (16%). The paper also reports that while correct colorings were sometimes generated at intermediate iterations, the LLM-as-verifier frequently failed to recognize them and continued, so any apparent gains vanish in final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Critical failures: GPT-4 is poor at verification in this domain — it hallucinates violating edges, often refuses to agree a coloring is correct, and produces spurious feedback that drives the generator away from previously correct answers. Verification behavior: out of 100 optimal colorings, GPT-4 marked only 2 as correct; across 500 colorings (118 correct) it claimed 30 were correct and only 5 of those were actually correct. The self-verifier frequently hallucinates edges/vertex-color relations, lacks a reliable stopping condition, and blindly provides false error lists that the generator dutifully 'fixes' locally without achieving global correctness. Thus iterative self-critique harms final answer quality in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5430.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5430.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative + External Verifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Backprompting with a Sound External Verifier (Pass/Fail / First / Full)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-and-check loop where GPT-4 generates colorings and an external, provably-correct algorithmic verifier checks them and returns either binary feedback, the first violating edge, or the full list of violating edges; feedback is appended and GPT-4 is re-prompted up to 15 rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (generator) + algorithmic external verifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generator: GPT-4 as above. External verifier: a deterministic, correct program that checks colorings by iterating edges and reporting violating same-color edge pairs (guaranteed correct). Generator runs with temperature=0 for iterative experiments; other experiments vary temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative backprompting with an external sound verifier (pass/fail, first-error, full-error)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Workflow: (1) GPT-4 generates a candidate coloring; (2) the external verifier checks it; (3) verifier returns one of three types of feedback: (a) Pass/Fail (binary), (b) First violating edge only, or (c) Full list of violating edges; (4) feedback appended to history and GPT-4 re-generates; repeat up to 15 iterations or until verifier sees a correct coloring.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph Coloring</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same graph coloring dataset and verification task as above (10–17 nodes, ~24 edges, 3-color constraint).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Iterative with sound external verifier improved final correctness from 16% (single-shot) to nearly 40% (≈40%) across feedback modes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct single-prompt baseline: 16% (single-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: iterative runs with the external verifier reached ~40% correctness, a substantial increase over the 16% single-shot baseline. However, the improvement was nearly identical across all three feedback styles (pass/fail, first error, full errors), and a 'try again' (binary) signal performed almost as well as full error lists. Additionally, generating multiple independent responses and post-filtering with the same external verifier (top-n) achieved similar gains, suggesting the improvement is due largely to having more independent attempts rather than doing substantive reasoning from the verifier's content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite aggregate improvement, the content of the verifier's feedback made little difference — 'try again' performed almost as well as full error lists. Local corrections are possible: when specific edges were pointed out, the LLM tended to fix those local errors (Table 3: e.g., 'First' feedback fixed ~94% of mentioned incorrect edges); however, high local correction rates did not translate into much larger global success. Thus the iterative loop with a sound verifier appears to succeed primarily because it provides more chances or because the correct coloring exists in the generator's top-k outputs, not because the model is truly learning or reasoning from the feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5430.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5430.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-k Multiple Generations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-n Independent Generations with External Verification (non-iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-iterative alternative: request multiple independent completions from GPT-4 (varying temperature and n), then use a sound external verifier to pick any correct coloring from the set of completions (no backprompting/back-and-forth).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 queried for multiple independent completions; temperature settings tested included 0.5, 1.0, and 1.5 for n=5 and temperature=1.0 for n=15.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Top-k multiple completions with post-hoc verification</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate n independent candidate colorings in one prompt call (or by requesting n completions) with varied temperature; apply an external sound verifier to the list and accept any correct coloring if present. This is not iterative backprompting; no feedback is fed back into the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph Coloring</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same graph coloring verification task; experiments included n=5 (various temperatures) and n=15 (temperature=1.0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>n=5 gave partial gains but was insufficient; n=15 with temperature=1.0 reached ~40% correctness, comparable to iterative external-verifier results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-shot baseline: 16% correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Empirical: top-15 independent generations (t=1.0) produced a final success rate (~40%) comparable to the iterative external-verifier pipeline. This supports the paper's conclusion that the gains attributed to iterative prompting are often due to having multiple attempts (top-k) where a correct solution happens to appear in one of the completions and is then selected by a verifier, rather than from any model-internal self-reflective improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires an external correct verifier to pick correct outputs from the set; offers no mechanism to improve the generator's internal reasoning or verification capability. Does not address the underlying verification/hallicunation problems of the LLM; success depends on the correct solution being present in the sampled outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5430.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5430.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) that proposes iterative self-reflection for language agents—agents use verbal reflection and reinforcement-like updates across trials to improve performance on complex tasks over a handful of trials; cited by this paper as an example of claims that LLM self-reflection can aid learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (self-reflection across trials)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Described in cited work as iterative verbal self-reflection and replay across trials to improve agent performance; the present paper cites it as an example of claims about emergent self-reflection in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work as claiming empirical usefulness for learning complex tasks over multiple trials; this paper references such claims but empirically questions them in the graph coloring domain.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; the authors use Reflexion as an example of prior claims and argue that such claims should be tested domain-by-domain since iterative schemes can fail when the model cannot reliably verify correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5430.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5430.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that presents an iterative self-feedback refinement procedure for LLMs and reports improvements; cited here as part of the literature claiming benefits from self-reflection/self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Described in cited work as an iterative pipeline where the model produces answers, critiques them, and refines its outputs across iterations; cited in this paper as part of the body of work advocating self-reflection/self-critique.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work; the present paper does not reproduce those methods exactly but evaluates analogous iterative methods and finds that self-critique can fail catastrophically when the model cannot verify its own answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not directly evaluated in this paper; the authors caution that iterative self-refinement claims should be validated because verification failure and hallucination can negate benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>REACT: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 1)</em></li>
                <li>Large language models are better reasoners with self-verification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5430",
    "paper_id": "paper-161b3e82567b9a9c6911171fa55f05695bf93217",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "LLM Self-Critique",
            "name_full": "LLM Self-Critique Iterative Backprompting (GPT-4)",
            "brief_description": "An iterative generate-then-reflect loop where GPT-4 first generates a candidate graph coloring and then the same GPT-4 instance is prompted to act as a verifier and produce feedback (edges that violate coloring constraints); that feedback is appended to the prompt and the generator is asked to produce a revised coloring, repeated up to 15 rounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Off-the-shelf GPT-4 accessed via the OpenAI API; prompted with a system role \"You are a constraint satisfaction solver\" and deterministic decoding (temperature=0) for the main experiments.",
            "reflection_method_name": "LLM self-critique (iterative backprompting)",
            "reflection_method_description": "Single LLM acts as both generator and verifier. Workflow: (1) prompt LLM to produce a coloring, (2) prompt same LLM to verify that coloring and list violating edges (if any), (3) append verification feedback to history and re-prompt generator, up to 15 iterations or until verifier says 'correct'. Verification output (natural language) is fed back unchanged regardless of correctness.",
            "num_iterations": 15,
            "task_name": "Graph Coloring",
            "task_description": "Solve and verify small graph coloring instances (Erdős–Rényi graphs, 10–17 vertices, ~24 edges on average) with an instruction to use at most 3 colors; also a verification subtask that requires listing violating same-color edges.",
            "performance_with_reflection": "Final answers: 1 out of 100 instances answered correctly (≈1%) when GPT-4 both generated and verified (i.e., iterative self-critique with GPT-4 verifier produced only a single final correct instance out of 100). If one counts any correct coloring produced at any point in the chain (even if the LLM-verifier failed to stop on it), the chain contains correct generations for roughly 40% of instances (charitable counting).",
            "performance_without_reflection": "Direct single-prompt baseline: 16% (16 out of 100 instances correct).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No empirical improvement in final answers when the same LLM self-verifies; in fact, the iterative self-critique setup performed much worse (1% final correctness) than the direct single-answer baseline (16%). The paper also reports that while correct colorings were sometimes generated at intermediate iterations, the LLM-as-verifier frequently failed to recognize them and continued, so any apparent gains vanish in final outputs.",
            "limitations_or_failure_cases": "Critical failures: GPT-4 is poor at verification in this domain — it hallucinates violating edges, often refuses to agree a coloring is correct, and produces spurious feedback that drives the generator away from previously correct answers. Verification behavior: out of 100 optimal colorings, GPT-4 marked only 2 as correct; across 500 colorings (118 correct) it claimed 30 were correct and only 5 of those were actually correct. The self-verifier frequently hallucinates edges/vertex-color relations, lacks a reliable stopping condition, and blindly provides false error lists that the generator dutifully 'fixes' locally without achieving global correctness. Thus iterative self-critique harms final answer quality in this task.",
            "uuid": "e5430.0",
            "source_info": {
                "paper_title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Iterative + External Verifier",
            "name_full": "Iterative Backprompting with a Sound External Verifier (Pass/Fail / First / Full)",
            "brief_description": "An iterative generate-and-check loop where GPT-4 generates colorings and an external, provably-correct algorithmic verifier checks them and returns either binary feedback, the first violating edge, or the full list of violating edges; feedback is appended and GPT-4 is re-prompted up to 15 rounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (generator) + algorithmic external verifier",
            "model_description": "Generator: GPT-4 as above. External verifier: a deterministic, correct program that checks colorings by iterating edges and reporting violating same-color edge pairs (guaranteed correct). Generator runs with temperature=0 for iterative experiments; other experiments vary temperature.",
            "reflection_method_name": "Iterative backprompting with an external sound verifier (pass/fail, first-error, full-error)",
            "reflection_method_description": "Workflow: (1) GPT-4 generates a candidate coloring; (2) the external verifier checks it; (3) verifier returns one of three types of feedback: (a) Pass/Fail (binary), (b) First violating edge only, or (c) Full list of violating edges; (4) feedback appended to history and GPT-4 re-generates; repeat up to 15 iterations or until verifier sees a correct coloring.",
            "num_iterations": 15,
            "task_name": "Graph Coloring",
            "task_description": "Same graph coloring dataset and verification task as above (10–17 nodes, ~24 edges, 3-color constraint).",
            "performance_with_reflection": "Iterative with sound external verifier improved final correctness from 16% (single-shot) to nearly 40% (≈40%) across feedback modes.",
            "performance_without_reflection": "Direct single-prompt baseline: 16% (single-shot).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: iterative runs with the external verifier reached ~40% correctness, a substantial increase over the 16% single-shot baseline. However, the improvement was nearly identical across all three feedback styles (pass/fail, first error, full errors), and a 'try again' (binary) signal performed almost as well as full error lists. Additionally, generating multiple independent responses and post-filtering with the same external verifier (top-n) achieved similar gains, suggesting the improvement is due largely to having more independent attempts rather than doing substantive reasoning from the verifier's content.",
            "limitations_or_failure_cases": "Despite aggregate improvement, the content of the verifier's feedback made little difference — 'try again' performed almost as well as full error lists. Local corrections are possible: when specific edges were pointed out, the LLM tended to fix those local errors (Table 3: e.g., 'First' feedback fixed ~94% of mentioned incorrect edges); however, high local correction rates did not translate into much larger global success. Thus the iterative loop with a sound verifier appears to succeed primarily because it provides more chances or because the correct coloring exists in the generator's top-k outputs, not because the model is truly learning or reasoning from the feedback.",
            "uuid": "e5430.1",
            "source_info": {
                "paper_title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Top-k Multiple Generations",
            "name_full": "Top-n Independent Generations with External Verification (non-iterative)",
            "brief_description": "Non-iterative alternative: request multiple independent completions from GPT-4 (varying temperature and n), then use a sound external verifier to pick any correct coloring from the set of completions (no backprompting/back-and-forth).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 queried for multiple independent completions; temperature settings tested included 0.5, 1.0, and 1.5 for n=5 and temperature=1.0 for n=15.",
            "reflection_method_name": "Top-k multiple completions with post-hoc verification",
            "reflection_method_description": "Generate n independent candidate colorings in one prompt call (or by requesting n completions) with varied temperature; apply an external sound verifier to the list and accept any correct coloring if present. This is not iterative backprompting; no feedback is fed back into the generator.",
            "num_iterations": null,
            "task_name": "Graph Coloring",
            "task_description": "Same graph coloring verification task; experiments included n=5 (various temperatures) and n=15 (temperature=1.0).",
            "performance_with_reflection": "n=5 gave partial gains but was insufficient; n=15 with temperature=1.0 reached ~40% correctness, comparable to iterative external-verifier results.",
            "performance_without_reflection": "Single-shot baseline: 16% correctness.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Empirical: top-15 independent generations (t=1.0) produced a final success rate (~40%) comparable to the iterative external-verifier pipeline. This supports the paper's conclusion that the gains attributed to iterative prompting are often due to having multiple attempts (top-k) where a correct solution happens to appear in one of the completions and is then selected by a verifier, rather than from any model-internal self-reflective improvement.",
            "limitations_or_failure_cases": "Requires an external correct verifier to pick correct outputs from the set; offers no mechanism to improve the generator's internal reasoning or verification capability. Does not address the underlying verification/hallicunation problems of the LLM; success depends on the correct solution being present in the sampled outputs.",
            "uuid": "e5430.2",
            "source_info": {
                "paper_title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reflexion (mentioned)",
            "name_full": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "brief_description": "Prior work (cited) that proposes iterative self-reflection for language agents—agents use verbal reflection and reinforcement-like updates across trials to improve performance on complex tasks over a handful of trials; cited by this paper as an example of claims that LLM self-reflection can aid learning.",
            "citation_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Reflexion (self-reflection across trials)",
            "reflection_method_description": "Described in cited work as iterative verbal self-reflection and replay across trials to improve agent performance; the present paper cites it as an example of claims about emergent self-reflection in LLMs.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Mentioned in related work as claiming empirical usefulness for learning complex tasks over multiple trials; this paper references such claims but empirically questions them in the graph coloring domain.",
            "limitations_or_failure_cases": "Not evaluated in this paper; the authors use Reflexion as an example of prior claims and argue that such claims should be tested domain-by-domain since iterative schemes can fail when the model cannot reliably verify correctness.",
            "uuid": "e5430.3",
            "source_info": {
                "paper_title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Refine (mentioned)",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "Prior work that presents an iterative self-feedback refinement procedure for LLMs and reports improvements; cited here as part of the literature claiming benefits from self-reflection/self-feedback.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-refine (iterative self-feedback)",
            "reflection_method_description": "Described in cited work as an iterative pipeline where the model produces answers, critiques them, and refines its outputs across iterations; cited in this paper as part of the body of work advocating self-reflection/self-critique.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Mentioned in related work; the present paper does not reproduce those methods exactly but evaluates analogous iterative methods and finds that self-critique can fail catastrophically when the model cannot verify its own answers.",
            "limitations_or_failure_cases": "Not directly evaluated in this paper; the authors caution that iterative self-refinement claims should be validated because verification failure and hallucination can negate benefits.",
            "uuid": "e5430.4",
            "source_info": {
                "paper_title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "REACT: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 1
        },
        {
            "paper_title": "Large language models are better reasoners with self-verification",
            "rating": 1
        }
    ],
    "cost": 0.0139215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems</h1>
<p>Kaya Stechly<em> ${ }^{</em>}$ Matthew Marquez<em> ${ }^{</em>}$ Subbarao Kambhampati*</p>
<h4>Abstract</h4>
<p>There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples-ranging from multiplication to simple planning, there is still the wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation-a rather classical argument from computational complexity, that should be irrelevant to LLMs to the extent what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings-both in direct and iterative modes. In iterative modes, we experiment both with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution-and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms-whether by LLMs or external solvers-seems largely irrelevant to the performance of iterative prompting. We show that the observed effectiveness of LLMs in iterative settings is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs), essentially n-gram models on steroids which have been trained on web-scale language corpus, have caught the imagination of the AI research community with linguistic behaviors that no one expected text completion systems to possess. Their seeming versatility has lead many researchers to wonder whether they can also do well on reasoning tasks typically associated with system 2 competency. Initial excitement based on anecdotal performance of LLMs on reasoning tasks has dissipated to some extent by the recent spate of studies questioning the robustness of such behaviors-be it planning [17, 8], simple arithmetic and logic [5], or general mathematical and abstract benchmark[14, 6]. There still exists considerable optimism that even if LLMs can't generate correct solutions in one go, their accuracy improves in a iterative prompting regime, where LLMs will be able to "self-critique" their candidate solutions and refine them to the point of correctness $[20,19,15,18,7]$. This belief seem to rest largely on the assumption that verification of correctness</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>should be easier than generation for many reasoning problems-a rather classical argument from computational complexity. There are grounds to be skeptical of this assumption as complexity of the reasoning task should be irrelevant to LLM performance if what they are doing is approximate retrieval.</p>
<p>In this paper, we set out to systematically investigate effectiveness of iterative prompting in the context of Graph Coloring, a canonical NP-complete reasoning problem. We chose graph coloring as it is representative both of standard classes of reasoning problems studied in AI-propositional satisfiability and constraint satisfaction-and practical problems like scheduling and allocation. Our methodology involves a principled empirical study of the performance of GPT4 on two tasks: solving a large suite of random graph coloring instances and, separately, verifying the correctness of the candidate colorings-both in direct and iterative modes. In iterative modes, we experiment both with an LLM critiquing LLM-produced solutions and an external, guaranteed correct reasoner verifying solutions. In both cases, we analyze whether the content of criticisms actually affects bottom line performance.</p>
<p>Our results indicate that in direct mode, LLMs are, perhaps not surprisingly, pretty bad at solving graph coloring instances. More interestingly, as we suspected, they are no better at verifying solutions. In iterative modes, given the inability of LLMs to verify solutions, it should come as no surprise that our experiments show that the strategy of LLMs self-critiquing their solutions does not improve over the baseline. It is actually worse because the system can't recognize a correct coloring and thus merrily passes over fortuitously correct colorings it has generated, ending up with a wrong one!</p>
<p>We next experimented with an iterative strategy where an external coloring verifier does the backprompting. Here we looked at three different types of back prompting: (1) the verifier just asks the LLM to try again when the coloring is incorrect, (2) the verifier gives a backprompt showing the first violated constraint in the current candidate coloring and (3) the verifier sends a backprompt showing all violated coloring constraints. We note that these three strategies do lead to modest improvements in the bottom-line performance-improving from about $16 \%$ to nearly $40 \%$. The surprising finding however is that the minimal information "try again" feedback is nearly as effective as the ones with meaningful backprompts. This lead us to consider whether the improvement is due to the type of backprompting (as authors who advocate these types of iterative approaches [20, 19, 15, 10, 4, 11] seem to assume) or because the answer just happens to be in the top-K completions (even if the LLM is itself not cognizant of it). To check this, we experiment with a version of the direct mode where we query the LLM so that it generates more than one potential solution, and have the external verifier pick out any correct solution in the list. The results show that top-k correctness with an external, guaranteed correct verifier is pretty competitive with any iterative backprompting.</p>
<p>Our investigation thus raises significant grounds to be skeptical about the effectiveness of iterative prompting techniques in general, and those relying on the self-critiquing capabilities of LLMs in particular. In the reminder of the paper, we discuss related work, present our experimental methodology, and then detail the results of our experiments.</p>
<h1>2 Related Work</h1>
<p>As mentioned in the introduction, there has been a large recent body of work investigating the reasoning capabilities of LLMs [15, 19, 9]. The studies span different types of reasoning problemsplanning [17], logic and arithmetic [5], or 24 puzzle [19]. The conclusions have also been divergentwith some studies highlighting the limitations of LLMs in reasoning[12, 2], and others arguing that iterative prompting of LLMs can improve their ability to reason. For example, [15] states we explore this emergent property of self-reflection in LLMs and empirically show that self-reflection is extremely useful to learn complex tasks over a handful of trials. This paper focuses on understanding these sorts of claims-and especially of the effectiveness of iterative prompting. The problem we chose-graph coloring-is a canonical NP-complete reasoning problem well studied in AI and computer science [13]. It has rich connections to propositional logical reasoning-specifically satisfiability, constraint satisfaction problems, and is also related to practical problems including resource allocation and scheduling.</p>
<h1>3 Methodology</h1>
<h3>3.1 The Graph Coloring Problem</h3>
<p>Because we are interested in LLMs' self-critique capabilities, we chose Graph Coloring, a reasoning domain which is human readable, provides relatively short description and critique lengths, and, most importantly, is very easy to verify and provide feedback for. Though it is difficult to be certain, we also believe that this domain is diverse enough even at low node and edge counts that the instances we examine are very unlikely to be found in the LLM's training data, thus minimizing the risk of model contamination and memorization.</p>
<p>Graph coloring is a a canonical NP-complete reasoning problem that is related to both propositional satisfiability as well as practical problems like scheduling and allocation. It is broad enough to give insights into reasoning more generally, and simple enough to be specified and evaluated by a human or basic pattern matching.
Common graph coloring benchmark sets consist of the sorts of problems that exact solvers struggle on, boasting triple or quadruple digit numbers of nodes and edges[16]. Current language models don't have sufficiently large context windows to process these, and-as we'll see later-are unlikely to do well on graphs with over twenty nodes.
Therefore, we built our own dataset. We use GrinPy ${ }^{2}$ to handle common graph operations. Each graph is constructed using the Erdős-Rényi method ( $p=0.4$ ), modified so that any generation that fails to be planar or happens to be isomorphic to a previously generated one is retried. Once a successful candidate is found, it is compiled into the standard DIMACS format[1], appended with a comment containing its precalculated chromatic number.</p>
<p>For the following experiments, we generated 100 instances with an average of 24 edges each spread across node counts from 10 to 17 -a distribution chosen because empirical probing revealed it to be an area with volatile enough performance to be interesting. An example of one of the graphs we used is shown in Figure 1, together with the LLM's first response, the backprompt on that response, and the final correct coloring.</p>
<h3>3.2 Architecture for Iterative Backprompting</h3>
<p>All code and results will be made public.</p>
<h2>Prompt Generator:</h2>
<p>The generator takes a DIMACS instance and constructs a natural language prompt by translating each edge into a sentence and then wrapping the whole in a common set of instructions. We deliberately minimize differences between instances' prompts to reduce how much problem-specific information we leak to the LLM. Examples of each prompt type can be found in the appendix.</p>
<h2>Large Language Model:</h2>
<p>Off the shelf, this system allows for the use of any LLM accessible through the OpenAI API: the user need only pass the model name through the appropriate flag at runtime. The present work focuses on GPT-4, the current state of the art, because of recent claims about its "emergent" reasoning capabilities[3].
We provide a system role of "You are a constraint satisfaction solver that solves various CSP problems." and set the temperature to 0 , thus ensuring output is mostly deterministic.</p>
<h2>Extensibility:</h2>
<p>This architecture easily extends to other domains of constraint satisfaction problem solving. In the public repository, we provide a way to add a new domain description by adding just one file to the project in plug-and-play fashion.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of backprompt architecture for a single instance. Clouds provide an illustrated interpretation of the current state of the problem at different points in the system. Red diamonds indicate progression of a single problem: a planar graph is first passed to GPT-4 acting as a generator (1), which returns a proposed coloring (2). GPT-4 will then be used as a verifier to determine whether the coloring is correct. When not correct, GPT-4 provides feedback, along with previous history, through a backprompt (3) that will be used in the next generation request (4). Each new coloring will be evaluated by the GPT-4 working as a verifier. If GPT-4 determines the coloring to be correct or 15 iterations have passed, it approves the final answer, where it is then evaluated against a sound verifier.</p>
<h1>3.3 Backprompt Generation</h1>
<p>In verification mode, the LLM receives a different sort of prompt. Apart from standard instructions, it contains only the graph description and the proposed coloring. It is tasked with verifying correctness, optimality, and whether every vertex has been given an assignment. If the coloring is incorrect, it must reply with a set of contradicting edges.</p>
<p>As a comparison point, we also construct a guaranteed correct verifier, with the ability to list every single contradicting edge. Since LLM responses are also in natural language, we first translate them into a format amenable to analysis. To make this process more consistent, we design our initial prompt to describe an exact output format to which the model conforms. Then, the response is evaluated for correctness.</p>
<p>In both cases, if the verifier says the answer is correct, we end there. If it has been more than 15 rounds ( 16 total queries), we give up. Otherwise, a backprompt is created, wrapped in standard instructions, appended to the previous message history, and sent back to the model as a new prompt.</p>
<p>In this domain a valid piece of error feedback consists of a pair of vertices which were given the same color but share an edge. To construct a backprompt, we have to decide exactly how much feedback to give. We examine five cases:</p>
<ol>
<li>None: A single iteration baseline. No backprompting.</li>
<li>Pass/Fail: The only feedback given is that the answer was incorrect.</li>
<li>First: Only the first error encountered is returned.</li>
<li>Full: A comprehensive list of errors.</li>
<li>LLM: Feedback is provided by the language model through a separate prompt, given in the appendix. We pass any and all response back to the generator, regardless of its validity or correctness.</li>
</ol>
<p>By comparing results under these regimes, we can deduce how much of the given information the LLM is actually using, versus how much of the performance increase stems from merely getting</p>
<p>more tries. We also compare these cases to four further cases: higher temperature, single iteration queries which ask for multiple answers. These do not involve any backprompting, reprompting, or giving any information past the original prompt to the LLM.</p>
<p>6-8. Top 5: With temperatures 0.5, 1, and 1.5, query the LLM for $n=5$ responses.
9. Top 15: With a temperature of 1 , query the LLM for $n=15$ responses.</p>
<h1>3.4 Verification</h1>
<p>In order to gain more insight into their LLM verification, we examine how well they find errors in proposed colorings. Intuitively, these should be very easy to identify: if the two vertices making up an edge share a color, immediately return that edge. Algorithmically, all this requires is looping over edges and comparing each vertex's color to that of its partner.
We use the same pipeline for this analysis, but construct a new domain we call color_verification. The LLM is prompted to check correctness, optimality, and if every vertex has been assigned in the coloring. If the coloring is incorrect, it is instructed to list errors in the coloring, that is, if two connected nodes share a color, it is to return the edge to represent the error. No backprompts are given. We use the same graph instances from before, but generate four kinds of colorings to test the model on:</p>
<ol>
<li>Correct: Optimal colorings with no errors, generated via iterated, randomized greedy algorithm (with a precomputed chromatic number to ensure optimality)</li>
<li>Ablated: The previous set of colorings, each with a random node changed to one of its neighbor's colors</li>
<li>Non-optimal: The correct set, with a randomly chosen color partially recolored to a new shade</li>
<li>Random: Completely randomly assigned colors, with the number of different colors equal to the graph's chromatic number</li>
<li>LLM: Colorings randomly selected from the LLM-generated outputs of the previous experiment</li>
</ol>
<h2>4 Results</h2>
<h3>4.1 Backprompting as Self-Critique</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Prompting the LLM, evaluating the answer, and moving on to the next instance without any backprompts whatsoever gives a baseline score of $16 \%$. When we run the same instances, but this time backprompt the LLM with feedback generated by the same language model acting as a verifier, performance plummets-only a single instance of the 100 was answered correctly.</p>
<p>Table 1: Summary of Backprompt Techniques</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Strategy</th>
<th style="text-align: center;">Example Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Direct LLM</td>
<td style="text-align: center;">Color the following graph, described as a set of edges, such that no two vertices on the same edge share a color. You may use at most 3 colors. Vertex 0 is connected to vertex 2...</td>
</tr>
<tr>
<td style="text-align: center;">Iterative: LLM Self-Critique</td>
<td style="text-align: center;">This is incorrect. Feedback: <br> Vertices 0 and 11 share an edge and are both colored with Color 1. <br> Vertices 5 and 11 [...] Using this feedback...</td>
</tr>
<tr>
<td style="text-align: center;">Iterative (with external Verifier): Pass/Fail</td>
<td style="text-align: center;">This is not correct. Using the previously provided graph...</td>
</tr>
<tr>
<td style="text-align: center;">Iterative (with external Verifier): First error</td>
<td style="text-align: center;">This is not correct. Vertex 1 and vertex 7 were both colored Color 1 despite being connected by an edge...</td>
</tr>
<tr>
<td style="text-align: center;">Iterative (with external Verifier): All errors</td>
<td style="text-align: center;">This is not correct. Vertex 1 and vertex 7 were both colored Color 1 despite being connected by an edge. Vertex 2 and vertex 4 were both colored Color 0 despite...</td>
</tr>
</tbody>
</table>
<p>The problem is caused by the lack of an accurate stopping condition. If the system ever outputs a correct coloring during a backprompting session, we expect a verifier to stop it. However, in the self-verification case, the LLM doing the verification can fail to notice success and instead produce spurious feedback. This is exactly what happens.
At some point in the backprompts of 40 instances, the generating model returned an optimal coloring. In none of those instances did the verifying GPT realize this. In 39 cases, it hallucinated pairs of vertices that it claimed were adjacent and same-colored. In the one case marked correct, the coloring was provided after the final backprompt, and so became the model's final answer by virtue of timeout. This also points to the model's hesitancy to agree that a coloring is correct. In fact, only 4 out of 100 cases were stopped by the LLM-as-verifier, and not one of those was correct. Whether bad feedback itself is worsening the results, or it's merely the case that correct responses tend to be earlier in the backprompt sequence-optimistically viewed as a result of being higher probability completions which are ruined by a self-destructive thinking process-is unclear. Our results here and in the next few subsections are so far conflicting.
The results when backprompted with a sound verifier seem, at first, a lot more promising. The number of instances correctly answered nears $40 \%$, but if this is supposed to indicate that GPT-4 is listening to, improving with, and reasoning from feedback, then we should expect more informative and accurate backprompts to yield better results. However, in this domain, the raw scores (see Figure 2) don't bear this out. When run with a sound verifier, the differences between binary feedback, a single error, or the full suite of mistakes are insignificant.
We can relax our analysis of the LLM self-critique case by labeling an instance as correct if at any point during the backprompt chain, the LLM generated a correct coloring. This is equivalent to rerunning the experiment with a combined feedback system: the sound verifier is in charge of stopping while allowing the LLM to write all the (still potentially spurious) feedback. Given this modification, it scores a comparable $40 \%$. Using this charitable number, all four types of backprompting give roughly similar results.
It seems then that feedback or lack thereof is thus less important to the improvement of the score than number of iterations: if the model has fifteen chances to generate a correct answer, it is much more likely to succeed. We test this idea by querying the same set of 100 instances, but now allowing for higher temperatures and receiving multiple, separate, non-interacting responses. The results make up</p>
<p>Table 2: Distribution of hallucinations during verification task. This table counts the number of instances that featured each type of hallucination and compares it to the total number of erroneous edges encountered across all coloring instances in each subset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Hallucinations</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Coloring</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Correct</td>
<td style="text-align: left;">Vertex</td>
<td style="text-align: left;">Edge</td>
<td style="text-align: left;">Both</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Errors</td>
<td style="text-align: left;">Correct</td>
</tr>
<tr>
<td style="text-align: left;">Ablated</td>
<td style="text-align: left;">29</td>
<td style="text-align: left;">72</td>
<td style="text-align: left;">7</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">100</td>
</tr>
<tr>
<td style="text-align: left;">Non-optimal</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">52</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">187</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: left;">18</td>
<td style="text-align: left;">65</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">LLM</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">26</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">66</td>
<td style="text-align: left;">736</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">26</td>
<td style="text-align: left;">41</td>
<td style="text-align: left;">6</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">240</td>
<td style="text-align: left;">18</td>
</tr>
</tbody>
</table>
<p>the rest of Figure 3. With $\mathrm{n}=5$, it's close, not quite there, but with $\mathrm{n}=15$ ( $\mathrm{t}=1.0$ ), the performance is comparable to backprompting, achieving a score of $40 \%$.
In other words: blindfolded guessing does just as well as careful, crafted feedback.
The rest of our analysis examines where the system is going wrong. We will attempt to answer two questions: to what extent is the LLM capable of determining if a solution is right or wrong? How, if at all, does the LLM respond to feedback?</p>
<h1>4.2 Verification by LLM</h1>
<p>We test GPT-4's ability to verify colorings on the same instances, but we generate five different kinds of colorings for each. What is immediately obvious is a result that exactly agrees with the LLM self-verification results above: the model is unwilling to mark almost any answer as correct. Out of 100 optimal colorings, it only agreed that 2 were correct. Expanding to the entire set of 500 colorings, of which 118 of them are correct, it only claimed 30 of them as correct. Of those, it was right 5 times. This isn't because of any special property of correctness-the same holds true in the non-optimal coloring set, in which it only marked $10 \%$ of instances as non-optimal.
Overall, this pattern holds. Fewer than ten percent of cases resulted in a "correct", "non-optimal", or "missing assignment" response from the LLM. Among those, the behavior looks somewhat random. In around a quarter of instances, it responds with a "this is incorrect" verification where the explanation matches reality, and it only manages this by naming no more than a single edge, which minimizes the chance of misstating something.
Table 2 summarizes the results. Note that, proportionally, hallucinations decrease when the error rate of the domain increases. That is to say, when there are more incorrect edges, the model is more likely to point to one of them. Intuitively, this makes sense: it's easier to guess one edge which is wrong when half of all the edges are miscolored, as is the case on average among randomly colored graphs.
Edge hallucinations are more common than vertex. Essentially, typical behavior is to pick two vertices that are the same color in the coloring, but which aren't associated by an edge in the graph description, and claim that they are connected and thus illegally colored. Vertex color hallucination is when the reverse happens: instead of ascribing an edge to same-color nodes, the colorings of two connected vertices are misstated. The overlap between the two cases, where a non-existent edge is declared to be violated by non-existent colorings is much rarer than either. Note that it never hallucinates new vertex names, only that vertices which are in graph have colors differing from reality.
Even rarer cases did spring up in the response data. At times the model lost track of the question being asked and reversed it, explicitly claiming that two same-colored vertices violate the conditions because they aren't connected; or it began to contradict itself mid-deduction, making multiple claims about a vertex's color. We relegate these examples to the appendix.
Our overall conclusion is that, despite the common-sense nature of this domain, the LLM's verification powers are surprisingly weak.</p>
<h1>4.3 Inside the Backprompt Chain</h1>
<p>To figure out what information it is or isn't using, we examine the evolution of GPT-4's responses within a backprompt chain. We compare three types of informative backprompting: providing the first wrong edge, listing all wrong edges, and choosing a random correct edge to claim is incorrect. The first two cases were described in more detail above. The final one, the so-called "evil" case is new, and provided as a way to check how blindly the system follows corrective advice.
Given a backprompt, we examine the response to it. We only look at the rates of local error correction. Given a backprompt, we consider it "listened to" if the edges it listed as incorrect were changed in the response so that each vertex is a different color from the other. We summarize the results by averaging over backprompts. The results are summarized in Table 3.</p>
<p>Table 3: Local error correction rates per backprompt information type. Full (any) gives credit if any edge mentioned in the backprompt was corrected. Full (all) gives each backprompt response a percentage score calculated from the number of mentioned edges which were corrected divided by the total number of edges mentioned in the backprompt. Evil backprompting claims a random correct edge is incorrect.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"># Backprompts</th>
<th style="text-align: left;"># Incorrect Edges Fixed</th>
<th style="text-align: left;">\% Incorrect Edges Fixed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">First</td>
<td style="text-align: left;">1066</td>
<td style="text-align: left;">1004</td>
<td style="text-align: left;">$94 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Full (any)</td>
<td style="text-align: left;">1102</td>
<td style="text-align: left;">1077</td>
<td style="text-align: left;">$98 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Full (all)</td>
<td style="text-align: left;">1102</td>
<td style="text-align: left;">2870</td>
<td style="text-align: left;">$84 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Evil</td>
<td style="text-align: left;">1083</td>
<td style="text-align: left;">1017</td>
<td style="text-align: left;">$94 \%$</td>
</tr>
</tbody>
</table>
<p>Even though performance was unaffected, GPT did correct most errors that were pointed out. However, it didn't discriminate between real errors or the evil case's false ones, blindly applying local "fixes" without regard for overall correctness.</p>
<h2>5 Conclusion</h2>
<p>In this work, we have set out to investigate the effectiveness of iterative prompting strategies in improving the accuracy of LLMs on reasoning problems. We were motivated, in particular, by claims in prior work that even when LLMs produce incorrect answers at first, they are good at self-critiquing and improving their answers. Our results on graph coloring call these claims into question. They show that LLMs are in fact very poor at verifying solutions (in our case, colorings), something that is critical for self-critiquing. Not surprisingly, iterative framework with LLMs self-critiquing does even worse than LLMs directly generating a single answer. We do show that iterative prompting can help when there is an external provably correct verifier in the loop. Even here, we found that the actual content of iterative back prompts is not important, and that the improvements seen can also be obtained by just having the LLM produce multiple answers, and letting verifier check and pick any correct answer that was fortuitously generated. Our results thus raise legitimate questions about claims of the effectiveness of iterative prompting, adding further fuel to the skepticism surrounding the reasoning capabilities of LLMs.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>Acknowledgements</p>
<h2>References</h2>
<p>[1] DIMACS Implementation Challenges. Archive available at http://archive.dimacs. rutgers.edu/Challenges/.
[2] Konstantine Arkoudas. Gpt-4 can’t reason. Preprints, August 2023.
[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.</p>
<p>[4] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug, 2023.
[5] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and Fate: Limits of Transformers on Compositionality. 2023. Publisher: arXiv Version Number: 2.
[6] Gaël Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not abstract reasoners, 2023.
[7] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models, 2022.
[8] Subbarao Kambhampati. Can LLMs Really Reason and Plan?, 2023. Available at https: //cacm.acm.org/blogs/blog-cacm/276268-can-llms-really-reason-and-plan/ fulltext.
[9] Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. ArXiv, May 2022.
[10] Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.
[11] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.
[12] R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of autoregression: Understanding large language models through the problem they are trained to solve, 2023.
[13] Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach (4th Edition). Pearson, 2020.
[14] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, and Aran Komatsuzaki. ARB: Advanced Reasoning Benchmark for Large Language Models. 2023. Publisher: arXiv Version Number: 2.
[15] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, June 2023. arXiv:2303.11366 [cs].
[16] Michael Trick. Graph Coloring Instances. Available at https://mat.tepper.cmu.edu/ COLOR/instances.html.
[17] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large Language Models Still Can’t Plan (A Benchmark for LLMs on Planning and Reasoning about Change), apr 2023. arXiv:2206.10498 [cs].
[18] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification, 2023.
[19] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.
[20] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023.</p>
<h1>A Appendix</h1>
<h2>A. 1 Prompts</h2>
<p>All of following examples are built on the same graph instance.</p>
<h2>DIMACS Format For Graphs</h2>
<p>e 07
e 08
e 09
e 011
e 113
e 29
e 38
e 311
e 312
e 412
e 511
e 69
e 710
e 713
e 911
e 1013
e 1113
c OPTIMAL CHROMATIC NUMBER $==3$</p>
<h2>Baseline. Direct Prompt</h2>
<p>Color the following graph, described as a set of edges, such that no two vertices on the same edge $\sim$ share a color.
You may use at most 3 colors.
Vertex 0 is connected to vertex 7 .
Vertex 0 is connected to vertex 8 .
Vertex 0 is connected to vertex 9 .
Vertex 0 is connected to vertex 11 .
Vertex 1 is connected to vertex 13 .
Vertex 2 is connected to vertex 9 .
Vertex 3 is connected to vertex 8 .
Vertex 3 is connected to vertex 11 .
Vertex 3 is connected to vertex 12 .
Vertex 4 is connected to vertex 12 .
Vertex 5 is connected to vertex 11 .
Vertex 6 is connected to vertex 9 .
Vertex 7 is connected to vertex 10 .
Vertex 7 is connected to vertex 13 .
Vertex 9 is connected to vertex 11 .
Vertex 10 is connected to vertex 13 .
Vertex 11 is connected to vertex 13 .
There are a total of 14 vertices. Please label every vertex, even if it is disconnected from the $\sim$ rest of the graph. Please provide each vertex's color. Do not skip any vertices. Each color $\sim$ must be provided on a new line in the response and should be formatted as "(VERTEX NUMBER):
$\sim$ (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your response.</p>
<h2>Example LLM Response</h2>
<p>0: Color1
1: Color1
2: Color1
3: Color2
4: Color1
5: Color2
6: Color2
7: Color2
8: Color3
9: Color3
10: Color3
11: Color3
12: Color3
13: Color1</p>
<h1>Prompt To Elicit Verification</h1>
<p>The following graph, described as a set of edges, has an optimal coloring number of 3 :
Vertex 0 is connected to vertex 7 .
Vertex 0 is connected to vertex 8 .
Vertex 0 is connected to vertex 9 .
Vertex 0 is connected to vertex 11 .
Vertex 1 is connected to vertex 13 .
Vertex 2 is connected to vertex 9 .
Vertex 3 is connected to vertex 8 .
Vertex 3 is connected to vertex 11 .
Vertex 3 is connected to vertex 12 .
Vertex 4 is connected to vertex 12 .
Vertex 5 is connected to vertex 11 .
Vertex 6 is connected to vertex 9 .
Vertex 7 is connected to vertex 10 .
Vertex 7 is connected to vertex 13 .
Vertex 9 is connected to vertex 11 .
Vertex 10 is connected to vertex 13 .
Vertex 11 is connected to vertex 13 .
Please check if this coloring is correct:0: Color1
1: Color1
2: Color1
3: Color2
4: Color1
5: Color2
6: Color2
7: Color2
8: Color3
9: Color3
10: Color3
11: Color3
12: Color3
13: Color1
If it is, say 'Verifier confirmed success.' Do not provide anything else in your response. If it $\sim$ is incorrect, please point out which same-color vertices share an edge.</p>
<h2>A. 2 Backprompts</h2>
<h2>Backprompt Generated From Self-Critique</h2>
<p>This is incorrect. Feedback:
Vertices 0 and 3 share an edge and are both colored with Color1. Vertices 9 and 11 share an edge $\sim$ and are both colored with Color3.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\sim$ vertices. Each color must be provided on a new line in the response and should be formatted as $\sim$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\sim$ response.</p>
<h2>Backprompt Generated From Sound Verifier (Pass/Fail)</h2>
<p>This is not correct. Using the previously provided graph, please provide a correct coloring. $\sim$ Please provide each vertex's color. Do not skip any vertices. Each color must be provided on a $\sim$ new line in the response and should be formatted as "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your response.</p>
<h2>Backprompt Generated From Sound Verifier (First)</h2>
<p>Vertex 9 and vertex 11 were both colored Color3 despite being connected by an edge.
This is wrong. Please recolor. Please provide each vertex's color. Do not skip any vertices. Each $\sim$ color must be provided on a new line in the response and should be formatted as "(VERTEX $\sim$ NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your response.</p>
<h1>Backprompt Generated From Sound Verifier (Full)</h1>
<p>Vertex 9 and vertex 11 were both colored Color3 despite being connected by an edge. Vertex 10 and $\sim$ vertex 13 were both colored Color3 despite being connected by an edge. Vertex 11 and vertex 13 $\sim$ were both colored Color3 despite being connected by an edge.
This is wrong. Please recolor. Please provide each vertex's color. Do not skip any vertices. Each $\sim$ color must be provided on a new line in the response and should be formatted as "(VERTEX $\sim$ NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your response.</p>
<h2>A. 3 Complete Example of Iterative Prompt Sequence</h2>
<h2>Full Sequence of 15 (LLM-Generated) Backprompts</h2>
<p>Color the following graph, described as a set of edges, such that no two vertices on the same edge $\sim$ share a color.
You may use at most 3 colors.
Vertex 0 is connected to vertex 7 .
Vertex 0 is connected to vertex 8 .
Vertex 0 is connected to vertex 9 .
Vertex 0 is connected to vertex 11 .
Vertex 1 is connected to vertex 13 .
Vertex 2 is connected to vertex 9 .
Vertex 3 is connected to vertex 8 .
Vertex 3 is connected to vertex 11 .
Vertex 3 is connected to vertex 12 .
Vertex 4 is connected to vertex 12 .
Vertex 5 is connected to vertex 11 .
Vertex 6 is connected to vertex 9 .
Vertex 7 is connected to vertex 10 .
Vertex 7 is connected to vertex 13 .
Vertex 9 is connected to vertex 11 .
Vertex 10 is connected to vertex 13 .
Vertex 11 is connected to vertex 13 .
There are a total of 14 vertices. Please label every vertex, even if it is disconnected from the $\sim$ rest of the graph. Please provide each vertex's color. Do not skip any vertices. Each color $\sim$ must be provided on a new line in the response and should be formatted as "(VERTEX NUMBER): $\sim$ (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 3 share an edge and are both colored with Color1. Vertices 9 and 11 share an edge $\sim$ and are both colored with Color3.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\sim$ vertices. Each color must be provided on a new line in the response and should be formatted as $\sim$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\sim$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
</code></pre></div>

<p>13: Color3
This is incorrect. Feedback:
Vertices 2 and 3 are both colored with Color2 and share an edge. Vertices 7 and 11 are both $\hookrightarrow$ colored with Color2 and share an edge. Vertices 9 and 13 are both colored with Color3 and $\hookrightarrow$ share an edge.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as $\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 9 share an edge and are both colored with Color1. Vertices 0 and 11 share an edge $\hookrightarrow$ and are both colored with Color2. Vertices 1 and 13 share an edge and are both colored with $\hookrightarrow$ Color1. Vertices 7 and 13 share an edge and are both colored with Color3.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as $\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 5 both have Color1 and share an edge. Vertices 0 and 11 both have Color3 and share $\hookrightarrow$ an edge. Vertices 7 and 10 both have Color2 and share an edge. Vertices 11 and 13 both have $\hookrightarrow$ Color3 and share an edge.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as $\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Vertices</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">both</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">Color1</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">share</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">edge</span>.<span class="w"> </span><span class="nv">Vertices</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nv">also</span><span class="w"> </span><span class="nv">both</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">Color1</span><span class="w"> </span><span class="nv">and</span>
\<span class="nv">hookrightarrow</span><span class="w"> </span><span class="nv">share</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">edge</span>.<span class="w"> </span><span class="nv">Vertices</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="nv">also</span><span class="w"> </span><span class="nv">both</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">Color1</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">share</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">edge</span>.<span class="w"> </span><span class="nv">Vertices</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">11</span>
\<span class="nv">hookrightarrow</span><span class="w"> </span><span class="nv">both</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">Color2</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">share</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">edge</span>.<span class="w"> </span><span class="nv">Vertices</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">11</span><span class="w"> </span><span class="nv">both</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">Color2</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">share</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">edge</span>.
<span class="nv">Using</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">feedback</span>,<span class="w"> </span><span class="nv">please</span><span class="w"> </span><span class="nv">try</span><span class="w"> </span><span class="nv">again</span>.<span class="w"> </span><span class="nv">Please</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">vertex</span><span class="err">&#39;s color. Do not skip any</span>
<span class="err">\hookrightarrow vertices. Each color must be provided on a new line in the response and should be formatted as</span>
<span class="err">\hookrightarrow &quot;(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)&quot;. Please do not provide anything else in your</span>
<span class="err">\hookrightarrow response.</span>
<span class="err">0: Color1</span>
<span class="err">1: Color2</span>
<span class="err">2: Color2</span>
<span class="err">3: Color2</span>
<span class="err">4: Color2</span>
<span class="err">5: Color3</span>
<span class="err">6: Color3</span>
<span class="err">7: Color3</span>
<span class="err">8: Color2</span>
<span class="err">9: Color3</span>
<span class="err">10: Color1</span>
<span class="err">11: Color2</span>
<span class="err">12: Color3</span>
<span class="err">13: Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 8 both have Color1 and share an edge. Vertices 0 and 11 both have Color1 and share
$\hookrightarrow$ an edge. Vertices 3 and 8 both have Color2 and share an edge. Vertices 3 and 11 both have
$\hookrightarrow$ Color2 and share an edge. Vertices 7 and 13 both have Color3 and share an edge. Vertices 9 and
$\hookrightarrow 11$ both have Color3 and share an edge. Vertices 11 and 13 both have Color2 and share an edge.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any
$\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as
$\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your
$\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 11 share an edge and are both colored with Color1. Vertices 2 and 9 share an edge
$\hookrightarrow$ and are both colored with Color2. Vertices 3 and 12 share an edge and are both colored with
$\hookrightarrow$ Color3.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any
$\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as
$\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your
$\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 11 and 13 both have Color3 and share an edge.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any
$\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as
$\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your
$\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 11 share an edge and both are colored with Color1.
Using this feedback, please try again. Please provide each vertex's color. Do not skip any
$\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as
$\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your
$\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 2 and 9 share an edge and are both colored with Color2. Similarly, vertices 11 and 13
$\hookrightarrow$ share an edge and are both colored with Color2.
Using this feedback, please try again. Please provide each vertex's color. Do not skip any
$\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as
$\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your
$\hookrightarrow$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 2 and 9 share an edge and are both colored with Color2. Vertices 3 and 11 share an edge
$\hookrightarrow$ and are both colored with Color3. Vertices 7 and 13 share an edge and are both colored with
$\hookrightarrow$ Color3.
Using this feedback, please try again. Please provide each vertex's color. Do not skip any
$\hookrightarrow$ vertices. Each color must be provided on a new line in the response and should be formatted as
$\hookrightarrow$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your
$\hookrightarrow$ response.</p>
<p>0: Color1</p>
<div class="codehilite"><pre><span></span><code><span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 11 share an edge and are both colored with Color1. Vertices 3 and 8 share an edge $\sim$ and are both colored with Color3. Vertices 7 and 13 share an edge and are both colored with $\sim$ Color2.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\sim$ vertices. Each color must be provided on a new line in the response and should be formatted as $\sim$ " (VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\sim$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
</code></pre></div>

<p>This is incorrect. Feedback:
The coloring is incorrect. Vertices 0 and 11 share an edge and are both colored with Color1. $\sim$ Vertices 7 and 13 share an edge and are both colored with Color2.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\sim$ vertices. Each color must be provided on a new line in the response and should be formatted as $\sim$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\sim$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 0 and 9 share an edge and are both colored with Color1. Vertices 7 and 11 share an edge $\sim$ and are both colored with Color3. Vertices 1 and 13 share an edge and are both colored with $\sim$ Color2.</p>
<p>Using this feedback, please try again. Please provide each vertex's color. Do not skip any $\sim$ vertices. Each color must be provided on a new line in the response and should be formatted as $\sim$ "(VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your $\sim$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<p>This is incorrect. Feedback:
Vertices 3 and 9 share an edge and are both colored with Color2.
Using this feedback, please try again. Please provide each vertex's color. Do not skip any
$\sim$ vertices. Each color must be provided on a new line in the response and should be formatted as
$\sim$ " (VERTEX NUMBER): (VERTEX COLOR ASSIGNMENT)". Please do not provide anything else in your
$\sim$ response.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">0</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">1</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">2</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">3</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">4</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">5</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">6</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">7</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">8</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">9</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">10</span><span class="o">:</span><span class="w"> </span><span class="n">Color2</span>
<span class="mi">11</span><span class="o">:</span><span class="w"> </span><span class="n">Color3</span>
<span class="mi">12</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
<span class="mi">13</span><span class="o">:</span><span class="w"> </span><span class="n">Color1</span>
</code></pre></div>

<h1>A. 4 LLM as Verifier</h1>
<p>In this section, multiple equals signs in a row ("===") are dividers between separate examples, not parts of the examples.</p>
<h2>Examples of LLM Output on the Verification Task</h2>
<p>The vertices 1 and 4 are both color 0 and share an edge.
$======================$
The vertices 9 and 12 are both colored with Color 1 and they share an edge.
$======================$
Vertex 1: Color 2 is connected to Vertex 2: Color 2.
$======================$
Vertex 5 and vertex 10 are both Color 2 and share an edge.
$======================$
Same-color vertices 0 and 1 share an edge.
$======================$
Vertex 0 (Color 0) and vertex 3 (Color 0) share an edge. Vertex 1 (Color 1) and vertex 2 (Color 1)
$\sim$ share an edge. Vertex 4 (Color 1) and vertex 8 (Color 1) share an edge. Vertex 2 (Color 1) and
$\sim$ vertex 4 (Color 1) share an edge. Vertex 4 (Color 1) and vertex 11 (Color 0) share an edge.
$\sim$ Vertex 12 (Color 2) and vertex 15 (Color 2) share an edge.
The vertices 3 and 0 both have Color 2 and they are adjacent. The vertices 1 and 11 both have
$\sim$ Color 1 and they are adjacent. The vertices 2 and 5 both have Color 0 and they are adjacent.
$\sim$ Therefore, the given coloring is incorrect.
$======================$
0: Color 0 and 9: Color 0 share an edge.
0: Color 0 and 10: Color 0 share an edge.
0: Color 0 and 14: Color 0 share an edge.
9: Color 0 and 13: Color 0 share an edge.
$======================$
Vertex 3 is colored with Color 4, which exceeds the optimal coloring number of 4. Therefore, this
$\sim$ coloring is non-optimal.
$======================$
Vertex 0 is missing in the coloring. Missing assignment.
$======================$
Vertex 0 (color 3) is connected to vertex 3 (color 2), sharing an edge. Vertex 0 (color 3) is also
$\sim$ connected to vertex 4 (color 0), and to vertex 5 (color 0), and to vertex 9 (color 0), and to
$\sim$ vertex 12 (color 2), sharing an edge with them. Therefore, this assignment is correctly
$\sim$ colored with color 0 , color 2 , and color 3 , making use of 3 different colors.</p>
<p>Vertex 1 (color 2) is connected to vertex 4 (color 0), and to vertex 5 (color 0), and to vertex 6 $\hookrightarrow$ (color 0), and to vertex 7 (color 3), and to vertex 8 (color 0), and to vertex 10 (color 1), $\hookrightarrow$ and to vertex 11 (color 1), sharing an edge with them. Therefore, this assignment is also $\hookrightarrow$ correctly colored.</p>
<p>Vertex 2 (color 0) is connected to vertex 12 (color 2), sharing an edge. Therefore, this $\hookrightarrow$ assignment is correctly colored.</p>
<p>Vertex 3 (color 2) is connected to vertex 5 (color 0), and to vertex 8 (color 0), and to vertex 10 $\hookrightarrow$ (color 1), sharing an edge with them. Therefore, this assignment is correctly colored.</p>
<p>Vertex 4 (color 0) is connected to vertex 7 (color 3), and to vertex 11 (color 1), sharing an edge. $\hookrightarrow$ Therefore, this assignment is correctly colored.</p>
<p>Vertex 5 (color 0) is connected to vertex 7 (color 3), sharing an edge. Therefore, this assignment $\hookrightarrow$ is correctly colored.</p>
<p>Vertex 6 (color 0) is connected to vertex 10 (color 1), and to vertex 12 (color 2), sharing an $\hookrightarrow$ edge. Therefore, this assignment is correctly colored.</p>
<p>Vertex 8 (color 0) is connected to vertex 10 (color 1), sharing an edge. Therefore, this $\hookrightarrow$ assignment is correctly colored.</p>
<p>Vertex 9 (color 0) is connected to vertex 11 (color 1), sharing an edge. Therefore, this $\hookrightarrow$ assignment is correctly colored.</p>
<p>Vertex 10 (color 1) is connected to vertex 12 (color 2), sharing an edge. Therefore, this $\hookrightarrow$ assignment is correctly colored.</p>
<p>Vertex 11 (color 1) isn't connected to vertex 10 (color 1), hence, this assignment is correctly $\hookrightarrow$ colored.</p>
<p>Vertex 12 (color 2) isn't connected to vertex 2 (color 0), hence, this assignment is correctly $\hookrightarrow$ colored.</p>
<p>No adjacent vertices share a same color, and no colors exceed the optimal coloring number. Hence, $\hookrightarrow$ the coloring is correct.
Verifier confirmed success.</p>
<h1>Examples of (Rare) Mangled, Blogical, and Otherwise Strange Output on the Verification</h1>
<p>Task</p>
<p>Vertex 10, colored 4, is connected to vertex 11, also colored 1. However, it is also colored 4, $\hookrightarrow$ which is greater than the optimal coloring of 4 . So, this coloring is non-optimal.
$====================$
Vertex 9 (Color 2) and Vertex 10 (Color 1) are connected and share the same color. Vertex 9 (Color $\hookrightarrow$ 2) and Vertex 11 (Color 1) are also connected and share the same color.
$====================$
Vertex 2 (Color 0) is connected to Vertex 0 (Color 0).
Vertex 5 (Color 0) is connected to Vertex 1 (Color 2) and Vertex 14 (Color 1), but also to Vertex $\hookrightarrow 7$ (Color 1).
Vertex 6 (Color 0) is connected to Vertex 3 (Color 2), Vertex 10 (Color 1) and 13 (Color 2), but $\hookrightarrow$ also to Vertex 8 (Color 2).</p>
<p>The coloring is incorrect. Vertex 1 (Color 2) and Vertex 3 (Color 3) are connected but have $\hookrightarrow$ different colors. Vertex 3 (Color 3) is connected to Vertex 6 (Color 3).
$====================$
Vertex 10, colored 4, is connected to vertex 11, also colored 1. However, it is also colored 4, $\hookrightarrow$ which is greater than the optimal coloring of 4 . So, this coloring is non-optimal.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://pypi.org/project/grinpy/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
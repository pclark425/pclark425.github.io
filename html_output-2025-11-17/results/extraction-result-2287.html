<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2287 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2287</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2287</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-273811997</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.00816v3.pdf" target="_blank">CycleResearcher: Improving Automated Research via Automated Review</a></p>
                <p><strong>Paper Abstract:</strong> The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper refinement. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves promising performance with a 26.89\% reduction in mean absolute error (MAE) compared to individual human reviewers in predicting paper scores, indicating the potential of LLMs to effectively assist expert-level research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, showing some competitiveness in terms of simulated review scores compared to the preprint level of 5.24 from human experts, while still having room for improvement compared to the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and exploring AI-driven research capabilities. The code, dataset and model weight are released at https://wengsyx.github.io/Researcher/.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2287.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2287.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CycleResearcher (policy model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM-based policy model fine-tuned to autonomously perform literature review, hypothesis/idea generation, experimental design (simulated), and manuscript writing; trained on Research-14k and iteratively improved via preference optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CycleResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-based policy agent (Mistral / Qwen variants, 12B/72B/123B) that ingests bibliographic references and abstracts, generates structured outlines and main-text sections (motivation, hypothesis, methods, experimental designs), fabricates simulated experimental results in a virtual RL environment, and outputs complete LaTeX papers. It is fine-tuned on Research-14k and iteratively updated via preference-pair training using SimPO; experiment execution can be delegated to code-execution modules (e.g., AI Scientist/GPT-01-preview) in an integrated pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning (ML research papers)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration / automated research ideation and paper generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Contribution score produced by CycleReviewer (1-4 scale) and components of the Overall Score (1-10) from simulated peer review; also use of reference inclusion counts as a proxy for literature grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>CycleResearcher-12B Contribution avg = 2.60 (1-4); CycleResearcher-72B = 2.55; CycleResearcher-123B = 2.53. Overall simulated peer-review avg score for CycleResearcher-12B = 5.36 (scale 1-10). Reference inclusion: CycleResearcher-12B avg references = 37.8 (vs AI Scientist 8.2).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Soundness score from CycleReviewer (1-4 scale) and acceptance rate under simulated review; additionally human evaluation (ICLR-style criteria) used in a small study.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>CycleResearcher-12B Soundness avg = 2.71 (1-4); CycleResearcher-72B = 2.71; CycleResearcher-123B = 2.70. Simulated acceptance rate (CycleResearcher-12B) = 35.13%. Human-eval overall score (expert reviewers) CycleResearcher avg overall = 4.8 (scale presumably 1-10) vs AI Scientist 3.6.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit quantitative analysis of a novelty-vs-feasibility trade-off is reported; the paper reports both Contribution (novelty proxy) and Soundness/acceptance (feasibility proxy) for generated outputs but does not present correlations or Pareto analyses between them.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative preference optimization (SimPO) using preference pairs generated by CycleReviewer scores; training objective mixes SimPO loss (length-normalized log-prob scaled reward with margin) and NLL (negative log-likelihood) to stabilize generation; rejection sampling is used at inference to select higher-scoring candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Three NLP experts evaluated 20 generated papers each (10 CycleResearcher-12B with N=100 rejection sampling, 10 AI Scientist). Expert average overall score: CycleResearcher = 4.8 vs AI Scientist = 3.6. Expert average Soundness/Presentation/Contribution for CycleResearcher were higher than AI Scientist (Soundness ≈2.6 vs 2.2; Presentation ≈2.8 vs 2.6; Contribution ≈2.2 vs 1.8), though below accepted-paper baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>AI Scientist (Lu et al., 2024) and human preprint/accepted paper baselines</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>CycleResearcher-12B avg simulated review score 5.36 (preprint baseline 5.24, accepted papers 5.69). Acceptance rate CycleResearcher-12B = 35.13% vs AI Scientist = 0.00% (simulated). Contribution and Soundness scores exceed AI Scientist (e.g., contribution: 2.60 vs 2.15; soundness: 2.71 vs 2.48). Human expert ratings also favor CycleResearcher over AI Scientist.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Framework and models are trained/optimized for ML research; authors note limited generalizability outside ML without domain-specific datasets and that CycleReviewer knowledge cutoff (Jan 2024) reduces novelty assessment accuracy for newer papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2287.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CycleReviewer (generative reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative LLM-based reward model trained on Review-5k to simulate multiple peer reviewers and a senior synthesizer, producing structured reviews and quantitative scores (Strengths, Weaknesses, Soundness, Presentation, Contribution, Overall Score 1–10) used as the reward signal for policy optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CycleReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A fine-tuned Mistral-Large-2 generative reward model trained on Review-5k (ICLR 2024 reviews). Given a paper, it simulates sequential reviewer comments across multiple reviewers (from most critical to most favorable), summarizes via a senior reviewer, and outputs numerical scores for multiple axes including Soundness (1–4), Presentation (1–4), Contribution (1–4), and an Overall Score (1–10). These scores are averaged to produce the reward used to construct preference pairs for SimPO training of CycleResearcher.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning peer review / automated research evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>evaluation/reward modeling for generated research (supports idea selection and policy optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Contribution score (1-4) as a proxy for novelty/impact; textual critique sections describing novelty/relatedness.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Reported CycleReviewer outputs used to compute avg contribution scores: e.g., applied to CycleResearcher-12B collection yielding contribution avg ≈2.60 (scale 1-4) in simulated reviews; distribution statistics show ability to assign both high (>3) and low (<2) contribution scores across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Soundness score (1-4) used as a feasibility/probable-correctness proxy; Overall Score (1-10) and simulated acceptance decision; Proxy MAE/MSE against human reviewer votes used to measure evaluator accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>CycleReviewer achieves soundness-consistency statistics in the paper: when used as evaluator, the system reduced Proxy MAE by 26.89% compared to individual human reviewers and reduced Proxy MSE by 48.77% relative to a human baseline (Table 1). Exact soundness averages per model application reported (e.g., CycleResearcher-12B soundness avg ≈2.71 as evaluated by CycleReviewer).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit analysis of novelty-feasibility trade-offs performed by CycleReviewer; it outputs separate axis scores that could be used for such analyses but the paper does not report correlations or multi-objective frontiers.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Provides scalar/axis rewards to produce preference pairs (highest-scoring vs lowest-scoring outputs) used by SimPO to optimize CycleResearcher; reviewer simulations are diverse (simulate low->high reviewers) to create varied reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>CycleReviewer was evaluated against human reviewers using Proxy MSE/MAE on Review-5k test set; it showed improved consistency (26.89% reduction in Proxy MAE compared to individual reviewers) and distributional similarity to human scoring (trimodal min, similar max and average score distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Human reviewers and closed-source LLM evaluators (GPT-4o-mini, GLM-4, Gemini, Claude etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>CycleReviewer (123B) reported better Proxy MSE/MAE and decision accuracy (decision accuracy ≈74.24%, Macro F1 ≈73.99) compared to closed-source evaluated models in the study (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>CycleReviewer performance resembles human reviewer distributions on ML peer-review data; however, knowledge cutoff and offline reward model status limit novelty assessment for very recent literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2287.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPO (Simple Preference Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reference-free offline preference optimization method that uses length-normalized log-probability-based rewards and a target margin to train policy models from preference pairs without requiring a reference model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SimPO: Simple preference optimization with a reference-free reward</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SimPO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Offline preference optimization algorithm that defines reward r_Simpo(x,y) = β|y| log π_θ(y|x) (length-normalized log-probability scaled by β) and uses a margin γ in a logistic preference loss to separate winners and losers; combined with NLL regularization (λ) to stabilize training for complex reasoning outputs. Chosen for computational efficiency and to avoid dependence on a separate reference model.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>language-model preference optimization for generative tasks (applied here to automated research generation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>policy optimization from pairwise preferences (improving generated research outputs according to reviewer rewards)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Preference-pair-based optimization: resample generated outputs, label highest/lowest via CycleReviewer, then optimize policy π_θ using SimPO loss + NLL to produce the next policy iteration; iterative resampling forms an approximate online policy optimization loop.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>DPO (Direct Preference Optimization) and other RL-from-preferences methods</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>SimPO was selected for computational savings; the paper reports successful iterative improvements when SimPO is combined with NLL and resampling but does not present head-to-head numeric comparisons to DPO within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>SimPO's length-normalized reward helps align long-form generation targets (papers) and reduces memory/computation by removing a reference model dependency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2287.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RejectionSampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection sampling for candidate paper selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that generates many candidate papers from the policy model and selects the highest-scoring ones according to CycleReviewer, improving average automated-review metrics as sample size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Rejection sampling (candidate resampling & select-by-reward)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At inference, repeatedly sample N candidate papers from CycleResearcher (temperature specified), evaluate each with CycleReviewer, and select papers with the highest reviewer scores; used to boost the quality of published outputs without changing the policy weights.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning research paper generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>selection/quality amplification for open-ended generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Measured indirectly via CycleReviewer Contribution and Overall Score; more samples lead to higher average reviewer-assessed novelty/quality.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Increasing generated papers from 1 to 100 raised average simulated review score from ≈5.36 to ≈7.02 and average max score from ≈6.72 to ≈8.02 (reported for CycleResearcher-12B).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Measured indirectly via CycleReviewer Soundness and simulated acceptance rates.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Minimum score increased from ≈3.52 to ≈6.01 as samples increased to 100; acceptance-related metrics improved correspondingly in simulated evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit novelty-feasibility trade-off analysis; rejection sampling increased both novelty proxies (contribution/overall) and feasibility proxies (soundness/acceptance) according to CycleReviewer scores.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Brute-force sampling + evaluator-based selection (score maximization under CycleReviewer).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Single-sample generation (no resampling)</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Large-sample rejection sampling substantially improved automated-review metrics vs single-sample outputs (see numeric improvements above).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Effective because generation cost is low relative to human review; authors caution about reward-hacking risks since selection depends on the learned reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2287.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist (Lu et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system for automated open-ended scientific discovery (prompt-driven pipeline) that can assist with experiment execution and code generation; used here as both a baseline and an execution component in integrated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt-driven pipeline for automated experiments and open-ended scientific discovery; capable of code generation and experiment execution (used in this paper as a code-execution backend, e.g., GPT-01-preview + AI Scientist for running experiments designed by CycleResearcher). Also used as a baseline system for comparative evaluation of generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>automated scientific discovery (general/ML simulations in referenced works)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration and experiment execution</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Reported baseline performance: AI Scientist overall simulated review avg ≈4.31 (scale 1-10) in Table 2; contribution avg ≈2.15 (1-4); presentation/soundness lower and minimums indicate inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>AI Scientist simulated acceptance rate = 0.00% in the experiments reported; soundness avg ≈2.48 (1-4). Human expert evaluation overall score ≈3.6.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Prompt-based pipeline and iterative refinement in original work; in this paper AI Scientist is used primarily as executor/baseline rather than retrained with preference optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>In human expert blind evaluation, AI Scientist papers scored on average 3.6 overall (vs CycleResearcher 4.8) with lower average soundness/presentation/contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Serves as primary baseline for CycleResearcher comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>CycleResearcher variants outperform AI Scientist across simulated reviewer metrics and human expert ratings (see numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>AI Scientist has strengths in experiment execution but underperforms in consistent research writing and novelty compared to CycleResearcher in these ML-focused tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2287.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfRewardingLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-rewarding language models / Iterative self-rewarding framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework where an LLM generates outputs and self-evaluates them to create rewards used for iterative improvement, increasing idea diversity and practicality over repeated refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rewarding language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative self-rewarding framework (self-rewarding LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM uses internal or learned reward signals to score its own outputs and iteratively refines them (self-rewarding loop). Paper references this approach as inspiration/background for enabling continuous refinement of research ideas, improving diversity and practicality.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>automated idea generation / creative refinement (general)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>iterative refinement of generated research ideas</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Self-evaluation-driven iterative improvement (conceptually similar to using an internal reward model to create preference signals).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Cited as a method to boost both diversity (novelty) and practicality (feasibility) of generated ideas, but no direct quantitative results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2287.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative LLM-driven system designed for research idea generation over scientific literature (cited as related work for ideation pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-step LLM approach that iteratively generates and refines research ideas grounded in literature retrieval; cited as related work addressing iterative idea generation over scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>automated research idea generation over scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>iterative idea generation / literature-grounded ideation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative literature-grounded sampling and refinement (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2287.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-LLM system for scientific idea generation by recombining facets of research papers; cited as related work exploring LLMs for creative scientific ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scideator: Human-LLM scientific idea generation grounded in research-paper facet recombination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scideator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>System that generates scientific idea candidates by recombining structured facets of existing papers (e.g., methods, tasks, datasets) and supports human-in-the-loop selection and refinement; cited as an approach to improve novelty and grounding in idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>scientific idea generation (general)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>creative recombination / literature-grounded ideation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Facet recombination + human-in-the-loop curation (as described in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2287.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2287.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieval-augmented generation methods are suggested as future work to supply up-to-date literature and improve novelty/accuracy judgments when evaluating generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-augmented generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Augmenting LLMs with external retrieval of papers/abstracts to supply current knowledge during generation and evaluation; proposed as an approach to reduce outdated-knowledge errors and improve novelty assessment for the reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>knowledge-enhanced LLM generation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>knowledge-augmentation for idea generation and novelty assessment</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>RAG to improve evaluator/context; not implemented in this work but listed as future direction.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Authors note RAG could address CycleReviewer's outdated-knowledge limitation for novelty evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CycleResearcher: Improving Automated Research via Automated Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Scideator: Human-LLM scientific idea generation grounded in research-paper facet recombination <em>(Rating: 2)</em></li>
                <li>Self-rewarding language models <em>(Rating: 2)</em></li>
                <li>SimPO: Simple preference optimization with a reference-free reward <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2287",
    "paper_id": "paper-273811997",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "CycleResearcher",
            "name_full": "CycleResearcher (policy model)",
            "brief_description": "An open-source LLM-based policy model fine-tuned to autonomously perform literature review, hypothesis/idea generation, experimental design (simulated), and manuscript writing; trained on Research-14k and iteratively improved via preference optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CycleResearcher",
            "system_description": "LLM-based policy agent (Mistral / Qwen variants, 12B/72B/123B) that ingests bibliographic references and abstracts, generates structured outlines and main-text sections (motivation, hypothesis, methods, experimental designs), fabricates simulated experimental results in a virtual RL environment, and outputs complete LaTeX papers. It is fine-tuned on Research-14k and iteratively updated via preference-pair training using SimPO; experiment execution can be delegated to code-execution modules (e.g., AI Scientist/GPT-01-preview) in an integrated pipeline.",
            "research_domain": "machine learning (ML research papers)",
            "problem_type": "open-ended exploration / automated research ideation and paper generation",
            "novelty_metric": "Contribution score produced by CycleReviewer (1-4 scale) and components of the Overall Score (1-10) from simulated peer review; also use of reference inclusion counts as a proxy for literature grounding.",
            "novelty_score": "CycleResearcher-12B Contribution avg = 2.60 (1-4); CycleResearcher-72B = 2.55; CycleResearcher-123B = 2.53. Overall simulated peer-review avg score for CycleResearcher-12B = 5.36 (scale 1-10). Reference inclusion: CycleResearcher-12B avg references = 37.8 (vs AI Scientist 8.2).",
            "feasibility_metric": "Soundness score from CycleReviewer (1-4 scale) and acceptance rate under simulated review; additionally human evaluation (ICLR-style criteria) used in a small study.",
            "feasibility_score": "CycleResearcher-12B Soundness avg = 2.71 (1-4); CycleResearcher-72B = 2.71; CycleResearcher-123B = 2.70. Simulated acceptance rate (CycleResearcher-12B) = 35.13%. Human-eval overall score (expert reviewers) CycleResearcher avg overall = 4.8 (scale presumably 1-10) vs AI Scientist 3.6.",
            "tradeoff_evidence": "No explicit quantitative analysis of a novelty-vs-feasibility trade-off is reported; the paper reports both Contribution (novelty proxy) and Soundness/acceptance (feasibility proxy) for generated outputs but does not present correlations or Pareto analyses between them.",
            "optimization_strategy": "Iterative preference optimization (SimPO) using preference pairs generated by CycleReviewer scores; training objective mixes SimPO loss (length-normalized log-prob scaled reward with margin) and NLL (negative log-likelihood) to stabilize generation; rejection sampling is used at inference to select higher-scoring candidates.",
            "human_evaluation": true,
            "human_evaluation_results": "Three NLP experts evaluated 20 generated papers each (10 CycleResearcher-12B with N=100 rejection sampling, 10 AI Scientist). Expert average overall score: CycleResearcher = 4.8 vs AI Scientist = 3.6. Expert average Soundness/Presentation/Contribution for CycleResearcher were higher than AI Scientist (Soundness ≈2.6 vs 2.2; Presentation ≈2.8 vs 2.6; Contribution ≈2.2 vs 1.8), though below accepted-paper baselines.",
            "comparative_baseline": "AI Scientist (Lu et al., 2024) and human preprint/accepted paper baselines",
            "comparative_results": "CycleResearcher-12B avg simulated review score 5.36 (preprint baseline 5.24, accepted papers 5.69). Acceptance rate CycleResearcher-12B = 35.13% vs AI Scientist = 0.00% (simulated). Contribution and Soundness scores exceed AI Scientist (e.g., contribution: 2.60 vs 2.15; soundness: 2.71 vs 2.48). Human expert ratings also favor CycleResearcher over AI Scientist.",
            "domain_specific_findings": "Framework and models are trained/optimized for ML research; authors note limited generalizability outside ML without domain-specific datasets and that CycleReviewer knowledge cutoff (Jan 2024) reduces novelty assessment accuracy for newer papers.",
            "uuid": "e2287.0",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CycleReviewer",
            "name_full": "CycleReviewer (generative reward model)",
            "brief_description": "A generative LLM-based reward model trained on Review-5k to simulate multiple peer reviewers and a senior synthesizer, producing structured reviews and quantitative scores (Strengths, Weaknesses, Soundness, Presentation, Contribution, Overall Score 1–10) used as the reward signal for policy optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CycleReviewer",
            "system_description": "A fine-tuned Mistral-Large-2 generative reward model trained on Review-5k (ICLR 2024 reviews). Given a paper, it simulates sequential reviewer comments across multiple reviewers (from most critical to most favorable), summarizes via a senior reviewer, and outputs numerical scores for multiple axes including Soundness (1–4), Presentation (1–4), Contribution (1–4), and an Overall Score (1–10). These scores are averaged to produce the reward used to construct preference pairs for SimPO training of CycleResearcher.",
            "research_domain": "machine learning peer review / automated research evaluation",
            "problem_type": "evaluation/reward modeling for generated research (supports idea selection and policy optimization)",
            "novelty_metric": "Contribution score (1-4) as a proxy for novelty/impact; textual critique sections describing novelty/relatedness.",
            "novelty_score": "Reported CycleReviewer outputs used to compute avg contribution scores: e.g., applied to CycleResearcher-12B collection yielding contribution avg ≈2.60 (scale 1-4) in simulated reviews; distribution statistics show ability to assign both high (&gt;3) and low (&lt;2) contribution scores across samples.",
            "feasibility_metric": "Soundness score (1-4) used as a feasibility/probable-correctness proxy; Overall Score (1-10) and simulated acceptance decision; Proxy MAE/MSE against human reviewer votes used to measure evaluator accuracy.",
            "feasibility_score": "CycleReviewer achieves soundness-consistency statistics in the paper: when used as evaluator, the system reduced Proxy MAE by 26.89% compared to individual human reviewers and reduced Proxy MSE by 48.77% relative to a human baseline (Table 1). Exact soundness averages per model application reported (e.g., CycleResearcher-12B soundness avg ≈2.71 as evaluated by CycleReviewer).",
            "tradeoff_evidence": "No explicit analysis of novelty-feasibility trade-offs performed by CycleReviewer; it outputs separate axis scores that could be used for such analyses but the paper does not report correlations or multi-objective frontiers.",
            "optimization_strategy": "Provides scalar/axis rewards to produce preference pairs (highest-scoring vs lowest-scoring outputs) used by SimPO to optimize CycleResearcher; reviewer simulations are diverse (simulate low-&gt;high reviewers) to create varied reward signals.",
            "human_evaluation": true,
            "human_evaluation_results": "CycleReviewer was evaluated against human reviewers using Proxy MSE/MAE on Review-5k test set; it showed improved consistency (26.89% reduction in Proxy MAE compared to individual reviewers) and distributional similarity to human scoring (trimodal min, similar max and average score distributions).",
            "comparative_baseline": "Human reviewers and closed-source LLM evaluators (GPT-4o-mini, GLM-4, Gemini, Claude etc.)",
            "comparative_results": "CycleReviewer (123B) reported better Proxy MSE/MAE and decision accuracy (decision accuracy ≈74.24%, Macro F1 ≈73.99) compared to closed-source evaluated models in the study (Table 1).",
            "domain_specific_findings": "CycleReviewer performance resembles human reviewer distributions on ML peer-review data; however, knowledge cutoff and offline reward model status limit novelty assessment for very recent literature.",
            "uuid": "e2287.1",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SimPO",
            "name_full": "SimPO (Simple Preference Optimization)",
            "brief_description": "A reference-free offline preference optimization method that uses length-normalized log-probability-based rewards and a target margin to train policy models from preference pairs without requiring a reference model.",
            "citation_title": "SimPO: Simple preference optimization with a reference-free reward",
            "mention_or_use": "use",
            "system_name": "SimPO",
            "system_description": "Offline preference optimization algorithm that defines reward r_Simpo(x,y) = β|y| log π_θ(y|x) (length-normalized log-probability scaled by β) and uses a margin γ in a logistic preference loss to separate winners and losers; combined with NLL regularization (λ) to stabilize training for complex reasoning outputs. Chosen for computational efficiency and to avoid dependence on a separate reference model.",
            "research_domain": "language-model preference optimization for generative tasks (applied here to automated research generation)",
            "problem_type": "policy optimization from pairwise preferences (improving generated research outputs according to reviewer rewards)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Preference-pair-based optimization: resample generated outputs, label highest/lowest via CycleReviewer, then optimize policy π_θ using SimPO loss + NLL to produce the next policy iteration; iterative resampling forms an approximate online policy optimization loop.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "DPO (Direct Preference Optimization) and other RL-from-preferences methods",
            "comparative_results": "SimPO was selected for computational savings; the paper reports successful iterative improvements when SimPO is combined with NLL and resampling but does not present head-to-head numeric comparisons to DPO within this work.",
            "domain_specific_findings": "SimPO's length-normalized reward helps align long-form generation targets (papers) and reduces memory/computation by removing a reference model dependency.",
            "uuid": "e2287.2",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RejectionSampling",
            "name_full": "Rejection sampling for candidate paper selection",
            "brief_description": "A method that generates many candidate papers from the policy model and selects the highest-scoring ones according to CycleReviewer, improving average automated-review metrics as sample size increases.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Rejection sampling (candidate resampling & select-by-reward)",
            "system_description": "At inference, repeatedly sample N candidate papers from CycleResearcher (temperature specified), evaluate each with CycleReviewer, and select papers with the highest reviewer scores; used to boost the quality of published outputs without changing the policy weights.",
            "research_domain": "machine learning research paper generation",
            "problem_type": "selection/quality amplification for open-ended generation",
            "novelty_metric": "Measured indirectly via CycleReviewer Contribution and Overall Score; more samples lead to higher average reviewer-assessed novelty/quality.",
            "novelty_score": "Increasing generated papers from 1 to 100 raised average simulated review score from ≈5.36 to ≈7.02 and average max score from ≈6.72 to ≈8.02 (reported for CycleResearcher-12B).",
            "feasibility_metric": "Measured indirectly via CycleReviewer Soundness and simulated acceptance rates.",
            "feasibility_score": "Minimum score increased from ≈3.52 to ≈6.01 as samples increased to 100; acceptance-related metrics improved correspondingly in simulated evaluations.",
            "tradeoff_evidence": "No explicit novelty-feasibility trade-off analysis; rejection sampling increased both novelty proxies (contribution/overall) and feasibility proxies (soundness/acceptance) according to CycleReviewer scores.",
            "optimization_strategy": "Brute-force sampling + evaluator-based selection (score maximization under CycleReviewer).",
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": "Single-sample generation (no resampling)",
            "comparative_results": "Large-sample rejection sampling substantially improved automated-review metrics vs single-sample outputs (see numeric improvements above).",
            "domain_specific_findings": "Effective because generation cost is low relative to human review; authors caution about reward-hacking risks since selection depends on the learned reward model.",
            "uuid": "e2287.3",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AI Scientist",
            "name_full": "AI Scientist (Lu et al., 2024)",
            "brief_description": "A prior system for automated open-ended scientific discovery (prompt-driven pipeline) that can assist with experiment execution and code generation; used here as both a baseline and an execution component in integrated experiments.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "use",
            "system_name": "AI Scientist",
            "system_description": "Prompt-driven pipeline for automated experiments and open-ended scientific discovery; capable of code generation and experiment execution (used in this paper as a code-execution backend, e.g., GPT-01-preview + AI Scientist for running experiments designed by CycleResearcher). Also used as a baseline system for comparative evaluation of generated papers.",
            "research_domain": "automated scientific discovery (general/ML simulations in referenced works)",
            "problem_type": "open-ended exploration and experiment execution",
            "novelty_metric": null,
            "novelty_score": "Reported baseline performance: AI Scientist overall simulated review avg ≈4.31 (scale 1-10) in Table 2; contribution avg ≈2.15 (1-4); presentation/soundness lower and minimums indicate inconsistency.",
            "feasibility_metric": null,
            "feasibility_score": "AI Scientist simulated acceptance rate = 0.00% in the experiments reported; soundness avg ≈2.48 (1-4). Human expert evaluation overall score ≈3.6.",
            "tradeoff_evidence": null,
            "optimization_strategy": "Prompt-based pipeline and iterative refinement in original work; in this paper AI Scientist is used primarily as executor/baseline rather than retrained with preference optimization.",
            "human_evaluation": true,
            "human_evaluation_results": "In human expert blind evaluation, AI Scientist papers scored on average 3.6 overall (vs CycleResearcher 4.8) with lower average soundness/presentation/contribution.",
            "comparative_baseline": "Serves as primary baseline for CycleResearcher comparisons",
            "comparative_results": "CycleResearcher variants outperform AI Scientist across simulated reviewer metrics and human expert ratings (see numbers above).",
            "domain_specific_findings": "AI Scientist has strengths in experiment execution but underperforms in consistent research writing and novelty compared to CycleResearcher in these ML-focused tests.",
            "uuid": "e2287.4",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SelfRewardingLM",
            "name_full": "Self-rewarding language models / Iterative self-rewarding framework",
            "brief_description": "A framework where an LLM generates outputs and self-evaluates them to create rewards used for iterative improvement, increasing idea diversity and practicality over repeated refinement.",
            "citation_title": "Self-rewarding language models",
            "mention_or_use": "mention",
            "system_name": "Iterative self-rewarding framework (self-rewarding LMs)",
            "system_description": "LLM uses internal or learned reward signals to score its own outputs and iteratively refines them (self-rewarding loop). Paper references this approach as inspiration/background for enabling continuous refinement of research ideas, improving diversity and practicality.",
            "research_domain": "automated idea generation / creative refinement (general)",
            "problem_type": "iterative refinement of generated research ideas",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Self-evaluation-driven iterative improvement (conceptually similar to using an internal reward model to create preference signals).",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Cited as a method to boost both diversity (novelty) and practicality (feasibility) of generated ideas, but no direct quantitative results provided in this paper.",
            "uuid": "e2287.5",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Researchagent",
            "name_full": "Researchagent",
            "brief_description": "An iterative LLM-driven system designed for research idea generation over scientific literature (cited as related work for ideation pipelines).",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "system_name": "Researchagent",
            "system_description": "Multi-step LLM approach that iteratively generates and refines research ideas grounded in literature retrieval; cited as related work addressing iterative idea generation over scientific literature.",
            "research_domain": "automated research idea generation over scientific literature",
            "problem_type": "iterative idea generation / literature-grounded ideation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Iterative literature-grounded sampling and refinement (details in cited work).",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2287.6",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Scideator",
            "name_full": "Scideator",
            "brief_description": "A human-LLM system for scientific idea generation by recombining facets of research papers; cited as related work exploring LLMs for creative scientific ideation.",
            "citation_title": "Scideator: Human-LLM scientific idea generation grounded in research-paper facet recombination",
            "mention_or_use": "mention",
            "system_name": "Scideator",
            "system_description": "System that generates scientific idea candidates by recombining structured facets of existing papers (e.g., methods, tasks, datasets) and supports human-in-the-loop selection and refinement; cited as an approach to improve novelty and grounding in idea generation.",
            "research_domain": "scientific idea generation (general)",
            "problem_type": "creative recombination / literature-grounded ideation",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Facet recombination + human-in-the-loop curation (as described in cited work).",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2287.7",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "Retrieval-augmented generation methods are suggested as future work to supply up-to-date literature and improve novelty/accuracy judgments when evaluating generated hypotheses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-augmented generation (RAG)",
            "system_description": "Augmenting LLMs with external retrieval of papers/abstracts to supply current knowledge during generation and evaluation; proposed as an approach to reduce outdated-knowledge errors and improve novelty assessment for the reward model.",
            "research_domain": "knowledge-enhanced LLM generation and evaluation",
            "problem_type": "knowledge-augmentation for idea generation and novelty assessment",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "RAG to improve evaluator/context; not implemented in this work but listed as future direction.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Authors note RAG could address CycleReviewer's outdated-knowledge limitation for novelty evaluation.",
            "uuid": "e2287.8",
            "source_info": {
                "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Scideator: Human-LLM scientific idea generation grounded in research-paper facet recombination",
            "rating": 2,
            "sanitized_title": "scideator_humanllm_scientific_idea_generation_grounded_in_researchpaper_facet_recombination"
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 2,
            "sanitized_title": "selfrewarding_language_models"
        },
        {
            "paper_title": "SimPO: Simple preference optimization with a reference-free reward",
            "rating": 2,
            "sanitized_title": "simpo_simple_preference_optimization_with_a_referencefree_reward"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 1,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        }
    ],
    "cost": 0.02400175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CYCLERESEARCHER: IMPROVING AUTOMATED RE-SEARCH VIA AUTOMATED REVIEW WARNING: THIS WORK IS NOT ADVOCATING THE USE OF LLMS FOR PAPER WRITING
8 Mar 2025</p>
<p>Yixuan Weng wengsyx@gmail.com 
Research Center for Industries of the Future
Westlake University</p>
<p>School of Engineering
Westlake University
3 Zhejiang University 4 William&amp;Mary</p>
<p>Minjun Zhu 
School of Engineering
Westlake University
3 Zhejiang University 4 William&amp;Mary</p>
<p>Guangsheng Bao 
School of Engineering
Westlake University
3 Zhejiang University 4 William&amp;Mary</p>
<p>Hongbo Zhang 
School of Engineering
Westlake University
3 Zhejiang University 4 William&amp;Mary</p>
<p>Jindong Wang 
Yue Zhang zhangyue@westlake.edu.cn 
Research Center for Industries of the Future
Westlake University</p>
<p>School of Engineering
Westlake University
3 Zhejiang University 4 William&amp;Mary</p>
<p>Linyi Yang yanglinyiucd@gmail.comzhuminjun 
University College London</p>
<p>CYCLERESEARCHER: IMPROVING AUTOMATED RE-SEARCH VIA AUTOMATED REVIEW WARNING: THIS WORK IS NOT ADVOCATING THE USE OF LLMS FOR PAPER WRITING
8 Mar 2025D9D10AF2A1A7D80BCEF3AFB51F803B4EarXiv:2411.00816v3[cs.CL]Review 2…… Strengthsxxxxxxx Weaknessesxxxxxx Soundness2 Presentation3 Contribution3 Score5 Weak Reject
The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation.While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored.This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper refinement.Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning.To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics.Our results demonstrate that CycleReviewer achieves promising performance with a 26.89% reduction in mean absolute error (MAE) compared to individual human reviewers in predicting paper scores, indicating the potential of LLMs to effectively assist expert-level research evaluation.In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, showing some competitiveness in terms of simulated review scores compared to the preprint level of 5.24 from human experts, while still having room for improvement compared to the accepted paper level of 5.69.This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and exploring AI-driven research capabilities.The code, dataset and model weight are released at https://wengsyx.github.io/Researcher/.</p>
<p>INTRODUCTION</p>
<p>Automating general scientific discovery has been a long-standing ambition of the research community, dating back to the late 1970s and 1980s (Lenat, 1977;1983;Langley, 1987) with the advent of computer science.In the field of AI, researchers have envisioned automating scientific research using AI itself (Hutter, 2001;Radensky et al., 2024).The recent emergence of large language models (LLMs) has opened new possibilities for this endeavor (Wang et al., 2023a;Lu et al., 2024), demonstrating their capacity to not only process but also contribute meaningfully to scientific research.Most current efforts have relied on commercial LLMs to build agents that propose research ideas (Wang et al., 2023b;Yang et al., 2023;Radensky et al., 2024;Baek et al., 2024;Liu et al., 2024b), as an assistant to conduct experiments (Du et al., 2024;Yang et al., 2024b;Li et al., 2024), or act as an AI scientist capable of generating automated open-ended scientific publications (Lu et al., 2024;Taniguchi et al., 2024).To date, the challenge of automating the entire scientific discovery process remains largely unresolved, particularly when it comes to generating and refining research outputs that meet the high standards of peer-reviewed work.Moreover, few efforts address the integration of iterative feedback, which is essential for maintaining academic soundness and novelty.Current models often struggle to adapt across the full spectrum of research stages, highlighting gaps in their ability to conduct comprehensive, multi-step scientific discovery.</p>
<p>Central to the scientific process is the iterative cycle of submission, peer review, and refinementan established mechanism that maintains the quality and integrity of academic work (Smith, 2006;Boughton et al., 2018).Feedback from reviewers and peers plays a critical role in this cycle, offering insights that help researchers refine their work and improve its rigor and impact.Drawing inspiration from this cyclical process, we propose a novel framework that post-trains LLMs as autonomous agents to simulate the full loop of the scientific discovery process.Our approach, built entirely on open-source models, aims to replicate the real-world dynamic of research development and peer review processes.By leveraging trainable models, we enable the utilization of the iterative preference training mechanism (Yuan et al., 2024) using sampling examples through reinforcement learning.Our objective is to determine whether LLMs can actively contribute to each stage of scientific inquiry, from literature review and idea generation to experimental design, manuscript preparation, peer review and paper refinement.</p>
<p>Automating the entire research lifecycle, " Oberg et al. (2022) presents a significant challenge to current agent-based methods (Lu et al., 2024;Si et al., 2024;Yang et al., 2024b), which predominantly rely on commercial models.Consequently, these methods cannot be effectively modeled as policy optimization problems using reinforcement learning.While self-correction methods (Weng et al., 2023;Yuan et al., 2024;Lee et al., 2024) have been developed to enhance reasoning performance by assessing the quality of LLM outcomes, they have not yet been adopted in the domain of paper writing, which demands more complex evaluations from multiple perspectives.Our research addresses this gap by introducing an iterative post-training framework.The central research question we pose is: "How can we automate the Research-Review-Refinement process by post-training LLMs?"So that automated research can be improved according to feedback from automated reviews.</p>
<p>We build a novel iterative training framework (Pang et al., 2024) that contains two core components: the policy model (namely CycleResearcher) and the reward model (namely CycleReviewer) ranging in size -from 12B to 123B -based on Mistral (Jiang et al., 2023) and Qwen 2.5 (Yang et al., 2024a;Team, 2024).In our framework, CycleResearcher acts as a scientific thinker, responsible for reading literature, identifying research problems, proposing solutions, and designing experiments, while specific experiment execution is delegated to specialized code models.In particular, the policy model performs a variety of research tasks ... for paper generation1 .The reward model, on the other hand, simulates the peer review process, evaluating the quality of the research output and providing feedback that informs reinforcement learning rewards.In the virtual RL environment, to accelerate training, we require the experimental results to be fabricated instead of conducting actual experiments To illustrate our framework's operation, when exploring the topic of "Hacking Rewards of VLMs," we first fed fine-tuned CycleResearcher with a set of relevant published papers to inspire it to propose novel ideas.After generating a batch of first-round papers corresponding with those ideas, fine-tuned CycleReviewer evaluates them to generate pairwise preference samples, which are used to optimize the policy model using SimPO (Meng et al., 2024), this process is repeated.</p>
<p>For training our models, we construct two large-scale, publicly available datasets: Review-5k and Research-14k (described in Section 2), which contain peer review and accepted papers from major ML conferences (e.g., ICLR, ICML, NeurIPS).For testing, we take both subjective human evaluation and objective model-based evaluations to assess the quality of CycleReviewer and CycleResearcher.Our experiments show that CycleReviewer demonstrates promising capabilities in supporting the peer review process, while CycleResearcher exhibits consistent performance in research ideation and experimental design compared to API-based agents (Lu et al., 2024).We also acknowledge that the generalizability across research domains remains a challenge for current LLMs.Our contributions can be summarized as:</p>
<p>DATASET CONSTRUCTION</p>
<p>In this section, We present an overview of how we collect a substantial corpus of academic papers and organize them into the Review-5k and Research-14k training dataset.As illustrated in Figure 1, we introduce structured outline extraction and segmentation to assist the LLM in planning before generating research papers.Notably, we will only make our datasets publicly available for those papers, which receive written consent from publishers (See in Appendix C).</p>
<p>REVIEW-5K</p>
<p>In order to collect a high-quality review dataset, we first gather paper information (including title, abstract, and PDF data) along with the corresponding review comments from ICLR 2024.This ensures that all papers are evaluated according to a consistent standard.We then attempt to retrieve the permitted LaTeX files from ArXiv.If the LaTeX files are unavailable, we use MagicDoc to convert the retrieved PDFs into markdown format.Then, inspired by the traditional peer review process, where a group of reviewers evaluates a paper, followed by a senior reviewer who synthesizes their feedback and makes the final decision, we collect each data point including key components: 1) summary of the work, 2) identified strengths and weaknesses, and 3) questions for clarification, along with 4) numerical scores for soundness, presentation, contribution, and an overall rating.Finally, we leave a dataset named Review-5k, containing 4,991 papers collected from ICLR 2024, comprising over 16,000 reviewer comments.Finally, we split our dataset into mutually exclusive training/testing sets, we keep 4,189 paper reviews for training and 782 samples for testing.</p>
<p>RESEARCH-14K</p>
<p>The research-14k dataset aims to capture structured outlines and detailed main text from academic papers.The data construction process involves three steps: (1).we first compile a list of accepted papers from major international machine learning conferences, such as ICLR, NeurIPS, ICML, ACL, EMNLP, CVPR, and ICCV, spanning from 2022 to 2024.Using Semantic Scholar2 , we retrieve the corresponding ArXiv links and LaTeX-format files for each paper, gathering a total of 14,911 papers.The main text of these papers is then pre-processed using rule-based filtering to remove irrelevant content such as comments ("%") and acknowledgments.</p>
<p>(2).Since the academic value of a research paper depends on its background, we also use the Semantic Scholar API to retrieve the cited works from the bib file and add their abstracts to it.(3).Finally, we organize the main body of each paper into outlines and separate sections to help the model better understand the research process.We use the Mistral-Large-2 model (Jiang et al., 2023) to extract outline information from the paper, following the outline structure shown in Figure 1, and concatenate each outline with its corresponding section.These components form the complete fine-tuning dataset, where the input consists of detailed reference files and the output contains the paper outlines and main text.</p>
<p>After filtering papers that do not meet the requirements, the final</p>
<p>ITERATIVE TRAINING FRAMEWORK</p>
<p>We use the iterative Simple Preference Optimization (SimPO) (Meng et al., 2024) framework to replicate the Research-Review-Refinement cycle typical in academic research.As mentioned in the introduction, we primarily focus on the development of ideas and the writing process, while the execution of actual experiments (Liu et al., 2024b;Zhu &amp; Zhou, 2024;Hu et al., 2025) is beyond the scope of this work.The process begins with initializing two models: a baseline language model fine-tuned for academic writing (the CycleResearcher), and an LLM specialized in evaluating research papers (the CycleReviewer).</p>
<p>As illustrated in Figure 2, each iteration encompasses two primary phases: (1) The CycleResearcher model simulates key research steps, including literature review, hypothesis formulation, experimental design, and paper writing, culminating in the production of an academic paper.</p>
<p>(2) Subsequently, the CycleReviewer model simulates a peer review process based on the generated paper, providing comprehensive feedback and quantitative scores.To facilitate iterative improvement, we implement a resampling procedure after each round, generating new preference data based on paper scores, which is then utilized to train the models for the subsequent iteration.</p>
<p>REWARD MODEL: CYCLEREVIEWER</p>
<p>We train CycleReviewer as the Generative Reward model on the Review-5k Dataset.To accurately reflect the academic peer review process, we establish a streamlined evaluation workflow:
Paper → R 1 , R 2 , . . . , R n → SR,(1)
Where the research paper (Paper) is reviewed by multiple reviewers (R 1 , R 2 , . . ., R n ).Each reviewer's opinion is then summarized by a Senior Reviewer (SR), forming the final decision.</p>
<p>The input to the CycleReviewer model is a complete research paper.Upon receiving the paper, the model generates sequential feedback and scores for key aspects including Strengths, Weaknesses, Soundness, Presentation, Contribution, and an Overall Score.The Overall Score is rated on a scale from 1 to 10, where 1 represents the lowest score and 10 the highest, with 5 indicating the paper is borderline for rejection and 6 suggesting it is near acceptance.The output of the model includes both the Overall Score and a recommendation labelled as the "Final Suggestion."The CycleReviewer simulates the review process across multiple reviewers, producing a set of Overall Scores.The final output is the average of these scores, representing the overall evaluation of the paper by the system.</p>
<p>Settings.We use the Mistral-Large-2 model with LoRA-GA (Wang et al., 2024a) on an 8x H100 80G cluster, with a learning rate of 1e-5 and a batch size of 4x8, for 12 epochs on the Reviewer-5k dataset.To ensure diversity in the generated reviews, CycleReviewer starts by simulating the feedback from the reviewer with the lowest rating, gradually progressing to the highest-rated reviewer.This approach ensures that a range of perspectives, from more critical to more favorable, are considered before the senior reviewer delivers the final assessment.</p>
<p>POLICY MODEL: CYCLERESEARCHER</p>
<p>The CycleResearcher model is trained on Research-14k, and the process begins with a literature review, where the input bib file contains all references and their corresponding abstracts.After gaining a comprehensive understanding of the research background, the model moves on to manuscript preparation.In this stage, generating outlines and main text alternates to ensure a logical flow.First, the model generates the motivations and main ideas in the outline and then follows up by producing the title, abstract, introduction, and method sections in the main text.Next, it outlines the experimental setup and results, and subsequently generates the experimental design and simulated results in the main text, where it also incorporates discussions.In the virtual RL environment, to accelerate training, we require the "experimental results" to be fabricated instead of conducting actual experiments.Finally, the model analyzes the experimental results and formulates the conclusion.Once all sections of the main text are generated, they are combined into a complete paper in LaTeX format.Notably, each part of the research paper in Research-14k is precisely segmented.Finally, the generated paper P is evaluated using the CycleReviewer, as described in Section 3.1.</p>
<p>Settings.</p>
<p>To build the policy model, we select widely used open-source LLMs: Mistral-Nemo-12B, Qwen2.5-Instruct-72B, and Mistral-Large-2 123B.All models are trained using 8x H100 GPUs and DeepSpeed + ZeRO2 (Rajbhandari et al., 2020;Rasley et al., 2020).We maximized context length by setting the 12B model to 32K tokens, while the 72B and 123B models were set to 24K tokens.Given memory constraints, samples exceeding the preset context length are randomly truncated.We use a batch size of 2 × 8, a learning rate of 4e −5 , and train for a total of 12,000 steps.These models support context windows up to 128K tokens, making them suitable for planning research projects and writing research papers.In response, we contribute three versions of policy models: CycleResearcher-12B, CycleResearcher-72B, and CycleResearcher-123B.During the reinforcement learning phase, we used a learning rate of 5e-7.For the 12B model, we used a text length of 18K, while for the 72B and 123B models, the maximum text length was 10K, with truncation applied from the end.Each iteration trains for one epoch using data obtained through sampling.</p>
<p>ITERATIVE SIMPO</p>
<p>We design an Iterative preference optimization alignment method (Xiong et al., 2024;Liu et al., 2024a) that simulates the peer review process as a reward mechanism.To construct a preference-pair dataset, we first collected 4,152 recent machine learning papers published on arXiv, retaining only the reference sections as the knowledge base.Then we sampled three times from the CycleResearcher with a temperature of 0.4 and processed the results into standard LaTeX-style texts M 1 , M 2 , M 3 .</p>
<p>Next, the CycleReviewer model simulated discussions among multiple reviewers, providing detailed evaluations of various aspects of the papers (e.g., novelty, methods, experimental design, result analysis).The average score r i from all simulated reviewers was assigned to each output M i .We then selected the output with the highest reward value as the positive sample y w and the one with the lowest reward value as the negative sample y l , forming a preference-pair dataset D 0 = (x, y w , y l ).</p>
<p>Policy Optimization.Instead of using the iterative DPO training framework (Pang et al., 2024), we adopt the SimPO as the base method for saving computational costs.To mitigate overfitting, we sample one-third of the full dataset in each round.Then, we generate a series of models P 1 , . . ., P T , where each model P t+1 is created using the preference data D t generated from the model P t .With the preference-pair dataset, we trained a new policy model π θ from P t to P t+1 .P 1 was initialized from the original fine-tuned CycleResearcher model using instruction tuning.</p>
<p>SimPO builds upon DPO (Rafailov et al., 2023), which is one of the most common offline preference optimization methods.It introduces a length-normalized reward function aligned with the generation target, thereby eliminating dependence on a reference model π ref , which reduces memory and computation requirements.The reward function for SimPO is as follows:
r Simpo (x, y) = β |y| log π θ (y | x) = β |y| |y| i=1 log π θ (y i | x, y <i ),(2)
where π θ is the policy model, |y| represents the length of the generated sequence, and β is a constant controlling the scaling of reward differences.SimPO also introduces a target reward margin γ > 0 to help differentiate between winning and losing responses.The objective for SimPO is as follows:
L SimPO (π θ ) = −E (x,yw,y l )∼D log σ β |y w | log π θ (y w |x) − β |y l | log π θ (y l |x) − γ (3)
Considering that the models used in the research process may involve complex reasoning and mathematical calculations, we combine the SimPO loss learned from preference pairs with the negative log-likelihood (NLL) loss to stabilize training (Pang et al., 2024).The loss function for each preference pair is as follows:
L Our (π θ ) = −E (x,yw,y l )∼D log σ β |y w | log π θ (y w |x) − β |y l | log π θ (y l |x) − γ −λE (x,yw)∼DNLL [log π θ (y w | x)] .(4)
Here, the hyperparameter λ balances the two loss terms.Each round of training resamples and optimizes based on the previous round's results, enabling an approximate online policy optimization process, which allows the CycleResearcher to continuously adapt to evolving publication standards.</p>
<p>SAFEGUARD ACADEMIC INTEGRITY</p>
<p>Beyond automating the research process, we are also concerned with safeguarding academic integrity.We aim to prevent the misuse of LLMs in the research community.To achieve that, we adopt the Fast-DetectGPT (Bao et al., 2024), which aims to use the metric of conditional probability curvature to determine whether the paper submission is generated by LLMs.Specifically, we use Llama-3-8B (Dubey et al., 2024) as the scoring model and determine if a paper was generated by an LLM by comparing the conditional probability curvature with a predefined threshold ϵ.If the curvature of a paper is larger than the threshold, we classify the paper as LLM-generated, otherwise human-written.which are unobservable due to the unknown true quality.Therefore, we introduce a proxy evaluation method using an independent, unbiased estimator as a stand-in for the ground truth score.Assuming we have n human experts with scores R = r 1 , r 2 , . . ., r n , we treat each reviewer's score r i as an unbiased estimator of the true quality.We define r ′ i = mean(R \ r i ), which serves as an unbiased estimator excluding r i .Thus, we measure the quality of r i using Proxy MSE = (r i − r ′ i ) 2 and Proxy MAE = |r i − r ′ i |.Simply put, for each submission, we use the average of the other n − 1 reviewers' scores as an estimator of the true score.</p>
<p>EXPERIMENTS</p>
<p>Our evaluation on the Reviewer-5k test set (average rating 5.53) uses this proxy approach for fair comparison.In the n − 1 mode, we randomly select one reviewer and use the average of remaining scores as the proxy ground truth.We apply this methodology to evaluate both human experts and closed-source models, including the AI Scientist review system (Lu et al., 2024) with one-shot reviews, self-reflection (Shinn et al., 2023), and ensembled reviews.(Lu et al., 2024) in generating reliable evaluation scores.However, we emphasize that these metrics focus on score consistency rather than capturing the full complexity of expert review, where human insight remains invaluable.Rejection sampling improves the quality of generated papers in terms of automated review metrics in Figure 3. Rejection sampling is especially valuable in the context of academic paper generation, where the cost of producing research plans and papers using language models is relatively low compared to other stages of research.As the number of generated papers increases from 1 to 100, the average score rises from approximately 5.36 to 7.02, surpassing both preprint papers (5.24) and accepted papers (5.69).The average maximum score improves from 6.72 to 8.02, while the average minimum score increases substantially from 3.52 to 6.01, both exceeding the preprint paper baseline.These findings indicate that larger sample sizes enable the model to consistently generate higher-quality research papers, making rejection sampling an effective strategy to enhance overall paper quality in terms of soundness, presentation, and contribution.</p>
<p>THE IMPORTANCE OF RESEARCH LIFECYCLE SIMULATION</p>
<p>Ablation Study in Table 4.When Reinforcement Learning is removed, leaving only the initial version with supervised training, the average score drops to 5.12, with an acceptance rate of 29.80%.</p>
<p>Removing the iterative training process results in a score of 5.21 and a slightly higher acceptance rate of 32.91%.When Negative Log-Likelihood (NLL) loss is removed, the results decrease significantly.This causes issues such as repetitive text generation and significant errors in the produced content, with the average score dropping sharply to 4.91 and the acceptance rate plummeting to 12.03%.These results highlight the importance of RL, iterative training, and NLL in maintaining the quality and stability of generated research papers in terms of automated review metrics.Overcoming these challenges is essential for developing robust models capable of producing academic content that performs well in automated reviews.To rigorously validate CycleResearcher's performance, we conducted a human evaluation study involving three NLP experts.These experts, each with a strong publication record (average 1,110 Google Scholar citations) and prior experience as reviewers for top-tier NLP conferences, were recruited for this task.To ensure relevance and expertise-based assessment, we carefully selected papers for each reviewer that aligned closely with their research interests.Prior to evaluation, all papers were converted from PDF to Markdown format, and manually checked to fix formatting issues (e.g., figure/table layout).Crucially, all identifying author information was anonymized, providing reviewers only with the main text of each paper.Each expert then evaluated 20 papers in total: 10 generated by CycleResearcher-12B (using N=100 rejection sampling) and 10 by AI Scientist (with 50 initial ideas and 3 refinement iterations).The evaluation process spanned one week, during which reviewers were instructed to strictly adhere to the ICLR 2024 review guidelines.We explicitly asked them to evaluate papers based on standard academic criteria, including soundness, presentation, and contribution, and to provide detailed comments and scores for each paper.Furthermore, we specifically directed reviewers to critically assess the experimental design and methodology of each paper, and to flag any potential flaws or inconsistencies they identified.As detailed in To ensure the responsible use of our models, we implemented the Fast-DetectGPT method to classify whether a paper is machine-generated.Table 6 shows the performance of our detection tool across different formats, achieving over 95% accuracy for review contents and nearly 99% accuracy for paper texts.This ensures that any outputs generated by CycleResearcher or CycleReviewer can be accurately identified, thus protecting the integrity of the research community.</p>
<p>LLMs for Research.In recent years, several studies have explored using language models for creative tasks in research, such as multi-agent collaborative writing (Baek et al., 2024) and multi-module retrieval (Yang et al., 2023) to improve research idea generation.These works aim to boost the novelty and diversity of AI in creative tasks.Si et al. (2024) conducted a comprehensive human evaluation of the task of idea generation by language models.Wang et al. (2024b) proposed using LLMs to automatically write survey papers.Additionally, LLMs have been used to automate the research process: Huang et al. ( 2024) introduced a benchmark for evaluating LLMs in coding solutions for machine learning problems; Wang et al. (2023b) proposed a method leveraging LLMs for scientific literature retrieval.The AI Scientist project (Lu et al., 2024) introduced a fully automated, promptdriven research pipeline.However, prompt-based methods often fail to generate ideas that are both diverse and practical, limiting their real-world application.To address this, we developed an iterative self-rewarding framework that enables the LLM to refine its ideas continuously, enhancing both diversity and practicality in research proposal generation.</p>
<p>LLMs for Science Discovery.The tradition of AI-assisted scientific discovery (Langley, 1987;2024) has a long history.As early as the last century, AI was applied in fields such as chemistry (Buchanan &amp; Feigenbaum, 1981), synthetic biology (Jumper et al., 2021;Hayes et al., 2024), material discovery (Pyzer-Knapp et al., 2022;Merchant et al., 2023), and mathematics (Romera-Paredes et al., 2024).</p>
<p>With the development of neural networks (LeCun et al., 2015), more researchers have focused on AI4Science (AI4Science &amp; Quantum, 2023;LI, 2024;Yakaboski et al., 2023).AI is mainly used for data analysis within a single domain, playing a passive role without driving scientific discovery.</p>
<p>The key challenge is enabling AI to go beyond analysis and actively contribute to generating new research ideas, which demands advanced reasoning and creativity.Our work builds on AI's historical role in science, aiming to shift AI from a supporting tool to a leader in scientific discovery.</p>
<p>Automated Evaluation of Research Papers.The use of AI tools in the scientific publishing process has garnered widespread attention (Bao et al., 2021;Liu &amp; Shah, 2023;Liang et al., 2024;D'Arcy et al., 2024), including summarizing research paper content (Collins et al., 2017), detecting inaccuracies (Nuijten et al., 2016), and identifying fairness disparities (Zhang et al., 2022).Hosseini &amp; Horbach (2023) conducted small-scale qualitative experiments to evaluate the effectiveness of ChatGPT in the peer review process, while Robertson (2023) invited 10 participants to assess the benefits of GPT-4 in assisting with peer review.Lu et al. (2024) and Tyser et al. (2024) used GPT-4 to evaluate full-text PDFs of scientific papers.However, when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024).This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews.In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review.Our CycleReviewer simulates reviewers with varying perspectives, documenting summaries, strengths, and weaknesses.In the final stage, a primary reviewer consolidates these insights to deliver the final decision.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduced a novel framework for automating the entire research lifecycle using large language models (LLMs).Our approach combines CycleResearcher, a policy model designed to autonomously conduct scientific research, and CycleReviewer, a reward model that simulates the peer review process.Through the integration of Iterative SimPO, we enable the models to selfimprove over multiple research-review-refinement cycles.To facilitate this, we constructed two new datasets, Review-5k and Research-14k, which capture the complexities of peer review and research paper writing in machine learning.CycleReviewer shows superior scoring consistency compared to evaluated closed-source models, while CycleResearcher generates papers approaching human preprint quality in simulated reviews, with competitive acceptance rates.These results indicate the feasibility of using LLMs to contribute meaningfully to both the scientific discovery and peer review processes.As we move forward, the potential of LLMs to transform research practices is vast.We hope this work sparks further investigation into how AI can assist researchers, while maintaining the highest standards of academic integrity and ethical responsibility.</p>
<p>ACKNOWLEDGEMENT</p>
<p>We want to express huge thanks to our reviewers who gave us insightful suggestions and helped us complete a more comprehensive ethical consideration checklist.Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Linyi Yang, and Yue Zhang have been supported by the Research Program No. WU2023C020 of Research Center for Industries of the Future, Westlake University.Jindong Wang is supported by the Commonwealth Cyber Initiative (CCI).Correspondence to Linyi Yang (yanglinyiucd@gmail.com) and Yue Zhang (zhangyue@westlake.edu.cn).</p>
<p>We thank the AI Scientist (Lu et al., 2024) for providing the foundational code required for automated experiments in our work.</p>
<p>ETHICAL CONSIDERATIONS</p>
<p>While our primary objective is to advance research automation via LLMs, it is crucial to clarify that we are not advocating for their misuse in academic paper generation.Recognizing the potential ethical risks associated with CycleResearcher and CycleReviewer models, we have implemented comprehensive safeguards.Our high-performance detection tool can identify AI-generated submissions with accuracy exceeding 95%, and all model outputs include embedded watermarks with clear disclosure statements.The licensing framework requires institutional affiliation disclosure and enables publishers to verify model access when concerns arise, while protecting user privacy.All papers must include a clear disclosure:</p>
<p>This paper was written with the assistance of CycleResearcher, including but not limited to the introduction, related work, experimental design, and experimental results sections.A portion of the content may have been generated using large language models (LLMs).</p>
<p>We extensively tested for potential misuse through red-teaming exercises, evaluating scenarios like cyber-attacks or harmful content generation.To prevent unlawful information dissemination, we implemented SafetyLock (Zhu et al., 2024) before releasing open-source weights.</p>
<p>The impact on the scientific community warrants careful consideration.On the positive side, our framework can accelerate scientific discovery by automating routine research tasks and enabling rapid hypothesis validation.It could particularly benefit resource-constrained researchers by providing sophisticated research assistance.However, we acknowledge concerns about potential academic integrity issues and the risk of flooding venues with AI-generated papers.To address this, we've developed a streamlined detection framework that can identify AI-generated content within approximately 2 seconds, making it practical for widespread deployment in submission systems.</p>
<p>To ensure accountability and responsible use, we've implemented a comprehensive licensing and monitoring system.Users must disclose their institutional affiliations and explicitly declare their intended use cases before accessing the models.Our licensing agreement includes a novel disclosure mechanism that balances transparency with privacy -when publishers have legitimate concerns about potential misuse, they can submit queries to verify if specific authors have accessed our models within a given timeframe.This verification process is designed to protect user privacy while maintaining academic integrity, as it only confirms model access without revealing specific usage details.Additionally, all users must agree not to utilize the models for official peer reviews or submissions without full disclosure of AI involvement.</p>
<p>To prevent the proliferation of low-quality research content, we advocate for a comprehensive disclosure and verification framework.We strongly encourage publishers to require authors to declare their use of LLMs in research -a practice already being adopted by major conferences including NeurIPS 2024, ICLR 2025, and ACL.Our detection tool complements this policy by enabling rapid verification of AI-generated content within 2 seconds.When discrepancies are found between author declarations and detection results, publishers can either request clarification or decline review.This system, combined with our licensing framework that tracks model access, creates a robust accountability mechanism.Moreover, our work aligns with the community's goal of maintaining contributions rather than increasing paper volume, complementing existing peer review processes in filtering out low-quality submissions.</p>
<p>We envision these technologies as augmenting rather than replacing human researchers, particularly in accelerating routine aspects of research while allowing scientists to focus on creative and critical thinking.To support this vision, we're developing collaborative frameworks where CycleResearcher serves as an intelligent research assistant, generating hypotheses and experimental designs that human researchers can refine and validate.This approach maintains the social and collaborative nature of scientific inquiry while leveraging AI's capabilities to enhance research productivity.Additionally, we've established guidelines for appropriate use in academic settings, in Appendix A, including recommendations for how departments and institutions can integrate these tools while maintaining research quality and fostering meaningful human collaboration.</p>
<p>By implementing these measures, we aim to contribute positively to the research community, fostering innovation while ensuring ethical responsibility in the development and application of LLMs for scientific discovery.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>We have made extensive efforts to ensure the reproducibility of all results presented in this paper.Firstly, the models discussed in this work, including CycleResearcher and CycleReviewer, will be made available as open-source, along with detailed documentation for setup and usage (See in Section 3.1, Section 3.2, and Appendix F).We provide the training datasets-Review-5k and Research-14k-which will be made publicly accessible to enable researchers to replicate the training process.Each dataset is accompanied by clear instructions regarding its collection, preprocessing steps, and structure (See in Section 2).</p>
<p>Additionally, we have included a thorough description of the model architectures, training procedures, and hyperparameters used in our experiments.Furthermore, we conducted all experiments using publicly available hardware and commonly used deep learning frameworks such as DeepSpeed.To further enhance transparency, we have included a detailed breakdown of evaluation metrics, such as Proxy MAE and Proxy MSE, to ensure that our performance claims can be independently verified.All code, datasets, and model weights will be released with a clear license to promote widespread reproducibility and ethical usage.</p>
<p>A GUIDELINES FOR RESPONSIBLE MODEL USAGE</p>
<p>A.1 CYCLEREVIEWER For CycleReviewer deployment, we recommend a hierarchical review framework that enhances traditional peer review processes while maintaining human oversight.In major conferences, after receiving initial scores from CycleReviewer, Area Chairs (ACs) make preliminary accept/reject recommendations.When significant discrepancies exist between CycleReviewer's evaluation and AC recommendations, Senior Area Chairs (SACs) can request additional AC review without specifying the source of the concern.This system effectively flags submissions requiring careful examination while preserving the integrity of the review process.</p>
<p>CycleReviewer can also enhance award selection processes, particularly for Best Paper designations.</p>
<p>Traditional selection mechanisms, where papers with high average scores or AC nominations form the candidate pool, can sometimes lead to controversial outcomes, as reviewers drawn from limited pools may exhibit strong biases.Given CycleReviewer's training on large-scale review data, it can provide a standardized evaluation metric.When substantial score differences exist between human reviewers and CycleReviewer, award committees should exercise additional caution in their deliberations.Furthermore, CycleReviewer can assist in prioritizing emergency reviewer recruitment, particularly for submissions with low confidence scores or significant score variations, helping maintain trust in the peer review system.</p>
<p>A.2 CYCLERESEARCHER</p>
<p>For CycleResearcher implementation, we recommend a systematic approach that maximizes the model's capabilities while ensuring research quality:</p>
<p>Hypothesis Generation and Refinement: We recommend using CycleResearcher in an iterative cycle for hypothesis development.First, generate multiple candidate hypotheses through broad exploration of the literature and potential research directions.Then, use the model to analyze each hypothesis's feasibility, testing requirements, and potential impact.Finally, employ CycleResearcher to identify potential weaknesses and refinements needed for each hypothesis.This process should be repeated until a robust, well-defined hypothesis emerges.</p>
<p>Experimental Design and Validation: The model should be used to develop comprehensive experimental protocols that include multiple control conditions and account for various confounding variables.We suggest using CycleResearcher to generate at least three variations of each experimental design, focusing on different methodological approaches or measurement techniques.These designs should then be critically evaluated for their ability to falsify the hypothesis, statistical power, and practical feasibility.For computational research, CycleResearcher can help design ablation studies and identify appropriate baselines.For empirical research, it can suggest methods to control for various biases and ensure reproducibility.</p>
<p>Results Analysis and Interpretation: CycleResearcher should be employed to analyze results from multiple perspectives.First, use it to generate various interpretations of the data, including potential alternative explanations.Then, leverage its literature knowledge to connect findings with existing theories and identify potential contradictions or alignments with previous work.Finally, use the model to suggest additional experiments or analyses that could strengthen or challenge the conclusions drawn.</p>
<p>Collaboration and Integration: To maximize research quality, we recommend integrating CycleResearcher into existing research workflows gradually.Begin with using it for literature review and hypothesis generation, then expand to experimental design as confidence in the tool grows.Maintain regular human oversight and discussion of the model's suggestions, treating it as a sophisticated research assistant rather than an autonomous researcher.Document all model-generated suggestions and maintain a clear record of which aspects of the research were AI-assisted versus human-directed.</p>
<p>These guidelines aim to harness CycleResearcher's capabilities while maintaining scientific rigor and research integrity.We emphasize that the model should be used as a tool to augment human research capabilities rather than as a replacement for human scientific judgment.</p>
<p>B LIMITATIONS</p>
<p>Our models are text-only transformer models (Vaswani et al., 2017), focused on handling LaTeX-style text content, but it does not include specific image information.While our models focus on the processes of research planning and academic writing, the envisioned LLM-led scientific discovery does not imply an isolated deployment.Instead, it should function as part of an integrated system.Crucially, human researchers or other agents are still needed to execute the experiments designed by the models and provide corresponding experimental details.We explicitly state that all experimental results mentioned in the generated papers within this work were fabricated by the CycleResearcher model (Hallucinated).Rather than based on real experimental data.This is significantly different from the real scientific research process, which limits the capability of our models in generating verifiable and reproducible scientific knowledge.For future research, we plan to introduce human researchers as experimenters who will collaborate with CycleResearcher in executing actual research plans.On the other hand, to address this issue, in our open-source code repository, we will integrate code from AI Scientist (Lu et al., 2024) and combine it with our models to achieve scientific discovery tasks across all stages to a certain extent.In Appendix I, we have included a paper featuring real-world experiments where the idea, methodology, and experimental design come from CycleResearcher, while the implementation of the experiments was carried out by AI Scientist using GPT-o1.For more details, please refer to Appendix H.</p>
<p>While our framework's core architecture is designed to be domain-agnostic, focusing on universal academic qualities like methodological soundness and clarity of presentation, its current implementation is primarily optimized for machine learning research.This domain specificity stems from practical considerations: machine learning offers abundant high-quality training data through open-access papers and peer reviews.Expanding to other scientific fields would require domain-specific training data and careful adaptation of evaluation criteria.We envision future collaborations with publishers and domain experts to access larger, more cohesive training datasets from diverse fields and adapt our framework accordingly.CycleResearcher's knowledge is updated only up to April 2024, but it has been trained to understand and develop new research plans based on references.This approach partially mitigates the limitations caused by outdated knowledge.However, for CycleReviewer, it is currently an offline model with its most recent knowledge updated until January 2024.This poses challenges in assessing novelty for newer papers and may result in incorrect references or outdated information.In future work, we aim to integrate retrieval-augmented generation (RAG) (Lewis et al., 2020) or other knowledge-enhancement techniques to improve its ability to assess research novelty and accuracy.</p>
<p>Another limitation we recognize is the potential issue of reward hacking, where the policy model could exploit loopholes in the reward model to maximize rewards without genuinely improving the quality of the generated research.Since the policy model and the reward model are not updated simultaneously, this creates a risk where the policy model learns to produce outputs that satisfy the reward criteria but do not necessarily reflect true academic rigor or novelty.For instance, the model might focus on superficial aspects of writing or repeat certain patterns that are disproportionately rewarded by the reward model.This shortcut behavior could undermine the long-term goal of fostering high-quality research output.In future work, we plan to address this by synchronizing updates between the policy and reward models, ensuring that the policy evolves alongside the reward criteria.</p>
<p>C LICENSE C.1 DATA COLLECTION PERMISSIONS</p>
<p>The original paper data and corresponding review comment data used to construct Review-5K are sourced from OpenReview, with a portion of papers originating from ArXiv.Data from OpenReview is distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, which permits us to copy and modify the review comment data.Research-14K data from ArXiv may include licenses such as CC BY 4.0 (Creative Commons Attribution), CC BY-SA 4.0 (Creative Commons Attribution-ShareAlike), CC BY-NC-SA 4.0 (Creative Commons Attribution-NonCommercial-ShareAlike), and CC Zero.Given that we have not modified the original papers, our usage is compliant with the original agreements.We do not claim copyright over these material.</p>
<p>C.2 DISTRIBUTION OF DATA AND MODELS</p>
<p>We acknowledge that the current datasets and models may contain biases or exhibit hallucinations, such as deviating from real-world physical laws or fabricating experimental results.To mitigate the potential negative consequences of open-sourcing, we have taken the following measures:</p>
<p>• Review-5K and Research-14K Datasets: We have applied a specific license to these datasets.</p>
<p>While we are open-sourcing them, we prohibit their use in training any model intended for real-world applications, especially in scenarios that could harm the community.</p>
<p>• CycleReviewer and CycleResearcher Models: We prohibit the use of these models in realworld peer review scenarios and the use of content generated by the CycleResearcher model in submissions without explicit disclosure.</p>
<p>All users are expected to adhere to our usage guidelines and make every effort to avoid causing any negative impact on the community.</p>
<p>D ADDITIONAL EXPERIMENTS D.1 DISTRIBUTION ANALYSIS OF REVIEW SCORES</p>
<p>To validate that CycleReviewer provides meaningful evaluations rather than simply defaulting to median scores, we conducted a comprehensive analysis of score distributions between human reviewers and our model.As illustrated in Figure 4, the distribution patterns of CycleReviewer closely mirror those of human reviewers across minimum, maximum, and average scores, demonstrating its ability to make nuanced quality assessments.</p>
<p>For minimum scores (Figure 4a), both human reviewers and CycleReviewer show a clear trimodal distribution, with peaks around scores of 3, 5, and 6.This pattern indicates that CycleReviewer has  ment, with both systems showing major peaks in the 6 and 8 ranges, suggesting that CycleReviewer can recognize and reward exceptional papers with appropriately high scores.</p>
<p>Most notably, the average score distributions (Figure 5) demonstrate remarkable similarity in their overall shape and variance.Both distributions show a broad spread from 4.0 to 7.0, with primary peaks around 5 and secondary peaks near 6.Furthermore, the presence of papers receiving both very high (&gt;7.0)and very low (&lt;4.0)average scores from CycleReviewer demonstrates its ability to make strong evaluative judgments rather than hedging toward central tendencies.</p>
<p>These distribution patterns provide strong evidence that CycleReviewer has learned to make meaningful quality assessments aligned with human reviewer behavior, rather than simply optimizing for evaluation metrics through conservative, median-centric scoring.The model appears to have captured the complex, multi-faceted nature of paper evaluation, reflecting similar patterns of discrimination and judgment as human experts.</p>
<p>D.2 FURTHER ANALYSIS OF CYCLERESEARCHER PERFORMANCE</p>
<p>In Figure 6, we present a detailed comparison of our CycleResearcher models (12B, 72B, and 123B variants) against the baseline AI Scientist model, evaluated on three key dimensions: soundness, presentation, and contribution.Across these dimensions, all CycleResearcher variants consistently outperform the AI Scientist model, demonstrating significant progress in narrowing the gap between AI-generated research and human expert evaluations.</p>
<p>Focusing first on the soundness score, both CycleResearcher-12B and CycleResearcher-72B achieve an average score of 2.71, surpassing the AI Scientist's 2.48 and closely approaching the preprint papers' score of 2.70.CycleResearcher-123B performs similarly with an average score of 2.70.Notably, CycleResearcher-72B achieves the highest minimum soundness score of 1.86, followed by CycleResearcher-123B at 1.78 and CycleResearcher-12B at 1.73, all significantly outperforming AI Scientist's 1.20.In terms of maximum soundness scores, CycleResearcher-12B leads with 3.17, closely followed by CycleResearcher-72B at 3.13 and CycleResearcher-123B at 3.10, approaching the accepted papers' benchmark of 3.21.</p>
<p>The presentation dimension showcases even stronger performance.CycleResearcher-72B and CycleResearcher-123B particularly excel here, achieving average presentation scores of 2.88 and 2.86 respectively, which approach the accepted paper score of 2.91 and significantly surpass both the preprint paper baseline (2.80) and AI Scientist (2.69).The 72B and 123B variants also demonstrate remarkable consistency with minimum presentation scores of 2.19 and 2.20 respectively, substantially higher than AI Scientist's 1.70.Maximum presentation scores remain competitive, with CycleResearcher-72B reaching 3.31 and CycleResearcher-123B achieving 3.27.</p>
<p>In terms of contribution, which measures research novelty and impact, CycleResearcher-12B leads with an average score of 2.60, followed by CycleResearcher-72B at 2.55 and CycleResearcher-123B at 2.53.All variants surpass the AI Scientist's score of 2.15 and approach the preprint papers' score of 2.57.CycleResearcher-72B demonstrates particularly strong consistency with the highest minimum contribution score of 1.81, while maximum contribution scores remain competitive across all variants (3.07, 3.04, and 3.05 for 12B, 72B, and 123B respectively).</p>
<p>These results demonstrate that all CycleResearcher variants significantly outperform the AI Scientist baseline across all evaluation dimensions.Particularly noteworthy is the strong performance of the 72B variant in presentation and consistency metrics, while the 12B variant excels in contribution scores.The 123B variant shows balanced performance across all metrics, particularly in presentation scores.These findings highlight the effectiveness of our iterative training approach and the importance of model scaling in achieving robust research generation capabilities.</p>
<p>D.3 LITERATURE REVIEW EXPERIMENT</p>
<p>In this subsection, we evaluate the ability of LLMs to generate research papers with substantial and relevant literature citations.Properly citing a wide range of references is essential in academic writing, as it not only demonstrates a comprehensive understanding of the field but also provides a basis for supporting claims and arguments.Therefore, we compare the quantity of references included in papers generated by CycleResearcher-12B and AI Scientist.The results in Table 7 clearly demonstrate that CycleResearcher-12B outperforms AI Scientist in terms of reference inclusion, improving the quantity of cited references by over four times.This improvement underscores the model's enhanced capacity to integrate and cite relevant work, ultimately leading to higher-quality research outputs that are better grounded in the academic literature.Specifically, our model benefits from the inclusion of structured input from reference bib files and corresponding abstracts, allowing it to better understand and cite the necessary literature for building a strong foundation of related work.</p>
<p>E PROXY MSE AS AN EVALUATION METRIC</p>
<p>In peer review, one of the key challenges is assessing the accuracy of review scores in estimating the true quality of a submission, since the ground truth is unknown.To overcome this limitation, we use proxy metrics such as Proxy Mean Squared Error (Proxy MSE) and Proxy Mean Absolute Error (Proxy MAE) to evaluate the performance of review scores, denoted as y.These proxy metrics provide a meaningful approximation by leveraging the assumption that multiple independent review scores for the same submission can act as unbiased estimators of its true quality.</p>
<p>E.1 PROXY MSE DERIVATION</p>
<p>Let y 1 , y 2 , . . ., y n represent the review scores given by n independent reviewers.For any submission, assume that y 1 is the review score of interest (i.e., the score we are evaluating), and that the average of the remaining scores y 2 , y 3 , . . ., y n is a reasonable proxy for the "true" score of the submission.Denote this average as ȳ′ , defined as:
ȳ′ = 1 n − 1 n i=2 y i (5)
Now, to evaluate the performance of the score y 1 , we compute its Proxy MSE, which measures the squared difference between y 1 and the proxy ȳ′ :
Proxy MSE = (y 1 − ȳ′ ) 2(6)
This gives us an approximation of how far y 1 deviates from the true score, assuming that ȳ′ reasonably estimates the submission's quality.</p>
<p>E.2 UNBIASEDNESS AND BIAS OF PROXY MSE</p>
<p>Although ȳ′ is not the true ground truth, it is an unbiased estimator when multiple independent scores are available.The expectation of Proxy MSE can be expressed as:
E[(y 1 − ȳ′ ) 2 ] = E[(y 1 − E[ȳ ′ ]) 2 ] + Var(ȳ ′ ) (7)
Here, the bias in Proxy MSE is equal to the variance of ȳ′ , which we refer to as the "noisy target."This additional variance causes an upward bias in Proxy MSE compared to the true Mean Squared Error (MSE) with respect to the ground truth.Despite this bias, Proxy MSE still allows for meaningful comparisons between different estimators.</p>
<p>E.3 COMPARING TWO ESTIMATORS WITH PROXY MSE</p>
<p>For two review scores y 1 and ỹ1 , we can still use Proxy MSE to compare their accuracy.The difference in their Proxy MSEs can be computed as:
E[(y 1 − ȳ′ ) 2 − (ỹ 1 − ȳ′ ) 2 ] = E[(y 1 − E[ȳ ′ ]) 2 − (ỹ 1 − E[ȳ ′ ]) 2 ] (8)
Because the variance of the proxy target ȳ′ cancels out, the difference in Proxy MSE reflects the difference in MSE between the two estimators.Thus, if y 1 has a smaller Proxy MSE than ỹ1 , we can conclude that y 1 is a better estimator of the submission's true quality in expectation.</p>
<p>E.4 IMPLICATIONS FOR HUMAN REVIEW ACCURACY</p>
<p>By applying Proxy MSE, we can quantitatively assess the accuracy of human reviewers in scoring submissions.Since human judgments can vary significantly, Proxy MSE offers a robust framework for identifying which review scores are closer to the true quality of the submission.It allows us to evaluate the consistency and reliability of different reviewers' scores, which is crucial for improving the peer review process and ensuring more accurate decisions in conference paper acceptance.</p>
<p>F MODEL CARD</p>
<p>We provide a detailed description of the models used in this work, shown in Table 8 specifically focusing on their key characteristics and configurations.These models are designed to tackle complex research-driven tasks.To address this concern, we trained an independent reward model using only the Review-5k test set on Mistral-Large-2, ensuring complete isolation from our training reward model.The results, shown in Table 9, demonstrate relatively minor performance differences (∆ = 0.14 in average score, ∆ = 2.42% in accept rate), suggesting that CycleResearcher has learned genuine research capabilities rather than merely exploiting specific reward patterns.However, we acknowledge that the slight performance degradation with the independent reward model warrants further investigation.Future work could explore techniques such as ensemble reward models or adversarial training to further strengthen robustness against reward exploitation.</p>
<p>These findings provide encouraging evidence for the reliability of our framework while highlighting the importance of continued vigilance against reward hacking in automated research systems.</p>
<p>H SYNERGISTIC INTEGRATION OF CYCLERESEARCHER WITH AI-POWERED EXPERIMENTATION FOR SCIENTIFIC DISCOVERY</p>
<p>To realize a truly comprehensive and automated scientific discovery the CycleResearcher framework can be effectively enhanced by integrating AI-powered experimentation capabilities, potentially leveraging systems like AI Scientist (Lu et al., 2024) for specific tasks.This synergistic approach creates a powerful workflow that leverages the unique strengths of CycleResearcher's research planning, manuscript generation, and iterative refinement alongside AI's potential for assisting with real-world or simulated experimentation.By carefully orchestrating these capabilities, researchers can achieve a more holistic and efficient approach to scientific inquiry, accelerating the pace of discovery and innovation.</p>
<p>The integrated workflow commences with Retriever initiating the research process by undertaking a comprehensive literature review and knowledge synthesis.Tasked with a specific research topic and provided with relevant bibliographic resources, Retriever employs its semantic search capabilities, powered by Semantic Scholar API, to delve deeply into the existing body of knowledge.Retriever meticulously identifies both foundational, "classic" publications and cutting-edge, "frontier" research, providing a nuanced understanding of the field's historical context and current research landscape.This knowledge graph, capturing inter-paper citations and thematic relationships, serves as a robust foundation for subsequent research planning and hypothesis generation within CycleResearcher's core modules.</p>
<p>Building upon the knowledge foundation, CycleResearcher takes center stage to orchestrate the subsequent research phases.This module extracts key insights and trends from the literature review, transforming the knowledge graph into a structured research paper outline.This outline meticulously delineates the essential components of a scientific manuscript, encompassing the research motivation, clearly defined objectives, a comprehensive methodological approach, and a detailed experimental design.The structured nature of this outline ensures a logical flow for the ensuing manuscript generation and provides a blueprint for the experimental phase.Crucially, CycleResearcher can generate experimental designs in a machine-readable JSON format, setting the stage for automated or AI-assisted experiment execution.</p>
<p>The experimental execution phase is where the integration with AI-powered experimentation becomes particularly relevant.CycleResearcher transmits the JSON-formatted experimental design to a dedicated code execution module (AI Scientist).Here, the system can be configured to operate in different modes depending on the desired level of automation and access to external tools.In a fully automated scenario, this module could leverage AI code generation capabilities, potentially drawing upon technologies similar to those explored in AI Scientist, to translate the experimental plan into executable code.This might involve employing AI models to generate or modify code based on the experimental design, allowing for autonomous execution of computational experiments.Alternatively, in a more hybrid approach, CycleResearcher could generate detailed experimental protocols and instructions, while delegating the actual code execution and potentially even code generation assistance to external AI systems or human experimenters.Regardless of the specific implementation, the experimental execution module gathers results which.These results are then meticulously integrated into the paper's experimental section, providing data-driven support for the research claims and informing subsequent analysis and refinement within CycleResearcher's workflow.</p>
<p>Finally, to ensure the rigor and quality of the generated research, the integrated system leverages CycleReviewer's automated peer review simulation.The completed manuscript is submitted to CycleReviewer, which acts as a virtual panel of expert reviewers, providing critical feedback that guides iterative improvement.This feedback loop, inherent to CycleResearcher's design, ensures continuous refinement of the research output.This synergistic integration of CycleResearcher with AI-powered experimentation capabilities, whether through direct code generation or human-AI collaboration, represents a significant step towards realizing a more versatile and efficient scientific discovery process, capable of accelerating research across diverse domains.Throughout the process, we employed CycleResearcher as the thinker, responsible for reading literature, contemplating the research process, and writing experimental reports.GPT-01-preview served as the executor, responsible for implementing the experimental setups planned by CycleResearcher step by step.</p>
<p>I EXAMPLES</p>
<p>I.1.1 OUTLINE</p>
<p>Motivation 1 ?</p>
<p>The increasing scale of deep neural networks has led to a diverse range of behaviors, some of which are predictable, like the improvement in predictive ability with more data, and others are surprising, like grokking and emergent abilities.Understanding these phenomena is crucial for anticipating and steering the impact of increasingly powerful AI systems.Grokking, a phenomenon where overfitting is followed by generalization, has been studied by various works but often in different settings, making it difficult to establish a unified understanding.Emergent abilities, where behaviors appear only at scale, are also important to study.However, previous works have focused on language models, leaving a gap in understanding grokking and emergent abilities in other settings.This paper aims to bridge this gap by studying grokking and emergent abilities in the context of neural networks trained on synthetic algorithmic tasks.The goal is to provide a clear framework for understanding these phenomena and to identify the underlying mechanisms that drive them.</p>
<p>Main Idea 1</p>
<p>⋆</p>
<p>The paper proposes the 'generalization gap' as a way to understand grokking and emergent abilities in neural networks.It defines the generalization gap as the difference in loss on the training and test sets and shows that these phenomena can be observed in simple synthetic algorithmic tasks.The paper introduces four measures of the generalization gap-peakness, inflection point, area of inflection, and length of inflection-to characterize different phases of training.Based on these measures, the paper hypothesizes that grokking and emergent abilities occur when the generalization gap takes a certain form.The hypothesis is validated through experiments on neural networks with varying architecture, parameterization, and training data.The paper also explores the relationship between grokking and double descent, finding that emergent abilities can be seen as a form of grokking, with the two phenomena sharing the same mathematical form.</p>
<p>G e n e r a t e d b y C y c l e R e s e a r c h e r 1 INTRODUCTION</p>
<p>The rise of deep learning has brought remarkable advances alongside puzzling phenomena that challenge our understanding of how neural networks learn.While certain behaviors, such as improved performance with increased data or parameters, follow predictable patterns, others remain enigmatic.Among these, "grokking" (Power et al., 2022) -where models transition from apparent overfitting to sudden generalization -exemplifies the complex dynamics that emerge during training.Understanding these learning phenomena has become increasingly crucial as neural networks grow in scale and capability.When models exhibit unexpected behaviors like grokking or emergent abilities (Wei et al., 2022), traditional metrics often fail to provide adequate insights into the underlying mechanisms.This limitation highlights the need for more sophisticated analytical frameworks that can characterize and predict such behaviors.</p>
<p>The generalization gap -the difference between training and validation performance -offers a promising lens through which to study these phenomena.While previous work has explored various aspects of neural network generalization (?), our approach uniquely focuses on using this gap as a quantitative tool for analyzing learning dynamics.Through systematic experimentation, we demonstrate how this metric can reveal distinct phases in the training process and predict generalization behavior.Our experimental methodology centers on controlled studies using synthetic algorithmic tasks, allowing for precise manipulation of network parameters and training conditions.We examine how various factors -including network architecture, optimization parameters, and regularization techniquesinfluence the generalization gap.This comprehensive approach enables us to isolate and analyze specific aspects of learning behavior while maintaining experimental rigor.</p>
<p>Our primary contributions include:</p>
<p>• These findings have significant implications for both theoretical understanding and practical applications.From a theoretical perspective, our work provides insights into how neural networks learn and when they might exhibit surprising behaviors like grokking.Practically, our framework offers tools for monitoring and potentially predicting model performance during training, which could inform better training strategies and model development approaches.Furthermore, our research suggests that the generalization gap might serve as an early indicator of model behavior, potentially allowing practitioners to anticipate and prepare for changes in model performance before they occur.This could be particularly valuable in resource-intensive training scenarios where early detection of potential issues could save significant computational resources.The insights gained from this work open several promising directions for future research.These include extending our framework to more complex architectures, investigating its applicability to real-world datasets, and exploring potential connections to other phenomena in deep learning.Our results also raise interesting questions about the fundamental nature of neural network learning and the conditions under which different types of generalization behavior emerge.</p>
<p>BACKGROUND</p>
<p>Deep neural networks have evolved significantly, as documented by Goodfellow (2016), revealing increasingly complex behaviors during training.Of particular interest are phenomena like grokking (Power et al., 2022), where networks demonstrate unexpected transitions from apparent overfitting to successful generalization.</p>
<p>The study of generalization in neural networks has focused on various metrics and phenomena.Notably, the emergence of capabilities at scale, suggests that networks can develop unexpected competencies through training.These observations highlight the need for more precise quantification of learning dynamics.</p>
<p>Central to understanding these dynamics is the challenge of measuring and predicting generalization performance.Traditional metrics often fail to capture subtle transitions in learning behavior, particularly when networks exhibit non-linear improvement patterns.This limitation motivates our focus on the generalization gap as a more nuanced measure of network behavior.</p>
<p>PROBLEM SETTING</p>
<p>We formally define the generalization gap as:
Generalization Gap = L train − L test
where L train and L test represent the training and testing loss respectively.This metric serves as our primary tool for analyzing network behavior during training.</p>
<p>In our experimental setup, we consider neural networks with architecture defined by:
y = W L (σ(W L−1 (• • • σ(W 1 x + b 1 ) • • • ) + b L−1 )) + b L (1)
where σ represents the activation function (ReLU or GELU), W l denotes layer weights, and b l represents biases.</p>
<p>For training, we employ both cross-entropy and mean squared error losses.The cross-entropy loss for classification tasks is computed as:
L CE = − i 1 N dout j=1 ⊮ j=yi log(f θ (x i ) j )(2)
Our analysis focuses on synthetic algorithmic tasks that enable controlled experimentation while maintaining sufficient complexity to exhibit interesting learning dynamics.These tasks include basic arithmetic operations and pattern recognition problems, designed to elicit various forms of generalization behavior.Previous work has explored various aspects of neural network performance metrics (Kingma, 2014), but our approach uniquely emphasizes the mathematical characterization of the generalization gap as a predictor of learning behavior.This framework provides a more rigorous foundation for understanding the relationship between training dynamics and generalization performance.</p>
<p>METHOD</p>
<p>Our methodology centers on quantifying and analyzing the generalization gap in neural networks through systematic experimentation.The generalization gap, defined as the difference between training and validation performance, serves as a key metric for understanding learning dynamics.Specifically, for a network with parameters θ at training step t, we define the primary gap measure as Gap(θ t ) = |L cross train (θ t ) − L cross val (θ t )|, where L cross train and L cross val represent the cross-entropy loss on training and validation sets respectively.</p>
<p>To comprehensively analyze this gap, we track multiple characteristics throughout the training process.The loss for each dataset split is computed as L(θ t ) = 1 |D| (x,y)∈D ℓ(f t (x), y; θ t ), where ℓ represents the cross-entropy loss function.We measure several key aspects of the generalization gap evolution: peak magnitude (maximum gap during training), inflection points (where gap behavior changes significantly), area of inflection (integrated gap measure around transition points), and length of inflection (duration of transition periods).</p>
<p>Our experimental framework employs neural networks trained on synthetic algorithmic tasks designed to exhibit varied learning dynamics.The network architecture consists of multiple layers with configurable dimensions, using either ReLU or GELU activation functions.Training utilizes the AdamW optimizer with learning rates ranging from 1e-4 to 5e-4, and weight decay values between 0.05 and 0.5.To ensure robust analysis, we implement dropout regularization with rates varying from 0.1 to 0.3.The training process extends over 5000-7000 update steps, with periodic validation every 100 steps to track the generalization gap progression.</p>
<p>The experimentation focuses on four fundamental tasks: division , subtraction, addition, and permutation operations.For each task, we maintain consistent dataset splits and evaluation protocols, allowing direct comparison of gap behaviors across different configurations.The validation process computes both loss-based metrics and accuracy measures, providing complementary views of model performance.This comprehensive measurement approach enables detailed analysis of how architectural choices and training parameters influence the generalization gap's evolution.</p>
<p>We emphasize gap analysis through carefully tracked metrics over time.For each training trajectory, we compute running statistics of the generalization gap, including its instantaneous magnitude, rate of change, and cumulative behavior.This detailed tracking allows us to identify patterns in how the gap evolves and potentially predicts generalization behavior.The computation of these metrics is standardized across all experiments to ensure comparable results, with particular attention to numerical stability and statistical significance in our measurements.</p>
<p>EXPERIMENTS</p>
<p>To validate our hypothesis, we conducted a series of experiments using varying network architectures and training configurations.Our experiments utilized a comprehensive dataset of labeled examples, maintaining consistent task distributions across all experimental variations.Each dataset was carefully divided into distinct training, validation, and testing sets to ensure reliable evaluation of model performance under different training conditions.</p>
<p>EXPERIMENTAL DESIGN</p>
<p>Architecture Setup In the baseline experiments, we utilize a uniform architecture for all networks consisting of an embedding layer with an input size equal to the input dataset value and a three-layer MLP with a hidden dimension of size 50.This simple architecture choice enables controlled and consistent comparisons across our experiments, focusing solely on the impact of varying training conditions without introducing unnecessary complexities due to complex architectures.Optimizer Configuration We employed the AdamW optimizer for training our networks.The default learning rates for our experiments were 1e-4 for random initialization (tuned from 1e-1 to 1e-5) and 5e-4 for structured initialization (tuned from 1e-3 to 1e-5).Consistent across runs, we use an L2 regularization coefficient (α) of 0.05, a dropout rate (p dropout ) of 0.3, gradient accumulation of 40, and a batch size of 50000.We set the first momentum (β 1 ) and second momentum weights (β 2 ) to 0.9 and 0.999 respectively.Each network was trained for 7000 update steps.</p>
<p>Task Selection Our experiments consider four basic algorithmic tasks: division, subtraction, addition, and permutation.These tasks were chosen to provide a controlled environment for observing and characterizing grokking behavior.Each task involves a well-defined set of operations that the model must learn to apply to new inputs during training.The tasks are as follows:</p>
<p>• x_div_y: Given integers x and y, predict x/y</p>
<p>• x_minus_y: Given integers x and y, predict x − y We chose these tasks to minimize potential confounds and focus directly on the model's ability to generalize its learned operations.The algorithmic nature of these tasks also allows for precise control over the data distribution, providing insights into the model's generalization capabilities across different data configurations.</p>
<p>Dataset Generation</p>
<p>We generate data for each task using the pytorch library.While dataset sizes vary for specific experiments, each dataset is divided into training, validation, and testing sets.This approach ensures that under identical data splits, we can make consistent comparisons across different experimental conditions.</p>
<p>Validation and Testing Procedure To evaluate the performance of our models, we employ both cross-entropy and MSE loss calculations.Our validation process utilizes each model's final checkpoint to compute the loss on the test set.This allows for a detailed comparison between training and validation sets.</p>
<p>Controlled Training Conditions</p>
<p>Our experiments span seven configurations: five configurations with varying hyperparameters and two configurations with different initialization schemes.Each configuration varies in its learning rate (lr), weight decay (α), and dropout rate (p dropout ).We utilize a grid search approach to identify optimal values for lr and α within a fixed budget.The final configuration converges on an optimal point for these two parameters.The results from these experiments are summarized in subsection 4.2.While it was found that the selected parameters are not significantly sensitive to the p dropout parameter, we still explore its effects in the later experiments.The results from this series of experiments are summarized in Table 6.</p>
<p>MAIN RESULTS</p>
<p>In our main experiments, we focus on a core configuration consisting of a three-layer MLP with three different tasks: division, subtraction, and addition.The results from these experiments are summarized in Table 1 and Figure   The generalization gap, as defined in Section 3 is computed as the absolute difference between training and validation losses:
Gap = |L cross train − L cross val |.(3)
Our analysis reveals distinct phases of training, with varying generalization gap behavior across phases.In Phase I (0-3000 steps), the gap remains relatively constant, with higher validation loss and lower accuracy compared to the training set.Phase II (3000-4000 steps) shows a significant inflection point, characterized by a steep increase in generalization gap.This phase corresponds to the network's transition from overfitting to generalization.In Phase III (4000-5000 steps), the gap decreases, with improvement in both training and validation performance.</p>
<p>The results from these experiments demonstrate that we can compute quantitative measures of the generalization gap to predict and characterize grokking behavior.Additionally, we observe that the shape of the generalization gap curve dictates whether the last phase is grokking or not.Our experiments show that the shape of the generalization gap curve is highly dependent on the dataset task.</p>
<p>The results from this set of experiments serve as a baseline for our main investigation.Using these learned parameters, we explore how different factors -like architecture, training data, and regularization -influence the generalization gap and overall model performance.Our experiments provide valuable insights into the conditions under which grokking occurs and the complex interplay of factors that affect its emergence.</p>
<p>EXTENDED DATASET EVALUATION</p>
<p>Here, we double the dataset size for each task to evaluate its impact on the generalization gap.The dataset now consists of 600,000 training examples for each task.The results are summarized in Table 2.</p>
<p>As expected, the extended dataset results confirm that an increase in dataset size extends the duration of Phase II in the generalization gap curves.This extends the network's phase of "learning to generalize" and effectively prevent it from overfitting to noise in the dataset.Additionally, the inflection point and area metrics show consistent relative values across different tasks for a given network.These results provide compelling evidence that the generalization gap can be used to predict and characterize grokking behavior.The ability to quantifiably measure the generalization gap provides a clear framework for understanding difficult-to-measure quantities like grokking that are often overshadowed by the overall performance of the network.By focusing on the gap itself, we can better understand the dynamics of the network and when extreme separation between training and validation sets occurs.</p>
<p>GENERALIZATION GAP ANALYSIS</p>
<p>Our study focused on the following generalization gap metrics to provide insights into generalization behavior.</p>
<p>Peakness measures the peak generalization gap value during training:
Peakness = max t∈T |L cross train (t) − L cross val (t)|.(4)
Inflection Point identifies when the generalization gap transition occurs:
Inflection Point = t where |L cross train (t) − L cross val (t)| ′′ &gt; ϵ.(5)
In our experiments, the gradient threshold (ϵ) is set to 0.01.</p>
<p>Area quantifies the cumulative measure of the inflection phase:
Area = t2 t1 |L cross train (t) − L cross val (t)|dt.(6)
where t 1 and t 2 define the phase where the gap metrics meet the Inflection Point condition.</p>
<p>Length measures the duration of phase transitions: Length = (t 2 − t 1 ).</p>
<p>Using these metrics, we perform an in-depth analysis of the generalization gap's formation and evolution.The results from this analysis are summarized in ?? and Table 3.In ??, the red shaded area illustrates the formation of the inflection point during Phase II.This formation marks the separation between Phase I (high validation loss) and Phase III (lower validation loss).From the results, we observe that peakness measurements reach its peak at the end of Phase II.This observation aligns with our main results, which show that the network begins to separate during this phase.In ??, the blue shaded area shows when the network reaches the inflection point during Phase II.The end of this phase signals the transition from overfitting to generalization.These metrics provide valuable insights into the dynamics of the network and when extreme separation between training and validation sets occurs.To further validate our hypothesis, we conducted experiments evaluating the scale of the network in relation to cross-entropy accuracy.Scale is defined as the number of parameters in the model.Our results are summarized in Figure 6 and Table 4.</p>
<p>From the inflection point in our results, we observe that larger models display an accelerated convergence towards the inflection point.This phenomenon is evident in both training and validation accuracy trajectories.Additionally, we notice that the intersection area measure, another indicator of generalization strength, increases as model scale increases.These results provide evidence that larger models exhibit different emergence characteristics than smaller ones.From the peaks in the figures, we observe that larger models have higher peaks than smaller ones.However, the difference is not as pronounced as the previous metrics.We hypothesize that this phenomenon might be attributed to the fact that the large model converges earlier.These results offer valuable insights into the relationship between model scale and the emergence of capabilities.We observe that the scale of the model plays a significant role in this relationship.However, we also note that the differences in generalization gap metrics, while notable, are not as pronounced as those observed when analyzing other architectural and training parameters.We hypothesize that this phenomenon might be due to the clean and simple nature of our experiments.</p>
<p>INITIALIZATION IMPACT STUDY</p>
<p>In our experiments, we evaluate various initialization schemes.The results are summarized in ?? and Table 5.These experiments maintain the baseline parameters and replace only the initialization scheme.All other configurations, including architecture, learning rates, and weight decay, remain consistent.The results from this experiment confirm our findings that the dataset operates as a significant factor in determining generalization behavior.The results suggest that initialization weights play a role in controlling generalization dynamics.We observe higher generalization gaps for random and structured initialization than tuned initialization.</p>
<p>Our experiments utilize the default He normalization technique, where network weights are initialized using a method that ensures the expected value of the weighted sum of activations from the previous layer is zero.These results provide interesting insights into the relationship between initialization strength and the network's ability to generalize.</p>
<p>THE RELATIONSHIP BETWEEN GROKKING AND DOUBLE DESCENT</p>
<p>The study of generalization in neural networks has gained prominence due to its significant impact on model performance and generalization capabilities.Previous research has examined the relationship between scale and performance metrics, revealing a complex interaction.In our study, we explore the potential link between double descent phenomena (Belkin et al., 2019;Nakkiran et al., 2021) and the emergence of generalization capabilities, particularly in the context of grokking behavior and architecture search in neural networks.Our findings contribute to the understanding of how these phenomena interact and the underlying mechanisms driving them.The results are summarized in Figure 9.As illustrated in Figure 9, the network exhibits a classic double-descent pattern where increasing the parameter count initially leads to improved performance, but eventually, the improvement levels off.The network eventually reaches a state of network saturation, where additional scale does more harm than good.These transitions are marked by distinct inflection points, highlighting the complex interplay between network capacity and dataset characteristics in network generalization.</p>
<p>LOSS FUNCTION COMPARISON</p>
<p>In this study, we explore the impact of different loss functions on generalization using a permutation task.We compare cross-entropy and square losses.Our experiments reveal distinct patterns in training dynamics.The generalization behavior differs significantly between the two loss functions.Varying loss functions inherently result in differences in generalization dynamics.The empirical evidence confirms the influence of the loss function on generalization and is characterized by loss differences during the inflection.</p>
<p>REGULARIZATION EFFECT ANALYSIS</p>
<p>In this study, we aim to understand the effects of regularization on model performance.We focus on two specific regularization techniques, namely weight decay and dropout.Our experiments maintain the "tuned" configuration and apply different regularization parameters.The results are summarized in Figure 12.Our findings suggest that dropout has a lower generalizing effect than weight decay in our architecture.This outcome is consistent with previous literature that has highlighted the shorter distance in the hidden layers between inputs in the Transformer architecture (Geva et al., 2020).The results indicate that weight decay and combined configurations exhibit near-random network performance, by which we mean that the accuracy on the validation set is approximately the same as the accuracy on a randomly generated key.In Table 6, we summarize our network's overall performance and generalization gap calculations.Notably, weight decay enhances validation accuracy compared to the baseline configuration but increases the generalization gap.Combined regularization schemes, however, reduce the generalization gap, though at the expense of overall performance.These results highlight the nuanced influence of regularization on model performance and generalization, offering valuable insights for practitioners.</p>
<p>Placing too much emphasis on the generalization gap can lead to suboptimal model performance.</p>
<p>Our results provide practical guidelines for balancing these objectives.</p>
<p>DISCUSSION</p>
<p>Conclusion Our work successfully establishes the "generalization gap" as a way of mathematically characterizing grokking using simple synthetic algorithmic tasks.By focusing on a small set of This paper was generated by CycleResearcher controlled variables and a simple network architecture, we created a manageable and focused experimental setup.This design allowed us to clearly isolate and examine the impact of these variables on the dynamics of generalization and generalization gaps.Our results clearly establish a strong relationship between the shape of the generalization gap and grokking, highlighting the importance of this gap for understanding grokking.Our focus on simple networks and synthetic data was a conscious choice.It allowed us to limit the number of variables we need to control and explore the fundamental properties of generalization in neural networks.This approach made our experiments more manageable and focused, allowing us to clearly establish a relationship between the generalization gap and grokking.</p>
<p>Our relationship is dependent on architecture and dataset, which is expected given previous research.</p>
<p>The specific dynamics of the generalization gap are sensitive to these factors, making it difficult to make generalizations across heterogeneous architectures and datasets.Our analysis effectively applies to datasets and architectures that follow the approach of exploring algorithmic and universal circuits as represented by our simple MCNs and Transformers.As models scale, the nature of grokking and related phenomena becomes even more complex (Wei et al., 2022).However, we believe that it is ultimately feasible to apply these methods directly, particularly as an increasing number of open-source language models become available.</p>
<p>Ethical Considerations As our experiments focus on controlled training scenarios for neural networks, they do not inherently pose ethical risks.However, we acknowledge a potential conflict of interest in our work, as the practice of training large neural networks may consume considerable energy.This raises concerns regarding the environmental impact of AI development.Our goal in studying grokking behavior is to provide insights for more efficient training and improved generalization capabilities.We aim to devise methods to better utilize smaller networks over larger ones, ultimately contributing to more energy-efficient network training.For researchers interested in applying our framework, the choice of network size and training duration significantly affects computational resource demands, both of which should be carefully considered.We encourage the use of local GPUs for initial experiments and recommend careful spending habits to ensure our work remains accessible and ethical.</p>
<p>Related Work</p>
<p>The study of grokking behavior has gained significant attention, particularly in the context of small transformers trained on addition and modular arithmetic tasks (? Liu et al., 2022;pre, 2023;?).These foundational studies, provided both historical and theoretical grounding.Other works build on these studies (? Olsson et al., 2022;Chughtai et al., 2023;Thilak et al., 2022;Nanda et al., 2023;Michaud et al., 2024;Davies et al., 2023;to, 2023).Our work builds upon research on algorithmic datasets (Power et al., 2022) and introduces the concept of the "generalization gap" and its measures as a way of mathematically characterizing grokking.</p>
<p>Deep double descent phenomena have been the focus of recent studies that explore the relationship between network scale and performance metrics (Nakkiran et al., 2021;Sorscher et al., 2022).</p>
<p>Research has worked to reconcile double descent phenomena with traditional machine learning theory (Belkin et al., 2019), particularly regarding the bias-variance trade-off.These works provide a quantitative framework to assess the impact of network scale on performance.Some studies examine the impact of different loss functions on generalization in neural networks.Zhang et al. (2021) utilizes a toy setting to emphasize how label noise, increasing loss convergence, and dataset size influence generalization in networks.Zhu et al. (2023) explores how network scale and structure affect the generalizing ability of distilled models.These perspectives enrich our understanding of how network scale, dataset conditions, and loss functions interact to shape generalization performance.</p>
<p>Research in the area of emergent abilities in LLMs (Wei et al., 2022;?;McKenzie et al., 2023;Zhou et al., 2024;Xie et al., 2023) emphasizes how suboptimal scaling practices can lead to unexpected outcomes.The findings highlight the importance of a combination of increased model size, dataset quality, and training steps for achieving optimal performance.Additionally, it notes that a network's size and state should be carefully balanced to avoid inverse scaling.In our work, we aim to provide a more comprehensive framework for understanding generalization in neural networks.Our goal is to offer a approach that goes beyond traditional measures like accuracy and focuses on the generalization gap to understand network behavior.</p>
<p>CONCLUSION</p>
<p>Our study advances the understanding of generalization in neural networks by quantifying and analyzing the "generalization gap."The experiments, conducted across varied experimental conditions, show the generalizability of our approach and the predicted trend across diverse factors.The simple designs used in these experiments underscore the critical role of network architecture, optimization parameters, and regularization in exhibiting grokking behavior.To the best of our knowledge, this is the first time that these factors have been systematically studied from this lens.Our results indicate that increasing model complexity and training time extends the length of inflection points in the generalization gap, allowing the model to learn more complex, potentially more general, features.Furthermore, our approach offers quantitative measures that are predictive of generalization behavior, including inflection points in the generalization gap.This is particularly important given the often black-box nature of neural networks, where predictability is crucial for managing and interpreting model performance.</p>
<p>Limitations While our study provides valuable insights into grokking and generalization, it has several limitations.The experiments focus on simple networks and synthetic datasets, which may not fully capture the complexity of real-world applications.This scope limits the direct applicability of our findings to large real-world datasets.Additionally, we examine generalization in fully connected layers while disregarding the impact of network structures like self-attention and batch normalization.Both limitations arise due to the constraints of our computational resources.To overcome this, future studies should explore the impact of more complex architectures and a wider range of datasets.</p>
<p>Investigating the role of self-attention and batch normalization on generalization is an intriguing direction.Another consideration for future research is the exploration of mechanisms in addition to regularization that affect network dynamics.We recognize that our research takes only a partial view of the question and hope to address these gaps in future work.</p>
<p>Future Research Directions Future research can build upon our findings by deepening the exploration of the generalization gap.This could involve expanding the range of tasks, architectures, and training conditions to better understand these phenomena across various settings.Investigating the impact of network initialization and scaling is of particular interest.Additionally, exploring how gap metrics can inform the selection and engineering of datasets for training could provide valuable insights for improving training efficiency and model performance.We also believe that more complex datasets like MNIST or CIFAR-10 will yield interesting results for future research.It is likely that more complex datasets will further emphasize the impact of network scale, initialization, and dataset quality for generalization.</p>
<p>Future Applications Our findings have significant implications for the practical guide of training deep learning models.The ability to predict model performance based on the generalization gap could enable the creation of new strategies for selecting model scale, initializing parameters, and pruning datasets.Practitioners could use our metrics to assess the impact of different training settings, allowing for more efficient model training.This could include early detection of suboptimal behavior, informing model development decisions, and guiding the development of more efficient and reliable AI pipelines.Our work represents a first step in this direction, and we anticipate many exciting directions for future research to explore.</p>
<p>DISCLOSURE</p>
<p>This paper was written with the assistance of CycleResearcher, including but not limited to the introduction, related work, experimental design, and experimental results sections.A portion of the content may have been generated using large language models (LLMs).</p>
<p>Figure 1 :
1
Figure 1: Data Construction pipeline of the Research-14k dataset and Review-5k dataset.The Review-8k dataset includes both the main text (M) and outlines (O) of research papers, covering key components such as motivation, methods, experimental setup, and results.The Research-5k dataset provides 3 reviews and 1 meta-review for each paper • We introduce an iterative reinforcement learning framework that automates the entire research lifecycle, which mirrors the real-world Research-Review-Refinement cycle.Our framework includes CycleResearcher, a policy model for research tasks, and CycleReviewer, a reward model simulating peer reviews.This framework enables large language models (LLMs) to iteratively improve research outputs through a Research-Review-Refinement cycle.• We release two large-scale datasets, Review-5k and Research-14k, which are publicly available and designed to capture the complexity of both peer review and research paper generation in machine learning.These datasets provide valuable resources for evaluating and training models in academic paper generation and review.• We demonstrate that the CycleResearcher model can generate papers with an average quality level close to human-written preprints, achieving an acceptance rate of 31.07%.Furthermore, our CycleReviewer model shows encouraging results with a 26.89% improvement in MAE compared to individual reviewers, suggesting the potential of automated research assessment in mean absolute error (MAE) in research evaluation tasks, setting a new benchmark for automated research assessment.</p>
<p>Figure 2 :
2
Figure 2: Iterative Training Framework.The CycleResearcher model generates Outline (O) and main texts (M) to organize papers, which are evaluated by the CycleReviewer and constructed into preference pairs based on rewards.This whole procedure is then iteratively refined, resulting in progressively enhanced research abilities with each iteration.</p>
<p>Figure 3 :
3
Figure 3: Performance improvement through rejection sampling in generated papers.The graphs show the average, max, and min scores across different numbers of generated papers (1, 5, 10, 50, 100) from CycleResearcher-12B.The red stars represent the performance of the generated papers, showing consistent improvements as the number of samples increases.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Distribution comparison between human reviewers and CycleReviewer scores.(a) Minimum score distributions show similar trimodal patterns, indicating consistent identification of paper weaknesses.(b) Maximum score distributions demonstrate aligned peaks at high-quality ranges, suggesting comparable recognition of exceptional work.(c) Average score distributions exhibit matching spread and variance, reflecting similar overall evaluation patterns.</p>
<p>Figure 6 :
6
Figure 6: We used the CycleReviewer model to test various paper collections, obtaining scores for Soundness, Presentation, and Contribution.</p>
<p>Development of a quantitative framework using generalization gaps to analyze neural network learning dynamics • Extensive empirical validation across diverse architectural configurations and training parameters • Demonstration of the generalization gap's effectiveness in predicting model performance • Analysis of how various hyperparameters influence learning trajectories and generalization behavior G e n e r a t e d b y C y c l e R e s e a r c h e r This paper was generated by CycleResearcher • Identification of consistent patterns in generalization gap evolution across different training scenarios</p>
<p>l e R e s e a r c h e r This paper was generated by CycleResearcher</p>
<p>l e R e s e a r c h e r This paper was generated by CycleResearcher</p>
<p>Figure 3 :
3
Figure 1: (a) Training and validation loss trajectories for division tasks Figure 2: (b) Training and validation accuracy for division tasks Figure 3: Training Dynamics Comparison.Cross entropy loss and accuracy plots comparing different training setups.Shows clear separation between training and validation performance, with characteristic grokking behavior visible in loss curves.</p>
<p>l e R e s e a r c h e r</p>
<p>Figure 4 :Figure 6 :
46
Figure 4: (a) Training accuracy small vs large Figure 5: (b) Validation accuracy small vs large Figure 6: Scale Impact Analysis: (a) Performance vs model size (b) Emergence timing analysis</p>
<p>l e R e s e a r c h e r</p>
<p>FigureFigure 9 :
9
Figure 7: * (a) Training loss Figure 8: * (b) Validation loss Figure 9: Double Descent Patterns: (a) Training and validation loss during double descent (b) Accuracy patterns during transition.</p>
<p>l e R e s e a r c h e r This paper was generated by CycleResearcher</p>
<p>FigureFigure 12 :
12
Figure 10: (a) Training accuracy Figure 11: (b) Validation accuracy Figure 12: Regularization Impact: (a) Training accuracy comparisons across regularization settings (b) Validation accuracy comparisons across regularization settings.</p>
<p>l e R e s e a r c h e r</p>
<p>l e R e s e a r c h e r This paper was generated by CycleResearcher</p>
<p>l e R e s e a r c h e r</p>
<p>Table 1 :
1
Comparison of automated models on generating review.
Method Expert Individual GPT-4o-mini GLM-4 DeepSpeek-2.5 Gemini-1.5-pro Claude-3.5-Sonnet GPT-4o CycleReviewer (123B) 48.77% reduction in Proxy MSE and a 26.89% reduction in Proxy MAE when compared to individual Proxy (Reviewer=n − 1) Proxy (Reviewer=n) Decision MSE ↓ MAE ↓ MSE ↓ MAE ↓ Accuracy ↑ Macro F1 ↑ 2.34 1.16 --75.40% 75.39 3.44 1.53 2.98 1.40 53.06% 34.72 4.45 1.81 3.91 1.70 49.49% 33.10 4.62 1.83 3.72 1.64 45.11% 39.98 3.02 1.34 2.56 1.23 50.98% 50.75 6.40 2.23 5.62 2.12 48.05% 32.44 6.61 2.24 6.53 2.30 52.58% 34.51 1.43 0.92 1.25 0.87 74.24% 73.99 CycleReviewer introduces better quality of review. Table 1 presents the perfor-mance comparison across var-ious models. CycleReviewer demonstrates encouraging re-sults in peer review tasks com-pared to both proprietary sys-tems and individual human re-viewers. Our model shows a reviewers' scores. These metrics suggest that CycleReviewer can provide consistent scoring that complements human expertise. With a decision accuracy of 74.24%, the model demonstrates compet-itive performance compared to other closed-source systems. These results suggest that our model can provide consistent scoring that complements human expertise, showing potential advantages over AI Scientist systems</p>
<p>Table 2 :
2
The evaluation results of a series of papers assessed by CycleReviewer.The range of these scores is 1-10.The CycleReviewer simulates a group of reviewers, and we report the average score for the lowest Overall Score, the average score for the highest Overall Score, and the overall average score.† indicates that all these papers were actually accepted for publication.
Paper Type Conference Accept PapersSource Human ExpertOverall Score Metrics Avg Min Score ↑ Avg Max Score ↑ Avg Score ↑ 3.91 6.98 5.69Accept Rate 100.00%  †Preprint Papers AI Scientist CycleResearcher-12B (Ours) CycleResearcher-72B (Ours) CycleResearcher-123B (Ours)Human Expert AI AI AI AI3.24 2.20 3.47 3.65 3.306.62 5.70 6.75 6.58 6.455.24 4.31 5.36 5.38 5.1529.63% 0.00% 35.13% 33.64% 24.28%</p>
<p>Table 2
2
presents the results of CycleResearcher, which simulates a program committee review process, evaluating papers across the entire score range and ultimately providing a final acceptance decision</p>
<p>Table 3 :
3
The evaluation results of papers reviewed by CycleReviewer across three criteria: Soundness, Presentation, and Contribution.The range of these scores is 1-4.Max.↑ Avg.↑ Min.↑ Max.↑ Avg.↑ Min.↑ Max.↑ Avg.↑
Paper Type Min. ↑ Conference Accept Papers Source Soundness Score Human Expert 2.03 3.21 2.83Presentation Score 2.24 3.35 2.91Contribution Score 1.94 3.17 2.72Preprint Papers AI Scientist CycleResearcher-12B (Ours) CycleResearcher-72B (Ours) CycleResearcher-123B (Ours)Human Expert AI AI AI AI1.76 1.20 1.73 1.86 1.743.16 3.10 3.17 3.13 3.142.70 2.48 2.71 2.71 2.692.07 1.70 1.91 2.19 2.103.28 3.40 3.24 3.31 3.312.80 2.69 2.70 2.88 2.831.75 1.30 1.68 1.81 1.723.13 2.90 3.07 3.04 3.082.57 2.15 2.60 2.55 2.53based on simulated reviews. We report the average scores for the lowest-scoring reviewer, the highest-scoring reviewer, and the overall score. For accepted papers, we use the test set of Research-14k, where all papers have been accepted, serving as a benchmark for human expert standards. For preprint papers, we evaluate 955 submissions from arXiv (Sep. 2024) in the domains of cs.ML, cs.CV, and cs.LG. Additionally, we evaluate the AI Scientist with a collection of 10 research papers generated by GPT-4o and Claude-3.5.CycleResearcher consistently performs better than AI Scientist in terms of automated review metrics. Table 2 shows that CycleResearcher-12B achieves an average score of 5.36, approaching the 5.69 average scores for conference-accepted papers and surpassing AI Scientist's score of 4.31. Notably, it achieves an acceptance rate of 35.13%, which is significantly higher than AI Scientist's 0% acceptance rate, demonstrating its superior ability to produce research-quality output.The comparison across soundness, presentation, and contribution further illustrates the advantages of CycleResearcher. The CycleResearcher-12B achieves an average soundness score of 2.71, surpassing AI Scientist (with GPT-4o)'s 2.48 and closely matching the 2.83 of accepted papers. For presentation and contribution, it attains average scores of 2.70 and 2.60 respectively, outperforming AI Scientist's scores in both metrics (2.69 and 2.15). In contrast, AI Scientist shows significant limitations, particu-larly in minimum scores for soundness (1.20) and contribution (1.30), indicating less consistency in producing quality research. Our model demonstrates greater reliability, with higher minimum scores across all metrics (soundness: 1.73, presentation: 1.91, contribution: 1.68) compared to AI Scientist. These results underscore our model's effectiveness in addressing the challenges of ensuring quality and consistency in AI-generated research.Table 4: Ablation study of different varia-tions of CycleResearcher-12B.Method CycleResearcherAvg Score ↑ 5.36Accept Rate ↑ 35.14%w/o RL w/o Iterative w/o NLL(−0.24) 5.12 (−0.15) 5.21(−5.34%) 29.80% (−2.23%) 32.91%
(−0.45)4.91 (−23.11%)12.03%</p>
<p>Table 5 :
5
Human evaluation scores.ICLR'24 scores are collected from the Review-5K test set and Research-14k.
PapersAvg. Overall Avg. Soundness Avg. Presentation Avg. ContributionICLR'24 Accepted ICLR'24 Submitted6.4 5.5------AI Scientist CycleResearcher3.6 4.82.2 2.62.6 2.81.8 2.2</p>
<p>Table 5
54.4 ETHICAL SAFEGUARD
, the human evaluation scores indicate that CycleResearcher outperformed AI Scientist across all measured dimensions (average overall score of 4.8 vs. 3.6).However, CycleResearcher's performance still remained below the average scores for both ICLR 2024 submissions (5.54) and accepted papers (6.44), suggesting room for further improvement.Nonetheless, the results demonstrate meaningful progress in automated research paper generation, with CycleResearcher showing particular strengths in presentation (2.8) and soundness (2.6) relative to the baseline system.</p>
<p>Table 6 :
6
Detect Performance Comparison in Different Formats.The human samples are from the test sets of Research-14k and Reviewer-5k.
ModelFormatAccuracy F1 ScoreCycleReviewer-123B CycleResearcher-12B CycleResearcher-72B CycleResearcher-123B Research Review Research Research95.14% 98.38% 97.52% 98.88%94.89 98.37 97.49 98.87</p>
<p>Table 7 :
7
Comparison of average references included in papers generated by CycleResearcher-12B and AI Scientist.
MethodAvg References Included ↑Improvement RatioCycleResearcher-12B37.84.61xAI Scientist (GPT-4o)8.21.00x</p>
<p>Table 8 :
8
Models.Description of the models evaluated in this effort.To investigate potential reward exploitation in our framework, we conducted additional experiments focusing on the robustness of CycleResearcher's performance across different reward models.A critical concern in reinforcement learning systems is whether the policy model truly learns desirable behaviors or merely exploits patterns in the reward model used during training.
ModelModel Creator Modality # Parameters # Hidden Layers # Vocab Size #Window Size Knowledge DateWhizResearcher-12B WhizResearcher-72B WhizResearcher-123B Mistral-Large-2 Mistral-Nemo QWEN-2.5Text Text Text12B 72B 12B40 80 88131072 152064 32768128K 128K 128K2024.4 2024.4 2024.4WhizReviewer-123BMistral-Large-2Text12B8832768128K2023.12G ADDITIONAL EXPERIMENTALG.1 ANALYSIS OF REWARD EXPLOITATION</p>
<p>Table 9 :
9
Performance comparison with independent reward model evaluation Model Configuration Avg Min Score Avg Max Score Avg Score Accept Rate
Original Evaluation Independent Reward3.52 3.386.72 6.655.36 5.2931.07% 28.65%</p>
<p>Specifically, we first used the CycleResearcher-12B model to generate the motivation, main idea, paper title, abstract, introduction, and methodology.The model then conducted detailed experimental planning, generating six different experimental groups.Building on this, we used GPT-01-preview model with AI Scientist as the baseline to generate code for these experiments, costing approximately $20 and taking 6 hours on a single A100 GPU server.After obtaining the experimental results, we compiled all results and experimental figures into a JSON file and input it back into the CycleResearcher-12B model.Finally, the CycleResearcher-12B model automatically analyzed the experimental results and wrote the remaining sections of the paper, including experimental analysis, related work, experimental conclusions, and ethical statements.
I.1 UNVEILING GENERALIZATION GAPS: A QUANTITATIVE ANALYSIS OF NEURAL NETWORK LEARNING DYNAMICSIn this subsection, all the following content comes from CycleResearcher-12B and CycleReviewer-123B.Below is a generation example from the CycleResearcher-12B model, with all experiments being gen-uine and valid.</p>
<p>Table 1 :
1
3. Baseline performance metrics for training and validation splits, along with the generalization gap for each metric.
MetricdivisionsubtractionadditionLoss ↓ Accuracy ↑ Loss ↓ Accuracy ↑ Train 0.0194 1.0000 0.0795 0.9945 Val 0.0182 1.0000 0.0486 0.9968Loss ↓ 0.5552 0.1653Gap0.00120.00320.03090.00170.3899</p>
<p>Table 2 :
2
Generalization gap characteristics for different tasks.
This paper was generated by CycleResearcherTaskPeak Inflection PointAreaLengthx_div_y x_minus_y x_plus_y permutation 4.929 4.695 4.693 4.70270.0 70.0 67.33 65.0179.13 673.67 185.32 663.00 164.43 656.33 290.80 669.67</p>
<p>Table 3 :
3
Generalization gap metrics are largely consistent across different architectural configurations.
Configuration Peakness Inflection Area LengthBaseline Tuned LR With Dropout Final4.736 4.732 4.731 4.6992523.81 696.59 740.43 2324.55988.0 973.67 985.67 1389.67</p>
<p>Table 4 :
4
Generalization gap metrics at different model scales show that larger models have higher inflection area and length.
Scale Peakness Inflection Area Inflection LengthSmall Large4.736 4.6992523.81 2324.55988.0 1389.67</p>
<p>Table 5 :
5
Initialization schemes impact the network's ability to generalize.Random and structured initialization result in higher generalization gap compared to tuned initialization, which requires significantly fewer steps to converge.
This paper was generated by CycleResearcherInitialization Final Train Loss Final Val Loss Convergence TimeRandom Structured Tuned0.4560 0.2307 0.00137.8115 5.4082 14.05587000 7000 5000</p>
<p>Table 6 :
6
Regularization effects on generalization.
ConfigTrain Acc Val Acc Gen GapNo Reg Weight Decay Dropout Combined0.9776 0.9724 0.9961 0.57840.0151 0.0046 0.1141 0.00110.9625 0.6678 0.8820 0.5773</p>
<p>This paper was generated by CycleResearcher Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.Lima: Less is more for alignment.Advances in Neural Information Processing Systems, 36, 2024.Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou.Pad: Program-aided distillation specializes large models in reasoning.arXiv preprint arXiv:2305.13888,2023.</p>
<p>Our current implementation delegates experiment execution to code generation models, allowing CycleResearcher to focus on high-level research planning and analysis. Notably, the experimental results generated by CycleResearcher in this work are fabricated and do not represent real experimental data.
https://www.semanticscholar.org/
Supported by Research Center for Industries of the Future, Westlake University.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. GPT-4 technical report. 2023arXiv preprint</p>
<p>arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Predicting paper acceptance via interpretable decision sets. Peng Bao, Weihui Hong, Xuanya Li, Companion Proceedings of the Web Conference 2021. 2021</p>
<p>Research integrity and peer review-past highlights and future directions. Maria K Stephanie L Boughton, Joerg J Kowalczuk, Elizabeth Meerpohl, Elizabeth C Wager, Moylan, 2018</p>
<p>Dendral and meta-dendral: Their applications dimension. G Bruce, Edward A Buchanan, Feigenbaum, Readings in artificial intelligence. Elsevier1981</p>
<p>A supervised approach to extractive summarisation of scientific papers. Ed Collins, Isabelle Augenstein, Sebastian Riedel, arXiv:1706.039462017arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, arXiv:2406.16253Llms assist nlp researchers: Critique paper (meta-) reviewing. 2024arXiv preprint</p>
<p>. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Roziere, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, David Livshits, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Frank Radenovic, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Graeme Lewis Anderson, Gregoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel Ibarra, Ishan Kloumann, Ivan Misra, Jade Evtimov, Jaewon Copet, Jan Lee, Jana Geffert, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Kalyan Jia, Kartikeya Vasuden Alwala, Kate Upasani, Ke Plawiak, Kenneth Li, Kevin Heafield, Khalid Stone, Krithika El-Arini, Kshitiz Iyer, Kuenley Malik, Kunal Chiu, Lauren Bhalla, Laurens Rantala-Yeary, Lawrence Van Der Maaten, Liang Chen, Liz Tan, Louis Jenkins, Lovish Martin, Lubo Madaan, Lukas Malo, Lukas Blecher, Luke Landzaat, Madeline De Oliveira, Mahesh Muzzi, Mannat Pasupuleti, Manohar Singh, Marcin Paluri, Mathew Kardas, Mathieu Oldham, Maya Rita, Melanie Pavlova, Mike Kambadur, Min Lewis, Mitesh Kumar Si, Mona Singh, Naman Hassan, Narjes Goyal, Nikolay Torabi, Nikolay Bashlykov, Niladri Bogoychev, Olivier Chatterji, Onur Duchenne, Patrick Çelebi, Pengchuan Alrassy, Pengwei Zhang, Petar Li, Peter Vasic, Prajjwal Weng, Pratik Bhargava, Praveen Dubal, Punit Krishnan, Puxin Singh Koura, Qing Xu, Qingxiao He, Ragavan Dong, Raj Srinivasan, Ramon Ganapathy, Ricardo Silveira Calderer, Robert Cabral, Roberta Stojnic, Rohit Raileanu, Rohit Girdhar, Romain Patel, Ronnie Sauvestre, Roshan Polidoro, Ross Sumbaly, Ruan Taylor, Rui Silva, Rui Hou, Saghar Wang, Sahana Hosseini, Sanjay Chennabasappa, Sean Singh, Bell, Sonia Seohyun, Sergey Kim, Shaoliang Edunov, Sharan Nie, Sharath Narang, Sheng Raparthy, Shengye Shen, Shruti Wan, Shun Bhosale, Simon Zhang, Soumya Vandenhende, Spencer Batra, Sten Whitman, Stephane Sootla, Suchin Collot, Sydney Gururangan, Tamar Borodinsky, Tara Herman, Tarek Fowler, Thomas Sheasha, Thomas Georgiou, Tobias Scialom, Todor Speckbacher, Tong Mihaylov, Ujjwal Xiao, Vedanuj Karn, Vibhor Goswami, Vignesh Gupta, Viktor Ramanathan, Vincent Kerkez, Virginie Gonguet, Vish Do, Vladan Vogeti, Weiwei Petrovic, Wenhan Chu, Wenyin Xiong, Whitney Fu, Xavier Meers, Xiaodong Martinet, Wang, Ellen Xiaoqing, Xinfeng Tan, Xuchao Xie, Xuewei Jia, Yaelle Wang, Yashesh Goldschlag, Yasmine Gaur, Yi Babaei, Yiwen Wen, Yuchen Song, Yue Zhang, Yuning Li, Zacharie Delpierre Mao, Zheng Coudert, Zhengxing Yan, Zoe Chen, Aaditya Papakipos, Aaron Singh, Abha Grattafiori, Adam Jain, Adam Kelsey, Adithya Shajnfeld, Adolfo Gangidi, Ahuva Victoria, Ajay Goldstand, Ajay Menon, Alex Sharma, Alex Boesenberg, Alexei Vaughan, Allie Baevski, Amanda Feinstein, Amit Kallet, Anam Sangani, Andrei Yunus, Andres Lupu, Andrew Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Ankit Ryan, Annie Ramchandani, Aparajita Franco, Arkabandhu Saraf, Ashley Chowdhury, Ashwin Gabriel, Assaf Bharambe, Azadeh Eisenman, Beau Yazdan, Ben James, Benjamin Maurer, Bernie Leonhardi, Beth Huang, Beto Loyd, Bhargavi De Paola, Bing Paranjape, Bo Liu, Boyu Wu, Braden Ni, Bram Hancock, Brandon Wasti, Brani Spence, Brian Stojkovic, Britt Gamido, Carl Montalvo, Carly Parker, Catalina Burton, Changhan Mejia, Changkyu Wang, Chao Kim, Chester Zhou, Ching-Hsiang Hu, Chris Chu, Chris Cai, Christoph Tindal, Damon Feichtenhofer, Dana Civin, Daniel Beaty, Daniel Kreymer, Danny Li, David Wyatt, David Adkins, Davide Xu, Delia Testuggine, Devi David, Diana Parikh, Didem Liskovich, Dingkang Foss, Duc Wang, Dustin Le, Edward Holland, Eissa Dowling, Elaine Jamil, Eleonora Montgomery, Emily Presani, Emily Hahn, Erik Wood, Esteban Brinkman, Evan Arcaute, Evan Dunbar, Fei Smothers, Felix Sun, Feng Kreuk, Firat Tian, Francesco Ozgenel, Francisco Caggioni, Frank Guzmán, Frank Kanayet, Gabriela Medina Seide, Gabriella Florez, Gada Schwarz, Georgia Badeer, Gil Swee, Govind Halpern, Grant Thattai, Grigory Herman, Sizov, Guangyi, Guna Zhang, Hamid Lakshminarayanan, Han Shojanazeri, Hannah Zou, Hanwen Wang, Haroun Zha, Harrison Habeeb, Helen Rudolph, Henry Suk, Hunter Aspegren, Ibrahim Goldman, Igor Damlaj, Igor Molybog, Irina-Elena Tufanov, Itai Veliche, Jake Gat, James Weissman, James Geboski, Japhet Kohli, Jean-Baptiste Asher, Jeff Gaya, Jeff Marcus, Jennifer Tang, Jenny Chan, Jeremy Zhen, Jeremy Reizenstein, Jessica Teboul, Jian Zhong, Jingyi Jin, Joe Yang, Jon Cummings, Jon Carvill, Jonathan Shepard, Jonathan Mcphie, Josh Torres, Ginsburg ; Ning, Ning Dong, Norman Zhang, Oleg Cheng, Sasha Chernoguz ; Sargun Dhillon, Satadru Sidorov, Saurabh Pan, Seiji Verma, Sharadh Yamamoto, Shaun Ramaswamy, Shaun Lindsay, Sheng Lindsay, Shenghao Feng, Lin, Cindy Shengxin, Shiva Zha, Shuqiang Shankar, Shuqiang Zhang, Sinong Zhang, Sneha Wang, Soji Agarwal, Soumith Sajuyigbe, Stephanie Chintala, Stephen Max, Steve Chen, Steve Kehoe, Sudarshan Satterfield, Sumit Govindaprasad, Sungmin Gupta, Sunny Cho, Suraj Virk, Sy Subramanian, Sydney Choudhury, Tal Goldman, Tamar Remez, Glaser, Nikolay Pavlovich Laptev. Victoria Ajayi, Victoria Montanez, Vijai Mohan2024Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier ; Vinay Satish Kumar, Vishal Mangla, Vítor AlbieroVlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen. Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models</p>
<p>Simulating 500 million years of evolution with a language model. Tomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Jonathan Vincent Q Tran, Marius Deaton, Wiggert, bioRxiv. 2024</p>
<p>Fighting reviewer fatigue or amplifying bias? considerations and recommendations for use of chatgpt and other large language models in scholarly peer review. Mohammad Hosseini, Serge Pjm Horbach, Research integrity and peer review. 8142023</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Towards a universal theory of artificial intelligence based on algorithmic probability and sequential decisions. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning. Springer2024. 2001European conference on machine learning</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Z 'ıdek, Anna Potapenko, nature. 59678732021</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Choi, arXiv:2403.13787Evaluating reward models for language modeling. 2024arXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. Langley, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMIT Press1987. 202438Pat Langley. Integrated systems for computational scientific discovery</p>
<p>RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with AI feedback. Yann Lecun, Yoshua Bengio, Geoffrey Hinton ; Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash, Forty-first International Conference on Machine Learning. 2015. 2024521Deep learning</p>
<p>Automated theory formation in mathematics. Douglas B Lenat, IJCAI. 197777</p>
<p>Eurisko: a program that learns new heuristics and domain concepts: the nature of heuristics iii: program design and results. Douglas B Lenat, Artificial intelligence. 211-21983</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, K" Heinrich, Mike Uttler, Wen-Tau Lewis, Tim Yih, " Rockt, Sebastian Aschel, Douwe Riedel, Kiela, Advances In Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Ai4r: The fifth scientific research paradigm. L I Guojie, Bulletin of Chinese Academy of Sciences (Chinese Version). 3912024</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, 2024</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Scott Daniel, Yian Smith, Yin, NEJM AI. 18AIoa2400196, 2024</p>
<p>Iterative length-regularized direct preference optimization: A case study on improving 7b language models to GPT-4 level. Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Han-Sen Zhong, Wanli Ouyang, 2024a</p>
<p>Ryan Liu, Nihar B Shah, arXiv:2306.00622Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023arXiv preprint</p>
<p>Zhihan Liu, Yubo Chai, Jianfeng Li, arXiv:2408.15512Towards fully autonomous research powered by llms: Case study on simulations. 2024barXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>SimPO: Simple preference optimization with a referencefree reward. Yu Meng, Mengzhou Xia, Danqi Chen, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Chris Hj Aykol ; Michèle B Nuijten, Hartgerink, Sacha Epskamp, and Jelte M Wicherts. The prevalence of statistical reporting errors in psychology. Marcel ALM Van Assen2023. 1985-2013. 2016624Behavior research methods</p>
<p>Teaching science as a process, not a set of facts: A case-study of a first-year science seminar. Gunilla " Oberg, Alice Campbell, Joanne Fox, Marcia Graves, Tara Ivanochko, Linda Matsuchi, Isobel Mouat, Ashley Welsh, Science &amp; Education. 2022</p>
<p>Iterative reasoning preference optimization. Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason E Weston, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. Jed W Edward O Pyzer-Knapp, Peter Wj Pitera, Seiji Staar, Teodoro Takeda, Laino, P Daniel, James Sanders, John R Sexton, Alessandro Smith, Curioni, Computational Materials. 81842022</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, 10.1145/3394486.3406703Proceedings Of The 26th ACM SIGKDD International Conference On Knowledge Discovery &amp; Data Mining, KDD '20. Of The 26th ACM SIGKDD International Conference On Knowledge Discovery &amp; Data Mining, KDD '20New York, NY, USA2020. 2020Association for Computing Machinery. ISBN 9781450379984</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>Gpt4 is slightly helpful for peer-review assistance: A pilot study. Zachary Robertson, arXiv:2307.054922023arXiv preprint</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 62579952024</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Peer review: a flawed process at the heart of science and journals. Richard Smith, Journal of the royal society of medicine. 9942006</p>
<p>Analysis of the icml 2023 ranking data: Can authors' opinions of their own papers assist peer review in machine learning?. Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie J Su, 2024</p>
<p>Collective predictive coding as model of science: Formalizing scientific activities towards generative science. Tadahiro Taniguchi, Shiro Takagi, Jun Otsuka, Yusuke Hayashi, Hiro Taiyo Hamada, 2024</p>
<p>5: A party of foundation models. Qwen Team, Qwen2, September 2024</p>
<p>Ai-driven review systems. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, arXiv:2408.10365Evaluating llms in scalable and bias-aware academic reviews. 2024arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances In Neural Information Processing Systems. I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023a</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.14259Scimon: Scientific inspiration machines optimized for novelty. 2023barXiv preprint</p>
<p>Lora-ga: Low-rank adaptation with gradient approximation. Shaowen Wang, Linxi Yu, Jian Li, 2024a</p>
<p>Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, arXiv:2406.10252Large language models can automatically write surveys. 2024barXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, 10.18653/v1/2023.findings-emnlp.167Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, Tong Zhang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Ai for open science: A multi-agent perspective for ethically translating data to knowledge. Chase Yakaboski, Gregory Hyde, Clement Nyanhongo, Eugene SantosJr, arXiv:2310.188522023arXiv preprint</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, arXiv:2407.10671Fei Huang, et al. Qwen2 technical report. 2024aarXiv preprint</p>
<p>. Xu Yang, Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, Xiao Yang, Shizhao Sun, Weiqing Liu, Jiang Bian, 2024bCollaborative evolving strategy for automatic data-centric development</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Self-rewarding language models. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, Jason E Weston, Forty-first International Conference on Machine Learning. 2024</p>
<p>Investigating fairness disparities in peer review: A language model enhanced approach. Jiayao Zhang, Hongming Zhang, Zhun Deng, Dan Roth, arXiv:2211.063982022arXiv preprint</p>
<p>Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv:2408.15240Generative verifiers: Reward modeling as next-token prediction. 2024arXiv preprint</p>
<p>Moss: Enabling code-driven evolution and context management for ai agents. Ming Zhu, Yi Zhou, 2024</p>
<p>REFERENCES Predicting grokking long before it happens: A look into the loss landscape of models which grok. 2023. To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. Minjun Zhu, Linyi Yang, Yifan Wei, Ningyu Zhang, Yue Zhang, arXiv:2410.103432024. 2023arXiv preprintLocking down the finetuned llms safety</p>
<p>Reconciling modern machine-learning practice and the classical bias-variance trade-off. Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal, 2019Proceedings of the National Academy of Sciences116</p>
<p>A toy model of universality: Reverse engineering how networks learn group operations. Bilal Chughtai, Lawrence Chan, Neel Nanda, International Conference on Machine Learning. PMLR2023</p>
<p>Unifying grokking and double descent. Xander Davies, Lauro Langosco, David Krueger, arXiv:2303.061732023arXiv preprint</p>
<p>Transformer feed-forward layers are key-value memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, arXiv:2012.149132020arXiv preprint</p>
<p>Ian Goodfellow, Deep learning. 2016</p>
<p>P Diederik, Kingma, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Towards understanding grokking: An effective theory of representation learning. Ziming Liu, Ouail Kitouni, Eric Niklas S Nolte, Max Michaud, Mike Tegmark, Williams, Advances in Neural Information Processing Systems. 202235</p>
<p>Alexander Ian R Mckenzie, Michael Lyzhov, Alicia Pieler, Aaron Parrish, Ameya Mueller, Euan Prabhu, Mclean, arXiv:2306.09479Alisa Liu, et al. Inverse scaling: When bigger isn't better. Aaron Kirtland, Alexis Ross2023arXiv preprint</p>
<p>The quantization model of neural scaling. Eric Michaud, Ziming Liu, Uzay Girit, Max Tegmark, Advances in Neural Information Processing Systems. 202436</p>
<p>Deep double descent: Where bigger models and more data hurt. Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever, Journal of Statistical Mechanics: Theory and Experiment. 2021121240032021</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, J Steinhardt, ArXiv, abs/2301.052172023</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, arXiv:2209.11895-context learning and induction heads. 2022arXiv preprint</p>
<p>Grokking: Generalization beyond overfitting on small algorithmic datasets. Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, 2022</p>
<p>Beyond neural scaling laws: beating power law scaling via data pruning. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, Ari Morcos, 202235</p>
<p>The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon. Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, Joshua Susskind, arXiv:2206.048172022arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Data selection for language models via importance resampling. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy S Liang, Advances in Neural Information Processing Systems. 202336</p>
<p>Understanding deep learning (still) requires rethinking generalization. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Communications of the ACM. 6432021</p>            </div>
        </div>

    </div>
</body>
</html>
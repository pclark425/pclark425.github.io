<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-625 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-625</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-625</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-d5901f15a0214b50e6a0085337e49a9b966775a7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d5901f15a0214b50e6a0085337e49a9b966775a7" target="_blank">Neurosymbolic AI: the 3rd wave</a></p>
                <p><strong>Paper Venue:</strong> Artificial Intelligence Review</p>
                <p><strong>Paper TL;DR:</strong> This paper relates recent and early research in neurosymbolic AI with the objective of identifying the most important ingredients of neurosympolic AI systems and focuses on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning.</p>
                <p><strong>Paper Abstract:</strong> Current advances in Artificial Intelligence (AI) and Machine Learning have achieved unprecedented impact across research communities and industry. Nevertheless, concerns around trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neurosymbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability by offering symbolic representations for neural models. In this paper, we relate recent and early research in neurosymbolic AI with the objective of identifying the most important ingredients of neurosymbolic AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. Finally, this review identifies promising directions and challenges for the next decade of AI research from the perspective of neurosymbolic computing, commonsense reasoning and causal explanation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e625.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e625.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Tensor Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic approach that grounds first-order logic formulas into differentiable constraints over tensor embeddings, using many-valued logic to turn symbolic statements into loss terms for neural learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Tensor Networks (LTN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LTN is a hybrid system where logical statements in (fragments of) first-order logic are interpreted in a continuous many-valued semantics and translated into differentiable real-valued constraints that are added to the loss function of a neural network. The neural module learns embeddings (tensors) for predicates, functions and constants; the logic module provides constraints and queries the trained network about the satisfaction of formulas. The overall architecture is modular: a neural embedding learner plus a symbolic logic layer that communicates via constraints implemented as differentiable penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order logic statements (many-valued semantics) used as constraints; logical formulas are represented symbolically and interpreted as soft constraints over embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks that learn tensor embeddings for predicates/functions/constants (tensor-based embedding models) trained via gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Symbol-to-loss integration: logical formulas are compiled into differentiable constraints (loss terms) using a many-valued logic interpretation in [0,1], enabling end-to-end gradient-based optimization where logic acts as a regularizer and as post-training queries.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables the system to incorporate prior symbolic knowledge as soft constraints during learning, to query arbitrary first-order statements against learned embeddings, and to combine continuous perception-grounded representations with symbolic-level queries for extrapolation and constrained generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General relational learning and reasoning tasks combining perceptual data and symbolic queries (paper describes framework rather than a single benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to improve extrapolation and constrain learning via symbolic knowledge; intended to generalize better than purely data-driven embeddings when logical constraints capture invariant relationships (out-of-distribution generalization claimed qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides a symbolic interface to query learned knowledge; logical constraints and formula satisfaction give a form of explainability and allow inspection of which formulas are (soft-)satisfied by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Logic is used as soft constraints (loss terms) rather than as a fully-integrated symbolic reasoner; theorem proving is left to symbolic counterpart, and mapping full first-order logic faithfully into differentiable losses has scalability and expressivity challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Many-valued logic grounding: symbolic formulas interpreted in [0,1] semantics and translated into differentiable constraints that guide neural learning; views logic and neural modules as communicating components of a hybrid system.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neurosymbolic AI: the 3rd wave', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e625.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e625.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>deepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>deepProbLog (ProbLog with neural predicates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system that augments a probabilistic logic programming framework by letting neural networks act as probabilistic predicates, enabling integration of perception (neural) with symbolic probabilistic reasoning (ProbLog).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>deepProbLog / ProbLog with neural predicates</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An architecture where a symbolic probabilistic logic engine (ProbLog) performs high-level probabilistic logical inference and allows nodes/predicates to be implemented by neural networks that process perceptual inputs. The neural components provide probabilistic facts or predicate evaluations that feed into the symbolic probabilistic inference tree.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ProbLog probabilistic logic programming (probabilistic facts and rules; symbolic probabilistic inference).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks used as predicate evaluators or to produce probabilistic facts from raw perceptual input (e.g., image classifiers), trained by gradient methods possibly in a joint/loosely-coupled fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Loosely-coupled hybrid integration: neural modules supply probabilities/predicate outputs to the symbolic probabilistic engine which performs reasoning; some approaches allow replacing nodes in the symbolic inference tree by neural networks so the modules communicate via input/output interfaces (not necessarily end-to-end differentiable across the entire system).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines robust perceptual processing with explicit probabilistic logical inference, enabling handling of uncertainty in symbolic reasoning while leveraging neural perception; supports interpretability at the symbolic level and probabilistic reasoning about perceptual outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Examples / applications combining perception and probabilistic logic (e.g., image-based reasoning tasks discussed as representative use cases); paper discusses framework rather than a single benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Hybridity intended to enable better compositional and abstract generalization by delegating perception to neural modules and higher-level reasoning to the symbolic probabilistic engine; expected to be more robust to distribution shift in symbolic aspects, qualitatively described.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic ProbLog component provides an explainable reasoning trace and probabilistic explanations over neural predicate outputs; neural parts remain less transparent but are isolated to predicate evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Integration can be loose; full end-to-end differentiability across probabilistic reasoning and neural perception may not be feasible, and probabilistic inference can be computationally heavy for large problems.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division of labor: continuous probabilistic perception (neural) feeding discrete probabilistic logical inference (ProbLog), leveraging strengths of each paradigm for uncertainty-aware reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neurosymbolic AI: the 3rd wave', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e625.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e625.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NS-CL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Symbolic Concept Learner (Neuro-symbolic concept learner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system where perception modules (neural) detect objects/concepts and a symbolic program induction/reasoner composes those concepts to answer compositional visual question-answering queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Symbolic Concept Learner (NS-CL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular pipeline where neural perception networks learn object/concept embeddings from images (grounding visual elements as symbols) and a symbolic reasoning / program execution module composes these symbols according to induced programs to perform question answering and compositional reasoning. Integration is via the exchange of symbol tokens/groundings between perception and symbolic reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic program representation and a symbolic executor/reasoner that manipulates discrete symbols and executes compositional programs (logic-like symbolic operations for queries).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception modules (e.g., convolutional networks) that learn to detect objects and map perceptual features to symbolic concepts (embeddings), trained with gradient-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular interface where neural perception produces symbolic groundings that are consumed by a symbolic program induction and execution module (loosely-coupled hybrid); the symbolic module may be learned or induced from supervision, and the modules interact through discrete symbol representations.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables compositional and interpretable reasoning over perceptual inputs by separating perception (neural) and symbolic composition/execution, facilitating systematic generalization and interpretable program-like explanations for outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Compositional visual question answering and scene understanding (examples in the literature; paper cites NS-CL as an example).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Aimed at systematic compositional generalization (extrapolation to novel compositions of known concepts) via symbolic program induction operating over neural-grounded symbols; claimed to generalize better on compositional tasks than end-to-end neural baselines (qualitative in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides program-like, human-readable explanations (the induced program) and symbolic traces of reasoning using the symbolic executor; perception remains in neural modules but outputs symbolic tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on good symbol grounding from neural perception; hybrid nature can be brittle if perception fails or if interfaces between perception and symbolic modules are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Modularity and symbol grounding: perception grounds symbols for a symbolic reasoner to perform compositional program-like reasoning, leveraging the complementary strengths of neural perception and symbolic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neurosymbolic AI: the 3rd wave', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e625.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e625.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differentiable Theorem Proving</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Unification / Neural Theorem Proving</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that implement theorem proving and unification operations inside differentiable neural architectures, enabling reasoning operations to be trained via gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Differentiable Theorem Proving / Differentiable Unification</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A class of systems that embed logical unification and proof search into neural networks by making unification and proof steps differentiable (e.g., soft unification, differentiable backward/forward chaining), allowing networks to learn to perform logical inference operations and enabling end-to-end training of reasoning behaviors within a neural substrate.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logical structures (clauses, rules) and unification/proof operations are represented implicitly and approximated within a differentiable many-valued framework rather than as discrete symbolic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural network architectures augmented with differentiable modules that mimic unification/theorem-proving steps (e.g., attention-like mechanisms or learned soft-unification operators), trained by gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Full integration inside neural architecture: symbolic reasoning primitives (unification, proof search) are implemented as differentiable operators so reasoning becomes part of the neural computation and can be optimized jointly with perception.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Potential to learn approximate logical inference within continuous models, permitting gradient-based learning of reasoning behaviors and the combination of perception and reasoning within a single differentiable pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Tasks requiring logical deduction and relational reasoning where differentiable proving substitutes classical discrete proof search (review mentions such systems generally).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Aims to enable learned approximate reasoning that may generalize across similar proof patterns; however, scalability and faithful simulation of full discrete logical inference are challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Because proofs are represented approximately inside continuous weights, extracting exact symbolic proofs is difficult; interpretability depends on additional extraction or constraints to yield readable symbolic traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Differentiable implementations of unification and theorem proving face computational efficiency and expressivity challenges; exact symbolic guarantees are generally lost and prior attempts had limited practical efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Approximation of symbolic proof operations by differentiable many-valued logic and soft unification, enabling gradient-based learning of reasoning within a neural computation paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neurosymbolic AI: the 3rd wave', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e625.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e625.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logical Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Networks (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that establishes a one-to-one correspondence between neurons and elements of logical formulas, providing a tightly coupled localist neural-symbolic mapping for logical reasoning inside networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logical Neural Networks (LNN / neuron-to-formula mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tightly-coupled neurosymbolic systems where logical formula elements (atoms, connectives) are mapped directly to neural units, enabling the neural architecture to implement symbolic logic computations in a localist fashion; the mapping can support reasoning inside the network with a clear correspondence to symbolic constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit logical formulas (often propositional or restricted fragments) represented locally with one-to-one neuron-to-logic element correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural network execution that directly implements logical connectives and inference via neural activations; training may include weight initialization from logic or further learning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Compilation of symbolic knowledge into the network's structure/weights (tight localist integration) so that logic is executed by the neural substrate; may include knowledge-based initialization or constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Provides clearer interpretability via direct mapping, allows reasoning to be performed inside the neural model with a traceable correspondence to logic, and can combine learning with symbolic guarantees when mappings are exact.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Symbolic reasoning and tasks where localist logical correspondence is appropriate (mentioned as Type 4 examples in ontology of systems).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Potential to preserve symbolic extrapolation and exact reasoning capabilities while benefiting from neural computation; however, often limited to propositional or restricted logic fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability due to the explicit neuron-to-formula correspondence; symbolic traces can be directly inspected.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability and expressiveness limits (difficulty implementing full first-order or higher-order logic); mapping complex symbolic operations into large networks can be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Compilation/decompilation view: symbolic formulas compiled into network architecture/weights (and extracted back), supporting a principled correspondence between neural computation and logical semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neurosymbolic AI: the 3rd wave', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e625.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e625.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tensorization / TPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tensor Product Representations (tensorization methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that encode symbolic structures into high-dimensional tensors so that symbolic combinations can be implemented as tensor operations within neural networks, used as soft constraints or embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Tensor Product Representations / tensorization methods</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approaches that map symbolic roles and fillers into tensor spaces (tensorization) enabling representation of compositional symbolic structures as vectors/tensors; these representations can be used as embeddings or soft constraints inside neural networks and integrated with gradient learning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic structures (roles, fillers, compositional formulas) encoded into tensorial symbolic representations that capture structure in a localist-to-distributed mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks that operate over tensorized embeddings and are trained with gradient descent to manipulate and learn from these structured embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Symbol-to-embedding compilation: symbolic structures are tensorized into distributed representations that the neural network uses and refines; tensorized knowledge can act as soft regularizers or be part of network initialisation.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables neural models to represent and manipulate structured symbolic information in distributed form, supporting compositionality and richer structure-aware learning than flat embeddings alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Compositional reasoning and relational learning tasks requiring structured representations (review-level mention).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Aims to improve compositional generalization by embedding symbolic structure explicitly into the neural substrate; expected to aid systematic extrapolation when structure is well captured.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Distributed tensor encodings are less directly interpretable than explicit symbolic formulas, though they preserve structural information which can be analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Choosing appropriate tensor encodings and scaling to large symbolic structures can be difficult; interpretability and exact symbolic guarantees are reduced compared to localist mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Tensorization principle: encode symbolic compositional structures into tensor spaces so they can be processed by gradient-based neural learners (bridging localist and distributed representations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neurosymbolic AI: the 3rd wave', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e625.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e625.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaGo (example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaGo (deep neural networks + Monte Carlo tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exemplar hybrid system where deep neural networks provide policy/value estimates and a symbolic search algorithm (Monte Carlo tree search) performs combinatorial planning, demonstrating effective division of labor between learning and search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaGo (neural + symbolic search hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A Type-2 hybrid architecture where deep neural networks (policy and value networks) are tightly integrated with a symbolic/planning module (Monte Carlo Tree Search, MCTS). The neural models provide priors and evaluations for game states, while MCTS performs combinatorial search to select moves; the system leverages learning for pattern recognition and search for exact combinatorial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Combinatorial search algorithm (Monte Carlo Tree Search) used for planning and decision making (symbolic-like discrete search over game trees).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Deep neural networks (policy and value networks, convolutional architectures) trained by reinforcement learning and supervised learning to provide move priors and state evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular hybrid: neural networks produce estimates and priors that guide the symbolic search algorithm (MCTS); interaction is via network outputs feeding into the search heuristics rather than via symbolic-to-neural compilation.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines fast perceptual/heuristic evaluation from neural networks with exacting combinatorial search from MCTS, enabling state-of-the-art planning and decision making in large discrete domains (games).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Board game playing (Go) and other game-playing benchmarks where planning over combinatorial spaces is required.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Neural components generalize from self-play to unseen positions, while MCTS enables robust decision-making by explicit search, improving out-of-distribution robustness in planning contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>MCTS provides a traceable search trajectory (plans/rollouts) for decisions, aiding interpretability of choices; neural networks remain less transparent but their outputs (priors/values) can be examined.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Hybrid coupling can be computationally intensive; reliance on large amounts of self-play data for neural training and heavy search at inference time are practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division of labor between fast approximate pattern-recognition (neural) and slow exact combinatorial reasoning/search (symbolic), illustrating complementary strengths of learning and search.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neurosymbolic AI: the 3rd wave', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic Tensor Networks: Deep Learning and Logical Reasoning From Data and Knowledge <em>(Rating: 2)</em></li>
                <li>DeepProbLog: Neural Probabilistic Logic Programming <em>(Rating: 2)</em></li>
                <li>Neuro-Symbolic Concept Learner <em>(Rating: 2)</em></li>
                <li>Mastering the game of Go with deep neural networks and tree search <em>(Rating: 2)</em></li>
                <li>Differentiable Theorem Proving <em>(Rating: 1)</em></li>
                <li>Tensor Product Representations and Neural Approaches to Structured Symbols <em>(Rating: 1)</em></li>
                <li>Logical Neural Networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-625",
    "paper_id": "paper-d5901f15a0214b50e6a0085337e49a9b966775a7",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "LTN",
            "name_full": "Logic Tensor Networks",
            "brief_description": "A neurosymbolic approach that grounds first-order logic formulas into differentiable constraints over tensor embeddings, using many-valued logic to turn symbolic statements into loss terms for neural learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Logic Tensor Networks (LTN)",
            "system_description": "LTN is a hybrid system where logical statements in (fragments of) first-order logic are interpreted in a continuous many-valued semantics and translated into differentiable real-valued constraints that are added to the loss function of a neural network. The neural module learns embeddings (tensors) for predicates, functions and constants; the logic module provides constraints and queries the trained network about the satisfaction of formulas. The overall architecture is modular: a neural embedding learner plus a symbolic logic layer that communicates via constraints implemented as differentiable penalties.",
            "declarative_component": "First-order logic statements (many-valued semantics) used as constraints; logical formulas are represented symbolically and interpreted as soft constraints over embeddings.",
            "imperative_component": "Neural networks that learn tensor embeddings for predicates/functions/constants (tensor-based embedding models) trained via gradient descent.",
            "integration_method": "Symbol-to-loss integration: logical formulas are compiled into differentiable constraints (loss terms) using a many-valued logic interpretation in [0,1], enabling end-to-end gradient-based optimization where logic acts as a regularizer and as post-training queries.",
            "emergent_properties": "Enables the system to incorporate prior symbolic knowledge as soft constraints during learning, to query arbitrary first-order statements against learned embeddings, and to combine continuous perception-grounded representations with symbolic-level queries for extrapolation and constrained generalization.",
            "task_or_benchmark": "General relational learning and reasoning tasks combining perceptual data and symbolic queries (paper describes framework rather than a single benchmark).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Designed to improve extrapolation and constrain learning via symbolic knowledge; intended to generalize better than purely data-driven embeddings when logical constraints capture invariant relationships (out-of-distribution generalization claimed qualitatively).",
            "interpretability_properties": "Provides a symbolic interface to query learned knowledge; logical constraints and formula satisfaction give a form of explainability and allow inspection of which formulas are (soft-)satisfied by the model.",
            "limitations_or_failures": "Logic is used as soft constraints (loss terms) rather than as a fully-integrated symbolic reasoner; theorem proving is left to symbolic counterpart, and mapping full first-order logic faithfully into differentiable losses has scalability and expressivity challenges.",
            "theoretical_framework": "Many-valued logic grounding: symbolic formulas interpreted in [0,1] semantics and translated into differentiable constraints that guide neural learning; views logic and neural modules as communicating components of a hybrid system.",
            "uuid": "e625.0",
            "source_info": {
                "paper_title": "Neurosymbolic AI: the 3rd wave",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "deepProbLog",
            "name_full": "deepProbLog (ProbLog with neural predicates)",
            "brief_description": "A hybrid system that augments a probabilistic logic programming framework by letting neural networks act as probabilistic predicates, enabling integration of perception (neural) with symbolic probabilistic reasoning (ProbLog).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "deepProbLog / ProbLog with neural predicates",
            "system_description": "An architecture where a symbolic probabilistic logic engine (ProbLog) performs high-level probabilistic logical inference and allows nodes/predicates to be implemented by neural networks that process perceptual inputs. The neural components provide probabilistic facts or predicate evaluations that feed into the symbolic probabilistic inference tree.",
            "declarative_component": "ProbLog probabilistic logic programming (probabilistic facts and rules; symbolic probabilistic inference).",
            "imperative_component": "Neural networks used as predicate evaluators or to produce probabilistic facts from raw perceptual input (e.g., image classifiers), trained by gradient methods possibly in a joint/loosely-coupled fashion.",
            "integration_method": "Loosely-coupled hybrid integration: neural modules supply probabilities/predicate outputs to the symbolic probabilistic engine which performs reasoning; some approaches allow replacing nodes in the symbolic inference tree by neural networks so the modules communicate via input/output interfaces (not necessarily end-to-end differentiable across the entire system).",
            "emergent_properties": "Combines robust perceptual processing with explicit probabilistic logical inference, enabling handling of uncertainty in symbolic reasoning while leveraging neural perception; supports interpretability at the symbolic level and probabilistic reasoning about perceptual outputs.",
            "task_or_benchmark": "Examples / applications combining perception and probabilistic logic (e.g., image-based reasoning tasks discussed as representative use cases); paper discusses framework rather than a single benchmark.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Hybridity intended to enable better compositional and abstract generalization by delegating perception to neural modules and higher-level reasoning to the symbolic probabilistic engine; expected to be more robust to distribution shift in symbolic aspects, qualitatively described.",
            "interpretability_properties": "Symbolic ProbLog component provides an explainable reasoning trace and probabilistic explanations over neural predicate outputs; neural parts remain less transparent but are isolated to predicate evaluation.",
            "limitations_or_failures": "Integration can be loose; full end-to-end differentiability across probabilistic reasoning and neural perception may not be feasible, and probabilistic inference can be computationally heavy for large problems.",
            "theoretical_framework": "Division of labor: continuous probabilistic perception (neural) feeding discrete probabilistic logical inference (ProbLog), leveraging strengths of each paradigm for uncertainty-aware reasoning.",
            "uuid": "e625.1",
            "source_info": {
                "paper_title": "Neurosymbolic AI: the 3rd wave",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "NS-CL",
            "name_full": "Neuro-Symbolic Concept Learner (Neuro-symbolic concept learner)",
            "brief_description": "A hybrid system where perception modules (neural) detect objects/concepts and a symbolic program induction/reasoner composes those concepts to answer compositional visual question-answering queries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neuro-Symbolic Concept Learner (NS-CL)",
            "system_description": "A modular pipeline where neural perception networks learn object/concept embeddings from images (grounding visual elements as symbols) and a symbolic reasoning / program execution module composes these symbols according to induced programs to perform question answering and compositional reasoning. Integration is via the exchange of symbol tokens/groundings between perception and symbolic reasoner.",
            "declarative_component": "Symbolic program representation and a symbolic executor/reasoner that manipulates discrete symbols and executes compositional programs (logic-like symbolic operations for queries).",
            "imperative_component": "Neural perception modules (e.g., convolutional networks) that learn to detect objects and map perceptual features to symbolic concepts (embeddings), trained with gradient-based methods.",
            "integration_method": "Modular interface where neural perception produces symbolic groundings that are consumed by a symbolic program induction and execution module (loosely-coupled hybrid); the symbolic module may be learned or induced from supervision, and the modules interact through discrete symbol representations.",
            "emergent_properties": "Enables compositional and interpretable reasoning over perceptual inputs by separating perception (neural) and symbolic composition/execution, facilitating systematic generalization and interpretable program-like explanations for outputs.",
            "task_or_benchmark": "Compositional visual question answering and scene understanding (examples in the literature; paper cites NS-CL as an example).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Aimed at systematic compositional generalization (extrapolation to novel compositions of known concepts) via symbolic program induction operating over neural-grounded symbols; claimed to generalize better on compositional tasks than end-to-end neural baselines (qualitative in this review).",
            "interpretability_properties": "Provides program-like, human-readable explanations (the induced program) and symbolic traces of reasoning using the symbolic executor; perception remains in neural modules but outputs symbolic tokens.",
            "limitations_or_failures": "Relies on good symbol grounding from neural perception; hybrid nature can be brittle if perception fails or if interfaces between perception and symbolic modules are imperfect.",
            "theoretical_framework": "Modularity and symbol grounding: perception grounds symbols for a symbolic reasoner to perform compositional program-like reasoning, leveraging the complementary strengths of neural perception and symbolic manipulation.",
            "uuid": "e625.2",
            "source_info": {
                "paper_title": "Neurosymbolic AI: the 3rd wave",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Differentiable Theorem Proving",
            "name_full": "Differentiable Unification / Neural Theorem Proving",
            "brief_description": "Approaches that implement theorem proving and unification operations inside differentiable neural architectures, enabling reasoning operations to be trained via gradient descent.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Differentiable Theorem Proving / Differentiable Unification",
            "system_description": "A class of systems that embed logical unification and proof search into neural networks by making unification and proof steps differentiable (e.g., soft unification, differentiable backward/forward chaining), allowing networks to learn to perform logical inference operations and enabling end-to-end training of reasoning behaviors within a neural substrate.",
            "declarative_component": "Logical structures (clauses, rules) and unification/proof operations are represented implicitly and approximated within a differentiable many-valued framework rather than as discrete symbolic procedures.",
            "imperative_component": "Neural network architectures augmented with differentiable modules that mimic unification/theorem-proving steps (e.g., attention-like mechanisms or learned soft-unification operators), trained by gradient descent.",
            "integration_method": "Full integration inside neural architecture: symbolic reasoning primitives (unification, proof search) are implemented as differentiable operators so reasoning becomes part of the neural computation and can be optimized jointly with perception.",
            "emergent_properties": "Potential to learn approximate logical inference within continuous models, permitting gradient-based learning of reasoning behaviors and the combination of perception and reasoning within a single differentiable pipeline.",
            "task_or_benchmark": "Tasks requiring logical deduction and relational reasoning where differentiable proving substitutes classical discrete proof search (review mentions such systems generally).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Aims to enable learned approximate reasoning that may generalize across similar proof patterns; however, scalability and faithful simulation of full discrete logical inference are challenging.",
            "interpretability_properties": "Because proofs are represented approximately inside continuous weights, extracting exact symbolic proofs is difficult; interpretability depends on additional extraction or constraints to yield readable symbolic traces.",
            "limitations_or_failures": "Differentiable implementations of unification and theorem proving face computational efficiency and expressivity challenges; exact symbolic guarantees are generally lost and prior attempts had limited practical efficiency.",
            "theoretical_framework": "Approximation of symbolic proof operations by differentiable many-valued logic and soft unification, enabling gradient-based learning of reasoning within a neural computation paradigm.",
            "uuid": "e625.3",
            "source_info": {
                "paper_title": "Neurosymbolic AI: the 3rd wave",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Logical Neural Networks",
            "name_full": "Logical Neural Networks (LNN)",
            "brief_description": "An approach that establishes a one-to-one correspondence between neurons and elements of logical formulas, providing a tightly coupled localist neural-symbolic mapping for logical reasoning inside networks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Logical Neural Networks (LNN / neuron-to-formula mapping)",
            "system_description": "Tightly-coupled neurosymbolic systems where logical formula elements (atoms, connectives) are mapped directly to neural units, enabling the neural architecture to implement symbolic logic computations in a localist fashion; the mapping can support reasoning inside the network with a clear correspondence to symbolic constructs.",
            "declarative_component": "Explicit logical formulas (often propositional or restricted fragments) represented locally with one-to-one neuron-to-logic element correspondences.",
            "imperative_component": "Neural network execution that directly implements logical connectives and inference via neural activations; training may include weight initialization from logic or further learning.",
            "integration_method": "Compilation of symbolic knowledge into the network's structure/weights (tight localist integration) so that logic is executed by the neural substrate; may include knowledge-based initialization or constraints.",
            "emergent_properties": "Provides clearer interpretability via direct mapping, allows reasoning to be performed inside the neural model with a traceable correspondence to logic, and can combine learning with symbolic guarantees when mappings are exact.",
            "task_or_benchmark": "Symbolic reasoning and tasks where localist logical correspondence is appropriate (mentioned as Type 4 examples in ontology of systems).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Potential to preserve symbolic extrapolation and exact reasoning capabilities while benefiting from neural computation; however, often limited to propositional or restricted logic fragments.",
            "interpretability_properties": "High interpretability due to the explicit neuron-to-formula correspondence; symbolic traces can be directly inspected.",
            "limitations_or_failures": "Scalability and expressiveness limits (difficulty implementing full first-order or higher-order logic); mapping complex symbolic operations into large networks can be challenging.",
            "theoretical_framework": "Compilation/decompilation view: symbolic formulas compiled into network architecture/weights (and extracted back), supporting a principled correspondence between neural computation and logical semantics.",
            "uuid": "e625.4",
            "source_info": {
                "paper_title": "Neurosymbolic AI: the 3rd wave",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Tensorization / TPR",
            "name_full": "Tensor Product Representations (tensorization methods)",
            "brief_description": "Methods that encode symbolic structures into high-dimensional tensors so that symbolic combinations can be implemented as tensor operations within neural networks, used as soft constraints or embeddings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Tensor Product Representations / tensorization methods",
            "system_description": "Approaches that map symbolic roles and fillers into tensor spaces (tensorization) enabling representation of compositional symbolic structures as vectors/tensors; these representations can be used as embeddings or soft constraints inside neural networks and integrated with gradient learning.",
            "declarative_component": "Symbolic structures (roles, fillers, compositional formulas) encoded into tensorial symbolic representations that capture structure in a localist-to-distributed mapping.",
            "imperative_component": "Neural networks that operate over tensorized embeddings and are trained with gradient descent to manipulate and learn from these structured embeddings.",
            "integration_method": "Symbol-to-embedding compilation: symbolic structures are tensorized into distributed representations that the neural network uses and refines; tensorized knowledge can act as soft regularizers or be part of network initialisation.",
            "emergent_properties": "Enables neural models to represent and manipulate structured symbolic information in distributed form, supporting compositionality and richer structure-aware learning than flat embeddings alone.",
            "task_or_benchmark": "Compositional reasoning and relational learning tasks requiring structured representations (review-level mention).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Aims to improve compositional generalization by embedding symbolic structure explicitly into the neural substrate; expected to aid systematic extrapolation when structure is well captured.",
            "interpretability_properties": "Distributed tensor encodings are less directly interpretable than explicit symbolic formulas, though they preserve structural information which can be analyzed.",
            "limitations_or_failures": "Choosing appropriate tensor encodings and scaling to large symbolic structures can be difficult; interpretability and exact symbolic guarantees are reduced compared to localist mappings.",
            "theoretical_framework": "Tensorization principle: encode symbolic compositional structures into tensor spaces so they can be processed by gradient-based neural learners (bridging localist and distributed representations).",
            "uuid": "e625.5",
            "source_info": {
                "paper_title": "Neurosymbolic AI: the 3rd wave",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "AlphaGo (example)",
            "name_full": "AlphaGo (deep neural networks + Monte Carlo tree search)",
            "brief_description": "An exemplar hybrid system where deep neural networks provide policy/value estimates and a symbolic search algorithm (Monte Carlo tree search) performs combinatorial planning, demonstrating effective division of labor between learning and search.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AlphaGo (neural + symbolic search hybrid)",
            "system_description": "A Type-2 hybrid architecture where deep neural networks (policy and value networks) are tightly integrated with a symbolic/planning module (Monte Carlo Tree Search, MCTS). The neural models provide priors and evaluations for game states, while MCTS performs combinatorial search to select moves; the system leverages learning for pattern recognition and search for exact combinatorial planning.",
            "declarative_component": "Combinatorial search algorithm (Monte Carlo Tree Search) used for planning and decision making (symbolic-like discrete search over game trees).",
            "imperative_component": "Deep neural networks (policy and value networks, convolutional architectures) trained by reinforcement learning and supervised learning to provide move priors and state evaluations.",
            "integration_method": "Modular hybrid: neural networks produce estimates and priors that guide the symbolic search algorithm (MCTS); interaction is via network outputs feeding into the search heuristics rather than via symbolic-to-neural compilation.",
            "emergent_properties": "Combines fast perceptual/heuristic evaluation from neural networks with exacting combinatorial search from MCTS, enabling state-of-the-art planning and decision making in large discrete domains (games).",
            "task_or_benchmark": "Board game playing (Go) and other game-playing benchmarks where planning over combinatorial spaces is required.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Neural components generalize from self-play to unseen positions, while MCTS enables robust decision-making by explicit search, improving out-of-distribution robustness in planning contexts.",
            "interpretability_properties": "MCTS provides a traceable search trajectory (plans/rollouts) for decisions, aiding interpretability of choices; neural networks remain less transparent but their outputs (priors/values) can be examined.",
            "limitations_or_failures": "Hybrid coupling can be computationally intensive; reliance on large amounts of self-play data for neural training and heavy search at inference time are practical limitations.",
            "theoretical_framework": "Division of labor between fast approximate pattern-recognition (neural) and slow exact combinatorial reasoning/search (symbolic), illustrating complementary strengths of learning and search.",
            "uuid": "e625.6",
            "source_info": {
                "paper_title": "Neurosymbolic AI: the 3rd wave",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic Tensor Networks: Deep Learning and Logical Reasoning From Data and Knowledge",
            "rating": 2
        },
        {
            "paper_title": "DeepProbLog: Neural Probabilistic Logic Programming",
            "rating": 2
        },
        {
            "paper_title": "Neuro-Symbolic Concept Learner",
            "rating": 2
        },
        {
            "paper_title": "Mastering the game of Go with deep neural networks and tree search",
            "rating": 2
        },
        {
            "paper_title": "Differentiable Theorem Proving",
            "rating": 1
        },
        {
            "paper_title": "Tensor Product Representations and Neural Approaches to Structured Symbols",
            "rating": 1
        },
        {
            "paper_title": "Logical Neural Networks",
            "rating": 1
        }
    ],
    "cost": 0.015163,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neurosymbolic AI: The $3^{r d}$ Wave</h1>
<p>Artur dAvila Garcez ${ }^{1}$ and Lus C. Lamb ${ }^{2}$<br>${ }^{1}$ City, University of London, UK<br>a.garcez@city.ac.uk<br>${ }^{2}$ Federal University of Rio Grande do Sul, Brazil<br>luislamb@acm.org</p>
<p>December, 2020</p>
<h4>Abstract</h4>
<p>Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.</p>
<p>Keywords: Neurosymbolic Computing; Machine Learning and Reasoning; Explainable AI; AI Fast and Slow; Deep Learning.</p>
<h1>1 Introduction</h1>
<p>Over the past decade, Artificial Intelligence and in particular deep learning have attracted media attention, have become the focus of increasingly large research endeavours, and have changed businesses. This led to influential debates on the impact of AI both on academia and industry [52], [66]. It has been claimed that Deep Learning (DL) caused a paradigm shift not only in AI, but in several Computer Science fields, including speech recognition, computer vision and image understanding, natural language processing (NLP) and machine translation [49]. The 2019 Montral AI Debate between Yoshua Bengio and Gary Marcus, mediated by Vincent Boucher [52], and the AAAI-2020 fireside conversation with Economics Nobel Laureate Daniel Kahneman, mediated by Francesca Rossi and including the 2018 Turing Award winners and DL pioneers Geoffrey Hinton, Yoshua Bengio and Yann LeCun, have pointed to new perspectives and concerns on the future of AI. It has now been argued eloquently that if the aim is to build a rich AI system, that is, a semantically sound, explainable and ultimately trustworthy AI system, one needs to include with it a sound reasoning layer in combination with deep learning. Kahneman corroborated this point at AAAI-2020 by stating that ...as far as I'm concerned, System 1 certainly knows language... System 2 does involve certain manipulation of symbols [41]. Kahneman's comments at AAAI-2020 go to the heart of the matter, with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making - reflected in his book "Thinking, Fast and Slow" [40] - and the so-called "AI systems 1 and 2", which would in principle be modelled by deep learning and symbolic reasoning, respectively. ${ }^{1}$</p>
<p>In this paper, we place 20 years of research from the area of neurosymbolic AI, known as neural-symbolic integration, in the context of the recent explosion of interest and excitement about the combination of deep learning and symbolic reasoning. We revisit early theoretical results of fundamental relevance to shaping the latest research, and identify bottlenecks and the most promising technical directions for the sound representation of learning and reasoning in neural and symbolic systems.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>As well as pointing to the various related and promising techniques within AI, ML and Deep Learning, this article seeks to help organise some of the terminology commonly used around AI. This seems important at this exciting time when AI becomes popularized and more people from other areas of Computer Science and from other fields altogether turn to AI: psychology, cognitive science, economics, medicine, engineering and neuroscience to name a few.</p>
<p>In Section 2, we position the current debate in the context of the necessary and sufficient building blocks of AI and long-standing challenges of variable grounding and commonsense reasoning. In Section 3, we seek to organise the debate, which can become vague if defined around the concepts of neurons versus symbols, around the concepts of distributed and localist representations. We argue for the importance of this focus on representation since representation precedes learning as well as reasoning. We also analyse a taxonomy for neurosymbolic AI proposed by Henry Kautz at AAAI-2020 from the angle of localist and distributed representations. In Section 4, we delve deeper into a more technical discussion of current neurosymbolic systems and methods with their pros and cons. In Section 5, we identify promising approaches and directions for neurosymbolic AI from the perspective of learning, reasoning and explainable AI. In Section 6, we return to the debate that was so present at AAAI-2020 to conclude the paper and identify exciting challenges for the third wave of AI.</p>
<h1>2 Neurons and Symbols: Context and Current Debate</h1>
<p>Deep learning researchers and AI companies have achieved groundbreaking results in areas such as computer vision, game playing and natural language processing $[49,83]$ Despite the impressive results, deep learning has been criticised for brittleness (being susceptible to adversarial attacks), lack of explainability (not having a formally defined computational semantics or even intuitive explanation, leading to questions around the trustworthiness of AI systems), and lack of parsimony (requiring far too much data, computational power at training time or unacceptable levels of energy consumption) [52]. Against this backdrop, leading entrepreneurs and scientists such as Bill Gates and the late Stephen Hawking have voiced concerns about AI's accountability, impact on humanity and the future of the planet [74]. The need for a better understanding of the underlying principles of AI has become generally accepted. A key question however is that of identifying</p>
<p>the necessary and sufficient building blocks of AI, and how systems that evolve automatically based on machine learning can be developed and analysed in effective ways that make AI trustworthy.</p>
<p>Turing award winner and machine learning theory pioneer Leslie Valiant pointed out that a key challenge for Computer Science is the principled combination of reasoning and learning, building a rich semantics and robust representation language for intelligent cognitive behaviour [92]. In Valiant's words: "The aim is to identify a way of looking at and manipulating commonsense knowledge that is consistent with and can support what we consider to be the two most fundamental aspects of intelligent cognitive behaviour: the ability to learn from experience and the ability to reason from what has been learned. We are therefore seeking a semantics of knowledge that can computationally support the basic phenomena of intelligent behaviour." Neuralsymbolic computing seeks to offer such a principled way of studying AI by establishing provable correspondences between neural models and logical representations $[4,23,16,20,12]$. In neural-symbolic computation, logic can be seen as a language with which to compile a neural network, as discussed in more detail later in this paper. ${ }^{2}$</p>
<p>The success of deep learning along with a number of drawbacks identified more recently such as a surprising lack of robustness [88] has prompted a heated debate around the value of symbolic AI by contrast with neural computation and deep learning. A key weakness, as Bengio et al. state in a recent article, is that current machine learning methods seem weak when they are required to generalize beyond the training distribution, which is what is often needed in practice [6]. In the recent AI debate between Yoshua Bengio and Gary Marcus, Marcus argues the case for hybrid systems [52] and seeks to define what makes an AI system effectively hybrid:
"Many more drastic approaches might be pursued. Yoshua Bengio, for example, has made a number of sophisticated suggestions for significantly broadening the toolkit of deep learning, including developing techniques for statistically extracting causal relationships through a sensitivity to distributional changes and techniques for automatically extracting modular structure, both</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>of which I am quite sympathetic to. But for reasons that will become apparent, I worry that even these sorts of tools will not suffice on their own for getting us to robust intelligence. Instead, I will propose that in order to get to robust artificial intelligence, we need to develop a framework for building systems that can routinely acquire, represent, and manipulate abstract knowledge, with a focus on building systems that use that knowledge in the service of building, updating, and reasoning over complex, internal models of the external world."</p>
<p>Key to the appreciation of the above statement by Marcus, also advocated in [46] and [28], is an understanding of the representational value of the symbolic manipulation of variables in logic and the compositionality of language. It is probably fair to assume that the next decade will be devoted to researching specific methods and techniques which seek to address the above issues of representation, robustness and extrapolation. Such techniques will be drawn from a broader perspective of neurosymbolic machine learning and AI which embraces hybrid systems, including:
(a) Variable Grounding and Symbol Manipulation: Embracing hybrid systems requires the study of how symbols may emerge and become useful in the context of what deep learning researchers have termed disentanglement. Once symbols emerge (which may happen at different levels of abstraction, ideally within a modular network architecture), it may be more productive from a computational perspective to refer to such symbols and manipulate (i.e. compute) them symbolically rather than numerically. Once it becomes known that a complex neural network serves to calculate, for example, the sum of two handwritten digits provided as input images, or equally that a complex neural network has learned the function $f(x)=x$, then it is probably the case that one would prefer such a calculation to be precise and to extrapolate well to any value of $x$. This is easily achieved symbolically. Reasoning, in many cases too, is preferred to be precise and not approximate, although there are cases where approximate or human-like reasoning become more efficient than logical deduction [34].
(b) Commonsense and Combinatorial Reasoning: Another key distinction that is worth making explicit refers to the difference between commonsense knowledge and expert knowledge. While the former is approximate and difficult to specify, the latter strives to be as precise as possible and to prove its properties. We believe that, once equipped with a solid understanding of the value of hybrid systems, variable manipulation and reasoning, the debate will be allowed to progress from the question of</p>
<p>symbols versus neurons to the research question:
How to compute and learn with symbols, inside or outside of a neural network, and how efficiently computationally, in a precise or approximate reasoning setting?</p>
<p>Foundational work about neurosymbolic models and systems such as $[16,17,20]$ will be relevant as we embark in this journey. In [20], correspondences are shown between various logical-symbolic systems and neural network models. The current limits of neural networks as essentially a propositional ${ }^{3}$ system are also evaluated. In a nutshell, current neural networks are capable of representing propositional logic, nonmonotonic logic programming, propositional modal logic and fragments of first-order logic, but not full first-order or higher-order logic. This limitation has prompted the recent work in the area of Logic Tensor Networks (LTN) [79, 53, 95] which, in order to use the language of full first-order logic with deep learning, translates logical statements into the loss function rather than into the network architecture. First-order logic statements are therefore mapped onto differentiable real-valued constraints using a many-valued logic interpretation in the interval $[0,1]$. The trained network and the logic become communicating modules of a hybrid system, instead of the logic computation being implemented by the network. This distinction between having neural and symbolic modules that communicate in various ways and having translations from one representation to the other in a more integrative approach to reasoning and learning should be at the centre of the debate in the next decade. ${ }^{4}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Among the recent neurosymbolic systems, one can identify quite a variety in range from integrative to hybrid systems: [51] can be seen as a loosely-coupled hybrid approach where image classification is combined with reasoning from text data; [50] offers further integration by allowing a node in the probabilistic inference tree of a symbolic ML system (ProbLog) to be replaced by a neural network; [79] takes another step towards integration by using a differentiable many-valued logic in the loss function of a neural network (in LTN, theorem proving is left for the symbolic counterpart of the system); [54] proposes to perform differentiable unification and theorem proving inside the neural network.</p>
<p>Out of the systems and techniques now available, some more integrative others more loosely-coupled, a common question clearly emerges: what are the fundamental building blocks, the necessary and sufficient components of neurosymbolic AI? For example, is the use of an attention layer necessary [96] or can it be replaced by richer structure such as graph networks [47]? Is the explicit use of probability theory necessary, and in this case inside the network or at the symbolic level or both? Is there a real computational gain in combinatorial problem solving by theorem proving using neural networks or is this task better left to the devices of a symbolic system? One thing is now very clearer: there is great practical value in the use of gradient-based learning on distributed representations[45].</p>
<p>In this paper, we also seek to bring attention to another perhaps less attractive but equally if not more relevant question of adopting a distributed versus a localist representation. In a localist representation the relevant concepts have an associated identifier. This is typically a discrete representation. By contrast, in a distributed representation, concepts are denoted by vectors with continuous values. This is therefore an issue of which representation is adequate or most appropriate. Symbolic machine learning takes a localist approach while neural networks are distributed, although neural networks can also be localist [57]. The next section will be devoted to the pros and cons of distributed and localist representations.</p>
<p>Forms of Neurosymbolic Integration: Within neurosymbolic AI one may identify systems that translate and encode symbolic knowledge in the set of weights of a network [27], or systems that translate and encode symbolic knowledge into the loss function of the network[79]. The neural-symbolic cycle translating symbolic knowledge into neural networks and vice-versa</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>offers a kind of compiler for neural networks ${ }^{5}$, whereby prior knowledge is translated into the network, and a decompiler whenever symbolic descriptions are extracted from a trained network. The compiler can either set-up the network's initial weights akin to a one-shot learning algorithm which is guided by knowledge, or define a knowledge-based penalty or constraint which is added to the network's loss function.</p>
<p>A third form of integration has been proposed in [6] which is based on changing the representation of neural networks into factor graphs. The value of this particular representation deserves to be studied. Change of representation is a worthwhile endeavour on its own right in that it may help us understand the strengths and limitations of different neural models and network architecture choices. This third form of integration, however, proposes to create an intermediate representation with factor graphs in between neural networks and logical representations.</p>
<p>A note about terminology: In [58], Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning [55] (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning. In what follows, we elaborate on the above misunderstandings one at a turn.</p>
<p>Symbolic Machine Learning and Deep Learning: In [58], Pearl proposes a hierarchy consisting of three levels: association, intervention and counterfactual reasoning, and claims that ML is only capable of achieving association. A neurosymbolic or purely symbolic ML system should be capable of satisfying the requirements of all three of Pearl's levels, e.g. by mapping the neural networks onto symbolic descriptions. It is fair to say in relation to Pearl's top level in the hierarchy - counterfactual reasoning - that</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>progress has only been made recently and that much research is still needed, although good progress is being made towards the extraction of local, measurable counterfactual explanations from black box ML systems [99]. Once a neural network has been endowed with a symbolic interpretation, one has no reason to doubt the ability of a neural system to ask what if questions. In fact, the very algorithm for extracting symbolic logic descriptions of the form $A \rightarrow B$ from trained neural networks [15] uses a form of interrogation of the network akin to the intervention of Bayesian models advocated by Pearl. We argue therefore that a more important question is representational: which representation is most effective, deep networks or Bayesian networks? While attempting to answer this question, as well as considering the demands of the practical applications, it is important to recognise that neural networks offer a concrete model of computation, one which can be implemented efficiently by message passing or propagation of activation, differently from Bayesian networks, and trained by differentiable learning algorithms. A limitation of having such a concrete computational model, however, may be a difficulty of pure neural networks at modelling rich forms of abstraction which are not dependent on the data (images, audio, etc.) but which exist instead at a higher conceptual level. We shall return to this challenge later in the paper.</p>
<p>Knowledge Representation and Reasoning in AI: Complex problem solving using AI requires a much richer language than that of expert systems as suggested by Hinton [36]. AI requires a language that can go well beyond Horn clauses to include relational knowledge, time and other modalities, negation by failure, variable substitution and quantification, etc. In statistical relational learning, the use of first-order logic does not require instantiating (or grounding) all possible combinations of the values of the variables (e.g. $X$ and $Y$ in a relation $R(X, Y)$ ). In relational reasoning with neural networks, borrowing from the field of relational databases, it is typically the grounded (and therefore propositional rather than first-order) representation that is learned and reasoned about. For the avoidance of confusion, we would term this latter task relationship learning. Two other equally important attributes of a rich language for complex problem solving are compositionality, in the sense of the compositionality of the semantics of a logical language, and modularity. It is worth noting that in the original paper about deep learning [37], before much of the attention turned to convolutional networks, modularity was a main objective of the proposed semisupervised greedy learning of stacks of restricted Boltzmann machines. The recently-proposed stacked capsule autoencoders [44] and neural-symbolic approaches such as Logic Tensor Networks [79] as well as other weakly-</p>
<p>supervised approaches revive the important stance of modularity in neural computation. Earlier efforts towards modularity in neurosymbolic AI can be traced back to the system for Connectionist Modal and Intuitionistic Logics [18, 19]. Modal logics with a possible-world semantics have been shown to offer a natural approach to modularity in neural computation [20].</p>
<p>With AI understood as a superset of ML which in turn is a superset of DL, we shall argue for the combination of statistical machine learning, knowledge representation (KR) and logical reasoning. By logical reasoning, we shall mean not only classical logic reasoning with the traditional true-false interpretation, but non-classical reasoning including nonmonotonic, modal and many-valued logics. In the study of the interplay between learning and reasoning and how best to implement it (e.g. in a continuous or discrete system), it shall become clear that universal quantification is easy to reason about and hard to learn using neural networks; existential quantification is easy to learn and harder to reason about in a symbolic system. Such limitations on either side of the spectrum will dictate a few practical design decisions to be discussed in this paper. In a nutshell, we claim that neurosymbolic AI is well placed to address concerns of computational efficiency, modularity, $K R+M L$ and even causal inference. More researchers than ever on both sides of the connectionist-symbolic AI divide are now open to studying and learning about each others' tools and techniques. This was not the case until very recently. The use of different terminology alongside some preconceived opinion or perhaps idleness, fuelled by the way that science normally rewards research that is carried out in silos, have prevented earlier progress. The fact that this is now changing will lead to faster progress in the overall field of AI. It is reassuring to see it happening in this way: the neural information processing community have shown the value of neural computation in practice, which has attracted the curiosity of great minds from symbolic AI. We hope that further collaboration in neurosymbolic AI will help solve many of the issues which are still outstanding.</p>
<h1>3 Distributed and Localist Representation</h1>
<p>The integration of learning and reasoning through neurosymbolic systems requires a bridge between localist and distributed representations. The success of deep learning indicates that distributed representations with gradientbased methods are more adequate than localist ones for learning and optimization. At the same time, the difficulty of neural networks at extrapolation, explainability and goal-directed reasoning point to the need of a bridge</p>
<p>between distributed and localist representations for reasoning.
Neural-symbolic computing has been an active area of research seeking to establish such a bridge for several years $[4,26,16,20,35,43,80,93]$. In neural-symbolic computation, knowledge learned by a neural network can be represented symbolically. Reasoning takes place either symbolically or within the network in distributed form. Despite their differences, both the symbolic and connectionist paradigms share common characteristics, offering benefits when put together in a principled way (see e.g. [17, 20, 84, 93]). Change of representation also offers a way of making sense of the value of different neural models and architectures with respect to what is a more formal and better understood area of research: symbolic logic.</p>
<p>Neural network-based learning and inference under uncertainty have been expected to address the brittleness and computational complexity of symbolic systems. Symbolism has been expected to provide additional knowledge in the form of constraints for learning [23, 32], which ameliorate neural network's well-known catastrophic forgetting or difficulty with extrapolation in unbounded domains or with out-of-distribution data. The integration of neural models with logic-based symbolism is expected therefore to provide an AI system capable of explainability, transfer learning and a bridge between lower-level information processing (for efficient perception and pattern recognition) and higher-level abstract knowledge (for reasoning, extrapolation and planning).</p>
<p>Suppose that a complex neural network learns a function $f(x)$. Once this function is known, or more precisely a simplified description of $f(x)$ is known, computationally it makes sense to use such a representation, not least for the sake of extrapolation, as exemplified earlier with the $f(x)=x$ function. One could argue that at this point the neural network has become superfluous. Symbol manipulation (once symbols have been discovered) is key to further learning at new levels of abstraction. This is exemplified well in [52] with the use of the concept of a container which may be learned from images.</p>
<p>Among the most promising recent approaches to neural-symbolic integration, so-called embedding techniques seek to transform symbolic representations into vector spaces where reasoning can take place through matrix computations over distance functions $[7,85,87,80,75,11,26,100,24,70]$. In such systems, learning of an embedding is carried out using backpropagation [98, 73]. Most of the research in this area is focused on the art of representing relational knowledge such as $P(X, Y)$ in a distributed neural network. The logical predicate $P$ relating variables $X$ and $Y$ could be used to denote, for example, the container relation between two objects in an image</p>
<p>such as a violin and its case, which are in turn described by their embedding. This process is known as relational embedding $[7,75,85,87]$. For representing more complex logical structures such as first order-logic formulas, e.g. $\forall X, Y, Z:(P(X, Y) \rightarrow Q(Y, Z))$, a system named Logic Tensor Networks (LTN) [80] was proposed by extending Neural Tensor Networks (NTN) [85], a state-of-the-art relational embedding method. Related ideas are discussed formally in the context of constraint-based learning and reasoning in [32].</p>
<p>Two powerful concepts of LTN are (1) the grounding of logical concepts onto tensors with the use of logical statements which act as constraints on the vector space to help learning of an adequate embedding, and (2) the modular and differentiable organisation of knowledge within the neural network which allows querying and interaction with the system. Any userdefined statement in first-order logic can be queried in LTN which checks if that knowledge is satisfied by the trained neural network. With such a tool, a user can decide when to keep using a distributed connectionist representation or switch to a localist symbolic representation. This last aspect brings the question of the emergence of symbols and their meaning in neural networks to the fore: recent work using the weakly supervision of auto-encoders and ideas borrowed from disentanglement have been showing promise in the direction of learning relevant concepts which can in turn be re-used symbolically [10]. Related work seeking to explore the advantages of distributed representations of logic include [11], which is based on stochastic logic programs, [26, 100, 24], with a focus on inductive programming, and [70], based on differentiable theorem proving.</p>
<p>A taxonomy for neurosymbolic AI: with an understanding of the role of localist and distributed approaches, we now provide an analysis of Henry Kautz's taxonomy for neurosymbolic AI [42], which was introduced at AAAI 2020: In Kautz's taxonomy, a TyPe 1 neural-symbolic integration is standard deep learning, which some may argue is a stretch, but which is included by Kautz to note that the input and output of a neural network can be made of symbols, e.g. text in the case of language translation or question answering applications. TyPe 2 are hybrid systems such as DeepMind's AlphaGo and other systems where the core neural network is loosely-coupled with a symbolic problem solver such as Monte Carlo tree search. TyPe 3 is also a hybrid system whereby a neural network focusing on one task (e.g. object detection) interacts via its input and output with a symbolic system specialising in a complementary task (e.g. query answering). Examples include the neuro-symbolic concept learner [51] and deepProbLog [50]. In a TyPe 4 neural-symbolic system, symbolic knowledge is compiled into the training set of a neural network. Kautz offers [48] as an example (to be</p>
<p>read alongside the critique in [21]). An approach to learn and reason over mathematical constructions is proposed in [2], and in [1] a learning architecture that extrapolates to harder symbolic maths reasoning problems is introduced. We would also include in Type 4 other tightly-coupled but localist neural-symbolic systems where various forms of symbolic knowledge, not restricted to if-then rules, is translated into the initial architecture and set of weights of a neural network, in some cases with guarantees of correctness [20], as well as Logical Neural Networks, where the key concept is to create a 1-to-1 correspondence between neurons and the elements of logical formulas [69]. Type 5 are those tightly-coupled but distributed neuralsymbolic systems where a symbolic logic rule is mapped onto an embedding which acts as a soft-constraint (a regularizer) on the network's loss function. Examples of these include Logic Tensor Networks [79] and Tensor Product Representations [39], referred to in [13] as tensorization methods. Finally, a Type 6 system should be capable, according to Kautz, of true symbolic reasoning inside a neural engine. This is what one could refer to as a fullyintegrated system. Early work in neural-symbolic computing has achieved this (see [20] for a historical overview). Some Type 4 systems are also capable of it, but using a localist rather than a distributed representation and using much simpler forms of embedding than Type 5 systems. Kautz adds that a Type 6 system should be capable of combinatorial reasoning, possibly by using an attention schema to achieve it effectively. Recent efforts in this direction include $[8,47,63]$, although a fully-fledged Type 6 system for combinatorial reasoning does not exist yet.</p>
<p>Further research into Type 5 systems will likely focus on the provision of rich embeddings and the study of the extent to which such embeddings may correspond either to pre-defined prior knowledge or to learned attention mechanisms. Further research onto TyPe 6 systems is highly relevant to the theory of neural-symbolic computing, as discussed in more detail in the next section. In practical terms, a tension exists between effective learning and sound reasoning, which may prescribe the use of a more hybrid approach of Types 3 to 5 , or variations thereof such as the use of attention with tensorization. Orthogonal to the above taxonomy, but mostly associated thus far with TyPe 4, is the study of the limits of reasoning within neural networks, which was already of interest since the first efforts by Valiant at providing a foundation for computational learning [91]. Recently, this has been the focus of experimental analyses of deep learning in symbolic domains [89], and it should include the study of first-order logic, higher-order, manyvalued and non-classical logic.</p>
<h1>4 Neurosymbolic Computing Systems: Technical Aspects</h1>
<p>In symbolic ML, symbols are manipulated as part of a discrete search for the best representation to solve a given classification or regression task. The most well-known form of symbolic ML are decision trees, but richer forms of representation exist, in particular relational representations using first-order logic to denote concepts ranging over variables $X, Y, Z \ldots$ within a (possibly infinite) domain, e.g. $\forall X, Y, Z: \operatorname{grandfather}(X, Y) \leftarrow$ (father $(X, Z) \wedge \operatorname{mother}(Z, Y))$ (the father of someone's mother is that person's grandfather). Probabilistic extensions of this approach seek to learn probability distributions for such logical rules (or functional programs) as a way of accounting for uncertainty in the training data. Work in these areas is probably best characterised by the conference series on Inductive Logic Programming [65, 56], Statistical Relational Learning [68, 25, 3, 81] and Probabilistic or Inductive Programming [77].</p>
<p>All of the excitement and industrial interest in the past 10 years surrounding AI and Machine Learning, though, have come from an entirely separate type of ML: deep learning. Deep learning uses neural networks and stochastic gradient descent to search through a continuous space, also to solve a given classification or regression task, but creating vector-based, distributed representations, rather than logical or symbolic ones. For this reason, such systems are called sub-symbolic.</p>
<p>Whilst it is clear now that AI will not be achieved by building expert systems by hand from scratch (GOFAI), but by learning from large collections of data, one would be misguided to conflate all of machine learning or dismiss the role of symbolic logic, which remains the most powerful and adequate representation for the analysis of computational systems. As put simply by Moshe Vardi "Logic is the Calculus of Computer Science" and, differently from statistics, machine learning can only exist within the context of a computational system.</p>
<p>Specifically, deep neural networks will require a language for description, as also advocated by Leslie Valiant. Neural network-based AI is distributed and continuous, deals well with large-scale multimodal noisy perceptual data such as text and audio, handles symbol grounding better than symbolic systems since concepts are grounded on feature vectors, and is by definition a computational model, frequently implemented efficiently using propagation of activation and tensor processing units. ${ }^{6}$ Symbolic AI is generally localist</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and discrete, capable of sophisticated reasoning, including temporal, epistemic and nonmonotonic reasoning, planning, extrapolation and reasoning by analogy. Neurosymbolic AI has shown that non-classical logics, in particular many-valued logics, offer an adequate language for describing neural networks [79, 69]. As the field of AI moves towards agreement on the need for combining the strengths of neural and symbolic AI, it should turn next to the question: what is the best representation for neurosymbolic AI?</p>
<p>To answer this question, one should seek to be informed by developments in neural-symbolic computing of the past 20 years, and to evaluate in a precise manner the methods, algorithms and applications of neurosymbolic AI. For instance, it is known that current recurrent neural networks are capable of computing the logical consequences of propositional modal logic programs and other forms of non-classical reasoning and fragment of first order logic programs [5, 20]. Obtaining results for full first-order logic has not been possible thus far, which reinforces John McCarthy's claim that neural networks are essentially propositional. In terms of applications of AI, these have been largely focused on perceptual or pattern matching tasks such as image and audio classification. Recent efforts at question answering and language translation as well as protein folding classification have highlighted the importance of the neurosymbolic approach. The ideal type of application for a neurosymbolic system, however, should be that where abstract information is required to be reasoned about at different levels beyond that what can be perceived from data alone, such as complex concept learning whereby simpler concepts are required to be organised systematically as part of the definition of a higher concept. Such a conceptual structure, which is still to be discovered using data, also requires knowledge which is governed by general rules and exceptions to the rules, allowing for sound generalization in the face of uncertainty but also capable of handling specific cases (the many exceptions, which may be important for the sake of robustness although not necessarily statistically relevant). Similarly, it seems hard to achieve true relational learning using only neural networks. A useful but simple example can be borrowed from the area of Inductive Logic Programming: learning the concept of ancestor from a few examples of the mother, father and grandparent relations. Grounding the entire knowledge-base in this case would not be productive since the chain of reasoning to derive the concept of ancestor may be arbitrarily large depending on the data available. In this case, one is better off learning certain relations by jumping to conclu-</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>sions, such as e.g. $\forall X, Y: f a t h e r(X, Y) \rightarrow \operatorname{ancestor}(X, Y)$, from relatively few examples and using similarity measures to infer new relations, at the same time deriving symbolic descriptions which can be used for reasoning beyond the distribution of the data, allowing in turn for extrapolation. In this example, once a description for ancestor is obtained, one should be able to reason about arbitrarily long chains of family relationships. Notice that key to this process is the ability to revise the conclusion taken once new evidence to the contrary of what has been inferred is made available from the data. In other words, the reasoning here is nonmonotonic [14].</p>
<p>In summary, at least two options exist for neurosymbolic AI. In Option 1, symbols are translated into a neural network and one seeks to perform reasoning within the network. In Option 2, a more hybrid approach is taken whereby the network interacts with a symbolic system for reasoning. A third option, which would not require a neurosymbolic approach, exists when expert knowledge is made available, rather than learned from data, and one is interested in achieving precise sound reasoning as opposed to approximate reasoning. We discuss each option briefly next.</p>
<p>In Option 1, it is desirable still to produce a symbolic description of the network for the sake of improving explainability (discussed later) or trust, or for the purpose of communication and interaction with the system. In Option 2, by definition, a neurosymbolic interface is needed. This may be the best option in practice given the need for combining reasoning and learning in AI, and the apparent different nature of both tasks (discrete and exact versus continuous and approximate). However, the value of distributed approximate reasoning using neural networks is only starting to be investigated as in the case of differentiable neural computers [33] and neural theorem proving [97], although early efforts did not prove to be promising in terms of practical efficiency $[62,38,82]$. In Option 3, a reasonable requirement nowadays would be to compare results with deep learning and the other options. This is warranted by the latest practical results of deep learning showing that neural networks can offer, at least from a computational perspective, better results than purely symbolic systems.</p>
<p>In practice, the choice between Options 1 and 2 above may depend on the application at hand and the availability of quality data and knowledge. A comparatively small number of scientists will continue to seek to make sense of the strengths and limitations of both neural and symbolic approaches. On this front, the research advances faster on the symbolic side due to the clear hierarchy of semantics and language expressiveness and rigour that exists at the foundation of the area. By contrast, little is known about the expressiveness of the latest deep learning models in relation to established</p>
<p>neural models beyond data-driven comparative empirical evaluations. As advocated by Paul Smolensky, neurosymbolic computing can help map the latest neural models into existing symbolic hierarchies, thus helping organise the extensively ad-hoc body of work in neural computation.</p>
<h1>5 Challenges for the Principled Combination of Reasoning and Learning</h1>
<p>For a combined perspective on reasoning and learning, it is useful to note that reasoning systems may have difficulties computationally when reasoning with existential quantifiers and function symbols, such as $\exists x P(f(x))$. Efficient logic-based programming languages such as Prolog, for example, assume that every logical statement is universally quantified. By contrast, learning systems may have difficulty when adopting universal quantification over variables. To be able to learn a universally quantified statement such as $\forall x P(x)$, a learning systems needs in theory to be exposed to all possible instances of $x$. This simple duality points to a possible complementary nature of the strengths of learning and reasoning systems. To learn efficiently $\forall x P(x)$, a learning system needs to jump to conclusions, extrapolating $\forall x P(x)$ given an adequate amount of evidence (the number of examples or instances of $x$ ). Such conclusions may obviously need to be revised over time in the presence of new evidence, as in the case of nonmonotonic logic. In this case, a statement of the form $\forall x P(x)$ becomes a data-dependent generalization, which is not to be assumed equivalent to a statement $\forall y P(y)$, as done in classical logic. Such statements may have been learned from different samples of the overall potentially infinite population. On the other hand, a statement of the form $\exists x P(x)$ is trivial to learn from data by identifying at least one case $P(a)$, although reasoning from $\exists x P(x)$ is more involved, requiring the adoption of an arbitrary constant $b$ such that $P(b)$ holds.</p>
<p>It is now accepted that learning takes place on a continuous search space of (sub)differentiable functions; reasoning takes place in general on a discrete space as in the case of goal-directed theorem proving. The most immediate way of benefiting from the combination of reasoning and learning, therefore, is to adopt a hybrid approach whereby a neural network takes care of the continuous search space and learning of probabilities, while a symbolic system consisting of logical descriptions of the network uses discrete search to achieve extrapolation and goal-directed reasoning. ${ }^{7}$ As hinted already in</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>this paper, a property of early deep learning may hold the key to the above hybrid perspective: modularity. In the original paper on deep learning [37], a modular system is proposed consisting of a stack of restricted Boltzmann machines (RBMs). The extraction of symbolic descriptions from each RBM is thus made considerably easier [90]. Each RBM learns a joint probability distribution while their symbolic description reflects the result of learning without manipulating probabilities explicitly, thus avoiding the complexity of probabilistic inference found in symbolic AI.</p>
<p>In [90], an efficient algorithm is presented that extracts propositional rules enriched with confidence values from RBMs, similar to what was proposed with Penalty Logic for Hopfield networks in [62]. When RBMs are stacked onto a deep belief network, however, the modular extraction of compositional rules may be accompanied by a compounding loss of accuracy, indicating that knowledge learned by the neural network might not have been as modular as one would have wished. Systems that impose a more explicit separation of modules may hold the answer to this problem, in particular systems where unsupervised learning is combined with weakly-supervised classification at distinct levels of abstraction, such as with the use of variational auto-encoders [86] or generative adversarial networks [31]. In the area of Reinforcement Learning too, deep learning systems combined with symbolic rules were first proposed in [29], and there is promise of considerable progress with the use of neurosymbolic approaches especially in the case of model-based Reinforcement Learning.</p>
<p>We therefore do not advocate the adoption of monoblock networks with millions of parameters. Even though this may be how the human brain works, loss of modularity seems to be, at least at present from a computational perspective, a price that is too high to pay. Modularity remains a fundamentally relevant property of any computing system.</p>
<p>Applications of Neurosymbolic AI: One major way of driving advances in AI continues to be through challenging applications, be it language translation, computer games or protein folding competitions. Language understanding in the broadest sense of the term, including question-answering that requires commonsense reasoning, offers probably the most complete application area for neurosymbolic AI. As an example, consider this question and its commonsense answer from the COPA data set [72]: It got dark outside. What happened as a result? (a) Snowflakes began to fall from</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the sky; (b) The moon became visible in the sky. Another very relevant application domain is planning, which requires learning and reasoning over time, as in this example adapted from [76]: Daniel picks up the milk; Daniel goes to the bedroom; Daniel places the milk on the table; Daniel goes to the bathroom. Where is the milk? Finally, an area where machine learning and knowledge representation and reasoning have complementary strengths is knowledge engineering, including knowledge-base completion and datadriven ontology learning. In this area of application, rich and large-scale symbolic representations exist alongside data, including knowledge graphs to be combined with neural networks such as graph neural networks.</p>
<p>A common thread across the above examples and applications is the need for modelling cause and effect with the use of implicit information. This requires learning of general rules and exceptions to the rules that evolve over time. In such cases, deep learning alone fails when presented with examples from outside the distribution of the training data. This motivated Judea Pearl's critique of Machine Learning [58] which we shall address in some detail next.</p>
<p>In Pearl's 3-level causal hierarchy (association, intervention and counterfactuals), association involves purely statistical relationships. In Pearl's words, observing a customer who buys toothpaste makes it more likely that this customer will also buy floss. Such associations can be inferred directly from the observed data using standard conditional probabilities and conditional expectation or other standard non-probabilistic ML model. Questions asked at this level require no causal information and, for this reason, this layer is placed at the bottom of the hierarchy. Answering such questions is the hallmark of current machine learning methods. Pearl's hierarchy may unintentionally give the impression that machine learning is confined to this bottom layer, since no reference is made in [58] to the body of work on symbolic machine learning which is unequivocally not confined to association rules [55, 70]. Pearl continues: the second level, intervention, ranks higher than association because it involves not just seeing what is but changing what we see. A typical question at this level would be: what will happen if we double the price? Such a question cannot be answered from sales data alone, as it involves a change in customers' choices in reaction to the new pricing. These choices may differ substantially from those taken in previous price-raising situations, unless we replicate precisely the market conditions that existed when the price reached double its current value. At this level there is a need for inference that can reach beyond the data distribution. Finally, the top level invokes counterfactuals, a mode of reasoning that reverts to the philosophers David Hume and John Stuart Mill and that has</p>
<p>been given computational semantics in the past two decades. A typical question in the counterfactual category is: what if I had acted differently?, thus necessitating retrospective reasoning.</p>
<p>As noted earlier in the paper, neural-symbolic computing can implement all three of Pearl's levels. Once a symbolic description of the form if $A$ then $B$ has been associated with a neural network, surely the idea of intervention [9] and counterfactual reasoning become possible, c.f. for example, [99] on the measurement and extraction of counterfactual knowledge from trained neural networks.</p>
<h1>Our conclusion from the above discussion is that in neurosymbolic AI:</h1>
<ul>
<li>Knowledge should be grounded onto vector representations for efficient learning from data based on message passing in neural networks as an efficient computational model.</li>
<li>Symbols should become available as a result of querying and knowledge extraction from trained networks, and offer a rich description language at an adequate level of abstraction, enabling infinite uses of finite means, but also compositional discrete reasoning at the symbolic level allowing for extrapolation beyond the data distribution.</li>
<li>The combination of learning and reasoning should offer an important alternative to the problem of combinatorial reasoning by learning to reduce the number of effective combinations, thus producing simpler symbolic descriptions as part of the neurosymbolic cycle.</li>
</ul>
<p>As an example, consider an Autoencoder which learns in unsupervised fashion to maximise mutual information between pixel inputs and a latent code or some other embedding consisting of fewer relevant features than pixels. Suppose that this neural network has learned to find regularities such as e.g. when it sees features of type $A$, it also sees features of type $B$ but not features of type $C$. At this point, such regularities can be converted into symbols: $\forall x A(x) \rightarrow(\exists y B(y) \wedge \neg \exists z C(z))$. As a result of the use of variables $x, y, z$ at the symbolic level, one can extrapolate the above regularity to any features of type $A, B$ or $C .{ }^{8}$ A symbolic description is also a constraint on the neurosymbolic cycle. It is generalised from data during learning and</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>it certainly includes an ability to ask what-if questions. Reasoning about what has been learned allows for extrapolation beyond the data distribution, and finally the symbolic description can serve as prior knowledge (as a constraint) for further learning in the presence of more data, which includes the case of knowledge-based transfer learning [90]. Further training can now take place at the perceptual or sub-symbolic level, or at the conceptual or symbolic level. This is when having a distributed (sub-symbolic) and a localist (symbolic or sub-symbolic) representation becomes relevant. Assuming that probabilities are dealt with at the sub-symbolic distributed level (as in RBMs) and that the symbolic level is used for a more qualitative representation of uncertainty in the form of general rules with exceptions, we avoid the complications of having to deal simultaneously with discrete and continuous learning of rules and probabilities.</p>
<p>A Note about Explainable AI (XAI): Knowledge extraction is an integral part of neural-symbolic integration and a major ingredient towards explainability of black-box AI systems. The main difficulty in XAI is the efficient extraction of compact and yet correct and complete knowledge. It can be argued that a large knowledge-base is not more explainable than a large neural network. ${ }^{9}$ Although this may be true at the level of the entire model explainability, in the case of local explanations, i.e. explanations of individual cases, a knowledge-base is certainly more explainable than a neural network because it offers a trace (a proof history) showing why an outcome was obtained, as opposed to simply showing how propagation of activation through the network has led to that outcome. It is a main goal of knowledge extraction algorithms to seek to derive compact relevant representations from large complex networks. This is not always possible to achieve efficiently, in which case one may need to resort to having local explanations only.</p>
<p>Many aspects of XAI are being investigated at present. Some of the research questions include: Is the explanation intended for an expert or lay person? Is an explanation required because one does not trust the system, expected a different outcome/would like to induce a different outcome, or would like to question the normative system that has led to the outcome? Is an explanation intended to try and improve system performance, reduce bias/increase fairness, or is it the case that one would simply like to be able to understand the decision process? The answers to these questions are likely</p>
<p><sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ The now apparent lack of explainability of Random Forests, which amount to a collection of Decision Trees and therefore propositional logic formulas with probabilities, serves as a good reminder that XAI is not confined to neural networks.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>